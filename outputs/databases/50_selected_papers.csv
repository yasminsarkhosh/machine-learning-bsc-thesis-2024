,title,header_no,header_title,text,volume
109,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,3.0,Experiments and Discussion,"dataset: we use the publicly available decath-pancreas dataset of 273
segmentations from patients who underwent pancreatic mass resection [24]. the
shapes of the pancreas are highly variable and have thin structures, making it a
good candidate for non-linear ssm analysis. the segmentations were isotropically
resampled, smoothed, centered, and converted to meshes with roughly 2000
vertices. although the dgcnn mesh autoencoder used in mesh2ssm does not require
the same number of vertices, uniformity across the dataset makes it
computationally efficient; hence, we pad the smallest mesh by randomly repeating
the vertices (akin to padding image for convolutions). the samples were randomly
divided, with 218 used for training, 26 for validation, and 27 for testing.
flowssm [15] with two templates: sphere, medoid. the color map and arrows show
the signed distance and direction from the mean shape.",1
142,SLPD: Slide-Level Prototypical Distillation for WSIs,2.3,Slide-Level Clustering,"many histopathologic features have been established based on the morphologic
phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and
mitoses, which are then used for cancer diagnosis, prognosis and the estimation
of response-to-treatment in patients [3,9]. to obtain meaningful representations
of slides, we aim to explore and maintain such histopathologic features in the
latent space. clustering can reveal the representative patterns in the data and
has achieved success in the area of unsupervised representation learning
[4,5,24,26].to characterize the histopathologic features underlying the slides,
a straightforward practice is the global clustering, i.e., clustering the region
embeddings from all the wsis, as shown in the left of fig. 1(d). however, the
obtained clustering centers, i.e., the prototypes, are inclined to represent the
visual bias related to staining or scanning procedure rather than medically
relevant features [33]. meanwhile, this clustering strategy ignores the
hierarchical structure ""region→wsi→whole dataset"" underlying the data, where the
id of the wsi can be served as an extra learning signal. therefore, we first
consider the slidelevel clustering that clusters the embeddings within each wsi,
which is shown in the right of fig. 1(d). specifically, we conduct k-means
algorithm before the start of each epoch over l n region embeddings {z l n } ln
l=1 of w n to obtain m prototypes {c m n ∈ r d } m m=1 . similar operations are
applied across other slides, and then we acquire n groups of prototypes {{c m n
} m m=1 } n n=1 . each group of prototypes is expected to encode the semantic
structure (e.g., the combination of histopathologic features) of the wsi.",1
144,SLPD: Slide-Level Prototypical Distillation for WSIs,2.5,Inter-Slide Distillation,"tumors of different patients can exhibit morphological similarities in some
respects [17,21], so the correspondences across slides should be characterized
during learning. previous self-supervised learning methods applied to
histopathologic images only capture such correspondences with positive pairs at
the patchlevel [22,23], which overlooks the semantic structure of the wsi. we
rethink this problem from the perspective how to measure the similarity between
two slides accurately. due to the heterogeneity of the slides, comparing them
with the local crops or the averaged global features are both susceptible to
being one-sided. to address this, we bridge the slides with their semantic
structures and define the semantic similarity between two slides w i and w j
through an optimal bipartite matching between two sets of prototypes:where
cos(•, •) measures the cosine similarity between two vectors, and s m enumerates
the permutations of m elements. the optimal permutation σ * can be computed
efficiently with the hungarian algorithm [19]. with the proposed setto-set
distance, we can model the inter-slide correspondences conveniently and
accurately. specifically, for a region embedding z belonging to the slide w and
assigned to the prototype c, we first search the top-k nearest neighbors of w in
the dataset based on the semantic similarity, denoted as { ŵk } k k=1 . second,
we also obtain the matched prototype pairs {(c, ĉk )} k k=1 determined by the
optimal permutation, where ĉk is the prototype of ŵk . finally, we encourage z
to be closer to ĉk with the inter-slide distillation:the inter-slide
distillation can encode the sldie-level information complementary to that of
intra-slide distillation into the region embeddings.the overall learning
objective of the proposed slpd is defined as:where the loss scale is simply set
to α 1 = α 2 = 1. we believe the performance can be further improved by tuning
this.",1
158,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,1.0,Introduction,"in recent years, the workload of radiologists has grown drastically, quadrupling
from 2006 to 2020 in western europe [4]. this huge increase in pressure has led
to long patient-waiting times and fatigued radiologists who make more mistakes
[3]. the most common of these errors is underreading and missing anomalies
(42%); followed by missing additional anomalies when concluding their search
after an initial finding (22%) [10]. interestingly, despite the challenging work
environment, only 9% of errors reviewed in [10] were due to mistakes in the
clinicians' reasoning. therefore, there is a need for automated second-reader
capabilities, which brings any kind of anomalies to the attention of
radiologists. for such a tool to be useful, its ability to detect rare or
unusual cases is particularly important. traditional supervised models would not
be appropriate, as acquiring sufficient training data to identify such a broad
range of pathologies is not feasible. unsupervised or self-supervised methods to
model an expected feature distribution, e.g., of healthy tissue, is therefore a
more natural path, as they are geared towards identifying any deviation from the
normal distribution of samples, rather than a particular type of pathology.there
has been rising interest in using end-to-end self-supervised methods for anomaly
detection. their success is most evident at the miccai medical
outof-distribution analysis (mood) challenge [31], where all winning methods
have followed this paradigm so far (2020-2022). these methods use the variation
within normal samples to generate diverse anomalies through sample mixing
[7,[23][24][25]. however all these methods lack a key component: structured
validation. this creates uncertainty around the choice of hyperparameters for
training. for example, selecting the right training duration is crucial to avoid
overfitting to proxy tasks. yet, in practice, training time is often chosen
arbitrarily, reducing reproducibility and potentially sacrificing generalisation
to real anomalies.",1
238,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,1.0,Introduction,"a common challenge for deploying deep learning to clinical problems is the
discrepancy between data distributions across different clinical sites
[6,15,20,28,29]. this discrepancy, which results from vendor or protocol
differences, can cause a significant performance drop when models are deployed
to a new site [2,21,23]. to solve this problem, many unsupervised domain
adaptation (uda) methods [6] have been developed for adapting a model to a new
site with only unlabeled data (target domain) by transferring the knowledge
learned from the original dataset (source domain). however, most uda methods
require sufficient target samples, which are scarce in medical imaging due to
the limited accessibility to patient data. this motivates a new problem of
few-shot unsupervised domain adaptation (fsuda), where only a few unlabeled
target samples are available for training.few approaches [11,22] have been
proposed to tackle the problem of fsuda. luo et. al [11] introduced adversarial
style mining (asm), which uses a pretrained style-transfer module to generate
augmented images via an adversarial process. however, this module requires extra
style images [9] for pre-training. such images are scarce in clinical settings,
and style differences across sites are subtle. this hampers the applicability of
asm to medical image analysis. sm-ppm [22] trains a style-mixing model for
semantic segmentation by augmenting source domain features to a fictitious
domain through random interpolation with target domain features. however, sm-ppm
is specifically designed for segmentation tasks and cannot be easily adapted to
other tasks. also, with limited target domain samples in fsuda, the random
feature interpolation is ineffective in improving the model's
generalizability.in a different direction, numerous uda methods have shown high
performance in various tasks [4,[16][17][18]. however, their direct application
to fsuda can result in severe overfitting due to the limited target domain
samples [22]. previous studies [7,10,24,25] have demonstrated that transferring
the amplitude spectrum of target domain images to a source domain can
effectively convey image style information and diversify training dataset. to
tackle the overfitting issue of existing uda methods, we propose a novel
approach called sensitivityguided spectral adversarial mixup (samix) to augment
training samples. this approach uses an adversarial mixing scheme and a spectral
sensitivity map that reveals model generalizability weaknesses to generate
hard-to-learn images with limited target samples efficiently. samix focuses on
two key aspects. 1) model generalizability weaknesses: spectral sensitivity
analysis methods have been applied in different works [26] to quantify the
model's spectral weaknesses to image amplitude corruptions. zhang et al. [27]
demonstrated that using a spectral sensitivity map to weigh the amplitude
perturbation is an effective data augmentation. however, existing sensitivity
maps only use single-domain labeled data and cannot leverage target domain
information. to this end, we introduce a domain-distance-modulated spectral
sensitivity (dodiss) map to analyze the model's weaknesses in the target domain
and guide our spectral augmentation. 2) sample hardness: existing studies
[11,19] have shown that mining hard-to-learn samples in model training can
enhance the efficiency of data augmentation and improve model generalization
performances. therefore, to maximize the use of the limited target domain data,
we incorporate an adversarial approach into the spectral mixing process to
generate the most challenging data augmentations. this paper has three major
contributions. 1) we propose samix, a novel approach for augmenting target-style
samples by using an adversarial spectral mixing scheme. samix enables
high-performance uda methods to adapt easily to fsuda problems. 2) we introduce
dodiss to characterize a model's generalizability weaknesses in the target
domain. 3) we conduct thorough empirical analyses to demonstrate the
effectiveness and efficiency of samix as a plug-in module for various uda
methods across different tasks.",1
274,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,1.0,Introduction,"longitudinal lesion or tumor tracking is a fundamental task in treatment
monitoring workflows, and for planning of re-treatments in radiation therapy.
based on longitudinal imaging for a given patient it requires establishing which
lesions are corresponding (i.e., same lesion, observed at different timepoints),
which lesions have disappeared and which are new compared to prior scanning.
this information can be leveraged to assess treatment response, e.g., by
analyzing the evolution of size and morphology for a given tumor [1], but also
for adaptation of (re-)treatment radiotherapy plans that take into account new
tumors.in practice, the development of automatic and reliable lesion tracking
solutions is hindered by the complexity of the data (over different modalities),
the absence of large, annotated datasets, and the difficulties associated with
lesion identification (i.e., varying sizes, poses, shapes, and sparsely
distributed locations). in this work, we present a multi-scale self-supervised
learning solution for lesion tracking in longitudinal studies using the
capabilities of contrastive learning [9]. inspired by the pixel-wise contrastive
learning strategy introduced in [5], we choose to learn pixel-wise feature
representations that embed consistent anatomical information from unlabeled
(i.e., without lesion-related annotations) and unpaired (i.e., without the use
of longitudinal scans) data, overcoming barriers to data collection. to increase
the system robustness and emulate the clinician's reading strategies, we propose
to use multi-scale embeddings to enable the system to progressively refine the
fine-grained location. in addition, as imaging offers contextual information
about the human body that is naturally consistent, we design the model to
benefit from biologically-meaningful points (i.e., anatomical landmarks). the
reasoning behind this strategy is that simple data augmentation methods cannot
faithfully model inter-subject variability or possible organ deformations.
hence, we ensure the spatial coherence of the tracked lesion location using
well-defined anatomical landmarks.our proposed method brings two elements of
novelty from a technical point of view: (1) the multi-scale approach for the
anatomical embedding learning and (2) a positive sampling approach that
incorporates anatomically significant landmarks across different subjects. with
these two strategies, the goal is to ensure a high degree of robustness in the
computation of the lesion matching across different lesion sizes and varying
anatomies. furthermore, a significant focus and contribution of our research is
the experimental study at a very large scale: we (1) train a pixel-wise
self-supervised system using a very large and diverse dataset of 52,487 ct
volumes and (2) evaluate on two publicly available datasets. notably, one of the
datasets, nlst, presents challenging cases with 68% of lesions being very small
(i.e., radius < 5 mm).",1
279,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,4.1,Datasets and Setup,"datasets: we train the universal and fine-grained anatomical point matching
model using an in-house ct dataset (variousct). the training dataset contains
52,487 unlabeled 3d ct volumes capturing various anatomies, including chest,
head, abdomen, pelvis, and more.the evaluation is based on two datasets, the
publicly released deep longitudinal study (dls) dataset [8] and the national
lung screening trial (nlst) dataset [12]. the dls dataset is a subset of the
deeplesion [11] medical imaging dataset, containing 3891 pairs of lesions with
information on their location and size. the dataset covers various types of
lesions across different organs. we follow the official data split for dls
dataset and perform evaluation on the testing dataset which comprises 480 lesion
pairs. for nlst, we randomly selected a subset of 1045 test images coming from
420 patients with up to 3 studies. a certified radiologist annotated the testing
data by identifying the location and size of the pulmonary nodules, resulting in
a total of 825 paired annotations. we evaluate lesion tracking in both
directions, from baseline to follow-up and from follow-up to baseline [8]. this
results in a total of 960 and 1650 testing lesion pairs in dls and nlst test
sets, respectively. the isotropic resolution of all ct volumes is adjusted to
2mm through bilinear interpolation.system training: our learning model is
implemented in pytorch and uses the torchio library [13] for medical data
manipulation and augmentation.we employ a u-net-based encoder-decoder
architecture [2] that utilizes an inflated 3d resnet-18 [3,4] as its encoder,
which extends all 2d convolutions table 1. comparison between the proposed
solution and several state-of-the-art approaches (reference results are from
[8]). the exact same test set was used to compute the performance of each
approach listed in the table; however, we retrained only sam. in the standard
resnet to 3d convolutions and allows the use of pre-trained imagenet weights.
the multi-scale embedding model employs s = 5 scales, and the embedding length
is fixed at l = 128 for each scale. convolution with a stride of (2, 2, 2) is
used to reduce the feature map size at the first and fifth levels, while a
stride of (1, 2, 2) is employed for intermediary levels 2 to 4. the u-net
decoder uses a convolution layer with a 3 × 3 × 3 kernel after every up-sampling
layer to generate the final cascade of feature embeddings. the model is trained
with adamw optimizer [6] for 64 epochs using an early stopping strategy with a
patience of 5 epochs, a batch size of 8 augmented 3d paired patches of 32 × 96 ×
96, and a learning rate of 0.0001.",1
284,Geometry-Invariant Abnormality Detection,1.0,Introduction,"the use of machine learning for anomaly detection in medical imaging analysis
has gained a great deal of traction over previous years. most recent approaches
have focused on improvements in performance rather than flexibility, thus
limiting approaches to specific input types -little research has been carried
out to generate models unhindered by variations in data geometries. often,
research assumes certain similarities in data acquisition parameters, from image
dimensions to voxel dimensions and fields-of-view (fov). these restrictions are
then carried forward during inference [5,25]. this strong assumption can often
be complex to maintain in the real-world and although image pre-processing steps
can mitigate some of this complexity, test error often largely increases as new
data variations arise. this can include variances in scanner quality and
resolution, in addition to the fov selected during patient scans. usually
training data, especially when acquired from differing sources, undergoes
significant preprocessing such that data showcases the same fov and has the same
input dimensions, e.g. by registering data to a population atlas. whilst making
the model design simpler, these pre-processing approaches can result in poor
generalisation in addition to adding significant pre-processing times
[11,13,26]. given this, the task of generating an anomaly detection model that
works on inputs with a varying resolution, dimension and fov is a topic of
importance and the main focus of this research.unsupervised methods have become
an increasingly prominent field for automatic anomaly detection by eliminating
the necessity of acquiring accurately labelled data [4,7] therefore relaxing the
stringent data requirements of medical imaging. this approach consists of
training generative models on healthy data, and defining anomalies as deviations
from the defined model of normality during inference. until recently, the
variational autoencoder (vae) and its variants held the state-of-the-art for the
unsupervised approach. however, novel unsupervised anomaly detectors based on
autoregressive transformers coupled with vector-quantized variational
autoencoders (vq-vae) have overcome issues associated with autoencoder-only
methods [21,22]. in [22], the authors explore the advantage of tractably
maximizing the likelihood of the normal data to model the long-range
dependencies of the training data. the work in [21] takes this method a step
further through multiple samplings from the transformer to generate a
non-parametric kernel density estimation (kde) anomaly map.even though these
methods are state-of-the-art, they have stringent data requirements, such as
having a consistent geometry of the input data, e.g., in a whole-body imaging
scenario, it is not possible to crop a region of interest and feed it to the
algorithm, as this cropped region will be wrongly detected as an anomaly. this
would happen even in the case that a scan's original fov was restricted [17].as
such, we propose a geometric-invariant approach to anomaly detection, and apply
it to cancer detection in whole-body pet via an unsupervised anomaly detection
method with minimal spatial labelling. through adapting the vq-vae transformer
approach in [21], we showcase that we can train our model on data with varying
fields of view, orientations and resolutions by adding spatial conditioning in
both the vq-vae and transformer. furthermore, we show that the performance of
our model with spatial conditioning is at least equivalent to, and sometimes
better, than a model trained on whole-body data in all testing scenarios, with
the added flexibility of a ""one model fits all data"" approach. we greatly reduce
the pre-processing requirements for generating a model (as visualised in fig.
1), demonstrating the potential use cases of our model in more flexible
environments with no compromises on performance.",1
293,Geometry-Invariant Abnormality Detection,5.0,Conclusion,"detection and segmentation of anomalous regions, particularly for cancer
patients, is essential for staging, treatment and intervention planning.
generally, the variation scanners and acquisition protocols can cause failures
in models trained on data from single sources. in this study, we proposed a
system for anomaly detection that is robust to variances in geometry. not only
does the proposed model showcase strong and statistically-significant
performance improvements on varying image resolutions and fov, but also on
whole-body data. through this, we demonstrate that one can improve the
adaptability and flexibility to varying data geometries while also improving
performance. such flexibility also increases the pool of potential training
data, as they dont require the same fov. we hope this work serves as a
foundation for further exploration into geometry-invariant deep-learning methods
for medical-imaging.",1
355,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,1.0,Introduction,"globally, cancer is a leading cause of death and the burden of cancer incidence
and mortality is rapidly growing [1]. in cancer diagnosis, treatment, and
management, pathologydriven information plays a pivotal role. cancer grade is,
in particular, one of the major factors that determine the treatment options and
life expectancy. however, the current pathology workflow is sub-optimal and
low-throughput since it is, by and large, manually conducted, and the large
volume of workloads can result in dysfunction or errors in cancer grading, which
have an adversarial effect on patient care and safety [2]. therefore, there is a
high demand to automate and expedite the current pathology workflow and to
improve the overall accuracy and robustness of cancer grading.recently, many
computational tools have shown to be effective in analyzing pathology images
[3]. these are mainly built based upon deep convolutional neural networks
(dcnns). for instance, [4] used dccns for prostate cancer detection and grading,
[5] classified gliomas into three different cancer grades, and [6] utilized an
ensemble of dcnns for breast cancer classification. to further improve the
efficiency and effectiveness of dcnns in pathology image analysis, advanced
methods that are tailored to pathology images have been proposed. for example,
[7] proposed to incorporate both local and global contexts through the
aggregation learning of multiple context blocks for colorectal cancer
classification; [8] extracted and utilized multi-scale patterns for cancer
grading in prostate and colorectal tissues; [9] proposed to re-formulate cancer
classification in pathology images as both categorical and ordinal
classification problems. built based upon a shared feature extractor, a
categorical classification branch, and an ordinal classification branch, it
simultaneously conducts both categorical and ordinal learning for colorectal and
prostate cancer grading; a hybrid method that combines dccns with hand-crafted
features was developed for mitosis detection in breast cancer [10]. moreover,
attention mechanisms have been utilized for an improved pathology image
analysis. for instance, [11] proposed a two-step framework for glioma sub-type
classification in the brain, which consists of a contrastive learning framework
for robust feature extractor training and a sparse-attention block for
meaningful multiple instance feature aggregation. such attention mechanisms have
been usually utilized in a multiple instance learning framework or as
self-attention for feature representations. to the best of our knowledge,
attention mechanisms have not been used for feature representations of class
centroids.in this study, we propose a centroid-aware feature recalibration
network (cafenet) for accurate and robust cancer grading in pathology images.
cafenet is built based upon three major components: 1) a feature extractor, 2) a
centroid update (cup) module, and 3) a centroid-aware feature recalibration
(cafe) module. the feature extractor is utilized to obtain the feature
representation of pathology images. cup module obtains and updates the centroids
of class labels, i.e., cancer grades. cafe module adjusts the input embedding
vectors with respect to the class centroids (i.e., training data distribution).
assuming that the classes are well separated in the feature space, the centroid
embedding vectors can serve as reference points to represent the data
distribution of the training data. this indicates that the centroid embedding
vectors can be used to recalibrate the input embedding vectors of pathology
images. during inference, we fix the centroid embedding vectors so that the
recalibrated embedding vectors do not vary much compared to the input embedding
vectors even though the data distribution substantially changes, leading to
improved stability and robustness of the feature representation. in this manner,
the feature representations of the input pathology images are re-calibrated and
stabilized for a reliable cancer classification. the experimental results
demonstrate that cafenet achieves the state-of-the-art cancer grading
performance in colorectal cancer grading datasets. the source code of cafenet is
available at https://github.com/col in19950703/cafenet.",2
379,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,1.0,Introduction,"liver cancer is one of the most deadly cancers and has the second highest
fatality rate [17]. focal liver lesions (flls) are the most common lesions found
in liver cancer, yet flls are challenging to diagnose because they can be either
benign lesions, such as focal nodular hyperplasia (fnh), hepatic abscess (ha),
hepatic hemangioma (hh), and hepatic cyst (hc) or malignant tumors, such as
intrahepatic cholangiocarcinoma (icc), hepatic metastases (hm), and
hepatocellular carcinoma (hcc). accurate early diagnosis of flls is thus
critical to increasing the 5-year survival rate, a task that remains challenging
as of today. dynamic contrast-enhanced ct is a common technique for liver cancer
diagnosis, where four different phases of imaging, namely, non-contrast (nc),
arterial (art), portal venous (pv), and delayed (dl) provide complementary
information about the liver. different types of flls acquired in the four phases
are shown in fig. 1. with the development of deep learning, computer-aided liver
lesion diagnosis has attracted much attention [5,8,16] in recent years. romero
et al. [16] presented an end-to-end framework based on inception-v3 and
inceptionresnet-v2 to discriminate liver lesions between cysts and malignant
tumors. heker et al. [8] combined liver segmentation and classification using
transfer learning and joint learning to increase the performance of cnn. as a
manner to elevate the accuracy of cnns, frid-adar et al. [5] designed a
gan-based network to generate synthetic liver lesion images, improving the
classification performance based on cnn. it is reported in many studies [9,18]
that using multi-phase data, like most professionals do in practice, can help
the network get a more accurate result, which also acts in liver lesion
classification [15,23,24]. yasaka et al. [24] proposed multi-channel cnn to
extract features from multi-phase liver ct by concatenation. roboh et al. [15]
proposed an algorithm based on cnns to handle 3d context in liver cts and
utilized clinical context to assist the classification. xu et al. [23]
constructed a knowledge-guided framework to integrate liver lesion features from
three phases using self-attention and fused them with a cross-feature
interaction module and a cross-lesion correlation module.a single-phase lesion
annotation means the annotation of both lesion position and its class. in
hospitals, collected multi-phase cts are normally grouped by patients rather
than lesions, which makes single-phase lesion annotation insufficient for
feature fusion learning. however, the number of lesions inside a single patient
can vary from one to dozens and they can be of different types in realistic
cases. multi-phase cts are also not co-registered in most cases, therefore, it
is necessary to make sure the lesions extracted from different phases are
somehow aligned for feature fusion, which is called as multi-phase lesion
annotation. moreover, while most works have attached much importance to liver
lesion segmentation [2], its outcome is usually organized at a single-phase
level. additional effort will be needed when consolidating segmentation and
multi-phase classification.self-attention based transformers [19] have shown
strong capability in natural language processing tasks. meanwhile, vision
transformers (vit) [4] have been shown to replace cnn with a transformer encoder
in computer vision tasks and can achieve obvious advantages on large-scale
datasets. to the best of our knowledge, we find no study using vit backbone
network in liver lesion classification. the reason for this is twofold. first,
pure vit has several limitations itself [6], including ignoring local
information within each patch, extracting only single-scale features, and
lacking inductive bias. second, no complete open liver lesion classification
datasets exist. most relevant studies are based on private datasets, which tend
to be small in size and cause overfitting in learning models.in this paper, we
construct a hybrid framework with vit backbone for liver lesion classification,
transliver. we design a pre-processing unit to reduce the annotation cost, where
we obtain lesion area on multi-phase cts from annotations marked on a single
phase. to alleviate the limitations of pure transformers, we propose a
multi-stage pyramid structure and add convolutional layers to the original
transformer encoder. we use additional cross phase tokens at the last stage to
complete a multi-phase fusion, which can focus on cross-phase communication and
improve the fusion effectiveness as compared with conventional modes. while most
multi-phase liver lesion classification studies use datasets with no more than
three phases (without dl phase for its difficulty of collection) or no more than
six lesion classes, we validate the whole framework on an in-house dataset with
four phases of abdominal ct and seven classes of liver lesions. considering the
disproportion of axial lesion slice number and the relatively small scale of the
dataset, we adopt a 2-d network in classification part instead of 3-d in
pre-processing part and achieve a 90.9% accuracy.",2
381,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.1,Pre-processing Unit,"the single-phase annotated lesion has the position and class labels in all
phases but they are not aligned, so we could have difficulty finding out which
lesions in different phases are the same with 2 or more lesions in one patient.
to reduce errors caused by unregistered data and address the situation that one
patient has multiple lesions of different types, we pre-process the multi-phase
liver cts registered and grouped by lesions.the registration network is based on
voxelmorph [1], with a u-net learning registration field and moving data
transformed by the field. we also use auxiliary dice loss function between fixed
image lesion masks and moved image lesion masks to help the registration field
learning. in [1], the network needs to specify an atlas image, otherwise, pairs
of images will be registered to each other. but in our work, we need to register
the original data in a cross-phase form. we choose an atlas phase art as
suggested by clinicians and other phases of cts are registered to the art phase
of every patient.after registration, a lesion matcher finds the same lesions in
different phases. we generate a minimum circumscribed cuboid with padding as the
lesion window for each lesion to keep the surrounding information. the windows
are then converted to 0-1 masks to calculate dice coefficient. lesions with the
maximal window dice coefficient that is no less than a set threshold are
considered the same. only lesions completely found in all phases will be used in
the following classification network.",2
385,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,3.1,Liver Lesion Classification,"dataset. the employed single-phase annotated dataset is collected from sir run
run shaw hospital (srrsh), affiliated with the zhejiang university school of
medicine, and has received the ethics approval of irb. the collection process
can be found in supplementary materials. the size of each ct slice is decreased
to 224×224 using cubic interpolation. after the pre-processing unit with window
dice threshold of 0.3, we screen 761 lesions from 444 patients with four phases
of cts, seven types of lesions (13.2% of hcc, 5.3% of hm, 11.3% of icc, 22.6% of
hh, 31.1% of hc, 8.7% of fnh, and 7.8% of ha), and totally 4820 slices. to
handle the imbalance of dataset, we randomly select 586 lesions as the training
and validation set with no more than 700 axial slices in each lesion type, and
the rest 175 lesions constitute the test set. lesions from the same patient are
either assigned to the training and validation set or the test set, but not
both.implementations. the training and validation set is randomly divided with a
4:1 ratio. the data is augmented by flip, rotation, crop, shift, and scale. we
initialize the backbone network using pre-trained weights of cmt-s [6]. our
models are implemented by pytorch1.12.1 and timm0.6.13 [22]. then, they are
trained on four nvidia tesla a100 gpus for 200 epochs using cross-entropy loss
function with label smoothing and sgd optimizer with learning rate warmup and
cosine annealing. the batch size is 32 and the learning rate is 1e-3. we
measured performance by precision (pre.), sensitivity (sen.), specificity
(spe.), f1-score (f1), area under the curve (auc), and accuracy (acc.).results.
we first compare the class-wise accuracy of our model against other advanced
methods applying different architectures in multi-phase liver lesion
classification with more than four lesion types [3,11,15,23,24]. transliver gets
the highest overall accuracy of 90.9% classifying the most lesion types of seven
(hcc 90.9%, hm 62.5%, icc 73.7%, hh 91.7%, hc 100.0%, fnh 100.0%, and ha 93.3%).
in the results of our method, hm has a relatively low performance of 62.5%,
mainly due to its low proportion in our dataset. the details can be found in
supplementary materials.because the sources of data are different among the
methods compared above and to the best of our knowledge, no relevant study based
on transformers was found, we further train some sota normal classification
models on our dataset. considering the fairness, all the models below are
initialized with pre-trained weights and adopt 2-d structures using the same
slice-level classification strategy. for completeness, we concatenate the
multi-phase features to execute the fusion. as illustrated in table 1, our
proposed transliver model gets better performance than other models in all
metrics. behind our model, cmt-s achieves the best performance, indicating the
effect of convolutional structures in transformer.",2
447,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.2,Fine-Tuning Datasets,"during the fine-tuning stage, we perform extensive experiments on three datasets
with respect to different regions of the human body. abd-110 is an abdomen
dataset from [25] that contains 110 ct images from patients with various
abdominal tumors and these ct images were taken during the treatment planning
stage. we report the average dsc on 11 abdominal organs (large bowel, duodenum,
spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney,
stomach and gallbladder).thorax-85 is a thorax dataset from [5] that contains 85
thoracic ct images. we report the average dsc on 6 thoracic organs (esophagus,
trachea, spinal cord, left lung, right lung, and heart).han is from [24] and
contains 120 ct images covering the head and neck region. we report the average
dsc on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right
optical nerve, left parotid, right parotid, left submandibular gland, and right
submandibular gland).",2
566,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.2,Knowledge Condensation and Interaction,"knowledge condensation. it is difficult to directly learn cross-modal
dependencies using the features obtained by the encoder because ct and x-ray
data were collected from different patients. this means that the data may not
have a direct correspondence between two modalities, making it challenging to
capture their relationship. as shown in fig. 1(a), we design a knowledge
condensation (kc) module by introducing a momentum-updated prototype learning
strategy to condensate valuable knowledge in each modality from the learned
features. for the x-ray modality, given its prototypes p cxr = {p cxr 1 , p cxr
2 , ..., p cxr k } initialized randomly and the feature sequence f cxr , kc
module first reduces the spatial resolution of f cxr and groups the reduced f
cxr into k prototypes by calculating the distance between each feature point and
prototypes, shown as followswhere c cxr i suggests the feature points closing to
the i-th prototype. σ(•) represents a linear projection to reduce the feature
sequence length to relieve the computational burden. then we introduce a
momentum learning function to update the prototypes with c cxr i , which means
that the updates at each iteration not only depend on the current c cxr i but
also consider the direction and magnitude of the previous updates, defined
aswhere λ is the momentum factor, which controls the influence of the previous
update on the current update. similarly, the prototypes p ct for ct modality can
be calculated and updated with the feature set f ct . the prototypes effectively
integrate the informative features of each modality and can be considered
modality-specific knowledge to improve the subsequent cross-modal interaction
learning. the momentum term allows prototypes to move more smoothly and
consistently towards the optimal position, even in the presence of noise or
other factors that might cause the prototypes to fluctuate. this can result in a
more stable learning process and more accurate prototypes, thus contributing to
condensate the knowledge of each modality better.knowledge-guided interaction.
the knowledge-guided interaction (ki) module is proposed for unpaired
cross-modality learning, which accepts the learned prototypes from one modality
and features from another modality as inputs. as shown in fig. 1(b), the ki
module contains two multi-head attention (mha) blocks. take ct features f ct and
x-ray prototypes p cxr as input example, the first block considers p cxr as the
query and reduced f ct as the key and value of the attention. it embeds the
x-ray prototypes through the calculated affinity map between f ct and p cxr ,
resulting in the adapted prototype p cxr . the first block can be seen as a
warm-up to make the prototype adapt better to the features from another
modality. the second block treats f ct as the query and the concatenation of
reduced f ct and p cxr as the key and value, improving the f ct through the
adapted prototypes. similarly, for the f cxr and p ct as inputs, the ki module
is also used to boost the x-ray representations. inspired by the knowledge
prototypes, ki modules boost the interaction between the two modalities and
allow for the learning of strong representations for covid-19 segmentation and
x-ray classification tasks.",3
568,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.1,Materials,"we used the public covid-19 segmentation benchmark [15] to verify the proposed
uci. it is collected from two public resources [5,8] on chest ct images
available on the cancer imaging archive (tcia) [4]. all ct images were acquired
without intravenous contrast enhancement from patients with positive reverse
transcription polymerase chain reaction (rt-pcr) for sars-cov-2. in total, we
used 199 ct images including 149 training images and 50 test images. we also
used two chest x-ray-based classification datasets including chestx-ray14 [18]
and chestxr [1] to assist the uci training. the chestx-ray14 dataset comprises
112,120 x-ray images showing positive cases from 30,805 patients, encompassing
14 disease image labels pertaining to thoracic and lung ailments. an image may
contain multiple or no labels. the chestxr dataset consists of 21,390 samples,
with each sample classified as healthy, pneumonia, or covid-19.",3
610,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,1.0,Introduction,"computed tomography (ct) and magnetic resonance (mr) are two widely used imaging
techniques in clinical practice. ct imaging uses x-rays to produce detailed,
cross-sectional images of the body, which is particularly useful for imaging
bones and detecting certain types of cancers with fast imaging speed. however,
ct imaging has relatively high radiation doses that can pose a risk of radiation
exposure to patients. low-dose ct techniques have been developed to address this
concern by using lower doses of radiation, but the image quality is degraded
with increased noise, which may compromise diagnostic accuracy [9].mr imaging,
on the other hand, uses a strong magnetic field and radio waves to create
detailed images of the body's internal structures, which can produce
high-contrast images for soft tissues and does not involve ionizing radiation.
this makes mr imaging safer for patients, particularly for those who require
frequent or repeated scans. however, mr imaging typically has a lower resolution
than ct [18], which limits its ability to visualize small structures or
abnormalities.motivated by the aforementioned, there is a pressing need to
improve the quality of low-dose ct images and low-resolution mr images to ensure
that they provide the necessary diagnostic information. numerous algorithms have
been developed for ct and mr image enhancement, with deep learning-based methods
emerging as a prominent trend [5,14], such as using the conditional generative
adversarial network for ct image denoising [32] and convolutional neural network
for mr image super-resolution (sr) [4].these algorithms are capable of improving
image quality, but they have two significant limitations. first, paired images
are required for training, e.g., low-dose and full-dose ct images;
low-resolution and high-resolution mr images). however, acquiring such paired
data is challenging in real clinical scenarios. although it is possible to
simulate low-quality images from high-quality images, the models derived from
such data may have limited generalization ability when applied to real data
[9,14]. second, customized models are required for each task. for example, for
mr super-resolution tasks with different degradation levels (i.e., 4x and 8x
downsampling), one may need to train a customized model for each degradation
level and the trained model cannot generalize to other degradation levels.
addressing these limitations is crucial for widespread adoption in clinical
practice.recently, pre-trained diffusion models [8,11,21] have shown great
promise in the context of unsupervised natural image reconstruction [6,7,12,28].
however, their applicability to medical images has not been fully explored due
to the absence of publicly available pre-trained diffusion models tailored for
the medical imaging community. the training of diffusion models requires a
significant amount of computational resources and training images. for example,
openai's improved diffusion models [21] took 1600-16000 a100 hours to be trained
on the imagenet dataset with one million images, which is prohibitively
expensive. several studies have used diffusion models for low-dose ct denoising
[30] and mr image reconstruction [22,31], but they still rely on paired
images.in this paper, we aim at addressing the limitations of existing image
enhancement methods and the scarcity of pre-trained diffusion models for medical
images. specifically, we provide two well-trained diffusion models on full-dose
ct images and high-resolution heart mr images, suitable for a range of
applications including image generation, denoising, and super-resolution.
motivated by the existing plug-and-play image restoration methods [26,34,35] and
denoising diffusion restoration and null-space models (ddnm) [12,28], we further
introduce a paradigm for plug-and-play ct and mr image denoising and
super-resolution as shown in fig. 1. notably, it eliminates the need for paired
data, enabling greater scalability and wider applicability than existing
paired-image dependent methods. moreover, it eliminates the need to train a
customized model for each task. our method does not need additional training on
specific tasks and can directly use the single pre-trained diffusion model on
multiple medical image enhancement tasks. the pre-trained diffusion models and
pytorch code of the present method are publicly available at
https://github.com/bowang-lab/ dpm-medimgenhance.",3
644,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.2,Model Backbone and Datasets,"we use the deepedit [11] model with a u-net backbone [15] and simulate a fixed
number of clicks n during training and evaluation. for each volume, n clicks are
iteratively sampled from over-and undersegmented predictions of the model as in
[16] and represented as foreground and background guidance signals. we
implemented our experiments with monai label [23] and will release our code.we
trained and evaluated all of our models on the openly available autopet [1] and
msd spleen [2] datasets. msd spleen [2] contains 41 ct volumes with voxel size
0.79×0.79×5.00mm 3 and average resolution of 512×512×89 voxels with dense
annotations of the spleen. autopet [1] consists of 1014 pet/ct volumes with
annotated tumor lesions of melanoma, lung cancer, or lymphoma. we discard the
513 tumor-free patients, leaving us with 501 volumes. we also only use pet data
for our experiments. the pet volumes have a voxel size of 2.0 × 2.0 × 2.0mm 3
and an average resolution of 400 × 400 × 352 voxels.",3
776,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.1,Datasets,"qubiq is a recent challenge held at miccai 2020 and 2021, specifically designed
to evaluate the inter-rater variability in medical imaging. following [23,41],
we use qubiq 2020, which contains 7 segmentation tasks in 4 different ct and mr
datasets: prostate (55 cases, 2 tasks, 6 raters), brain growth (39 cases, 1
task, 7 raters), brain tumor (32 cases, 3 tasks, 3 raters), and kidney (24
cases, 1 task, 3 raters). for each dataset, we calculate the average dice score
between each rater and the majority votes in table 1. in some datasets, such as
brain tumor t2, the inter-rater disagreement can be quite substantial. in line
with [23], we resize all images to 256 × 256. lits contains 201 high-quality ct
scans of liver tumors. out of these, 131 cases are designated for training and
70 for testing. as the ground-truth labels for the test set are not publicly
accessible, we only use the training set. following [36], all images are resized
to 512×512 and the hu values of ct images are windowed to the range of [-60,
140]. kits includes 210 annotated ct scans of kidney tumors from different
patients. in accordance with [36], all images are resized to 512 × 512 and the
hu values of ct images are windowed to the range of [-200, 300].",3
870,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,3.0,Experiments,"dataset and implementation. we evaluate the our uml network on two datasets
refuge [14] and ispy-1 [13]. refuge contains two tasks, classification of
glaucoma and segmentation of optic disc/cup in fundus images. the overall 1200
images were equally divided for training, validation, and testing. all images
are uniformly adjusted to 256 × 256 px. the tasks of ispy-1 are the pcr
prediction and the breast tumor segmentation. a total of 157 patients who suffer
the breast cancer are considered -43 achieve pcr and 114 non-pcr.for each case,
we cut out the slices in the 3d image and totally got 1,570 2d images, which are
randomly divided into the train, validation, and test datasets with 1,230, 170,
and 170 slices, respectively. we implement the proposed method via pytorch and
train it on nvidia geforce rtx 2080ti. the adam optimizer is adopted to update
the overall parameters with an initial learning rate 0.0001 for 100 epochs. the
scale of the regularizer is set as 1 × 10 -5 . we choose vgg-16 and res2net as
the encoders for classification and segmentation, separately.compared methods
and metrics. we compared our method with singletask methods and multi-task
methods. (1) single-task methods: (a) ec [17], (b) tbrats [31] and (c) transunet
[2]. evidential deep learning for classification (ec) first proposed to
parameterize classification probabilities as dirichlet distributions to explain
evidence. tbrats then extended ec to medical image segmentation. meriting both
transformers and u-net, transunet is a strong model for medical image
segmentation. (2) multi-task methods: (d) bcs [25] and (e) dsi [28]. the
baseline of the joint classification and segmentation framework (bcs) is a
simple but useful way to share model parameters, which utilize two different
encoders and decoders for learning respectively. the deep synergistic
interaction network (dsi) has demonstrated superior performance in joint task.
we adopt overall accuracy (acc) and f1 score (f1) as the evaluation criteria for
the classification task. dice score (di) and average symmetric surface distance
(assd) are chosen for the segmentation task. comparison under noisy data. to
further valid the reliability of our model, we introduce gaussian noise with
various levels of standard deviations (σ) to the input medical images. the
comparison results are shown in table 2. as can be observed that, the accuracy
of classification and segmentation significantly decreases after adding noise to
the raw data. however, benefiting from the uncertainty-informed guiding, our uml
consistently deliver impressive results. in fig. 3, we show the output of our
model under the noise. it is obvious that both the image-level uncertainty and
the pixel-wise uncertainty respond reasonably well to noise. these experimental
results can verify the reliability and interpre of the uncertainty guided
interaction between the classification and segmentation in the proposed uml. the
results of more qualitative comparisons can be found in the supplementary
material.ablation study. as illustrated in table 3, both of the proposed un and
ui play important roles in trusted mutual learning. the baseline method is
bcs.md represents the mutual feature decoder. it is clear that the performance
of classification and segmentation is significantly improved when we introduce
supervision of mutual features. as we thought, the introduction of un and ui
takes the reliability of the model to a higher level.",4
946,Certification of Deep Learning Models for Medical Image Segmentation,1.0,Introduction,"for the past decade, deep neural networks have dominated the computer vision
community and provided near human performance on many different tasks, including
classification [18], segmentation [24], and image generation [16]. given these
impressive results, convolutional neural networks are now used on a daily basis
in fields like healthcare, self-driving cars, and robotics, to cite a few. in
medical imaging, convolutional neural networks are particularly used to segment
organs or regions of interest on different modalities such as x-rays, ct scans,
mris, or ultrasound [36]. indeed, segmentation techniques and variations of 2d
and 3d u-nets are currently the state-of-the-art to identify and isolate tumors,
blood vessels, organs, or other structures within an image and provide crucial
help to physicians for medical diagnosis, screening, and prognosis
[32].nowadays, segmentation models are gaining widespread adoption in modern
clinical practice and are being used with increasing frequency, making the
results of these models critical for many patients. however, it is now commonly
known that neural networks can be vulnerable to adversarial attacks [17,34],
i.e., small input perturbations invisible to humans crafted specifically such
that the network performs errors. over the past few years, a large body of work
has devised empirical defenses against adversarial attacks for classification
tasks [3,17,25], as well as segmentation tasks [37], including applications on
medical imaging [27]. although state-of-the-art empirical defenses provide
significant robustness, these defenses do not guarantee theoretical robustness
and stronger attacks can be crafted to break them [5]. recently, certified
defenses, for classification [2,11,26] and segmentation [15,23], have been
proposed to guarantee the accuracy and reliability of neural networks. however,
certified defenses for segmentation in the context of medical imaging are still
lacking, even if models are getting market approvals (e.g., fda, ce) and are
already adopted in clinical practice.in this paper, we provide the first method
for certified robustness in the context of segmentation for medical imaging. we
leverage the randomized smoothing strategy [11,15], and the recent work on
diffusion models [7] to achieve stateof-the-art certified robustness for
segmentation models. randomized smoothing consists in convolving the neural
network with a gaussian distribution (i.e., by adding noise to the input) in
order to obtain a smooth segmentation model. from the smoothness properties of
the segmentation model, we can derive a robustness guarantee and compute a
certified dice score. we go even further by using diffusion models to first
denoise the perturbed input and boost the certified robustness. by extension, we
show that current diffusion models, trained on 'classical images' generalize
well to medical datasets for denoising tasks. extensive experiments on five
public medical datasets of chest x-rays [21,31], skin lesions [10], and
colonoscopies [6], and different popular segmentation models, prove the
potential of our method. we hope that this study will provide the first step
towards robustness guarantees for medical image segmentation.",4
1063,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,1.0,Introduction,"dynamic contrast-enhanced magnetic resonance imaging (dce-mri) revealing tumor
hemodynamics information is often applied to early diagnosis and treatment of
breast cancer [1]. in particular, automatically and accurately segmenting tumor
regions in dce-mri is vital for computer-aided diagnosis (cad) and various
clinical tasks such as surgical planning. for the sake of promoting segmentation
performance, recent methods utilize the dynamic mr sequence and exploit its
temporal correlations to acquire powerful representations [2][3][4]. more
recently, a handful of approaches take advantage of hemodynamic knowledge and
time intensity curve (tic) to improve segmentation accuracy [5,6]. however, the
aforementioned methods require the complete dce-mri sequences and overlook the
difficulty in assessing complete temporal sequences and the missing time point
problem, especially post-contrast phase, due to the privacy protection and
patient conditions. hence, these breast cancer segmentation models cannot be
deployed directly in clinical practice.recently, denoising diffusion
probabilistic model (ddpm) [7,8] has produced a tremendous impact on image
generation field due to its impressive performance. diffusion model is composed
of a forward diffusion process that add noise to images, along with a reverse
generation process that generates realistic images from the noisy input [8].
based on this, several methods investigate the potential of ddpm for natural
image segmentation [9] and medical image segmentation [10][11][12].
specifically, baranchuk et al. [9] explores the intermediate activations from
the networks that perform the markov step of the reverse diffusion process and
find these activations can capture semantic information for segmentation.
however, the applicability of ddpm to medical image segmentation are still
limited. in addition, existing ddpm-based segmentation networks are generic and
are not optimized for specific applications. in particular, a core question for
dce-mri segmentation is how to optimally exploit hemodynamic priors.based on the
above observations, we innovatively consider the underlying relation between
hemodynamic response function (hrf) and denoising diffusion process (ddp). as
shown in fig. 1, during hrf process, only tumor lesions are enhanced and other
non-tumor regions remain unchanged. by designing a network architecture to
effectively transmute pre-contrast images into post-contrast images, the network
should acquire hemodynamic inherent in hrf that can be used to improve
segmentation performance. inspired by the fact that ddpm generates images from
noisy input provided by the parameterized gaussian process, this work aims to
exploit implicit hemodynamic information by a diffusion process that predict
post-contrast images from noisy pre-contrast images. specifically, given the
pre-contrast and post-contrast images, the latent kinetic code is learned using
a score function of ddpm, which contains sufficient hemodynamic characteristics
to facilitate segmentation performance.once the diffusion module is pretrained,
the latent kinetic code can be easily generated with only pre-contrast images,
which is fed into a segmentation module to annotate cancers. to verify the
effectiveness of the latent kinetic code, the sm adopts a simple u-net-like
structure, with an encoder to simultaneously conduct semantic feature encoding
and kinetic code fusion, along with a decoder to obtain voxel-level
classification. in this manner, our latent kinetic code can be interpreted to
provide tic information and hemodynamic characteristics for accurate cancer
segmentation.we verify the effectiveness of our proposed diffusion kinetic model
(dkm) on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot
dataset [13]. compared to the existing state-of-the-art approaches with complete
sequences, our method yields higher segmentation performance even with
precontrast images. in summary, the main contributions of this work are listed
as follows:• we propose a diffusion kinetic model that implicitly exploits
hemodynamic priors in dce-mri and effectively generates high-quality
segmentation maps only requiring pre-contrast images. • we first consider the
underlying relation between hemodynamic response function and denoising
diffusion process and provide a ddpm-based solution to capture a latent kinetic
code for hemodynamic knowledge. • compared to the existing approaches with
complete sequences, the proposed method yields higher cancer segmentation
performance even with pre-contrast images.",4
1068,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,3.0,Experiments,"dataset: to demonstrate the effectiveness of our proposed dkm, we evaluate our
method on 4d dce-mri breast cancer segmentation using the breast-mri-nact-pilot
dataset [13], which contains a total of 64 patients with the contrastenhanced
mri protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time
points (as shown in fig. 3). each mr volume consists of 60 slices and the size
of each slice is 256 × 256. regarding preprocessing, we conduct zeromean
unit-variance intensity normalization for the whole volume. we divided the
original dataset into training (70%) and test set (30%) based on the scans.
ground truth segmentations of the data are provided in the dataset for tumor
annotation. no data augmentation techniques are used to ensure fairness.",4
1115,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,1.0,Introduction,"the periodic acquisition and analysis of volumetric ct and mri scans of oncology
patients is essential for the evaluation of the disease status, the selection of
the treatment, and the response to treatment. currently, scans are acquired
every 2-12 months according to the patient's characteristics, disease stage, and
treatment regime. the scan interpretation consists of identifying lesions
(primary tumors, metastases) in the affected organs and characterizing their
changes over time. lesion changes include changes in the size of existing
lesions, the appearance of new lesions, the disappearance of existing lesions,
and complex lesion changes, e.g., the formation of conglomerate lesions. as
treatments improve and patients live longer, the number of scans in longitudinal
studies increases and their interpretation is more challenging and
time-consuming.radiological follow-up requires the quantitative analysis of
lesions and patterns of lesion changes in subsequent scans. it differs from
diagnostic reading since the goal is to find and quantify the differences
between the scans, rather than to find abnormalities in a single scan. in
current practice, quantification of lesion changes is partial and approximate.
the recist 1.1 guidelines call for finding new lesions (if any), identifying up
to the five largest lesions in each scan in the ct slice where they appear
largest, manually measuring their diameters, and comparing their difference [1].
while volumetric measures of individual lesions and of all lesions (tumor
burden) have long been established as more accurate and reliable than partial
linear measurements, they are not used clinically because they require manual
lesion delineation and lesion matching in unregistered scans, which is usually
time-consuming and subject to variability [2].in a previous paper, we presented
an automatic pipeline for the detection and quantification of lesion changes in
pairs of ct liver scans [3]. this paper describes a graph-based lesion tracking
method for the comprehensive analysis of lesion changes and their patterns at
the lesion level. the tasks are formalized as graph-theoretic problems (fig. 1).
complex lesion changes include merged lesions, which occurs when at least two
lesions grow and merge into one (possible disease progression), split lesions,
which occurs when a lesion shrinks and cleaves into several parts (possible
response to treatment) and conglomeration of lesions, which occurs when clusters
of lesions coalesce. while some of these lesion changes have been observed [4],
they have been poorly studied. comprehensive quantitative analysis of lesion
changes and patterns is of clinical importance, since response to treatment may
vary among lesions, so the analysis of a few lesions may not be
representative.the novelties of this paper are: 1) identification and
formalization of longitudinal lesion matching and patterns of lesion changes in
ct in a graph-theoretic framework; 2) new classification and detection of
changes of individual lesions and lesion patterns based on the properties of the
lesion changes graph and its connected components; 3) a simultaneous lesion
matching method with more than two scans; 4) graph-based methods for the
detection of changes in individual lesions and patterns of lesion changes.
experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18
patients) datasets show that our method yields high classification accuracy.to
the best of our knowledge, ours is the first method to perform longitudinal
lesion matching and lesion changes pattern detection. only a few papers address
lesion matching in pairs of ct/mri scans [5][6][7][8][9][10][11][12][13] -none
performs simultaneous matching of all lesions in more than two scans. also, very
few methods [3,14] handle matching of split/merged lesions. although many
methods exist for object tracking in optical images and videos [15][16][17],
they are unsuited for analyzing lesion changes since they assume many
consecutive 2d images where objects have very similar appearance and undergo
small changes between images. overlap-based methods pair two lesions in
registered scans when their segmentations overlap, with a reported accuracy of
66-98% [3,[5][6][7][8][9][10][11]18]. these methods assume that organs and
lesions undergo minor changes, are very sensitive to registration errors, and
cannot handle complex lesion changes. similarity-based methods pair two lesions
with similar features, e.g., intensity, shape, location [13][14][15][16] with an
84-96% accuracy on the deeplesion dataset [14]. they are susceptible to major
changes in the lesion appearance and do not handle complex lesion changes.
split-andmerge matching methods are used for cell tracking in fluorescence
microscopy [19]. they are limited to 2d images, assume registration between
images, and do not handle conglomerate changes.",5
1117,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.1,Problem Formalization,"let s = s 1 , . . . , s n be a series of n ≥ 2 consecutive patient scans
acquired at timesis a set of vertices v i j corresponding to the lesions
associated with the lesion segmentation masks l i = l i 1 , l i 2 , . . . , l i
n i , where n i ≥ 0 is the number of lesions in scan s i at time t i . by
definition, any two lesionsj,l indicates that the lesions corresponding to
vertices v i j , v k l are the same lesion, i.e., that the lesion appears in
scans s i , s k in the same location. edges of consecutive scans s i , s i+1 are
called consecutive edges; edges of non-consecutive scans, s i , s k , i < k -1,
are called non-consecutive edges. the in-and out-degree of a vertex v i j , d in
(v i j ) and d out (v i j ), are the number of incoming and outcoming edges,
respectively.let cc = {cc m } m m=1 be the set of connected components of the
undirected graph version of g, where m is the number of connected components
andby definition, for each 1 ≤ m ≤ m , the sets v m , e m are mutually disjoint
and their unions are v , e, respectively. in a connected component cc m , there
is an undirected path between any two vertices v i j , v k l consisting of a
sequence of undirected edges in e m . . in this setup, connected components
correspond to matched lesions and their pattern of evolution over time (fig.
1d).we define seven mutually exclusive individual lesion change labels for
lesion v i j in scan s i based on the vertex in-and out-degrees (fig. 2). in the
following definitions we refer to the indices: 1 ≤ k < i < l ≤ n ; 1) lone: a
lesion present in scan s i and absent in all previous scans s k and subsequent
scans s l ; 2) new: a lesion present in scan s i and absent in all previous
scans s k ; 3) disappeared: a lesion present in scan s i and absent in all
subsequent scans s l ; 4) unique: a lesion present in scan s i and present as a
single lesion in a previous scan s k and/or in a subsequent scan s l ; 5)
merged: a lesion present in scan s i and present as two or more lesions in a
previous scan s k ; 6) split: a lesion present in scan s i and present as two or
more lesions in a subsequent scan s l ; 7) complex: a lesion present as two or
more lesions in at least one previous scan s k and at least one subsequent scan
s l . we also define as existing a lesion present in scan s i and present in at
least one previous scan s k and one subsequent scan s l , (d in (v i j ) ≥ 1, d
out (v i j ) ≥ 1). for the first and current scans s 1 and s n , we set d in (v
1 j ) = 1, d out (v n j ) = 1, i.e., the lesion existed before the first scan or
remains after the last scan. thus, lesions in the first (last) scan can only be
unique, disappeared or split (unique, new or merged). finally, when lesion v i j
is merged and d out (v i j ) = 0, i < n , it is also labeled disappeared; when
it is split and d in (v i j ) = 0, i > 1, it is also labeled new. we define five
patterns of lesion changes based on the properties of the connected components
cc m of g and on the labels of lesion changes: 1) single_p: a connected
component cc m = v i j consisting of a single lesion labeled as lone, new,
disappeared; 2) linear_p: a connected component consisting of a single earliest
vertex v the changes in individual lesions and the detection and classification
of patterns of lesion changes consist of constructing a graph whose vertices are
the corresponding lesion in the scans, computing the graph consecutive and
non-consecutive edges that correspond to lesion matchings, computing the
connected components of the resulting graph, and assigning an individual lesion
change label to each vertex and a lesion change pattern label to each connected
component according to the categories above.",5
1119,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,3.0,Experimental Results,"we evaluated our method with two studies on retrospectively collected patient
datasets that were manually annotated by an expert radiologist.dataset: lung and
liver ct studies were retrospectively obtained from two medical centers
(hadassah univ hosp jerusalem israel) during the routine clinical examination of
patients with metastatic disease. each patient study consists of at least 3
scans.dlung consists of 83 chest ct scans from 19 patients with a mean 4.4 ± 2.0
scans/patient, a mean time interval between consecutive scans of 125.9 ± 81.3
days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 1.0-3.0 mm 3 . dliver consists of
77 abdominal cect scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a
mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel
sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 .lesions in both datasets were
annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver
lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with
<20 voxels were excluded). ground-truth lesion matching graphs and lesion
changes labeling were produced by running the method on the datasets and then
having the radiologist review and correct the resulting node labels and
edges.study 1: lesion changes labeling, lesion matching, evaluation of patterns
of lesion changes. we ran our method on the dlungs and dliver lesion
segmentations. the settings of the parameters were: dilation distance d = 1 mm,
overlap percentage p = 10%, number of iterations r = 5 and 7, and centroid
maximum distance δ = 17 and 23 mm for the lungs and liver lesions,
respectively.we compared the computed and ground-truth lesion changes graphs
with two metrics: 1) lesion changes classification accuracy, which is the % of
correct computed labels from the ground truth labels; 2) lesion matching
precision and recall based on the presence/absence of computed vs. ground truth
edges. the precision and recall definitions were adapted so that wrong or missed
non-consecutive edges are counted as true positive when there is a path between
their vertices in either the ground-truth or the computed graph. table 1
summarizes the results. the distribution of lesion changes labels for dlungs
(1,178 lesions) is unique 785 (67%), new 215 (18%), lone 109 (9%), disappeared
51 (4%), merged 12 (1%), split 6 (1%), complex 0 (0%) with class accuracy ≥ 96%
for all except split (66%). for dliver (800 lesions) it is unique 450 (56%), new
185 (23%), lone 45 (6%), disappeared 77 (10%), merged 27 (3%), split 18 (2%),
complex 1 (0.05%) with class accuracy ≥ 81% for all except disappeared (71%) and
split (67%).for the patterns of lesion changes, we compared the computed and
ground truth patterns of lesion changes. the accuracy is the % of identical
connected components in each category. table 1 summarizes the results. note that
the split_p, merged_p and complex_p patterns jointly account for 3% and 8% of
the cases. these patterns are hard to detect manually but their correct
classification and tracking are crucial for the proper application of the recist
1.1 follow-up protocol [1]. study 2: detection of missed lesions in the ground
truth. the expert radiologist was asked to examine non-consecutive edges and
lesions labeled as lone in the lesion changes graph and determine if lesions
were unseen or undetected (actual or presumed false negative) in the skipped or
contiguous scans (fig. 1d). for each non-consecutive edge connecting lesions v i
j , v k l , he analyzed the corresponding region in the skipped scans s j at t j
∈ ]t i , t k [ for possible missed lesions. for the dlungs dataset, 25 visible
and 5 faintly visible or surmised to be present unmarked lesions were found for
27 nonconsecutive edges. for the dliver dataset, 20 visible and 21 faintly
visible or surmised to be present unmarked lesions were found for 25
non-consecutive edges.after reviewing the 42 and 37 lesions labeled as lone in
dlungs and dliver with > 5mm diameter, the radiologist determined that 1 and 8
of them had been wrongly identified as a cancerous lesion. moreover, he found
that 14 and 16 lesions initially labeled as lone, had been wrongly classified:
for these lesions he found 15 and 21 previously unmarked matching lesions in the
next or previous scans. in total, 45 and 62 missing lesions were added to the
ground truth dlungs and dliver datasets, respectively. these hard-to-find
ground-truth false negatives (3.7%, 7.2% of all lesions) may change the
radiological interpretation and the disease status. see the supplemental
material for examples of these scenarios.",5
1120,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,4.0,Conclusion,"the use of graph-based methods for lesion tracking and detection of patterns of
lesion changes was shown to achieve high accuracy in classifying changes in
individual lesion and identifying patterns of lesion changes in liver and lung
longitudinal ct studies of patients with metastatic disease. this approach has
proven to be useful in detecting missed, faint, and surmised to be present
lesions, otherwise hardly detectable by examining the scans separately or in
pairs, leveraging the added information provided by evaluating all patient's
scans simultaneously using the labels from the lesion changes graph and
non-consecutive edges.",5
1122,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,1.0,Introduction,"neuropsychiatric systemic lupus erythematosus (npsle) refers to a complex
autoimmune disease that damages the brain nervous system of patients. the
clinical symptoms of npsle include cognitive disorder, epilepsy, mental illness,
etc., and patients with npsle have a nine-fold increased mortality compared to
the general population [11]. since the pathogenesis and mature treatment of
npsle have not yet been found, it is extremely important to detect npsle at its
early stage and put better clinical interventions and treatments to prevent its
progression. however, the high overlap of clinical symptoms with other
psychiatric disorders and the absence of early non-invasive biomarkers make
accurate diagnosis difficult and time-consuming [3].although conventional
magnetic resonance imaging (mri) tools are widely used to detect brain injuries
and neuronal lesions, around 50% of patients with npsle present no brain
abnormalities in structural mri [17]. in fact, metabolic changes in many brain
diseases precede pathomorphological changes, which indicates proton magnetic
resonance spectroscopy ( 1 h-mrs) to be a more effective way to reflect the
early appearance of npsle. 1 h-mrs is a non-invasive neuroimaging technology
that can quantitatively analyze the concentration of metabolites and detect
abnormal metabolism of the nervous system to reveal brain lesions. however, the
complex noise caused by overlapping metabolite peaks, incomplete information on
background components, and low signal-tonoise ratio (snr) disturb the analysis
results of this spectroscopic method [15]. meanwhile, the individual differences
in metabolism and the interaction between metabolites under low sample size make
it difficult for traditional learning methods to distinguish npsle. figure 1
shows spectra images of four participants including healthy controls (hc) and
patients with npsle. it can be seen that the visual differences between patients
with npsle and hcs in the spectra of the volumes are subtle. therefore, it is
crucial to develop effective learning algorithms to discover metabolic
biomarkers and accurately diagnose npsle. the machine learning application for
biomarker analysis and early diagnosis of npsle is at a nascent stage [4]. most
studies focus on the analysis of mr images using statistical or machine learning
algorithms, such as mann-whitney u test [8], support vector machine (svm)
[7,24], ensemble model [16,22], etc. generally, machine learning algorithms
based on the minimum mean square error (mmse) criterion heavily rely on the
assumption that noise is of gaussian distribution. however, measurement-induced
non-gaussian noise in 1 h-mrs data undoubtedly limits the performance of
mmse-based machine learning methods.on the other hand, for the discovery task of
potential biomarkers, sparse codingbased methods (e.g., 2,1 norm, 2,0 norm,
etc.) force row elements to zero that remove some valuable features [12,21].
more importantly, different brain regions have different functions and
metabolite concentrations, which implies that the metabolic features for each
brain region have different sparsity levels. therefore, applying the same
sparsity constraint to the metabolic features of all brain regions may not
contribute to the improvement of the diagnostic performance of npsle.in light of
this, we propose a robust exclusive adaptive sparse feature selection (reasfs)
algorithm to jointly address the aforementioned problems in biomarker discovery
and early diagnosis of npsle. specifically, we first extend our feature learning
through generalized correntropic loss to handle data with complex non-gaussian
noise and outliers. we also present the mathematical analysis of the adaptive
weighting mechanism of generalized correntropy. then, we propose a novel
regularization called generalized correntropy-induced exclusive 2,1 to
adaptively accommodate various sparsity levels and preserve informative
features. the experimental results on a benchmark npsle dataset demonstrate the
proposed method outperforms comparing methods in terms of early noninvasive
biomarker discovery and early diagnosis.",5
1123,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.0,Method,"dataset and preprocessing: the t2-weighted mr images of 39 participants
including 23 patients with npsle and 16 hcs were gathered from our affiliated
hospital. all images were acquired at an average age of 30.6 years on a signa
3.0t scanner with an eight-channel standard head coil. then, the mr images were
transformed into spectroscopy by multi-voxel 1 h-mrs based on a point-resolved
spectral sequence (press) with a two-dimensional multi-voxel technique. the
collected spectroscopy data were preprocessed by a sage software package to
correct the phase and frequency. an lcmodel software was used to fit the
spectra, correct the baseline, relaxation, and partial-volume effects, and
quantify the concentration of metabolites. finally, we used the absolute naa
concentration in single-voxel mrs as the standard to gain the absolute
concentration of metabolites, and the naa concentration of the corresponding
voxel of multi-voxel 1 h-mrs was collected consistently. the spectra would be
accepted if the snr is greater than or equal to 10 and the metabolite
concentration with standard deviations (sd) is less than or equal to 20%. the
absolute metabolic concentrations, the corresponding ratio, and the linear
combination of the spectra were extracted from different brain regions: rpcg,
lpcg, rdt, ldt, rln, lln, ri, rpwm, and lpwm. a total of 117 metabolic features
were extracted, and each brain region contained 13 metabolic features: cr,
phosphocreatine (pcr), cr+pcr, naa, naag, naa+naag, naa+naag/cr+pcr, mi,
mi/cr+pcr, cho+phosphocholine (pch), cho+pch/cr+pcr, glu+gln, and
glu+gln/cr+pcr.",5
1127,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,3.0,Experimental Results and Conclusion,"experimental settings: the parameters α and λ 1 are are set to 1, while β and λ
2 are searched form {0.5, 1, 2, 5} and {0.1, 0.5, 1}, respectively. we use adam
optimizer and the learning rate is 0.001. to evaluate the performance of
classification, we employ a support vector machine as the basic classifier,
where the kernel is set as the radial basis function (rbf) and parameter c is
set to 1. we average the 3-fold cross-validation results.results and discussion:
we compare the classification accuracy of the proposed reasfs with several sota
baselines, including two filter methods: maximal information coefficient (mic)
[5], gini [23], and four sparse coding-based methods: multi-task feature
learning via 2,1 norm [6,12], discriminative feature selection via 2,0 norm
[21], feature selection via 1,2 norm [10] and exclusive 2,1 [9]. the proposed
reasfs is expected to have better robustness and flexibility. it can be seen
from fig. 2 that the sparse coding-based methods achieve better performance than
filter methods under most conditions, where ""0%"" represents no noise
contamination. the highest accuracy of our reasfs demonstrates the effectiveness
and flexibility of the proposed gcie 2,1 . generally speaking, the probability
of samples being contaminated by random noise is equal. therefore, we randomly
select features from the training set and replace the selected features with
pulse noise. the number of noisy attributes is denoted by the ratio between the
numbers of selected features and total features, such as 15% and 30%. the
classification performances of the npsle dataset contaminated by attribute noise
are shown in fig. 3(a) and fig. 3(b), where one clearly perceives that our
reasfs achieves the highest accuracy under all conditions. besides, it is
unreasonable to apply the same level of sparse regularization to noise features
and uncontaminated features, and our gcie 2,1 can adaptively increase the sparse
level of noise features to remove redundant information, and vice versa. for
label noise, we randomly select samples from the training set and replace
classification labels of the selected samples with opposite values, i.e., 0 → 1
and 1 → 0. the results are shown in fig. 3(c) and fig. 3(d), where the proposed
reasfs is superior to other baselines. it can be seen from fig. 3 that our
reasfs achieves the highest accuracy in different noisy environments, which
demonstrates the robustness of generalized correntropic loss. for non-invasive
biomarkers, our method shows that some metabolic features contribute greatly to
the early diagnosis of npsle, i.e., naag, mi/cr+pcr, and glu+gln/cr+pcr in rpcg;
cr+pcr, naa+naag, naa+naag/cr+pcr, mi/cr+pcr and glu+gln in lpcg; naa, naag, and
cho+pch in ldt; pcr, cr+pcr, cho+pch, cho+pch/cr+pcr and glu+gln/cr+pcr in rln;
mi/cr+pcr, cho+pch and cho+pch/cr+pcr in lln; naa+naag/cr+pcr and cho+pch in ri;
cho+pch/cr+pcr and glu+gln/cr+pcr in rpwm; and pcr, naag and naa+naag/cr+pcr in
lpwm. moreover, we use isometric feature mapping (isomap) [19] to analyze these
metabolic features and find that this feature subset is essentially a
low-dimensional manifold. meanwhile, by combining the proposed reasfs and
isomap, we can achieve 99% accuracy in the early diagnosis of npsle. in
metabolite analysis, some studies have shown that the decrease in naa
concentration is related to chronic inflammation, damage, and tumors in the
brain [18]. in the normal white matter area, different degrees of npsle disease
is accompanied by different degrees of naa decline, but structural mri is not
abnormal, suggesting that naa may indicate the progress of npsle. we also found
that glu+gln/cr+pcr in ri decreased, which indicates that the excitatory
neurotransmitter glu in the brain of patients with npsle may have lower
activity. to sum up, the proposed method provides a shortcut for revealing the
pathological mechanism of npsle and early detection.",5
1158,Automatic Bleeding Risk Rating System of Gastric Varices,1.0,Introduction,"esophagogastric varices are one of the common manifestations in patients with
liver cirrhosis and portal hypertension and occur in about 50 percent of
patients with liver cirrhosis [3,6]. the occurrence of esophagogastric variceal
bleeding is the most serious adverse event in patients with cirrhosis, with a
6-week acute bleeding mortality rate as high as 15%-20% percent [14]. it is
crucial to identify high-risk patients and offer prophylactic treatment at the
appropriate time. regular endoscopy examinations have been proven an effective
clinical approach to promptly detect esophagogastric varices with a high risk of
bleeding [7]. different from the grading of esophageal varices (ev) that is
relatively complete [1], the bleeding risk grading of gastric varices (gv)
involves complex variables including the diameter, shapes, colors, and
locations. several rating systems have been proposed to describe gv based on the
anatomical area. sarin et al. [16] described and divided gv into 2 groups
according to their locations and extensions. hashizume et al. [10] published a
more detailed examination describing the form, location, and color. although the
existing rating systems tried to identify the risk from different perspectives,
they still lack clear quantification standard and heavily rely on the
endoscopists' subjective judgment. this may cause inconsistency or even
misdiagnosis due to the variant experience of endoscopists in different
hospitals. therefore, we aim to build an automatic gv bleeding risk rating
method that can learn a stable and robust standard from multiple experienced
endoscopists.recent works have proven the effectiveness and superiority of deep
learning (dl) technologies in handling esophagogastroduodenoscopy (egd) tasks,
such as the detection of gastric cancer and neoplasia [4]. it is even
demonstrated that ai can detect neoplasia in barrett's esophagus at a higher
accuracy than endoscopists [8]. intuitively we may regard the gv bleeding risk
rating as an image classification task and apply typical classification
architectures (e.g., resnet [12]) or state-of-the-art gastric lesion
classification methods to it. however, they may raise poor performance due to
the large intra-class variation between gv with the same bleeding risk and small
inter-class variation between gv and normal tissue or gv with different bleeding
risks. first, the gv area may look like regular stomach rugae as it is caused by
the blood vessels bulging and crumpling up the stomach (see fig. 1). also, since
the gv images are taken from different distances and angles, the number of
pixels of the gv area may not reflect its actual size. consequently, the model
may fail to focus on the important gv areas for prediction as shown in fig. 3.
to encourage the model to learn more robust representations, we constructively
introduce segmentation into the classification framework. with the segmentation
information, we further propose a region-constraint module (rcm) and a
cross-region attention module (cram) for better feature localization and
utilization. specifically, in rcm, we utilize the segmentation results to
constrain the cam heatmaps of the feature maps extracted by the classification
backbone, avoiding the model making predictions based on incorrect areas. in
cram, the varices features are extracted using the segmentation results and
combined with an attention mechanism to learn the intra-class correlation and
cross-region correlation between the target area and the context.to learn from
experienced endoscopists, gv datasets with bleeding risks annotation is needed.
while most works and public datasets focus on colonoscopy [13,15] and esophagus
[5,9], with a lack of study on gastroscopy images. in the public dataset of
endocv challenge [2], the majority are colonoscopies while only few are
gastroscopy images. in this work, we collect a gv bleeding risks rating dataset
(gvbleed) that contains 1678 gastroscopy images from 411 patients with different
levels of gv bleeding risks. three senior clinical endoscopists are invited to
grade the bleeding risk of the retrospective data in three levels and annotated
the corresponding segmentation masks of gv areas.in sum, the contributions of
this paper are: 1) a novel gv bleeding risk rating framework that constructively
introduces segmentation to enhance the robustness of representation learning; 2)
a region-constraint module for better feature localization and a cross-region
attention module to learn the correlation of target gv with its context; 3) a gv
bleeding risk rating dataset (gvbleed) with high-quality annotation from
multiple experienced endoscopists. baseline methods have been evaluated on the
newly collected gvbleed dataset. experimental results demonstrate the
effectiveness of our proposed framework and modules, where we improve the
accuracy by nearly 5% compared to the baseline model.",5
1164,Automatic Bleeding Risk Rating System of Gastric Varices,3.0,GVBleed Dataset,"data collection and annotation. the gvbleed dataset contains 1678 endoscopic
images with gastric varices from 527 cases. all of these cases are collected
from 411 patients in a grade-iii class-a hospital during the period from 2017 to
2022. in the current version, images from patients with ages elder than 18 are
retained 1 . the images are selected from the raw endoscopic videos and frames.
to maximize the variations, non-consecutive frames with larger angle differences
are selected. to ensure the quality of our dataset, senior endoscopists are
invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and nbi
pictures.criterion of gv bleeding risk level rating. based on the clinical
experience in practice, the gv bleeding risks in our dataset are rated into
three levels, i.e., mild, moderate, and severe. the detailed rating standard is
as follows: 1) mild: low risk of bleeding, and regular follow-up is sufficient
(usually with a diameter less than or equal to 5 mm). 2) moderate: moderate risk
of bleeding, and endoscopic treatment is necessary, with relatively low
endoscopic treatment difficulty (usually with a diameter between 5 mm and 10
mm). 3) severe: high risk of bleeding and endoscopic treatment is necessary,
with high endoscopic treatment difficulty. the varices are thicker (usually with
a diameter greater than 10 mm) or less than 10mm but with positive red signs.
note that the diameter is only one reference for the final risk rating since the
gv is with 1 please refer to the supplementary material for more detailed
information about our dataset. various 3d shapes and locations. the other facts
are more subjectively evaluated based on the experience of endoscopists. to
ensure the accuracy of our annotation, three senior endoscopists with more than
10 years of clinical experience are invited to jointly label each sample in our
dataset. if three endoscopists have inconsistent ratings for a sample, the final
decision is judged by voting. a sample is selected and labeled with a specific
bleeding risk level only when two or more endoscopists reach a consensus on it.
the gvbleed dataset is partitioned into training and testing sets for
evaluation, where the training set contains 1337 images and the testing set has
341 images. the detailed statistics of the three levels of gv bleeding risk in
each set are shown in table 1. the dataset is planned to be released in the
future.",5
1208,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,1.0,Introduction,"the early detection of lesions in medical images is critical for the diagnosis
and treatment of various conditions, including neurological disorders. stroke is
a leading cause of death and disability, where early detection and treatment can
significantly improve patient outcomes. however, the quantification of lesion
burden is challenging and can be time-consuming and subjective when performed
manually by medical professionals [14]. while supervised learning methods
[10,11] have proven to be effective in lesion segmentation, they rely heavily on
large fig. 1. overview of phanes (see fig. 2). our method can use both expert
annotatedor unsupervised generated masks to reverse and segment anomalies
annotated datasets for training and tend to generalize poorly beyond the learned
labels [21]. on the other hand, unsupervised methods focus on detecting patterns
that significantly deviate from the norm by training only on normal data.one
widely used category of unsupervised methods is latent restoration methods. they
involve autoencoders (aes) that learn low-dimensional representations of data
and detect anomalies through inaccurate reconstructions of abnormal samples
[17]. however, developing compact and comprehensive representations of the
healthy distribution is challenging [1], as recent studies suggest aes perform
better reconstructions on out-of-distribution (ood) samples than on training
samples [23]. various techniques have been introduced to enhance representation
learning, including discretizing the latent space [15], disentangling
compounding factors [2], and variational autoencoders (vaes) that introduce a
prior into the latent distribution [26,29]. however, methods that can enforce
the reconstruction of healthy generally tend to produce blurry
reconstructions.in contrast, generative adversarial networks (gans) [8,18,24]
are capable of producing high-resolution images. new adversarial aes combine
vaes' latent representations with gans' generative abilities, achieving sota
results in image generation and outlier detection [1,5,6,19]. nevertheless,
latent methods still face difficulties in accurately reconstructing data from
their low-dimensional representations, causing false positive detections on
healthy tissues.several techniques have been proposed that make use of the
inherent spatial information in the data rather than relying on constrained
latent representations [12,25,30]. these methods are often trained on a pretext
task, such as recovering masked input content [30]. de-noising aes [12] are
trained to eliminate synthetic noise patterns, utilizing skip connections to
preserve the spatial information and achieve sota brain tumor segmentation.
however, they heavily rely on a learned noise model and may miss anomalies that
deviate from the noise distribution [1]. more recently, diffusion models [9]
apply a more complex de-noising process to detect anomalies [25]. however, the
choice and granularity of the applied noise is crucial for breaking the
structure of anomalies [25]. adapting the noise distribution to the diversity
and heterogeneity of pathology is inherently difficult, and even if achieved,
the noising process disrupts the structure of both healthy and anomalous regions
throughout the entire image.in related computer vision areas, such as industrial
inspection [3], the topperforming methods do not focus on reversing anomalies,
but rather on detecting them by using large nominal banks [7,20], or pre-trained
features from large natural imaging datasets like imagenet [4,22]. salehi et al.
[22] have employed multi-scale knowledge distillation to detect anomalies in
industrial and medical imaging. however, the application of these networks in
medical anomaly segmentation, particularly in brain mri, is limited by various
challenges specific to the medical imaging domain. they include the variability
and complexity of normal data, subtlety of anomalies, limited size of datasets,
and domain shifts.this work aims to combine the advantages of constrained latent
restoration for understanding healthy data distribution with generative
in-painting networks. unlike previous methods, our approach does not rely on a
learned noise model, but instead creates masks of probable anomalies using
latent restoration. these guide generative in-painting networks to reverse
anomalies, i.e., preserve healthy tissues and produce pseudo-healthy in-painting
in anomalous regions. we believe that our proposed method will open new avenues
for interpretable, fast, and accurate anomaly segmentation and support various
clinical-oriented downstream tasks, such as investigating progression of
disease, patient stratification and treatment planning. in summary our main
contributions are:• we investigate and measure the ability of sota methods to
reverse synthetic anomalies on real brain t1w mri data. • we propose a novel
unsupervised segmentation framework, that we call phanes, that is able to
preserve healthy regions and utilize them to generate pseudo-healthy
reconstructions on anomalous regions. • we demonstrate a significant advancement
in the challenging task of unsupervised ischemic stroke lesion segmentation.",5
1215,SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,1.0,Introduction,"breast cancer is one of the high-mortality cancers among women in the 21st
century. every year, 1.2 million women around the world suffer from breast
cancer and about 0.5 million die of it [3]. accurate identification of cancer
types will make a correct assessment of the patient's risk and improve the
chances of survival. however, the traditional analysis method is time-consuming,
as it mainly depends on the experience and skills of the doctors. therefore, it
is essential to develop computer-aided diagnosis (cadx) for assisting doctors to
realize the rapid detection and classification.due to being collected by various
devices, the resolution of histopathological images extracted may not always be
high. low-resolution (lr) images lack of lots of details, which will have an
important impact on doctors' diagnosis. considering the improvement of
histopathological images' acquisition equipment will cost lots of money while
significantly increasing patients' expense of detection. the super-resolution
(sr) algorithms that improve the resolution of lr images at a small cost can be
a practical solution to assist doctors in diagnosis. at present, most single
super-resolution methods only have fixed receptive fields [7,10,11,18]. these
models cannot capture multi-scale features and do not solve the problems caused
by lr in various magnification factors well. mrc-net [6] adopted lstm [9] and
multi-scale refined context to improve the effect of reconstructing
histopathological images. it considered the problem of multi-scale, but only
fused two scales features. this limits its performance in the scenarios with
various magnification factors. therefore, designing an appropriate feature
extraction block for sr of the histopathological images is still a challenging
task.in recent years, a series of deep learning methods have been proposed to
solve the breast cancer histopathological image classification issue by the
highresolution (hr) histopathological images. [12,21,22] improved the specific
model structure to classify breast histopathology images, which showed a
significant improvement in recognition accuracy compared with the previous works
[1,20]. ssca [24] considered the problem of multi-scale feature extraction which
utilized feature pyramid network (fpn) [15] and attention mechanism to extract
discriminative features from complex backgrounds. however, it only concatenates
multi-scale features and does not consider the problem of feature fusion. so it
is still worth to explore the potential of extraction and fusion of multi-scale
features for breast images classification.to tackle the problem of lr breast
cancer histopathological images reconstruction and diagnosis, we propose the
single histopathological image super-resolution classification network
(shisrcnet) integrating super-resolution (sr) and classification (cf) modules.
the main contributions of this paper can be described as follows:(1) in the sr
module, we design a new block called multi-features extraction block (mfeblock)
as the backbone. mfeblock adopts multi-scale receptive fields to obtain
multi-scale features. in order to better fuse multi-scale features, a new fusion
method named multi-scale selective fusion (msf) is used for multi-scale
features. these make mfeblock reconstruct lr images into sr images well.(2) the
cf module completes the task of image classification by utilizing the sr images.
like sr module, it also needs to extract multi-scale features. the difference is
that the cf module can use the method of downsampling to capture multi-scale
features. so we combine the multi-scale receptive fields (sknet) [13] with the
feature pyramid network (fpn) to achieve the feature extraction of this module.
in fpn, we design a cross-scale selective fusion block (csfblock) to fuse
features of different scales.(3) through the joint training of these two
designed modules, the superresolution and classification of low-resolution
histopathological images are integrated into our model. for improving the
performance of cf module and reducing the error caused by the reconstructed sr
images, we introduce hr images to cf module in the training stage. the
experimental results demonstrate that the effects of our method are close to
those of sota methods that take hr breast cancer histopathological images as
inputs.",5
1231,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,1.0,Introduction,"magnetic resonance imaging (mri) has been extensively applied to clinical
diagnosis [16]. compared with computed tomography (ct), a brain mri is more
sensitive for multiple stroke types [3], therefore considered as the gold
standard for stroke diagnosis. nevertheless, the long acquisition time for a
brain mri (20 to 30 min) imposes challenges, especially in cases of acute stroke
where rapid diagnosis is essential and patient movement during this distressing
period of time commonly limits evaluation. as a result, mri acceleration
techniques have been developed to achieve more rapid diagnosis, increasing
resource availability while reducing costs [14,18]. a k-space sub-sampling (ks)
approach serves as a simple mri acceleration solution [20], compared with other
hardware-based acceleration methods. however, the signal loss by ks leads to
blurry reconstructed mr images that are less than ideal for a reliable clinical
diagnosis.artificial intelligence (ai) plays an increasingly important role in
mri-based diagnosis, for both mr image reconstruction and clinical decision
making. deep neural networks (dnn) were trained to reconstruct the mr images
from the sub-sampled k-space [10,13], which provides a better reconstruction
than the inverse fast fourier transform (ifft). nevertheless, detailed
information in the brain may still be lost in the reconstructed mr images due to
the signal sparsity in the k-space. on the other hand, traditional convolutional
neural network (cnn) [15] and the latest vision transformer (vit)-based [6]
predictive models have shown impressive prediction accuracy on stroke diagnosis
tasks, such as slice classification and lesion segmentation [7,11]. however,
these dnns trained on clean images through empirical risk minimization (erm) are
vulnerable to perturbations in the input images [2]. whatever the reconstruction
method used, even the slightest perturbation in accelerated mr images can lead
to a wrong stroke prediction from the ai models. therefore, building robust dnn
models to handle the perturbed mr image input is important for mri
acceleration.in this paper, we introduce a distributionally robust learning
(drl)-based approach [4] into the deep mr image classifier training, in order to
improve the model robustness to the image perturbation resulting from the signal
sparsity in accelerated mri. compared with erm, drl is an optimization method
minimizing the worst-case loss over an ambiguity set, therefore, can tolerate
outliers in the data [5]. we implemented drl to different linear layers in deep
cnn/vit classifiers, and applied a randomized training approach to improve the
training efficiency. our results show that on a real-world dataset, drl can
significantly improve the stroke classification performance of erm and other
baseline defensive training methods, when the signal sparsity and noise in
accelerated mri are generated by the cartesian undersampling (cu) method [20]
and white gaussian noise (wgn). we further show that in highly perturbed mr
images where the erm model and even clinicians cannot give a reliable diagnosis,
our drl model can still correctly recognize stroke, which establishes that our
method can assist accelerated mri diagnosis.",5
1234,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,3.1,Experimental Materials and Settings,"our dataset included mri brain scans from 226 patients performed at an urban
tertiary referral academic medical center that is a comprehensive stroke center.
clinical scans of adult patients aged 18-89 years with recent (acute or
subacute) strokes were identified between 1/1/2013 and 1/1/2021 for inclusion in
this study via a search of the philips performance bridge. scans meeting this
criteria were downloaded and simultaneously anonymized to preserve patient
anonymity and prevent disclosure of protected health information as part of this
irb exempt study. no patient demographic information was retained for the scans,
as it was considered to represent an unnecessary risk for accidental release of
protected health information. the diffusion weighted images with a gradient of
b=1000 were utilized for the analysis (see the supplement1 for information about
the mri scanner and parameter settings). each mr image contains multiple slices,
and every slice was annotated as normal or stroke by a board-certified
neuroradiologist with a subspecialty certification. annotation of the strokes
was performed on the diffusion weighted images using itk-snap (ver. 3.80) [19],
and all included mri examinations were reviewed by the neuroradiologist during
the annotation process to ensure that the images were of diagnostic quality
without significant motion degradation or other artifacts. to avoid the
dependency among the slices from the same subject, we applied a 2-d acquisition
during the mr imaging, and implemented a slice-level mr image preprocessing.
while the whole dataset includes 4,883 (74.7%) normal slices and 1,650 (25.3%)
stroke slices, we further randomly split them into training/validation/test sets
using the ratio 80%/10%/10%. for the training set, we implemented data
augmentation strategies by rotating or flipping each slice. finally, the
training/validation/test set contains 31,356/653/654 slices, correspondingly. we
implemented drl to both cnn and vit models. for the cnn model, we used a
resnet-18 [9] architecture, while for the vit model, we first pre-trained a
4-layer vit using a self-supervised pre-training method called masked
autoencoder (mae) [8], using the t1/t2-weighted brain mr images in the ixi
dataset [1]. mae pre-training first randomly masks 75% of the image patches in
an mri slice input, and then uses a vit encoder-decoder architecture to
reconstruct the masked mri patches, in order to learn the dependency among
different locations in the brain. after 400 pre-training epochs, an overall
satisfying reconstruction result can be observed in fig. 2.to evaluate the
binary classification performance of different models, we use the area under the
receiver operating characteristic (auroc) curve as our main metric. as our
dataset is unbalanced, we also considered the area under precision-recall curve
(auprc). we ran the experiments 3 times using different random seeds. the
training of our dnns were implemented on 3 nvidia rtx a6000 (48gb vram) gpus,
and each drl training epoch can be completed within 0.03 gpu hours. we used a
learning rate of 1 × 10 -5 and batchsize of 128 for drl training, while no
weight decay was applied. to solve the lmi problem in (5), we used sdpt3 v4.0
[17] as the solver. we set the cu perturbation with the acceleration factor of
4, 6, 8, 12 with the central fraction of 8%, 6%, 4% and 2% in k-space
respectively, and the remaining parts were chosen randomly in the peripheral
region accordingly.",5
1271,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,1.0,Introduction,"renal cancer is the most lethal malignant tumor of the urinary system, and the
incidence is steadily rising [13]. conventional b-mode ultrasound (us) is a good
screening tool but can be limited in its ability to characterize complicated
renal lesions. contrast-enhanced ultrasound (ceus) can provide information on
microcirculatory perfusion. compared with ct and mri, ceus is radiation-free,
cost-effective, and safe in patients with renal dysfunction. due to these
benefits, ceus is becoming increasingly popular in diagnosing renal lesions.
however, recognizing important diagnostic features from ceus videos to diagnose
lesions as benign or malignant is non-trivial and requires lots of experience.to
improve diagnostic efficiency and accuracy, many computational methods were
proposed to analyze renal us images and could assist radiologists in making
clinical decisions [6]. however, most of these methods only focused on
conventional b-mode images. in recent years, there has been increasing interest
in multi-modal medical image fusion [1]. directly concatenation and addition
were the most common methods, such as [3,4,12]. these simple operations might
not highlight essential information from different modalities. weight-based
fusion methods generally used an importance prediction module to learn the
weight of each modality and then performed sum, replacement, or exchange based
on the weights [7,16,17,19]. although effective, these methods did not allow
direct interaction between multi-modal information. to address this,
attention-based methods were proposed. they utilized cross-attention to
establish the feature correlation of different modalities and self-attention to
focus on global feature modeling [9,18]. nevertheless, we prove in our
experiments that these attentionbased methods may have the potential risks of
entangling features of different modalities.in practice, experienced
radiologists usually utilize dynamic information on tumors' blood supply in ceus
videos to make diagnoses [8]. previous researches have proved that temporal
information is effective in improving the performance of deep learning models.
lin et al. [11] proposed a network for breast lesion detection in us videos by
aggregating temporal features, which outperformed other image-based methods.
chen et al. [2] showed that ceus videos can provide more detailed blood supply
information of tumors allowing a more accurate breast lesion diagnosis than
static us images.in this work, we propose a novel multi-modal us video fusion
network (muvf-yolox) based on ceus videos for renal tumor diagnosis. our main
contributions are fourfold. (1) to the best of our knowledge, this is the first
deep learning-based multi-modal framework that integrates both b-mode and
ceusmode information for renal tumor diagnosis using us videos. (2) we propose
an attention-based multi-modal fusion (amf) module consisting of cross-attention
and self-attention blocks to capture modality-invariant and modality-specific
features in parallel. (3) we design an object-level temporal aggregation (ota)
module to make video-based diagnostic decisions based on the information from
multi-frames. (4) we build the first multi-modal us video datatset containing
b-mode and ceus-mode videos for renal tumor diagnosis. experimental results show
that the proposed framework outperforms single-modal, single-frame, and other
state-of-the-art methods in renal tumor diagnosis.",5
1370,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,1.0,Introduction,"the spreading digitalisation of pathology labs has enabled the development of
deep learning (dl) tools that can assist pathologists in their daily tasks.
however, supervised dl methods require detailed annotations in whole-slide
images (wsis) which is time-consuming, expensive and prone to inter-observer
disagreements [6]. multiple instance learning (mil) alleviates the need for
detailed annotations and has seen increased adoption in recent years. mil
approaches have proven to work well in academic research on histopathology data
[1,17,29] as well as in commercial applications [26]. most mil methods for
digital pathology employ an attention mechanism as it increases the reliability
of the algorithms, which is essential for successful clinical adoption
[14].domain shift in dl occurs when the data distributions of testing and
training differs [20,34]. this remains a significant obstacle to the deployment
of dl applications in clinical practice [7]. to address this problem previous
work either use domain adaptation when data from the target domain is available
[32], or domain generalisation when the target data is unavailable [34]. domain
adaptation has been explored in the mil setting too [22,23,27]. however, it may
not be feasible to perform an explicit domain adaptation, and an already adapted
model could still experience problems with domain shifts. hence, it is important
to provide indications of the expected performance on a target dataset without
requiring annotations [5,25]. another related topic is out-of-distribution (ood)
detection [33] which aims to detect individual samples that are ood, in contrast
to our objective of estimating a difference of expected performances between
some datasets. for supervised algorithms, techniques of uncertainty estimation
have been used to measure the effect of domain shift [4,15,18] and to improve
the robustness of predictions [19,21,30]. however, the reliability of
uncertainty estimates can also be negatively affected by domain shifts [11,31].
alternatively, a drop in performance can be estimated by comparing the model's
softmax outputs [8] or some hidden features [24,28] acquired on in-domain and
domain shift datasets. although such methods have been demonstrated for
supervised algorithms, as far as we know no previous work has explored domain
shift in the specific context of mil algorithms. hence, it is not clear how well
they will work in such a scenario.in this work, we evaluate an attention-based
mil model on unseen data from a new hospital and propose a way to quantify the
domain shift severity. the model is trained to perform binary classification of
wsis from lymph nodes of breast cancer patients. we split the data from the new
hospital into several subsets to investigate clinically realistic scenarios
triggering different levels of domain shift. we show that our proposed
unsupervised metric for quantifying domain shift correlates best with the
changes in performance, in comparison to multiple baselines. the approach of
validating a mil algorithm in a new site without collecting new labels can
greatly reduce the cost and time of quality assurance efforts and ensure that
the models perform as expected in a variety of settings. the novel contributions
of our work can be summarised as:1. proposing an unsupervised metric named
fréchet domain distance (fdd) for quantifying the effects of domain shift in
attention-based mil; 2. showing how fdd can help to identify subsets of patient
cases for which mil performance is worse than reported on the in-domain test
data; 3. comparing the effectiveness of using uncertainty estimation versus
learnt representations for domain shift detection in mil.",5
1375,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,3.0,Datasets,"grand challenge camelyon data [16] potentially large shift as some patients have
already started neoadjuvant treatment as well as the tissue may be affected from
the procedure of sentinel lymph node removal. 2a. 207 wsis with ductal carcinoma
(83 wsis with metastases): a small shift as it is the most common type of
carcinoma and relatively easy to diagnose. 2b. 68 wsis with lobular carcinoma
(28 wsis with metastases): potentially large shift as it is a rare type of
carcinoma and relatively difficult to diagnose.the datasets of lobular and
ductal carcinomas each contain 50 % of wsis from sentinel and axillary lymph
node procedures. the sentinel/axillary division is motivated by the differing dl
prediction performance on such subsets, as observed by jarkman et al. [13].
moreover, discussions with pathologists led to the conclusion that it is
clinically relevant to evaluate the performance difference between ductal and
lobular carcinoma. our method is intended to avoid requiring dedicated wsi
labelling efforts. we deem that the information needed to do this type of subset
divisions would be available without labelling since the patient cases in a
clinical setting would already contain such information. all datasets are
publicly available to be used in legal and ethical medical diagnostics research.",5
1383,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,1.0,Introduction,"liver cancer is the third leading cause of cancer death world-wide in 2020 [14].
early detection and accurate diagnosis of liver tumors may improve overall
patient outcomes, in which imaging plays a key role [11]. computed tomography
(ct) is one of the most important imaging modalities for liver tumors. dynamic
contrast-enhanced (dce) ct is widely used for diagnostics, but it requires
iodine contrast injection which can cause reaction and potential risks in
patients. recently, non-contrast (nc) ct scans are gaining attention as they are
cheaper and safer to acquire, thus can be potential tools for opportunistic
tumor screening [18,20]. meanwhile, finding and diagnosing tumors in nc cts is
also extremely challenging because of the poor contrast between tumors and
normal tissues compared to those in dce cts. prior works on pancreas [18] and
esophagus [20] have shown that latest deep learning techniques can detect subtle
texture and shape changes in nc ct that even human eyes may miss. thus, we aim
to investigate the performance of liver tumor segmentation and classification in
nc cts. such an approach will be helpful to discover asymptomatic incidental
tumors [12] from routine nc ct scans indicated for general diagnostic purposes
at no additional cost and radiation exposure. after an incidental tumor is
found, the patient may undergo further imaging examination such as a multi-phase
dce ct for differential diagnosis [11], which can provide useful discriminative
information such as the vascularity of lesions and the pattern of contrast agent
enhancement [19]. liver is largest solid organ in body and is the site of many
tumor types [11]. therefore, accurate tumor type classification is important for
the decision of treatment plans and prognosis.many researchers have developed
algorithms to automatically segment [1,9,13,15,23] or classify [19,21,25] liver
tumors in ct to help radiologists improve their accuracy and efficiency. for
example, public datasets such as the liver tumor segmentation benchmark (lits)
[1] fostered a series of works aiming to segment liver tumors with improved
convolutional neural network (cnn) backbones [9,13] and lesion edge information
[15]. lits only has single-phase cts (venous phase). several studies
investigated methods to exploit multi-phase ct by methods such as hetero-phase
fusion [5] and modality-aware mutual learning [23]. there are few work
discussing liver tumor analysis in nc ct [5]. besides lesion segmentation,
cnn-based lesion classification algorithms have been studied to distinguish
common lesion types [19,21,25].in this paper, we build a comprehensive framework
to address both tumor screening and diagnosis. (1) tumor screening involves
finding tumor patients in a large pool of healthy subjects and patients. most
existing works in tumor segmentation and detection did not explicitly consider
it since their training and testing images are all tumor patients. such models
may generate false positives in real-world screening scenario when facing
diverse tumor-free images. we collect a large-scale dataset with both tumor and
non-tumor subjects, where the non-tumor subjects includes not only healthy ones,
but also patients with various diffuse liver diseases such as steatosis and
hepatitis to improve the robustness of the algorithm. (2) most works studied
liver tumor segmentation alone without differentiating tumor types, while a few
works classify liver tumors on cropped tumor patches [19,21,25]. meanwhile, we
learn tumor segmentation and classification with one network using an instance
segmentation framework [3]. we train two networks for nc and multi-phase dce
cts, respectively. (3) for evaluation, previous segmentation works typically use
pixel-level metrics such as dice coefficient. such metrics cannot reflect the
lesion-level accuracy (how many lesion instances are correctly detected and
classified) and may bias to large lesions when a patient has multiple tumors.
patient-level metrics (e.g. classifying whether a subject has malignant tumors)
are also useful for treatment recommendation in clinical practice [18,20].
therefore, we assess our algorithm thoroughly with pixel, lesion, and
patient-level metrics.algorithms for liver tumor segmentation have focused on
improving the feature extraction backbone of a fully-convolutional cnn
[9,13,15,23]. the pixelwise segmentation architectures may not be optimal for
lesion and patient-level evaluation metrics since they cannot consider a lesion
or an image holistically. recently, a series of mask transformer algorithms
[3,4,17] have emerged in the computer vision community and achieved the
state-of-the-art performance in instance segmentation tasks. in brief, they use
object queries to interact with image feature maps and with each other to
produce mask and class predictions for each instance. inspired by them, we
propose a novel end-to-end framework named pixel-lesion-patient network (plan)
for lesion segmentation and classification, as well as patient classification.
it contains three branches with bottomup cooperation: the segmentation map from
the pixel branch helps to initialize the lesion branch, which is an improved
mask transformer aiming to segment and classify each lesion; the patient branch
aggregates information from the whole image and predicts image-level labels of
each lesion type, with regularization terms to encourage consistency with the
lesion branch.we collected a large-scale multi-phase dataset containing 810
non-tumor subjects and 939 tumor patients. 4010 tumor instances of eight types
are extensively annotated based on pathological reports. on the non-contrast
tumor screening and diagnosis task, plan achieves 95.0%, 96.4%, and 0.965 in
patient-level sensitivity, specificity, and average auc for malignant and benign
patients, in contrast to 94.4%, 93.7%, and 0.889 for the widely-used nnu-net
[8]. on multi-phase dce ct, our lesion-level detection precision, recall, and
classification accuracy are 92.2%, 89.0%, 85.9%, outperforming nnu-net [8] and
mask2former [3]. we further conduct a reader study on a holdout set of 250
cases. our algorithm is on par with a senior radiologist (16 yrs experience),
showing the clinical significance of our results. our codes will be made public
upon institutional approval.",5
1385,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,2.2,Pixel-Lesion-Patient Network (PLAN),"our goal is to segment the mask and classify the type of each tumor in a liver
ct. we also hope to make patient-level diagnoses for each ct scan. plan is
inspired by mask2former [3] with three key improvements: (1) a pixel branch is
added to provide anchor queries to the lesion branch. (2) the lesion branch is
composed of the transformer decoder in mask2former, and we improve its
segmentation loss to enhance recall of small lesions. (3) a patient branch is
attached to make dedicated image-level predictions with a proposed
lesion-patient consistency loss. our framework is shown in fig. 1.pixel branch
and anchor queries. the pixel branch is a convolutional layer after the pixel
decoder and learns to predict pixel-wise segmentation maps similar to
traditional segmentators. we do cc analysis to the predicted mask to extract
lesion instances, and then average the pixel embeddings inside each predicted
lesion to obtain a feature vector. the feature vectors are regarded as anchor
queries and work the same way as the randomly initialized queries in the lesion
branch. compared to the random queries in the original mask2former, the anchor
queries contain prior information of the lesions to be segmented, helping the
lesion branch to match with the lesion targets more easily [10].lesion branch
and foreground-enhanced sampling loss. similar to mask2former, the lesion branch
predicts a binary mask and a class label for each query, see fig. 1. mask2former
calculates its segmentation loss on k sampled pixels instead of on the whole
image, which is shown to both improve accuracy and reduce gpu memory usage [3].
however, in lesion segmentation, some tumors are very small compared to the
whole 3d image. the importance sampling strategy [3] can hardly select any
foreground pixels in such cases, so the loss only contains background pixels,
degrading the segmentation recall of small lesions. we propose a simple approach
to remedy this issue by sampling an extra n foreground pixels for each
lesion.patient branch. a patient-level diagnosis is useful for triage. for
example, diagnosing the subject as normal, benign, or malignant will result in
completely different treatments [24]. intuitively, we can also infer
patient-level labels from segmentation results by checking if there is any
lesion in the predicted mask. however, certain tumors are often related to signs
outside the tumor, e.g. hepatocellular carcinoma and cirrhosis,
cholangiocarcinoma and bile duct dilatation, etc. we equip plan with a dedicated
patient branch to aggregate such global information to make better patient-level
prediction. since one patient can have multiple liver tumors of different types,
in our problem, we give each image several hierarchical binary labels. the first
label classifies normal and tumor subjects (whether the image contains any
tumor); the second and third labels indicate the existence of respectively
benign and malignant tumors; the rest c labels suggest the existence of c
fine-grained types of tumors. we employ the dual-path transformer block [17] to
fuse multi-scale features from the pixel encoder and decoder to generate a
feature map, followed by global average pooling and a linear classification
layer to predict the c + 3 labels.a lesion-patient consistency loss is further
proposed to encourage coherence of the lesion and patient-level predictions.
inspired by multi-instance learning [6], we compute a pseudo patient-level
prediction c ∈ r c from the lesion-level predictions by max-pooling the class
probability of each class across all lesion queries (discarding the no-object
class). we also have the probability vector from the patient branch p ∈ r c
corresponding to the c fine-grained classes. then, we compute the l2 loss
between them:the overall loss of plan is listed in eq. 1, where l pixel is the
combined crossentropy (ce) and dice loss for the pixel branch as in nnu-net [8];
l lesion-class is the ce loss [3] for lesion classification in the lesion
branch; l lesion-mask is the combined ce and dice loss [3] for binary lesion
segmentation in the lesion branch with the foreground-enhanced sampling
strategy; l patient is the binary ce loss for the multi-label classification
task in the patient branch.",5
1386,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,3.0,Experiments,"data. our dataset contains 810 normal subjects and 939 patients with liver
tumors. each normal subject has a non-contrast (nc) ct, while each patient has a
dynamic contrast-enhanced (dce) ct scan with nc, arterial, and venous phases. we
use deeds [7] to register nc and arterial phases to the venous phase, and then
invite a senior radiologist with 10 years of experience to annotate on the
multi-phase cts using ct labeler [16]. the 3d mask and the type of all liver
tumors are annotated based on pathological reports and magnetic resonance scans
if necessary. eight tumor types are considered in our study: hepatocellular
carcinoma (hcc), intrahepatic cholangiocarcinoma (icc), metastasis (meta),
hepatoblastoma (hepato), hemangioma (heman), focal nodular hyperplasia (fnh),
cyst, and others (all other tumor types). if a lesion's type cannot be
determined according to image signs [11] and pathology, it will be marked as
""unknown"" and ignored in training and evaluation. in total, 4010 tumor instances
are annotated, whose volumes range from 11 to 3.7×10 6 mm 3 . detailed
statistics and examples of the lesions are shown in the supplementary material.
we train two separate networks for nc and dce cts. in the former setting, both
normal and patient data are used and randomly split into 1149 training, 100
validation, and 500 testing. in the latter one, only patient data are used with
641 training, 100 validation, and 200 testing. another hold-out set of 150
patients and 100 normal cts are used for reader study to compare our accuracy
with two radiologists. implementation details. each ct is resampled to
0.7×0.7×5mm in spacing. we first train an nnu-net on public datasets to segment
liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and
pancreas), and then crop the liver region to train plan. to help plan
differentiate liver tumors and other organs, we train the network to segment
both tumors and organs using the predicted organ labels. plan is built on top of
the nnu-net framework [8]. its pixel encoder is a u-net encoder, whereas its
pixel decoder is a light-weight feature pyramid network [3]. the lesion branch
incorporates three transformer decoder blocks with masked attention [3] which
use feature maps of strides 16, 8, 4 from the pixel decoder. the number of
random queries is q = 20; the embedding dimension is m = 64; the number of
sampled pixels is k = 12544 [3], foreground pixels n = 3; the loss weight is 0.1
for the no-object class while 1 for other classes in the lesion branch [3]. the
weights in eq. 1 arewe use the radam optimizer with an initial learning rate of
0.0001. each training batch contains two patches of size 256 × 256 × 24. for dce
ct, the three phases form a 3-channel image as the network input. extensive data
augmentation is applied including random cropping, scaling, flipping, elastic
deformation, and brightness adjustment [8].during training, we first pretrain
the backbone and the pixel branch for 500 epochs, and then train the whole
network for another 500 epochs.patient-level results. this paper has three major
goals: tumor screening in nc ct (classifying a subject as normal or tumor),
preliminary diagnosis in nc ct (predicting the existence of malignant and benign
tumors), and fine-grained diagnosis in dce ct (predicting the existence of 8
tumor types). among the 8 tumor types, hcc, icc, meta, and hepato are malignant;
heman, fnh, and cyst are benign. ""others"" can be either malignant or benign,
thus are excluded in the preliminary diagnosis task. the nc test set contains
198 tumor cases, 202 completely normal cases, and 100 ""hard"" non-tumor cases
which may have larger image noise, artifact, ascites, diffuse liver diseases
such as hepatitis and steatosis. these cases are used to test the robustness of
the model in real-world screening scenario with diverse tumor-free images. we
compare plan with a widely-used strong baseline, nnu-net [8]. the recent mask
transformer, mask2former [3], is also adapted to 3d for comparison. for the
baselines, patient-level labels are inferred from their predicted masks by
counting lesion pixels. as displayed in table 1, plan achieves the best accuracy
on all tasks, especially in nc preliminary diagnosis tasks, which demonstrates
the effectiveness of its dedicated patient branch that can explicitly aggregate
features from the whole image.lesion and pixel-level results. in lesion-level
evaluation, we treat a prediction as a true positive if its overlap with a
ground-truth lesion is >0.2 in dice. lesions smaller than 3 mm in radius are
ignored. as shown in table 2, the pixellevel accuracy of nnu-net and plan are
comparable, but plan's lesion-level accuracy is consistently higher than
nnu-net. in this work, we focus more on patient and lesion-level metrics.
although nc images have low contrast, they can still be used to segment and
classify lesions with ∼ 80% precision, recall, and classification accuracy. it
implies the potential of nc ct, which has been understudied in previous works.
mask2former has higher precision but lower recall in nc ct, especially for small
lesions, while plan achieves the best recall using the foreground-enhanced
sampling loss. both plan and mask2former achieve better classification accuracy,
which illustrates the mask transformer architecture is good at lesion-level
classification.",5
1387,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,0.0,Comparison with Radiologists.,"in the reader study, we invited a senior radiologist with 16 years of experience
in liver imaging, and a junior radiologist with 2 years of experience. they
first read the nc ct of all subjects and provided a diagnosis of normal, benign,
or malignant. then, they read the dce scans and provided a diagnosis of the 8
tumor types. we consider patients with only one tumor type in this study. their
reading process is without time constraint. in table 3 and fig. 2, all methods
get good specificity probably because the normal subjects are completely
healthy. our model achieves comparable accuracy with the senior radiologist but
outperforms the junior one by a large margin in sensitivity and classification
accuracy. an ablation study for our method is shown in table 4. it can be seen
that our proposed anchor queries produced by the pixel branch, fes loss, and
lesionpatient consistency loss are useful for the final performance. the
efficacy of the lesion and patient branches has been analyzed above based on the
lesion and patient-level results. due to space limit, we will show the accuracy
for each tumor type and more qualitative examples in the supplementary
material.comparison with literature. in the pixel level, we obtain dice scores
of 77.2% and 84.2% using nc and dce cts, respectively. the current state of the
art (sota) of lits [1] achieved 82.2% in dice using cts in venous phase; [23]
achieved 81.3% in dice using dce ct of two phases. in the lesion level, our
precision and recall are 80.1% and 81.9% for nc ct, 92.2% and 89.0% for dce ct,
at 20% overlap. [25] achieved 83% and 93% for dce ct. sota of lits achieved
49.7% and 46.3% at 50% overlap. [21] classified lesions into 5 classes,
achieving 84% accuracy for dce and 49% for nc ct. we classify lesions into 8
classes with 85.9% accuracy for dce and 78.5% for nc ct. in the patient level,
[5] achieved auc=0.75 in nc ct tumor screening, while our auc is 0.985. in
summary, our results are superior or comparable to existing works.",5
1398,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,1.0,Introduction,"head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is
among the most common cancers worldwide [1]. survival prediction, a regression
task that models the survival outcomes of patients, is crucial for h&n cancer
patients: it provides early prognostic information to guide treatment planning
and potentially improves the overall survival outcomes of patients [2].
multi-modality imaging of positron emission tomography -computed tomography
(pet-ct) has been shown to benefit survival prediction as it offers both
anatomical (ct) and metabolic (pet) information about tumors [3,4]. therefore,
survival prediction from pet-ct images in h&n cancer has attracted wide
attention and serves as a key research area. for instance, head and neck tumor
segmentation and outcome prediction challenges (hecktor) have been held for the
last three years to facilitate the development of new algorithms for survival
prediction from pet-ct images in h&n cancer [5][6][7].traditional survival
prediction methods are usually based on radiomics [8], where handcrafted
radiomics features are extracted from pre-segmented tumor regions and then are
modeled by statistical survival models, such as the cox proportional hazard
(coxph) model [9]. in addition, deep survival models based on deep learning have
been proposed to perform end-to-end survival prediction from medical images,
where pre-segmented tumor masks are often unrequired [10]. deep survival models
usually adopt convolutional neural networks (cnns) to extract image features,
and recently visual transformers (vit) have been adopted for its capabilities to
capture long-range dependency within images [11,12]. these deep survival models
have shown the potential to outperform traditional survival prediction methods
[13]. for survival prediction in h&n cancer, deep survival models have achieved
top performance in the hecktor 2021/2022 and are regarded as state-of-the-art
[14][15][16]. nevertheless, we identified that existing deep survival models
still have two main limitations.firstly, existing deep survival models are
underdeveloped in utilizing complementary multi-modality information, such as
the metabolic and anatomical information in pet and ct images. for survival
prediction in h&n cancer, existing methods usually use single imaging modality
[17,18] or rely on early fusion (i.e., concatenating multi-modality images as
multi-channel inputs) to combine multi-modality information [11,[14][15][16]19].
in addition, late fusion has been used for survival prediction in other diseases
such as gliomas and tuberculosis [20,21], where multi-modality features were
extracted by multiple independent encoders with resultant features fused.
however, early fusion has difficulties in extracting intra-modality information
due to entangled (concatenated) images for feature extraction, while late fusion
has difficulties in extracting inter-modality information due to fully
independent feature extraction. recently, tang et al. [22] attempted to address
this limitation by proposing a multi-scale non-local attention fusion (mnaf)
block for survival prediction of glioma patients, in which multi-modality
features were fused via non-local attention mechanism [23] at multiple scales.
however, the performance of this method heavily relies on using tumor
segmentation masks as inputs, which limits its generalizability.secondly,
although deep survival models have advantages in performing end-to-end survival
prediction without requiring tumor masks, this also incurs difficulties in
extracting region-specific information, such as the prognostic information in
primary tumor (pt) and metastatic lymph node (mln) regions. to address this
limitation, recent deep survival models adopted multi-task learning for joint
tumor segmentation and survival prediction, to implicitly guide the model to
extract features related to tumor regions [11,16,[24][25][26]. however, most of
them only considered pt segmentation and ignored the prognostic information in
mln regions [11,[24][25][26]. meng et al. [16] performed survival prediction
with joint pt-mln segmentation and achieved one of the top performances in
hecktor 2022. however, this method extracted entangled features related to both
pt and mln regions, which incurs difficulties in discovering the prognostic
information in pt-/mln-only regions.in this study, we design an x-shape
merging-diverging hybrid transformer network (named xsurv, fig. 1) for survival
prediction in h&n cancer. our xsurv has a merging encoder to fuse complementary
anatomical and metabolic information in pet and ct images and has a diverging
decoder to extract region-specific prognostic information in pt and mln regions.
our technical contributions in xsurv are three folds: (i) we propose a
merging-diverging learning framework for survival prediction. this framework is
specialized in leveraging multi-modality images and extracting regionspecific
information, which potentially could be applied to many survival prediction
tasks with multi-modality imaging. (ii) we propose a hybrid parallel
cross-attention (hpca) block for multi-modality feature learning, where both
local intra-modality and global inter-modality features are learned via parallel
convolutional layers and crossattention transformers. (iii) we propose a
region-specific attention gate (rag) block for region-specific feature
extraction, which screens out the features related to lesion regions. extensive
experiments on the public dataset of hecktor 2022 [7] demonstrate that our xsurv
outperforms state-of-the-art survival prediction methods, including the
top-performing methods in hecktor 2022.",6
1404,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.1,Dataset and Preprocessing,"we adopted the training dataset of hecktor 2022 (refer to
https://hecktor.grand-cha llenge.org/), including 488 h&n cancer patients
acquired from seven medical centers [7], while the testing dataset was excluded
as its ground-truth labels are not released. each patient underwent pretreatment
pet/ct and has clinical indicators. we present the distributions of all clinical
indicators in the supplementary materials. recurrence-free survival (rfs),
including time-to-event in days and censored-or-not status, was provided as
ground truth for survival prediction, while pt and mln annotations were provided
for segmentation. the patients from two centers (chum and chuv) were used for
testing and other patients for training, which split the data into 386/102
patients in training/testing sets. we trained and validated models using 5-fold
cross-validation within the training set and evaluated them in the testing
set.we resampled pet-ct images into isotropic voxels where 1 voxel corresponds
to 1 mm 3 . each image was cropped to 160 × 160 × 160 voxels with the tumor
located in the center. pet images were standardized using z-score normalization,
while ct images were clipped to [-1024, 1024] and then mapped to [-1, 1]. in
addition, we performed univariate and multivariate cox analyses on the clinical
indicators to screen out the prognostic indicators with significant relevance to
rfs (p < 0.05).",6
1432,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,1.0,Introduction,"accurate spatial characterization of tumor immune microenvironment is critical
for precise therapeutic stratification of cancer patients (e.g. via
immunotherapy). currently, this characterization is done manually by individual
pathologists on standard hematoxylin-and-eosin (h&e) or singleplex
immunohistochemistry (ihc) stained images. however, this results in high
interobserver variability among pathologists, primarily due to the large (> 50%)
disagreement among pathologists for immune cell phenotyping [10]. this is also a
big cause of concern for publicly available h&e/ihc cell segmentation datasets
with immune cell annotations from single pathologists. multiplex staining
resolves this issue by allowing different tumor and immune cell markers to be
stained on the same tissue section, avoiding any phenotyping guesswork from
pathologists. multiplex staining can be performed using expensive multiplex
immunofluorescence (mif) or via cheaper multiplex immunohistochemistry (mihc)
assays. mif staining (requiring expensive scanners and highly skilled lab
technicians) allows multiple markers to be stained/expressed on the same tissue
section (no co-registration needed) while also providing the utility to turn
on/off individual markers as needed. in contrast, current brightfield mihc
staining protocols relying on dab (3,3'-diaminobenzidine) alcohol-insoluble
chromogen, even though easily implementable with current clinical staining
protocols, suffer from occlusion of signal from sequential staining of
additional markers. to this effect, we introduce a new brightfield mihc staining
protocol using alcoholsoluble aminoethyl carbazole (aec) chromogen which allows
repeated stripping, restaining, and scanning of the same tissue section with
multiple markers. this requires only affine registration to align the digitized
restained images to obtain non-occluded signal intensity profiles for all the
markers, similar to mif staining/scanning.in this paper, we introduce a new
dataset that can be readily used out-ofthe-box with any artificial intelligence
(ai)/deep learning algorithms for spatial characterization of tumor immune
microenvironment and several other use cases.to date, only two denovo stained
datasets have been released publicly: bci h&e and singleplex ihc her2 dataset
[7] and deepliif singleplex ihc ki67 and mif dataset [2], both without any
immune or tumor markers. in contrast, we release the first denovo mif/mihc
stained dataset with tumor and immune markers for more accurate characterization
of tumor immune microenvironment. we also demonstrate several interesting use
cases: (1) ihc quantification of cd3/cd8 tumor-infiltrating lymphocytes (tils)
via style transfer, (2) virtual translation of cheap mihc stains to more
expensive mif stains, and (3) virtual tumor/immune cellular phenotyping on
standard hematoxylin images.",6
1433,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.0,Dataset,"the complete staining protocols for this dataset are given in the accompanying
supplementary material. images were acquired at 20× magnification at moffitt
cancer center. the demographics and other relevant information for all eight
head-and-neck squamous cell carcinoma patients is given in table 1.",6
1434,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.1,Region-of-Interest Selection and Image Registration,"after scanning the full images at low resolution, nine regions of interest
(rois) from each slide were chosen by an experienced pathologist on both mif and
mihc images: three in the tumor core (tc), three at the tumor margin (tm), and
three outside in the adjacent stroma (s) area. the size of the rois was
standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total
surface area of 0.343 mm 2 . hematoxylin-stained rois were first used to align
all the mihc marker images in the open source fiji software using affine
registration. after that, hematoxylin-and dapi-stained rois were used as
references to align mihc and mif rois again using fiji and subdivided into
512×512 patches, resulting in total of 268 co-registered mihc and mif patches
(∼33 co-registered mif/mihc images per patient).",6
1438,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,0.0,(b) Style Transfer:,"this sub-network creates the stylized ihc image using an attention module, given
(1) the input hematoxylin and the mif marker images and (2) the style and its
corresponding marker images. for synthetically generating stylized ihc images,
we follow the approach outlined in adaattn [8]. we use a pre-trained vgg-19
network [12] as an encoder to extract multi-level feature maps and a decoder
with a symmetric structure of vgg-19. we then use both shallow and deep level
features by using adaattn modules on multiple layers of vgg. this sub-network is
used to create a stylized image using the structure of the given hematoxylin
image while transferring the overall color distribution of the style image to
the final stylized image. the generated marker image from the first sub-network
is used for a more accurate colorization of the positive cells against the blue
hematoxylin counterstain/background; not defining loss functions based on the
markers generated by the first sub-network leads to discrepancy in the final
brown dab channel synthesis.for the stylized ihc images with ground truth
cd3/cd8 marker images, we also segmented corresponding dapi images using our
interactive deep learning impartial [9] tool
https://github.com/nadeemlab/impartial and then classified the segmented masks
using the corresponding cd3/cd8 channel intensities, as shown in fig. 4. we
extracted 268 tiles of size 512×512 from this final segmented and co-registered
dataset. for the purpose of training and testing all the models, we extract four
images of size 256 × 256 from each tile due to the size of the external ihc
images, resulting in a total of 1072 images. we randomly extracted tiles from
the lyon19 challenge dataset [14] to use as style ihc images. using these
images, we created a dataset of synthetically generated ihc images from the
hematoxylin and its marker image as shown in fig. 3.we evaluated the
effectiveness of our synthetically generated dataset (stylized ihc images and
corresponding segmented/classified masks) using our generated dataset with the
nuclick training dataset (containing manually segmented cd3/cd8 cells) [6]. we
randomly selected 840 and 230 patches of size 256 × 256 from the created dataset
for training and validation, respectively. nuclick training and validation sets
[6] comprise 671 and 200 patches, respectively, of size 256 × 256 extracted from
lyon19 dataset [14]. lyon19 ihc cd3/cd8 images are taken from breast, colon, and
prostate cancer patients. we split their training set into training and
validation sets, containing 553 and 118 images, respectively, and use their
validation set for testing our trained models. we trained three models including
unet [11], fpn [5], unet++ [15] with the backbone of resnet50 for 200 epochs and
early stopping on validation score with patience of 30 epochs, using binary
cross entropy loss and adam optimizer with learning rate of 0.0001. as shown in
table 2, models trained with our synthetic training set outperform those trained
solely with nuclick data in all metrics.we also tested the trained models on
1,500 randomly selected images from the training set of the lymphocyte
assessment hackathon (lysto) [1], containing image patches of size 299 × 299
obtained at a magnification of 40× from breast, prostate, and colon cancer whole
slide images stained with cd3 and cd8 markers. only the total number of
lymphocytes in each image patch are reported in this dataset. to evaluate the
performance of trained models on this dataset, we counted the total number of
marked lymphocytes in a predicted mask and calculated the difference between the
reported number of lymphocytes in each image with the total number of
lymphocytes in the predicted mask by the model. in table 2, the average
difference value (diffcount) of lymphocyte number for the whole dataset is
reported for each model. as seen, the trained models on our dataset outperform
the models trained solely on nuclick data.",6
1441,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,4.0,Conclusions and Future Work,"we have released the first ai-ready restained and co-registered mif and mihc
dataset for head-and-neck squamous cell carcinoma patients. this dataset can be
used for virtual phenotyping given standard clinical hematoxylin images, virtual
clinical ihc dab generation with ground truth segmentations (to train
highquality segmentation models across multiple cancer types) created from
cleaner mif images, as well as for generating standardized clean mif images from
neighboring h&e and ihc sections for registration and 3d reconstruction of
tissue specimens. in the future, we will release similar datasets for additional
cancer types as well as release for this dataset corresponding whole-cell
segmentations via impartial https://github.com/nadeemlab/impartial.",6
1449,Detection of Basal Cell Carcinoma in Whole Slide Images,0.0,"Acc(W, d, D","3 experiments the dataset, comprised of 194 skin slides acquired from the
southern sun pathology laboratory, includes 148 bcc cases and 46 other types
(common nevus, scc), all manually annotated by a dermatopathologist. bcc slides
served as positive samples and the rest as negatives. these slides were scanned
at ×20 magnification with a 0.44 µm pixel size using a leica aperio at2 scanner.
the patient data were separated between training and testing to prevent overlap.
details are shown in table 1. the experimental setup involved training models on
two nvidia rtx a6000 gpus using pytorch. these models, initialized from a
zero-mean gaussian with standard deviation σ = 0.001, were trained for 200
epochs with a batch size of 256. training used the adam optimizer with a dynamic
learning rate reduction strategy, starting with a learning rate of 5e-5
following a cosine schedule.",6
1468,Multi-scale Prototypical Transformer for Whole Slide Image Classification,2.1,MIL Problem Formulation,"mil is a typical weakly supervised learning method, where the training data
consists of a set of bags, and each bag contains multiple instances. the goal of
mil is to learn a classifier that can predict the label of a bag based on the
instances in it. in binary classification, a bag can be marked as negative if
all in-stances in the bag are negative, otherwise, the bag is labeled as
positive with at least one positive instance. in the mil setting, a wsi is
considered as a bag and the numerous cropped patches in wsi are regarded as
instances in the bag. a wsi dataset t can be defined as:where x i denotes a
patient, y i the label of x i , i j i is the j-th instance of x i , n is the
number of patients and n is the number of instances.",6
1484,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,3.0,Experiments,"dataset. our dataset contained 282 consecutive patients who underwent thyroid
nodule examination at nanjing drum tower hospital. all patients performed
dynamic ceus examination by an experienced sonographer using an iu22 scanner
(philips healthcare, bothell, wa) equipped with a linear transducer l9-3 probe.
these 282 cases included 147 malignant nodules and 135 benign nodules. on the
one hand, the percutaneous biopsy based pathological examination was implemented
to determine the ground-truth of malignant and benign. on the other hand, a
sonographer with more than 10 years of experience manually annotated the nodule
lesion mask to obtain the pixel-level groundtruth of thyroid nodules
segmentation. all data were approved by the institutional review board of
nanjing drum tower hospital, and all patients signed the informed consent before
enrollment into the study.implementation details. our network was implemented
using pytorch framework with the single 12 gb gpu of nvidia rtx 3060. during
training, we first pre-trained the talr backbone via dice loss for 30 epochs and
used adam optimizer with learning rate of 0.0001. then, we loaded the
pre-trained weights to train the whole model for 100 epochs and used adam
optimizer with learning rate of 0.0001. here, we set batch-size to 4 during the
entire training process the ceus consisted the full wash-in and wash-out phases,
and the resolution of each frame was (600 × 800). in addition, we carried out
data augmentation, including random rotation and cropping, and we resize the
resolution of input frames to (224 × 224). we adopted 5-fold cross-validation to
achieve quantitative evaluation. three indexes including dice, recall, and iou,
were used to evaluate the lesion recognition task, while five indexes, namely
average accuracy (acc), sensitivity (se), specificity (sp), f1-score (f1), and
auc, were used to evaluate the diagnosis task.experimental results. as in table
1, we compared our method with sota method including v-net, unet3d, transunet.
for the task of identifying lesions, the index of recall is important, because
information in irrelevant regions can be discarded, but it will be disastrous to
lose any lesion information. v-net achieved the highest recall scores compared
to others; thus, it was chosen as the backbone of tlar. table 1 revealed that
the modules (tpa, saf, and ipo) used in the network greatly improved the
segmentation performance compared to baseline, increasing dice and recall scores
by 7.60% and 7.23%, respectively. for the lesion area recognition task, our
method achieved the highest dice of 85.54% and recall of 90.40%, and the
visualized results were shown in fig. 3. to evaluate the effectiveness of the
baseline of lightweight c3d, we compared the results with sota video
classification methods including c3d, r3d, r2plus1d and convlstm. for fair
comparison, all methods used the manually annotated lesion mask to assist the
diagnosis. experimental results in table 2 revealed that our baseline network
could be useful for the diagnosis. with the effective baseline, the introduced
modules including tlar, saf and ipo further improved the diagnosis accuracy,
increasing the accuracy by 9.5%. the awareness of microvascular infiltration
using saf and ipo unit was helpful for ceus-based diagnosis, as it could improve
the diagnosis accuracy by 7.69% (as in table 2). as in appendix fig. a1,
although sota method fails to focus on lesion areas, our method can pinpoint
discriminating lesion areas. influence of α values. the value of α in saf is
associated with simulating microvessel infiltration. figure 3 (c) showed that
the diagnosis accuracy increased along with the increment of α and then tended
to become stable when α was close to 9. therefore, for balancing the efficiency
and performance, the number of ipo was set as n = 3 and α was set as α = {1, 5,
9} to generate a group of confidence maps that can simulate the process of
microvessel infiltration. (more details about the setting of n is in appendix
fig. a4 of the supplementary material.)",6
1495,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,1.0,Introduction,"the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b
lymphocytes, and t lymphocytes), stromal, and other cells together with
noncellular tissue components [3,5,24,30]. it is well acknowledged that tumors
evolve in close interaction with their microenvironment. quantitatively
characterizing tme has the potential to predict tumor aggressiveness and
treatment response [3,23,24,30]. different types of lymphocytes such as cd4+
(helper t cells), cd8+ (cytotoxic t cells), cd20+ (b cells), within the tme
naturally interact with tumor and stromal cells. studies [5,9] have shown that
quantifying spatial interplay of these different cell families within the tme
can provide more prognostic/predictive value compared to only measuring the
density of a single biomarker such as tumor-infiltrating lymphocytes (tils)
[3,24]. immunotherapy (io) is the standard treatment for patients with advanced
non-small cell lung cancer (nsclc) [19] but only 27-45% of patients respond to
this treatment [21]. therefore, better algorithms and improved biomarkers are
essential for identifying which cancer patients are most likely to respond to io
in advance of treatment. quantitative features that relate to the complex
spatial interplay between different types of b-and t-cells in the tme might
unlock attributes that are associated with io response. in this study, we
introduce a novel approach called triangular analysis of geographical interplay
of lymphocytes (triangil), representing a unique and interpretable way to
characterize the distribution, and higher-order interaction of various cell
families (e.g., cancerous cells, stromal cells, lymphocyte subtypes) across
digital histopathology slides. we demonstrate the efficacy of triaangil for
characterizing tme in the context of predicting 1) response to io with immune
checkpoint inhibitors (ici), 2) overall survival (os), in patients with nsclc,
and 3) providing novel insights into the spatial interplay between different
immune cell subtype. triangil source code is publicly available at
http://github.com/sarayar/triangil.",6
1496,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,2.0,Previous Related Work and Novel Contributions,"many studies have only looked at the density of a single biomarker (e.g. tils),
to show that a high density of tils is associated with improved patient survival
and treatment response in nsclc [3,24]. other works have attempted to
characterize the spatial arrangement of cells in tme using computational
graphbased approaches. these approaches include methods that connect cells
regardless of their type (1) using global graphs (gg) such as voronoi that
connect all nuclei [2,14], or (2) using cell cluster graphs (ccg) [16] to create
multiple nuclear subgraphs based on cell-to-cell proximity to predict tumor
aggressiveness and patient outcome [16]. others have explored (3) the spatial
interplay between two different cell types [5].one example approach is spatial
architecture of til (spatil) [9] which attempted to characterize the interplay
between immune and cancer cells and has proven to be helpful in predicting the
recurrence in early-stage nsclc. all of these approaches point to overwhelming
evidence that spatial architecture of cells in tme is critical in predicting
cancer outcome. however, these approaches have not been able to exploit
higher-order interactions and dependencies between multiple cell types (> 2),
relationships that might provide additional actionable insights. the
contributions of this work include:(1) triangil is a computational framework
that characterizes the architecture and relationships of different cell types
simultaneously. instead of measuring only simple two-by-two relations between
cells, it seeks to identify triadic spatial relations (hyperedges [18,20] have
shown great capabilities in solving complex problems in the biomedical field,
these tend to be black-box in nature. a key consideration in cancer immunology
is the need for actionable insights into the spatial relationships between
different types of immune cells. not only does triangil provide predictions that
are on par or superior compared to dl approaches, but also provides a way to
glean insights into the spatial interplay of different immune cell types. these
complex interactions enhance our understanding of the tme and will help pave the
way for new therapeutic strategies that leverage these insights.",6
1499,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.1,Dataset,"the cohort employed in this study was composed of pre-treatment tumor biopsy
specimens from patients with nsclc from five centers (two centers for training
(s t ) and three centers for independent validation (s v )). the entire analysis
was carried out using 122 patients in experiment 1 (73 in s t , and 49 in s v )
and 135 patients in experiment 2 (81 in s t , and 54 in s v ). specimens were
analyzed with a multiplexed quantitative immunofluorescence (qif) panel using
the method described in [22]. from each whole slide image, 7 representative
tiles were obtained and used to train the software inform to define background,
tumor and stromal compartments. then, individual cells were segmented based on
nuclear dapi staining and the segmentation performance was controlled by direct
visualization of samples by a trained observer. next, the software was trained
to identify cell subtypes based on marker expression (cd8, cd4, cd20, ck for
tumor epithelial cells and absence of these markers for stromal cells).",6
1501,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,TIL density (DenTIL):,"for every patient, multiple density measures including the number of different
cells types and their ratios are calculated [3,24] (supplemental table 2).",6
1502,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,"GG: A Delaunay triangulation, a Voronoi diagram, and a Minimum Spanning","tree were constructed [2,14] on all nuclei regardless of their type.
architectural features (e.g., perimeter, triangle area, edge length) were then
calculated on these global graphs for each patient.",6
1503,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,CCG:,"for every patient, subgraphs are built on nuclei regardless of their type and
only based on their euclidean distance. local graph metrics (e.g. clustering
coefficient) [16] are then calculated from these subgraphs. spatil: for each
patient, first, subgraphs are built on individual cell types based on a distance
parameter. the convex hulls are then constructed on these subgraphs. after
selecting every two cell types, features are extracted from their convex hulls
(e.g. the number of clusters of each cell type, area intersected between
clusters [9]; complete list of combinations in supplemental table 3).",6
1505,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.3,Experiment 1: Immunotherapy Response Prediction in Lung Cancer,"design: triangil was also trained to differentiate between patients who
responded to io and those who did not. for our study, the responders to io were
identified as those patients with complete response, partial response, and
stable disease, and non-responders were patients with progressive disease. a
linear discriminant analysis (lda) classifier was trained on s t to predict
which patients would respond to io. for creating the model, the minimum
redundancy maximum relevance (mrmr) method [1] was used to select the top
features. the same procedure using mrmr and lda was performed for the
comparative hand-crafted approaches. the ability to identify responders post-io
was assessed by the area under the receiver operating characteristic curve (auc)
in s v .",6
1506,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,Results:,"the two top predictive triangil features were found to be the number of edges
between stroma and cd4+ cells, and the number of edges between stroma and tumor
cells with more interactions between stromal cells and both cd4+ and tumor cells
being associated with response to io. this finding is concordant with other
studies [13,17,22,27] that stromal tils were significantly associated with
improved os. therefore, triangil approach is not only predictive of treatment
response but more critically it enables biological interpretations that a dl
model might not be able to provide. in s v , this lda classifier was able to
distinguish responders from non-responders to io with au c t ri =0. design: s t
was used to construct a least absolute shrinkage and selection operator (lasso)
[28] regularized cox proportional hazards model [6] using the triangil features,
to obtain risk score for each patient. lasso features are listed in supplemental
table 4. the median risk score in s t was used as a threshold in both s t and s
v to dichotomize patients into low-risk/high-risk categories. kaplan-meier (km)
survival curves [26] were plotted and the model performance was summarized by
hazard ratio (hr), with corresponding (95% confidence intervals (ci)) using the
log-rank test, and harrell's concordance index (c-index) on s v . the c-index
evaluates the correlation between risk predictions and survival times, aiming to
maximize the discrimination between high-risk and low-risk patients [11]. os is
the time between the initiation of io to the death of the patient. the patients
were censored if the date of death was unknown.result: figure 2 presents some
triangil features in a field of view for a patient with long-term survival and
another with short-term survival. more triangular relationships, shorter
triangle edges, and smaller triangles with smaller perimeters are found in the
long-term survival case when analyzing the triadic interactions within
tumor-stroma-cd4, thereby suggesting higher relative presence and closer
interaction of these cell families. figure 3 illustrates the km plots for the
six approaches. we also calculated the concordance index (c-index) for the two
prognostic approaches in s v . the c-index for triangil and gnn methods were
0.64, and 0.63 respectively. therefore, overall triangil worked marginally
better than gnn, with much higher biological interpretability.",6
1507,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,5.0,Concluding Remarks,"we presented a new approach, triangular analysis of geographical interplay of
lymphocytes (triangil), to quantitatively chartacterize the spatial arrangement
and relative geographical interplay of multiple cell families across
pathological images. compared to previous spatial graph-based methods, triangil
quantifies the spatial interplay between multiple cell families, providing a
more comprehensive portrait of the tumor microenvironment. triangil was
predictive of response after io (n = 122) and also demonstrated a strong
correlation with os in nsclc patients treated with io (n = 135). triangil
outperformed other graph-and dl-based approaches, with the added benefit of
provoding interpretability with regard to the spatial interplay between cell
families. for instance, triangil yielded the insight that more interactions
between stromal cells and both cd4+ and tumor cells appears to be associated
with better response to io. although five cell families were studies in this
work, triangil is flexible and could include other cell types (e.g.,
macrophages). future work will entail larger validation studies and also
evaluation on other use cases.",6
1633,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,3.0,Experiments,"image dataset. we construct a clinical thyroid cytopathology dataset with images
of both image-wise and pixel-wise labels as a benchmark (appear in github upon
acceptance) some representative images are presented in fig. 2, together with
the profile of the dataset. the dataset comprises 4,965 h&e stained image
patches and labels of tbsrtc, where a subset of 1,473 images was densely
annotated for nuclei boundaries by three experienced cytopathologists and
reached a total number of 31,064 elaborately annotated nuclei. patient-level
images were partitioned first for training and test images, and patch-level
curation was performed. we divided the dataset with image-wise labels into 80%
training samples and the remaining 20% testing samples. our collection of
thyroid cytopathology images was granted with an ethics approval document. table
1. quantitative comparisons in both fully-supervised and semi-supervised
manners. the best performance is highlighted in bold, where we can observe that
both tcsegnet and its semi-supervised extension outperform state-of-the-art.",6
1684,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,2.2,Experimental Setting,"as experimental architecture, use the dual-stream mil approach proposed by li et
al [10]. since this model combines both, embedding-based and an instance-based
encoding, the effect of both paths can be individually investigated without
changing any other architectural details. since the method represents a
state-of-the-art approach, it further serves as well-performing baseline. in
instance-based mil, the information per patch is first condensed to a single
scalar value, representing the classification per patch. finally, all of these
patch-based values are aggregated. in embedding-based mil, the information per
patch is translated into a feature vector. all feature vectors from a wsi are
then aggregated followed by a classification. in the investigated model [10] an
instance-and an embedding-based pathway are employed in parallel and are merged
in the end by weighted addition. the embedding-based pathway contains an
attention mechanism, to higher weight patches that are similar to the so-called
critical instance. the model makes use of an individual feature extraction
stage. due to the limited number of wsis, we did not train the feature
extraction stage [7], but utilize a pre-trained network instead. specifically,
we applied a resnet18 pre-trained on the image-net challenge data, due to the
high performance in previous work on similar data [5]. resnet18 was assessed as
particularly appropriate due to the rather low dimensional output (512
dimensions). we actively decided not to use a self-supervised contrastive
learning approach [10] as feature extraction stage since invariant features
could interfere with the effect of data augmentation. we investigated various
settings consisting of instancebased only (inst), embedding-based only (emb) and
the dual-stream approach with weightings 3/1, 2/2 (balanced) and 1/3 for the
instance and the embedding-based pathways.as comparison, several other
augmentation methods on feature level are investigated including random
sampling, selective random sampling and random noise. random sampling
corresponds to the random selection of patches (feature vectors) from each wsi.
thereby the amount of investigated data per wsi is reduced with the benefit of
increasing the variability of the data. in the experiments, we adjust the sample
ratio q between the patch-based features for training and testing. a q of 50 %
indicates that 512 descriptors are used for training while for testing always a
fixed number of 1024 is used. selective random sampling corresponds to the
random sampling strategy, with the difference that the ratio of features is not
fixed but drawn from a uniform random distribution (u (q, 100 %)). here, a q of
50 % indicates that for each wsi, between 512 and 1024 feature vectors are
selected. in the case of the random noise setting, to each feature vector x i ,
a random noise vector r is added (x i = x i + r). the elements of r are randomly
sampled (individually for each x i ) from a normal distribution n (0, σ ).to
incorporate for the fact that the feature dimensions show different magnitudes,
σ is computed as the product of the meta parameter σ and the standard deviation
of the respective feature dimension.in this work, we aimed at distinguishing
different nodular lesions of the thyroid, focusing especially on benign
follicular nodules (fn) and papillary carcinomas (pc). this differentiation is
crucial, due to the different treatment options, in particular with respect to
the extent of surgical resection of the thyroid gland [19]. the data set
utilized in the experiments consists of 80 wsis overall. one half (40) of the
data set consists of frozen and the other half (40) of paraffin sections [5]),
representing the different modalities. all images were acquired during clinical
routine at the kardinal schwarzenberg hospital. procedures were approved by the
ethics committee of the county of salzburg (no. 1088/2021). the mean and median
age of patients at the date of dissection was 47 and 50 years, respectively. the
data set comprised 13 male and 27 female patients, corresponding to a slight
gender imbalance. they were labeled by an expert pathologist with over 20 years
experience. a total of 42 (21 per modality) slides were labeled as papillary
carcinoma while 38 (19 per modality) were labeled as benign follicular nodule.
for the frozen sections, fresh tissue was frozen at -15 • celsius, slides were
cut (thickness 5 µm) and stained immediately with hematoxylin and eosin. for the
paraffin sections, tissue was fixed in 4 % phosphate-buffered formalin for 24 h.
subsequently formalin fixed paraffin embedded tissue was cut (thickness 2 µm)
and stained with hematoxylin and eosin. the images were digitized with an
olympus vs120-ld100 slide loader system. overviews at a 2x magnification were
generated to manually define scan areas, focus points were automatically defined
and adapted if needed. scans were performed with a 20x objective (corresponding
to a resolution of 344.57 nm/pixel). the image files were stored in the oympus
vsi format based on lossless compression. q q q q q q q q 0 25 % 50 % 75 % 100 %
0.4 the data set was randomly separated into training (80 %) and test data (20
%). the whole pipeline, including the separation, was repeated 32 times to
achieve representative scores. due to the almost balanced setting, the overall
classification accuracy (mean and standard deviation) is finally reported. adam
was used as optimizer. the models were trained for 200 epochs with an initial
learning rate of 0.0002. random shuffling of the vector tupels (shuffling within
the wsis) was applied for all experiments.the patches were randomly extracted
from the wsi, based on uniform sampling. for each patch, we checked that at
least 75 % of the area was covered with tissue (green color channel) in order to
exclude empty areas [5]. to obtain a representation independent of the wsi size,
we extracted 1024 patches with a size of 256 × 256 pixel per wsi, resulting in
1024 patch-descriptors per wsi [5]. for feature extraction, a resnet18 network,
pretrained on the image-net challenge was deployed [10]. data and source code
are publicly accessible via https://gitlab.com/mgadermayr/mixupmil. we use the
reference implementation of the dual-stream mil approach [10]. to obtain further
insight into the feature distribution, we randomly selected patch descriptor
pairs and computed the euclidean distances. in detail, we selected 10,000 pairs
(a) from different classes, (b) from different wsis (similar and dissimilar
classes), (c,d) from the same class and different wsis, and (e) from the same
wsi.",6
1696,Gene-Induced Multimodal Pre-training for Image-Omic Classification,1.0,Introduction,"pathological image-omic analysis is the cornerstone of modern medicine and
demonstrates promise in a variety of different tasks such as cancer diagnosis
and prognosis [12]. with the recent advance of digital pathology and sequencing
technologies, modern cancer screening has jointly incorporated genomics and
histology analysis of whole slide images (wsis).though deep learning techniques
have revolutionized medical imaging, designing a task-specific algorithm for
image-omic multi-modality analysis is challenging. (1) the gigapixel wsis, which
generally yield 15,000 foreground patches during pre-processing, make
attention-based backbones [6] hard to extract precise image (wsi)-level
representations. (2) learning features from genomics data which have tens of
thousands of genes make models such as transformer [16] impractical to use due
to its quadratic computation complexity. (3) image-omic feature fusion [2,3] may
fail to model high-order relevance and the inherent structural characteristics
of each modality, making the fusion less effective.specifically, to our
knowledge, most multi-modality techniques have been designed for modalities such
as chest x-ray and reports [1,17,23], ct and x-ray [18], ct and mri [21], h&e
cross-staining [22] via global feature, local feature or multi-granularity
alignment. but, none of these works considers the challenges in wsis and genes
processing. besides, vision-language models in the computer vision community
stand out for their remarkable versatility [13,14]. nevertheless, constrained by
computing resources, the most commonly used multimodal representation learning
strategy, contrastive learning, which relies on a large number of negative
samples to avoid model collapse [8], is not affordable for gigapixel wsis
analysis. a big domain gap also hampers their usage in leveraging the structural
characteristic of tumor micro-environment and genomic assay. recently, the
literature corpus has proposed some methods for accomplishing specific
image-omic tasks via kronecker product fusion [2] or co-attention mapping
between wsis and genomics data [3]. but, the kronecker product overly concerns
feature interactions between modalities while ignoring high-order relevance,
w.r.t. decision boundaries across multiple samples, which is critical to
classification tasks. as for the co-attention module, it is unidirectional and
cannot localize significant regions from genetic data with a large amount of
information.in this paper, we propose a task-specific framework dubbed
gene-induced multimodal pre-training (gimp) for image-omic classification.
concretely, we first propose a transformer-based gene encoder, group multi-head
self attention (groupmsa), to capture global structured features in gene
expression cohorts. next, we design a pre-training paradigm for wsis, masked
patch modeling (mpm), masking random patch embeddings from a fixed-length
contiguous subsequence of a wsi. we assume that one patch-level feature
embedding can be reconstructed by its adjacent patches, and this process
enhances the learning ability for pathological characteristics of different
tissues. our mpm only needs to recover the masked patch embeddings in a
fixed-length subsequence rather than processing all patches from wsis.
furthermore, to model the high-order relevance of the two modalities, we combine
cls tokens of paired image and genomic data to form unified representations and
propose a triplet learning module to differentiate patient-level positive and
negative samples in a mini-batch. it is worth mentioning that although our
unified representation fuses features from the whole gene expression cohort and
partial wsis in a mini-batch, we can still learn high-order relevance and
discriminative patient-level information between these two modalities in
pre-training thanks to the triplet learning module. in addition, note that our
proposed method is different from self-supervised pre-training. specifically, we
focus not only on superior representation learning capability, but also
category-related feature distributions, w.r.t. intra-and inter-class variation.
with the training process going on, complete information from wsis can be
integrated and the fused multimodal representations with high discrimination
will make it easier for the classifier to find the classification hyperplane.
experimental results demonstrate that our gimp achieves significant improvement
in accuracy than other image-omic competitors, and our multimodal framework
shows competitive performance even without pre-training.",6
1700,Gene-Induced Multimodal Pre-training for Image-Omic Classification,2.3,Gene-Induced Multimodal Fusion,"in this section, we first describe the formulation of masked patch modeling.
then we introduce the overall pipeline of our pre-training framework and
illustrate how to apply it to downstream classification tasks.masked patch
modeling. in wsis, the foreground patches are spatially contiguous, which means
the adjacent patches have similar feature embeddings. thus, we propose a masked
patch modeling (mpm) pre-training strategy that masks random patch embeddings
from a fixed-length contiguous subsequencej=i in h p and reconstruct the
invisible information. the fixed subsequence length l is empirically set to
6,000 and the sequences shorter than l are duplicated to build mini batches.
besides, the masking ratio is set to 50% and the set of masked subscripts is
denoted as m ∈ r 0.5l . next, a two-layer nystrom-based patch aggregator
followed by a lightweight reconstruction decoder are adopted to process the
masked sequence h mpm and the reconstructed sequence is denoted as. note that we
reconstruct the missing feature embeddings rather than the raw pixels of the
masked areas, which is different from traditional mim methods like simmim [19]
and mae [5]. in this way, the model could consider latent pathological
characteristics of different tissues, which makes the pretext task more
challenging. the reconstruction l 1 loss is computed by:where 1[•] is the
indicator function.gene-induced triplet learning. the transformer-based
backbones in the classification task require the cls token to be able to extract
accurate global information, which is even more important yet difficult in wsis
due to the long sequence challenge. in addition, in order to construct the
mini-batch, the subsequences we intercept in the mpm pre-training phase may not
be sufficiently representative of the image-level characteristics. to overcome
these issues, we further propose a gene-induced triplet learning module, which
uses pathological images and genomic data as input and extracts high-order and
discriminative features via cls tokens. firstly, we pre-train the groupmsa
module by patientlevel annotations in advance and froze it in the following
iterations. next, a learnable cls token cls img for wsis is added to the input
masked sequence h mpm . after extracting the input patch embeddings and gene
sequence separately, we concatenate cls img and cls ge as cls pat ∈ r 2d to
represent patient-level characteristics.suppose we obtain a triplet list {x, x +
, x -} during current iteration, where x, x + , x -are concatenated tokens of
anchor cls pat , positive cls pat , and negative cls pat , respectively. to
enhance the global modeling capability, i.e., extracting more precise
patient-level features, we expect that the distance between the anchor and the
positive sample gets closer, while the negative sample is farther away. the loss
function for optimizing triplet learning is computed by:δ indicates a threshold,
e.g., δ = 0.8. finally, the loss function for gimp pretraining is:multimodal
fine-tuning. applying the pre-trained backbone to image-omic classification task
is straightforward, since gimp pre-training allows it to learn representative
patient-level features. we use a simple multi-layer perceptron (mlp) head to map
cls pat to the final class predictions p , which can be written as p =
softmax(mlp(cls pat )).3 experiments",6
1701,Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.1,Experimental Setup,"datasets. we verify the effectiveness of our method on the caner genome atlas
(tcga) non-small cell lung cancer (nsclc) dataset, which contains two cancer
subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adenocarcinoma
(luad). after pre-processing [11], the patch number extracted from wsis at 20×
magnification varies from 485 to 148,569. we collect corresponding rna-seq fpkm
data for each patient and the length of the input genomic sequence is 60,480.
among 946 image-omic pairs, 470 of them belong to luad and 476 cases are lusc.
we randomly split the data into 567 for training, 189 for validation and 190 for
testing.implementation details. the pre-training process of all algorithms is
conducted on the training set, without any extra data augmentation. note that
our genetic encoder, groupmsa, is fully supervised pre-trained on unimodal
genetic data to accelerate convergence and it is frozen during gimp training
process. the maximum pre-training epoch for all methods is set to 100 and we
finetune the models at the last epoch. during fine-tuning, we evaluate the model
on the validation set after every epoch, and save the parameters when it
performs the best. adamw [10] is used as our optimizer and the learning rate is
10 -4 with cosine decline strategy. the maximum number of fine-tune epoch is 70.
at last, we measure the performance on the test set. training configurations are
consistent throughout the fine-tuning process to ensure fair comparisons. all
experiments are conducted on a single nvidia geforce rtx 3090.",6
1702,Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.2,Comparison Between GiMP and Other Methods,"we conduct comparisons between gimp and three competitors under different
settings. firstly, we compare our proposed patch aggregator with the current
state-of-the-art deep mil models on unimodal tcga-nsclc dataset, i.e., only
pathological wsis are included as input. as shown in table 1, our proposed patch
aggregator outperforms all the compared attention based multiple instance
learning baselines in classification accuracy. in particular, 1.6% higher than
the abmil [6] 0.7737 dsmil [9] 0.7566 clam-sb [11] 0.8519 clam-mb [11] 0.8889
transmil [15] 0.8836 pathology w/o pre-train gimp (w/o groupmsa) 0.8995 porpoise
[4] 0.9524 pathomic fusion [2] 0.9684 mcat [3] 0.9632 w/o pre-train gimp (ours)
0.9737 mgca [17] 0.9105 biovil [1] 0.9316 refers [23] 0 second best compared
method transmil [15]. we then explore the superiority of gimp by comparing to
state-of-the-art medical multi-modal approaches. we particularly compare our
method to biovil [1], mgca [17] and refers [23], three popular multimodal
pre-training algorithms in medical text-image classification task. we can
observe in the table that, our gimp raises acc from 91.05% to 99.47% on
tcga-nsclc dataset. even without pre-training stage, gimp shows competitive
performance compared to porpoise [4], pathomic fusion [2], and mcat [3], three
influential image-omic classification architectures. we further explore why gimp
works by insightful interpretation of the proposed method with t-sne
visualisation. figure 2 shows the feature mixtureness of pre-trained cls pat
extracting global information on training set. compari- son between fig. 2 (a)
and (b) indicates that the addition of the genomic data is indispensable in
increasing the inter-class distance and reducing the intra-class distance, which
confirms our motivation that gene-induced multimodal fusion could model
high-order relevance and yield more discriminative representations. moreover,
compared to the mentioned self-supervised methods biovil [1] and mgca [17] in
fig. 2 (c) and (d), cls pat with gimp pre-trained are well separated between
luad and lusc, i.e., gimp pays more attention to the categoryrelated feature
distribution and could extract more discriminative patient-level features during
triplet learning.",6
1703,Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.3,Ablation Study,"table 2 summarizes the results of ablation study. we first evaluate the
effectiveness of the proposed groupmsa. in the first two rows, groupmsa achieves
0.53% improvement compared to snn [7], a popular genetic encoders used in
porpoise [4] and pathomic fusion [2]. we then analyze the effect of adding
genetic modality during pre-training. the evaluation protocol is first
pre-training, and then fine-tuning on downstream multimodal classification task.
""aggregator + mpm"" means gimp only uses wsis as input and reconstructs the
missing patch embeddings during the pre-training phase. since the fixed
subsequence length l = 6000 is used in our setting, it is sometimes smaller than
the original patch number, e.g., the maximum size 148,569, the pre-trained model
without genetic guidance may be not aware of sufficiently accurate patientlevel
characteristics, i.e., ineffectively focused on normal tissues. ""aggregator +
triplet"" indicates using unimodal image features to build triplets. we can
likewise find that the lack of precise global representation leads to worse
performance. finally, we evaluate the necessity of the mpm module. ""aggregator +
groupmsa + triplet"" means gimp only combines the cls tokens of each modality and
calculates triplet loss during pre-training. we can observe a performance drop
without mpm module, e.g., from 99.47% to 95.26%, which demonstrates that local
pathological information is equally critical as high-order relevance.",6
1710,Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.1,Datasets,"we tested our proposed method on two different tasks: (1) intrahepatic
cholangiocarcinomas(ihccs) subtype classification and (2) liver cancer type
classification. the dataset for the former task was collected from 168 patients
with 332 wsis from seoul national university hospital. ihccs can be further
categorized into small duct type (sdt) and large duct type (ldt). using gene
mutation information as prior knowledge, we collected wsis with wild kras and
mutated idh genes for use as training samples in sdt, and wsis with mutated kras
and wild idh genes for use in ldt. the rest of the wsis were used as testing
samples. the liver cancer dataset for the latter task was composed of 323 wsis,
in which the wsis can be further classified into hepatocellular carcinomas
(hccs) (collected from pathology ai platform [1]) and ihccs. we collected 121
wsis for the training set, and the remaining wsis were used as the testing set.",6
1879,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,1.0,Introduction,"breast cancer is the most common cancer and the leading cause of cancer death in
women [18]. early detection of breast cancer allows patients to receive timely
treatment, which may have less burden and a higher probability of survival [6].
among current clinical imaging modalities, magnetic resonance imaging (mri) has
the highest sensitivity for breast cancer detection [12]. especially,
contrastenhanced mri (ce-mri) can identify tumors well and has become an
indispensable technique for detecting and defining cancer [13]. however, the use
of gadolinium-based contrast agents (gbca) requires iv-cannulation, which is a
burden to patients, time consuming and cumbersome in a screening situation.
moreover, contrast administration can lead to allergic reactions and finaly
ce-mri may be associated with nephrogenic systemic fibrosis and lead to
bioaccumulation in the brain, posing a potential risk to human health
[4,9,[14][15][16]. in 2017, the european medicines agency concluded its review
of gbca, confirming recommendations to restrict the use of certain linear gbca
used in mri body scans and to suspend the authorization of other contrast
agents, albeit macrocyclic agents can still be freely used [10].with the
development of computer technology, artificial intelligence-based methods have
shown potential in image generation and have received extensive attention. some
studies have shown that some generative models can effectively perform mutual
synthesis between mr, ct, and pet [19]. among them, synthesis of ce-mri is very
important as mentioned above, but few studies have been done by researchers in
this area due to its challenging nature. li et al. analyzed and studied the
feasibility of using t1-weighted mri and t2-weighted mri to synthesize ce-mri
based on deep learning model [11]. their results showed that the model they
developed could potentially synthesize ce-mri and outperform other cohort
models. however, mri source data of too few sequences (only t1 and t2) may not
provide enough valuable informative to effectively synthesize ce-mri. in another
study, chung et al. investigated the feasibility of using deep learning (a
simple u-net structure) to simulate contrast-enhanced breast mri of invasive
breast cancer, using source data including t1-weighted non-fatsuppressed mri,
t1-weighted fat-suppressed mri, t2-weighted fat-suppressed mri, dwi, and
apparent diffusion coefficient [5]. however, obtaining a complete mri sequence
makes the examination costly and time-consuming. on the other hand, the
information provided by multi-sequences may be redundant and may not contain the
relevant information of ce-mri. therefore, it is necessary to focus on the most
promising sequences to synthesize ce-mri.diffusion-weighted imaging (dwi) is
emerging as a key imaging technique to complement breast ce-mri [3]. dwi can
provide information on cell density and tissue microstructure based on the
diffusion of tissue water. studies have shown that dwi could be used to detect
lesions, distinguish malignant from benign breast lesions, predict patient
prognosis, etc [1,3,7,8,17]. in particular, dwi can capture the dynamic
diffusion state of water molecules to estimate the vascular distribution in
tissues, which is closely related to the contrast-enhanced regions in ce-mri.
dwi may be a valuable alternative in breast cancer detection in patients with
contraindications to gbca [3]. inspired by this, we develop a multi-sequence
fusion network based on t1-weighted mri and multi-b-value dwi to synthesize
ce-mri. our contributions are as follows:i from the perspective of method, we
innovatively proposed a multi-sequence fusion model, designed for combining
t1-weighted imaging and multi-b-value dwi to synthesize ce-mri for the first
time. ii we invented hierarchical fusion module, weighted difference module and
multi-sequence attention module to enhance the fusion at different scale, to
control the contribution of different sequence and maximising the usage of the
information within and across sequences. iii from the perspective of clinical
application, our proposed model can be used to synthesize ce-mri, which is
expected to reduce the use of gbca.",7
1880,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.1,Patient Collection and Pre-processing,"this study was approved by institutional review board of our cancer institute
with a waiver of informed consent. we retrospectively collected 765 patients
with breast cancer presenting at our cancer institute from january 2015 to
november 2020, all patients had biopsy-proven breast cancers (all cancers
included in this study were invasive breast cancers, and ductal carcinoma in
situ had been excluded). the mris were acquired with philips ingenia all mris
were resampled to 1 mm isotropic voxels and uniformly sized, resulting in
volumes of 352 × 352 pixel images with 176 slices per mri, and subsequent
registration was performed based on advanced normalization tools (ants) [2].",7
1887,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,4.0,Conclusion,"we have developed a multi-sequence fusion network based on multi-b-value dwi to
synthesize ce-mri, using source data including dwis and t1-weighted
fatsuppressed mri. compared to existing methods, we avoid the challenges of
using full-sequence mri and aim to be selective on valuable source data dwi.
hierarchical fusion generation module, weighted difference module, and
multisequence attention module have all been shown to improve the performance of
synthesizing target images by addressing the problems of synthesis at different
scales, leveraging differentiable information within and across sequences. given
that current research on synthetic ce-mri is relatively sparse and challenging,
our study provides a novel approach that may be instructive for future research
based on dwis. our further work will be to conduct reader studies to verify the
clinical value of our research in downstream applications, such as helping
radiologists on detecting tumors. in addition, synthesizing dynamic
contrastenhanced mri at multiple time points will also be our future research
direction. our proposed model can potentially be used to synthesize ce-mri,
which is expected to reduce or avoid the use of gbca, thereby optimizing
logistics and minimizing potential risks to patients.",7
2003,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,1.0,Introduction,"machine learning (ml), specifically deep learning (dl), algorithms have shown
exceptional performance on numerous medical image analysis tasks [2].
never-theless, comprehensive reviews highlight major issues of generalizability,
robustness, and reproducibility in medical imaging ai/ml [9,15]. for a
generalizability assessment, reporting only aggregate performance measures is
not sufficient. due to model complexity and limited training data, ml
performance often varies across data subgroups or domains, such as different
patient subpopulations or varied data acquisition scenarios. aggregate
performance measures (e.g., sensitivity, specificity, roc auc) can be dominated
by the larger subgroups, masking the poor ml model performance on smaller but
clinically important subgroups [11]. thus, achieving (through training) and
demonstrating (as part of testing) satisfactory ml model performance across
relevant subgroups is crucial before the real-world clinical deployment of a
medical ml system [13].however, a challenging situation arises when relevant
subgroups are unrecognized. one solution to this issue is to apply a clustering
algorithm to the data, with the goal of identifying the unannotated subgroups.
the main objective of unsupervised clustering is to group data points into
distinct classes of similar traits. however, due to the complexity and high
dimensionality of the medical imaging data and the resulting difficulty in
establishing a concrete notion of similarity, extracting low-dimensional
characteristics becomes the key to establishing the best criteria for grouping.
unsupervised generative clustering aims to simultaneously address both domain
identification and dimensionality reduction. deep unsupervised clustering
algorithms could map the medical imaging data back to their causal factors or
underlying domains, such as image acquisition equipment, patient subpopulations,
or other meaningful data subgroups. however, there is a practical need to be
able to guide the deep clustering model towards the identification of grouping
structures in a given dataset that have not been already annotated. to that end,
we propose a mechanism that is intended to constrain the model towards
identifying clusters in the data that are not associated with given variables of
choice (already known class labels or subgroup structures). the resulting
algorithmic cluster assignments could then be used to improve ml algorithm
training, or for generalizability and robustness evaluation.",8
2013,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,1.0,Introduction,"brachial plexopathy is a form of peripheral neuropathy [1]. it occurs when there
is damage to the brachial plexus (bp) which is a complex nerve network under the
skin of the shoulder. there is a wide range of disease that may cause a brachial
plexopathy.radiation fibrosis, primary and metastatic lung cancer, and
metastatic breast cancer account for almost three-fourths of causes [2].
brachial plexus syndrome occurs not infrequently in patients with malignant
disease. it is due to compression or direct invasion of the nerves by tumor
which will bring many serious symptoms [3]. our research focuses on the brachial
plexopathy caused by metastatic breast cancers.magnetic resonance imaging (mri)
and ultrasound of the brachial plexus have become two reliable diagnostic tools
for brachial plexopathy [4]. automatic identification of the bp in mri and
ultrasound images has become a hot topic. currently, most of relevant research
in this field are focusing on ultrasound modality [5][6][7][8]. compared with
ultrasound, mri has become the primary imaging technique in the evaluation of
brachial plexus pathology [9]. however, to our knowledge, radiomics related bp
studies utilizing mri have not been reported previously.many radiomics studies
have experimentally demonstrated that image texture has great potential for
differentiation of different tissue types and pathologies [10]. in the past
several decades, many state-of-the-art methods have been proposed to extract
texture patterns [11,12]. however, how to most effectively combine texture
features with deep learning, called deep texture, is still an open area of
research. one prior approach, termed glcm-cnn, was proposed to carry out a polyp
differentiation task [13]. however, how to arrange these glcms to form the 3d
volume to optimize the performance is a major challenge.with the goal of
classifying normal from abnormal bp, we explored the approach of deep texture
learning. this paper constructed a bp dataset with the most commonly used bp
mris in our clinical practice. considering the shortcoming of traditional
patterns, triple point pattern (tpp) is proposed for the quantitative
representation of the heterogeneity of abnormal bp's. in contrast to glcm-cnn,
tppnet is designed to train models by feeding tpp matrices as the input with a
huge number of channels. finally, we analyze the model's performance in the
experimental section. the major contributions of this study include 1) directed
triangle construction idea for tpp, 2) huge number of tpp matrices as the
heterogeneity representations of bp, 3) tppnet with 15 layers and huge number of
channels, 4) the bp dataset containing mr images and their corresponding roi
masks.",8
2014,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.1,Dataset Preparation and Preprocessing,"following irb approval for this study, we search for patients with metastatic
breast cancer who had a breast cancer mri performed between 2010 and 2020 and
had morphologically positive bp on the mri report from our electronic medical
records (emr) in * hospital. totally, we collect approximate 807 series which
include 274 t2, 254 t1 and 279 post-gadolinium. since some scans are seriously
degraded due to motion artifacts. therefore, each case underwent several
essential image adjustments such as multi-series splitting, two-series merging,
slice swapping, artifact checking and boundary corrections. to yield the roi,
firstly, we randomly sampled -40% of the sequences including both normal and
abnormal ones that were manually segmented with itk-snap by two skilled trainees
[14,15]. then, the manual segmentations were utilized to train a 3d nnunet model
which was utilized to train the model which was used to predict rois for the
rest series [16]. the predicted segmentations were manually divided into three
groups, i.e. good, fair and poor. good cases were added to the training set.
this process was repeated until no improvements in the predictions for the
remaining sequences was seen. the final dataset for radiomic analysis was
constructed by merging the datasets for each sequence type. only patients that
had all three sequences segmented (t2, t1 and post-gadolinium) were included in
the dataset. table 1 shows a breakdown of the final dataset.",8
2065,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.1,Datasets and Implementation Details,"consep 1 [10] is a colorectal nuclear dataset with three types, consisting of 41
h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide images
(wsis). the wsis are at 20× magnification and the size of the slides is 500 ×
500. we split them following the official partition [1,10].is a breast cancer
dataset with three types and consists of 120 image tiles from 113 patients. the
wsis are at 20× magnification and the size of the slides ranges from 465 × 465
to 504 × 504. we follow the work [1] to apply the slic [2] algorithm to generate
superpixels as instances and split them into 80/10/30 slides for
training/validation/testing.lizard 3 [9] has 291 histology images of colon
tissue from six datasets, containing nearly half a million labeled nuclei in h&e
stained colon tissue. the wsis are at 20× magnification with an average size of
1,016 × 917 pixels. our implementation and the setting of hyper-parameters are
based on mmdetection [5]. the number of grouping prompts g is 64. random crop,
flipping, and scaling are used for data augmentation. our method is trained with
pytorch on a 48 gb gpu (nvidia a100) for 12-24 h (depending on the dataset
size). more details are listed in the supplementary material.",8
2105,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.3,In Vivo Data: Neurite Morphology,"figure 4(a) shows the relaxation times of the restricted compartment in white
matter lesions, indicating that relaxation times are longer in gliomas than
normal white matter tissue. the higher t 2 in grade 4 glioma is associated with
changes in metabolite compositions, resulting in remarkable changes in neurite
morphology in lesioned tissues (fig. 4(c-d)), consistent with previous
observations [12,23]. the rate of longitudinal relaxation time has been shown to
be positively correlated with myelin content. our results indicate that mte dmri
is more sensitive to neurite morphology than ste dmri (fig. 4(b)).figures 4(c-d)
show that the estimated mean nr in the gray matter is approximately in the range
of 10 µm, which is in good agreement with the sizes of somas in human brains,
i.e., 11 ± 7 µm [26]. rdsi improves the detection of small metastases,
delineation of tumor extent, and characterization of the intratumoral
microenvironment when compared to conventional microstructure models (fig.
4(c)). our studies suggest that rdsi provides useful information on
microvascularity and necrosis helpful for facilitating early stratification of
patients with gliomas (fig. 4(d)).",8
2123,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,1.0,Introduction,"diffusion-weighted mri enables visualization of brain white matter structures.
it can be used to generate tractography data consisting of millions of synthetic
fibers or streamlines for a single subject stored in a tractogram that
approximate groups of biological axons [1]. many applications require
streamlines to be segmented into individual tracts corresponding to known
anatomy. tract segmentations are used for a variety of tasks, including surgery
planning or tract-specific analysis of psychiatric and neurodegenerative
diseases [2,11,12,17].automated methods built on supervised machine learning
algorithms have attained the current state-of-the-art in segmenting tracts
[3,18,21]. those are trained using various features, either directly from
diffusion data in voxel space or from tractography data. models may output
binary masks containing the target white matter tract, or perform a
classification on streamline level. however, such algorithms are commonly
trained on healthy subjects and have shown issues in processing cases with
anatomical abnormalities, e.g. brain tumors [20]. consequently, they are
unsuitable for tasks such as preoperative planning of neurosurgical patients, as
they may produce incomplete or false segmentations, which could have harmful
consequences during surgery [19]. additionally, supervised techniques are
restricted to fixed sets of predetermined tracts and are trained on substantial
volumes of hard-to-generate pre-annotated reference data.manual methods are
still frequently used for all cases not yet covered by automatic methods, such
as certain populations like children, animal species, new acquisition schemes or
special tracts of interests. experts determine regions of interest (roi) in
areas where a particular tract is supposed to traverse or through which it must
not pass, and segmentations can be accomplished either (1) by virtually
excluding and maintaining streamlines from tractography according to the defined
roi or (2) by using these regions for tract-specific roi-based tractography.
both approaches require a similar effort, although the latter is more commonly
used. the correct definition of rois can be time-consuming and challenging,
especially for inexperienced users. despite these limitations, roi-based
techniques are currently without vivid alternatives for segmenting tracts that
automated methods cannot handle.methods to simplify tract segmentation have been
proposed before. clustering approaches were developed to reduce complexity of
large amounts of streamlines in the input data [4,6]. tractome is a tool that
allows interactive segmentation of such clusters by representing them as single
streamlines that can interactively be included or excluded from the target tract
[14]. although the approach has shown promise, it has not yet supplanted
conventional roibased techniques.we propose a novel semi-automated tract
segmentation method for efficient and intuitive identification of arbitrary
white matter tracts. the method employs entropy-based active learning of a
random forest classifier trained on features of the dissimilarity representation
[13]. active learning has been utilized for several cases in the medical domain,
while it has never been applied in the context of tract segmentation [7,9,16].
it reduces manual efforts by iteratively identifying the most informative or
ambiguous samples, here, streamlines, during classifier training, to be
annotated by a human expert. the method is implemented as the tool attractive in
mitk diffusion1 , enabling researchers to quickly and intuitively segment tracts
in pathological datasets or other situations not covered by automatic
techniques, simply by annotating a few but informative streamlines.",8
2126,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.1,Data,"the proposed technique was tested on a healthy-subject dataset and on a dataset
containing tumor cases. the first comprises 21 subjects of the human connectome
project (hcp) that were used for testing the automated methods tractseg and
classifyber [3,18]. visual examinations revealed false-negatives in the
reference tracts, meaning that some streamlines that belong to the target tract
were not included in the reference. these false-negatives did not affect the
generation of accurate segmentation masks, since most false-negatives are
occupied by true-positive streamlines, but negatively influenced our
experiments. to reduce false-negatives, the reference segmentation mask as well
as start-and end-region segmentations were used to reassign streamlines from the
tractogram using two criteria: streamlines must be inside the binary reference
segmentation (1) and start and end in the assigned regions (2). as the initial
size of ten million streamlines is computationally challenging and unsuitable
for most tools, all tractograms were randomly down-sampled to one million
streamlines. we focused on the left optic radiation (or), the left
cortico-spinal tract (cst), and the left arcuate-fasciculus (af), representing a
variety of established tracts.to test the proposed method on pathological data,
we used an in-house dataset containing ten presurgical scans of patients with
brain tumors. tractography was performed using probabilistic streamline
tractography in mitk diffusion. to reduce computational costs, we retained one
million streamlines that passed through a manually inserted roi located in an
area traversed by the or [15]. subjects have tumor appearance with varying sizes
((17.87±12.73 cm 3 )) in temporoloccipital, temporal, and occipital regions,
that cause deformations around the or and lead to deviations of the tract from
the normative model.",8
2146,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,1.0,Introduction,"glioblastomas (gbms, known as grade iv gliomas) are the most common primary
malignant brain tumors with high spatial heterogeneity and varying degrees of
aggressiveness [22]. patients with gbm generally have a very poor survival rate;
the median overall survival time is about 14 months [17]; and the overall
survival time is affected by many factors, including patient characteristics
(e.g., age and physical status), tissue histopathology (e.g., cellular density
and nuclear atypia), and molecular pathology (e.g., mutations and gene
expression levels) [1,14,15]. although these factors, particularly molecular
information, have usually proved to be strong predictors of survival in gbm,
there remain substantial challenges and unmet clinical needs to exploit easily
accessible, noninvasive neuroimaging data acquired preoperatively to predict
overall survival time of gbm patients, which can benefit treatment planning.to
do so, magnetic resonance imaging (mri) and its derived radiomics have been
widely used to study gbm preoperative prognosis over the last few decades. for
example, anand et al. [2] first applied a forest of trees to assign an
importance value to each of the 1022 radiomic features extracted from t1 mri,
and then the 32 most important features were fed to the random forest regressor
for predicting overall survival time of a gbm patient. based on patches from
multi-modal mri images, nie et al. [19] trained a 3d convolutional neural
network (cnn) to learn the high-level semantic features, which were eventually
input to a support vector machine (svm) for classifying long-and short-term gbm
survivors. in addition, an integrated model by fusing radiomics features,
mri-based cnn features, and clinical features, was presented for gbm survival
group classification, resulting in better performance than using any single type
of features [12].although both mri and its derived radiomics features have been
demonstrated to have predictive power for survival analysis in the
aforementioned literature, they do not account for brain's functional
alternations caused by tumors, which are clinically significant as
biologically-interpretable biomarkers of recovery and therapy. these
alternations can be reflected by changes in resting-state functional mri
(fmri)-derived functional connectivities/connections (fcs) between the blood
oxygenation level-dependence (bold) time series of paired brain regions.
therefore, the use of fcs to predict overall survival time for gbm has recently
attracted increasing attention [7,16,24], and more importantly, survival-related
fc patterns or brain regions were found to guide therapeutic solutions aimed at
inhibiting tumor-brain communication.nevertheless, current fc-based survival
prediction still suffers from two main deficiencies when applied to gbm
prognosis. first, due to mass effect and physical infiltration of gbm in the
brain, fcs estimated directly from gbm patients' resting-state fmri might be
inaccurate, especially when the tumors are near or in the regions of interest.
second, resting-state fmri data are not routinely collected for gbm clinical
practices, which restricts the size of annotated datasets such that it is
infeasible to train a reliable prediction model based on deep learning for
survival prediction. in order to circumvent these issues, in this paper we
introduce a novel neuroimaging feature family, namely functional lesion network
(fln) maps that are generated by our augmented lesion network mapping (a-lnm),
for overall survival time prediction of gbm patients. our a-lnm is motivated by
lesion network mapping (lnm) [8] which can localize neurological deficits to
functional brain networks and identify regions relate to a clinical syndrome. by
embedding the lesion into a normative functional connectome and computing
functional connectivity between the lesion and the rest of the brain using fmri
of all healthy subjects in the normative cohort, lnm has been successfully
employed to the identification of the brain network underlying particular
symptoms or behavioral deficits in stoke [4,13].the details of our workflow are
described as follows.1) we first manually segment the whole tumor (regarded as
lesion in this paper) on structural mri for all gbm patients, and the resulting
lesion masks are mapped onto a reference brain template, e.g., the mni152 2mm 3
template.2) the proposed a-lnm is next used to generate fln maps for each gbm
patient by using resting-state fmri from a large cohort of healthy subjects.
specifically, for each patient, we correlate the mean bold time series of all
voxels within the lesion with the bold time series of every voxel in the whole
brain for all n subjects in the normative cohort, producing n functional
disconnection (fdc) maps of voxel-wise correlation values (transformed to
zscores). these resulting n fdc maps are partitioned into m disjoint subsets of
equal size, and m fln maps are separately obtained by averaging the fdc maps in
each of the m subsets. similar to data augmentation schemes, we can artificially
boost data volume (i.e., fln maps) up to m times through producing m fln maps
for each patient in the a-lnm, which helps to mitigate the risk of over-fitting
and improve the performance of overall survival time prediction when learning a
deep neural network from a small sized dataset.for this reason, we propose the
name ""augmented lnm (a-lnm)"", compared to the traditional lnm where only one fln
map is generated per patient by averaging all the n fdc maps. 3) finally, these
augmented fln maps are fed to a 3d resnet-based backbone network followed by the
average pooling operation and fully-connected layers for gbm survival
prediction.to our knowledge, this paper is the first to demonstrate a successful
extension of lnm for survival prediction in gbm. to evaluate the predictive
power of the fln maps generated by our a-lnm, we conduct extensive experiments
on 235 gbm patients in the training dataset of brats 2020 [18] to classify the
patients into three overall survival time groups viz. long, mid, and short.
experimental results show that our a-lnm based survival prediction framework
outperforms previous state-of-the-art methods. in addition, an explainable
analysis driven by the gradient-weighted class activation mapping (grad-cam)
[10] for survivalrelated brain regions is fulfilled.",8
2147,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.0,Materials and Methods,"2.1 materials gsp1000 processed connectome. it publicly released preprocessed
restingstate fmri data of 1000 healthy right-handed subjects with an average age
21.5 ± 2.9 years and approximately equal numbers of males and females from the
brain genomics superstruct project (gsp) [5], where the concrete image
acquisition parameters and preprocessing procedures can be found as well.
specifically, a slightly modified version of yeo's computational brain imaging
group (cbig) fmri preprocessing pipeline (https://github.com/bchcohenlab/cbig)
was employed to obtain either one or two preprocessed resting-state fmri runs of
each subject that had 120 time points per run and were spatially normalized into
the mni152 template with 2mm 3 voxel size. we downloaded and used the first-run
preprocessed resting-state fmri of each subject for the following analysis.brats
2020. it provided an open-access pre-operative imaging training dataset to
segment brain tumors of glioblastoma (gbm, belonging to high grade glioma) and
low grade glioma (lgg) patients, as well as to predict overall survival time of
gbm patients [18]. this training dataset contained 133 lgg and 236 gbm patients,
and each patient had four mri modalities, including t1, post-contrast
t1-weighted, t2-weighted, and t2 fluid attenuated inversion recovery. manual
expert segmentation delineated three tumor sub-regions, i.e., the gd-enhancing
tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core.the
union of all the three tumor sub-regions was considered as the whole tumor,
which is regarded as the lesion in this paper.",8
2148,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.2,Methods,"in this paper, we propose to investigate the feasibility of the novel
neuroimaging features, i.e., fln maps, for overall survival time prediction of
gbm patients in the training dataset of the brats 2020, in which one patient
alive was excluded, and the remaining 235 patients consisted of 89 short-term
survivors (less than 10 months), 59 mid-term survivors (between 10 and 15
months), and 87 long-term survivors (more than 15 months). to this end, our
framework for the three-class survival classification is shown in fig. 1, and
the details are described as follows.lesion mapping procedures. as stated above,
the whole tumor is referred to as a lesion for each gbm patient. from the manual
expert segmentation labels of lesions in the 235 gbm patients of the brats 2020,
we co-register the lesion masks to the mni152 2mm 3 template by employing a
symmetric normalization algorithm in antspy [3].",8
2149,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,0.0,Augmented Lesion Network Mapping (A-LNM).,"after lesion mapping, we introduce a modified lnm (called augmented lnm (a-lnm)
in this paper) to generate fln maps for each gbm patient by using resting-state
fmri of all 1000 gsp healthy subjects, as described below. i) for each patient,
the lesion is viewed as a seed region to calculate fdc in the healthy subjects
with restingstate fmri. specifically, to compute fdc, the mean bold time series
of voxels within each lesion is correlated with the bold time series of every
voxel in the whole brain for all the 1000 healthy subjects, yielding 1000 fdc
maps of voxelwise correlation values (transformed to z-scores), where an fdc map
is actually a three-dimensional voxel-wise matrix of size 91 × 109 × 91 (spatial
resolution: 2mm 3 voxel size). ii) different from the commonly used lnm where
the resulting 1000 fdc maps are thresholded or averaged to obtain a single fln
map for each patient, the a-lnm generates many fln maps for each patient in a
manner that partitions all the 1000 fdc maps into disjoint subsets of equal size
and averages each subset to produce one fln map. one can clearly see that
similar to data augmentation schemes, we artificially boost the number of
training samples (i.e., fln maps) by our a-lnm, which helps to mitigate the risk
of over-fitting and improve the performance of overall survival time prediction
when learning a deep neural network from such a small sized training set used in
this paper. note that in sect. 3 of this paper, according to experimental
results, we divided the 1000 fdc maps into 100 subsets, and randomly chose 10
out of the resulting 100 fln maps for each patient as input to the downstream
prediction model.deep neural network for overall survival time prediction. by
taking the obtained fln maps as input, we apply a 3d resnet-based backbone
network transferred from the encoder of medicalnet [6] to extract cnn features
from each fln map. the features are then combined using the average pooling
operation and fed to a fully-connected layer with kernel size (1, 1, 1) to
classify each gbm patient into one of the three overall survival time groups
(i.e., short-term survival, mid-term survival, and long-term survival).",8
2150,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.1,Experimental Settings,"implementation details. our proposed method was implemented in pytorch 1.13.1 on
nvidia a100 tensor core gpus. the loss function was the standard cross-entropy
loss. the adam optimizer with the weight decay of 10 -5 was adopted. three 3d
resnet-based backbones with different numbers of layers (10, 50, and 101) were
performed, where the initial learning rates were set as 10 -4 , 10 -4 , and 10
-5 , respectively, and would decrease by a factor of 5 if the classification
performance is not improved within 5 epochs. the number of epochs for training
was 50, and the batch size was fixed as 64.performance evaluation. we evaluated
the classification performance of our proposed method using 235 gbm patients in
the brats 2020 training dataset, because only these 235 patients had both
overall survival time and manual expert segmentation labels of lesions. in all
experiments, we conducted five-fold crossvalidation ten times in order to reduce
the effect of sampling bias. moreover, the a-lnm was performed ten times
randomly to avoid particular data distribution and obtain more reliable results.
the classification results were reported in terms of accuracy, macro precision
(macro-p), macro recall (macro-r), and macro f1 score (macro-f1), respectively.",8
2152,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.3,Brain Regions in Relation to GBM Survival,"to identify the most discriminative brain regions associated with overall
survival time in gbm, we estimated the relative contribution of each voxel to
the classification performance in our proposed method by using the grad-cam
[10]. to obtain steady results, as shown in fig. 2(a), the voxels with top 5%
weights in the class activation maps (cams) of all candidate models were
overlapped by class, and the position covered by more than half of the models is
displayed. the cams of three classes of survivors overlapped in fig. 2(b) where
both coincident and non-coincident areas exist.the association of an increased
degree of invasion within the frontal lobe with decreased survival time can be
observed, which is in concordance with a previous study [20]. patients whose
frontal lobe is affected by tumors showed more executive dysfunction, apathy,
and disinhibition [11]. on the dominant left hemisphere, the cams of long-term
survivors and mid-term survivors overlapped at the superior temporal gyrus and
wernicke's area which are involved in the sensation of sound and language
comprehension respectively, and have been associated with decreased survival in
patients with high-grade glioma [26]. in addition, the cam of mid-term survivors
covered more areas of the middle and inferior temporal gyri which were
considered as one of the higher level ventral streams of visual processing
linked to facial recognition [25].",8
2153,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,4.0,Conclusion,"in this paper, we introduce a novel neuroimaging feature family, called a-lnm
derived fln maps, for overall survival time prediction of gbm patients. a-lnm
was presented to generate plenty of fln maps for each gbm patient by
partitioning the fdc maps obtained from resting-state fmri of 1000 gsp healthy
subjects into disjoint subsets of equal size and averaging each subset. we
applied a 3d resnet-based backbone network to extract features from the
generated fln maps and classify gbm patients into three overall survival time
groups. experimental results on the brats 2020 training dataset validated the
effectiveness of the a-lnm derived fln maps for gbm survival prediction.
moreover, a visualization analysis implemented by the grad-cam revealed the
brain regions associated with gbm survival. in future work, we will try to fuse
the fln maps and mri-based radiomics features to study their combined predictive
power for gbm survival analysis.",8
2192,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,1.0,Introduction,"gliomas are the most common central nervous system (cns) tumors in adults,
accounting for 80% of primary malignant brain tumors [1]. early surgical
treatment to remove the maximum amount of cancerous tissues while preserving the
eloquent brain regions can improve the patient's survival rate and functional
outcomes of the procedure [2]. although the latest multi-modal medical imaging
(e.g., pet, diffusion/functional mri) allows more precise pre-surigcal planning,
during surgery, brain tissues can deform under multiple factors, such as
gravity, intracranial pressure change, and drug administration. the phenomenon
is referred to as brain shift, and often invalidates the pre-surgical plan by
displacing surgical targets and other vital anatomies. with high flexibility,
portability, and cost-effectiveness, intra-operative ultrasound (us) is a
popular choice to track and monitor brain shift. in conjunction with effective
mri-us registration algorithms, the tool can help update the pre-surgical plan
during surgery to ensure the accuracy and safety of the intervention.as the true
underlying deformation from brain shift is impossible to obtain and the
differences of image features between mri and us are large, quantitative
validation of automatic mri-us registration algorithms often rely on homologous
anatomical landmarks that are manually labeled between corresponding mri and
intra-operative us scans [3]. however, manual landmark identification requires
strong expertise in anatomy and is costly in labor and time. moreover, inter-and
intra-rater variability still exists. these factors make quality assessment of
brain shift correction for us-guided brain tumor resection challenging. in
addition, due to the time constraints, similar evaluation of inter-modal
registration quality during surgery is nearly impossible, but still highly
desirable. to address these needs, deep learning (dl) holds the promise to
perform efficient and automatic inter-modal anatomical landmark
detection.previously, many groups have proposed algorithms to label landmarks in
anatomical scans [4][5][6][7][8][9]. however, almost all earlier techniques were
designed for mono-modal applications, and inter-modal landmark detection, such
as for usguided brain tumor resection, has rarely been attempted. in addition,
unlike other applications, where the full anatomy is visible in the scan and all
landmarks have consistent spatial arrangements across subjects, intra-operative
us of brain tumor resection only contains local regions of the pathology with
noncanonical orientations. this results in anatomical landmarks with different
spatial distributions across cases. to address these unique challenges, we
proposed a new contrastive learning (cl) framework to detect matching landmarks
in intra-operative us with those from mri as references. specifically, the
technique leverages two convolutional neural networks (cnns) to learn features
between mri and us that distinguish the inter-modal image patches which are
centered at the matching landmarks from those that are not. our approach has two
major novel contributions to the field. first, we proposed a multi-modal
landmark detection algorithm for us-guided brain tumor resection for the first
time. second, cl is employed for the first time in inter-modal anatomical
landmark detection. we developed and validated the proposed technique with the
public resect database [10] and compared its landmark detection accuracy against
the popular scale-invariant feature transformation (sift) algorithm in 3d [11].",9
2201,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,5.0,Results,"table 1 lists the mean and standard deviation of landmark identification errors
(in mm) between the predicted position and the ground truth in intra-operative
us for each patient of the resect dataset. in the table, we also provide the
severity of brain shift for each patient. here, tissue deformation measured as
mean target registration errors (mtres) with the ground truth anatomical
landmarks is classified as small (mtre below 3 mm), median (3-6 mm), or large
(above 6 mm). the results show that our cl-based landmark selection technique
can locate the corresponding us landmarks with a mean landmark identification
error of 5.88±4.79 mm across all cases while the sift algorithm has an error
18.78±4.77 mm. with a two-sided paired-samples t-test, our method outperformed
the sift approach with statistical significance (p <1e-4). when reviewing the
mean landmark identification error using our proposed technique, we also found
that the magnitude is associated with the level of brain shift. however, no such
trend is observed when using sift features for landmark identification. when
inspecting landmark identification errors across all subjects between the cl and
sift techniques, we also noticed that our cl framework has significantly lower
standard deviations (p <1e-4), implying that our technique has a better
performance consistency.",9
2232,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,1.0,Introduction,"image-to-physical registration is a necessary process for computer-assisted
surgery to align preoperative imaging to the intraoperative physical space of
the patient to in-form surgical decision making. most intraoperatively utilized
image-to-physical regis-trations are rigid transformations calculated using
fiducial landmarks [1]. however, with better computational resources and more
advanced surgical field monitoring sensors, nonrigid registration techniques
have been proposed [2,3]. this has made image-guided surgery more tractable for
soft tissue organ systems like the liver, prostate, and breast [4][5][6]. this
work focuses specifically on nonrigid breast registration, although these
methods could be adapted for other soft tissue organs. current guidance
technologies for breast conserving surgery localize a single tumor-implanted
seed without providing spatial information about the tumor boundary. as a
result, resections can have several centimeters of tissue beyond the cancer
margin. despite seed information and large resections, reoperation rates are
still high (~17%) emphasizing the need for additional guidance technologies such
as computer-assisted surgery systems with nonrigid registration
[7].intraoperative data available for registration is often sparse and subject
to data collection noise. image-to-physical registration methods that accurately
model an elastic soft-tissue environment while also complying with
intraoperative data constraints is an active field of research. determining
correspondences between imaging space and geometric data is required for
image-to-physical registration, but it is often an inexact and ill-posed
problem. establishing point cloud correspondences using machine learning has
been demonstrated on liver and prostate datasets [8,9]. deep learning image
registration methods like voxelmorph have also been used for this purpose [10].
however, these methods require extensive training data and may struggle with
generalizability. other non-learning image-to-physical registration strategies
include [11] which utilized a corotational linear-elastic finite element method
(fem) combined with an iterative closest point algorithm. similarly, the
registration method introduced in [12] iteratively updated the image-to-physical
correspondence between surface point clouds while solving for an optimal
deformation state.in addition to a correspondence algorithm, a technique for
modeling a deformation field is required. both [11] and [12] leverage fem, which
uses a 3d mesh to solve for unique deformation solutions. however, large
deformations can cause mesh distortions with the need for remeshing. mesh-free
methods have been introduced to circumvent this limitation. the element-free
galerkin method is a mesh-free method that requires only nodal point data and
uses a moving least-squares approximation to solve for a solution [13]. other
mesh-free methods are reviewed in [14]. although these methods do not require a
3d mesh, solving for a solution can be costly and boundary condition designation
is often unintuitive. having identified these same shortcomings, [15] proposed
regularized kelvinlet functions for volumetric digital sculpting in computer
animation applications. this sculpting approach provided de-formations
consistent with linear elasticity without large computational overhead.in this
work, we propose an image-to-physical registration method that uses regularized
kelvinlet functions as a novel deformation basis for nonrigid registration.
regularized kelvinlet functions are analytical solutions to the equations for
linear elasticity that we superpose to compute a nonrigid deformation field
nearly instantaneously [15].we utilize ""grab"" and ""twist"" regularized kelvinlet
functions with a linearized iterative reconstruction approach (adapted from
[12]) that is well-suited for sparse data registration problems. sensitivity to
regularized kelvinlet function hyperparameters is explored on a supine mr breast
imaging dataset. finally, our approach is validated on an exemplar breast cancer
case with a segmented tumor by comparing performance to previously proposed
registration methods.",9
2236,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.0,Experiments and Results,"in this section, two experiments are conducted. the first explores sensitivity
to regularized kelvinlet function hyperparameters k grab , k twist , ε grab ,
and ε twist and establishes optimal hyperparameters in a training dataset of 11
breast deformations. the second validates the registration method in a breast
cancer patient and compares registration accuracy and computation time to
previously proposed methods.",9
2238,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.2,Registration Methods Comparison,"this dataset consists of supine breast mr images simulating surgical
deformations from one breast cancer patient. a 71-year-old patient with invasive
mammary carcinoma in the left breast was enrolled in a study approved by the
institutional review board at vanderbilt university. skin fiducial placement,
image acquisition, arm placement, and preprocessing steps followed the same
protocol detailed in sect. 3.1. the tumor was segmented in both images by a
subject matter expert, and a 3d tumor model was created to evaluate tumor
overlap metrics after registration.regularized kelvinlet function registration
was compared to 3 other registration methods: rigid registration, an fem-based
image-to-physical registration method, and an image-to-image registration
method. a point-based rigid registration using the skin fiducials provided a
baseline comparator for accuracy without deformable correction. the fem-based
image-to-physical registration method, detailed in [12] and implemented in
breast in [16], utilizes the same optimization scheme as this method but with an
fem-generated basis. k = 40 control points were used for the fem-based
registration. the image-to-image registration method was a symmetric
diffeomorphic method with explicit b-spline regularization publicly available in
the advanced normalization toolkit (ants) repository [19,20]. image-to-image
registration would not be possible for intraoperative registration in most
surgical settings. however, it was included to demonstrate accuracy when
volumetric imaging data is available, as opposed to sparse geometric point data
as in the surgical application case. the rigid and image-to-physical
registrations were performed on a single thread of a 3.6 ghz amd ryzen 7 3700x
cpu. image-to-image registration was multithreaded on 2.3 ghz intel xeon
(e5-4610 v2) cpus.registration results for the 4 methods are shown in table 1.
the regularized kelvinlet method accuracy was comparable (if not slightly
improved) to the fem-based method for this example case. runtime for the
regularized kelvinlet method was improved compared to the fem-based method. as
expected, registration without deformable correction was poor, and
image-to-image registration had the best accuracy. registered tumor geometry
results are shown in fig. 4.",9
2273,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,1.0,Introduction,"resection of early-stage brain tumors can greatly reduce the mortality rate of
patients. during the surgery, brain tissue deformation (called brain shift) can
occur due to various causes, such as gravity, drug administration, and pressure
change after craniotomy. while modern magnetic resonance imaging (mri)
techniques can provide rich anatomical and physiological information with
various contrasts (e.g., fmri) for more elaborate pre-surgical planning,
intra-operative mri that can track brain shift requires a complex setup and is
costly. in contrast, intra-operative ultrasound (ius) has gained popularity for
real-time imaging during surgery to monitor tissue deformation and surgical
tools because of its lower cost, portability, and flexibility [1]. accurate and
robust mri-ius registration techniques [2] can greatly enhance the value of ius
for updating pre-surgical plans and guiding the interpretation of ius, which has
an unintuitive contrast and non-standard orientations. this can greatly enhance
the safety and outcomes of the surgical procedure by allowing maximum brain
tumor removal while avoiding eloquent regions [3]. however, as the true
underlying tissue deformation is unknown due to the 3d nature of the surgical
data and the time constraint, real-time manual inspection of mri-ius
registration results is challenging and error-prone, especially for
precision-sensitive neurosurgery. therefore, algorithms that can detect and
quantify unreliable inter-modal medical image registration results are highly
beneficial.recently, automatic quality assessment for medical image registration
has attracted increasing attention [4] from the domains of big medical data
analysis and surgical interventions. with high efficiency, machine, and deep
learning techniques have been proposed to allow automatic grading and dense
estimation of medical image registration errors. early endeavors on this topic
primarily relied on hand-crafted features, including information theory-based
metrics [5][6][7][8][9][10]. more recently, deep learning (dl) techniques that
learn task-specific features have also been adopted in automatic evaluation of
medical image registration, with a primary focus on intra-contrast/modal
applications, including ct [9,10] and mri [11]. unfortunately, so far, error
grading and estimation in inter-contrast/modal registration have rarely been
explored, despite the particular demand in surgical applications. in this
direction, bierbrier et al. [12] made the first attempt using simulated ius from
mri to train 3d convolutional neural networks (cnns) to perform dense error
regression for mri-ius registration in brain tumor resection. although their
algorithm performed well in simulated cases, the results on real clinical scans
still required improvements. in this paper, we propose a novel 3d cnn to perform
patch-wise error estimation for mri-ius registration in neurosurgery, by using
focal modulation [13], a recent alternative dl technique to self-attention [14]
for encoding contextual information, and uncertainty estimation. we call our
method focalerrornet, which has three main novelties. first, we adapted the
focal modulation network [13] from 2d to 3d and employed the technique in
registration error assessment for the first time. second, we incorporated
uncertainty estimation using monte carlo (mc) dropouts [15] to offer assurance
for error regression. lastly, we developed and thoroughly evaluated our
technique against a recent baseline model [12] using real clinical data and
showed excellent results.",9
2274,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.1,Dataset and Preprocessing,"for methodological development and assessment, we used the resect
(retro-spective evaluation of cerebral tumors) dataset [16], which has
pre-operative mri, and ius scans at different surgical stages from 23 subjects
who underwent low-grade glioma resection surgeries. as it is still challenging
to model ius scans with tissue resection, we took 22 cases with t2flair mri that
better depicts tumor boundaries and ius acquired before resection. an example of
an mri-ius pair from a patient is shown in fig. 1. we hypothesized that directly
leveraging clinical ius could help learn more realistic image features with
potentially better outcomes in clinical applications than with simulated
contrasts [9,12]. however, since the true brain shift model is impossible to
obtain, we followed the strategy of creating silver ground truths for image
alignment [9,12], upon which simulated misalignment is augmented in the ius to
build and test our dl model. to create the silver registration ground truths, we
used the homologous landmarks between mri and ius in the resect dataset to
perform landmark-based 3d b-spline nonlinear registration to register ius to the
corresponding mri for all 22 cases. to tackle the limited field of view (fov) in
ius, we cropped the t2flair mri to the same fov of the ius, which was resampled
to a 0.5 × 0.5 × 0.5 mm 3 resolution. to perform spatial misalignment
augmentation, we continued to leverage 3d b-spline transformation, similar to
earlier reports on the same topic [10,12,17]. in short, b-spline transformation
can be modeled by a grid of regularly spaced control points and the associated
parameters to allow various levels of nonlinear deformation. while the spacing
of the control points determines the levels of details in local deformation
fields, the displacement parameters control the magnitude of the deformation. to
ensure that simulated registration errors are of different varieties and sizes,
we randomly selected the number of control points and the associated
displacements (in each 3d axis) with a maximum of 20 points and 30 mm,
respectively. note that the control point grid is isotropic, and the density is
arbitrarily determined per deformation in our case. each coregistered ius scan
was deformed ten times. after misalignment augmentation on the previously
co-registered ius, matching pairs of 3d image patches of size 33 × 33 × 33
voxels were taken from both the ius volume and the corresponding mri. as ius has
limited fov and may contain no anatomical features, to ensure that the patches
we extracted contain useful information (e.g. to avoid the dark background) in
ius, we focused on acquiring patches centered around the anatomical landmark
locations available through the resect database. since b-spline transformation
offers a displacement vector at each voxel of the ius volume, we directly
considered the norm of the vector as the simulated registration error at the
associated voxel. in our design, we determined the registration error of the
image patch pair as the mean of all voxel-wise errors within the ius patch.
finally, the image patch pairs, along with corresponding registration errors
were then fed to the proposed dl algorithm for training and validation.",9
2276,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.3,Uncertainty Quantification,"for registration error regression in surgical applications, knowledge regarding
the reliability of the automated results is instrumental for the safety and
wellbeing of the patients. uncertainty estimation has gained popularity in
probing the trustworthiness and credence of dl algorithms. although the concept
has been widely applied in image segmentation and classification, it has not
been employed for registration error estimation, especially in the case of
multi-modal situations, such as mri-ius alignment. therefore, we incorporated
uncertainty estimation in our proposed focalerrornet. for each mri-ius patch
pair, 200 regression samples were collected by random sampling from mc dropouts
[15] at test time. while the final patch registration error was obtained as the
mean of all the samples, the sample standard deviation was used as the
uncertainty metric.",9
2277,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.4,Experimental Setup and Implementation,"from the transformation augmentation, we acquired 3380 samples of mri-ius pairs.
for our experiments, we arbitrarily split the subjects into training,
validation, and test sets with the proportion of 60%, 20%, and 20%,
respectively. to prevent information leakage, we ensured that each patient was
included in only one of the split sets. for model training, we adopted the adam
optimization with a learning rate of 5 × 10 -5 and a batch size of 64. for the
loss function, we used mean squared error (mse) to minimize the difference
between the predicted mri-ius registration error and the ground truths.
furthermore, in addition to the transformation augmentation, we also included
additional data augmentation, including random noise addition and random image
flipping on training sets to mitigate overfitting and increase the model's
generalizability. to assess our proposed focalerrornet, we compared it against a
3d cnn [9,12] (see fig. 3) that was employed for medical image registration
error regression. the two dl models were trained with the same dataset and
procedure, and their prediction accuracies, measured as the absolute error
between the predicted and ground truths mis-registration on the test set were
compared with two-sided pairedsamples t-tests to confirm the superiority of the
proposed method, in addition to correlations between their estimated and ground
truth errors. to validate the proposed uncertainty estimation method, we
calculated the correlation between the uncertainty measure and absolute error of
focalerrornet, and the correlation between the uncertainty and mutual
information between mri and ius, which is often used to measure the information
overlap in multi-modal registration. finally, to test the robustness of the
focalerrornet, we acquired additional mri-ius patch pairs from the test
subjects, by introducing random linear shifts (the max displacement from
landmark locations is 10 voxels) from the selected locations in the original
set, and evaluated the dl model performance.",9
2281,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,4.0,Discussion,"in image-guided interventions, there is an urgent need for automatic assessment
of image registration quality. multi-modal registration quality evaluation poses
major challenges due to three main factors. first, dissimilar contrasts between
images require more elaborate strategies to derive relevant features for error
assessment. second, unlike segmentation or classification, the ground truths of
registration errors are difficult to obtain. finally, compared with
classification, regression tasks tend to be more error-prone for deep learning
algorithms. to tackle these challenges, we employed 3d focal modulation with
depth-wise convolution to encode contextual information for the image pair.
compared with the vit and its variants, focal modulation allows a more
lightweight setup, which could be desirable for 3d data. although we admit that
residual errors still remain after landmark-based b-spline nonlinear alignment,
this approach has been adopted in different prior studies, considering the
residual landmark registration error is fairly low (mtre of 0.0008 ± 0.0010mm).
although simulated ultrasound has been used to provide a perfect alignment with
mris, the fidelity of the simulated results is still suboptimal, and this may
explain the underperformance of the previous technique in real clinical data
[12]. to ensure the performance of our focalerrornet, we opted to regress the
mean registration error of image patches than simplistic error grades or
voxel-wise error maps. we believe that this design choice offers a more stable
performance, which is supported by our validation. we adopted uncertainty
estimation in inter-modal registration error assessment for the first time.
while other techniques exist to provide model uncertainty [18], mc dropout is
more flexible for various dl models. furthermore, the use of standard deviation
as an uncertainty measurement maintains the same unit as the regressed errors,
thus making the interpretation more intuitive. from quantitative and qualitative
evaluations using correlation coefficients and scatter plots to assess the
association of uncertainty measures with the prediction errors and image
entropy, we confirmed the validity of the proposed uncertainty estimation
approach. for our focalerrornet, we achieved a prediction error of 0.59 ± 0.57
mm, which is on par with the image resolution (0.5 mm). additionally, the
standard deviation of our results is lower than the baseline model [12]. these
signify a robust performance of the focalerrornet. one limitation of our work
lies in the limited patient data, as public ius datasets are scarce, while the
settings and properties of us scanners can vary, potentially affecting the dl
model designs. therefore, we created random deformations for patch-wise error
estimation, and will further explore data-efficient approaches for registration
error assessment.",9
2283,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,1.0,Introduction,"cancer remains a significant public health challenge worldwide, with a new
diagnosis occurring every two minutes in the uk (cancer research uk 1 ). surgery
is one of the main curative treatment options for cancer. however, despite
substantial advances in pre-operative imaging such as ct, mri, or pet/spect to
aid diagnosis, surgeons still rely on the sense of touch and naked eye to detect
cancerous tissues and disease metastases intra-operatively due to the lack of
reliable intraoperative visualization tools. in practice, imprecise
intraoperative cancer tissue detection and visualization results in missed
cancer or the unnecessary removal of healthy tissues, which leads to increased
costs and potential harm to the patient. there is a pressing need for more
reliable and accurate intraoperative visualization tools for minimally invasive
surgery (mis) to improve surgical outcomes and enhance patient care. a recent
miniaturized cancer detection probe (i.e., 'sensei r ' developed by lightpoint
medical ltd.) leverages the cancer-targeting ability of nuclear agents typically
used in nuclear imaging to more accurately identify cancer intraoperatively from
the emitted gamma signal (see fig. 1b) [6]. however, the use of this probe
presents a visualization challenge as the probe is non-imaging and is air-gapped
from the tissue, making it challenging for the surgeon to locate the
probe-sensing area on the tissue surface.it is crucial to accurately determine
the sensing area, with positive signal potentially indicating cancer or affected
lymph nodes. geometrically, the sensing area is defined as the intersection
point between the gamma probe axis and the tissue surface in 3d space, but
projected onto the 2d laparoscopic image. however, it is not trivial to
determine this using traditional methods due to poor textural definition of
tissues and lack of per-pixel ground truth depth data. similarly, it is also
challenging to acquire the probe pose during the surgery.problem redefinition.
in this study, in order to provide sensing area visualization ground truth, we
modified a non-functional 'sensei' probe by adding a miniaturized laser module
to clearly optically indicate the sensing area on the laparoscopic images -i.e.
the 'probe axis-surface intersection'. our system consists of four main
components: a customized stereo laparoscope system for capturing stereo images,
a rotation stage for automatic phantom movement, a shutter for illumination
control, and a daq-controlled switchable laser module (see fig. 1a). with this
setup, we aim to transform the sensing area localization problem from a
geometrical issue to a high-level content inference problem in 2d. it is
noteworthy that this remains a challenging task, as ultimately we need to infer
the probe axis-surface intersection without the aid of the laser module to
realistically simulate the use of the 'sensei' probe.",9
2332,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,1.0,Introduction,"tomographic imaging estimates body density using hundreds of x-ray projections,
but it's slow and harmful to patients. acquisition time may be too high for
certain applications, and each projection adds dose to the patient. a quick,
low-cost 3d estimation of internal structures using only bi-planar x-rays can
revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and
more. this can improve image-guided therapies and preoperative planning,
especially for radiotherapy, which requires precise patient positioning with
minimal radiation exposure.however, this task is an ill-posed inverse problem:
x-ray measurements are the result of attenuation integration across the body,
which makes them very fig. 1. current methods vs our method. feed-forward
methods do not manage to predict a detailed and matching tomographic volume from
a few projections. iterative methods based on neural radiance fields lack prior
for good reconstruction. by learning an embedding for the possible volumes, we
can recover an accurate volume from very few projections with an optimization
based on a bayesian formulation.ambiguous. traditional reconstruction methods
require hundreds of projections to get sufficient constraints on the internal
structures. with very few projections, it is very difficult to disentangle the
structures for even coarse 3d estimation. in other words, many 3d volumes may
have generated such projections a priori.classical analytical and iterative
methods [8] fail when very few projections are available. several works have
attempted to largely decrease the number of projections needed for an accurate
volumetric reconstruction. some deep learning methods [7,12,24,25,30] predict
directly a 3d volume in a forward way from very few projections. the volume is
however not guaranteed to be consistent with the projections and it is not clear
which solution is retrieved. other recent methods have adapted nerfs [20] to
tomographic reconstruction [23,31]. these non-learning methods show good results
when the number of input projections remains higher than a dozen but fail when
very few projections are provided, as our experiments in sect. 3.3 show.as
illustrated in fig. 1, to be able to reconstruct a volume accurately given as
low as two projections only, we first learn a prior on the volume. to do this,
we leverage the potential of generative models to learn a low-dimensional
manifold of the target body part. given projections, we find by a bayesian
formulation the intermediate latent vectors conditioning the generative model
that minimize the error between synthesized projections of our reconstruction
and these input projections. our work builds on hong et al. [10]'s 3d
style-based generative model, which we extend via a more complex network and
training framework.compared to other 3d gans, it is proven to provide the best
disentanglement of the feature space related to semantic features [2].by
contrast with feed-forward methods, our approach does not require paired
projections-reconstructions, which are very tedious to acquire, and it can be
used with different numbers of projections and different projection geometries
without retraining. compared to nerf-based methods, our method exploits prior
knowledge from many patients to require only two projections. we evaluate our
method on reconstructing cancer patients' head-and-neck cts, which involves
intricate and complicated structures. we perform several experiments to compare
our method with a feed-forward-based method [30] and a recent nerf-based method
[23], which are the previous state-of-the-art methods for the very few or few
projections cases, respectively.we show that our method allows to retrieve
results with the finest reconstructions and better matching structures, for a
variety of number of projections. to summarize, our contributions are two-fold:
(i) a new paradigm for 3d reconstruction with biplanar x-rays: instead of
learning to invert the measurements, we leverage a 3d style-based generative
model to learn deep image priors of anatomic structures and optimize over the
latent space to match the input projections; (ii) a novel unsupervised method,
fast and robust to sampling ratio, source energy, angles and geometry of
projections, all of which making it general for downstream applications and
imaging systems.",10
2336,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.3,Reconstruction from Biplanar Projections,"since our generative model provides a volume v as a function of vectors w and n,
we can reparameterize our optimization from eq. ( 1) into:note that by contrast
with [18] for example, we optimize on the noise vectors n as well: as we
discovered in our early experiments, the n are also useful to embed
high-resolution details. we take our regularization term r(w, n) as:term l w (w)
=k log n (w k |μ, σ) ensures that w lies on the same distribution as during
training. n (•|μ, σ) represents the density of the standard normal distribution
of mean μ and standard deviation σ.term l c (w) =i,j log m(θ i,j |0, κ)
encourages the w i vectors to be collinear so to keep the generation of
coarse-to-fine structures coherent. m(•; μ, κ) is the density of the von mises
distribution of mean μ and scale κ, which we take fixed, and θ i,j = arccos(
wi•wj wi wj ) is the angle between vectors w i and w j .term l n (n) =j log n (n
j |0, i ) ensures that the n j lie on the same distribution as during training,
i.e., a multivariate standard normal distribution. the λ * are fixed
weights.projection operator. in practice, we take operator a as a 3d cone beam
projection that simulates x-ray attenuation across the patient, adapted from
[21,27]. we model a realistic x-ray attenuation as a ray tracing projection
using material and spectrum awareness:with μ(m, e) the linear attenuation
coefficient of material m at energy state e that is known [11], t m the material
thickness, i 0 the intensity of the source x-ray.for materials, we consider the
bones and tissues that we separate by threshold on electron density. a inverts
the attenuation intensities i atten to generate an x-ray along few directions
successively. we make a differentiable using [21] to allow end-to-end
optimization for reconstruction.3 experiments and results",10
2337,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.1,Dataset and Preprocessing,"manifold learning. we trained our model with a large dataset of 3500 cts of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available the cancer imaging archive (tcia) [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations. we split this
data into 3000 cases for training, 250 for validation, and 250 for testing. we
focused ct scans on the head and neck region above shoulders, with a resolution
of 80 × 96 × 112, and centered on the mouth after automatic segmentation using a
pre-trained u-net [22]. the cts were preprocessed by min-max normalization after
clipping between -1024 and 2000 hounsfield units (hu).3d reconstruction. to
evaluate our approach, we used an external private cohort of 80 patients who had
undergone radiotherapy for head-and-neck cancer, with their consent. planning ct
scans were obtained for dose preparation, and cbct scans were obtained at each
treatment fraction for positioning with full gantry acquisition. as can be seen
in fig. 3 and the supplementary material, all these cases are challenging as
there are large changes between the original ct scan and the cbct scans. we
identified these cases automatically by comparing the cbcts with the planning
cts. to compare our reconstruction in the calibrated hu space, we registered the
planning cts on the cbcts by deformable registration with mrf minimization [4].
we hence obtained 3d volumes as virtual cts we considered as ground truths for
our reconstructions after normalization. from these volumes, we generated
projections using the projection module described in sect. 2.3.",10
2339,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.3,Results and Discussion,"manifold learning. we tested our model's ability to learn the low-dimensional
manifold. we used fid [9] to measure the distance between the distribution of
generated volumes and real volumes, and ms-ssim [29] to evaluate volumes'
diversity and quality. we obtained a 3d fid of 46 and a ms-ssim of 0.92. for
reference, compared to 3dstylegan [10], our model achieved half their fid score
on another brain mri dataset, with comparable ms-ssim. this may be due to a more
complex architecture, discriminator augmentation, or simpler anatomy.baselines.
we compared our method against the main feed-forward method x2ct-gan [30] and
the neural radiance fields with prior image embedding method nerp [23] meant for
modest sparsely-sampled reconstruction. recent methods like [24] and [12] were
excluded because they provide only minor improvements compared to x2ct-gan [30]
and have similar constraints to feed-forward methods. additionally, no public
implementation is available. [26] uses a flow-based generative model, but the
results are of lower quality compared to gans and similar to x2ct-gan [30].3d
reconstruction. to evaluate our method's performance with biplanar projections,
we focused on positioning imaging for radiotherapy. figure 3 compares our
reconstruction with those of the baselines from biplanar projections. our method
achieves better fitting of the patient structure, including bones, tissues, and
air separations, almost matching the real ct volume. x2ct-gan [30] produced
realistic structures, but failed to match the actual structures as it does not
enforce consistency with the projections. in some clinical procedures, an
earlier ct volume of the patient may be available and can be used as an
additional input for nerp [23]. without a previous ct volume, nerp lacks the
necessary prior to accurately solve the ill-posed problem. even when initialised
with a previous ct volume, nerp often fails to converge to the correct volume
and introduces many artifacts when few projections are used. in contrast, our
method is more versatile and produces better results. we used quantitative
metrics (psnr and ssim) to evaluate reconstruction error and human perception,
respectively. table 1 shows these metrics for our method and baselines with 1 to
8 cone beam projections. deviation from projections, as in x2ct-gan, leads to
inaccurate reconstruction. however, relying solely on projection consistency is
inadequate for this ill-posed problem. nerp matches projections but cannot
reconstruct the volume correctly. our approach balances between instant and
iterative methods by providing a reconstruction in 25 s with 100 optimization
steps, while ensuring maximal consistency. in contrast, nerp requires 7 min, and
x2ct-gan produces structures instantly but unmatching. clinical cbct acquisition
and reconstruction by fdk [3] take about 1-2 min and 10 s respectively. our
approach significantly reduces clin-ical time and radiation dose by using
instant biplanar projections, making it promising for fast 3d visualization
towards complex positioning.",10
2340,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,4.0,"Conclusion, Limitations, and Future Work","we proposed a new unsupervised method for 3d reconstruction from biplanar x-rays
using a deep generative model to learn the structure manifold and retrieve the
maximum a posteriori volume with the projections, leading to stateof-the-art
reconstruction. our approach is fast, robust, and applicable to various human
body parts, making it suitable for many clinical applications, including
positioning and visualization with reduced radiation.future hardware
improvements may increase resolution, and our approach could benefit from other
generative models like latent diffusion models. this approach may provide coarse
reconstructions for patients with rare abnormalities, as most learning methods,
but a larger dataset or developing a prior including tissue abnormalities could
improve robustness.",10
2342,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,1.0,Introduction,"magnetic resonance imaging (mri) and computed tomography (ct) are two commonly
used cross-sectional medical imaging techniques. mri and ct produce different
tissue contrast and are often used in tandem to provide complementary
information. while mri is useful for visualizing soft tissues (e.g. muscle, [20]
fails to preserve the smooth anatomy of the mri. (c) attentiongan [12] inflates
the head area in the synthetic ct, which is inconsistent with the original mri.
quantitative evaluations in mae (lower is better) are shown in yellow.fat), ct
is superior for visualizing bony structures. some medical procedures, such as
radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically
require both mri and ct for planning. unfortunately, ct imaging exposes patients
to ionizing radiation, which can damage dna and increase cancer risk [9],
especially in children and adolescents. given these issues, there are clear
advantages for synthesizing anatomically accurate ct data from mri.most
synthesis methods adopt supervised learning paradigms and train generative
models to synthesize ct [1][2][3]6,17]. despite the superior performance,
supervised methods require a large amount of paired data, which is prohibitively
expensive to acquire. several unsupervised mri-to-ct synthesis methods [4,6,14],
leverage cyclegan with cycle consistency supervision to eliminate the need for
paired data. unfortunately, the performance of unsupervised ct synthesis methods
[4,14,15] is inferior to supervised counterparts. due to the lack of direct
constraints on the synthetic outputs, cyclegan [20] struggles to preserve the
anatomical structure when synthesizing ct images, as shown in fig. 1(b). the
structural distortion in synthetic results exacerbates when data from the two
modalities are heavily misaligned, which usually occurs in pediatric scanning
due to the rapid growth in children.recent unsupervised methods impose
structural constraints on the synthesized ct through pixel-wise or shape-wise
consistency. pixel-wise consistency methods [8,14,15] capture and align
pixel-wise correlations between mri and synthesized ct. however, enforcing
pixel-wise consistency may introduce undesirable artifacts in the synthetic
results. this problem is particularly relevant in brain scanning, where both the
pixel-wise correlation and noise statistics in mr and ct images are different,
as a direct consequence of the signal acquisition technique. the alternative
shape-wise consistency methods [3,4,19] aim to preserve the shapes of major body
parts in the synthetic image. notably, shape-cyclegan [4] segments synthesized
ct and enforces consistency with the ground-truth mri segmentation. however,
these methods rely on segmentation annotations, which are time-consuming,
labor-intensive, and require expert radiological annotators. a recent natural
image synthesis approach, called attention-gan [12], learns attention masks to
identify discriminative structures. atten-tiongan implicitly learns prominent
structures in the image without using the ground-truth shape. unfortunately, the
lack of explicit mask supervision can lead to imprecise attention masks and, in
turn, produce inaccurate mappings of the anatomy, as shown in fig. 1(c). in this
paper, we propose maskgan, a novel unsupervised mri-to-ct synthesis method, that
preserves the anatomy under the explicit supervision of coarse masks without
using costly manual annotations. unlike segmentationbased methods [4,18],
maskgan bypasses the need for precise annotations, replacing them with standard
(unsupervised) image processing techniques, which can produce coarse anatomical
masks. such masks, although imperfect, provide sufficient cues for maskgan to
capture anatomical outlines and produce structurally consistent images. table 1
highlights our differences compared with previous shape-aware methods [4,12].
our major contributions are summarized as follows. 1) we introduce maskgan, a
novel unsupervised mri-to-ct synthesis method. maskgan is the first framework
that maintains shape consistency without relying on human-annotated
segmentation. 2) we present two new structural supervisions to enforce
consistent extraction of anatomical structures across mri and ct domains. 3)
extensive experiments show that our method outperforms state-of-the-art methods
by using automatically extracted coarse masks to effectively enhance structural
consistency.",10
2347,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3.1,Experimental Settings,"data collection. we collected 270 volumetric t1-weighted mri and 267 thinslice
ct head scans with bony reconstruction performed in pediatric patients under
routine scanning protocols1 . we targeted the age group from 6-24 months since
pediatric patients are more susceptible to ionizing radiation and experience a
greater cancer risk (up to 24% increase) from radiation exposure [7].
furthermore, surgery for craniosynostosis, a birth defect in which the skull
bones fuse too early, typically occurs during this age [5,16]. the scans were
acquired by ingenia 3.0t mri scanners and philips brilliance 64 ct scanners. we
then resampled the volumetric scans to the same resolution of 1.0 × 1.0 × 1.0 mm
3 . the dataset comprises brain mr and ct volumes from 262 subjects. 13 mri-ct
volumes from the same patients that were captured less than three months apart
are registered using rigid registration algorithms. the dataset is divided into
249, 1 and 12 subjects for training, validating and testing set. following [13],
we conducted experiments on sagittal slices. each mr and ct volume consists of
180 to 200 slices, which are resized and padded to the size of 224 × 224. the
intensity range of ct is clipped into [-1000, 2000]. all models are trained
using the adam optimizer for 100 epochs, with a learning rate of 0.0002 which
linearly decays to zero over the last 50 epochs. we use a batch size of 16 and
train on two nvidia rtx 3090 gpus.evaluation metrics. to provide a quantitative
evaluation of methods, we compute the same standard performance metrics as in
previous works [6,14] including mean absolute error (mae), peak signal-to-noise
ratio (psnr), and structural similarity (ssim) between ground-truth and
synthesized ct. the scope of the paper centers on theoretical development;
clinical evaluations such as dose calculation and treatment planning will be
conducted in future work.",10
2417,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,3.0,Method,"we train our model to approximate the three-dimensional lc 2 similarity, as it
showed good performance on a number of tasks, including ultrasound [2,22]. the
lc 2 similarity quantifies whether a target patch can be approximated by a
linear combination of the intensities and the gradient magnitude of the source
patch. in order to reduce the sensitivity on the scale, our target is actually
the average lc 2 over different radiuses of 3, 5, and 7. in order to be
consistent with the original implementation of lc 2 we use the same weighting
function w based on local patch variance. note that the network will be trained
only once, on a fixed dataset that is fully independent of the datasets that
will be used in the evaluation (see sect. 4).dataset. our neural network is
trained using patches from the ""gold atlas -male pelvis -gentle radiotherapy""
[14] dataset, which is comprised of 18 patients each with a ct, mr t1, and mr t2
volumes. we resample each volume to a spacing of 2 mm and normalize the voxel
intensities to have zero mean and standard variation of one. since our approach
is unsupervised, we don't make use of the provided registration but leave the
volumes in their standard dicom orientation. as lc 2 requires the usage of
gradient magnitude in one of the modalities, we randomly pick it from either ct
or mr. we would like to report that, initially, we also made use of a
proprietary dataset including us volumes. however, as our investigation
progressed, we observed that the incorporation of us data did not significantly
contribute to the generalization capabilities of our model. consequently, for
the purpose of ensuring reproducibility, all evaluations presented in this paper
exclusively pertain to the model trained solely on the public mr-ct
dataset.patch sampling from unregistered datasets. for each pair of volumes (m,
f ) we repeat the following procedure 5000 times: (1) select a patch from m with
probability proportional to its weight w; (2) compute the similarity with all
the patches of f ; (3) uniformly sample t ∈ [0, 1]; (4) pick the patch of f with
similarity score closest to t. running this procedure on our training data
results in a total of 510000 pairs of patches.",10
2421,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.2,Deformable Registration of Abdominal MR-CT,"our second application is the abdomen mr-ct task of the learn2reg challenge 2021
[8]. the dataset comprises 8 sets of mr and ct volumes, both depicting the
abdominal region of a single patient and exhibiting notable deformations. we
estimate dense deformation fields using the methodology outlined in [6] (without
inverse consistency) which first estimates a discrete displacement using
explicit search and then iteratively enforces global smoothness. segmentation
maps of anatomical structures are used to measure the quality of the
registration. in particular, we compute the 25th, 50th, and 75th quantile of the
dice similarity coefficient (dsc) and the 95th quantile of the hausdorff
distance (hd95) between the registered label maps. we compare mind-scc and
disa-lc 2 used with different strides and followed by a downsampling operation
that brings the spacing of the descriptors volumes to 8 mm. the hyperparameters
of the registration algorithm have been manually optimized for each approach.
table 2 shows that our method obtains significantly better results than mind-scc
on the dsc metrics while being not significantly better on hd95.",10
2422,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.3,Deformable Registration of Abdominal US-CT and US-MR,"as the most challenging experiment, we finally use our method to achieve
deformable registration of abdominal 3d freehand us to a ct or mr volume.we are
using a heterogeneous dataset of 27 cases, comprising liver cancer patients and
healthy volunteers, different ultrasound machines, as well as optical vs.
electro-magnetic external tracking, and sub-costal vs. inter-costal scanning of
the liver. all 3d ultrasound data sets are accurately calibrated, with overall
system errors in the range of commercial ultrasound fusion options. between 4
and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder,
kidney) were manually annotated by an expert. in order to measure the capture
range, we start the registration from 50 random rigid poses around the ground
truth and calculate the fiducial registration error (fre) after optimization.
for local optimization, lc 2 is used in conjunction with bobyqa [15] as in the
original paper [22], while mind-scc and disa-lc 2 are instead used with bfgs.
due to an excessive computation time, we don't do global optimization with lc 2
while with other methods we use bfgs with 500 random initializations within a
range of ±40 • and ±150 mm. we use six parameters to define the rigid pose and
two parameters to describe the deformation caused by the ultrasound probe
pressure.from the results shown in table 3 and fig. 2, it can be noticed that
the proposed method obtains a significantly larger capture range than mind-scc
and lc 2 while being more than 300 times faster per evaluation than lc 2 (the
times reported in the table include not just the optimization but also
descriptor extraction). the differentiability of our objective function allows
our method to converge in fewer iterations than derivative-free methods like
bobyqa. furthermore, the evaluation speed of our objective function allows us to
exhaustively search the solution space, escaping local minima and converging to
the correct solution with pose and deformation parameters at once, in less than
two seconds.note that this registration problem is much more challenging than
the prior two due to difficult ultrasonic visibility in the abdomen, strong
deformations, and ambiguous matches of liver vasculature. therefore, to the best
of our knowledge, these results present a significant leap towards reliable and
fully automatic fusion, doing away with cumbersome manual landmark placements.",10
2472,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,1.0,Introduction,"liver cancer is the most prevalent indication for liver surgery, and although
there have been notable advancements in oncologic therapies, surgery remains as
the only curative approach overall [20].liver laparoscopic resection has
demonstrated fewer complications compared to open surgery [21], however, its
adoption has been hindered by several reasons, such as the risk of unintentional
vessel damage, as well as oncologic concerns such as tumor detection and margin
assessment. hence, the identification of intrahepatic landmarks, such as
vessels, and target lesions is crucial for successful and safe surgery, and
intraoperative ultrasound (ious) is the preferred technique to accomplish this
task. despite the increasing use of ious in surgery, its integration into
laparoscopic workflows (i.e., laparoscopic intraoperative ultrasound) remains
challenging due to combined problems.performing ious during laparoscopic liver
surgery poses significant challenges, as laparoscopy has poor ergonomics and
narrow fields of view, and on the other hand, ious demands skills to manipulate
the probe and analyze images. at the end, and despite its real-time
capabilities, ious images are intermittent and asynchronous to the surgery,
requiring multiple iterations and repetitive steps (probe-in -→ instruments-out
-→ probe-out -→ instruments-in). therefore, any method enabling a continuous and
synchronous us assessment throughout the surgery, with minimal iterations
required would significantly improve the surgical workflow, as well as its
efficiency and safety.to overcome these limitations, the use of intravascular
ultrasound (ivus) images has been proposed, enabling continuous and synchronous
inside-out imaging during liver surgery [19]. with an intravascular approach, an
overall view and full-thickness view of the liver can quickly and easily be
obtained through mostly rotational movements of the catheter, while this is
constrained to the lumen of the inferior vena cava, and with no interaction with
the tissue (contactless, a.k.a. standoff technique) as illustrated in fig. 1.
however, to benefit from such a technology in a computer-guided solution, the
different us images would need to be tracked and possibly integrated into a
volume for further processing. external us probes are often equipped with an
electromagnetic tracking system to track its position and orientation in
realtime. this information is then used to register the 3d ultrasound image with
the patient's anatomy. the use of such an electromagnetic tracking system in
laparoscopic surgery is more limited due to size reduction. the tracking system
may add additional complexity and cost to the surgical setup, and the tracking
accuracy may be affected by metallic devices in the surgical field [22].several
approaches have been proposed to address this limitation by proposing a
trackerless ultrasound volume reconstruction. physics-based methods have
exploited speckle correlation models between different adjacent frames [6][7][8]
to estimate their relative position. with the recent advances in deep learning,
recent works have proposed to learn a higher order nonlinear mapping between
adjacent frames and their relative spatial transformation. prevost et al. [9]
first demonstrated the effectiveness of a convolution neural network to learn
the relative motion between a pair of us images. xie et al. [10] proposed a
pyramid warping layer that exploits the optical flow features in addition to the
ultrasound features in order to reconstruct the volume. to enable a smooth 3d
reconstruction, a case-wise correlation loss based on 3d cnn and pearson
correlation coefficient was proposed in [10,12]. qi et al. [13] leverages past
and future frames to estimate the relative transformation between each pair of
the sequence; they used the consistency loss proposed in [14]. despite the
success of these approaches, they still suffer significant cumulative drift
errors and mainly focus on linear probe motions. recent work [15,16] proposed to
exploit the acceleration and orientation of an inertial measurement unit (imu)
to improve the reconstruction performance and reduce the drift error. motivated
by the weakness of the state-of-the-art methods when it comes to large
non-linear probe motions, and the difficulty of integrating imu sensors in the
case of minimally invasive procedures, we introduce a new method for pose
estimation and volume reconstruction in the context of minimally invasive
trackerless ultrasound imaging. we use a siamese architecture based on a
sequence to vector(seq2vec) neural network that leverages image and optical flow
features to learn relative transformation between a pair of images.our method
improves upon previous solutions in terms of robustness and accuracy,
particularly in the presence of rotational motion. such motion is predominant in
the context highlighted above and is the source of additional nonlinearity in
the pose estimation problem. to the best of our knowledge, this is the first
work that provides a clinically sound and efficient 3d us volume reconstruction
during minimally invasive procedures. the paper is organized as follows: sect. 2
details the method and its novelty, sect. 3 presents our current results on ex
vivo porcine data, and finally, we conclude in sect. 4 and discuss future work.",10
2478,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,3.1,Dataset and Implementation Details,"to validate our method, six tracked sequences were acquired from an ex vivo
swine liver. a manually manipulated ivus catheter was used (8 fr lateral firing
acunav tm 4-10 mhz) connected to an ultrasound system (acuson s3000 helx touch,
siemens healthineers, germany), both commercially available. an electromagnetic
tracking system (trakstar tm , ndi, canada) was used along with a 6 dof sensor
(model 130) embedded close to the tip of the catheter, and the plus toolkit [17]
along with 3d slicer [18] were used to record the sequences. the frame size was
initially 480 × 640. frames were cropped to remove the patient and probe
characteristics, then down-sampled to a size of 128 × 128 with an image spacing
of 0.22 mm per pixel. first and end stages of the sequences were removed from
the six acquired sequences, as they were considered to be largely stationary,
and aiming to avoid training bias. clips were created by sliding a window of 7
frames (corresponding to a value of k = 2) with a stride of 1 over each
continuous sequence, yielding a data set that contains a total of 13734 clips.
the tracking was provided for each frame as a 4×4 transformation matrix.we have
converted each to a vector of six degrees of freedom that corresponds to three
translations in mm and three euler angles in degrees. for each clip, relative
frame to frame transformations were computed for the frames number 0, 3 and 6.
the distribution of the relative transformation between the frames in our clips
is illustrated in the fig. 5. it is clear that our data mostly contains
rotations, in particular over the axis x. heatmaps were calculated for two
points (m = 2) and with a quality level of 0.1, a minimum distance of 7 and a
block size of 7 for the optical flow algorithm (see [4] for more details). the
number of heatmaps m and the frame jump k were experimentally chosen among 0, 2,
4, 6.the data was split into train, validation and test sets by a ratio of
7:1.5:1.5. our method is implemented in pytorch1 1.8.2, trained and evaluated on
a geforce rtx 3090. we use an adam optimizer with a learning rate of 10 -4 . the
training process converges in 40 epochs with a batch size of 16. the model with
the best performance on the validation data was selected and used for the
testing.",10
