Paper Title,Header Number,Header Title,Text
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,1,Introduction,"Sarcopenia is a prevalent musculoskeletal disease characterized by the inevitable loss of skeletal muscle, causing increased risks of all-cause mortality and disability that result in heavy healthcare costs [1][2][3][4][5][6]. Measuring body composition, such as lean muscle mass (excluding fat contents), is essential for diagnosing musculoskeletal diseases, where dual-energy X-ray absorptiometry (DXA) [7,8] and computed tomography (CT) [9][10][11] are often used. However, DXA and CT require special equipment that is much less accessible in a small clinic. Furthermore, CT requires high radiation exposure, and DXA allows the measurement of only overall body composition, which lacks details in individual muscles such as the iliacus muscle, which overlays with the gluteus maximus muscle in DXA images. Although several recent works used X-ray images for bone mineral density (BMD) estimation and osteoporosis diagnosis [12][13][14][15], only a few works estimated muscle metrics and sarcopenia diagnosis [16,17], and the deep learning technology used is old. Recently, BMD-GAN [15] was proposed for estimating BMD through X-ray image decomposition using X-ray and CT images aligned by 2D-3D registration. However, they did not target muscles. Nakanishi et al. [17] proposed an X-ray image decomposition for individual muscles. However, they calculated only the affected and unaffected muscle volumes ratio without considering the absolute volume and lean mass, which is more relevant to sarcopenia diagnosis.In this study, we propose MSKdeX: Musculoskeletal (MSK) decomposition from a plain X-ray image for the fine-grained estimation of lean muscle mass and volume of each individual muscle, which are useful metrics for evaluating muscle diseases including sarcopenia. Figure 1 illustrates the meaning of our fine-grained muscle analysis and its challenges. The contribution of this paper is three-fold: 1) proposal of the object-wise intensity-sum (OWIS) loss, a simple yet effective metric invariant to muscle deformation and projection direction, for quantitative learning of the absolute volume and lean mass of the muscles, 2) proposal of partially aligned training utilizing the aligned (paired) dataset for the rigid object for the pixel-wise supervision in an unpaired image translation task, 3) extensive evaluation of the performance using a 539-patient dataset.   [18], intensity conversion [19,20], 2D-3D registration for bones [21], and projection, embedding information of volume and mass. A decomposition model was trained using GAN loss and proposed GC loss chain, OWIS loss, and bone loss to decompose an X-ray image into DRRs whose intensity sum derives the metric of volume and mass."
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,2,Method,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,2.1,Dataset Preparation,"Figure 2 illustrates the overview of the proposed MSKdeX. We collected a dataset of 552 patients subject to the total hip arthroplasty surgery (455 females and 97 males, height 156.9 ± 8.3 cm, weight 57.5 ± 11.9 kg, BMI 23.294 ± 3.951 [mean ± std]). Ethical approval was obtained from the Institutional Review Boards of the institutions participating in this study (IRB approval numbers: 15056-3 for Osaka University and 2019-M-6 for Nara Institute of Science and Technology). We acquired a pair of pre-operative X-ray and CT images from each patient, assuming consistency in bone shape, lean muscle mass, and muscle volume. Automated segmentation of individual bones and muscles was obtained from CT [18]. Three different intensity conversions were applied to the segmented CT; 1) the original intensity, 2) intensity of 1.0 for voxels inside the structure and 0.0 for voxels outside to estimate muscle volume, 3) intensity corresponding to the lean muscle mass density based on a conversion function from the Hounsfield unit (HU) to the mass density [19,20] to estimate lean muscle mass. Following [19], we assumed the voxels with less than -30 HU consisted of the fat, more than +30 HU consisted of the lean muscle, and the voxels in between -30 to +30 HU contained the fat and lean muscle with the ratio depending on linear interpolation of the HU value. The mass of the lean muscle was calculated by the conversion function proposed in [20]. Then, object-wise DRRs for the three conversions were generated for each segmented individual object (bone/muscle) region. (Note: When we refer to a DRR in this paper, it is objectwise.) We call the three types of the DRRs weighted volume DRR (WVDRR), volume DRR (VDRR), and mass DRR (MDRR). The intensity sum of VDRR and MDRR amounts to each object's muscle volume (cm 3 ) and lean muscle mass (g), respectively. (Note: ""Muscle volume"" includes the fat in addition to lean muscle.) The summation of all the objects of WVDRRs becomes an image with a contrast similar to the real X-ray image used to calculate the reconstruction gradient correlation (GC) loss [17,22]. A 2D-3D registration [21] of each bone between CT and X-ray image of the same patient was performed to obtain its DRR aligned with the X-ray image, which is used in the proposed partially aligned training.Since muscles deform depending on the joint angle, they are not aligned. Instead, we exploited the invariant property of muscles using the newly proposed intensity-sum loss."
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,2.2,Model Training,"We train a decomposition model G to decompose an X-ray image into the DRRs = {V DRR, M DRR, W V DRR} to infer the lean muscle mass and muscle volume, adopting CycleGAN [23]. The model backbone is replaced with HRNet [24]. The GAN loss L up GAN we use is formulated in supplemental materials.Structural Consistency. We call the summation of a DRR over all the channels (objects) the virtual X-ray image defined as, where I DRR i is the i-th object image of a DRR. We applies reconstruction GC loss [17] to maintain the structure consistency between an X-ray image and decomposed DRR, where G(I X ) W V DRR is the decomposed WVDRR. However, we do not apply reconstruction GC loss for VDRR and MDRR because of lacking attenuation coefficient information. Instead, we propose inter-DRR/intra-object GC loss L β GC defined asto chain the structural constraints from WVDRR to VDRR and MDRR, where the, and G(I X ) MDRR i are i-th object image of the decomposed WVDRR, VDRR, and MDRR, respectively. The st(•) operator stops the gradient from being back-propagated in which the decomposed VDRR and MDRR are expected to be structurally closer to WVDRR (not vice-versa) to stabilize training. Thus, our structural consistency constant L up GC is defined aswhere the λ gca balances the two GC losses.Intensity Sum Consistency. Unlike general images, our DRRs embedded specific information so that the intensity sum represents physical metrics (mass and volume). Furthermore, the conventional method did not utilize the paired information of an X-ray image and DRR (obtained from the same patient). We took advantage of the paired information, proposing the object-wise intensitysum loss, a simple yet effective metric invariant to patient pose and projection direction, for quantitative learning. The OWIS loss L IS is defined as:where I DRR i and S(•) are the i-th object image of DRR and the intensity summation operator (sum over the intensity of an image), respectively. The H and W are the image height and weight, respectively, served as temperatures for numeric stabilizability. The intensity consistency objective L all IS is defined asPartially Aligned Training. A previous study [15] suggested that supervision by the aligned (paired) data can improve the quantitative translation. Therefore, we incorporated 2D-3D registration [21] to align the pelvis and femur DRRs with the paired X-ray images for partially aligned training to improve overall performance, including muscle metrics estimation. We applied L1 and GC loss to maintain quantitative and structural consistencies, respectively. However, we preclude using GAN loss and feature matching loss to avoid the training burden by additional discriminators. The paired bone loss for a DRR is defined aswhere the K is a set of indexes containing aligned bone indexes. The N b is the size of the set K. The λ l1 tries to balance structural faithfulness and quantitative accuracy. The objective of partially aligned pixel-wise learning is defined asFull Objective. The full objective, aiming for realistic decomposition while maintaining structural faithfulness and quantitative accuracy, is defined aswhere the λ is re-weights the penalty on the proposed OWIS loss. "
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,3,Experiments and Results,"The automatic segmentation results of 552 CTs were visually verified, and 13 cases with severe segmentation failures were omitted from our analysis, resulting in 539 CTs. Four-fold cross-validation was performed, i.e., 404 or 405 training data and 134 or 135 test data per fold. The baseline of our experiment was the vanilla CycleGAN with the reconstruction GC loss proposed in [17]. We evaluated the predicted lean muscle mass and volume using the ground truth derived from 3D CT images with three metrics, Pearson correlation coefficient (PCC), intra-class correlation coefficient (ICC), and mean absolute error (MAE). Additionally, we evaluated the image quality of predicted DRRs of the bones by comparing them with the aligned DRRs using peak-signal-noise-ratio (PSNR) and structural similarity index measure (SSIM). Implementation details are described in supplemental materials. Figure 3 shows the prediction results on the gluteus medius and iliacus muscles. The conventional method (without using the intensity constraints) resulted in low PCCs of 0.441 and 0.522 for the lean muscle mass and muscle volume estimations, respectively, for the gluteus medius, and 0.318 and 0.304, respectively, for the iliacus. Significant improvements by the proposed method were observed, achieving high PCCs of 0.877 and 0.901 of the lean muscle mass and muscle volume estimations, respectively, for the gluteus medius, and 0.865 and 0.873, respectively, for the iliacus. Figure 4 visualized the decomposed VDRR and MDRR of four objects of a representative case. Our method (MSKdeX) reduced the hallucinating features in the decomposed DRRs by the proposed losses. The overall intensity of the conventional method was clearly different from the reference, while the proposed method decomposed the X-ray image considering the structural faithfulness and quantitative accuracy, outperforming the conventional method significantly. Table 1 shows evaluation for other objects. Statistical test (one-way ANOVA) was performed on the conventional and proposed methods using prediction absolute error, where the differences are significant (p < 0.001) for all the objects. More detailed results and a visualization video can be found in supplemental materials. Ablation Study. We performed ablation studies to investigate the impact of proposed OWIS loss and the use of aligned bones using 404 training and 135 test data. The re-weighting parameter λ is of 0, 10, and 1000 with and without the partially aligned training L B was tested. The λ is of 0 without partially aligned training [λ is = 0 (False)] is considered our baseline. The results of the ablation study were summarized in Table 2, where the bold font indicated the best setting in a column. We observed significant improvements from the baseline by both proposed features, OWIS loss and partially aligned training L B .The average PCC for the muscles was improved from 0.457 to 0.826 by adding the OWIS loss (λ is = 100) and to 0.796 by adding the bone loss, while their combination achieved the best average PCC of 0.855, demonstrating the superior ability of quantitative learning of the proposed MSKdeX. The results also suggested that the weight balance for loss terms needs to be made to achieve the best performance. More detailed results are shown in supplemental materials.  "
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,4,Summary,"We proposed MSKdeX, a method for fine-grained estimation of the lean muscle mass and volume from a plain X-ray image (2D) through the musculoskeletal decomposition, which, in fact, recovers CT (3D) information. Our method decomposes an X-ray image into DRRs of objects to infer the lean muscle mass and volume considering the structural faithfulness (by the gradient correlation loss chain) and quantitative accuracy (by the object-wise intensity-sum loss and aligned bones training), outperforming the conventional method by a large margin as shown in Sect. 3. The results suggested a high potential of MSKdeX for opportunistic screening of musculoskeletal diseases in routine clinical practice, providing a new approach to accurately monitoring musculoskeletal health. The aligned bone DRRs positively affected the quantification of the density and volume of the muscles as shown in the ablation study in Sect. 3, implying the deep connection between muscles and bones. The prediction of muscles overlapped with the pelvis in the X-ray image can leverage the strong pixel-wise supervision by the aligned pelvis's DRR, which can be considered as a type of calibration.Our future works are the validation with a large-scale dataset and extension to the decomposition into a larger number of objects."
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Fig. 1 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Fig. 2 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Fig. 3 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Fig. 4 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Table 1 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Table 2 .,
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,1,Introduction,"Abdominal organ segmentation from medical images is an essential work in clinical diagnosis and treatment planning of abdominal lesions [17]. Recently, deep learning methods based on Convolution Neural Network (CNN) have achieved impressive performance in medical image segmentation tasks [2,24]. However, their success relies heavily on large-scale high-quality pixel-level annotations that are too expensive and time-consuming to obtain, especially for multiple organs in 3D volumes. Weakly supervised learning with a potential to reduce annotation costs has attracted great attention. Commonly-used weak annotations include dots [6,11], scribbles [1,11,13,15], bounding boxes [5], and imagelevel tags [20,25]. Compared with the other weak annotations, scribbles can provide more location information about the segmentation targets, especially for objects with irregular shapes [1]. Therefore, this work focuses on exploring high-performance models for multiple abdominal organ segmentation based on scribble annotations.Training CNNs for segmentation with scribble annotations has been increasingly studied recently. Existing methods are mainly based on pseudo label learning [11,15], regularized losses [10,18,22] and consistency learning [7,13,26]. Pseudo label learning methods deal with unannotated pixels by generating fake semantic labels for learning. For example, Luo et al. [15] introduced a network with two slightly different decoders that generate dynamically mixed pseudo labels for supervision. Liang et al. [11] proposed to leverage minimum spanning trees to generate low-level and high-level affinity matrices based on color information and semantic features to refine the pseudo labels. Arguing that the pseudo label learning may be unreliable, Tang et al. [22] introduced the Conditional Random Field (CRF) regularization loss for image segmentation directly. Obukhov et al. [18] proposed to incorporate the gating function with CRF loss considering the directionality of unsupervised information propagation. Recently, consistency strategies that encourage consistent outputs of the network for the same input under different perturbations have achieved increasing attentions. Liu et al. [13] introduced transformation-consistency based on an uncertaintyaware mean teacher [4] model. Zhang et al. [26] proposed a framework composed of mix augmentation and cycle consistency. Although these scribble-supervised methods have achieved promising results, their performance is still much lower than that of fully-supervised training, leaving room for improvement.Differently from most existing weakly supervised methods that are designed for 2D slice segmentation with a single or few organs, we propose a highly optimized 3D triple-branch network with one encoder and three different decoders, named TDNet, to learn from scribble annotations for segmentation of multiple abdominal organs. Particularly, the decoders are assigned with different dilation rates [25] to learn features from different receptive fields that are complementary to each other for segmentation, which also improves the robustness of dealing with organs at different scales as well as the feature learning ability of the shared encoder. Considering the features at different scales learned in these decoders, we fuse these multi-dilated predictions to obtain more accurate soft pseudo labels rather than hard labels [15] that tend to be over-confidence predictions. For more stable unsupervised learning, we use voxel-wise uncertainty to rectify the soft pseudo labels and then impose consistency constraints on the output of each branch. In addition, we extend the consistency to the class-related information level [23] to constrain inter-class affinity for better distinguishing them. Specifically, we generate the class affinity matrices in different decoders and encourage them to be consistent after projection in different views. The contributions of this paper are summarized as follows: 1) We propose a novel 3D Triple-branch multi-Dilated network called TDNet for scribblesupervised segmentation. By equipping with varying dilation rates, the network can better leverage multi-scale context for dealing with organs at different scales.2) We propose two novel consistency loss functions, i.e., Uncertainty-weighted Soft Pseudo label Consistency (USPC) loss and Multi-view Projection-based Class-similarity Consistency (MPCC) loss, to regularize the prediction from the pixel-wise and class-wise perspectives respectively, which helps the segmentation network obtain reliable predictions on unannotated pixels. 3) Experiments results show our proposed method outperforms five existing scribble-supervised methods on the public dataset WORD [17] for multiple abdominal organ segmentation."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,2,Method,"Figure 1 shows the proposed framework for scribble-supervised medical image segmentation. We introduce a network with one encoder and three decoders with different dilation rates to learn multi-scale features. The decoders' outputs are averaged to generate a soft pseudo label that is rectified by uncertainty and then used to supervise each branch. To better deal with multi-class segmentation, a class similarity consistency loss is also used for regularization.For the convenience of following description, we first define several mathematical symbols. Let X, S be a training image and the corresponding scribble annotation, respectively. Let C denote the number of classes for segmentation, and Ω = Ω S ∪ Ω U denote the whole set of voxels in X, where Ω S is the set of labeled pixels annotated in S, and Ω U is the unlabeled pixel set."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,2.1,Triple-Branch Multi-Dilated Network (TDNet),"As shown in Fig. 1(a), the proposed TDNet consists of a shared encoder (θ e ) and three independent decoders (θ d1 , θ d2 , θ d3 ) with different dilation rates to mine unsupervised context from different receptive fields. Specifically, decoders using convolution with small dilation rates can extract detailed local features but their receptive fields are small for understanding a global context. Decoders using convolution with large dilation rates can better leverage the global information but may lose some details for accurate segmentation. In this work, our TDNet is implemented by introducing two auxiliary decoders into a 3D UNet [3]. The dilation rate in the primary decoder and the two auxiliary decoders are 1, 3 and 6 respectively, with the other structure parameters (e.g., kernel size, channel number etc.) being the same in the three decoders. To further introduce perturbations for obtaining diverse outputs, the three branches are initialized with Kaiming initialization, Xavier and Normal initialization methods, respectively. In addition, the bottleneck's output features are randomly dropped out before sending into the auxiliary decoders. The probability prediction maps obtained by the three decoders are denoted as P 1 , P 2 and P 3 , respectively."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,2.2,Pixel-Wise and Class-Wise Consistency,"Uncertainty-Weighted Soft Pseudo Label Consistency (USPC). As the three decoders capture features at different scales that are complementary to each other, an ensemble of them would be more robust than a single branch. Therefore, we take an average of P 1 , P 2 , P 3 to get a better soft pseudo label P = (P 1 + P 2 + P 3 )/3 that is used to supervise each branch during training. However, P may also contain noises and be inaccurate, and it is important to highlight reliable pseudo labels while suppressing unreliable ones. Thus, we propose a regularization term named Uncertainty-weighted Soft Pseudo label Consistency (USPC) between P n (n = 1, 2, 3) and P :where Pi refers to the prediction probability at voxel i in P , and Pn,i is the corresponding prediction probability at voxel i in Pn . KL() is the Kullback-Leibler divergence. w i is the voxel-wise weight based on uncertainty estimation:where the uncertainty is estimated by entropy. c is the class index, and P c i means the probability for class c at voxel i in the pseudo label. Note that a higher uncertainty leads to a lower weight. With the uncertainty-based weighting, the model will be less affected by unreliable pseudo labels."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Multi-view Projection-Based Class-Similarity Consistency (MPCC).,"For multi-class segmentation tasks, it is important to learn inter-class relationship for better distinguishing them. In addition to using L USP C for pixel-wise supervision, we consider making consistency on class relationship across the outputs of the decoders as illustrated in Fig. 1. In order to save computing resources, we project the soft pseudo labels along each dimension and then calculate the affinity matrices, which also strengthens the class relationship information learning. We first project the soft prediction map of the n-th decoder P n ∈ R C×D×H×W in axial view to a tensor with the shape of C × 1 × H × W . It is reshaped into C ×(W H) and multiplied by its transposed version, leading to a class affinity matrixSimilarly, P n is projected in the sagittal and coronal views, respectively, and the corresponding normalized class affinity matrices are denoted as Q sagittal n and Q coronal n , respectively. Here, the affinity matrices represents the relationship between any pair of classes along the dimensions. Then we constraint the consistency among the corresponding affinity matrices by Multi-view Projection-based Class-similarity Consistency (MPCC) loss:where v ∈ {axial, sagittal, coronal} is the view index, and Qv is the average class affinity matrix in a certain view obtained by the three decoders."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,2.3,Overall Loss Function,"To learn from the scribbles, the partially Cross-Entropy (pCE) loss is used to train the network, where the labeled pixels are considered to calculate the gradient and the other pixels are ignored [21]:where S represents the one-hot scribble annotation, and Ω S is the set of labeled pixels in S. The total object function is summarized as:where α t and β t are the weights for the unsupervised losses. Following [13], we define α t based on a ramp-up function: α t = α • e (-5(1-t/tmax) 2 ) , where t denotes the current training step and t max is the maximum training step. We define β t = β • e (-5(1-t/tmax) 2 ) in a similar way. In this way, the model can learn accurate information from scribble annotations, which also avoids getting stuck in a degenerate solution due to low-quality pseudo labels at an early stage."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3,Experiments and Results,
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3.1,Dataset and Implementation Details,"We used the publicly available abdomen CT dataset WORD [17] for experiments, which consists of 150 abdominal CT volumes from patients with rectal cancer, prostate cancer or cervical cancer before radiotherapy. Each CT volume contains 159-330 slices of 512×512 pixels, with an in-plane resolution of 0.976 × 0.976 mm and slice spacing of 2.5-3.0 mm. We aimed to segment seven organs: the liver, spleen, left kidney, right kidney, stomach, gallbladder and pancreas. Following the default settings in [17], the dataset was split into 100 for training, 20 for validation and 30 for testing, respectively, where the scribble annotations for foreground organs and background in the axial view of the training volumes had been provided and were used in model training. For pre-processing, we cut off the Hounsfield Unit (HU) values with a fixed window/level of 400/50 to focus on the abdominal organs, and normalized it to [0, 1]. We used the commonlyadopted Dice Similarity Coefficient (DSC), 95% Hausdorff Distance (HD 95 ) and the Average Surface Distance (ASD) for quantitative evaluation.Our framework was implemented in PyTorch [19] on an NVIDIA 2080Ti with 11 GB memory. We employed the 3D UNet [3] as the backbone network for all experiments, and extended it with three decoders by embedding two auxiliary decoders with different dilation rates, as detailed in Sect. 2.1. To introduce perturbations, different initializations were applied to each decoder, and random perturbations (ratio = (0, 0.5)) were introduced in the bottleneck before the auxiliary decoders. The Stochastic Gradient Descent (SGD) optimizer with momentum of 0.9 and weight decay of 10 -4 was used to minimize the overall loss function formulated in Eq. 5, where α=10.0 and β=1.0 based on the best performance on the validation set. The poly learning rate strategy [16] was used to decay learning rate online. The batch size, patch size and maximum iterations t max were set to 1, [80, 96, 96] and 6 × 10 4 respectively. The final segmentation results were obtained by using a sliding window strategy. For a fair comparison, we used the primary decoder's outputs as the final results during the inference stage and did not use any post-processing methods. Note that all experiments were conducted in the same experimental setting. The existing methods are implemented with the help of open source codebase from [14].Table 1. Quantitative comparison between our method and existing weakly supervised methods on WORD testing set. * denotes p-value < 0.05 (paired t-test) when comparing with the second place method [15]. The best values are highlighted in bold."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Organ,FullySup [3] pCE TV [9] USTM [13]  
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3.2,Comparison with Other Methods,"We compared our method with five weakly supervised segmentation methods with the same set of scribbles, including pCE only [12], Total Variation Loss (TV) [9], Uncertainty-aware Self-ensembling and Transformation-consistent Model (USTM) [13], Entropy Minimization (EM) [8] and Dynamically Mixed Pseudo Labels Supervision (DMPLS) [15]. They were also compared with the upper bound by using dense annotation to train models (FullySup) [3]. The results in Table 1 show that our method leads to the best DSC, ASD and HD 95 .Compared with the second best method DMPLS [15], the average DSC was increased by 2.67 percent points, and the average ASD and HD 95 were decreased by 5.44 mm and 16.16 mm, respectively. It can be observed that TV [9] obtained a worse performance than pCE, which is mainly because that method classifies pixels by minimizing the intra-class intensity variance, making it difficult to achieve good segmentation due to the low contrast. Figure 2 shows a visual comparison  between our method and the other weakly supervised methods on the WORD dataset (word 0014.nii). It can be obviously seen that the results obtained by our method are closer to the ground truth, with less mis-segmentation in both slice level and volume level."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3.3,Ablation Experiment,"We then performed ablation experiments to investigate the contribution of each part of our method, and the quantitative results on the validation set are shown in Table 2, where L USP C (-ω) means using L USP C without pixelwise uncertainty rectifying. Baseline refers to a triple-branch model with different initializations and random feature-level dropout in the bottleneck, supervised by pCE only. It can be observed that by using L USP C (-ω) with mutiple decoders, the model segmentation performance is greatly enhanced with average DSC increasing by 7.70%, ASD and HD 95 decreasing by 16.11 mm and 48.87 mm, respectively. By equipping each decoders with different dilation rates, the model's performance is further improved, especially in terms of ASD and HD 95 , which proves our hypothesis that learning features from different scales can improve the segmentation accuracy. Replacing L USP C (-ω) with L USP C further improved the DSC to 84.21%, and reduced the ASD and HD 95 by 0.52 mm and 1.01 mm through utilizing the uncertainty information. Visual comparison in Fig. 3 demonstrates that over-segmentation can be mitigated by using different dilation rates in the three decoders, and using the uncertainty-weighted pseudo labels can further improve the segmentation accuracy with small false positive regions removing.Additionally, Table 2 shows that combining L USP C and L MP CC obtained the best performance, where the average DSC, ASD and HD 95 were 84.75%, 2.64 mm and 7.91 mm, respectively, which demonstrates the effectiveness of the proposed class similarity consistency. In order to find the optimal number of decoders, we set the decoder number to 2, 3 and 4 respectively. The quantitative results in the last three rows of Table 2 show that using three decoders outperformed using two and four decoders."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,4,Conclusion,"In this paper, we proposed a scribble-supervised multiple abdominal organ segmentation method consisting of a 3D triple-branch multi-dilated network with two-level consistency constraints. By equipping each decoder with different dilation rates, the model leverages features at different scales to obtain high-quality soft pseudo labels. In addition to mine knowledge from unannotated pixels, we also proposed USPC Loss and MPCC Loss to learn unsupervised information from the uncertainty-rectified soft pseudo labels and class affinity matrix information respectively. Experiments on a public abdominal CT dataset WORD demonstrated the effectiveness of the proposed method, which outperforms five existing scribble-based methods and narrows the performance gap between weakly-supervised and fully-supervised segmentation methods. In the future, we will explore the effect of our method on sparser labels, such as a volumetric data with scribble annotations on one or few slices."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Fig. 1 .,
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Fig. 3 .,
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Table 2 .,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,1,Introduction,"Breast cancer is the most prevalent form of cancer among women and can have serious physical and mental health consequences if left unchecked [5]. Early detection through mammography is critical for early treatment and prevention [19]. Mammograms provide images of breast tissue, which are taken from two views: the cranio-caudal (CC) view, and the medio-lateral oblique (MLO) view [4]. By identifying breast cancer early, patients can receive targeted treatment before the disease progresses.Deep neural networks have been widely adopted for breast cancer diagnosis to alleviate the workload of radiologists. However, these models often require a large number of manual annotations and lack interpretability, which can prevent their broader applications in breast cancer diagnosis. Radiologists typically focus on areas with breast lesions during mammogram reading [11,22], which provides valuable guidance. We propose using real-time eye tracking information from radiologists to optimize our model. By using gaze data to guide model training, we can improve model interpretability and performance [24].Radiologists' eye movements can be automatically and unobtrusively recorded during the process of reading mammograms, providing a valuable source of data without the need for manual labeling. Previous studies have incorporated radiologists' eye-gaze as a form of weak supervision, which directs the network's attention to the regions with possible lesions [15,23]. Leveraging gaze from radiologists to aid in model training not only increases efficiency and minimizes the risk of errors linked to manual annotation, but also can be seamlessly implemented without affecting radiologists' normal clinical interpretation of mammograms.Mammography primarily detects two types of breast lesions: masses and microcalcifications [16]. The determination of the benign or malignant nature of masses is largely dependent on the smoothness of their edges [13]. The gaze data can guide the model's attention towards the malignant masses. Microcalcifications are small calcium deposits which exhibit irregular boundaries on mammograms [9]. This feature makes them challenging to identify, often leading to missed or false detection by models. Radiologists need to magnify mammograms to differentiate between benign scattered calcifications and clustered calcifications, the latter of which are more likely to be malignant and necessitate further diagnosis. Leveraging gaze data can guide the model to locate malignant calcifications.In this work, we propose a novel diagnostic model, namely Mammo-Net, which integrates radiologists' gaze data and interactive information between CC-view and MLO-view to enhance diagnostic performance. To the best of our knowledge, this is the first work to integrate gaze data into multi-view mammography classification. We utilize class activation map (CAM) [18] to calculate the attention maps for the model. Additionally, we apply pyramid loss to maintain consistency between radiologists' gaze heat maps and the model's attention maps at multiple scales of the pyramid [1]. Our model is designed for singlebreast cases. Mammo-Net extracts multi-view features and utilizes transformerbased attention to mutualize information [21]. Furthermore, there are differences between multi-view mammograms of the same patient, arising from variations in breast shape and density. Capturing these multi-view shared features can be a challenge for models. To address this issue, we develop a novel method called bidirectional fusion learning (BFL) to extract shared features from multi-view mammograms.Our contributions can be summarized as follows:• We emphasize the significance of low-cost gaze to provide weakly-supervised positioning and visual interpretability for the model. Additionally, we develop a pyramid loss that adapts to the supervised process. • We propose a novel breast cancer diagnosis model, namely Mammo-Net. This model employs transformer-based attention to mutualize information and uses BFL to integrate task-related information to make accurate predictions. • We demonstrate the effectiveness of our approach through experiments using mammography datasets, which show the superiority of Mammo-Net.2 Proposed Method"
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,2.1,Overall Architecture,"The pipeline of Mammo-Net is illustrated in Fig. 1. Mammo-Net feeds two-view mammograms of the same breast into two ResNet-style [7] CNN branch networks. We use several ResNet blocks pre-trained on ImageNet [3] to process mammograms. Then, we use global average pooling (GAP) and fully connected layers to compute the feature vectors produced by the model. Before the final residual block, we employ cross-view attention to mutualize multi-view information. Our proposed method employs BFL to effectively fuse multi-view information to improve diagnostic accuracy. Additionally, by integrating gaze data from radiologists, our proposed model is able to generate more precise attention maps.The fusion network combines multi-view feature representations using a stack of linear-activation layers and a fully connected layer, resulting in a classification output."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,2.2,Gaze Supervision,"In this module, we utilize CAM to calculate the attention map for the network by examining gradient-based activations in back-propagation. After that, we employ pyramid loss to make the network attention being consistent with the supervision of radiologists' gaze heat maps, guiding the network to focus on the same lesion areas as the radiologists. This module guides the network to accurately extract pathological features."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Class Activation Map.,"At the final convolutional layer of our model, the activation of the ith feature map f i (x, y) at coordinates (x, y) is associated with a weight w k i for class k. This allows us to generate the attention map H k for class k as:Pyramid Loss. To enhance the learning of important attention areas, we propose a pyramid loss constraint that requires consistency between the network and gaze attention maps. The pyramid loss is based on using a pyramid representation of the attention map:where H is the network attention map generated by the CAM and R is the radiologist's gaze heat map. square error (MSE) between the attention maps generated by the radiologist and the model at each level of the Gaussian pyramid. This allows the model to mimic the attention of radiologists and enhance diagnostic performance. Moreover, the pyramid representation enables the model to learn from the important pathological regions on which radiologists are focusing, without the need for precise pixel-level information. Layernorm is also employed to address the issue of imprecise gaze data. This reduces noise in the consistency process by performing consistency loss only in the regions where radiologist spent most time."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,2.3,Interactive Information,"Transformer-Based Mutualization Model. We use transformer-based attention to mutualize information from the two views at the level of the spatial feature map. For each attention head, we compute embeddings for the source and target pixels. Our model does not utilize positional encoding, as it encodes the relative position of each pixel and is not suitable for capturing information between different views of mammograms [21]. The target view feature maps are transformed into Q, the source view feature maps are transformed into K, and the original source feature maps are transformed into V . We can then obtain a weighted sum of the features from the source view for each target pixel using [21]:Subsequently, the output is transformed into attention-based feature maps X and mutualized with the feature maps Y from the other view. The mutualized feature maps are normalized and used for subsequent calculations:Bidirectional Fusion Learning. To enable the fusion network to retain more of the shared features between the two views and filter out noise, we propose to use BFL to learn a fusion representation that maximizes the cross-view mutual information. The optimization target is to generate a fusion representation I from multi-view representations p v , where v ∈ {cc, mlo}. We employ the Noise-Contrastive Estimation framework [6] to maximize the mutual information, which is a contrastive learning framework:where s(I, p v ) evaluates the correlation between multi-view fused representations and single-view representations [17]:where N (I) is a reconstruction of p v generated by a fully connected network N from I and the Euclidean norm || • || 2 is applied to obtain unit-length vectors.In contrastive learning, we consider the same patient mammograms as positive samples and those from different patient mammograms in the same batch P i v = P v \{p i v } as negative samples [17]. Minimizing the similarity between the same patient mammograms enables the model to learn shared features. Maximizing the dissimilarity between different patient mammograms enhances the model's robustness.In short, we require the fusion representation I to reversely reconstruct multiview representations p v so that more view-invariant information can be passed to I. By aligning the prediction N (I) to p v , we enable the model to decide how much information it should receive from each view.The overall loss function for this module is the sum of the losses defined for each view:"
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,2.4,Loss Function,"We use binary cross entropy loss (BCE) between the network prediction and the ground-truth as the classification loss. In conclusion, we have proposed a total of three loss functions to guide the model training: L BCE , L BF L , and L P yramid . The overall loss function is defined as the sum of these three loss functions, with coefficients λ and μ used to adjust their relative weights:3 Experiments and Results"
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,3.1,Datasets,"Mammogram Dataset. Our experiments were conducted on CBIS-DDSM [12] and INbreast [16]. The CBIS-DDSM dataset contains 1249 exams that have been divided based on the presence or absence of masses, which we used to perform mass classification. The INbreast dataset contains 115 exams with both masses and micro-calcifications, on which we performed benign and malignant classification. We split the INbreast dataset into training and testing sets in a 7:3 ratio. It is worth noting that the official INbreast dataset does not provide image-level labels, so we obtained these labels following Shen et al. [20].Eye Gaze Dataset. Eye movement data was collected by reviewing all cases in INbreast using a Tobii Pro Nano eye tracker. The scenario is shown in Appendix and can be accessed at https://github.com/JamesQFreeman/MicEye. Participated radiologist has 11 years of experience in mammography screening."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,3.2,Implementation Details,"We trained our model using the Adam optimizer [10] with a learning rate of 10 -4 (partly implemented by MindSpore). To overcome the problem of limited data, we employed various data augmentation techniques, including translation, rotation, and flipping. To address the problem of imbalanced classes, we utilized a weighted loss function that assigns higher weights to malign cases in order to balance the number of benign and malign cases. The coefficients λ and μ of L overall were set to 0.5 and 0.2, respectively, based on 5-fold cross validation on the training set. The network was trained for 300 epochs. We used Accuracy (ACC) and the Area Under the ROC Curve (AUC) [25] as our evaluation metrics, and we selected the final model based on the best validation AUC. Considering the relatively small size of our dataset, we used ResNet-18 as the backbone of our network.  1, we compare our model to other methods and find that our model performs better. Lopez et al. [14] proposed the use of hypercomplex networks to mimic radiologists. By leveraging the properties of hypercomplex algebra, the model is able to continually process two mammograms together. Lee et al. [26] proposed a 2-channel approach that utilizes a Gaussian model to capture the spatial correlation between lesions across two views, and an LT-GAN to achieve a robust mammography classification. We also compare our model with other methods that use eye movement supervision as shown in Table 1. The GA-Net [23] proposed a ResNet-based model with class activation mapping guided by eye gaze data. We developed a multi-view model using this approach for a fair comparison, and found that our method performed better. We believe that one possible reason for the inferior performance of GA-Net compared to Mammo-Net might be the use of a simple MSE loss by GA-Net, which neglects the coarse nature of the gaze data. Jiang et al. [8] proposed a Double-model that fuses gaze maps with original images before training. However, this model did not consider the gap between research and clinical workflow. This model requires gaze input during both the training and inference stages, which limits its practical use in hospitals without eyetrackers. In contrast, our method does not rely on gaze input during inference stage."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,3.3,Results and Analysis,"Visualization. Figure 2 illustrates the visualization of our proposed model on three representative exams from the INbreast dataset that includes masses, calcifications, and a combination of both. For each exam, we present gaze heat maps generated from eye movement data. The preprocessing process is shown in Fig. 5 (see Appendix). To make an intuitive comparison, we exhibit attention maps generated by the model under both unsupervised and gaze-supervised cases. Each exam is composed of two views, i.e., the CC-view and the MLO-view. More exams can be found in Fig. 6 (see Appendix).The results of the visualization demonstrate that the model's capability in localizing lesions becomes more precise when radiologist attention is incorporated in the training stage. The pyramid loss improves the model's robustness even when the radiologist's gaze data is not entirely focused on the breast. This intuitively demonstrates the effectiveness of training the model with eye-tracking supervision.Ablation Study. We perform an ablation analysis to assess each component (radiologist attention, cross-view attention and BFL) in Mammo-Net. Table 1 suggests that each part of the proposed framework contributes to the increased performance. This shows the benefits of adapting the model to mimic the radiologist's decision-making process."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,4,Conclusion and Discussion,"In this paper, we have developed a breast cancer diagnosis model to mimic the radiologist's decision-making process. To achieve this, we integrate gaze data as a form of weak supervision for both lesion positioning and interpretability of the model. We also utilize transformer-based attention to mutualize multi-view information and further develop BFL to fully fuse multi-view information. Our experimental results on mammography datasets demonstrate the superiority of our proposed model. In future work, we intend to explore the use of scanning path analysis as a means of obtaining insights into the pathology-relevant regions of lesions."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Fig. 1 .,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Fig. 2 .,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Table 1 .,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,.889 0.849 Performance Comparison.,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 7.
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,1,Introduction,"The increased availability of radiological data and rapid advances in medical image analysis has led to an exponential growth in prediction models that utilize features extracted from clinical imaging scans to detect and diagnose diseases and predict response to treatment [1][2][3][4]. However, variations in the acquisition and reconstruction of CT scans result in quantitative image features with poor reproducibility [5,6]. Several studies have demonstrated that differences in dose, slice thickness, reconstruction method, and reconstruction kernel negatively impact radiomic feature reproducibility. Predicting prediction model performance is confounded by how medical images are acquired and reconstructed [7][8][9][10]. Many studies have developed techniques to address sources of CT parameter variability [5,11]. However, inverse problems such as recovering full radiation dose scans from lower dose scans are inherently ill-posed. A range of outputs may be possible when using image restoration algorithms, including potential artifacts that impact the performance of downstream algorithms. Like GANs or variational autoencoders, normalizing flows is a method for learning complex data representations but with an explicit ability to infer the output as a probability distribution and with the added benefit of more stable training [12]. While normalizing flows has shown success in image synthesis tasks for natural images [13], few studies have examined them in medical image harmonization tasks. Denker et al. employed a normalizing flow model conditioned on LDCT reconstruction by filtered backprojection to improve reconstruction quality from the raw sinogram data [14]. This paper presents CTFlow, which aims to utilize normalizing flows to harmonize variations in image appearance of CT scans by maximizing the explicit likelihood of a target condition (e.g., 100% dose, medium kernel, 1 mm slice thickness) given a CT scan that was acquired using different parameters (50% dose, sharp kernel, 1 mm slice thickness). Normalizing flow has two important advantages: 1) the translated low-dose CTs have minimal artifacts because the output is a maximum likelihood estimate that closely matches the target reference distribution, and 2) unlike GANs, which are susceptible to mode collapse, CTFlow can generate multiple solutions to reduce inference uncertainty. We demonstrate how CTFlow compares with current state-of-the-art methods for mitigating dose and reconstruction kernel differences. We evaluated using image quality metrics and a lung nodule detection task."
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2,Methods,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2.1,Datasets,"This study used two unique datasets: (1) the UCLA low-dose chest CT dataset, a collection of 186 exams acquired using Siemens CT scanners at an equivalent dose of 2 mGy following an institutional review board-approved protocol. The raw projection data of scans were exported, and Poisson noise was introduced, as described in Zabic et al. [15], at levels equivalent to 10% of the original dose. Projection data were then reconstructed into an image size of 512 × 512 using three reconstruction kernels (smooth, medium, sharp) at 1.0 mm slice thickness. The dataset was split into 80 scans for training, 20 for validation, and 86 for testing. (2) AAPM-Mayo Clinic Low-Dose CT ""Grand Challenge"" dataset, a publicly available Grand Challenge dataset consisting of 5,936 abdominal CT images from 10 patient cases reconstructed at 1.0 mm slice thickness. Each case consists of a paired 100% ""normal dose"" scan and a simulated 25% ""low dose"" scan. Images from eight patient cases were used for training, and two cases were reserved for validation. All images were randomly cropped into patches of 128 × 128 pixels, generated from regions containing the body. This dataset was only used for evaluating image quality against other harmonization techniques."
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2.2,Normalizing Flows,"In this section, we describe the normalizing flows and modifications that were made to improve computational efficiency. Deterministic approaches to image translation (e.g., using a convolutional neural network) attempt to find a mapping function y = g θ (x) that takes an input image x and outputs an image y that mimics the appearance of a target condition. For example, x could be an image acquired using a low dose protocol (e.g., 25% dose, smooth kernel), and y represents the image acquired at the target acquisition and reconstruction parameter (e.g., 100% dose, medium kernel). Flow-based image translation aims to approximate the density function y|x (y|x, θ) using maximum likelihood estimation.  which can be trained by maximizing the log-likelihood. In practice, a multilayer flow operation is preferred because a single-layer flow cannot represent complex non-linear relationships within the data. f is decomposed into a series of invertible neural network layers h n where h n = f n θ (h n-1 ; e(x)), n represents the number of layers, and e(x) represents a deep convolutional neural network that extracts salient feature maps of x upon which the flow layers are conditioned. For an N-layer flow model, the objective is to maximizeOnce the training is complete, the decoding function g θ (z; x) is applied using random latent variable z, which is drawn from the independent and identically distributed Gaussian density function. The use of z allows us to generate a range of possible restored images y , conditioned on the same input image x. Flow Layers. Flow layers must meet two requirements: 1) be invertible and 2) be a tractable Jacobian determinant. To compute the second term in Eq. 2, we apply the triangulation trick developed by Dinh et al. [16] We use affine coupling layers with a conditional variable. We first equally split the channels into h n 1 , h n 2 and apply an affine transformation on h n 2 while keeping an identity transform on h n 1 . We apply scale and shift factor computed by a shallow convolutional neural network (CNN) given h n 1 in spatial coordinates i, j to compute the n + 1 layer flow of h n+1 2 . Finally, we concatenated the splitting components back to obtain the next layer flowThus, by definition, Jacobian of h n+1 is a lower triangular matrix. Figure 2a depicts the components of the flow module, which are described below:• Activation normalization: A channel-wise batch normalization [17] was applied, yielding an output with zero mean and unit variance. • Invertible 1 x 1 conv: Following the approach in [18], we utilized a learnable 1x1 convolution h n i,j = Wh n-1 i,j where W is a square matrix with dimension c × c (c is the number of channels). Each spatial element i, j in h is multiplied by this 1x1 convolution matrix. The log determinant is computed using PLU factorization.• Feature conditional affine. We compute the scale and shift factor from e(x) again using a shallow CNN to apply the n-th layer flow transformation h. The motivation is to impose a relationship between feature maps extracted e(x) and activation maps h.The deep convolutional neural network extractor e(x) is based on Residual-in-Residual Dense Blocks (RRDB) [19]. This network contains 14 RRDB blocks and is our feature extractor for low-dose images. The RRDB network was trained using L 1 loss for 60k iterations. The batch size was 16 and the learning rate was set to 2e-4. The Adam optimizer was used with β 1 = 0.9, β 2 = 0.99. After training, all layers of RRDB were frozen and used only for feature extraction. Feature maps were derived from 2, 6, 10, 14 block outputs. Afterward, the outputs of each block were concatenated into e(x).Multiscale Architecture. Since the flow approach is invertible, input x and latent space vector z must have the same dimensions. However, in most cases, y|x (y|x, θ) is a lowdimensional manifold in high-dimensional input space. Computation is inefficient when a flow model is imposed with a higher dimensionality than the dimension of true latent space. Given the multiscale architecture in RealNVP, we can simplify the model and improve the density estimation at multiple levels. The overall multiscale architecture is depicted in Fig. 2b, where we equally divide each output z into (z out , z next ), while recursively feeding z next to the next level. Once all levels have been reached, z out is outputted, representing the maximum log-likelihood estimation.Network Training. We trained CTFlow using a batch size of 16 and 50k iterations. The learning rate was set to 1e-4 and halved at 50%, 75%, 90%, and 95% of the total training steps. A negative log-likelihood loss was used."
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2.3,Experiments,"We conducted two experiments to evaluate CTFlow: image quality metrics and impact on the performance of a lung nodule computer-aided detection (CADe) algorithm.Image Quality. Using the Grand Challenge dataset, we assessed image quality and compared it with other previously published low-dose CT denoising techniques. We computed image quality metrics using the peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) [20]. Our comparison was conducted using adversarial-based approaches (WGAN using mean squared error loss, WGAN using perceptual loss, and a 3D spectral-norm GAN called SNGAN [21][22][23], previously developed in our group), a convolutional neural networkbased approach (SRResNet) [24], and a denoising algorithm based on collaborative filtering Block-matching and 3D filtering (BM3D) [25].Nodule Detection. We evaluated the ability of CTFlow to harmonize differences in reconstruction kernels and their effect on the performance of a lung nodule detection algorithm. Our CADe system was based on the RetinaNet model, a composite model comprised of a backbone network called feature pyramid net and two subnetworks responsible for object classification with bounding box regression. The model was trained and validated on the LIDC-IDRI dataset, a public de-identified dataset of diagnostic and low-dose CT scans with annotations from four experienced thoracic radiologists. As part of the training process, we only considered nodules annotated by at least three readers in the LIDC dataset. A total of 7,607 slices (with 4,234 nodule annotations) were used for training and 2,323 slices (with 1,454 nodule annotations) for testing in a single train-test split. A bounding box was then created around the union of all the annotator contours to serve as the reference for the detection model. After training for 200 epochs with Focal loss and Adam optimizer, the model achieved an average precision (AP@0.5) of 0.62 on the validation set.We hypothesized that the CTFlow models should yield better consistency in lung nodule detection performance compared with not normalizing or other state-of-the-art methods. As a comparison, we trained a 3D SNGAN model using the same training and validation set as CTFlow to perform the same task. We trained three separate CTFlow and SNGAN models to map scans reconstructed using smooth, medium, or sharp kernels to a reference condition. We computed the F1 score (the harmonic mean of the CADe algorithm's precision and recall) when executing the model on the CTFlow and SNGAN normalized scans. We then determined the Concordance Correlation Coefficient [26] on the F1 scores, comparing the F1 score of the model when executed on the normalized scan to when executed on the reference scan."
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,3,Results,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,3.1,Network Training,"On the Grand Challenge dataset, CTFlow took 3 days to train on an NVIDIA RTX 8000 GPU. The peak GPU memory usage was 39 GB. Unlike GANs that required two loss functions, our network was optimized with only one loss function. The negative log-likelihood loss was stable and decreased monotonically. "
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,3.2,Image Quality,"Table 1 summarizes the results for image quality metrics, while Fig. 3 depicts the same representative slice outputted by each method. While BM3D and SRResNet generated the highest PSNR and SSIM, the images were overly smooth and lacked high-frequency components. Important texture details were lost in the restoration, which may negatively impact downstream tasks (e.g., radiologist interpretation, CAD algorithm performance) that rely on maintaining texture features to characterize lesions. CTFlow achieved 6% better perceptual quality than SNGAN. "
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,3.3,Nodule Detection,"Table 2 summarizes the CCC values for each kernel pair. McBride [27] suggested the following guidelines for interpreting Lin's concordance correlation coefficient. Poor: <0:9; moderate: 0.90 to 0.95; substantial: 0.95 to 0.99; perfect: >0.99 and above. CTFlow achieved CCC scores within the ""perfect"" range when assessing the agreement in F1 scores when given images reconstructed using varying kernels. "
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,4,Conclusion,"We developed CTFlow, a normalizing flows approach to mitigating variations in CT scans. We demonstrated that CTFlow achieved consistent performance across image quality metrics, yielding the best perceptual quality score. Moreover, CTFlow was better than a GAN-based method in maintaining consistent lung nodule detection performance. Compared to generative models, the normalizing flows approach offers exact and efficient likelihood computation and generates diverse outputs that are closer to the target distribution.We note several limitations of this work. In our evaluations, we trained separate CTFlow and comparison models for each mapping (e.g., transforming a 'smooth' kernel to a 'medium' kernel scan), allowing us to troubleshoot models more easily. A single model conditioned on different doses and kernels would be more practical. Also, CTFlow depends on tuning a variance parameter; better PSNR and SSIM may have been achieved with the optimization of this parameter. Finally, this study focused on mitigating the effect of a single CT parameter, either dose (in image quality) or kernel (in nodule detection). In the real world, multiple CT parameters interact (dose and kernel); these more complex interactions are being investigated as part of future work.One underexplored area of normalizing flow is its ability to generate the full distribution of possible outputs. Using this information, we can estimate where high uncertainty exists in the model output, providing information to downstream image processing steps, such as segmentation, object detection, and classification. For example, Chan et al. [28] applied an approximate Bayesian inference scheme based on posterior regularization to improve uncertainty quantification on covariate-shifted data sets, resulting in improved prognostic models for prostate cancer. Investigating how this uncertainty can be incorporated into downstream tasks, such as our lung nodule CADe algorithm, is also part of future work."
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Fig. 1 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Figure 1,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Fig. 2 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Fig. 3 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Table 1 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Table 2 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Acknowledgments,". This work was supported by the National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health under awards R56 EB031993 and R01 EB031993. The authors thank John M. Hoffman, Nastaran Emaminejad, and Michael McNitt-Gray for providing access to the UCLA low-dose CT dataset. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,1,Introduction,"Retinal diseases are one of the most common eye disorders, which can lead to vision impairment and blindness if left untreated. Computer-aided diagnosis has been increasingly used as a tool to detect ophthalmic diseases at the earliest possible time and to ensure rapid treatment. Optical Coherence Tomography (OCT) [6] is an innovative imaging technique with the ability to capture micrometer-resolution images of retina layers, which provides a deeper view compared to alternative methods, such as fundus photographs [17], thereby allowing diseases to be detected earlier and more accurately. Because of this, OCT imaging has become the primary diagnostic test for many diseases, such as age-related macular degeneration, central serous chorioretinopathy, and retinal vascular occlusion [2].Traditional methods manually design OCT features and adopt machine learning classifiers for prediction [11,16,23]. In recent years, deep learning methods have achieved outstanding performance on various medical imaging analysis tasks and have also been successfully applied to retinal disease classification with OCT images [8][9][10]. However, diagnosing disease with a single OCT modality, as shown in Fig. 1 (a), is still challenging since OCT scans are inadequate compared with fundus photos due to their more expensive cost in data collection. Some methods attempt to use extra layer-related knowledge from the segmentation task to improve prediction despite limited OCT data [3,7,13,15], but this leads to increased training costs since an additional segmentation model is required.Recent works have attempted to include additional modalities for classification through multi-modal learning shown in Fig. 1 (b), where fundus and OCT images are jointly used to detect various retinal diseases and achieve promising results [4,12,14,18,[24][25][26]. Wang et al. [24,25] used a two-stream structure to extract fundus and OCT features, which are then concatenated for prediction. He et al. [4] designed modality-specific attention networks to tackle differences in modal characteristics. Nevertheless, there are still limitations in these existing approaches. Firstly, existing multi-modal learning approaches require strictly paired images from both modalities for training and testing. This necessitates the collection of multi-modal images for the same patients, which can be laborious, costly, and not easily achievable in real-world clinical practice. Secondly, previous works mostly focused on a limited set of diseases, such as age-related macular degeneration (AMD), diabetic retinopathy, and glaucoma, which cannot reflect the complexity and diversity of real-world clinical settings.To this end, we propose Fundus-enhanced Disease-aware Distillation Model (FDDM) for retinal disease classification from OCT images, as shown in Fig. 1 (c). FDDM is motivated by the observation that fundus images and OCT images provide complementary information for disease classification. For instance, in the case of AMD detection, fundus images can provide information on the number and area of drusen or atrophic lesions of AMD, while OCT can reveal the aggressiveness of subretinal and intraretinal fluid lesions [26]. Utilizing this complementary information from both modalities can enhance AMD detection accuracy.Our main goal is to extract disease-related information from a fundus teacher model and transfer it to an OCT student model, all without relying on paired training data. To achieve this, we propose a class prototype matching method to align the general disease characteristics between the two modalities while also eliminating the adverse effects of a single unreliable fundus instance. Moreover, we introduce a novel class similarity alignment method to encourage the student to learn similar inter-class relationships with the teacher, thereby obtaining additional label co-occurrence information. Unlike existing works, our method is capable of extracting valuable knowledge from any accessible fundus dataset without additional costs or requirements. Moreover, our approach only needs one modality during the inference process, which can help greatly reduce the prerequisites for clinical application.To summarize, our main contributions include 1) We propose a novel fundusenhanced disease-aware distillation model for retinal disease classification via class prototype matching and class similarity alignment; 2) Our proposed method offers flexible knowledge transfer from any publicly available fundus dataset, which can significantly reduce the cost of collecting expensive multi-modal data. This makes our approach more accessible and cost-effective for retinal disease diagnosis; 3) We validated our proposed method using a clinical dataset and other publicly available datasets. The results demonstrate superior performance when compared to state-of-the-art alternatives, confirming the effectiveness of our approach for retinal disease classification."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,2,Methodology,"Our approach is based on two ideas: class prototype matching, which distills generalized disease-specific knowledge unaffected by individual sample noise, and class similarity alignment, which transfers additional label co-occurrence information from the teacher to the student. Details of both components are discussed in the sections below. An overview of our framework is shown in Fig. 2.We denote the fundus dataset as, and the OCT dataset asTo utilize knowledge from the fundus modality during training, we build a teacher model, denoted F t , trained on D f . Similarly, an OCT model F s is built to learn from OCT images D o using the same backbone architecture as the fundus model. We use binary cross-entropy loss as the classification loss L CLS for optimization, to allow the same input to be associated with multiple classes. During inference time, only OCT data is fed into the OCT model to compute the probabilities p = {p c } C c=1 for each disease, c."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,2.1,Class Prototype Matching,"To distill features from the teacher model into the student model, we aim to ensure that features belonging to the same class are similar. However, we note that individual sample features can be noisy since they contain variations specific to the sample instance instead of the class. In order to reduce noise and ensure disease-specific features are learnt, we compress features of each class into a class prototype vector to represent the general characteristics of the disease. During the training per batch, the class prototype vector is the average of all the feature vectors belonging to each category, which is formulated as:where e c f and e c o denote the prototype vector for class c of the fundus and OCT modality respectively, v f,i (v o,j ) represents the feature vector of the input image, and y c f,i y c o,j is a binary number which indicates whether the instance belongs to class c or not. P demotes an MLP projector that projects OCT features into the same space as fundus features.In the class prototype matching stage, we apply softmax loss to the prototype vectors of fundus modality to formulate soft targets E c f = σ(e c f /τ ), where τ is the temperature scale that controls the strength to soften the distribution. Student class prototypes E c o are obtained in the same way. KL divergence is then used to encourage OCT student to learn matched class prototypes with fundus teacher:By doing so, the OCT model is able to use the global information from fundus modality for additional supervision. Overall, our approach adopts class prototypes from fundus modality instead of noisy features from individual samples, which provides more specific knowledge for OCT student model."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,2.2,Class Similarity Alignment,"We also note that for multi-label classification tasks, relationships among different classes also contain important information, especially since label cooccurrence is common for eye diseases. Based on this observation, we additionally propose a class similarity alignment scheme to distill knowledge concerning inter-class relationships from fundus model to OCT model. First, we estimate the disease distribution by averaging the obtained logits of fundus and OCT model in a class-wise manner to getThen, to transfer information on inter-class relationships, we enforce cosine similarity matrices of the averaged logits to be consistent between teacher and student model. The similarity matrix for teacher model is calculated as (q c f , q f )/τ ) and is obtained similarly for student model, Q c o . KL divergence loss is used to encourage alignment between the two similarity matrices:In this way, disease distribution knowledge is distilled from fundus teacher model, forcing OCT student model to learn additional knowledge concerning inter-class relationships, which is highly important in multi-label scenarios."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,2.3,Overall Framework,"The overall loss is the combination of classification loss and distillation enhancement loss:where α and β are loss weights that control the contribution of each distillation loss. Admittedly, knowledge distillation strategies in computer vision [1,5,[20][21][22]27] can be applied to share multi-modal information as well. Unlike classical distillation methods, our two novel distillation losses allow knowledge about disease-specific features and inter-class relationships to be transferred, thereby allowing knowledge distillation to be conducted with unpaired data."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3,Experiments,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3.1,Experimental Setup,"Dataset. To evaluate the effectiveness of our approach, we collect a new dataset TOPCON-MM with paired fundus and OCT images from 369 eyes of 203 patients in Guangdong Provincial Hospital of Integrated Traditional Chinese and Western Medicine using a Topcon Triton swept-source OCT featuring multimodal fundus imaging. For fundus images, they are acquired at a resolution of 2576 × 1934. For OCT scans, the resolution ranges from 320 × 992 to 1024 × 992. Specifically, multiple fundus and OCT images are obtained for each eye, Implementation Details. Following prior work [24,25], we use contrastlimited adaptive histogram equalization for fundus images and median filter for OCT images as data preprocessing. We adopt data augmentation including random crop, flip, rotation, and changes in contrast, saturation, and brightness.All the images are resized to 448 × 448 before feeding into the network. For a fair comparison, we apply identical data processing steps, data augmentation operations, model backbones and running epochs in all the experiments. We use SGD to optimize parameters with a learning rate of 1e-3, a momentum of 0.9, and a weight decay of 1e-4. The batch size is set to 8. For weight parameters, τ is set to 4, α is set to 2 and β is set to 1. All the models are implemented on an NVIDIA RTX 3090 GPU. We split the dataset into training and test subsets according to the patient's identity and maintained a training-to-test set ratio of approximately 8:2. To ensure the robustness of the model, the result was reported by five-fold cross-validation.Evaluation Metrics. We follow previous work [12] to evaluate image-level performance. As each eye in our dataset was scanned multiple times, we use the ensemble results from all the images of the same eye to determine the final prediction. More specifically, if any image indicates an abnormality, the eye is predicted to have the disease."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3.2,Compare with State-of-the-Arts,"To prove the effectiveness of our proposed method, we compare our approach with single-modal, multi-modal, and knowledge distillation methods. From Table 2, it is apparent that the model trained with OCT alone performs better than the fundus models. It is noteworthy that current multi-modality methods [4,25] and knowledge distillation methods [1,5,20,21,27] do not yield improved results on our dataset. Table 2 also demonstrates that compared with the single-modal OCT baseline, our method improves MAP from 66.44% to 69.06%, F1 from 64.16% to 69.17%. This shows that it is still possible to learn valuable information from the fundus modality to assist the OCT model, despite being a weaker modality. It can be observed that our approach outperforms the state-of-the-art multi-modal retinal image classification method [4] by 9.57% in MAP (69.06% v.s. 59.49%). Notably, our method excels the best-performing knowledge distillation method [1] by 3.96% in MAP (69.06% v.s. 65.10%). We also note that the alternative methods are limited to training with eye-paired fundus and OCT images only, whilst our approach does not face such restrictions.To further demonstrate the efficiency of our proposed distillation enhancement approach, we validate our method on a publicly available multi-modal dataset with fundus and OCT images, MMC-AMD [24]. MMC-AMD dataset contains four classes: normal, dry AMD, PCV, and wet AMD. We reproduce single-modal ResNet, Two-Stream CNN [25], and KD methods [5,21] as baselines and show results in Fig. 3 (a). It can be seen that our method improves MAP to 92.29%, largely surpassing existing methods."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3.3,Results Trained with Other Fundus Datasets,"Since we implement distillation in a disease-aware manner, multi-modal fundus and OCT training data do not need to be paired. Theoretically, any publicly available fundus dataset could be applied as long as it shares a label space that overlaps with our OCT data. To verify this hypothesis, we separately reproduce our methods with fundus images from two datasets, MMC-AMD [24] and  "
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3.4,Ablation Studies,"Table 3 shows the ablation study of our method. To provide additional insight, we also show the results on majority classes, which contain over 10% images of the dataset, and minority classes with less than 10%. It can be seen that individually using CPM and CSA can improve the overall result by 1.32% and 1.06% in MAP, respectively. Removing either of the components degrades the performance. Results also show that CPM improves classification performance in majority classes by distilling disease-specific knowledge, while CSA benefits minority classes by attending to inter-disease relationships. By simultaneously adopting CPM and CSA, the overall score of all the classes is improved."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,4,Conclusion,"Our work proposes a novel fundus-enhanced disease-aware distillation module, FDDM, for retinal disease classification. The module incorporates class prototype matching to distill global disease information from the fundus teacher to the OCT student, while also utilizing class similarity alignment to ensure the consistency of disease relationships between both modalities. Our approach deviates from the existing models that rely on paired instances for multi-modal training and inference, making it possible to extract knowledge from any available fundus data and render predictions with only OCT modality. As a result, our approach significantly reduces the prerequisites for clinical applications. Our extensive experiments demonstrate that our method outperforms existing baselines by a considerable margin."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Fig. 1 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Fig. 2 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Fig. 3 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Table 1 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Table 2 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Table 3 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 60.
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,1,Introduction,"Coronary artery disease (CAD) is one of the most prevalent critical cardiovascular diseases with up to 32% mortality rate [18]. The CAD diagnosis necessitates reconstructing a 3D coronary artery tree, e.g., from CCTA images, so that the diagnosis decision could be finalized according to the vascular anatomical information, e.g., annotations of vascular branches and vascular morphological properties [7]. However, conventional reconstruction methods merely exploit the images obtained from the diastolic phase that only reveals partial coronary arteries [1,15,22], which potentially makes vessel lesions invisible, i.e., misdiagnosis.In fact, a cardiac cycle has two phases, i.e., diastole and systole. The reconstructed arteries in the two-phased CCTA images are incomplete coronary trees, but they complement each other. By accurately aligning the arteries in both phases, the complete coronary tree can be reconstructed. Nevertheless, there are three challenges for successful coronary reconstruction. 1) Since the heart beats vigorously, its surrounding arteries can be squeezed by heart chambers and become invisible in one of the phases, easily causing the misalignment of a significant number of arteries in the two-phased images (short for component variation), as pointed by yellow (visible in diastole only) and cyan (visible in systole only) arrows in Fig. 1(a). 2) Arteries deform along with heartbeats, their shape, size, and location may vary significantly across the two phases, causing difficulties in alignment, as demonstrated in Fig. 1 For vessel registration, there are mainly three main branches of methods, i.e., image-based, point-cloud-based, and hybrid-based registration. Image-based methods utilize image features to register the entire volume, and the obtained deformation field is then used to align vessels to the target space. Those methods have been extensively applied to the registration of coronary arteries [14,16], pulmonary vessels [13,17], cerebral vessel [10], heart chamber [11], etc. Although those methods demonstrate promising performance on the whole image scale, the vessels are not necessarily well-aligned and cannot be employed to reconstruct the complete coronary tree. By contrast, point-cloud-based registration directly aligns the vessels, which are firstly labeled or segmented from CCTA images and then modeled as point clouds for registration. For example, point-cloud networks [20,21] or graph convolutional networks [24] commonly exploit geometric features of the vascular point-cloud, which are more flexible and accurate than those image-based methods. The limitation of those point-cloud-based methods mainly involves the disability in geometric feature representation to distinguish the arteries, because different arteries or artery branches can share very similar morphology [12]. Similarly, the hybrid-based methods [4,8] also extract the vessel masks in the images for registration, but the lack of effective image information limits its performance. Integrating the advantages of both domains (image and point cloud) may produce improved outcomes, but has not yet been explored.In this paper, we propose a structural point registration network (SPR-Net) to align coronary arteries from the systolic and diastolic phases. The SPR-Net is designed to exploit both image-based and point-cloud-based features, in which the image and point cloud are encoded as intrinsic features. Additionally, we propose a transformer-based feature fusion module to fully exploit the obtained intrinsic features in extracting structural points, i.e., key points that delineate the anatomical morphology of arteries across the two phases and are solely used to compute the deformation field. For those obtained structural points, a simple thin-plate spline [5] method is employed to align coronary arteries of systole and diastole. Extensive experiment results demonstrate the superiority of our method over eight methods (Fig. 2). "
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2,Method,"We propose the SPR-Net method, which simultaneously utilizes geometric features extracted from point clouds and image features extracted from CCTA images with the goal of generating structural points to align arteries across systole and diastole, with shape, location, and component variations. In this section, we first introduce the extraction of geometric features (Sect. 2.1), then the extraction of image features (Sect. 2.2), next the extraction of structural points and their usage in registration procedures (Sect. 2.3), and finally the loss function (Sect. 2.4)."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2.1,Geometric Feature Learning,"The coronary arteries share a tubular structural shape. The point cloud network has the advantages of effectively learning the spatial geometric shape of arteries and providing accurate relative positional relationships of points [24], so that the obtained point features are more discriminative. Inspired by [6], we employ the point cloud encoder, with the same structure as [6] that composes three layers (i.e., sampling layer, multi-scale grouping layer, and PointNet layer), to extract the geometric features of each point.Given the input diastolic and systolic point clouds P and Q, we first use a sampling layer in the point cloud encoder to obtain the down-sampled pointsrespectively. P and Q are then filled into the multi-scale grouping layer to aggregate its neighboring points within different radii r. After that, the multi-scale aggregated points are fed into the PointNet layer to extract geometric features."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2.2,Geometric and Image Feature Encoding,"Point clouds can provide good geometric shapes and spatial location information, but they lack sufficient semantic features of coronary arteries. Meanwhile, the images contain rich contextual information that can complement the geometric features. Therefore, we design a transformer-based module to integrate both advantages. Specifically, 1) we employ a shallow 3D vision transformer (ViT) [9] to extract image features of the artery; and 2) we employ general transformers [19] to fuse image features and geometric features extracted by the point-cloud encoder. 1) Image Feature Extraction. For efficiency, we only crop image blocks of size h × w × d, with each point as the centroid, and the ViT block is employed to extract local features. Since these blocks are extracted along the tubular structures, the extracted local features reveal intrinsic relationships. To exploit their correlations, we employ a self-attention mechanism-based transformer. The coordinates of each point serve as the position encoding, which is added to its local image feature as the input to the following transformer blocks.where E i and I i respectively indicate the position encoding and image features for the i-th rectangular volume. (a, b, c) ∈ R 3 is the point coordinates. f img i ∈ R l is the self-attention input of transformer layer, and l is the feature dimension.2) Geometry and Image Co-embedding. Given concatenated features of pointwise and image features, four transformer layers are employed to further explore comprehensive contextual features between the two phases. The transformer layer incorporates an encoder and decoder block, which are based on a multi-head attention mechanism. We use the concatenated features of the diastolic phase as input to the transformer encoder and decoder respectively, and the opposite for the systolic phase, to learn the feature dependencies between the two phases."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2.3,Registration via Structural Point Correspondences,"1) Integration of Structural Points. The input of MLP is the contextual features extracted by the transformer, and the output is the probability of each point. Specifically, given the sampled points P with the fused features F P from diastole, we input the features into the shared MLP to generate the probability mapsThus, the diastolic structural points S p can be calculated as follows:Note that, the systolic structural points S q are calculated in the same way as the diastolic structural points.2) Structural Points based Registration using TPS. Based on the correspondence established between the structural points S p and S q in the two phases, we apply a simple but effective idea of the TPS method to interpolate the dense deformation field. For the two sets of structural points, S p and S q , the nearest projection from structural points S q to the S p is calculated, and the S q is warped to the S p in the diastolic phase. Eventually, each systolic point is re-meshed by the closest point to the structural point and further warped to the original points Q using the estimated dense deformation field."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2.4,Loss Function,"We design a structure-constrained registration loss for SPR-Net,where,Here L rec is chamfer distance, and X and Y denote two point clouds respectively. The first part L rec (S p , P ), and the second part L rec (S q , Q) assure the predicted structural points in two different phases are close to their corresponding original point clouds. The third part L rec (S p , S q ) encourages an accurate alignment of structural points between the two phases, ensuring that structural points with the same semantics align on the same vessel branch."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3,Experiments and Results,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3.1,Dataset and Evaluation Metrics,"Data Processing. In our experiments, we collected 58 pairs of CCTA images with both diastolic and systolic phases. All coronary artery masks are first extracted using [25] and refined by three experts. Then, the annotated arteries were down-sampled and modeled as 3D point clouds; meanwhile, their coordinates were normalized to the range of [0,1]. We choose the five-fold crossvalidation evaluation strategy, with 40 training subjects and 18 testing subjects. Evaluation Metrics. Since the artery branches of systole and diastole only partially overlap, i.e., some coronary branches only appear in one phase, we define a common Dice coefficient (CoDice) to accurately evaluate the results.where P o and Q o denote the set of coronary branches common to diastolic and systolic phases, respectively. Moreover, the Dice coefficient (Dice), Chamfer distance (CD), and Hausdorff distance (HD) are also employed for evaluation."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3.2,Implementation Details,"The initial inputs of the SPR-Net contain 4096 point clouds for each phase, and a volume size of 16 × 16 × 8 is cropped around each point. The point cloud encoder consists of two set abstraction blocks with 1024 and 256 grouping centers respectively. In each set abstraction block, we utilize the grouping layer with two scales r to combine the multi-scale features, containing scales (0.1, 0.2, 0.4) and (0.2, 0.4, 0.8) respectively. The transformer blocks we used are composed of vanilla transformer layers. The outputs of the point cloud encoder and ViT have 512-D and 128-D features, respectively, which are concatenated together to form 640-D contextual features. The configuration of the MLP block in the structural point integration depends on the number of structural points. All experiments were implemented using Pytorch on 1 NVIDIA Tesla A100 GPU. We trained the networks using Adam optimizer with an initial learning rate of 10 -4 , epoch of 600, and batch size of 8."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3.3,Comparison with State-of-the-Art Methods,"Our SPR-Net was quantitatively and qualitatively evaluated, compared with eight SOTA registration methods, which belong to three categories:1) imagebased registration, including SyN [2], VoxelMorph [3], and DiffuseMorph [11]; 2) hybrid-based registration, TMM [8]; 3) point-cloud based registration, including Go-ICP [23], DCP [20], STORM [21], and ISRP [6].Quantitative Results. The quantitative results are listed in Table 1. We can find the superiority of point cloud-based methods if compared to image-based methods, which supports the previous conclusion about the limitation of imagebased methods. We can also find that our proposed method significantly outperforms other methods since SPR-Net fully encodes and fuses features of the images and point clouds. Notably, SPR-Net achieves significantly better performance than ISRP, the closest competing method, with an improvement of 10% (i.e., increasing Dice from 58.31% to 68.58%). Qualitative Visualization. Since the correspondence of structural points is vital for registration, we show the structural points (colored) in systole (green) and diastole (red) in Fig. 3 for demonstrating their correspondence. Those structural points with correspondence to the same vascular branch are marked by the same color denoted by the dashed boxes in the 2nd column of Fig. 3. Notably, we can find that the structural points are distributed at positions such as the endpoints or bifurcation points, as shown in the 1st and 2nd columns, which properly delineate the morphology of the point clouds when the number of structural points is small. With increased number, structural points do not only locate at endpoints or bifurcation points but also diffuse along the vessel branches, forming the vessel skeleton, as shown in the 3rd and 4th columns of Fig. 3. In the 5th column of Fig. 3, a complete coronary tree is obtained by exploiting the registration (K = 768). "
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3.4,Ablation Study,"We also conduct the ablation studies with the same backbone point cloud encoder by following three groups of configurations: 1) Whether using the four transformer layers, denoted as CoF, to encode and fuse the systolic and diastolic geometry. 2) Whether fusing the geometry features of point cloud with imagelevel semantic features, denoted GIF. 3) Testing the network on different numbers of structural points (Number-SP). Table 2 summarizes the ablation study results.If without employing CoF and GIF, only the backbone encoder is used to generate structural points. 1) With the same 768 structural points, we can find the individual modules of CoF and GIF can both improve the Dice performance. Meanwhile, combining the two modules lead to the best performance, which may suggest the importance of fusing the two different aspects of features. 2) By equipping both CoF and GIF, we can find that SPR-Net's performance has been improved when the structural points number increases from 256 to 768. However, the performance decreases when it is further increased to 1024, indicating that dense structural points negatively affect the results, which is probably caused by the increasing number of outlier points. It can also be found that SPR-Net demonstrates inferior performance than both backbone+CoF and backbone+GIF when using 256 structural points, which is probably caused by the sparsity of structural points that are largely located at the endpoints and bifurcation positions, which cannot well delineate the morphology of vessel tree. Therefore, the number of structural points is a key parameter that affects registration performance. Through extensive experiments, we determine the optimal number of structural points to ensure one-to-one correspondences between diastole and systole (Table 2)."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,4,Conclusion,"In this paper, we have proposed an intrinsic structural point learning-based framework for systolic and diastolic coronary artery registration. The framework identifies structural points in the arteries across the two different phases using both the spatial geometric features extracted by the point cloud network and the complementary image semantic information extracted by ViT. By strategically fusing the image and point geometric features through a transformer, structural points with strong correlations in two different phases are extracted and used to guide the registration process. Compared with the existing image-based registration methods and point cloud-based methods, our integrated method achieves superior performance and outperforms the state-of-the-art methods by a large margin, which suggests the potential applicability of our framework in real-world clinical scenarios for CAD diagnosis."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Fig. 1 .,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Fig. 2 .,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Fig. 3 .,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Table 1 .,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Table 2 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,1,Introduction,"The automatic segmentation of abdominal multiple organs is clinically significant in extremely that can significantly reduce clinical resource costs. However, the task of abdominal organ segmentation is difficult. The number of abdominal organs is large, and these multiple organs show diverse characteristics among themselves. For example, the shape of the stomach varies greatly even in the same individual at different times, making precise pixel segmentation extremely challenging. Accurate and automatic segmentation of readable results from abdominal multiple organs can provide accurate evidence of reality for surgical navigation, visual enhancement, radiation therapy, and biomarker measurement systems. Therefore, how to accurately make the segmentation results more readable in the case of multiple organs influencing each other has a great contribution to clinical examination and diagnosis.Abdominal multi-organ network models based on deep neural networks (DNN) are difficult to train. Training such a good enough model usually requires a large amount of labeled data, or the model performance is likely to meet a heavy drop. However, manual annotation of organs requires doctors to make accurate judgments based on their professional knowledge and rich experience, this leads to making manual labeling both expensive and time-consuming. In addition to pixel-level annotated datasets, deep neural networks can also benefit from other types of supervision. For example, boundary-level annotation can provide more detailed boundary information. In addition, weakly supervised [6,12,14] learning techniques can be used, such as training with pixel-level labels and unlabeled data. Additionally, visual perceptual [1,7] supervision can be employed by utilizing visual perceptual theory in the training of deep networks to increase their sensitivity to image features. Furthermore, pre-trained models can be utilized for transfer learning, which allows the model to learn features from previous tasks and improve its performance. In summary, deep neural networks can benefit from various types of supervision, which can improve their performance in a variety of visual tasks. These studies have demonstrated that incorporating finer-grained additional supervision can enhance the accuracy of deep neural networks and improve the interpretability of network models.However, the practical process of collecting additional annotations remains challenging, as it may require clinicians to repeatedly provide specific and refined annotations to fine-tune the network model. There is a need to minimize the impact of the annotation process on clinical work. To address this, we investigate novel annotation information that can be used for abdominal multi-organ segmentation. In the context of medical image analysis, it has been observed that radiologists tend to focus their attention on specific regions of interest (ROIs) or lesions when interpreting medical images. Specifically, our method utilizes eye gaze information collected by an eye-tracker during radiologists' image interpretation as a source of additional supervision. In clinical practice, experienced radiologists can usually quickly locate specific organs when reading abdominal images. In this process, the doctor's eye movement information can reflect the location information of organs to a certain extent. Compared with manual label-ing, this information is cheap and fast and can be used as effective supervision information to assist the localization and segmentation of each organ. The literature studies have implied that the potential of the radiologist's gaze data can be high in improving disease diagnosis [2,17]. Recently, Wang et al. [16] applied eye-tracking technology to diagnose knee osteoarthritis, while Men et al. [9] used eye-trackers to provide visual guidance to sonographers during ultrasound scanning. It can be seen that the use of eye movement attention information has great value and potential in automated auxiliary diagnosis.In this paper, we propose a novel eye-guided multi-organ segmentation network for diverse abdominal organ images. The network model is forced to focus on relevant objects or features required for the segmentation task by fully and synergistically utilizing the radiologist's cognitive information about the abdominal image. This method of information collection is convenient and can make the positioning of each organ more accurate. The overall architecture is shown in Fig. 1. The proposed network has three special designs: 1) a dual-path encoder that integrates human cognitive information; 2) a cross-attention transformer module (CATM) that communicates information in network semantic perception and human semantic perception; and 3) multi-feature skip connection (MSC), which effectively combines spatial information during down-sampling to offset the internal details of segmentation. "
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,2,Methodology,"As shown in Fig. 1. The proposed network adopts an encoder-decoder structure, where the encoder part consists of parallel dual paths that utilize multi-feature skip connection (MSC) to combine spatial information during down-sampling to offset the internal details of segmentation. A cross-attention transformer module (CATM) is designed at the bottleneck stage to effectively communicate information in network perception and human perception."
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,2.1,Wavelet Transform for Composite Information,"Wavelet transform is able to obtain global information and edge information in different directions in gaze attention heatmaps so that the network can effectively fuse the composite information in the heatmaps. In the clinic, when radiologists read abdominal images, the more important location, the longer the radiologists' gaze. We convert this information into a heatmap representation. The heatmap reflects the rough position information of the target to be segmented. The single heatmap is unable to reflect the composite information it contains, therefore DWT is utilized for extracting it. Discrete wavelet transform [8] (DWT) is applied to decompose the approximation coefficients and detail coefficients of abdominal organ distribution information on the gaze attention heatmap to locate the position and edge of multiple organs in the abdomen. In the decoding phase, we fuse the approximation coefficient in the gaze heatmap so that compensates for the global topological information of decoding features at the final segmentation. The detail coefficients are input into the image encoder together with the original image, which is used to guide the dual-path encoder to reserve detailed information. "
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,2.2,Multi-feature Skip Connection,"Multi-feature skip connection (MSC) comprehensively utilizes multiple features to guide the segmentation results of each abdominal organ toward accurate internal details. As shown in Fig. 2, we choose to integrate the encoding features on the two paths concatenated with features in the up-sampling process to offset the internal details of segmentation. Instead of using a simple concatenated and fusion strategy, we use multiple composite splicing and fusion to obtain matching features. The residual connection can aggregate the features of different levels to avoid additional noise and thus improve the network performance. The MSC can be expressed as follows:where F i and F g represent the output features from the down-sampling layers of the two paths, and F fusion denotes the final multiple fusion features. Following the MSC, the dimension of the concatenated multiple features remains the same as the dimension of the upsampled features. "
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,2.3,Cross-Attention Transformer Module,"The cross-attention transformer module (CATM) creatively enables the communication between network semantic perception and human semantic perception. Different from the traditional self-attention mechanism in transformer block [15], by using CATM, information interactive collaboration on two paths is enabled effectively. CATM, which is a multi-path structure, is embedded in the bottleneck layer between the encoder and decoder. As shown in Fig. 3, it consists of two paths: the image attention path and the gaze attention path. In our work, CATM is composed of L (we set L = 6) cross-attention transformer blocks (CTB) and Conv2D. The expression of CATM can be represented as:where F i and F g represent the final output features of the encoder on the image attention and gaze attention encoding pathways. Our experimental results demonstrate that the cross-attention operation within the CATM design efficiently enhances the communication of information between these two paths.The convolution operation of CATM is used to fuse the feature information from two paths, which makes up for the possible information shortage in the decoding process. The CTB is the core design of the CATM, which is a variant of the Transformer [15] that exchanges the network semantic perception and human semantic perception on the two different paths of the image and gaze attention. The key of cross-attention is to exchange Q, K and V of respective features between different path features and fuse them. As shown in Fig. 3, the K and V in the image attention path exchange with the gaze attention path, and each original Q fuse with the exchanged K and V . It is represented by a formula:), and V in the image and gaze attention path, respectively; B denotes the learnable relative positional encoding; d is the dimension of K, and we set the number of head of multi-headed self-attention is 12."
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,3,Experiments,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,3.1,Datasets and Evaluation,"Our experiments use the Synapse multi-organ segmentation dataset(Synapse). Each CT volume consists of 85 -198 slices of 512 × 512 pixels, with a voxel spatial resolution of ([0.54 -0.54] × [0.98 -0.98] × [2.5 -5.0]) mm 3 . We use the 30 abdominal CT scans and split it 18 training cases and 12 testing cases randomly. Following [3,4], all 3D volumes are inferenced in a slice-by-slice fashion and the predicted 2D slices are stacked together to reconstruct the 3D prediction. We use the average Dice-Similarity coefficient(DSC) and average Hausdorff distance (HD) as the evaluation metric to evaluate our method on the full resolution of the original slice.  2). TransUnet and SwinUnet (without gaze attention information) predict coarser edges and shapes compared to our method; 3). In the 3rd row, our method correctly identifies the stomach, while SwinUnet (with and without gaze attention information) failed to predict the shape of the stomach.Comparison with Existing Methods Performance. As shown in Table 1, We also train Unet gaze , TransUnet gaze , and SwinUnet gaze networks with gaze attention information by simply concatenating the gaze attention information with the input image. Our experimental results reveal that this approach of introducing gaze attention information as an auxiliary supervision mechanism leads to an appreciable improvement in network segmentation performance. Specifically, Unet, TransUnet, and SwinUnet have an improvement of approximately 1% in terms of the DSC evaluation metric and 2-4% in terms of the HD evaluation metric by concatenating the gaze attention. Furthermore, our method surpasses the SwinUnet (without gaze attention information) by approximately 3% in terms of the DSC evaluation metric and approximately 10% in terms of the HD evaluation metric. Comparing our method with U-Net, TransUnet, and SwinUnet (with gaze attention information), we also observe a significant improvement of approximately 2-4% in terms of the DSC evaluation metric and 7-24% in terms of the HD evaluation metric, respectively.Qualitative Visualization Results. As shown in Fig. 4. 1st row of the figure highlights that the approach leveraging gaze attention as auxiliary supervision yields fewer erroneous labels in comparison to the other methods, implying that eye-tracking attention can aid the model in attending to relevant objects or features. TransUnet (with gaze attention information) predicts coarser edges and shapes compared to our method (e.g. in the 2nd row, the model's prediction for the liver). In the 3rd row, our method correctly identifies the stomach, while SwinUnet (with and without gaze attention information) failed to predict the shape of the stomach. The results demonstrate that our method can leverage gaze attention as auxiliary supervision and better segment and retain edge and shape information. Ablation Study. We verified the DWT, MSC, and CATM separately using three different network configurations. We summarize the experimental results in Table 2. It can be seen that the w/o DWT performs the worst, indicating that the detail coefficients extracted from eye-tracking heatmaps can effectively locate organs in the image and provide strong support for edge segmentation. The w/o CATM does not effectively fuse the image features and eye-tracking attention features using CATM, resulting in a less-than-ideal improvement in segmentation results. In the results of w/o MSC and the full version of the network, we observed that MSC can further improve segmentation results by integrating down-sampling information from both paths."
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,4,Conclusion,"In this paper, we propose a novel network that can realize the interactive communication between network semantic perception and human semantic perception, and apply it to the task of abdominal multi-organ segmentation for information interactive collaboration. The network is innovatively built with 1) a dual-path encoder that integrates human cognitive information; 2) a cross-attention transformer module (CATM) that communicates information in network semantic perception and human semantic perception; and 3) multi-feature skip connection (MSC), which effectively combines spatial information during down-sampling to offset the internal details of segmentation. Extensive experiments with promising results reveal gaze attention has great clinical value and potential in multi-organ segmentation."
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Fig. 1 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Fig. 2 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Fig. 3 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Fig. 4 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Table 1 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,.16 3.2 Results and Analysis Overall Performance.,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Table 2 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Acknowledgements,". This study was supported by the National Natural Science Foundation (No. 62101249 and No. 62136004), the Natural Science Foundation of Jiangsu Province (No. BK20210291), and the China Postdoctoral Science Foundation (No. 2021TQ0149 and No. 2022M721611)."
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,1,Introduction,"Coronary Computerized Tomography Angiography (CCTA) is a commonly used non-invasive approach for the diagnosis of potential coronary artery diseases [11]. In clinical practice, accurate labeling of coronary artery segments (see Fig. 1(a)) is a crucial step toward the subsequent diagnosis and analysis of the image. However, the vast variability of coronary artery anatomy across individuals makes it challenging to achieve precise and automatic labeling. Previous studies on labeling coronary arteries using deep learning-based methods [20,[22][23][24] have shown promising results by introducing graph convolutional networks and point cloud analysis. However, these approaches have overlooked the essential prior knowledge that different categories of coronary arteries have anatomically predetermined connections [3]. For instance, LAD, LCX, and RI originate from LM, while S and D arise from LAD. All of the anatomical connections form a tree structure with prior topology, as shown in Fig. 1(b). We call this structure category topological trees. Due to lacking the utilization of the category topological trees, existing methods [20,[22][23][24] often make some obvious mistakes that violate the prior topology as illustrated in Fig. 1(c). We argue that incorporating the category topological trees into the network explicitly is the key to improving automatic labeling performance, especially in reducing topology-violation labeling errors.In this paper, we propose a novel framework called TopoLab to perform topology-preserving automatic labeling of coronary arteries. Our model mainly contains two components: the hierarchical feature extraction module and the anatomy-aware connection classifier. The hierarchical feature extraction module introduces the segment query to achieve intra-segment feature aggregation via Transformer [17] and relies on graph convolutional network [8] to establish inter-segment feature interactions. Moreover, to incorporate the category topological trees into the network explicitly, we further propose the anatomy-aware connection classifier (AC-Classifier). Unlike previous methods that classify each segment independently, AC-Classifier performs classification for every connected segment pair. Specifically, all of the connections derived from the category topological trees are used to construct the ground truth connection templates, and each connected segment pair is categorized into one of these templates. Since the connection templates inherently conform to the topology, AC-Classifier has effectively prioritized the anatomically predetermined connections and the network is enabled to preserve the topology by design.To the best of our knowledge, there is currently no publicly available dataset with annotations for artery labeling. In this work, we contribute high-quality annotations to the orCaScore dataset [19]. The experimental results on the public dataset orCaScore and an in-house dataset have demonstrated that our TopoLab outperforms previous state-of-the-art methods, especially in the topology-related metrics.Our contributions can be summarized as follows. (1) We are the first to incorporate the category topological trees into the deep learning models for automatic labeling of coronary arteries by introducing AC-Classifier. (2) We propose a novel hierarchical feature extraction module to achieve intra-and inter-segment feature aggregations. (3) Our approach achieves state-of-the-art performance on both public and in-house datasets. (4) We provide high-quality annotations of artery labeling for the public dataset orCaScore, which is available at https:// github.com/zutsusemi/MICCAI2023-TopoLab-Labels/."
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,2,Related Work,"Traditional methods [1,21] for automatic labeling of coronary arteries usually align the extracted artery trees with a 3D coronary artery tree model which provides the anatomical connections as prior knowledge. However, these works rely heavily on logical rules, which can not always capture the complexities of the anatomical structure. To overcome the limitations, deep learning has been introduced in this area [20,[22][23][24] with its great success in medical imaging [7,[14][15][16]18]. For instance, TreeLab-Net [20] uses bidirectional tree-structural LSTM [6] to model the coronary artery trees, while CPR-GCN [22] constructs a vessel graph by treating each segment as a node and leverages GCN [8] to aggregate segment features. CorLab-Net [24] regards spatial and anatomical dependencies as the explicit guidance for artery labeling based on point cloud networks [13]. Nevertheless, all of these deep learning based methods ignore the important priority about the predetermined anatomical structure -category topological trees. In this paper, we aim to incorporate this priority into the network design explicitly for developing the topology-preserving models."
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,3,Methodology,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,3.1,Overview,"We start by extracting centerlines from the vessel segmentation annotations in a CCTA image, using a traditional 3D thinning algorithm [9]. Next, we use the minimum spanning tree algorithm [5] to construct two coronary artery trees, one for the left domain (LD) and one for the right domain (RD). Following the branch bifurcation rules in [22] (details can be found in supplementary materials), the coronary artery trees are split into several segments, denoted by S = {S i } N i=1 , where N is the number of segments, S i ∈ R Li×3 denotes the i-th segment comprised of the 3D positions of L i centerline points. Our model takes as input the vessel segments S and their connections C = {(i 1 , j 1 ), (i 2 , j 2 ), ..., (i Nc , j Nc )}, where (i k , j k ) indicates that the i k -th segment is connected with the j k -th segment. The objective is to predict the classes of the vessel segments.As illustrated in Fig. 2, we design a novel framework named TopoLab for the automatic labeling of coronary arteries. In Sect. 3.2, we will introduce the hierarchical feature extraction module comprised of intra-segment feature aggregation and inter-segment feature interaction. In Sect. 3.3, we will elaborate on the details of AC-Classifier which exploits the category topological trees effectively. "
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,3.2,Hierarchical Feature Extraction,"We first feed the CCTA image X ∈ R H×W ×D into an image encoder (e.g. U-Net [2]) to obtain the downscaled feature map F ∈ R H/4×W/4×D/4×C , where C is the channel size.For each segment S i , trilinear interpolation in the downsampled feature map F is adopted for the centerline point v ∈ S i to obtain the corresponding point featuresIntra-Segment Feature Aggregation. To extract the segment-level features for labeling, aggregation of sequential point features belonging to the same segment is required. Previous studies [20,22] employ Bidirectional LSTM [6] to summarize the tubular sequential features. However, due to the weak representative ability of LSTM for the vessel segments with huge length variability, the performance of these methods is limited.We introduce Transformer [17] as our intra-segment feature aggregator for its strong capability to model the relationships among the sequences with varying lengths. Concretely, we set a learnable embedding q ∈ R C called segment query to aggregate intra-segment point features. The segment query is concatenated with the segment features E i to obtain the new tensor Ẽi ∈ R (Li+1)×C . Following [4], we feed Ẽi augmented by the learnable 3D positional encodings into a Transformer encoder containing several standard sub-blocks. Each standard sub-block has the same architecture as in [17], which consists of multi-head attention, feed-forward network, layer normalization and ReLU activation. The state of the segment query q at the output of the Transformer encoder serves as the aggregated segment representation Êi ∈ R C .Inter-Segment Feature Interaction. The branching structure of coronary arteries is inherently graph-like, with each segment serving as a node and the connections between segments serving as edges. Thus, we leverage graph convolutional network [8] (GCN) to capture the interactions among different segments.Specifically, let Ê ∈ R N ×C denotes the aggregated segment features where the i-th item of Ê is Êi , and A ∈ R N ×N is the adjacency matrix for the vessel segment graph derived from the segment connections C. The process of GCN layers is as follows:where σ is the ReLU activation, W l ∈ R C×C is the learnable parameters for the l-th GCN layer, the input for the first layer is Ê0 = Ê. Finally, we fuse the input segment features Ê and the output of the final GCN layer Êf to obtain E = [ Êf , Ê]W f ∈ R N ×C with the parameters W f ∈ R (C+C)×C . The enhanced segment features {E 1 , E 2 , ..., E N } are forwarded to the classifier for segment labeling, where E i ∈ R C is the i-th item of E."
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,3.3,Anatomy-Aware Connection Classifier,"The direct approach for labeling the coronary arteries is to use a linear layer to classify each segment independently as in previous methods. To incorporate the category topological trees into the classifier design, we propose to conduct the classification task for every connected segment pair.We begin by defining the ground truth segment connections which are composed of the topology-conforming connections derived from the category topological trees (like LM→LAD, LCX→OM, etc.). Note that the self-connections (e.g. RCA→RCA) are also considered as the ground truth connections. The ground truth segment connections have the corresponding template embeddings for classification, which are represented by G ∈ R Ng×2C , where N g is the number of ground truth segment connections. Denote g i = Concate(Enc(x), Enc(y)) ∈ R 2C as the i-th item of G, where x and y stand for the segment classes indexed by the i-th ground truth connection, Enc denotes the sinusoidal encoding as in [17].The connection templates G are used to enable classification for the connected segment pairs. Then, given the segment features {E i } N i=1 , and all of the connected segment pairs C = {(i 1 , j 1 ), (i 2 , j 2 ), ..., (i Nc , j Nc )}, the connection features P ∈ R Nc×2C are obtained by rearrangement of the segment features, where the k-th item of P is Concate(E i k , E j k ) ∈ R 2C . We use an MLP layer to further fuse the featuresTraining Loss. The loss function can be written as:where y i is the ground truth of the i-th segment connection, pi denotes the i-th item of P . sim in Eq. 3 stand for the cosine similarity, sim(x, y) = x T y ||x||2||y||2 , and the temperature τ is a hyperparameter which is set as 0.05 by default.Inference. During the inference stage, for each segment, we first select the connection with the largest confidence score among all segment connections that have covered the given segment. And then the corresponding category indexed by the selected connection serves as the prediction of the specific segment."
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4,Experiments,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4.1,Setup,"Datasets. We train and evaluate our method on two datasets. The orCaScore [19] MICCAI 2014 Challenge contains 72 contrast-enhanced CTA images and non-contrast enhanced CT scans. As the original dataset only contains the labels of calcifications, we have annotated vessel segmentation and anatomical categories for coronary arteries with experienced radiologists. Considering the small amount of data, we randomly split the dataset into five folds to perform cross-validation. The mean values of cross-validation results are reported. We also collect an In-house Dataset containing 1200 CTA scans which have been annotated by at least two experts. The dataset is collected in compliance with the terms of the licensing agreement and ethical certification. We randomly split the dataset into train, validation and test set with 800, 200 and 200 scans respectively. The annotations for both datasets adhere to the same standard, which includes 14 classes of coronary artery segments (see Fig. 1(b)). It is a challenging task, surpassing the scope of studies like TreeLab-Net [20] and CPR-GCN [22], which only consider 10 and 11 categories, respectively. Evaluation Metrics. Following previous methods [20,22,24], we adopt the mean metrics of all categories of the segments including recall, precision and F1. Note that the mean metric is the weighted average based on the number of segments of different categories. To further evaluate the topological accuracy of connected segments, we propose two new metrics: viola and viola c , which reflect the segment-level topological accuracy and the case-level topological accuracy, respectively. Specifically, viola is calculated as the ratio of the number of connections violating the topology to the total number of connections, while viola c is calculated as the ratio of the number of test cases containing any topologyviolating connection to the total number of test cases. "
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4.2,Implementation Details,3D ResUNet [25] is employed as the image encoder for feature extraction with channel dimension C = 64. The transformer encoder has 3 standard blocks and the number of graph convolution layers is set to 4. We train the network using AdamW optimizer [10] with a base learning rate of 5e-4 and the cosine learning rate schedule during the training stage. The batch size is set to 4. All networks are implemented by Pytorch [12] and trained on four NVIDIA GeForce RTX 3090 GPUs. We train our model on the orCaScore dataset for 3.5k iterations and in-house dataset for 12.5k iterations.
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4.3,Comparison with Other Methods,"Quantitative Results. We compare TopoLab with other deep learning based approaches including TaG-Net [23], CPR-GCN [22], TreeLab-Net [20], and CorLab-Net [24] which are implemented by ourselves with the same training configurations for a fair comparison. Note that for the point cloud-based methods [23,24], we use intra-segment voting to transform the point-level predictions into segment-level predictions. From the results on the orCaScore dataset in Table 1 and the in-house dataset in Table 2, we can conclude that the proposed TopoLab outperforms all existing methods by a large margin. The performance gains are more significant in the topology-related metrics, which demonstrates the effectiveness of the utilization of prior knowledge. More detailed results on each category of coronary arteries can be found in the supplementary materials.Qualitative Results. In Fig. 3, we present a qualitative comparison of TopoLab with other methods. Consider the first case as an example, where our approach successfully avoids topology-violating errors, whereas other methods incorrectly classify RI as D, leading to the RI→D, D→RI, or LM→D connections that violate topology. These visualizations demonstrate the effectiveness of the usage of category topological trees. More qualitative results can be found in supplementary materials. "
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4.4,Ablation Study,"In this subsection, we explore the effectiveness of different components in Topo-Lab on orCaScore dataset, as shown in Table 3.Intra-segment Feature Aggregation (IFA). Transformer [17] is leveraged to achieve the intra-segment feature aggregation in our model. To validate its benefits, we replace it with the Bi-LSTM used in CPR-GCN. From the results in the first line of Table 3, our method surpasses Bi-LSTM by 7.94% in F1 score and 26.57% in Viola c .Inter-segment Feature Interaction (IFI). We use GCN to establish intersegment feature interactions due to the natural graph structure of coronary arteries. When directly removing this module, the performance drops by 1.57% in F1 score and 19.14% in Viola c as illustrated in the second line of Table 3.Anatomy-Aware Connection Classifier (ACC). AC-Classifier which exploits the prior knowledge from category topological trees is adopted to classify each connected segment pair. We replace it with a commonly used linear layer to enable classification for single segments as in previous methods, and the performance drops by 1.33% in Viola and 15.42% in Viola c , which effectively demonstrates the superiority of the proposed method. "
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,5,Conclusion,"In this study, we review the essential task of coronary artery labeling and exploit the prior knowledge of the predetermined anatomical connections. The proposed strategies of intra-and inter-segment feature aggregation guarantee effective feature extraction, while the AC-Classifier preserves the clinical logic in the network design. The extensive experiments on orCaScore dataset and in-house dataset reveal that the proposed TopoLab has achieved new state-of-the-art performance. We hope our paper could encourage the community to explore the use of clinical priority to facilitate the design of more effective algorithms."
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Fig. 1 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Fig. 2 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Fig. 3 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Table 1 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Table 2 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Table 3 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_71.
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,1,Introduction,"The 3D dental mesh segmentation is aimed to accurately separate the dental mesh into distinct components, namely individual teeth and gums. Therefore stable and accurate 3D dental mesh segmentation plays an essential role in various areas of oral medicine, including orthodontics and denture design, where precise tooth segmentation is of great importance for subsequent procedures and treatments. However, this task is accompanied by notable challenges arising from the inherent limitations in scan accuracy and the presence of considerable noise within the reconstructed 3D dental mesh, consequently leading to the blurring of tooth boundary.To solve these challenges, some methods have been widely explored. Some conventional methods usually utilize specific geometric properties [15,16] like coordinates and normal vectors to perform threshold segmentation. These methods mainly use the pre-defined attributes, making it hard to achieve high-quality results through automated segmentation.In recent years, many deep learning-based methods have been proposed to perform more accurate automated dental mesh segmentation. Some methods [10,12] pack the point-wise features into 2D image-like inputs which are fed into a multi-layer CNN-like network and some other methods [4,5,9,13] extend the point cloud feature learning frameworks to perform the segmentation. These methods tend to ignore the inherent topology of mesh and thus the quality of the segmentation is not good enough. Some methods [2,8] design a two-stage network consisting of tooth centroid extraction and tooth segmentation. But these methods are not equally effective for models with crowding or missing teeth which are common in real scenes. And they need centroid labels which will incur additional computational cost. Some recent methods like TSGCNet [14] design a two-stream graph convolution-based feature extraction network to extract the features of the C-stream (coordinate) and the N-stream (normal vector) separately and predict the cell/vertex-wise segmentation results according to the concatenated features of two streams. TSGCNet pioneered a new dental mesh feature process paradigm of decoupling the initial feature into C-stream (coordinate) and N-stream (normal vector), and used the graph convolution [11] to consider the topological continuity when updating the features. However, these methods still suffer from the following deficiencies.First of all, these methods still have some difficulty on the tooth boundary especially for the crowded teeth. Although there always exist individual differences in the shape of teeth from person to person, the teeth at different positions do have distinctive shape priors that distinguish them from other teeth, such as canine teeth and molar teeth. If we can make full use of the shape priors attached to the semantic information of the teeth, these segmentation errors on the boundaries can be decreased greatly. Thus it provides a more promising way to perform the segmentation guided by the semantic information. Secondly, these methods are often confused at the molars since they only use graph convolution to model the dependencies. Therefore a better alternative would be utilizing the local and non-local semantic information at the same time to further enhance the features, that means the long distance dependencies also need to be considered. Lastly, the existing methods always directly perform concatenation on the features from different angles, resulting in the incorrect segmentation on the misaligned teeth. This is due to the fact that the importance of features from different perspectives can vary a lot in different regions. Hence regressing a specific weight and fusing the features with these weight parameters can effectively eliminate the feature imbalance and pay more emphasis on salient features.To address these issues, we propose a novel semantics-based feature learning network to fully utilize the semantic information and grasp the local and non-local dependencies. We first follow the TSGCNet [14] to decouple the fea-tures into coordinate domain (C-domain) and normal domain (N-domain) which indicate the spatial and geometric features respectively. And then we design a multi-scale encoder network and at each scale we utilize a coarse classifier which accepts the adaptively fused features from the C-domain and the embedded Ndomain to predict a semantic pseudo label. Then with the semantic label we use the graph-transformer module to model the long distance dependencies in the neighbourhood of cells with the minimal semantic distance and perform feature aggregation according to the dependencies. Last but not least, we use the global graph-transformer module on the cross-domain features to further learn the semantic information and fuse them by adaptive feature fusion module before fed into the decoder.To conclude, our contributions are three-fold: (1) We propose a novel semantics-based feature learning which can fully utilize semantic information to enhance the local and global mesh features. (2) We design a new feature fusion module that obtains global dependencies in C-domain and N-domain to further utilize the semantic information and adaptively fuses cross-domain features. (3) We compare with several recent methods on the real 3D dental mesh collected by the hospital. The OA (Overall Accuracy) and mIoU (mean Intersection over Union) both indicates that we perform superior performance on the 3D dental mesh segmentation task. Extensive evaluations prove that our method significantly outperforms the state-of-the-art one-stage methods."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,2,Method,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,2.1,Overview,"Our network mainly consists of a semantics-based graph-transformer module and an adaptive cross-domain feature fusion module, as shown in Fig. 1. We follow the TSGCNet [14] to fetch the initial cross-domain features as two N × 12 matrices, but our N-domain features serve as the embedding domain instead. The semantics-based graph-transformer is a multi-scale encoder and at each scale we aggregate the features in the neighbourhood of cells with minimal semantic distance provided by the coarse semantics prediction module. The N-domain features are embedded through the adaptive feature fusion module to extract more accurate semantic information. And for the concatenated features from the different scales, we perform the global graph-transformer block respectively in the two domains and fuse them through the same adaptive fusion strategy for the subsequent cell-wise segmentation."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,2.2,Semantics-Based Graph-Transformer,"The semantics-based graph-transformer module aims to generate the multi-scale cell-wise feature vectors in C-domain embedded with N-domain which can represent the geometric features of dental mesh at different positions accurately and discriminatively. We denote the initial feature vectors extracted from the dental mesh as a N × 24 matrix, N is the number of the cells and the 24-dimensional vector consists of 12-dimensional relative coordinates and 12-dimensional normal vectors. Then through a normal STN module [3], we make the C-domain and Ndomain space invariant due to the fact that the position and orientation of dental mesh can be various. Formed as two domains of N × 12 matrices which represent the spatial position and the geometric features respectively, we perform semantic prediction on the C-domain with the N-domain features embedded to generate a pseudo semantic label L = {l 1 , l 2 , ..., l n } for each cell. And then according to the semantic information, we use the graph-transformer for each cell in its local neighbourhood where the cells have the minimal semantic differences and update their features to make the difference between the cells with different labels greater. For N-domain, in a geometric sense, they can help classify the semantic label and enhance the local features, hence we mainly upsample it to adapt to the different scales of C-domain without any other modifications.Semantics Prediction. The semantic prediction is mainly used to generate a pseudo cell-wise label for each cell which can effectively extract the semantic information. Denote the C-domain feature as C that has a shape of N × k, and the N-domain feature as N that has a shape of N ×k, and we regress a C-domain weight and a N-domain weight which indicates the weights of the domain fusion. So we have the cross-domain features F adaptively fused as:where ⊕ is the channel-wise concatenation, and j indicates the layer of the semantics-based graph-transformer modules, and c j , n j is the adaptive weights of C-domain and N-domain in layer j, and C j , N j is the output from the previous layer. And after that we perform a simple MLP to generate the final pseudo semantic cell-wise label formed as:where softmax can get the probability that the cells belong to each class and max outputs the index of the maximum value. Thus we have the cell-wise pseudo semantic label which can be used to make the difference between the features of cells belonging to different categories greater."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Graph-Transformer.,"The graph-transformer is composed of a semantic KNN and a Transformer Encoder Block. For the input C-domain, we first construct a KNN graph based on the semantic biased Euclidean distance formed as:where cell i and cell j indicate the ith cell and jth cell and the Semantic Dist function measures the difference of the two cells which is formulated as:where λ is a positive parameter that can be set according to the specific task and l i indicates the pseudo label of the ith cell. Then we perform a transformer encoder block on each cell to get the local dependencies which can enhance the local features belong to each class. The attention we used is a standard multihead attention, and we set the query and value as the matrices of neighbour features of the cells and the key as the distance matrix between cells and their neighbours."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,2.3,Adaptive Cross-Domain Feature Fusion,"The adaptive cross-domain feature fusion module aims to fuse the C-domain and N-domain features for the cell-wise segmentation. Through the above semanticsbased graph-transformer module we have obtained the accurate multi-scale Cdomain features embedded by the N-domain features. Then we need to fuse the features to integrate the spatial information and the geometric information of the cells. Therefore, we first perform the concatenation on the features of different scales and use MLP to fuse the multi-scale features together which can balance the features at different scales. This process is formulated as:where ⊕ is the channel-wise concatenation while c and n represent C-domain and N-domain features. Then through a global graph-transformer block, we just use the standard multi-head attention on the C-domain and N-domain respectively, so as to capture long distance dependencies and have global knowledge of the semantic information. Since the learnable weights can fuse the cross-domain features adaptively, we use the same cross-domain feature fusion strategy similar to that we used in the semantics prediction module. Further more, we use a single MLP to generate a feature mask and perform the dot product on the feature and the mask which is formulated as:where is the element-wise multiplication and F is the fused cross-domain features for segmentation."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3,Experiments and Results,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3.1,Implementation Details,"Our network is implemented with PyTorch 1.11.0 on four NVIDIA GeForce RTX 3090 GPUs. The input meshes are all downsampled to 12000 cells. And in the training process, we optimize the network through minimizing the cross-entropy loss which is very commonly used in the segmentation task. The learning rate was empirically set as 10 -3 , and reduced by 0.2 decay every 20 epochs."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3.2,Dataset,"Our dataset consists of 200 3D dental meshes obtained by an intraoral scanner on real orthodontic patients from hospital and each raw mesh contains even more than 150,000 cells, so we down sample the raw mesh to 12000 cells while preserving the surface topology. We randomly split the whole dataset as a training set with 160 meshes and a testing set with 40 meshes. And we segment the raw mesh into 14 teeth and gums, following the FDI World Dental Federation notation [1]. This means that each input mesh has 15 labels. For convenience, we do not distinguish between the maxillary teeth and mandibular teeth, and treat the teeth on the opposite side (from maxillary and mandibular teeth respectively) as the same class. And we augment the training set by the random translation between [-10, 10], the random rotation between [-π, π] and the random scaling between [0.8, 1.2].For evaluation, overall accuracy (OA) and mean intersection over union (mIoU) are adopted for quantitative comparison."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3.3,Comparing with SOTA Methods,"We compare our network against five recent methods, including PointNet++ [7], DGCNN [11], PVCNN [6], MeshSegNet [5] and TSGCNet [14]. For a fair comparison, we utilize the public implementations of compared methods to fine-tune their network for generating their best segmentation results. All the methods are trained for 200 epochs.The segmentation results are shown in Table 1. All the metrics show that our method outperforms the other methods a lot. Specifically, the TSGCNet [14] is the state-of-the-art one-stage method of the 3D dental mesh segmentation task which pioneered a two-stream graph convolution network to extract the features more accurately and TSGCNet [14] improve the segmentation performance on the teeth greatly compared to MeshSegNet [5]. Compared with TSGCNet [14], we still increase the OA and m-IoU on the 3D dental mesh segmentation task by 1.34% and 3.1% respectively. We also perform the qualitative experiments to further evaluate the segmentation results in Fig. 2. From the visualization results, we can find out that our method also outperforms other methods. In particular, we present some complicated cases where there may exist wisdom teeth, the crowded arrangement or the misplaced teeth. In the first row, the raw mesh has a total of 16 teeth which is different from our setting, and in this case, other methods tend to merge some of the small teeth which will cause confusion, while with semantic information, our method successfully labels all the teeth except the two wisdom teeth. In the second and third row where there exist misplaced teeth and worn teeth, we can see that the previous methods cannot perform segmentation correctly. But with adaptive feature fusion which balances the coordinate features and normal vector features, our method can segment the worn teeth accurately as well as the misplaced teeth guided by the semantic information. And the fourth row demonstrates that in terms of extracting the local geometric features, we can also achieve the superior performance. This qualitative experiment further suggests that our design is more effective and accurate on this segmentation task."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3.4,Ablation Study,"We evaluate the effectiveness of semantics prediction, graph-transformer and adaptive feature fusion as three critical components of our method. We perform the evaluation by excluding one of these critical components each time. Specifically, when we remove the semantics prediction module, we will use a naive KNN to construct the KNN graph. When we remove the graph-transformer module, we replace it with max-pooling layers. In the absence of the adaptive feature fusion module, we directly concatenate the features from C-domain and N-domain for cross-domain feature fusion. The results of the ablation study are presented in Table 2. It turns out that semantics prediction, graph-transformer and adaptive feature fusion all bring performance improvement on the 3D den- tal mesh segmentation task. And we can see that although semantics prediction module improves a little in metrics, it is primarily attributed to the fact that the segmentation errors focus on the boundary cells and the number of such errors is relatively small. But the impact of these boundary cells on the overall segmentation quality is significant. "
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,4,Conclusion,"We propose a novel semantics-based feature learning to make full use of local and unlocal semantic information to enhance the features extracted. This architecture can decouple the spatial and geometric features into C-domain and Ndomain and embed the N-domain into C-domain to further utilize the semantic pseudo label to perform the local graph-transformer module. Lastly we fuse the features from spatial and geometric domain adaptively by using the global graph-transformer module and adaptive feature fusion module. The effectiveness of our proposed method is evaluated on the real dental mesh from real orthodontic patients. In the future work, we will try to utilize the feature decoupling and fusing strategy in other segmentation tasks."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Fig. 1 .,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Fig. 2 .,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Table 1 .,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Table 2 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,1,Introduction,"Colorectal cancer (CRC) remains a major health burden with elevated mortality worldwide [1]. Most cases of CRC arise from adenomatous polyps or sessile serrated lesions in 5 to 10 years [9]. Colonoscopy is considered the gold standard for the detection of colorectal polyps. Polyp segmentation is a fundamental task in the computer-aided detection (CADe) of polyps during colonoscopy, which is of great significance in the clinical prevention of CRC.Traditional machine learning approaches in polyp segmentation primarily focus on learning low-level features, such as texture, shape, or color distribution [14]. In recent years, encoder-decoder based deep learning models such as U-Net [12], UNet++ [22], ResUNet++ [6], and PraNet [4] have dominated the field. Furthermore, Transformer [3,17,19,20] models have also been proposed for polyp segmentation, and achieve the state-of-the-art(SOTA) performance.Despite significant progress made by these binary mask supervised models, challenges remain in accurately locating polyps, particularly in complex clinical scenarios, due to their insensitivity to complex lesions and high false-positive rates. More specifically, most polyps have an elliptical shape with well-defined boundaries. However, supervised segmentation learning solely based on binary masks may not be effective in discriminating polyps in complex clinical scenarios. Endoscopic images often contain pseudo-polyp objects with strong boundaries, such as colon folds, blood vessels, and air bubbles, which can result in false positives. In addition, sessile and flat polyps have ambiguous and challenging boundaries to delineate. To address these limitations, Qadir et al. [11] proposed using Gaussian masks for supervised model training. This approach reduces false positives significantly by assigning less attention to outer edges and prioritizing surface patterns. However, this method has limitations in accurately segmenting polyp boundaries, which are crucial for clinical decision-making.Therefore, the primary challenge lies in enhancing polyp segmentation performance in complex scenarios by precisely preserving the polyp segmentation boundaries, while simultaneously maximizing the decoder's attention on the overall pattern of the polyps.In this paper, we propose a novel transformer-based polyp segmentation framework, PETNet, which addresses the aforementioned challenges and achieves SOTA performance in locating polyps with high precision. Our contributions are threefold:• We propose a novel Gaussian-Probabilistic guided semantic fusion method for polyp segmentation, which improves the decoder's global perception of polyp locations and discrimination capability for polyps in complex scenarios.• We evaluate the performance of PETNet on five widely adopted datasets, demonstrating its superior ability to identify polyp camouflage and small polyp scenes, achieving state-of-the-art performance in locating polyps with high precision. Furthermore, we show that PETNet can achieve a speed of about 27FPS in edge computing devices (Nvidia Jetson Orin). • We design several polyp instance-level evaluation metrics, considering that conventional pixel-level calculation methods cannot explicitly and comprehensively evaluate the overall performance of polyp segmentation algorithms."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2,Methods,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2.1,Architecture Overview,"As shown in Fig. 1, PETNet is an end-to-end polyp segmentation framework consists of three core module groups. (1) The Encoder Group employs a vision transformer backbone [18] cascaded with a mixed transformer attention layer to encode long-range dependent features at four scales. (2) The Gaussian-Probabilistic Modeling Group consists of a Gaussian Probabilistic Guided UNet-like decoder branch(GUDB) and Gaussian Probabilistic-Induced transition(GIT) modules. (3) The Ensemble Binary Decoders Group includes a UNet-like structure branch(UDB) [12], a fusion module(Fus), and a cascaded fusion module(CFM) [3].  "
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2.2,Encoder Group,"To balance the trade-off between computational speed and feature representation capability, we utilize the pre-trained PVTv2-B2 model [18] as the backbone.Mixed transformer attention(MTA) layer is composed of Local-Global Gaussian-Weighted Self-Attention (LGG-SA) and External Attention (EA). We add a MTA layer to encode the last level features, enhancing the model's semantic representation and accelerating the training process [16]. Moreover, the encoder output features are presented as {X E i } 4 i=1 with channels of [2C, 4C, 8C, 16C]."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2.3,Gaussian-Probabilistic Modeling Group,"To incorporate both polyp location probability and surface pattern information in a progressive manner, we propose the Gaussian Probabilistic-induced Transition (GIT) method. This method involves the interaction between a Gaussian auxiliary decoder and multiple binary decoders in a layer-wise fashion, as shown in Fig. 2.Gaussian Probabilistic Mask. Inspired by [11] and [21], in addition to utilizing binary representation, polyps can also be represented as probability heatmaps with blurred borders. We present a method of converting the binary polyp maskW ×H×1 by utilizing elliptical Gaussian kernels. Specifically, for every polyp in a binary mask, after masking other polyp pixels as background, we calculatewhere (x o , y o ) is the mass of each polyp in the binary image f (x, y). To rotate the output 2D Gaussian masks according to the orientation, we set a, b, c as followings,where σ 2 x and σ 2 y are the polyp size-adaptive standard deviations [21], and θ is the orientation of each polyp [11]. Finally, we determine the final Gaussian probabilistic mask P G for all polyps within an image mask by computing the element-wise maximum.Gaussian Guided UNet-Like Decoder Branch. The Gaussian Guided UNet-like decoder branch(GUDB) module is a simple UNet-like decoding branch supervised by Gaussian Probabilistic masks. We employ four levels of encoder output features, and adjust encoder featureswith channels of [C, 2C, 2C, 2C] in each level. At the final layer, a 1 × 1 convolution is used to convert the feature vector to one channel, producing a size of H × W × 1 Gaussian mask.Gaussian Probabilistic-Induced Transition Module. We use the Gaussian probabilistic-induced transition module(GIT) to achieve transition between binary features and gaussian features. Given the features originally sent to the Decoder as binary features {X B i } 4 i=1 , and the transformed encoder features sent to GUDB as X G . We first splits 4 levels of X B and X G into fixed groups as:where M is the corresponding number of groups. Then, we periodically arrange groups of X B i,m and X G i,m for each level, and generate the regrouped feature Q i ∈ R (Ci+Cg)×Hi×Wi in an Multi-layer sandwiches manner. Soft grouping convolution [7] is then applied to provide parallel nonlinear projections at multiple fine-grained sub-spaces (Fig. 2). We further introduce residual learning in a parallel manner at different group-aware scales. The final outputCi×Hi×Wi is obtained for the UDB decoder. Considering the computation cost, The binary features X B ← X E for the Fus decoder have channel numbers of [4C, 4C, 4C, 4C]. The Fus decoder and CFM share identical transited output features, while CFM exclusively utilizes the last three levels of features."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2.4,Ensemble Binary Decoders Group,"During colonoscopy, endoscopists often use the two-physician observation approach to improve the detection rate of polyps. Building on this manner, we propose the ensemble method that integrates multiple simple decoders to enhance the detection and discrimination of difficult polyp samples. We demonstrate the effectiveness of our approach using three commonly used convolutional decoders. After GIT process, diverse level of Gaussian probabilistic-induced binary features were sent to these decoders. The output mask P is obtained by element-wise summation of P i , where i represents the binary decoder index.Fusion Module. As shown in Fig. 1, Set X i, i∈(1, 2, 3, 4) represent multiscale mixed features. Twice convolution following with bilinear interpolation are applied to transform these feature with same 4C channels as X 1 , X 2 , X 3 , X 4 . Afterward, we get X out with the resolution of H/4 × W/4 × C1 through following formula, where F represents twice 3×3 convolution:UNet Decoder Branch and CFM Module. The structure of the UDB is similar to that of the GUDB, except for the absence of channel reduction prior to decoding. In our evaluation, we also examine the decoder CFM utilized in [3], which shares the same input features (excluding the first level) as the Fus.3 Experiments"
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.1,Datasets Settings,"To evaluate models fairly, we completely follow P raNet [4] and use five public datasets, including 548 and 900 images from ClinicDB [2] and Kvasir-SEG [5] as training sets, and the remaining images as validation sets. We also test the generalization capability of all models on three unseen datasets (ETIS [13] with 196 images, CVC-ColonDB [8] with 380 images, and EndoScene [15] with 60 images). Training settings are the same as [3]."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.2,Loss Setting,"Our loss function formulates as L = N i=1 L i + λL g , andwhere N is the total number of binary decoders, L g represents the L1 loss between the ground truth Gaussian mask G G and GUDB prediction mask P G . λ is a hyperparameter used to balance the binary and Gaussian losses. Furthermore, we employ intermediate decoder outputs to calculate auxiliary losses for convergence acceleration."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.3,Evaluation Metrics,"Conventional evaluation metrics for polyp segmentation are typically limited to pixel-level calculations. However, metrics that consider the entire polyp are also crucial. Here we assess our model from both pixel-level and instance-level perspectives.Pixel-level evaluation is based on mean intersection over union (mIoU ), mean Dice coefficient (mDic), and weighted F 1 score (wF m ). For polyp instance evaluation, a true positive (TP) is defined when the detection centroid is located within the polyp mask. False positives (FPs) occur when a wrong detection output is provided for a negative region, and false negatives (FNs) occur when a polyp is missed in a positive image. Finally, we compute sensitivity nSen = T P/(T P + F N) × 100, precision nPre = T P/(T P + F P ) × 100, and nF 1 = 2 × (Sen × Pre)/(Sen + Pre) × 100 based on the number count for instance evaluation."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.4,Results,"Training and Learning Ability. Table S1 displays the results of our model's training and learning performance. Our model achieves comparable performance to the SOTA model on the Kvasir-SEG and ClinicDB datasets. Notably, our model yields superior results in false-positive instance evaluation.Generalization Ability. The generalization results are shown in Table 1. We conduct three unseen datasets to test models' generalizability. Results show that P ET Net achieves excellent generalization performance compared with previous models. Most importantly, our false-positive instance counts(45 in ETIS and 55 in CVC-ColonDB) reduce significantly of other models. We also observe a performance mismatch phenomenon in pixel-level evaluation and instance-level evaluation.Small Polyp Detection Ability. The detection capability results of small polyps are shown in Table 2. Diminutive polyps are hard to precisely detect, while they are the major targets of optical biopsies performed by endoscopists. We selected images from two unseen datasets with 0∼2% polyp labeled area to perform the test. As shown, P ET Net demonstrates great strength in both datasets, which indicates that one of the major advantages of our model lies in detecting small polyps with lower false-positive rates.Ablation Analysis. Table 3 presents the results of our ablation study, where we investigate the contribution of the two key components of our model, namely the Gaussian-Probabilistic Guided Semantic Fusion method and ensemble decoders. We observe that while the impact of each binary decoder varies, all sub binary decoders contribute to the overall performance. Furthermore, the GIT method significantly enhances instance-level evaluation without incurring performance penalty in pixel-level evaluation, especially in unseen datasets.   "
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.5,Comparative Analysis,"Fig. S1 shows that our proposed model, P ET Net, outperforms SOTA models in accurately identifying polyps under complex scenarios, including lighting disturbances, water reflections, and motion blur. Faluire cases are shown in Fig. S2."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.6,Running in the Real World,"Furthermore, we deployed P ET Net on the edge computing device Nvidia Jetson Orin and optimized its performance using TensorRT. Our results demonstrate that P ET Net achieves real-time denoising and segmentation of polyps with high accuracy, achieving a speed of 27 frames per second on the device(Video S1)."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,4,Conclusion,"Based on intrinsic characteristics of the endoscopic polyp image, we specifically propose a novel segmentation framework named PETNet consisting of three key module groups. Experiments show that P ET Net consistently outperforms most current cutting-edge models on five challenging datasets, demonstrating its solid robustness in distinguishing other intestinal analogs. Most importantly, P ET Net shows better sensitivity to complex lesions and diminutive polyps."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Fig. 1 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Fig. 2 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Table 1 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Table 2 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Table 3 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_54.
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,1,Introduction,"Post-ischemic Ventricular Tachycardia (VT) is an arrhythmia that occurs after a myocardial infarction event. During Myocardial Infarction (MI), the blood flow to an area of the heart is blocked, producing tissue death and scarring [8]. After tissue healing, partially viable areas can appear within the scar tissue. These areas, usually referred to as Border Zone (BZ), contain a complex mixture of scar and viable myocardium [8]. These can produce channel-like structures that act as paths for an abnormal depolarization wave re-entry that sustains the VT [8,22].Catheter ablation is recommended for ischemic heart disease patients, in the presence of recurrent monomorphic VT despite anti-arrhythmic drug therapy. Ablation therapy aims at destroying a part of the re-entry path (channel) that sustains the VT [22]. Precise identification of the target for ablation is paramount to maximize the efficacy of the procedure in terms of reduction of VT recurrence, while minimizing the amount and extension of ablation lesions. Both intracardiac electrograms and pre-operative Late Gadolinium Enhanced MRI have been adopted for channel identification [25], however the rate of success of catheter ablation is still unsatisfactory [22,25].Virtual-Heart Arrhythmia Ablation Targeting (VAAT) methods have been developed to improve the utility of pre-operative imaging by using image-based computational models to reproduce the patient-specific re-entrant dynamics of the arrhythmia and thus increase precision in the localization of ablation targets [2,9,25]. These approaches generally suffer from two limitations. On the one hand, they have not shown the capability to consistently reproduce the same reentries observed in-vivo and as measured by 12-lead ECG. On the other hand, they require a laborious process and time-intensive simulations incompatible with clinical use.In this paper, we present a validation study of a novel VAAT method based on efficient phenomenological models of electrophysiology for high-fidelity simulation of VT. We successfully reproduce the inducibility of sustained monomorphic VT (more than 30 s) in two animal models of scar-related VT. In addition, we reproduce the ECG signature of each measured VT circuit, in terms of the polarity in the QRS complex of each lead as well as the VT cycle length. Finally, we demonstrate the feasibility of using this method in clinical practice by showing considerable speed up compared to the state-of-the art."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2,Methods,
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.1,Data Description,"Two porcine models of MI [11] were used for this study. The animal study was reviewed and approved by the Johns Hopkins University Animal Care and Use Committee (Baltimore, MD). High-resolution LGE-MRI (Aera, Siemens Healthineers) images of the subjects were acquired 6 weeks after the MI induction procedure. One week after, the swine underwent the VT ablation procedure. During the procedure, one VT was successfully induced in case 1, and three different VTs were induced in case 2. The 12-lead ECG (CardioLab, GE Healthcare) was acquired pre-operatively and during VT for both cases. The QRS and QT duration in the ECGs measured during sinus rhythm were annotated by an electrophysiologist. Additionally, the ECG VT morphology was reviewed by an electrophysiologist, which determined the VT cycle length and estimated the approximate exit site location using the ECG lead polarity [1]."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.2,Anatomical Model Generation,"The right ventricular (RV) endocardium, left ventricular (LV) endocardium, and LV epicardium were manually contoured and reviewed by an electrophysiologist. The computational domain for the electrophysiology model was defined as a Cartesian grid of isotropic 0.5 mm resolution, obtained by rasterization of the segmented surfaces [18]. A rule-based model of the myocardial fibers was also included [3]. The complete anatomical generation process was applied as previously described [26]. A generic swine torso model was manually deformed and oriented to fit the scout MRI images of each subject.The scar and BZ were semi-automatically segmented by an expert and validated by an electrophysiologist. The full-width half-maximum method [23] was applied to the stack of LGE-MRI images after resampling to an isotropic 3D grid of 0.4 mm. The BZ segmentation was constrained to be within a maximum distance of 4 mm from the closest scar following the assumption that BZ tissue closely surrounds the scar core [23], which was consistent with measured voltage maps. The tissue segmentation was then post-processed. First, we applied morphological closing with a spherical kernel of 4 voxel radius to close small holes of BZ within the scar. Then we applied morphological opening with a spherical kernel of 1 voxel radius to remove small and isolated scar patches. Lastly, we applied a dilation (max 2 mm) of the scar and BZ tissues in the transmural direction to preserve transmurality after the domain rasterization. Preserving scar transmurality prevents the generation of spurious endocardial or epicardial re-entry pathways inconsistent with evidence from LGE-MRI and voltage maps. See the Supplementary Materials for more details about the implementation."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.3,Electrophysiology Simulation,"The electrophysiology model is based on the monodomain equation using the modified Mitchell-Schaeffer cellular model [6,19]. The numerical solver is an efficient GPU implementation of the Lattice Boltzmann Method [17,21]. The time-varying extracellular potential on the animal torso is computed following the Boundary Element Method (BME) [26]. Virtual electrodes were placed on the animal torso in the locations used in the clinical study, which were validated by an electrophysiologist, and virtual 12-lead ECG signals were derived [17]. Each individual lead signal amplitude is then normalized to mitigate uncertainty on the torso anatomy.Scar tissue was modeled as a non-conductive material. The BZ tissue was modeled as conductive, isotropic, and with a longer Action Potential Duration (APD) than the healthy tissue [16]. The transversal Conduction Velocity (CV) in the healthy myocardium was set to 0.45 times the longitudinal CV [7].To reproduce the fast conduction produced by the Purkinje network, a Fast Endocardial Conductive (FEC) layer of 3 mm was added to the model to account for the increased transmurality of Purkinje networks in porcine models [24]. Longitudinal CV was set 4 times higher than in the healthy myocardium, while the transversal CV was unchanged [7,14]. The FEC layer was not placed over scar or BZ areas, which were measured as non-conductive by voltage mapping during the ablation procedure."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.4,Parameter Personalization,"The model parameters were tuned to match the ECG measurements during sinus rhythm. To simulate sinus rhythm, a one-second-long simulation was run with four pacing points placed in the basal and apical areas of the LV and RV septal walls to produce depolarization patterns consistent with previously published observations [10]. A grid-search approach was used to optimize the myocardial diffusivity and the closing time constant of the current gate (τ close ) of the Mitchell and Schaeffer cellular model, to match the measured QRS and QT interval respectively.Myocardial diffusivity was sampled in a range of values corresponding to CV between 0.4 and 0.75 m/s, with 0.01 m/s steps, consistent with previous approaches [7,14]. After adjusting the CV, the τ close parameter was sampled in a range of values corresponding to myocardial APD between 0.14 and 0.3 s with 0.005 s steps, consistent with reported values [13]. The APD was defined as the amount of time with voltage higher than 20% of its maximum value for each experiment.The reference QRS and QT duration in the measured ECGs were annotated by an electrophysiologist in a digital trace plotted with a paper speed of 50 mm/s. The QRS and QT duration in the simulated ECGs were visually annotated by an expert and the set of parameters leading to the best match with the measured ECG were validated by an electrophysiologist. Due to the infarction features present in the ECG, such as ST elevation [12], the measurement procedure and thus the optimization procedure could not be fully automated, since automatic extraction of QRS and QT duration led to a relatively large uncertainty margin of up to 20 ms depending on the selected ECG lead and signal morphology.Electrophysiology properties of the BZ were not personalized based on sinus rhythm ECG measurements, since we observed minimal effect on simulated sinus rhythm QRS and QT duration from variations of the BZ parameters within the range of previously reported values [7]."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.5,VT Induction Procedure,"The simulation of a programmed stimulation experiment included three phases: model preconditioning, artificial stimulation delivery, and spontaneous activity simulation. First, the internal states of the model were pre-conditioned by simulating 5 s of sinus rhythm at 60 bpm. Then, 8 stimuli were delivered at a constant period S1, which ranged between 0.3 and 0.6 s, followed by three extra stimuli delivered with delays S2, S3, and S4. The pacing times S2, S3, and S4 were automatically reduced until VT was induced or no activation was produced, following the procedure described in the Supplementary Materials. The stimuli were delivered from one point at a time, which could be located in the RV outflow tract, RV apex, or each AHA region barycenter. Other points in proximity to areas that were visually identified as possible re-entry channels were additionally tested. For efficiency purposes, the internal states of the simulation were saved after the preconditioning phase and after the S1 stimuli train and were re-used for later simulations.To elucidate the role of Electrophysiological (EP) properties of the BZ on the generation and maintenance of VT, multiple programmed stimulation experiments were conducted with varying BZ parameters. BZ diffusivity was sampled in a range of values corresponding to CV between 0.14 and 0.23 m/s, with 0.01 m/s steps, consistent with previous approaches [5,16]. BZ APD was increased from 0.015 s to 0.06 s over the myocardial APD with 0.015 s steps, consistent with reported values [5,16].All simulations were performed on a high-performance GPU cluster with 24 GPUs (Tesla-V100-SXM2, NVIDIA Corporation) scheduled to perform multiple experiments in parallel. Additionally, to save computation time, the simulations were stopped 1 s after the last delivered stimulus if no spontaneous activity was present. If spontaneous activation was present, the simulations were computed for a maximum of 30 s after the last delivered stimulus. Each simulation was classified as inducible with sustained activity if the spontaneous activity lasted for 30 s after the last stimulus. For these simulations, the pseudo-ECG was computed and the intracardiac potentials sampled in each of the AHA region barycenters were saved for later analysis."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.6,Analysis of VT Inducibility,"The intracardiac potentials of each inducible simulation were analyzed by calculating the Cycle Length (CL) of spontaneous activity using a moving window of 4 s. The mean and standard deviation time between Action Potential (AP) peaks in the moving window was computed. This was done to study the evolution of the sustained VT during its duration. Additionally, this allowed us to determine whether the VT was monomorphic, in which case CL standard deviation is minimized."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,3,Results,
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,3.1,Personalization Results,"The personalization resulted in a myocardial CV of 0.68 m/s and 0.64 m/s, which produced a QRSd consistent with the measurements of 70 ms and 80 ms, for case 1 and 2 respectively. The personalized APDs of 0.155 s for both cases produced a QTd of 370 ms and 380 ms for case 1 and case 2 respectively, which were in accordance with the measurements. The personalization was done with a BZ CV of 0.23 m/s and an APD increase of 0.03 s for both cases."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,3.2,Inducibility of VT,"The personalized values obtained in Sect. 3.1 were used for the VT inducibility simulations following the protocol described in Sect. 2.4.For case 1, the artificial stimulation was delivered from 3 points as tested in the clinical scenario: the RV apex, AHA regions 1 and 2. An additional point in the LV apex was placed near a visually identified channel. A total of 7050 virtual programmed stimulation experiments were performed with an average compute time of 5 h and 31 min per GPU, resulting in 201 induced VTs. All VTs shared the same ECG signature, and were induced after LV apex pacing. The ECG signature was consistent with that of the measured VT. Additionally, one parameter combination produced VT with the same CL as the measured VT (0.27 s). This was achieved with a BZ CV of 0.21 m/s and BZ APD of 0.17 s. The 12-lead-ECG polarity matched in all leads. A qualitative comparison is shown in Fig. 1a. After inspection of the computed time-dependent transmembrane potential field (movie included in the Supplementary Materials), it was determined that the VT was a result of a re-entry through an LV apex channel. This was consistent with the measurements as confirmed by an electrophysiologist. For case 2, the artificial pacing was delivered from the RV outflow tract, RV Apex, and LV apex. Additionally, AHA regions 17, 15, 9, 8, 7, which surrounded the scar and BZ tissue were also tested. An extra point was placed inside the BZ tissue in the RV septum. Due to the more extensive BZ area and low CL of the measured VTs in this case, a larger span of parameter values for the BZ diffusivity was tested, corresponding to BZ CVs in the range [0.14, 0.32] m/s. In this case, 26646 inducibility experiments were done with an average computation time of 27 h and 39 min per GPU. 2414 experiments produced sustained monomorphic VT, with 4 distinct ECG signatures. Two of these ECG signatures were identified in VTs sustained by a LV apical re-entry pathway, and 2 in VTs sustained by a RV septal re-entry pathway. Both re-entry pathways are consistent with two of the VT exit sites estimated based on measured VT ECG. For these cases, one parameter combination produced optimal match between computed and measured VTs, in terms of CL and ECG lead polarity. This was achieved with BZ CV of 0.324 s and BZ APD of 0.1725 s. The best matching computed VT with LV apical exit site (VT 1) had a CL of 0.257 s (measured 0.240 s), and same lead polarity as the measured ECG. The best matching VT with RV septal exit site (VT 2) had a CL of 0.261 s (measured 0.190 s) and lead polarity consistent with the measured ECG except lead I. The qualitative comparison between the results can be observed in Figs. 1b and1c. The re-entry movies for these cases can be found in the Supplementary Materials. The clinical agreement in both VTs was confirmed by an electrophysiologist."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,3.3,Comparison with Previous Modeling Approaches,"The modeling configuration reported by Mendonca Costa et al. [7] was reproduced in our study for Case 1. A FEC layer of 0.5 mm was placed in the endocardial layer, also covering BZ and Scar tissue. The FEC layer was configured with a longitudinal CV 6 times the healthy myocardial longitudinal CV, while the transversal CV was set equal. In the endocardium over the scar and BZ tissue the isotropic CV was set to 6 times the BZ CV. Our personalization approach was tested with this configuration, but we could not match the sinus rhythm QRSd with the largest considered myocardial CV of 0.75 m/s. The inducibility tests were repeated for this configuration using the baseline parameters reported by Mendonca-Costa et al. No sustained inducibility was achieved in this case."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,4,Discussion,"In this manuscript, a pipeline and methodology to simulate monomorphic VT reentrant dynamics was described. This method utilized fast and computationally efficient methods to solve the monodomain equation for cardiac electrophysiology, and phenomenological models for the cellular AP description. The results of this paper show that this approach allows to match clinical measurements of monomorphic VT, matching both the cycle length and ECG lead polarity, after myocardial parameter personalization and BZ parameter exploration. The optimal BZ tissue properties, maximizing the agreement between simulated and measured VT, could not be identified based on commonly adopted modeling assumptions (e.g. as a pre-defined ratio of myocardial tissue properties). Instead, we observed significant case-by-case variability, suggesting that improved methods for estimation of BZ properties are necessary to increase fidelity of Virtual Heart models for monomorphic VT simulation. Although this was only validated in 2 cases, we plan to add more in the continuation of the project. To our knowledge, this is the largest validation study of Virtual Heart models focusing on VT ECG. Lopez Perez et al. performed a similar study only using one case, showing a good agreement of VT morphology but not CL matching [15].Our results show that BZ parameter exploration is necessary to reproduce the clinical VT. Accurately determining BZ tissue properties in a prospective setting, when VT ECG measurements are not available, requires future investigation. We hypothesize that it is possible to identify BZ features in sinus rhythm ECG measurements, possibly using more complex signal features than QRS and QT duration, although this has not been investigated in our study.Additionally, we reproduced the alternative modeling approach described by Mendonca Costa et al. [7]. This approach showed a sub-optimal match of sinus rhythm ECG measurements. VT inducibility testing resulted in no sustained activity for the case considered in this study. We hypothesize that the lack of VT inducibility is related to incorrect selection of the BZ properties, and also to the assumption of a FEC covering the endocardial scar. This could produce spurious sub-endocardial re-entry pathways, preventing the maintenance of wave re-entry. Modeling assumptions can have a significant impact on the fidelity of the results, so it is important to evaluate them individually in the context of the available observations from each specific case.The proposed pipeline is potentially capable of delivering results with times compatible with clinical practice: for a given parameter combination, a virtual programmed stimulation experiment requires 20 min of computation on a single GPU (i.e. 41 s per simulated second). This is faster than other approaches: 1 h per simulated second [20], or 17 min per simulated second (VARP) [4]. Nonetheless, absent a definitive way to characterize BZ parameters, the grid search requires extensive computational efforts, limiting direct application in a clinical setting. Other methods can identify plausible re-entry circuits in almost real-time using simplified electrophysiology models [4]. In contrast, our work focuses on identifying the circuits of clinical relevance, with the goal of proposing the minimal set of ablation targets with the maximal efficacy.Clinical applicability will also require the full automation of several steps of this pipeline, including the tissue segmentation, parameter personalization, and unique VT signature characterization. At the current stage of our research, we have focused on careful manual curation of the input data to help reduce the potential impact of uncertainty due to data quality or algorithm performance.An additional limitation of this study is that the virtual ECG model is not able to produce the full range of ECG morphologies observed in measured signals, in particular high frequency features in ECG leads including changes in slope, narrowing and notches within the QRS complex, as observed in Figs. 1a, 1b, and 1c. These signal characteristics might be produced by tissue properties heterogeneity which we do not include in our model. Nonetheless, the good agreement achieved in lead polarity suggests that the simulated re-entrant VTs follows a similar re-entry pathway as the corresponding measured VT.The Virtual Heart model was not capable to match all measured VTs in case 2 in this study, characterized by a comparatively larger infarct extent, suggesting that additional VT exit sites may have been manifest in vivo, while not being produced in the model. As observed in previous computational modeling studies, we still have an incomplete understanding of the role of uncertainty on scar and BZ extent, as determined by the image processing and segmentation pipeline, on the model fidelity. This is a limitation of the current study to be addressed with larger validation studies including a wide variety of infarction presentations in pre-operative imaging."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,,Fig. 1 .,
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,1,Introduction,"Breast cancer (BC) is the most common cancer in women and incidence is increasing [14]. With the wide adoption of population-based mammography screening programs for early detection of BC, millions of mammograms are conducted annually worldwide [23]. Developing artificial intelligence (AI) for abnormality detection is of great significance for reducing the workload of radiologists and facilitating early diagnosis [21]. Besides using the data-driven manner, to achieve accurate diagnosis and interpretation of the AI-assisted system output, it is essential to consider mammogram domain knowledge in a model-driven fashion.Authenticated by the BI-RADS lexicon [12], the asymmetry of bilateral breasts is a crucial clinical factor for identifying abnormalities. In clinical practice, radiologists typically compare the bilateral craniocaudal (CC) and mediolateral oblique (MLO) projections and seek the asymmetry between the right and left views. Notably, the right and the left view would not have pixel-level symmetry differences in imaging positions for each breast and biological variations between the two views. Leveraging bilateral mammograms (Bi-MG) is one of the key steps to detect asymmetrical abnormalities, especially for subtle and non-typical abnormalities. To mimic the process of radiologists, previous studies only extracted simple features from the two breasts and used fusion techniques to perform the classification [6,20,22,24,25]. Besides these simple feature-fusion methods, recent studies have demonstrated the powerful ability of transformerbased methods to fuse information in multi-view (MV) analysis (CC and MLO view of unilateral breasts) [1,16,26]. However, most of these studies formulate the diagnosis as an MV analysis problem without dedicated comparisons between the two breasts.The question of ""what the Bi-MG would look like if they were symmetric?"" is often considered when radiologists determine the symmetry of Bi-MG. It can provide valuable diagnostic information and guide the model in learning the diagnostic process akin to that of a human radiologist. Recently, two studies explored generating healthy latent features of target mammograms by referencing contralateral mammograms, achieving state-of-the-art (SOTA) classification performance [18,19]. None of these studies is able to reconstruct a normal pixellevel symmetric breast in the model design. Image generation techniques for generating symmetric Bi-MG have not yet been investigated. Visually, the remaining parts after the elimination of asymmetrical abnormalities are the appearance of symmetric Bi-MG. A more interpretable and pristine strategy is disentanglement learning [9,17] which utilizes synthetic images to supervise the model in separating asymmetric anomalies from normal regions at the image level.In this work, we present a novel end-to-end framework, DisAsymNet, which consists of an asymmetric transformer-based classification (AsyC) module and an asymmetric abnormality disentanglement (AsyD) module. The AsyC emulates the radiologist's analysis process of checking unilateral and comparing Bi-MG for abnormalities classifying. The AsyD simulates the process of disentangling the abnormalities and normal glands on pixel-level. Additionally, we leverage a self-adversarial learning scheme to reinforce two modules' capacity, where the feedback from the AsyC is used to guide the AsyD's disentangling, and the AsyD's output is used to refine the AsyC in detecting subtle abnormalities. To "
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2,Methodology,"In this study, the ""asymmetric"" refers to the visual differences on perception level that can arise between the left and right breasts due to any abnormality, including both benign and malignant lesions. Thus, a paired Bi-MG is considered symmetrical only if both sides are normal and the task is different from the malignancy classification study [13]. The paired Bi-MG of the same projection is required, which can be formulated as I = {x r , x l , y asy , y r , y l }. Here, x ∈ R H×W represents a mammogram with the size of H × W , x r and x l correspond to the right and left view respectively. y r , y l , y asy ∈ {0, 1} are binary labels, indicating abnormality for each side, and the asymmetry of paired Bi-MG. The framework of our DisAsymNet is illustrated in Fig. 1. Specifically, the AsyC module takes a pair of Bi-MG as input and predicts if it is asymmetric and if any side is abnormal. We employ an online Class Activation Mapping (CAM) module [10,11] to generate heatmaps for segmentation and localization. Subsequently, the AsyD module disentangles the abnormality from the normal part of the Bi-MG through the self-adversarial learning and Synthesis method."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.1,Asymmetric Transformer-Based Classification Module,"The AsyC module consists of shared encoders ψ e and asymmetric transformer layers ψ asyt to extract features and learn bilateral-view representations from the paired mammograms. In this part, we first extract the starting features f of each side (f r , f l represent the right and left features respectively) through ψ e in the latent space for left-right inspection and comparison, which can be denoted as f = ψ e (x). Then the features are fed into the ψ asyt .Unlike other MV transformer methods [1,16] that use only cross-attention (CA), our asymmetric transformer employs self-attention (SA) and CA in parallel to aggregate information from both self and contralateral sides to enhance the side-by-side comparison. This is motivated by the fact that radiologists commonly combine unilateral (identifying focal suspicious regions according to texture, shape, and margin) and bilateral analyses (comparing them with symmetric regions in the contralateral breasts) to detect abnormalities in mammography [6]. As shown in the right of Fig. 1, starting features f are transformed into query (f Q ), key (f K ), and value (f V ) vectors through feed-forward network (FFN) layers. The SA and CA modules use multi-head attention (MHA),with the number of heads h = 8, which is a standard component in transformers and has already gained popularity in medical image fields [1,16,26]. In the SA, the query, key, and value vectors are from the same features,While in the CA, we replace the key and value vectors with those from the contralateral features,Then, the starting feature f , and the attention features f SA and f CA are concatenated in the channel dimension and fed into the FFN layers to fuse the information and maintain the same size as f . The transformer block is repeated N = 12 times to iteratively integrate information from Bi-MG, resulting in the output feature f r out , f l out = ψ N =12 asyt (f r , f l ). To predict the abnormal probability ŷ of each side, the output features f out are fed into the abnormal classifier. For the asymmetry classification of paired mammograms, we compute the absolute difference of the output features between the right and left sides (f asy out = abs(f r out -f l out ), which for maximizing the difference between the two feature) and feed it into the asymmetry classifier. We calculate the classification loss using the binary cross entropy loss (BCE) L bce , denoted as L diag = L cls (y asy , y r , y l , x r , x l ) = L bce (y asy , ŷasy )+L bce (y, ŷ)."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.2,Disentangling via Self-adversarial Learning,"What would the Bi-MG look like when the asymmetrical abnormalities have been removed? Unlike previous studies [18,19], which only generated normal features in the latent space, our AsyD module use weights shared U-Net-like decoders ψ g , to generate both abnormal (x ab ) and normal (x n ) images for each side through a two-channel separation, as x n , x ab = ψ g (f out ). We constrain the model to reconstruct images realistically using L1 loss (L l1 ) with the guidance of CAMs (M ), as follows, L rec = L l1 ((1 -M )x, (1 -M )x n ) + L l1 (M x, x ab ). However, it is difficult to train the generator in a supervised manner due to the lack of annotations of the location for asymmetrical pairs. Inspired by previous self-adversarial learning work [10], we introduce a frozen discriminator ψ d to impose constraints on the generator to address this challenge. The frozen discriminator comprises the same components as AsyC. In each training step, we update the discriminator parameters by copying them from the AsyC for leading ψ g to generate the symmetrical Bi-MG. The ψ d enforces symmetry in the paired Bi-MG, which can be denoted as L dics = L cls (y asy = 0, y r = 0, y l = 0, x r n , x l n ). Furthermore, we use generated normal Bi-MG to reinforce the ability of AsyC to recognize subtle asymmetry and abnormal cues, as L ref ine = L cls (y asy , y r , y l , x r n , x l n )."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.3,Asymmetric Synthesis for Supervised Reconstruction,"To alleviate the lack of annotation pixel-wise asymmetry annotations, in this study, we propose a random synthesis method to supervise disentanglement.Training with synthetic artifacts is a low-cost but efficient way to supervise the model to better reconstruct images [15,17]. In this study, we randomly select the number n ∈ [1, 2, 3] of tumors t from a tumor set T inserting into one or both sides of randomized selected symmetric Bi-MG (x r , x l |y asy = 0). For each tumor insertion, we randomly select a position within the breast region. The tumors and symmetrical mammograms are combined by an alpha blending-based method [17], which can be denoted by x|fake =The alpha weights α k is a 2D Gaussian distribution map, in which the co-variance is determined by the size of k-th tumor t, representing the transparency of the pixels of the tumor. Some examples are shown in Fig. 1. The tumor set T is collected from real-world datasets. Specifically, to maintain the rule of weaklysupervised learning of segmentation and localization tasks, we collect the tumors from the DDSM dataset as T and train the model on the INBreast dataset.When training the model on other datasets, we use the tumor set collected from the INBreast dataset. Thus, the supervised reconstruction loss is L syn = L l1 (x|real, x n |fake), where x|real is the real image before synthesis and x n |fake is the disentangled normal image from the synthesised image x|fake."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.4,Loss Function,"For each training step, there are two objectives, training AsyC and AsyD module, and then is the refinement of AsyC. For the first, the loss function can be denoted byThe values of weight terms λ 1 , λ 2 , λ 3 , and λ 4 are experimentally set to be 1, 0.1, 1, and 0.5, respectively. The loss of the second objective is L ref ine as aforementioned."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3,Experimental,
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3.1,Datasets,"This study reports experiments on four mammography datasets. The INBreast dataset [7] consists of 115 exams with BI-RADS labels and pixel-wise anno-tations, comprising a total of 87 normal (BI-RADS = 1) and 342 abnormal (BI-RADS = 1) images. The DDSM dataset [3] consists of 2,620 cases, encompassing 6,406 normal and 4,042 (benign and malignant) images with outlines generated by an experienced mammographer. The VinDr-Mammo dataset [8] includes 5,000 cases with BI-RADS assessments and bounding box annotations, consisting of 13,404 normal (BI-RADS = 1) and 6,580 abnormal (BI-RADS = 1) images. The In-house dataset comprises 43,258 mammography exams from 10,670 women between 2004-2020, collected from a hospital with IRB approvals. In this study, we randomly select 20% women of the full dataset, comprising 6,000 normal (BI-RADS = 1) and 28,732 abnormal (BI-RADS = 1) images. Due to a lack of annotations, the In-house dataset is only utilized for classification tasks. Each dataset is randomly split into training, validation, and testing sets at the patient level in an 8:1:1 ratio, respectively (except for that INBreast which is split with a ratio of 6:2:2, to keep enough normal samples for the test).Table 1. Comparison of asymmetric and abnormal classification tasks on four mammogram datasets. We report the AUC results with 95% CI. Note that, when ablating the ""AsyC "", we only drop the ""asyt"" and keep the encoders and classifiers.  "
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3.2,Experimental Settings,"The mammogram pre-processing is conducted following the pipeline proposed by [5]. Then we standardize the image size to 1024 × 512 pixels. For training models, we employ random zooming and random cropping for data augmentation. We employ the ResNet-18 [2] with on ImageNet pre-trained weights as the common backbone for all methods. The Adam optimizer is utilized with an initial learning rate (LR) of 0.0001, and a batch size of 8. The training process on the INBreast dataset is conducted for 50 epochs with a LR decay of 0.1 every 20 epochs. For the other three datasets, the training is conducted separately on each one with 20 epochs and a LR decay of 0.1 per 10 epochs. All experiments are implemented in the Pytorch framework and an NVIDIA RTX A6000 GPU (48 GB). The training takes 3-24 h (related to the size of the dataset) on each dataset.To assess the performance of different models in classification tasks, we calculate the area under the receiver operating characteristic curve (AUC) metric. The 95% confidence interval (CI) of AUC is estimated using bootstrapping (1,000 times) for each measure. For the segmentation task , we utilize Intersection over Union (IoU), Intersection over Reference (IoR), and Dice coefficients. For the localization task , we compute the mean accuracies of IoU or IoR values above a given threshold, following the approach [11]. Specifically, we evaluated the mean accuracy with thresholds for IoU at 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, and 0.7, while the thresholds for IoR are 0.1, 0.25, 0.5, 0.75, and 0.9."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3.3,Experimental Results,"We compare our proposed DisAsymNet with single view-based baseline ResNet18, attention-driven method HAM [11], MV-based late-fusion method [4], current SOTA MV-based methods cross-view-transformer (CVT) [16], and attention-based MV methods proposed by Wang et al., [20] on classification, segmentation, and localization tasks. We also conduct an ablation study to verify the effectiveness of ""AsyC "", ""AsyD"", and ""Synthesis"". Note that, the asymmetric transformer (asyt) is a core component of our proposed ""AsyC "". Thus, when ablating the ""AsyC "", we only drop the asyt and keep the encoders and classifiers. The features from the Bi-MG are simply concatenated and passed to the classifier."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,,Comparison of Performance in Different Tasks:,"For the classification task, the AUC results of abnormal classification are shown in Table 1. Our method outperforms all the single-based and MV-based methods in these classification tasks across all datasets. Furthermore, the ablation studies demonstrate the effectiveness of each proposed model component. In particular, our ""AsyC"" only method already surpasses the CAT method, indicating the efficacy of the proposed combination of SA and CA blocks over using CA alone. Additionally, our ""AsyD"" only method improves the performance compared to the late-fusion method, demonstrating that our disentanglement-based self-adversarial learning strategy can refine classifiers and enhance the model's ability to classify anomalies and asymmetries. The proposed ""Synthesis"" method further enhances truth from the ""Synthesis"" method, our generator tends to excessively remove asymmetric abnormalities at the cost of leading to the formation of black holes or areas that are visibly darker than the surrounding tissue because of the limitation of our discriminator and lack of pixel-level supervision. The incorporation of proposing synthetic asymmetrical Bi-MG during model training can lead to more natural symmetric tissue generation."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,4,Conclusion,"We present, DisAsymNet, a novel asymmetrical abnormality disentangling-based self-adversarial learning framework based on the image-level class labels only. Our study highlights the importance of considering asymmetry in mammography diagnosis in addition to the general multi-view analysis. The incorporation of pixel-level normal symmetric breast view generation boosts the classification of Bi-MG and also provides the interpretation of the diagnosis. The extensive experiments on four datasets demonstrate the robustness of our DisAsymNet framework for improving performance in classification, segmentation, and localization tasks. The potential of leveraging asymmetry can be further investigated in other clinical tasks such as BC risk prediction."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,,Fig. 1 .,
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,,Fig. 2 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,1,Introduction,"Multi-slice cine cardiovascular magnetic resonance (CMR) scanning is a common method for the accurate diagnosis and evaluation of cardiovascular diseases [16,25]. Although this provides a series of images of the heart and blood vessels over time, it is often a lengthy process that obtains only certain slices of the heart which limit the visualization of certain structures, and has breath-hold motion artefacts resulting in misalignment between slicesTo better evaluate heart disease, plan interventions, and monitor heart disease, a 3D heart model can be created from cine CMRs, which is a digitized heart object visualized as triangular meshes [9]. This reconstruction is accomplished in several steps: segmentation, registration, reconstruction, refinement, and visualisation. Because of the time cost and expert knowledge required for this task, it is desirable to create 3D or 4D (3D+time) heart models automatically for every patient [3,23]. However, it is challenging to create accurate 3D heart models, because of the impact of low spatial resolution and motion artefacts [11,18].To this end, we proposed a Recurrent Graph Neural Network based method, ModusGraph, to fully automate the reconstruction of 4D heart models from cine CMR. This includes i) a voxel processing module with Modality Handles (Modhandle) and ResNet decoder for super-resolution and correction of motion artefacts from the acquired cine CMR, ii) a Residual Spatial-temporal Graph Convolution module (R-StGCN) for 4D mesh models generation by hierarchically spatial deformation and temporal motion estimation, and iii) a Signed Distance Sampling process bridge voxel features from segmentation and vertex features from deformation."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,2,Related Works,"Surface meshing involves constructing polygonal representations of geometric objects or surfaces, and creating high-quality and feature-aware surface meshes for medical imaging applications it is of particular interest.With available large-volume training data and advanced computational resources, more studies harness the strength of deep learning and traditional methods to avoid user supervision. Aubert et al. [2] use convolutional neural networks to automatically detect anatomical landmarks for spine reconstruction. Ma et al. [22] propose a dense SLAM technique for colon surface reconstruction. Gopinath et al. [7] presented SegRecon, an end-to-end deep learning approach that for simultaneous reconstruction and segmentation of cortical surfaces directly from an MRI volume. Wang et al. [26] reconstructed 3D surfaces from 2D images using a neural network that learns a Signed Distance Function (SDF) representation from 2D images. Ma et al. [21] proposed a deep learning framework that uses neural ordinary differential equations (ODEs) for efficient cortical surface reconstruction from brain MRI scans. Similarly, Lebrat et al. [17] presents CorticalFlow, a geometric deep learning model that learns to deform a reference template mesh towards a targeted object in a 3D image, by solving ODEs from stationary velocity fields.In contrast to those methods applying directly to the surface manifold, others, similar to our method, combine image segmentation with explicit surface representations and mesh deformation with coarse to fine controls. Wickramasinghe et al. [27] introduced Voxel2Mesh, a two-stage method that uses a CNN for voxel labelling and a GCN for mesh generation. Bongratz et al. [4] presented a deep learning algorithm that reconstructs explicit meshes of cortical surfaces from brain MRI scans using a convolutional and graph neural network, resulting in four meshes. Kong et al. [14] proposed a method that generates simulation-ready meshes of cardiac structures using atlas-based registration and shape-preserving interpolation. They also introduced a deep learning approach that constructs whole heart meshes by learning to deform a small set of deformation handles on a whole heart template [15]. Here, we utilize sparse CMRs to generate temporally coherent dynamic meshes for the cardiac cycle, leveraging unpaired high-resolution CT datasets (Fig. 1). These meshes are geometrically and topologically well-defined, with trackable vertices across consecutive frames. Such features enhance the analysis of myocardium function and enable biomechanical computational analysis (e.g., for stiffness or contractility estimation from finite element analysis of imaging data). "
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,3,Method,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,3.1,Voxel Processing Module,"Modality Handles. Compared to cine CMRs, CT imaging provides higher spatial resolution and enables easier high-resolution segmentation of the heart. The developed network to generate such segmentation is transferable to process cine CMRs, allowing for comparable cardiac structural information to be extracted from similar patient populations. The following module is thus proposed to estimate high-resolution segmentations from unpaired CT and cine CMR image volumes. An input image volume X CT ∈ R H×W×D is cropped around the anatomy of interest to the size of 128 × 128 × 128, and then down-sampled following bilinear interpolation method to X' CT ∈ R 16×16×16 , which enables a common low-resolution segmentation space for cine CMRs. Its predicted segmentation Ỹ' CT ∈ R 16×16×16 is generated by a CT Modality Handle (CT Mod-handle, h(X CT )), using ResNet blocks followed by ReLU non-linear activity and a oneby-one convolution layer. A MR Modality Handle (MR Mod-handle, h(X MR )) generates predicted segmentation ỸMR from cine CMRs in the same way.Super-Resolution Decoder. Ỹ' CT is passed to a decoder for super-resolution reconstruction to a size of 128×128×128. The decoder ψ includes three layers of up convolution followed by ResNet blocks. The high-resolution segmentation of cine CMR is generated similarly through ỸMR = W ψ W h X MR , where W ψ and W h are trainable weights of decoder and Mod-handle, respectively. To include heart morphological features in the graph convolution process, the signed distance is calculated from the decoder's output. This signed distance is computed as geodesic distances from each voxel to the surface boundary [1,5]. The mesh is then scaled to the output size and each mesh surface vertex is assigned a signed distance based on the output channel and vertex's coordinates."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,3.2,Residual Spatial-Temporal Graph Convolution Module (R-StGCN),"Graph convolution networks can be utilized to reshape a heart mesh model, but regressing the surface near sharp edges or areas with aggressive Laplacian changes is challenging due to the networks' lack of awareness of the position relationships. We borrowed the idea of graph construction from human joints to overcome this issue [30]. The area of the 1-connected neighbourhood near sharp edges is divided into 3 subsets k: one subset includes vertices on the valves' edges, and the other two subsets include vertices on different sides of the myocardium surfaces. This defines the robust position relationships of sharp edges and other convex areas of the mesh, allowing us to use Adaptive Graph Convolutional (AGC) layers [24] to learn such relationships.Spatial Deformation. Given a dynamic mesh at level l and frame t, i.e. M l t = (V l t , E l t ), where V l t and E l t are N vertices and M edges, respectively. A dense adjacency matrix A k ∈ R N ×N denotes the edges between every two vertices.A data-dependent matrix C k ∈ R N ×N determines the similarity of every two vertices as normalized Gaussian function,. W θk and W φk are the parameters of the 1 × 1 convolution layer θ and φ, respectively. f in ∈ R 3×T ×N is input feature matrix of the convolution layer, where T is the number of frames for each cardiac cycle. The sampling area of convolution is a 1-connected neighbourhood includes 3 subsets, which conforms with the aforementioned mesh topology. It is described asf out is output feature of the convolution layer, W k is trainable weights for the convolution. Following the AGC layer, we used graph convolution with first-order Chebyshev polynomial approximation. It is formalized as, where W θ0 , W θ1 are trainable weights and L = 2L norm /λ max -I, L ∈ R N ×N is the scaled and normalized Laplacian matrix [6]. The signed distance was added to mesh vertices prior to the graph convolution, and a straightforward Loop method [19] for surface subdivision is applied to refine the coarse mesh.Temporal Deformation. The deformation field vector V t-1→t and V t→t-1 are learnt through temporal convolutions, where the sampling neighbourhood is defined as a vertex in consecutive frames. It is a T × 1 convolution performed on the output feature matrix f out in a bidirectional manner. V is regularized following the principle of motion estimation. With vertices of meshes at consecutive frames V 0 and V 1 , the vertices of intermediate mesh V t , 0 < t < 1 is approximated under symmetric assumption [28], as Ṽt = 0.5), and we measure the L1 difference between Ṽt and V t ."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,3.3,Training Scheme,"Generally, the MR Mod-handle was trained on cine CMRs and down-sampled segmentation, described as L seg,MR (h(X MR ), Y' MR ). The CT Mod-handle and ResNet decoder were trained on CT image volumes, segmentation and their down-sampled counterparts using dice loss and cross-entropy loss, i.e. L seg,CT (ψ• h(X CT ), Y CT , Y' CT ). Supervised by the ground-truth point clouds from CT segmentation, meshes were predicted from the R-StGCN module, where Chamfer distance is minimized together with surface regularization [27] and deformation field vector regularization as). P t was generated via Marching Cubes [20] and uniform surface sampling applied to the ground-truth segmentation. Similarly, pseudo point clouds Pt from super-resolved cine CMRs segmentation were used for fine-tuning the R-StGCN module, i.e. L mesh,MR . The total loss is L total = λ seg • (L seg,CT + L seg,MR ) + λ mesh • (L mesh,CT + L mesh,MR ), where λ seg = 0.5, λ mesh = 1.0 and λ reg = 0.1 were selected by extensive experiments from [0, 1]. Find a detailed training/testing scheme in Appendix V. ModusGraph is implemented with PyTorch 1.12.1 and the experiment was conducted on an RTX 3090 GPU, with Adam optimizer and a learning rate of 1e-4. The training and validation losses converge after 200 epochs in less than 2 h."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,4,Results and Discussion,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,4.1,Datasets,"The training and validation data consisted of CT image volumes from the SCOT-HEART study [12] and cine CMRs from the Cardiac Atlas Project (CAP) tetralogy of Fallot [8] database. CT data were included to provide high-resolution geometry information while tetralogy of Fallot CMR cases were used because functional analyses are important for these patients. The SCOT-HEART dataset provided 400 and 200 image volumes for training and testing, while the CAP dataset provided 84 and 48 time-series image volumes for training and testing. Data augmentation techniques included random intensity shifting, scaling, contrast adjustment, random rotation, and intensity normalization. Groundtruth segmentations of four heart chambers, left ventricle myocardium, and aorta artery from a previously validated method [29] was used for the whole heart meshing on the SCOT-HEART dataset while left and right ventricle and myocardium manual segmentations were used for the dynamic meshing with the CAP dataset. 0.25 ± 0.86 0.20 ± 0.32 0.18 ± 0.13 RES (14k) 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 MG (6k)"
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,4.2,Evaluation of Whole Heart Meshes Quality,"1.79 ± 0.68 2.39 ± 0.49 1.17 ± 0.14 0.43 ± 0.29 0.18 ± 0.10 1.50 ± 1.80We evaluated the quality of meshes generated by ModusGraph, by comparing them to those produced by other state-of-the-art methods using the SCOT-HEART dataset. ModusGraph, Voxel2Mesh [27], and CorticalFlow [17] started from the same template mesh and progressively deformed it to a finer mesh using 128 × 128 × 128, while nnU-Net [13] segmentation and Point2Mesh [10] reconstruction used a user controlled, differentiable refinement process to warp the segmentation boundary point clouds to meshes. These methods produce meshes with around 6,000 vertices for each anatomy. The Marching Cubes method was also applied to the ResNet decoder's segmentation to generate a finer mesh with around 14,000 vertices for evaluation. All methods for generating meshes had training times ranging from 2-5 hours.Regarding the Dice score and Average Surface Distance (ASD) in Table 1, RES is a high benchmark since it was straightforward to reconstruct a highresolution marching cubes mesh from the well-defined morphologies in the segmentations, but this is not suitable for tracking or computational models. Modus-Graph's mesh accuracy is compromised due to information loss in the QuickHullderived template mesh (Appendix I), alignment issues with deformed meshes and segmentation in different coordinate systems, and difficulties in capturing patient-specific geometric variations. However, refining the template mesh and registration process could potentially enhance the results. Figure 2 and the intersection scores in Table 1 that measure the ratio of surface collision show that ModusGraph can generate accurate whole-heart meshes with less surface distortion and collision when compared to its closest result from CorticalFlow."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,4.3,Dynamic Mesh for Biomechanical Simulation,"We evaluated ModusGraph and other methods on the task of creating dynamic ventricle myocardium meshes. When them to ground-truth short-axis slices segmentation, the Average Surface Distance (ASD) at end-diastole (ED) and  Mesh quality for biomechanical simulations, including aspect ratio, min and max angle, and Jacobian of surface mesh triangular cells, was also evaluated. ModusGraph showed less distorted cells, leading to faster convergence for mechanical simulations, as shown in Table 2. Changes in mesh surface from ED to ES phase were compared to a reference dynamic mesh, Bi-ventricle [8], and ModusGraph was found to more accurately describe the deformation in areas surrounding valves and the displacement of the myocardium surface, as shown in Fig 3-b. The dynamic mesh generated by ModusGraph is included in supplementary materials, along with details for creating template meshes for the two tasks and reference Bi-ventricle mesh."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,5,Conclusion,"Our proposed method, ModusGraph, automates 4D heart model reconstruction from cine CMR using a voxel processing module, a Residual Spatial-temporal Graph Convolution module and a Signed Distance Sampling process. Modus-Graph outperforms other state-of-the-art methods in reconstructing accurate 3D heart models from high-resolution segmentations on computed tomography images, and generates 4D heart models suitable for biomechanical analysis, which will aid in the understanding of congenital heart disease. This approach offers an efficient and automated solution for creating 3D and 4D heart models, with potential benefits for heart disease assessment, intervention planning, and monitoring."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Fig. 1 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Fig. 2 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Fig. 3 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Table 1 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,.92 ± 0.02 0.82 ± 0.05 0.91 ± 0.03 0.89 ± 0.03 0.90 ± 0.04 0.87 ± 0.05,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Table 2 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Acknowledgements. YD was funded by the,"Kings-China Scholarship Council PhD Scholarship Program. HX was funded by Innovate UK (104691) London Medical Imaging & Artificial Intelligence Centre for Value Based Healthcare. SCOT-HEART was funded by The Chief Scientist Office of the Scottish Government Health and Social Care Directorates (CZH/4/588), with supplementary awards from Edinburgh and Lothian's Health Foundation Trust and the Heart Diseases Research Fund. AAY and KP acknowledge funding from the National Institutes of Health R01HL121754 and Welcome ESPCR Centre for Medical Engineering at King's College London WT203148/Z/16/Z."
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,1,Introduction,"Digital Radiography (DR) and Computed Tomography (CT) techniques are extensively utilized in the diagnosis of various clinical conditions [15,24]. However, one major concern associated with these imaging methods is the exposure of patients to X-ray radiation [12]. Reducing the X-ray radiation dose unavoidably leads to a decline in the number of photons detected, resulting in measurements with a low signal-to-noise ratio and consequent regression in image quality. Consequently, accurately diagnosing clinical conditions based on degraded DR/CT images poses significant challenges. Hence, the development of sophisticated and efficient image-denoising techniques that can effectively address both DR and CT modalities becomes imperative and urgent in clinical applications.Traditional image denoising methods have relied on exploring spatial pixel features and properties in the transform domain [2,5,8]. These methods include approaches like non-local mean (NLM) [23] and BM3D [5]. However, the need for complex parameter adjustment and their relatively slow speed have limited the practical applications of these traditional denoising methods.With the advancement of neural networks, deep learning-based denoising techniques have shown superior performance compared to traditional methods [1,3,9]. Supervised denoising methods, such as U-Net [6], DnCNN [25], and RED-CNN [4], have demonstrated promising results. However, when it comes to digital radiography (DR) and computed tomography (CT), these supervised methods face challenges since they rely on paired noisy and clean image data, which are not readily available in DR and CT imaging [7].Toovercomethelimitationsofsupervisedtechniques,self-supervisedorunsupervised denoising methods have been proposed and developed, leveraging the similaritybetweennoisyimages [18,26].Severalunsupervisedlearning-basedmethodshave been introduced for image denoising, including Noise2Noise [10,13,16], Noise2Sim [20],Noise2Void [14],andNeighbor2Neighbor [11]amongothers [17].However,these methods have defects in the DR and CT denoising tasks. They either require noisy and noisy image pairs in training, or sampling strategies resulting in information missing and the same noise level of the sub-image pairs. Therefore, we designed a new self-supervised image denoising technique, i.e., the full image-index remainder (FIRE), adapting to low-dose DR and CT to address the challenge. Specifically, our FIRE method first divided the whole high-dimensional image space into a series of low-dimensional sub-image spaces with the image-index remainder technique. Based on the remainder of the full image index, a specific image sampler is designed to sample the sub-images. With this strategy, a set of sampled noisy sub-images from a single noisy image is obtained for self-supervised training of the denoising neural network without clean images. We further proved that our proposed sampling strategy is effective in supplementary materials. In addition, we proposed a new loss function to train the unsupervised image-denoising network with dedicated parameter tuning. For further optimizing the feasible domain, a regularization strategy is introduced to reduce the gap between the self-supervised and the supervised denoising network. We evaluated the FIRE on several large-scale clinical DR and CT datasets without clean data. The experimental results show that our FIRE method achieves the best results in noise suppression and visual perception. We also compared our FIRE and other methods using public clinical data with ground truth, and the quantitative and qualitative results demonstrate the out-performance of our method.The remaining sections are organized as follows. Sections 2 and 3, introduce our proposed FIRE method in terms of the network architectures, sampler design, and loss functions. In Sect. 4, we will test our model on large-scale low-dose DR and CT image datasets and compare it with other well-performing methods. Finally, a summary will be given."
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,2,Methodology,
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,2.1,Related Theories,"Given a clean image x and its corresponding noisy image y, the supervised image denoising methods try to train the denoising network f parameterized by θ minimizing the loss function below:(Implementing the supervised denoising network on noisy and clean image pairs, the usual loss function can be formulated as:However, in actual situations, it is difficult to obtain the corresponding clean and noisy image pairs, especially in medical imaging scans. To address this issue, Noise2Noise proposed a self-denoising method that does not require real clean images, but it depends on pairs of independent noisy images of the same scene. The method proves that the results obtained by minimizing the following equation are the same as the results obtained by the supervised case above:where y and z are two independent noisy images conditioned on x. In previous studies [16], it has been proved that the results of training using only noisy and noisy image pairs can be approximately equal to supervised cases. Neigh-bor2Neighbor [11] focuses on sampling a single noisy image with independent noisy and noisy image pairs from the same scene for network training. However, there are still challenges in applying this method to DR and CT image denoising tasks. Specifically, DR and CT images have a larger range of pixel values and more details in the image, while the current method uses only the part image information in each iteration. In the following section, we propose our FIRE method for DR and CT image denoising tasks."
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,2.2,Framework,"In this section, we propose a self-supervised framework for training a single noisy image-denoising network. First, we design a new subspace sampling technique for generating subspace image groups to train the network. Next, for recovering finer image details and features, a specialized loss function consisting of reconstruction terms and regularization terms is proposed and used for training the network. The overall of our FIRE framework is shown in Fig. 1  Subspace Sampling: The remainder with respect to a positive integer (i.e., N ) of image pixel coordination is excellent to be used to develop a sampler. With it, a raw noisy image can be divided into a series of sub-image spaces, and such design can fully use the advantages of the image pixels index resulting in satisfying the condition of independence. Let y ∈ R (W * H) be one noisy image sample, where W and H represent the width and height of the raw image y. Considering the size of the medical image is usually a multiple of 256, the N is chosen as 4 in this study. In fact, the N can be adjusted depending on the image size. In this case, the sampler G consists of four sub-sampler G = (g 1 , g 2 , g 3 , g 4 ).The details of the remainder sampler G can be developed as follows: a) Making the sampled pictures contain all the pixels within the original noisy image, it is necessary to ensure that the length and width of the image y can be divisible by N, which is chosen as 4; b) Encoding pixel index of the image y from the left-upper corner, all image pixels can be accessed by the abscissa i and ordinate j. The image indexes can range from (0, 0) to (W -1, H -1). c) Calculating the remainder of (2×i+j)%4 and defined it as k. k is an integer in [0, 3]. All pixel coordinations satisfying k = 0 are retained, and the remaining pixel values are set to 0 to generate the first mask. The second mask is obtained with pixel coordinations satisfying k = 1. The third and fourth masks are obtained by satisfying k = 2 and k = 3. d) Selecting the reserved pixel value from the four pixels in each 2×2 area as the pixel value for the corresponding position of the subsampled image, and put it into (g 1 , g 2 , g 3 and g 4 ). By doing this, four sampled pictures are obtained, i.e., (g 1 (y), g 2 (y), g 3 (y) and g 4 (y)). The width and length of all sampled subspace images are W/2 and H/2 respectively.To clearly demonstrate the generation diagram of the subspace image group, Fig. 1(b) summarizes the idea of our proposed remainder subspace subsampler. Since the corresponding pixels of (g 1 (y), g 2 (y), g 3 (y), g 4 (y)) generated by the sampler are adjacent but different in the original image. Each sampled subspace image is independent of the others, and the completeness and difference of image content in the four sampled sub-images can be guaranteed. Here we can divide the four sub-images into two groups of g 1 (y) and (g 2 (y), g 3 (y), g 4 (y)). Please refer to the supplementary materials for detailed explanations of this grouping.Regularization Optimization: For self-supervised training, the following minimization optimization problem is formulated by taking advantage of the constraintAccording to E x,y = E x E y|x , it can be converted into the following regularization optimization problemFinally, the loss function incorporating the regularization term is proposed to train the denoising network withwhere f θ is the denoising network and λ is a hyperparameter to balance the regularization term. "
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,3,Experiments,"In this section, we first introduce the details and configuration of clinical experiments. To evaluate the effectiveness of this method, several advanced imagedenoising methods for CT and DR were involved in the comparison. The experiments were conducted on several large-scale CT and DR data sets. Experimental Configuration: The network architecture of our proposed FIRE framework was a modified U-Net [21]. All experiments were conducted on a PC server equipped with Python3.9.7, PyTorch1.8, and NVIDIA TITAN RTX graphics processors.In terms of the comparisons, we include BM3D [5], DIP [17], Noise2Sim [20], Noise2Void [14], Blind2Unblind [22], Neighbor2Neighbor(Nei2Nei) [11] and two supervised denoising algorithms DnCNN [25] and DD-net [27]. According to the applicability of denoising methods, we chose different comparison methods in different experiments. More details can be found in Figs. 2 to   Experimental Results: In three different denoising tasks, our FIRE method showed the best results. For the clinical brain CT image denoising, the excellent denoising results of FIRE can be seen in Fig. 2. Due to a certain gap in ground truth between adjacent frames images used in network training, Noise2Sim cannot achieve good enough results. DIP produces a lot of artifacts after removing the noise and misses a lot of details and features. Besides, BM3D and DIP consume long time, and the parameters need to be manually adjusted on different denoising conditions. For the real DR image dataset, Fig. 3 demonstrates the DR image denoising results of different body parts for different patients. It can be seen that the denoising performance of Neighbor2Neighbor is limited. It may be because the Neighbor2Neighbor only uses information of half pixels for training, and the entire structure and pixel information within the DR image is partially missed. On the public CT image data set, we can intuitively compare the advantages of our method over other methods from the PSNR and SSIM measures. Figure 4 shows the typical slice denoising results. Figure 5(a)-(j) presents the noise power spectrum(NPS) of all methods, where blue indicates it is closer to the reference. As seen, our FIRE obtains the most blue and the least red results. Figure 5(k) reflects the comparison of pixel values on the profile line of a slice of the test set. Among them, our FIRE is closest to the reference. "
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,Influence of Regularization Term:,"We proposed a regularization term in Sect. 3, and the hyperparameter λ is used to adjust the regularization term. Figure 6 shows the visual results of FIRE with different λ values. When λ = 0, the regularization term is removed. When λ = 1, the denoising effect of the network is the best. As λ increases, the residual degree of noise increases. This shows that the regularization term can adjust the smoothness and noisiness of denoising. Therefore, choosing an appropriate λ value helps to obtain better denoising results. "
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,4,Conclusion,"We proposed a full image-index remainder method (FIRE) using only a single noisy image. The proposed FIRE first divided the whole high-dimensional image space into a series of low-dimensional sub-image spaces with the full image-index remainder technique. Our FIRE retains the complete information within the original noisy image. In addition, we proposed a new regularization optimization function to regularize sub-space image training by reducing the gap between the self-supervised and supervised denoising networks. Quantitative and qualitative experiment results indicate that the proposed FIRE is effective in both DR and CT image denoising."
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,,
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,Fig. 1 .,
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,Fig. 2 .,
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,Fig. 3 .,
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,4 .,
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,Fig. 4 .,
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,Fig. 5 .,
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,Fig. 6 .,
Full Image-Index Remainder Based Single Low-Dose DR/CT Self-supervised Denoising,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 44.
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,1,Introduction,"Catheter ablation surgery is a common operation that treats premature ventricular contraction arrhythmia effectively, and it mainly includes two steps: electrical physiological measurement and radiofrequency ablation. For electrical physiological measurement, it requires doctors to insert an electrode catheter in patient's heart, analyze the causes and parts of arrhythmia through the electrical signal obtained by the catheter, and finally determine the specific location of ectopic pacing and catheter ablation [1]. It highly depends on the doctors' experience and is very time-consuming [2]. Thus a noninvasive localization method that determines the ectopic pacing location in advance will significantly shorten the process of catheter ablation surgery and brings benefits for both doctors and patients.There is a trend to use computational tools based on 12-lead electrocardiogram (ECG) to analyze ablation location since ECG has been proven that can provide information about pacing areas in patients [3][4][5]. Earlier studies focused on extracting features of QRS axis through mathematical statistics and machine learning methods [6]. In recent years, deep learning has become a research hotspot due to its excellent feature extraction ability, which can automatically learn the mapping relationship between signal characteristics and pacing position [7][8][9][10]. Most methods are purely end-to-end data-driven architecture based on a large number of clinical databases. However, that will be greatly influenced by the comprehensiveness and size of the labeled data [11], especially when obtaining clinical data of whole ventricular ectopic beats is difficult.Rapid developments in computer performance and theoretical knowledge have enabled detailed, physiologically realistic whole-heart simulations of arrhythmias and pacing [12,13]. Based on this, a forward-solution computational mapping system for accurate localization of atrial and ventricular arrhythmias has been proposed, where a comprehensive arrhythmia simulation library is generated [14]. It can eliminate the impact of insufficient clinical trial data on the algorithm's accuracy. However, since the computational models are not specific, it will introduce errors in the ectopic pacing location process.In this paper, inspired by work in [14], we propose a forward-solution aided deeplearning framework to realize noninvasive prediction of ectopic pacing from 12-lead ECG. Patient-specific heart-torso forward model is built, and a time-frequency fusion network based on the local-global feature extraction module is designed. The advantages of this paper can be summarized from three aspects:1. Propose a framework that is trained based on ECG simulation data from the specific patient's CT for noninvasive cardiac ectopic pacing localization. It can eliminate the effect of insufficient clinical data and patient variance error on location accuracy. 2. Propose a network that combines time-frequency information and local-global information to achieve precise ectopic pacing location based on a small training data set. 3. Proposed method achieves great performance on PVC patient data, which demonstrates its potential for clinical cardiac treatment."
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,2,Methodology,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,2.1,Overview of Location Framework,"Figure 1 outlines our framework for cardiac ectopic pacing localization. In brief, we construct a computational model of patient-specific anatomical structures, using a suitable cardiac source model and a transfer matrix H to obtain the solution of the electrocardiography forward problem for simulating whole-ventricle focal pacing as well as the corresponding ECGs. We then utilized this modeled dataset to train a deep-learning structure to locate the ectopic pacing. The structure is first evaluated on the simulated data and subsequently tested on clinical PVC patients' ECG data. We will describe the specific principles of our pipeline in detail in the following sections. "
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,2.2,Process of Forward-Solution Obtaining,"Establishing a specific heart-torso model requires the geometric structure of both patient's heart and torso, which can be constructed from CT imaging data from the common preoperative scanning. The solution to the forward problem is obtained from the process of calculating torso surface potentials based on known cardiac source parameters [15], which can be mathematically represented as:where y denotes the torso surface and, x denotes the cardiac source, A is a transfer function dependent on the source model, which has been demonstrated in prior research that can be expressed as a transfer matrix H ∈ R N * M . . This transfer matrix can be calculated through the resolution of the relationship between the cardiac and torso domains based on the finite element method (FEM) or boundary element method (BEM). In terms of the cardiac source, the two most common models are the activation-based model and the potential-based model [16]. One regards the arrival time of the depolarized wavefront as the main characteristic of the electrical activity of the heart; the latter uses the time-varying potential of the cardiac surface. Compared to activation-based model, the potential-based model considers spatial distribution of cardiac potentials, offering insights into cardiac electric field and potential formation and propagation. Its comprehensive representation of cardiac electrical activity is advantageous for analyzing complex arrhythmias and cardiac disorders. In our experiment, we considered a simple two-variable potential-based model [17]; the process of cardiac excitation can be described as the follows:where u is the transmembrane potentials, v is the conduction current, D is the diffusion tensor dependent on 3-D myocardial structure, and tissue conductive anisotropy, ∇(D∇u)is the diffusion term. Functions f 1 and f 2 produce TMP shapes, which can be represented as: According to [17], parameter k, a, e is set as: a = 0.15, k = 8, e = 0.01. The longitudinal and transverse tensors of the diffusion tensor D are set to 4 and 1."
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,2.3,Network Structure,"Figure 2 shows the structure of the proposed network. It mainly consists of the feature extraction part and the feature fusion part. Briefly, the 12-lead signal data (512 sampling points) is represented as a 512 * 12 matrix. Each lead signal undergoes Fourier transform to obtain frequency domain information, resulting in matrices of signal amplitude and signal phase with the same dimensions as the time domain matrix. These three matrices are input into the local-global feature extraction module to obtain encoded time and frequency information. An early fusion method is selected to concatenate these multi-level features, which is transformed into 3-dimensional coordinates of the predicted ectopic pacing location P(x, y, z) by multilayer perceptron (MLP). The loss of the proposed network is defined as:where N is the number of samples and P (x, y, z) is the labeled coordinates. To minimize the loss function, we can finally get a well-trained model. The training uses the Adam optimizer with a learning rate of 1e-4 and a batch size of 32.Feature Fusion in Time-Frequency Domain. The electrocardiogram signal is a temporal signal, and therefore its analysis can be performed in both time and frequency domains.The algorithm proposed in reference [18] was the first to convert the ECG signal to a Fourier spectrum to achieve discrimination of ventricular tachyarrhythmia. Currently, most algorithms process ECG signals in the time domain but ignore information in the frequency domain. Therefore, we choose the feature fusion method to combine time and frequency information for better accuracy localization. The Fourier transform breaks down a function into its constituent frequencies [19]. Given an ECG signal vector {φ t } with t ∈ [0, T -1] where T is the total number of samples, its Discrete Fourier transform (DFT) can be expressed as: For each sample moment t, DFT algorithm generates a new representation k as the sum of all the original input φ t . { k } is a complex matrix that can be represented as:where r k and θ k , respectively, denote amplitude and phase. We then separate amplitude matrix {r 0 , r 1 , . . . , r T -1 } and phase matrix {θ 0 , θ 1 , . . . , θ T -1 } as new features that are sent to the network along with signal matrixLocal-Global Feature Extraction Module. Clinical diagnosis of heart disease based on 12-lead ECG mainly depends on its local features, such as P wave, T wave, or QRS complex. Convolutional neural network (CNN) has proven effective for extracting local information of ECG waveforms [20]. However, CNN's limited receptive field size makes it challenging to capture global feature representations, including dependence on long-distance signals and different leads. On the other hand, models like GRU [21] and attention mechanism [22], typical models in natural language processing, excel at capturing long-range dependencies but may compromise local feature details [23].Inspired by these, our feature extraction module is divided into local feature extraction module and global feature extraction module, ensuring the acquisition of detailed and comprehensive information and improving the network's ability to encode ECG signals.The local feature extraction block consists of three Conv1D blocks, each containing a 1DConv-BN-Relu layer and a max pooling layer. The Conv1D-BN-Relu layer comprises a one-dimensional convolutional layer, a batch normalization layer, and an activation layer with a Relu activation function. The kernel size of the convolutional layer is sequentially set to 7, 5, and 3; the stride is sequentially set to 2, 2, and 1.Figure 2 shows the global feature extraction block. Gate Recurrent Unit (GRU)is a kind of recurrent neural network that occupies less memory and is more suitable for small data training [21], which is in line with our needs. The GRU layer is defined as: where GRU (.) is the GRU network, H GRU is the hidden state, F local is the input and W GRU represents the corresponding weight and biases.In order to extract a broader dependence relationship, we introduced the additive self-attention mechanism after the GRU layer. Its network structure is shown in Fig. 3, which can be represented by the formula: (10) where, F GRU is the input of this part; more precisely, it denotes the output matrix from the previous GRU layer, with dimensions set to (14,40). W * represents the weights to be learned and set to be (40,32); in this case, they are configured as a matrix with dimensions (40, 32). MLP(.) means single layer MLP used for calculating the similarity of the Query matrix and Key matrix. This operation yields an attention map that captures the correlation among the elements of the matrix. Finally, by applying the softmax function, we obtain the output F Global , which encompasses the global feature.3 Experimental Result"
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,3.1,Overall Performance,"The experimental data of this part are generated by ECGSIM software based on the UDL source model [24]. The solution of the electrocardiography forward problem is calculated to simulate 3485 sets of 12-lead ECG with 697 nodes of ectopic pacing on a 3-D heart model, and we set the active and resting TMP to 15 and 20, respectively. The training, validation, and test sets were divided into an 8:1:1 ratio. Figure 4 shows the visual predictive performance of the test data. Figure 4(a) displays the coordinates of labeled data and the predicted location, showing that the predicted coordinate points nearly covered the actual value coordinate points. Figure 4(b) shows each test point's prediction error, indicating that the localization error was mainly below 2 mm. Figure 4(c)~(e) respectively displays the error in the x, y, and z-axis. The localization errors for each axis are mostly concentrated in the 1.5 mm range. Quantitative error data are presented in Table 1, which shows that the mean localization error for all test data is 1.76 mm, and the mean localization error in the x-axis, y-axis, and z-axis, respectively, is 0.84 mm, 0.95 mm, and 0.91 mm.   Noise Robustness Experiment. In practical clinical measurement processes, noise is inevitably introduced; therefore, the prediction model needs robustness to noise.In this section, we added Gaussian white noise with 5 dB, 15 dB, and 25 dB to the testing 12-lead data, respectively. From the results, we can see that network has certain robustness to different noise levels. At 25 dB noise, the prediction error is still only 2.97 mm. Though the error reached 10 mm at 5 dB, it is still within the clinically acceptable range considering the small signal-to-noise ratio (SNR).  To further demonstrate the capacity of the forward-solution aided method for ectopic pacing localization, we compared our method with the typical inverse solution methods shown in [25], which is also based on ECGSIM to simulate ectopic pacing data [26]. As is shown in Fig. 6, SSNet, Tikhonov, TV, and VAENet are inverse solution methods, and their localization errors are all >10 mm, while that of our method can reach within 2 mm. "
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,3.2,Clinical Data Experiment,"Previous experiments confirmed the efficacy of our prediction framework on simulated cardiac data and the network's superior ability to learn the relationship between 12-lead data and ectopic pacing location. We now transfer the method to clinical PVC patients' data using the double variation cardiac source for calculating the forward solution of the specific heart-torso model and use them as the training data.Table 2 shows the quantitative results of simulated patient pacing data. The location error ranges from 7.50 mm to 4.44 mm, with an average error of 5.28 mm. The network demonstrates precision and accuracy in localizing ectopic pacing in different cardiac models. Figure 7 shows the ectopic pacing location results for patient 1 and 2. The first column displays the activation of cardiomyocytes on the heart surface measured by the gold standard Ensite3000 system, with the red area indicating the earliest activation at the pacing location. The second column shows the activation mapping of the whole heart by Ensite3000. Our framework's localization results are consistent with the gold standard, with both pacing positions located in the posterior right ventricular outflow tract (RVOT), highlighting the practical significance of our study."
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,4,Discussion,"In this paper, we developed a forward solution-aided deep learning framework for analyzing ectopic pacing from 12-lead ECG data. Only CT data is needed to establish the specific heart-torso model for simulating ECG data as the training set for the designed network. Time-frequency fusion module and local-global feature extraction module are the core component of the network. Experiments have shown that the framework performs well on both simulated and clinical data. In the future, to enhance the robustness of our proposed method, additional datasets and comprehensive simulations involving a broader spectrum of cardiac conditions should be incorporated. This paper primarily emphasizes the clinical potential of the proposed approach rather than extensively comparing actual and simulated data. The discrepancies observed could be attributed to noise, slight variations in physiological parameters, and consistent electrode placement in clinical practice. Further investigation and discussion on these aspects will be addressed in future research endeavors."
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 1 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 2 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 3 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 4 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 5 .Fig. 6 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 7 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Table 1 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Table 2 .,
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,1,Introduction,"Intra-operative X-ray imaging supports the assessment of fracture reduction and implant positioning, which significantly increases surgical precision [7] and the overall outcome [1]. Therefore, standardized image rotation is essential to improve reading performance and interpretation. While in diagnostic imaging, this can be reached by careful alignment of the imaging system with the patient, in intra-operative setups, this alignment is hard to achieve due to mechanical constraints of the widely used C-arm devices and limited space in the operating room. The rotation of the images must then be corrected in a post-processing step [11]. Therefore, a common task in the radiography workflow is to manually rotate digital X-ray images to a preferred orientation suitable for diagnostic reading. However, during a surgery, user interaction of the surgeon with the system must be minimized due to sterility considerations. Thus, even improved user interaction with the system does not improve the situation for the surgeon much. Supporting staff is often not well trained in operating the vendorspecific systems and changes job positions frequently. Therefore, the speed and quality of the image alignment depends on the experience of the operator and leaves room for mistakes. An automatic X-ray alignment system can help ensure proper alignment resulting in improved reading performance and interpretation also [2]. Previous attempts [3,14,17,18] to automate this rotation typically only allow the identification of a limited set of orientations. In addition, they are often specially designed for certain examinations. More recent publications [4] work on the regression of the correct angular offset. Conventional vision algorithms based on feature extraction with subsequent feature registration to an atlas, typically fail to automate the image rotation of intra-operative images due to the enormous variety of images with different collimation settings, fracture types, and instruments/casts that obscure many image features. Additionally, they hardly generalize to a large number of body regions due to the nature of the hand crafted image features. In this paper, we extend the indirect regression approach of Kunze et al. [11]. It bases on the insight that for many anatomical structures a line can be defined that strongly corresponds with the upright position of the X-ray image. After this line has been determined as a heatmap by a Convolutional neural network (CNN) according to the idea of Kordon et al. [8] this heatmap can be post-processed to calculate the orientation of the line. Kunze et al. [11] have demonstrated that this indirect approach is more robust than the direct regression approaches for angle regression [2,6] based on ResNet or PoseNet. In applications with different crops of anatomic regions, Kordon et al. [10] have shown that the quality of the heatmap can be improved if a companion objective function is integrated into the training that optimizes the regression target directly. Therefore, the derivable calculation of the position and direction based on the Hu moments (HM) was added to the cost function of the segmentation task, enabling end-to-end gradient flow. This approach can be also adapted for the angle regression. However, the HM are an indirect measure leaving much room for the algorithm to shape the heatmap in an uncontrolled manner. Therefore, to limit the space of solutions, we propose to substitue the HM based angle regression with a Hough transform (HT) based one. By doing so, the HM based cost term is replaced by a term, that favors straight, narrow lines. Thereto, we present an algorithm to implement the computation of the line angle based on the HT differentiable to keep end-to-end training based on angular values.The main contribution of the paper can be summarized as follows:-proposition of a derivable calculation of the image rotation angle derived from the HT heatmap in the manner of a differentiable spatial to numerical transform (DSNT) [16] -ablation study which examines the properties of the proposed method and shows that the proposed method increases the robustness of the image rotation compared to state-of-the-art indicated by the 90 th percentile of the absolute angular error. -verification of the approach on a the body regions spine, wrist, and knee."
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,2,Materials and Methods,
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,2.1,Determination of Rotation Angle,"In Kunze et al. [11], two classes of algorithms are compared with which the rotation angle of images can be derived: The direct regression method, uses an encoder structure determining festures of the image followed by a regression network converting them into an angular valued, represented by its sine and cosine value for stability reasons. The indirect method computes a heatmap of a line which represents the upwards direction of the image. Analysing the direction of the line returns the rotation angle of the image. While the first approach consisting of a modified version of ResNet-34 with the fully connected (FC) layer being substituted by two FC layers serves as baseline algorithm [11], in the following the angle determination of the indirect approch is revised.Angle Regression Exploiting the Hough Transform. As pointed out in the introduction, the training with a cost function based on the HM does not enforce a narrow destinct line, but any structure with a defined main axis contributes to a low cost. A cost function which enforces this behaviour would potentially more distinctive heatmap.A common approach to detect lines in pattern recognition applications is the HT: Given a line L ρ,θ , with ρ the angle between line and the x-axis and θ its distance from the origin, its corresponding pixels {x ρ,θ,i , y ρ,θ,i } vote in the Hough space for the closest bin (ρ, θ). In contrast to the HM based direction calculation, the maximum of the HT is only obtained for a straight line of given extent. The extent can be controlled by the bin width of the HT. This feature of the HT enables the training of a narrow structure with a defined width punishing wide structures. However, the HT does not return an angle itself but returns a new 2-dimensional representation of the heatmap, with the line to be sought represented as its maximum. When this representation shall be integrated in an end-to-end trainable network, a differentiable calculation of the position of this maximum is needed. Thereto, we follow the idea of the DSNT layer introduced by Nibali et al. [16] that calculates the channel-wise centroid of the input. For the current scenario, merely the centroid θ of the θ coordinate indicating the direction of the line is of interest. The θ can be written asEquation ( 1) involves a summation over ρ coordinate. Since the zeroth-order Helgason-Ludwig condition of the Radon transform [13] states that the sum over each projection is constant, also the Hough histogram yields a constant first-order moment. This renders the direct implementation incapable for the angle computation.To overcome this shortcoming, A non-linear function needs to be applied to the Hough histogram to assign high pixel values a greater weight than small ones. Keeping pixel values between 0 and 1 while weighting maximum values and suppressing low values, a steep sigmoid function is employed, which maps input values to a range from 0 to 1. At best, only values from roughly 0.8 to 1 should be used. The steeper the sigmoid function is, the more it approximates the step function, which ideally assigns a weight of 1 to values of 0.8 and higher.For the end-to-end training, the same issue holds as for the direct regression: the angular value θ has a discontinuity for θ = -90 • and θ = 90 • . Both θ values represent the same line. To pass this boundy problem, the cosine-and sine-value are computed instead of the angular value itself. This can be incorporated in Eq. ( 1) by using a sine-and cosine-weighting instead of linear one. So the total angular value calculation given the HT can be written aswhere α = 20 and β = 17 were determined heuristically. θ can be calculated by the atan2 function from its sine and cosine value. We call the layer, that implements Eq. ( 2) in combination with the atan2 method differential spatial to angular transform (DSAT).Neither the HT layer parameters nor the DSAT layer have parameters, that need to be trained: The HT can be written as a multiplication of a vector representing the values of the image with a sparse matrix mapping the input image into a [N ρ × N θ ] Hough histogram Y , where N ρ and N θ are the numbers of discrete offsets and angles [12]. For the DSAT layer, Eq. 1 can be implemented. So both parts of the algorithms can be treated as known operators [15]."
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,2.2,Data,"For the experiments, three different datasets containing the spine, the knee and the wrist, were used. The spinal dataset consists of 958 images derived from 148 patients, where 289 images represent the cervical, 150 the lumbar, and 519 the thoracic region. The wrist dataset contains 257 and the knee dataset 113 images. The 16-bit, gray-scale X-ray images were selected randomly retrospectively from anonymized databases which were acquired using the Cios Spin mobile C-arm system of Siemens Healthcare GmbH during orthopedic surgeries. The images are depicted in the AP view and may contain screws, plates, and other surgical tools. In these images, the centerline was marked using the labelme tool [19] by a trained medical engineer. During annotation, special care has been taken to ensure that the rotation angle of the image can be calculated as the angle between the line and the y-axis by consistently drawing the orientation of the line."
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,2.3,"Training Protocol, Experiments, and Evaluation","To identify the effect of the post processing method on the angle regression error, an ablation study was set up. Following Kunze et al. [11], for the indirect (segmentation-based) method, a D-LinkNet [20] was chosen as underlying network architecture. The output of the D-LinkNet, which is the heatmap of the line, is fed into the HT layer and its result is processed by the DSAT layer. As loss function for the training, the weighted sum of the segmentation loss and the regression loss was chosen. The segmentation loss consists of the sum of Binary Cross Entropy and Mean Squared Error(MSE). For the regression loss, the MSE of the regressed rotation's sine-and cosine-values was selected. Thus, the total loss can be defined aswhere λ ∈ R is a multiplicative weighting term. Using this we examined the following 3 cases:1. For λ = 1, the training corresponds to one without the regression loss. In this case, the performance of the HM based and the HT based angle regression methods can be compared on the heatmap. 2. With a constant weighting term λ = 0.5, the regression loss is considered along with the segmentation loss during the training. 3. When decreasing lambda over the epochs from 1 to 0, we obtain a training which is kick-started using the segmentation loss only. But in the end, only the regression loss is used for the training.For the parameter selection of the HT layer, a grid search was performed on synthetic line heatmaps, resulting in discretization values dθ = 0.4 • and dρ = 1 8px.For comparison reasons, the direct regression method based on a ResNet-34 was trained using the MSE based on the sine-and cosine-vales as cost function.During all trainings, online augmentation for rotation (α ∈ [-45 • , 45 • ], p = 0.5), inverting (p = 0.5), shifting (s ∈ [-40px, 40px], p = 0.5) and contrast enhancement was applied. After the augmentation, the images were scaled to the dimension [H:256×W:256] px, using zero-padding the image to preserve the ratio. Thereafter, sample-wise data normalization using z-scoring was used and combined with batch normalization [9]. The heatmaps for segmentation training were created by placing a Gaussian function of width 7px in a neighborhood of 21px on the line connecting the augmented start and end point of the center line. The Gaussian was scaled such that the center point obtains a value of 1 [11].For the evaluation, different matrices based on the angular error were used. The mean error (ME) reveals a directional offset of the algorithm. Further on, the mean absolute error (MSE) and the standard deviation of the absolute error (Std.) are reported. Practically more relevant are the percentiles P 50 , P 90 and P 95 based on the absolute error as they reveal to what extent manual rotation corrections are required after the automatic correction was performed. Based on the MAE a Wilcoxon signed-rank test was performed to verify the significance of the observed differences.To evaluate the performance of the presented methods, a 5-fold crossvalidation scheme is employed. The mean performance score across all folds served as final result of the evaluated algorithm. Training was performed using the Adam optimizer for 500 epochs with a learning rate of l = 0.001 divided by two every 50 epochs. The batch size was selected to be 8. The weights of the segmentation network as well as of the direct regression model were initialized by He's method [5]. Implementation was done in PyTorch v1.11 (Python v3.9.12, CUDA v11.0), training was performed on a Windows 10 system with 64 GB RAM and 24 GB NVIDIA Titan RTX. Reproducibility was confirmed by repeated trainings."
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,3,Results,"The results of the experiments are shown in Table 1. A comparison of the segmentation results is made in Fig. 1.The results show for all three anatomies, that the HT based methods are superior to the HM based method. So already as post-processing method, it significantly outperforms the HM based method (spine: p = 1.0 × 10 -95 , wrist: p = 5.6 × 10 -21 , knee: p = 6.7 × 10 -8 ). Further improvements of the angle regression results can be realized by incorporating the regression loss in the training (spine: p = 9.5 × 10 -91 , wrist: p = 5.7 × 10 -21 , knee: p = 4.5 × 10 -9 ). Best results except for wrist are obtained when the final training is performed on the regression loss only (spine: p = 1.3 × 10 -2 , wrist: p = 7.7 × 10 -1 , knee: p = 2.1 × 10 -2 ). For the wrist adding the regression loss to the overall loss does not improve the training. The comparable P 50 value for most of the anatomies shows, that HT based and HM based angle regression both work well for standard images. For all anatomies, an improvement in the 90 th percentile of the absolute angular error can be observed. While for the spine the improvement is about 17%, for the wrist and knee, this value an reduction by a factor of 4.6 and 3.1, respectively, can be observed.A case-by-case analysis shows that on images for which the centerline is well determined, the HT analysis returns roughly the same result as the HM analysis. However, since the dataset also contains many challenging cases, e.g., images with cropped anatomical structures of interest or images containing metal implants, the centerline cannot always be well determined by the segmentation network. Then occasionally, the heatmap generates short and fragmented estimates of the centerline. For these images, the error of the regressed angle by the HT based method typically is thinner compared to that by the HM computed one (Fig. 2). "
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,4,Discussion,"We introduced a new HT based method to calculate the rotation of straight heatmaps. For the implementation, special care needs to be taken to cope with constant first-order moments of the HT and boundary problems at θ = ±90 • . Doing so, the proposed method has similar performance to HM for X-ray images of good quality. In more difficult cases, when the segmentation algorithm returns multiple and only short segments, like for the wrist or knee, the proposed algorithm is more robust than the HM based one.The largest benefit of the HT based image rotation calculation can be achieved with its embedding into the cost function of a segmentation network. Then it helps to improve the accuracy compared to a segmentation-only approach. Especially outliers can be reduced resulting in a lower MAE and standard deviation of the error. The presented results confirm the work of Kordon et al. [10] that direct optimization of the target measure in addition to a proxy objective is beneficial to the underlying heatmap-generating problem. That is especially true for body regions like the wrist or knee. For these regions, the lengths of the structures and the annotated lines defining the orientation of the image vary. Then, the algorithm can benefit from a more generic description. For the spine, a line from the top to the bottom of the image can be drawn. Therefore, the regression-related term provides less additional information during the training.The inspection of the generated heatmaps reveals that the HT based approach can enforce more pronounced, thinner heatmaps compared to the HM -as desired. This observation can be explained by the property of the HT based cost function: it penalizes structures that are not oriented along the desired direction. Only thin straight heatmaps along the direction have a positive effect on the loss term. Structures that are parallel, like the centerlines of the radius and ulna for the wrist images, are penalized. In contrast to that, the HM derived cost term does not punish broad heatmaps as long as the largest main axis points toward the correct direction. Thus, this regression term does not enforce one single, thin line. Also, clusters are tolerated as the results depict. That explains the reduced number of outliers for the HT bases method compared to the HM based, indicated by P 90 and P 95 values. The fact that the adaptation of the parameter λ during the training has only a small effect can be explained by the regression loss being larger compared to the segmentation loss by magnitudes as soon as a certain segmentation level is reached. So the segmentation loss only at the beginning of the training has a distinct influence on the training. Thus, the training process is adapting the influence of the segmentation loss by itself.Finally, the results confirm the finding of Kunze et al. [11] that the segmentation-based rotation regression is more robust compared to the direct regression by a ResNet-34, at the cost of the uncertainty of an up-down flip. For the wrist, slightly worse results are reported compared to [11]. That can be attributed to the dataset for this study containing only intra-operative X-ray images which are acquired not as standardized as the diagnostic images used in [11].Data Use Declaration: The data was obtained retrospectively from anonymized databases and not generated intentionally for the study. The acquisition of data from patients had a medical indication. Informed consent was not required."
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,,Fig. 1 .,
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,,Fig. 2 .,
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,,Table 1 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",2,Method,"As shown in Fig. 2, MM-RAF is a two-phase framework consisting of three modules, i.e., BCA, MILR, and HAF. Inspired by Vision Transformer [7] for modeling long-range dependency, we incorporate two classical transformer encoders throughout our framework to make comprehensive interaction between different modalities. In the contrastive phase, pretraining on unlabeled multi-modal data enables BCA to align the features from different modalities into the same semantic space, diminishing the semantic gap. In the following phase, MILR employs Multiple Instance Learning to refine the cumbersome OCT branch to a semantic structure. Then, with balanced streams from two modalities, HAF renders two cross-modality interaction strategies to make inter-and intra-modal diagnoses. In the following sections, we will clarify each module specifically.Vanilla Encoder and Co-attention Encoder. Two basic encoders enable effective intra-and inter-modal interaction. As shown in Eqs. 1-2, the Vanilla encoder duplicates the input stream into query, key and value (Q m , K m and V m ) components and concentrates on intra-modal interaction, while the Co-attention receives two input streams from different modalities and focuses on inter-modal interaction, with the primary stream acting as query, Q m , and the subordinate stream replicated as key and value (K m, V m). Due to the high computational cost of the self-attention mechanism in the OCT branch, we propose block embedding, partitioning the OCT volume into n OCT blocks and each block will be embedded aswhere m and m denote different modalities, CFP or OCT in this task. d k denotes dimension of self-attention."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",2.1,Bilateral Contrastive Alignment,"The BCA module aims to align the extracted features with the self-supervised strategy before interaction. The semantic gap between CFP and OCT is huge, and direct interaction between different modalities without alignment will lead to a mismatch. ALBEF [10] adopts a similar strategy with the momentum encoders and negative queues, but the weakly-correlated phenomenon and stronglycorrelated negative sample in the medical area are so common that ALBEF can hardly reach convergence. To simplify the proxy task, BCA employs the theory of MoCov3 [5] and redesigns the ""ctr loss"" to adapt to multi-modal tasks.Considering preserving the stereo information of OCT, each block-level token, T k O blk , is averaged in the token dimension before being projected. To equally align both branches, 4 projectors are followed symmetrically in both branches to map the tokens to contrastive space. Different augmentation will cause huge discrepancies, especially for OCT images. Therefore, as shown in Fig. 2 and Eq. 3 (Bilateral loss), to mitigate the alignment difficulty for multi-modal tasks, we align both modalities by concentrating on the same modality m with different augmentation ū and different modalities m with the same augmentation u.where q m,u ∈ R dim (dim is 256 by default) denotes modality m with augmentation u in contrastive space with the query encoder. k m,ū denote the vectors from the key encoder. preditor m,m,u denotes the predictor to map modality m to another modality m. ctr() denotes ""ctr loss"" [5]. Intuitively, each token behaves like a ""query"", and BCA aligns the corresponding ""values"" by projection."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",2.2,Multiple Instance Learning Representation,"Direct interaction between different modalities with unbalanced amounts is computational-consuming, and the cross-modality relationship is difficult to build. Therefore, we conjecture that the OCT block-level tokens(features) can be formulated as an embedding-level MIL problem in two aspects: (1) In an OCT volume, only certain salient slices are related to glaucoma. ( 2) High-level embedding features after the BCA module are more distinguishable. In MILR, by defining the i th OCT block, namely T i O,blk , as an embedding instance, the integral OCT volume is taken as the bag B = T i O,blk |1 ≤ i ≤ n . Then we concatenate the OCT bag, B, with an aggregated tokens T O,agg ∈ R N ×dim . As shown in Fig. 2, several Vanilla encoders render the interaction among B and T O,agg , and eventually, T i O,agg get semantic information from OCT block instances to form a bag prediction. To ensure that MILR aggregates glaucoma-related instances, a supervised signal is incorporated. For efficiency and effectiveness, we only pass semantic T depth O,agg to the subsequent fusion module. MILR can be formulated as:where depth is the depth of the Vanilla Encoder in MILR."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",2.3,Hierarchical Attention Fusion,"Before HAF, each modality interacts within its internal modality. To extract modal-specific features and modal-agnostic features respectively, HAF implements a mid-fusion strategy consisting of two fusion stages, Merged-attention and Cross-attention. As shown in Fig. 2, in the Merged-attention blocks, CFP tokens and T depth O,agg will be concatenated, T all ∈ R 2 * N ×dim , to pass through several Vanilla encoders. Except for interaction within their own modality, the salient area will also engage mildly with the other modality, e.g., CFP will leverage intra-modal (CFP) information and inter-modal (OCT) refined knowledge to reinforce the modal-specific features in CFP. Co-attention encoders in the Crossattention stage render the CFP tokens to interact solely with OCT and thus extract the modal-agnostic features related in both modalities. Eventually, the modal-specific and modal-agnostic features from CFP and OCT will be fed into a projector for joint diagnosis. We will mention the HAF module again in the interpretability experiments in Sect. 3.4."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3,Experiments,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3.1,Datasets,"For multi-modal glaucoma recognition, the existing public dataset, GAMMA [19] on glaucoma grading, includes macular OCT and CFP. GAMMA dataset consists of 100 accessible labeled cases and another 100 unlabeled cases as the benchmark. As the dataset is limited in size for Transformer-based models, we construct a new dataset. 872 multi-modal cases are collected using Topcon Maestro-1 at the outpatient clinic in the Department of Ophthalmology of a state hospital from July 2020 to January 2021. The scan mode is 3D Optic Disc with one CFP image and 128 horizontal OCT B-scan images obtained simultaneously. Due to the expensive human annotations, we acquire pseudo labels of CFP by our advanced ensemble model for training. To build a trust-worthy test set for evaluation, we first split the dataset in train/val/test in 6:2:2. A clinician relabels the test set (172 cases) as GON/normal by considering CFP and OCT thickness map. The performance is evaluated on both private test set and GAMMA."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3.2,Experiment Details,"Due to the high computational cost, the fixed sampling interval technique is employed to extract 32 OCT images. To avoid over-fitting, we reduce the depths of the encoders to 3 layers. For a fair comparison between all models in this study, we use the following standard setup: initializing with the pre-trained weight of ViT-Base-16 on ImageNet. In the first experiment, the existing multi-modal methods and classical baselines are compared with MM-RAF on the private test set. The baseline includes ResNet, ViT, DeiT [17], and Swin-Transformer [12] (pre-trained weight from timm [18]) with single-modal or early-fusion multimodal experiments. Multi-modal methods include COROLLA [3], MBT [14]and MM-MIL [11]. The robustness is evaluated on the GAMMA dataset by comparing it with CNNs. The metrics are averaged over three runs. All experiments are implemented in Python 3.7 and Pytorch 1.7 with four NVIDIA TITAN X GPUs and the training configuration is included in Supplementary Material. 1, ResNet50 for CFP attains the best AP score in single modality, proving that CFP is more sensitive than OCT for glaucoma diagnosis. Besides, the transformer-based method is inferior to ResNet in CFP but surpasses CNN in OCT modality, indicating that the transformer-based methods need sufficient data to learn inductive bias. MM-ViT outperforms CFP-ViT and OCT-ViT, exemplifying that Transformer can benefit from multi-modal learning. Our framework, MM-RAF, outperformed MM-ViT with a 6% improvement in AP score and achieved SOTA with F1, AP, and AUC metrics in this study. Robustness. Due to the limited size of GAMMA(100 cases), our transformerbased method is likely to get overfitting if training from scratch. To this end, pre-training on our private mid-scale dataset which captures optic disc OCT can gain enhancements even when transferring to the cross-domain dataset (GAMMA scans macular area). Cohen's kappa coefficient is implemented as metrics. As shown in Table 2, our method has better cross-domain generalization after applying BCA and transfer learning strategy. Furthermore, MM-RAF achieves results comparable to CNNs, highlighting our framework's robustness to learn inductive biases from images even with a limited dataset. When providing more domain-related data, our approach has the potential to perform marginally better than CNNs for compensating the lack of inductive bias."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3.3,Experimental Results,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Compared with Single-Modal and Multi-modal Solutions. As shown in Table,"Ablation Study. The ablation study on the private dataset examines the contribution of three modules, the order of HAF, and the depth of each module. From Table .3, MILR and HAF modules bring 0.03 and 0.02 AP increases, respec-  tively. Reversing the order of the HAF module brings a decrease, which indicates that the modal-agnostic features should be extracted after the Merged-attention."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3.4,Visualization,"For visualization, we employ a class-dependent relevance-based method [4] that captures inter-and intra-modal relevance. Since MILR has aggregated the OCT into high-level T depth O,agg features which are complex to visualize, we choose CFP images to interpret the mechanism of how different modalities interact. For each case presented in Fig. 3, intra-modal and inter-modal heatmaps are calculated by CFP tokens and OCT tokens, respectively. In intra-modal maps, the salient area is centralized on the optic disc, while the inter-modal maps are sensitive to the temporal region where OCT can provide fine-grain stereo information of the optic nerve, indicating that our framework incorporates multi-modal information. The sparsely distributed situation, e.g., case(a) GON's inter-modal view, may be attributed to the lack of significant lesions in the image, causing a random selection of tokens. Also, we visualize how the framework considers the correct prediction with the network going deeper. In case (b), deeper layers concentrate intra-and inter-modal features on lesion areas. The intra-modal Mergedattention focuses on the optic disc(modal-specific feature), and the inter-modal Cross-attention maps are more sensitive to the temporal region (modal-agnostic feature), demonstrating the effectiveness of HAF in extracting and combining modal-agnostic and modal-specific features to make the multi-modal decision."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",4,Conclusion,"The challenges in multi-modal glaucoma recognition include the huge discrepancies, the unbalanced amounts, and the lack of spatial information interaction between different modalities. To this end, we propose MM-RAF, a pure selfattention multi-modal framework consisting of three modules dedicated to the problems: BCA fills the semantic gap between CFP and OCT and promotes robustness. MILR and HAF complete semantic aggregation and comprehensive relationship probing with better performance. While MM-RAF outperforms other solutions in multi-modal glaucoma recognition, the performance can be further improved with sufficient data. Our next direction is to utilize a lightweight transformer to leverage more information from both modalities. Besides, addressing the issue of uncertainty measurement and preventing the bias of any specific modality from influencing the overall decision in the multi-modal recognition scenario is crucial, especially when diagnosing glaucoma using OCT for its limited specificity. Cross-modal uncertainty measurement is also our further research direction."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Fig. 1 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Fig. 2 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Fig. 3 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Table 1 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Table 2 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,"transfer learning, BCA 0.8467",
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Table 3 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 66.
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,1,Introduction,"Minimally Invasive Endovascular Procedures (MIEP) are becoming increasingly popular due to their non-invasive nature and quicker recovery time compared to traditional open surgeries. MIEP encompasses various applications, from peripheral artery disease treatments to complex procedures in kidneys, liver, brain, aorta, and heart. Catheter tracking is an essential component of these procedures, allowing for accurate guidance of the catheter through the vasculature [2].Electromagnetic (EM) tracking is a widely used technology for catheter tracking in MIEP. However, accurate registration between preoperative images and the EM tracking system remains a challenge [4,15]. Existing registration methods often require manual interaction, which can be time-consuming and may alter the procedural workflow. Achieving accurate and effective registration is essential for a seamless and fast integration of EM tracking, preoperative data, and the patient into the same intraoperative coordinate space."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,1.1,Related Work,"Electromagnetic Tracking is a popular tracking technology that uses integrated sensors in the catheter tip and a field generator to enable localization of the sensor's pose in 3D. EM tracking does not require line-of-sight, which makes it particularly advantageous for MIEP [20]. The sensors come in various shapes and sizes and can be tracked in translation and orientation, with respect to the generated EM field. Therefore, EM tracking is typically used in conjunction with intra-or preoperative images to guide the procedure [4,15].Registration is an essential step that enables intraoperative guidance of procedures through tracking technologies such as EM. Numerous methods have been developed and presented for EM tracking registration, particularly in vascular procedures [6,10]. One method commonly used for registering all involved components in the same intraoperative coordinate space involves the use of external markers or fiducials, which must remain affixed to the patient's body throughout the preoperative and intraoperative phases of the procedure. In order to achieve this registration, surgeons are required to match multiple physical marker points with their corresponding preoperative positions in the image, generating a set of point correspondences that can then be used to calculate the registration transformation. Some of the works using the marker-based registration method are presented in [7,9,11,19]. However, this method is subject to several disadvantages, including changes to the procedural workflow, the necessity of ensuring marker stability between the pre-and intraoperative phase, and sensitivity to natural changes in the patient's anatomy.Other registration methods include the use of the EM-tracked catheter paths and registration to the vessel's centerline, using Iterative Closest Point (ICP) algorithms, as described in [8,13], while others explored the registration of EM paths to points within the vasculature, as reported in [5]. Extensive research has been conducted on these methods, which have demonstrated accurate and reliable registration results. However, these methods are also subject to some drawbacks, such as limitations in accurately capturing the movement of the catheter, requiring a good initialization for accurate registration, and reduced accuracy when missing data segments or limited number of data points.Dynamic Time Warping is a widely-used technique in signal processing to compare and align two time-dependent sequences. Dynamic Time Warping (DTW) calculates the similarity between the sequences by optimally aligning them in a nonlinear fashion, taking into account time differences and sequence sampling rates [1,3]. The warping function enables the matching of corresponding features in the sequences, allowing for accurate alignment even when there are temporal differences or missing data points. DTW has many applications, including biomedical signal processing, speech, and gesture recognition [1,3].Despite the well-known advantages of DTW as a technique for signal alignment, its application as a registration method in EM-tracked MIEP is still unexplored. Previous works have reported using DTW and EM tracking; however, these works have primarily used DTW for data processing and evaluation due to its advantages in interpreting intravariability, rather than for registration purposes [17,21]. This paper introduces a novel approach to registering EM tracking systems and preoperative images using DTW. To the best of the authors' knowledge, this is also the first work that explores the temporal analysis of 3D EM catheter paths, which are subsequently used to register two systems within a single coordinate space. The introduced method utilizes DTW to warp the timedependent 3D EM-tracked catheter path to the vasculature's centerline, creating corresponding points between the two. These points are then filtered based on the minimal cost in the DTW algorithm to generate a set of correspondence points that are used for registration between the EM path and centerline."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,2,Method,"This paper introduces a novel approach for automatically registering EM tracking systems to preoperative images using DTW. The introduced method includes a preoperative phase, during which the targeted vasculature is segmented from preoperative images such as Magnetic Resonance Imaging (MRI), computed Tomography (CT), or CT Angiography (CTA), and the centerlines are extracted using the SlicerVMTK toolkit. In the intraoperative phase, the catheter with an integrated EM sensor in its tip is guided through the respective branch of the vascular tree and records a 3D EM path. In this particular implementation, a catheter-shaped EM sensor is used instead. The recorded EM path is then processed using DTW and warped to the centerline, providing point correspondences between the EM path and the centerline. These correspondences are used to perform a closed-form solution using Coherent Point Drift (CPD) algorithm, translating the EM path to the centerline, resulting in the registration of the two coordinate spaces. A detailed overview of the method is presented in Fig. 2."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,2.1,EM Tracking System,"The tracking technology used in this paper is the Aurora tracking system from Northern Digital Inc. -NDI (Waterloo, Ontario, Canada). It comprises of a system control unit, sensor interface unit, and a tabletop field generator, which allows for tracking of EM sensors in a 420 × 600 × 600 mm space. This tracking system is capable of recording data with a frequency of up to 40 Hz. The Aurora 5DOF FlexTube, which has a 1 mm diameter, was the catheter-shaped EM sensor used in this paper. It is highly versatile in applications since it can be navigated independently or integrated into catheters. The EM sensor was used without a catheter and was navigated in the phantom through its long cable. The tracking data is recorded using ImFusion Suite software (ImFusion GmbH, Munich, Germany), running on a laptop computer with the following specifications: Windows 11Pro, Intel Core i7-8565U CPU, 16 GB RAM, and Intel UHD 620 Graphics. Figure 1 provides a detailed overview of the experimental setup, including the EM tracking system and the phantom utilized in this paper. "
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,2.2,Phantom,"For EM tracking data acquisition purposes of this paper, an STL model obtained from [18] was utilized. The STL model was simplified while preserving six of the primary branches that represent natural vasculature features, including bifurcations, stenosis, and curvatures. Additionally, the model was resized to dimensions suitable for catheterization, with vessel diameters ranging from 1.5 cm to 5 mm and a length of 22 cm. In order to enable visibility of the catheter-shaped EM sensor from the outside, the phantom was 3D printed using rigid transparent Polylactic Acid (PLA) material. The phantom was then rigidly fixed in a box and positioned on top of the EM tracking field generator to conduct the experiments and record EM tracking data. Figure 1b provides a detailed illustration of the phantom and the catheter-shaped EM sensor utilized in this paper."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,2.3,Dynamic Time Warping Registration,"During the intraoperative phase, two preoperative components are used, namely the segmented vascular model and its corresponding branch centerline points.The centerline points are referred to as p n c ∈ P 3 preop , where n represents the number of points, and P 3 preop represents the 3D preoperative coordinate space. Firstly, the EM catheter-shaped sensor is guided through the vascular phantom, and the resulting EM path is recorded. The EM path points are referred to as p m em ∈ P 3 em , where m represents the number of points, and P 3 em represents the 3D coordinate space of the EM tracker. In order to use DTW with 3D signals, the number of points must be consistent across the signals; therefore, the signal with fewer points is linearly interpolated to match the number of points in the other signal. Here, the centerlines are interpolated to match the EM paths, p n c → p m c . In the next step of the introduced method, the centerline and the EM path signals are normalized between -1 and 1 to bring the signals into a temporary common coordinate space. The DTW algorithm registration process assumes that the orientation of the phantom (patient) and the preoperative model are similar. Here forwards, DTW decomposes both 3D signals into their respective axes, namely x, y, and z over time, and matches each point from the EM path to the centerline's counterpart. This iterative process stretches the two signals until the sum of the euclidean distances between corresponding points is minimized. The output warp paths represent the corresponding indices that have been warped from the DTW algorithm, creating a set of minimum cost correspondences between the EM path and centerline, c u i,j = (p i c , p j em ). The variable c in the equation refers to the set of corresponding points between the two signals, u represents the total number of correspondences, while i and j represent the indices of the corresponding matched points. For each point in the EM path, there exist one or more points in the centerline that have been warped together and vice versa. For further reading on the DTW algorithm used in this paper, please refer to the following works with more implementation details [14,16].In order to register the two signals, we leverage the point correspondences generated by the DTW algorithm to select three sets of equally distributed points from three equal segments of the signals, c 3 i,j = (p i c , p j em ). The correspondences in each signal segment are selected based on the minimum cost return function of the DTW algorithm, where the sum of the euclidean distances between corresponding points is minimal. Utilizing the three segments facilitates the equitable distribution of point matching across the entirety of the signal. This step is crucial to ensure high confidence matching and produce a reliable registration that does not rely entirely on one part of the vasculature. The selected correspondence points are then used to find the rigid transformation between them using the CPD algorithm, as described in [12]. The introduced solution is a closed-form algorithm that produces a transformation T = (R, t) between the set of correspondence points while minimizing the distance between them. In the transformation matrix T , R represents the 3×3 rotation matrix, and t represents the 3 × 1 translation vector. Finally, the registration transformation is applied to the recorded EM path, registering the two systems in the same coordinate space, p m c , p m em ∈ P 3 intraop , which represents the intraoperative 3D coordinate space. "
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,3,Evaluation,"In this study, the introduced DTW method for registration of EM-guided MIEP is evaluated using the mean registration error criterion. The method is compared to path-based ICP registration methods from state-of-the-art. To evaluate the DTW method for registration accuracy, we conducted experiments by recording EM-tracked paths while navigating through each of the six branches of the phantom five times. For each run, the EM catheter-shaped sensor was manually pulled from the main inlet of the phantom towards the outlets of each branch, with an average speed of 1-2 cm/s. Subsequently, the recorded EM paths were registered to the phantom's centerline using the introduced DTW method.The ground truth registration used in this study is a marker-based method, which is considered a benchmark in the literature. Ten unique easily-identifiable landmarks throughout the phantom are used to register the preoperative model to the EM tracking system for calculating the ground truth."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,3.1,Mean Registration Error,"This criterion is employed to assess the accuracy of the introduced DTW method compared to the ground truth registration. The mean registration error is computed by summing the euclidean distance of each DTW-registered EM path point to its closest point in the ground truth EM-registered path. Equation (1) represents the mathematical expression employed for this computation.The p i em represents an EM path point transformed by the introduced DTW registration method, p j gt represents the closest point from ground truth to p i em , m represents the total number of points in both signals, and || • || represents the Euclidean norm. The results of this evaluation criterion are presented in Fig. 3."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,4,Results and Discussion,"Based on the experimental setup and evaluation criterion mentioned above, the results of this proof-of-concept study are presented in detail in Fig. 3. The mean registration error over all branches and individual runs is 2.22 mm, which falls well within the clinically acceptable range of <5 mm, as reported in [13]. The proposed DTW registration method performed slightly better than the pathbased ICP registration method, with a mean registration error of 2.86 mm. The results presented in this paper demonstrate that the introduced DTW registration method achieves accurate and reliable registration. The method outperforms the established path-based registration methods that use ICP in all reported branch results. The variability of registration accuracy among different runs is higher in mostly straight vessels, when the automatic selection of registration feature points follows a straight line. This variability is mostly noticeable in results from Branch 4 and 5, where the translation is correctly matched, but the orientation of the signals is not perfectly aligned, and the standard deviation in the results exceed that of ICP.The introduced methods offers the advantage of automation in the registration process, with no changes in the intraoperative workflow. Unlike markerbased registration methods, which require manual interactions to match point correspondences, the DTW method automatically warps the signals, selects corresponding points, and performs registration. Additionally, the DTW method is not dependent on initialization compared to other path-based registration methods, which may fail to provide a transformation if not correctly initialized. In this paper, the ICP method was consistently initialized from the registered position of the DTW method. This step was necessary as the direct application of ICP registration would not converge to provide a transformation due to the significant distance between the signals.Furthermore, unlike other path-based registration methods, the DTW registration method does not rely on registering the entire EM path to the centerline. In ICP-based methods, the aim is to minimize the difference between all points to be registered, which means that any deformations in one part of the signal would affect the entire registration. In contrast, the DTW method matches points between signals beforehand and employs algorithms to check the confidence of the matched correspondences, ensuring that the set of points to be registered would result in a reliable registration.In comparison to prior studies, the proposed method aligns well with other state-of-the-art approaches in the research community. Marker-based techniques outlined in [9] report registration errors of 1.28 mm in phantom studies and 4.18 mm in in-vivo studies. Similarly, two additional studies employ path-based registration methods with ICP and report registration accuracies of 3.75 mm and 4.50 mm in [13] and [8] respectively. Another ICP registration approach proposed in [5] reports a mean registration accuracy of 1.30 mm. While these comparisons are relative due to differences in tracking data utilized, they demonstrate acceptable registration accuracy, which the introduced DTW method achieves.Future research directions for the introduced DTW registration include conducting additional evaluations with various phantoms and potentially in-vivo studies, which may provide more evidence of the method's effectiveness in clinical settings. In addition to exploring the impact of catheter movements on DTW matching, another research direction involves improving the approach to registering EM paths to centerlines. In time-dependent series, alternating forward or backward catheter movement can change the signal appearance and result in incorrect matching. To overcome this, one solution is to use the 3D localization and motion-capturing abilities of EM to detect forward and backward movements and then backward warp the signal when the catheter direction changes. Last, advanced algorithms that more accurately depict catheter motion dynamics could be implemented to improve registration accuracy. Current solutions still exhibit significant variability between ground truth catheter movement and the centerline."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,5,Conclusion,"Accurate catheter tracking is essential in MIEP, and this paper introduces a novel catheter registration method using DTW. As far as the authors know, this study represents the first attempt at using temporal analysis of 3D EM signals for catheter registration. The method is evaluated on a vascular phantom with marker-based registration as the ground truth. The results indicate that the DTW registration method achieves accurate and reliable registration, outperforming path-based ICP registration methods. Furthermore, it provides several advantages compared to existing solutions, including high registration accuracy, registration process automation, preservation of procedural workflow, and elimination of the need for initialization. The method introduced in this paper is a proof-of-concept study, and further experiments by the research community are necessary to establish its applicability and effectiveness in clinical settings. Overall, the introduced DTW registration method has the potential to enhance the accuracy and reliability of catheter tracking in MIEP."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,,Fig. 1 .,
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,,Fig. 2 .,
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,,Fig. 3 .,
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 75.
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,1,Introduction,"Over 430,000 new cases of renal cancer were reported in 2020 in the world [1] and this number is expected to rise [22]. When the tumor size is large (greater than 7 cm) often the whole kidney is removed, however, when the tumor size is small (less than 4 cm), partial nephrectomy is the preferred treatment [20] as it could preserve kidney's function. Thus, early detection of kidney tumors can help to improve patient's prognosis. However, early-stage renal cancers are usually asymptomatic, therefore they are often incidentally found during other examinations [19], which includes non-contrast CT (NCCT) scans.Segmentation of kidney tumors on NCCT images adds challenges compared to contrast-enhanced CT (CECT) images, due to low contrast and lack of multiphase images. On CECT images, the kidney tumors have different intensity values compared to the normal tissues. There are several works that demonstrated successful segmentation of kidney tumors with high precision [13,21]. However, on NCCT images, as shown in Fig. 1b, some tumors called isodensity tumors, have similar intensity values to the surrounding normal tissues. To detect such tumors, one must compare the kidney shape with tumors to the kidney shape without the tumors so that one can recognize regions with protuberance.3D U-Net [3] is the go-to network for segmenting kidney tumors on CECT images. However, convolutional neural networks (CNNs) are biased towards texture features [5]. Therefore, without any intervention, they may fail to capture the protuberance caused by isodensity tumors on NCCT images.In this work, we present a novel framework that is capable of capturing the protuberances in the kidneys. Our goal is to segment kidney tumors including isodensity types on NCCT images. To achieve this goal, we create a synthetic dataset, which has separate annotations for normal kidneys and protruded regions, and train a segmentation network to separate the protruded regions from the normal kidney regions. In order to segment whole tumors, our framework consists of three networks. The first is a base network, which extracts kidneys and an initial tumor region masks. The second protuberance detection network receives the kidney region mask as its input and predicts a protruded region mask. The last fusion network receives the initial tumor mask and the protruded region mask to predict a final tumor mask. This proposed framework enables a better segmentation of isodensity tumors and boosts the performance of segmentation of kidney tumors on NCCT images. The contribution of this work is summarized as follows:1. Present a pioneering work for segmentation of kidney tumors on NCCT images. 2. Propose a novel framework that explicitly captures protuberances in a kidney to enable a better segmentation of tumors including isodensity types on NCCT images. This framework can be extended to other organs (e.g. adrenal gland, liver, pancreas). 3. Verify that the proposed framework achieves a higher dice score compared to the standard 3D U-Net using a publicly available dataset. "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,2,Related Work,"The release of two public CT image datasets with kidney and tumor masks from the 2019/2021 Kidney and Kidney Tumor Segmentation challenge [8] (KiTS19, KiTS21) attracted researchers to develop various methods for segmentation.Looking at the top 3 teams from each challenge [6,11,13,17,21], all teams utilized 3D U-Net [3] or V-Net [16], which bears a similar architecture. The winner of KiTS19 [13] added residual blocks [7] to 3D U-Net and predicted kidney and tumor regions directly. However, the paper notes that modifying the architecture resulted in only slight improvement. The other 5 teams took a similar approach to nnU-Net's coarse-to-fine cascaded network [12], where it predicts from a low-resolution image in the first stage and then predicts kidneys and tumors from a high-resolution image in the second stage. Thus, although other attempts were made, using 3D U-Net is the go-to method for predicting kidneys and tumors. In our work, we also make use of 3D U-Net, but using this network alone fails to learn some isodensity tumors. To overcome this issue, we developed a framework that specifically incorporates protuberances in kidneys, allowing for an effective segmentation of tumors on NCCT images.In terms of focusing on protruded regions in kidneys, our work is close to [14,15]. [14] developed a computer-aided diagnosis system to detect exophytic kidney tumors on NCCT images using belief propagation and manifold diffusion to search for protuberances. An exophytic tumor is located on the outer surface of the kidney that creates a protrusion. While this method demonstrated high sensitivity (95%), its false positives per patient remained high (15 false positives per patient). In our work, we will not only segment protruded tumors but also other tumors as well. The first base network is responsible for predicting kidney and tumor region masks. Our architecture is based on 3D U-Net, which has an encoder-decoder style architecture, with few modifications. To reduce the required size of GPU memory, we only use the encoder that has only 16 channels at the first resolution, but instead we make the architecture deeper by having 1 strided convolution and 4 max-pooling layers. In the decoder, we replace the up-convolution layers with a bilinear up-sampling layer and a convolution layer. In addition, by only having a single convolution layer instead of two in the original architecture at each resolution, we keep the decoder relatively small. Throughout this paper, we refer this architecture as our 3D U-Net.The second protuberance detection network is the same as the base network except it starts from 8 channels instead of 16. We train this network using synthetic datasets. The details of the dataset and training procedures are described in Sect. 3.2.The last fusion network combines the outputs from the base network and the protuberance detection network and makes the final tumor prediction. In detail, we perform a summation of the initial tumor mask and the protruded region mask, and then concatenate the result with the input image. This is the input of the last fusion network, which also has the same architecture as the base network with an exception of having two input channels. This fusion network do not just combine the outputs but also is responsible for removing false positives from the base network and the protuberance detection network.Our combined three network is fully differentiable, however, to train efficiently, we train the model in 3 steps."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.1,Step1: Training Base Network,"In the first step, we train the base network, which is a standard segmentation network, to extract kidney and tumor masks from the images. We use a sigmoid function for the last layer. And as a loss function, we use the dice loss [16] and the cross-entropy loss equally."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.2,Step2: Training Protuberance Detection Network,"In the second step, we train the protuberance detection network alone to separate protruded regions from the normal kidney masks. Here, we only use the crossentropy loss and label smoothing with a smoothing factor of = 0.01. Synthetic Dataset. To enable a segmentation of protruded regions only, a separate annotation of each region is usually required. However, annotating such areas is time-consuming and preparing a large number of data is challenging. Alternatively, we create a synthetic dataset that mimics a kidney with protrusions. The synthetic dataset is created through the following steps:1. Randomly sample a kidney mask without protuberance and a tumor mask. 2. Apply random rotation and scaling to the tumor mask. 3. Randomly insert the tumor mask into the kidney mask. 4. If both of the following conditions are met, append to the dataset.where k i is a voxel value (0 or 1) in the kidney mask and t i is a voxel value in the tumor mask. Equation 1 ensures that only up to 30% of the kidney is covered with a tumor. Equation 2ensures that not all tumors are covered by the kidney (at least 5% of the tumor is protruded from the kidney)."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.3,Step3: End-to-End Training with Fusion Network,"In the final step, we train the complete network jointly. Although our network is fully differentiable, since there is no separate annotation for protruded regions other from the synthetic dataset, we freeze the parameters in protuberance detection network.The output of the protuberance detection network will likely have more false positives than the base network since it has no access to the input image. Thus, when the output of the protuberance detection network is concatenated with the output of the base network, the fusion network can easily reduce the loss by ignoring the protuberance detection network's output, which is suboptimal. To avoid this issue, we perform summation not concatenation to avoid the model from ignoring all output from the protuberance detection network. We then clip the value of the mask to the range of 0 and 1. As a result, the input to the fusion network has two channels. The first channel is the input image, and the second channel is the result of summation of the initial tumor mask and the protruded region mask. We concatenate the input image so that the last network can remove false positives from the predicted masks as well as predicting the missing tumor regions from the protuberance detection network.We use the dice loss and the cross-entropy loss as loss functions for the fusion network. We also keep the loss functions in the base network for predicting kidneys and tumors. The loss function for tumors in the base network acts like an intermediate supervision. Our network shares some similarities with the stacked hourglass network [18] where the network consists of multiple U-Net like hourglass modules and has intermediate supervision at the end of each hourglass module. By having multiple modules in this manner, the network can fix the initial mistakes in early modules and corrects in later modules."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,4,Experiments,"No prior work exists that uses NCCT images from KiTS19 [8,9]. Thus, we first created our baseline model and compared the performance with existing methods on CECT images. This allows us to ensure that our baseline model and training procedure is appropriate. We then trained the model using NCCT images and compared with our proposed method."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,4.1,Datasets and Preprocessing,"We used a dataset from KiTS19 [8] which contains both CECT and NCCT images. For CECT images, there are 210 images for training and validation and, 90 images for testing. For NCCT images, there are 108 images, which are different series of the 210 images. The ground truth masks are only available for the 210 CECT images. Thus, we transfer the masks to NCCT images. This is achieved by extracting kidney masks and adjusting the height of each kidney. The ground truth mask contains a kidney label and a kidney tumor label. Cysts are not annotated separately and included in the kidney label on this dataset. The data can be downloaded from The Cancer Imaging Archive (TCIA) [4,9].The images were first clipped to the intensity value range of [-90, 210] and normalized from -1 to 1. The voxel spacings were normalized to 1 mm. During the training, the images were randomly cropped to a patch size of 128×128×128 voxels. We applied random rotation, random scaling and random noise addition as data augmentation.During the Step2 phase of the training, where we used the synthetic dataset, we created 10,000 masks using the method from Sect. 3.2. We applied some augmentations during training to input masks to simulate the incoming inputs from the base network. The output of the base network is not binarized to keep gradient from flowing, so the values are in the range [0, 1] and the edge of kidneys are usually smooth. Therefore, we applied gaussian blurring, gaussian noise addition and intensity value shifting. "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,4.2,Training Details and Evaluation Metrics,"Our model was trained using SGD with a 0.9 momentum and a weight decay of 1e-7. We employed a learning rate scheduler, which we warm-up linearly from 0.0001 to 0.1 during the first 30% (for Step1 and Step3) or 10% (for Step2) of the total training steps and decreased following the cosine decay learning rate.A mini-batch size of 8, 16 and 4 were used, and trained for 250k, 100k and 100k steps during Step1 to 3 respectively. We conducted our experiments using JAX (v.0.4.1) [2] and Haiku (v.0.0.9) [10]. We trained the model using a single NVIDIA RTX A5000 GPU.For the experiment on CECT images, we used the dice score as our evaluation metrics following the same formula from KiTS19. For the experiment on NCCT images, we also evaluated the sensitivity and false positives per image (FPs/image). We calculated as true positive when the predicted mask has the dice score greater than 0.5, otherwise we calculated as false negative. On the other hand, false positives were counted when the predicted mask did not overlap with any ground truth masks."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,5,Results,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,5.1,Performance on CECT Images,"To show that our model is properly tuned, we compare our baseline model with an existing method using CECT images. As can be seen from Table 1, our model showed comparable scores to the winner of KiTS19 challenge. We used this baseline model as our base network for the experiments on NCCT images."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,5.2,Performance on NCCT Images,"Table 2 shows our experimental results and ablation studies on NCCT images. The proposed method (Table 2-bottom) outperformed the baseline model (Table 2-top). The ablation studies show that adding each component (CECT images and the protuberance detection network) resulted in an increase in the performance. While adding CECT images contributed the most for the increase in tumor dice and sensitivity, adding the protuberance detection network further pushed the performance. However, the false positives per image (FPs/image)  increased from 0.283 to 0.421. The protuberance detection network cannot distinguish the protrusions that were caused by tumors or cysts, so the output from this network has many FPs at this stage. Thus, the fusion network has to eliminate cysts by looking again the input image, however, it may have failed to eliminate some cysts (Fig. 3 second row)."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,6,Conclusion,"In this paper, we proposed a novel framework for kidney tumor segmentation on NCCT images. To cope with isodensity tumors, which have similar intensity values to their surrounding tissues, we created a synthetic dataset to train a network that extracts protuberance from the kidney masks. We combined this network with the base network and fusion network. We evaluated our method using the publicly available KiTS19 dataset, and showed that the proposed method can achieve a higher sensitivity than existing approach. Our framework is not limited to kidney tumors but can also be extended to other organs (e.g., adrenal gland, liver, pancreas)."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Fig. 1 .,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Fig. 2 .,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Fig. 3 .,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Table 1 .,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Table 2 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,1,Introduction,"The assessment of fractional flow reserve (FFR) is significant for diagnosing coronary artery disease (CAD) and determining the patients and lesions in need of revascularization [1,2]. Although CAD is widely diagnosed by angiography technique in routine, the anatomical markers often underestimate or overestimate a lesion's functional severity [3]. FFR is the gold standard for the functional diagnosis of CAD and guides the revascularization strategy, due to its ability to assess the ischemic potential of a stenosis [4]. According to a multicenter trial, the use of FFR recalls an additional 30% of severe stenosis required revascularization [5]. Additionally, another trial demonstrates a 5.1% reduction of the 1-year adverse event rate by using FFR [3]. Therefore, it advocates the routine use of FFR in the clinical practice guidelines of the European Society and American Heart Association [1,6]. However, the widespread adoption of FFR is restricted by the risk associated with maneuvering a pressure wire down a coronary artery and the added time to assess multiple vessels [7]. Therefore, it is an eager need for assessing FFR derived from angiography.Deep learning has become a promising approach for the assessment of FFR, due to its high computation efficiency in contrast to the computational fluid dynamics [8][9][10][11]. However, it suffers from the lack of appropriate priors. An appropriate prior is challenging to be found, due to FFR being ruled by the complex physical process (i.e. Navier-Stokes equations) [12].Physics-informed neural networks (PINNs) have the potential to address the aforementioned challenge of lacking appropriate priors. PINNs add priors to the loss function by penalizing the residual of physical equations and the boundary conditions [13]. This prior guides the learning to a direction in compliance with the physical principles and boundary conditions. For example, PINNs add the Navier-Stokes equations and the boundary conditions to the loss function to generate a physically consistent prediction of blood pressure and velocity on the arterial networks [14][15][16]. However, only adding the prior to the loss function is insufficient, because the learned features are weakly related to boundary conditions in the loss function.In this paper, we apply a prior to the inputs. Adding boundary conditions as prior to the inputs resulting in a strong relationship between learned features and boundary conditions. Therefore, the learned features contain more direct and powerful boundary condition information. Additionally, a graph network is introduced as a prior to enforce the coronary topology constraint, because the interaction between the nodes on the graph is similar to the interaction of FFR between the spatial points on the coronary. To this end, we propose a conditional physics-informed graph neural network (CPGNN) for FFR assessment under the constraint of the morphology and boundary conditions. CPGNN adds morphology and boundary conditions as priors to the inputs for learning the conditioned features, besides adding priors to the loss function by penalizing the residual of physical equations and the boundary conditions. Specially, CPGNN consists of a multi-scale graph fusion module (MSGF) and a physics-informed loss. The purpose of MSGF is to generate the features constrained by the coronary topology and better represents the different-range dependence. The physics-informed loss uses the finite difference method to calculate the residuals of physical equations.The main contributions in the paper are three-fold:(1) CPGNN provides FFR assessment under the condition of morphology and boundary.(2) CPGNN introduces a prior by adding the information of the morphology and boundary into inputs and a multi-scale graph fusion module is designed to capture the conditional features related to those information. (3) The extensive experiments on the 6600 synthetic coronary and 183 clinical angiography including 40 CT and 143 X-ray. The performance of our CPGNN demonstrates the advantages over six existing methods."
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,2,Method,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,2.1,Problem Statement,"The purpose of CPGNN is to add the appropriate prior, which makes sure that the prediction of blood pressure and flow meet boundary conditions and the conservation of physical principle. The idea is to find a loss term describing the rules of blood pressure and flow. There exists the hemodynamic theory [12] to solve the pressure Q and flow P on spatial coordinate z, which is defined as:where Ω is the domain of the coronary, ∂Ω is the boundary, θ is the morphology parameters, γ is boundary condition, F and B are operators to describe the control equation and boundary constraints respectively. A neural network with parameter ω is introduced to approximate the pressure and flow, defined as Q ω (z) and P ω (z). Then, the residuals of the Eq. ( 1) can be added to the loss function as prior. The prediction can be constrained by minimizing the residuals on coronary, namely arg minFurther, considering the feature learned by Eq. ( 2) have a weak relation with the γ and θ, both are added as prior conditional inputs to approximate the flow Q ω (z|γ, θ) and pressure P ω (z|γ, θ) by the process: "
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,2.2,Conditional Physics-Informed Graph Neural Network,"As shown in the Fig. 1, CPGNN contains an encoding-decoding architecture to predict the pressure and flow, by the produce:According to hemodynamic theory [12], the morphology θ is the cross-sectional area, and boundary condition γ consists of the outlet resistances and inlet pressure. By sampling uniformly on coronary, the inputs of CPGNN the morphology Multi-scale Graph Fusion. The graph convolution operator is introduced to enforce the coronary topology constraint. The morphology features M (z|θ) compose the graph on coronary. The graph adds B(γ) as new points at the corresponding boundary. Besides, a multi-scale mechanism is introduced to enhance the representation of the features, because it is beneficial to capture the differentrange dependency. According to the hemodynamic theory [12], the flow is constant on the branch due to mass conservation under steady-state one-dimensional modeling, and the pressure varies associated with the position. The featureis obtained by the element-wise fusion on up-sampling feature graphs while the feature H q (z|θ, γ) ∈ R Nv×Nq×C is obtained by concatenating feature graphs at the branch channel. The N q is the concatenated dimension according to the number of feature graphs used to fuse.Conditional Feature Decoding. A serial of 1D convolution and up-sampling is used to decode the feature H p (z|θ, γ) and generate the pressure prediction P (z|θ, γ) ∈ R Nv×N d ×1 . Considering the conversation of the mass, the points share the same prediction on branch. Thus, a full connection layer is used to decode the feature H q (z|θ, γ) to generate the flow prediction Q(z|θ, γ) ∈ R Nv×1Momentum Loss. The pointset D = (Z, S, Q, P ) contain the coordinate Z, cross-sectional area S, flow Q, and pressure P of the points at the coronary. The vessel wall is assumed to be rigid and blood is Newtonian fluid. The momentum loss term combines the hemodynamic equation and finite differences [12,17], defined aswhere (i, j) denote the j-th point at the i-th branch , N d is point number on branch, N v is branch number on coronary, h i is the point interval on i-th branch defined as, C is a coefficient describing the stenosis influence defined in [12]. Junction Loss. Given the pointset J = (Q, P ) containing flow Q and pressure P of the points at the branch junction, the conservation of momentum and mass is constrained bywhere N j is the number of junction on coronary tree, N i p is the number of points of the i-th junction, (i, j) denotes the j-th point at the i-th junction and the 1-st point is the nearest point to the coronary inlet. Boundary Loss. Given the pointset B = (Q, P, γ) containing flow Q, pressure P and condition γ of the points at the boundary, the boundary constraint is penalized bywhere N b is the number of boundary, the 1-st point is inlet, the rest points are outlet. Eventually, the objective of our CPGNN is to minimize the total loss L total :where λ 1 and λ 2 are the trade-off parameters."
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,3,Experiments and Results,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,3.1,Materials and Experiment Setup,"The experiment contains one synthetic dataset and two in-vivo datasets. (1) the synthetic data. We generate 6600 synthetic coronary trees for training. The parameters of geometry and boundary conditions are randomly set in the appropriate ranges, including vessel radius, the length and number of branches, inlet pressure, and outlet resistances [18]. The pressure ground truth (GT) is simulated by Simvascular [12]. (2) the in-vivo data. There are 143 X-ray and 40 CT angiography from 183 patients. The acquisition process obeys the standard clinical practice [19]. The setting of the boundary conditions is based on the TIMI count method [20] and PP-outlet strategy [21]. The FFR was performed using pressure guide-wire by the manufacturer Abbott with model HI-TORQUE for all patients in the clinic dataset.All experiments run on a platform with NVIDIA RTX A6000 48 GB GPU. The Adam optimizer is used with 16 batch size per step. Initial learning rate is 0.001 and the decay rate is 0.95. The ratio of the training and verification of the synthetic dataset is 8:2. CPGNN is trained on the synthetic dataset and tested on both synthetic and clinical datasets.CPGNN is compared with six state-of-the-art methods, GCN [22], GATv2 [23], GIN [24], SAGE [25], PAN [26] and UniMP [27]. The evaluation metrics are Root Mean Square Errors (RMSE), Mean Absolute Errors (MAE), Mean Absolute Percentage Errors (MAPE), the residual of momentum equation (L mom ) and the residual of junction equation (L junc ). The units of RMSE, MAE, L mom and L junc are mmHg, mmHg, e -6 and e -2 . "
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,3.2,Results on Synthetic Data,"Comparison of Pressure Prediction: The performance of CPGNN is closer to the GT with an overall MAE of 0.74, RMSE of 0.86, and MAPE of 0.79. Table 1 shows that CPGNN performs well than six state-of-the-art methods."
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Ablation of Conditional Inputs and Multi-scale Mechanism:,"As shown in Table 2, the prediction of PINNs has a poor performance on unseen coronary without the morphology and boundary condition inputs. Thus, it is necessary to add both as inputs. The results gradually improve MAE, RMSE, and MAPE from 1.15 to 0.74, 1.32 to 0.86, and 1.21 to 0.79 when adding the number of graph scale-level. Thus, the multi-scale mechanism plays a key role."
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,3.3,Results on In-Vivo Data,"Comparison of FFR Assessment: As shown in Fig. 4, Bland-Altman analysis and Pearson correlation are conducted to evaluate the differences between CPGNN and in-vivo FFR measurement based on X-ray and CT data. Compared to the computational fluid dynamics, CPGNN has a well consistency with in-vivo FFR in the Fig. 4. Figure 2 and Fig. 3 directly display FFR results of CPGNN and six state-of-the-art methods. Figure 4 presents the area under curve (AUC) of CPGNN is best for the stenosis diagnosis (FFR<0.8). Those results demonstrate that CPGNN outperforms the other six existing methods in the clinical FFR assessment."
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Ablation of CPGNN Components:,"As shown in Table 2, the AUC of different settings is shown. The results also demonstrate that the conditional inputs and the multi-scale mechanism are important for FFR Assessment in the clinic."
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,4,Conclusion,"In this paper, we propose a conditional physics-informed graph neural network (CPGNN) for FFR assessment under the condition of morphology and boundary. Compared to the current reduce-order computation method [12], CPGNN does not need to couple stenosis detection algorithm and enable automatic stenosis feature extraction, which avoids error accumulation and achieves better performance. CPGNN introduces a prior by adding the information of the morphology and boundary into inputs and a multi-scale graph fusion module is designed to capture the conditional features related to those information. The method is conducted on 143 X-ray and 40 CT subjects. The performance of CPGNN is higher than the six state-of-the-art methods. The FFR of CPGNN correlates well with FFR measurements (r = 0.89 in X-ray and r = 0.88 in CT). The computation speed of CPGNN is 0.03 s per case, which is 600× faster than the SimVascular computation method [12]. Those results demonstrate that our CPGNN can aid in the clinical FFR assessment."
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Fig. 1 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Fig. 2 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Fig. 3 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Fig. 4 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Table 1 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Table 2 .,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,1,Introduction,"Cerebral X-ray digital subtraction angiography (DSA) is a widely used imaging modality in interventional radiology for blood flow visualization and therapeutic guidance in endovascular treatments [25]. It is a 2D+T image series obtained by subtracting an initial pre-contrast image from subsequent post-contrast frames, leaving only the contrast-filled vessels visible. The injection of contrast medium and the subtraction process effectively eliminate soft tissue and bone, enabling high-resolution visualization of the vessels and the blood flow. However, this subtraction technique assumes the absence of motion between frames during exposure. In clinical practice, this premise is often violated. Involuntary motions, caused by swallowing, coughing, stroke, or endovascular procedures, are nearly inevitable. Body motion results in undesired artifacts in subtracted images, leading to decreased image quality and impaired interpretability of DSA (Fig. 1).Over the last three decades, various motion correction techniques have been proposed to mitigate the impact of body motion retrospectively [18]. Registration algorithms typically employ template matching with corresponding control points or landmarks to align images [3,4,[6][7][8][9][10]16,17,19,22,[26][27][28]. These algorithms rely on features based on vessels [8], edges [9,17,19,28], corners [30], textures [20], temporal correspondence [3], and non-uniform grids [27]. To capture both local and global transformations, multi-resolution search [21,31] matching [9], and iterative estimations [20,30] have been proposed. To limit undesirable vessel distortions, sparse key points [19] and non-rigidity penalties [26] have been used. Although these methods are effective in motion compensation, they require time-consuming iterative computation for each frame, limiting their clinical applicability.Recent generative models, such as pix2pix [13], have been adapted to address subtraction artifacts without registration [11,12,29]. These models leverage deep learning techniques to predict a subtraction image from an input post-contrast image by discerning foreground contrast from the body background, resulting in reduced artifacts. However, these models do not explicitly compensate for motion-induced misalignment between frames. More importantly, they may cause hallucinations or modification of contrast and vessels, and lack interpretability as there is no subtraction. Consequently, these shortcomings hinder the serial evaluation of blood flow and impede the diagnostic utility of DSA.To overcome these limitations, we introduce AngioMoCo, a fast learningbased motion correction method for DSA that avoids severe contrast distortion. We employ a supervised CNN module that distinguishes between motion displacement and contrast intensity change. The output contrast-removed image and the pre-contrast image are then input to a subsequent self-supervised learning-based registration model for deformable registration, where a deformation regularization loss limits the local irregularity. By excluding contrast enhancements from the deformation learning processing, AngioMoCo avoids undesired distortion of the vessels. This results in trustworthy visualization of continuous blood flow and promises to assist in automated analysis of flow-based biomarkers relevant to endovascular treatments.Overall, classical non-rigid registration methods use various regularization strategies to limit vessel distortion, but are prohibitively time-consuming. Recent learning-based methods are fast, but do not explicitly model the motion between frames, and as a result can negatively distort the very clinical information we aim to highlight. We build on the strengths of both directions while avoiding their limitations. Specifically, we propose a novel learning-based strategy that is significantly faster than traditional non-rigid registration methods. AngioMoCo not only removes subtraction artifacts on each frame but does so by explicitly compensating for motion between frames, which is not available in existing image-to-image models. We demonstrate that AngioMoCo achieves high-quality registration while avoiding undesirable contrast reduction or vessel erasure. We define a contrast extraction module f θ f (x t ) = c t with parameters θ f that takes as input a post-contrast frame x t . This function separates x t into a contrast image c t and a contrast-removed image m t where m t = x t -c t . The values in c t are within [-1, 0] as the injected contrast medium can only lead to a decrease in pixel intensity relative to the input image with an intensity range of [0, 1]. The contrast extraction module aims to reduce contrast discrepancies between the pre-and post-contrast frames. Such image-to-image modules can lead to hallucination and may not fully capture distal vessels, relatively less contrasted vessels, and vessels behind bone structures. Therefore, in AngioMoCo, we only employ this module to enable easier registration of the frame x t to the precontrast x 0 using the intermediate contrast-extracted m t image."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,2,Method,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,2.1,Model,"We define a registration function r θr (x 0 , m t ) = φ t with parameters θ r to estimate the deformation φ t . We obtain the motionless subtraction angiography y t by subtracting the pre-contrast frame x 0 from the warped post-contrast frame w t : y t = w t -x 0 = x t • φ t -x 0 , where • defines a spatial warp."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,2.2,Training,"We train the contrast extraction f θ f (•) and deformable registration r θr (•, •) modules separately. The contrast extraction module is trained on a motionless subset of data with an MSE loss between the ground truth contrast, estimated via subtraction between post-and pre-contrast frames (x t -x 0 ), and the predicted c t :We train the deformable registration module on a motion subset, with the pretrained contrast extraction module frozen, using a loss function that combines an MSE loss between m t and x 0 and a smoothness loss L smooth , weighted by λ:where L smooth is the mean squared horizontal and vertical gradients of displacement u t in deformation field φ t , that enforces spatial smoothness of deformation:(3)"
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,2.3,Architecture,"We design the contrast extraction module f θ f (•, •) using a U-Net architecture, which includes a contracting path (encoder) and an expanding path (decoder) connected by skip connections. The encoder stage comprises eight convolutional and max-pooling layers with the number of channels being 8, 16, 32, 64, 128, 256, 512, and 512 respectively. The convolutions operate with a 3 × 3 kernel size and a stride of 2. Similarly, the decoding path employs eight upsampling, 3 × 3 convolution, and concatenation operations with 32 feature maps per layer to restore the spatial dimension up to the input size. Each convolution is accompanied by an instance normalization and a LeakyReLU activation layer. We also use three additional 3 × 3 convolutions. The final convolution employs a negative sigmoid activation, confining the output pixel intensity to [-1, 0]. We employ a deformable registration module r θr (•, •) based on VoxelMorph to learn motion correction in DSA [2]. We add instance normalization between the convolution layers of the encoder and decoder. We utilize this deformable registration module to predict bi-directional dense deformation fields using diffeomorphism that allows to spatially transform either pre-or post-contrast frames."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,3,Experiments,"We assess AngioMoCo in terms of vessel contrast preservation, artifact removal, and computation efficiency compared to existing approaches."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,3.1,Experimental Setup,"Data. We identified 272 patients with unsubtracted cerebral angiographic images available from MR CLEAN registry [14], an ongoing prospective observational multi-center registry of patients with acute ischemic stroke who underwent endovascular thrombectomy (EVT). This comprised 788 angiographic series, consisting of 16,641 frames in total, acquired between attempts of thrombus retrieval. The DSA series were acquired using various imaging systems, including Philips, GE, and Siemens, and had a size of 1024×1024 pixels. The series had varying lengths, ranging from 10 to 50 frames, and temporal resolutions between 0.5 and 4 frames per second (fps). We performed image resizing to 512 × 512 pixels and min-max intensity normalization to obtain intensity values within the range of [0, 1]. To ensure the coherency of the intensity along the series, the maximum intensity is calculated on the series level based on the stored bits in the DICOM header.Based on visual assessment, we categorized the dataset into two subsets: motionless and motion. We use the motionless subset, consisting of 107 series (1933 frames) from 21 patients, for pre-training the contrast extraction module. The motion subset, which contains 681 series (14708 frames) from 251 patients, is used for overall training and evaluation. We split data on the patient level independently on the motionless and motion subsets, with a ratio of 50%, 20%, and 30% for training, validation, and testing, respectively.Baselines. We compare AngioMoCo with two widely used image registration approaches, elastix-based affine registration and VoxelMorph [1,2], and an imageto-image approach employing a U-Net [24] architecture. We followed the implementation of [2] for VoxelMorph with deformation regularization λ = 0.01. For the U-Net, we employed the same architecture as the contrast extraction module f θ f (•, •) with the same preprocessing and augmentations. We trained the U-Net using the motionless subset and used mean squared error (MSE) as the optimizing objective. We implemented the methods using Python 3.10.6 and PyTorch [23].Training Details. We use an NVIDIA 2080 Ti GPU (11 GB), the Adam optimizer [15] and the ReduceLROnPlateau scheduler with an initial learning rate of 0.001, a patience of 300 epochs, and a decay of 0.1. We set the batch size to 8 and applied early stopping with a patience of 500 epochs. We selected these optimization parameters based on validation performance using a grid search. We applied data augmentations using Albumentations [5], including HorizontalFlip, ShiftScaleRotate, and RandomSizedCrop, each with a probability of 0.5.Evaluation. We carry out both qualitative and quantitative analyses on the hold-out test set of the motion subset. A key challenge is to minimize motion and subtraction artifacts while retaining clinically important features. We use mean squared intensity (MSI) as a proxy to quantify the preservation of contrast intensity within vessels and the ability of motion correction outside vessels. As ground truth deformations are not available for image sequences with motion, we manually segment the blood vessels in post-contrast frames (Supplemental Fig. 6), and use the resulting masks to quantify MSI inside and outside blood vessels. We used paired t-tests for statistical significance."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,3.2,Results,"Quantitative Analysis. The optimal outcome is represented by the top left corner of Fig. 3, indicating high vessel contrast preservation and complete artifact removal (Supplemental Table 1). Compared to elastix affine registration, Fig. 3. Mean squared intensity (MSI) on the test set. Better methods will preserve the MSI (i.e., vessel contrast) inside vessels (↑, y-axis) while minimizing the MSI (i.e., artifacts) outside vessels (←, x-axis), moving towards the top left of the graph.AngioMoCo(λ = 0.001) achieves similar vessel preservation (P = 0.2), while substantially decreasing the MSI outside vessels (by about half). Compared to Vox-elMorph, AngioMoCo demonstrates substantial improvement, with higher vessel preservation and better (more to the left) artifact removal. While the imageto-image U-Net yields the lowest MSI outside vessels, it sacrifices a substantial amount (30%) of contrast inside vessels, harming the precise clinical signal we are interested in.Qualitative Analysis. Figure 4 presents visual comparisons of the methods through three representative examples. The image-to-image U-Net generates images with fewer motion artifacts than other methods, but it often fails to capture vessel contrast behind bone structures (Row 1), distal vessels (Row 1), and loses high-frequency spatial features, leading to blurry images (Row 2). These errors can have substantial negative effects on downstream clinical applications. VoxelMorph operates on pre-and post-contrast images, which can cause considerable modifications in the vessel contrast flow. For example, the motioncorrected image of VoxelMorph in Row 3 has lighter vessel contrast than its counterparts. In contrast, AngioMoCo overcomes these limitations of U-Net and VoxelMorph by learning to disentangle contrast flow from motion.Runtime. Compared to iterative registration methods, deep-learning-based registration methods, including AngioMoCo, require orders of magnitude less time. For example, AngioMoCo takes less than a second to process a series on GPU, while iterative registration methods are mostly implemented on CPU where they require minutes. "
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,4,Discussion,"We find that AngioMoCo achieves high-quality motion correction in DSA, while preserving vessel details, which is of critical clinical importance. While the imageto-image U-Net resulted in fewer artifacts, it substantially degrades the vessel contrast, harming its usability in clinical usefulness.These results suggest that AngioMoCo is clinically relevant for endovascular applications, enhancing the utility of DSA in diagnosis and treatment planning. The tool can extract contrast flow while outputting smooth bi-directional deformation fields that provide interpretability. Unlike image-to-image models, the contrast flow visualization is driven by motion-compensation of the post-contrast frames to the pre-contrast image, and hence avoids undesirable hallucinations and modifications of vessel contrast.We also examined the end-to-end training strategy of AngioMoCo, which did not yield superior results to VoxelMorph or the modularly trained AngioMoCo (Supplemental Fig. 5). To further enhance registration accuracy, future research may explore the integration of 3D spatio-temporal CNN and the utilization of vessel masks as auxiliary supervision."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,5,Conclusion,"We have presented AngioMoCo, a deep learning-based strategy towards motionfree digital subtraction angiography. The approach leverages a contrast extraction module to disentangle contrast flow from body motion and a deformable registration module to concentrate on motion-induced deformations. The experimental results on a large clinical dataset demonstrate that AngioMoCo outperforms iterative affine registration, learning-based VoxelMorph, and imageto-image U-Net. Overall, AngioMoCo achieves high registration accuracy while preserving vascular features, improving the quality and clinical utility of DSA for diagnosis and treatment planning in endovascular procedures."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,Fig. 1 .,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,Figure 2 Fig. 2 .,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,Fig. 4 .,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_72.
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,1,Introduction,"Optical coherence tomography angiography (OCTA) is a noninvasive ophthalmic imaging modality that captures the thin vessels and capillaries, named as microvasculature, present around the fovea and parafovea regions at various retinal depths. The 2D en face OCTA images have been increasingly used in clinical investigations for inspecting retinal eye diseases and systemic conditions at the capillary level resolution. More specifically, the morphological changes of retinal vasculature distributed within parafovea regions are associated with diseases such as diabetic retinopathy, early-stage glaucomatous optic neuropathy, macular telangiectasia type 2, uveitis, and age-related macular degeneration [1][2][3]. In addition, analyzing retinal microvasculature at different depth layers can offer new pathological features that have not been reported before for clinically related findings. For instance, recent studies [4,5] have manifested that the variations in the vascular morphology exhibited in OCTA images are associated with Alzheimer's disease, mild cognitive impairment, and chronic kidney disease. Therefore, the extraction of microvasculature from OCTA image is of a great interest. The reliability of phenotypes calculated for diagnosing retinal vascular-related diseases depend on the quality of segmented vascular trees. Since retinal vascular structures appear as a wire meshlike network that is associated with numerous branching and fusing, and considerable variations in contrast, the manual annotation of vasculature network is a labor-intensive, time-consuming, and error-prone procedure.The automated delineation of retinal vasculature from OCTA images encounters several problems such as low signal-to-noise ratio (SNR), projection and motion artifacts, and inhomogeneous image background. A few methodologies have been used in the literature for detecting vessel trees in OCTA images. The majority of the vessel delineation approaches have been presented for color fundus images. Generally, the existing methods for vessel extraction from OCTA images can be categorized into supervised [2,6,7] and unsupervised approaches [3,8,9]. Li et al. [6] proposed an image projection network that takes 3D OCTA data as input and produces 2D vessel segmentation results. A channel and spatial attention network was introduced by Mou et al. [7] for extracting fine vessels from OCTA images. Recently, an OCTA-Net model by Ma et al. [2] was presented to segment fine and coarse vessels separately. In spite of the popularity of deep learning models in the vessel detection task, these algorithms typically need a large amount of annotated samples used in intensive training processes to achieve vessel extraction task. Apart from supervised learning, Yousefi et al. [8] developed a filtering approach based on multi-scale Hessian filter and morphological operations for the detection of microvascular structures in OCTA images. Zhang et al. [3] combined curvelet denoising and optimally oriented flux algorithms to effectively enhance OCTA microvasculature. Gao et al. [9] introduced a reflectance-adjusted thresholding method for binarizing superficial vascular complexes of en face retinal OCTA images. Further, the delineation methods [10][11][12][13][14][15] have attained encouraging results for vessel delineation in fundus color images. An operator inspired by the push-pull inhibition in the visual cortex was introduced by Strisciuglio et al. [10] to increase the robustness of vessel detection against noise. Regularized volumed ratio [13] considered vessels as rounded structures to produce strong responses for vessels with low contrast and preserve various vessel features. A bowler-hat transform [15] was proposed to detect the inherent features of vessel-like structures. The existing algorithms have achieved a great progress for vessel enhancement and extraction. However, the following problems remain unsolved and need to be overcome for a reliable vasculature delineation in OCTA images. (i) Some vessels and capillaries suffer from the problem of weak continuity, which leads to a disjoint detection in segmented vascular trees. (ii) Due to the poor SNR of OCTA images, some capillaries are presented with an inadequate contrast, causing difficulty in differentiating them from inhomogeneous background. (iii) OCTA images are associated with high noise level, which significantly interferes with vessel structures and makes their boundary irregular.This paper introduces a new modulatory elongated model to advance the delineation methodology of retinal microvasculature in OCTA images via addressing the above problems. The contributions of this work are summarized as follows. (1) A modulatory function is proposed to include two simultaneous facilitatory and inhibitory delineation processes that distinguish vascular trees more conspicuously from background. (2) The responses of the elongated representation encode the intrinsic profile of the vessels and capillaries, elongated-like shape, which retains the subtle intensity changes of vessels and capillaries for solving the continuity issue. (3) The proposed method disambiguates the region surrounding vessel structures for addressing the disturbance of noise at vascular regions. (4) Our method achieves the best quantitative and qualitative results over the state-of-the-art vessel delineation benchmarks."
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,2,Methodology,"The primary visual cortex (V1) has an essential role in the perception of objects in the visual system. The physiological studies revealed that the response of a V1 neuron to a stimulus in its receptive field can be either increased or decreased when more stimuli are added in the region surrounding the receptive field. The stimuli falling outside the classical receptive field (CRF) can exert modulatory effects on the activities of neurons in V1. The surrounding area beyond the CRF is named as a non-classical receptive filed (nCRF) modulation region, which captures long-range contextual information for modulating the neuron responses [16][17][18]. The modulatory effects resulted from the interaction between the CRF and nCRF extend the cortical area and allow visual cortex neurons to incorporate a broader range of visual field information to identify complex scenes. Based on the above neurophysiological evidences, we propose our modulatory elongated model as follows:Responses of V1 Neurons. We propose to describe the responses of orientationselectivity V1 neurons to the stimuli placed within the classical receptive field (CRF) by an elongated representation. The elongated kernels [19] can effectively simulate neuron responses at various characteristics such as preferred orientation, spatial scale and profile shape. The elongated responses are defined as:where σ > 0 is the scaling factor that determines the width of the kernel and ρ > 1 represents the anisotropic index that controls the elongation of kernel profiles. T is the matrix transpose and R θ denotes the rotation matrix with angle θ while (x, y) is a point location. In our implementation, we set σ ∈ [1 : 0.5 : 2.5], ρ ∈ (1 : 0.1 : 1.5] and θ ∈ π 16 , 2π 16 , 3π 16 , . . . , 15π 16 , π . θ σ,ρ (x, y) forms a pool of neuron responses for processing the local stimuli within the CRF. For an input OCTA image I(x, y), the final CRF response of a V1 neuron is obtained as:where the notation represents the convolution operation in the spatial domain.Contextual Influences for V1 Neurons. Physiological findings [20][21][22] illustrated that the region outside the CRF, which is termed as nCRF, of V1 neurons is alone unresponsive to visual stimuli, but it can manifest contextual influences on the neural responses to stimuli within the CRF. The contextual influences perform as a modulatory process that allows neurons in V1 to accumulate information from relatively large parts of visual space for participating in complex perceptual tasks. Also, the majority of modulatory influences are inhibitory while the reminders are facilitatory. Statistical data [20] exhibited that around 80% of the orientation-selectivity neurons in V1 manifest the inhibitory effect. About 40% of these neurons show the inhibition regardless of the relative orientation between the surrounding stimuli and the optimal stimulus. Further, it has been indicated that the modulatory strength from the nCRF decays exponentially with increasing the distance from the center of the CRF [20][21][22]. Therefore, the intrinsic connections between the neurons and the region around them are distance related influences. Accordingly, we consider an isotropic modulatory behavior, in which contextual influences are independent of the orientation of surrounding patterns, but they take into account the distance to the surroundings. Then, an annular function (H i ) that captures contextual influences (H i ) for modulating V1 neurons is expressed as:where F and F -1 represent forward and inverse discrete Fourier transforms, respectively while . 1 and . 2 indicate L 1 norm and L 2 norm, respectively. . denotes a truncated function that replaces negative values with zero. W(x, y) is a distance weighting function that is calculated by a 2D non-negative difference of Gaussian function DoG(x, y) [20] with a standard deviation σ w = 7, which is utilized to simulate the strengths of neuron connection in distance. The factor k calculates the size of the nCRF region, and we set k = 4 to be consistent with the neurophysiological evidence [21] that the extent of the nCRF is 2 to 5 times larger than that of CRF."
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Contrast Influence on Contextual Information.,"Physiological experiments [22] have shown that contextual influences can vary with contrast attributes. For example, the inhibitory strength is decreased by lowering the contrast of stimulus within CRF, but when increasing the contrast, the inhibition grows relatively stronger. Thus, we introduce a contrastive operator (S i ), as defined below, to control the modulatory strength of the nCRF so that the contextual influences are adaptively varied with contrast at each location.where C l is a local range measure that returns the range value (maximum value-minimum value) using a patch of N × N neighborhood around the pixel (x, y) in the response of ψ.S d computes the local standard deviation of the N × N patch around the corresponding input pixel of ψ response. We use N = 5 in our implementation.Modulatory Function. The essence of the proposed modulatory model is that the response of the elongated representation at a specific point is modulated by the response of the representation presented in the area outside the region of the representation interest. We integrate the elongated responses with the contextual influences at each pixel to produce spatial coherent responses that enhance vessel structures more conspicuously from their background while reducing noise disturbances with vascular regions. The proposed modulatory model (M) is defined as:where is a small value to avoid division by zero. When there are no spurious signals in the region surrounding the elongated responses, the modulatory influence from (H i and S i ) produces a weak response and the numerator (ψ -S i .H i ) of M becomes almost equal to the ψ. As a result, the term (H i + S i ) in the denominator together performs a faciliatory process for improving the responses of ψ. However, the influence of (H i and S i ) has a strong response when containing spurious signals in the surroundings. Then, the (H i and S i ) behave as an inhibitory process in both the numerator and denominator, which drops off the contribution of ψ to almost zero response."
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,3,Experimental Results,"Datasets and Metrics. The proposed method is assessed on the ROSE-1 database [2], which is captured from 39 participants that consist of 26 subjects with Alzheimer disease and 13 healthy controls. We perform the experiments on the superficial vascular complexes (SVC) dataset and the inner retinal vascular plexus that includes both SVC and deep vascular complexes (DVC)-termed as (SVC + DVC) dataset. Vessel delineation results are only obtained on test set images of the ROSE-1 datasets. Also, we examine our method on OCTA500_6mm dataset (300 images) [6] using the maximum projection between the internal limiting membrane (ILM) layer and the outer plexiform layer (OPL), named as an ILM_ OPL projection map. Most of OCTA500_6mm subjects (69.7%) were taken from a population with various retinal diseases such as age-related macular degeneration, choroidal neovascularization, central serous chorioretinopathy, diabetic retinopathy and retinal vein occlusion. For the used datasets, the pixel-level annotations of human experts are employed as the ground truths in our calculations. The following metrics are used to measure the quality of the delineation results in comparison with human annotations: precision-recall curve, accuracy (ACC), false discovery rate (FDR), geometric mean value (P VR ) of the positive predictive rate and the negative predictive rate."
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Results and Comparisons,". Figure 1 illustrates some examples of vascular delineation results produced by the proposed method and their corresponding manual annotations on OCTA images from the ROSE-1 (SVC and SVC + DVC) and OCTA500_6mm datasets. The performance of the proposed method on the used datasets is quantified using the precision-recall curve and compared with seven state-of-the-art vessel delineation benchmarks, as exhibited in Fig. 2. The benchmark approaches are robust inhibitionaugmented curvilinear operator (RUSTICO) [10], morphological bowler-hat transform (MBT) [15], regularized volume ratio (RVR) [13], probabilistic fractional tensor (PFT) [14], scale and curvature invariant ridge detector (SCIRD) [12], phase congruency tensor (PCT) [11], and the second order generalized Gaussian directional derivative (SOGGDD) filter [19]. Table 1 reports the delineation results of all methods from the precisionrecall curve by selecting the best threshold that returns the highest average ACC on each dataset. As shown in Fig. 2, the proposed method yields encouraging delineation results on the ROSE-1 and OCTA500_6mm images, validating its ability in extracting vascular structures from background interferences more effectively than the benchmark   1, by achieving the best results of ACC, FDR, and P VR on the used datasets. Our delineation scores show large margins of improvement compared with benchmarks. For example, the proposed method attains the best scores of ACC = 0.910, P VR = 0.883, and FDR = 0.149 on ROSE-1 (SVC) dataset whilst the second-best scores obtained by benchmarks are ACC = 0.890 [12], P VR = 0.851 [15], and FDR = 0.188 [15]. However, the proposed method consumes in average a computational time T s of 2.80 s and 3.86 s for processing an image of ROSE-1 and OCTA500_6mm datasets, respectively, which is longer than the benchmarks.Performance on Challenging Cases. In Fig. 3, we investigate the delineation performance of the proposed method and comparative benchmarks on some OCTA patch images that have the challenging cases of vessels and capillaries with weak continuity (first row), noise interferences with vessel structures (second row), and inadequate contrast of capillaries in a foveal avascular zone (third row). As illustrated in Fig. 3-first row, the proposed method attains a superior delineation performance over the benchmarks for sustaining the spatial continuity of capillaries and vessels (red arrows) in resolving the disconnections in extracted vessel trees. Also, our method produces better results over the benchmarks in overcoming the interferences of noise that damage the spatial intensity of vessel structures, as shown in Fig. 3-second row (yellow arrows). Figure 3-third row shows the encouraging ability of the proposed method in addressing the challenge of the capillaries with low visibility, which are much better extracted than the benchmark approaches."
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,4,Conclusion,"This paper presents a modulatory elongated model that exploits contextual modulatory effects presented for tuning the responses of neurons in V1. The proposed method incorporates the elongated responses, neuron responses, at a certain location with either a facilitatory or inhibitory process, contextual information, to reliably enable the delineation of vasculatures in OCTA images and the suppression of background inhomogeneities simultaneously. The validation phase on clinically relevant OCTA images illustrates the effectiveness of our method in producing promising vessel delineation results. The proposed method not only obtains the better quantitative results over the state-of-the-art benchmarks, but also produces an encouraging performance for retaining the continuity of vessels and capillaries, detecting the low-visibility capillaries, and handling spurious signals of noise and artifacts that interfere with vessel structures. Encouraging experimental results demonstrate the effectiveness of the proposed modulatory elongated model in improving the vessel delineation performance. Further, the proposed model is a non-learning approach that does not require any annotated samples or training procedures for performing vasculature detection. As the proposed method is not only responsive to the intrinsic profile of vessels, but also is sensitive to the region surrounding vessel structures, it can be a better alternative to the existing vessel delineation approaches that depend on surroundings-unaware operators."
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Fig. 1 .,
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Fig. 2 .,
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Fig. 3 .,
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Table 1 .,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,1,Introduction,"Segmenting the left atrium (LA) from magnetic resonance images (MRI) is critical in treating atrial fibrillation. Recent success of deep learning (DL)-based methods usually requires a large amount of high-quality (HQ) labeled data (termed as Set-HQ). However, since labeling medical images is expertisedemanding and laborious, acquiring massive HQ labeled data from experts is expensive and not always feasible. Without sufficient HQ labels, the DL approaches often struggle with inferior performance. Despite the recent success of semi-supervised learning (SSL) that leverages abundant unlabeled data [3,11,20,21], it is still difficult for SSL to accurately propagate label information at the voxel level especially when the HQ labeled data is extremely scarce. Thus, an intuitive cost-efficient alternative is to collect additional labels via cheaper ways, e.g., crowdsourcing from non-experts, as depicted in Fig. 1. Unfortunately, the quality of cheap labels is always unsatisfactory. Directly introducing additional data with low-quality (LQ) noisy labels (termed as Set-LQ) may mislead the model training, easily causing performance degradation [10,18]. Such a pervasive dilemma poses a challenging yet practical scenario: how to robustly learn segmentation from scarce HQ labeled data and abundant LQ noisy labeled data?The existing works on mining LQ labeled data for medical image segmentation can be categorized by two distinct application scenarios: (i) HQ-agnostic, e.g., Set-HQ and Set-LQ are mixed as one dataset [6,9,24,[26][27][28]. TriNet [24] uses a tri-network that integrates predictions from two peer networks to supervise the third network; PNL [28] introduces an image-level label quality evaluation module to identify clean labels to tune the network. (ii) HQ-aware, e.g., recruiting experts to obtain a reasonable amount of HQ labeled data and thus Set-HQ and Set-LQ are separate. Such scenario extends SSL [3,11,20,21] to further exploit the potentially useful information of LQ labels, which will be more beneficial when the HQ labeled data is extremely scarce (as detailed in Sect. 3). Luo et al. [10] proposed to implicitly decouple the learning processes for Set-HQ and Set-LQ using two separate decoders; KDEM [5] extends [10] with knowledge distillation and entropy minimization regularization. However, this implicit decoupling strategy is experimentally hard-to-control. Thus, MTCL [18] estimates the joint distribution matrix between observed and latent true labels to explicitly characterize mislabeled locations for smooth label refurbishment. However, MTCL is based on the class-conditional noise (CCN) assumption that the noise is independent of input features given the true label, which may be impractical [2]. Considering the clinical practice, we advocate the HQ-aware scenario because: (a) HQ/LQ labeled data can be separated since the sources of medical annotation are usually recorded and acquiring a reasonable amount of HQ labels from radiologists is feasible; (b) the separation may implicitly embed rewarding prior knowledge on discriminating HQ/LQ labeled data into training.Tailoring for the HQ-aware scenario, in this work, we propose the Prototypical Label Isolation Learning (PLIL) framework for left atrium segmentation, enabling effective expert-amateur collaboration. Specifically, PLIL is built upon the popular teacher-student framework. Besides the prime supervised signals from HQ labeled data, PLIL robustly exploits the additional LQ labeled data via two steps: (i) Considering the structural characteristics that semantic regions of the same class are often highly correlated and the higher noise tolerance in the high-level feature space [13,23], the self-ensembling teacher model isolates clean and noisy labeled voxels by exploiting their relative feature distances to the class prototypes via multi-scale voting. Besides the advantage of explicit spatial isolation, this strategy takes the input features into account, which is more realistic compared to [18] as the mislabeled voxels often present difficult and ambiguous regions in the image. (ii) Synergistically, the student follows the teacher's instruction for adaptive learning, wherein the clean voxels are further introduced as supervised signals and the noisy ones are especially regularized via perturbed stability learning, considering their vulnerable large intra-class variation in general. Comprehensive experiments on left atrium segmentation under extreme budget settings demonstrate the superior performance of our approach. The ablation study further verifies the effectiveness of each component."
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,2,Methods,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,2.1,Problem Formulation,"Our PLIL framework is depicted in Fig. 1. Following the HQ-aware scenario, we have access to scarce expert-examined HQ labeled datathat only contains M samples, and abundant non-expert LQ noisy labeled data S l = x l(i) , y l(i) N i=M +1 that consists of N -M (usually M ) samples, where x h(i) , x l(i) ∈ R Ωi denote the images and y h(i) , y l(i) ∈ {0, 1} Ωi×C are the given HQ or LQ label (C denotes the class number). Our goal is to learn segmentation with scarce Set-HQ and abundant Set-LQ by optimizing the following loss function:where L HQ and L LQ denote the guidance from HQ and LQ labeled data, respectively. λ is a trade-off weight for L LQ , scheduled by the time-dependent ramp-upGaussian function [4] , where t is the current iteration and t max is the maximal iteration. Since our method heavily relies on the manipulation in the feature space, such weighting schedule can reduce the interference of LQ labeled data to the feature space learning at the early training stage. The HQ labeled data provides prime HQ supervised guidance L hs , i.e., L HQ = L hs . Following [21], we adopt the cross-entropy loss L ce and Dice loss L dice with equal weights for L hs . To further exploit Set-LQ while alleviating confirmation bias [10], we aim to spatially isolate the clean and noisy labeled voxels and make better use of the suspected noisy labeled voxels rather than discarding them."
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,2.2,Prototypical Label Isolation for Adaptive Learning,"Teacher-Student Architecture. Our framework is built upon the popular teacher-student architecture [15], where the student model F s is updated by back-propagation and the teacher F t is updated by the exponential moving average (EMA) weights of the student θ across training steps. Denoting the weights of the teacher model at step t as θt , θt is updated by: θt = α θt-1 + (1α)θ t , where α is the EMA decay rate and empirically set to 0.99 [15]. As such, the teacher model owns the self-ensembling property [4], which can avoid sharp deterioration of the feature quality and thus suits our following prototypical label isolation strategy that appreciates high-quality and smooth embedding space.Multi-scale Voting-Based Prototypical Label Isolation. Considering the structural characteristics that the targeted segmentation regions of the same class are often highly correlated and the higher noise tolerance in the high-level feature space [1,13,14,23], our label isolation strategy is inherently motivated by the assumption that for a clean labeled voxel, its features should lie closer to its corresponding class prototype (class-wise feature centroid); otherwise, a potential noisy labeled voxel is suspected. Specifically, we determine whether a voxel-wise label is a clean one by exploiting the relative feature distances to the class prototypes. Considering that different layers perceive the entire image with different perspectives, a multi-scale voting mechanism is introduced. Technically, given a medical scan x l of Set-LQ and its noisy label y l , we denote the last i-th feature map from the teacher model F t as e temp i , which is then upsampled to e i ∈ R H×W ×Z×Li (H, W and D denote height, width and depth of x l , respectively, and L i is the channel number) to be consistent with the size of segmentation mask via trilinear interpolation. Then, we resort to the pseudo label from the teacher model as the ""mask"" for the target class, which will be utilized to extract the class features. Denoting the teacher's prediction of x l as F t (x l ), the pseudo label corresponds to the class with the maximal posterior probability. Since the HQ labeled data is scarce which makes it hard to obtain confident prediction, Monte Carlo dropout [7] based model uncertainty is leveraged to calibrate the pseudo label. For the LQ labeled image x l , K stochastic forward inferences through F t are performed with random dropout. Then, the normalized predictive entropy of the mean of the K softmax predictions is regarded as the uncertainty map u [21]. When the uncertainty u v at voxel v is smaller than a threshold η, i.e., u v < η, this voxel will be used as the final pseudo mask Ft (x l ). As such, at the i-th scale, the object prototype q obj i can be obtained via the masked average pooling [20,25] as:, where the predicted probabilities of object p obj t(v) from the teacher model weight the contribution of voxel v to prototype generation. Similarly, the background prototype q bg i can be also obtained. Then, the relative feature distances d obj i(v) and d bg i(v) between the feature vector of voxel v and the prototypes are defined as:andIntuitively, if the given label y l(v) at voxel v is object (background) yet its feature vector e v lies closer to the background (object) prototype than the object (background) prototype, this voxel will be isolated to the noisy group. Otherwise, it will be selected as the clean labeled one. Formally, the i-th scale determines the clean-label selection mask m i for image x l as:We select the last three scales of features from the teacher model to perform multi-scale voting. Thus, for the final clean-label selection mask,Adaptive Learning Scheme for Isolated Voxels. As shown in Fig. 1, the additional supervised loss for Set-LQ (L ls ) is applied to the isolated clean labeled voxels, which takes the form of m-masked cross-entropy loss and Dice loss as:For the noisy group, since it is extremely difficult to perfectly find out the noisy labels, we do not advocate label refinement as in [18] to avoid additional error propagation. Instead, we regularize the model behavior on these ambiguous noisy voxels via perturbed stability learning [15], i.e., encouraging consistent presoftmax predictions between the student and teacher model for the same input with different perturbations ξ and ξ , formulated as:The design of m-masked stability loss is motivated by the fact that the estimated noisy group correlates with the voxels with large intra-class variation, wherein these voxels often exhibit difficult and ambiguous nature, which potentially have serious instability problem. Besides, compared to [15], such a noise-selective stability learning avoids the distraction by the redundant easy regions, considering this loss takes the form of mean squared error (MSE) with the average nature. As such, the LQ loss L LQ in Eq. 1 can be formulated as L LQ = L ls + βL nsl , where β is a tradeoff weight for the two learning manners. By combining L HQ and L LQ , the model can not only receive HQ supervision from the scarce Set-HQ but also adaptively exploit different kinds of productive information in Set-LQ towards effective expert-amateur collaboration."
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,3,Experiments and Results,"Materials. The left atrium (LA) segmentation dataset [17] provides 100 3D gadolinium-enhanced magnetic resonance images (GE-MRIs) with expert labels. The images have the isotropic resolution of 0.625 × 0.625 × 0.625 mm 3 . Following the same data preprocessing and split in [21], 80 samples are selected for training and the remaining 20 samples for testing. All the images are cropped to the center of the heart region and the intensities are normalized to zero mean and unit variance. We investigate the scenarios of scarce HQ labeled data, where only 4 (5%) or 6 (7.5%) samples are used as Set-HQ and the rest is utilized as non-expert Set-LQ, simulated by the commonly used label corruption scheme [22,28] including random erosion and dilation with 3-15 voxels."
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Implementation and Evaluation Metrics. The framework is based on,"PyTorch using an NVIDIA GeForce RTX 3090 GPU. 3D V-Net [12] is adopted as the backbone, referring to [21]. We randomly crop patches of 112 × 112 × 80 voxels as the input and use sliding window strategy with stride of 18 × 18 × 4 voxels for inference. The batch size is set to 4 including 2 labeled samples and 2 unlabeled samples. t max is set to 8,000. K, η and β are empirically set to 8, 0.1 and 0.1. The learning rate is initialized as 0.01 and decayed by multiplication with (1.0t/t max ) 0.9 . Data augmentation, including random flip and rotation, is applied. Four metrics, including Dice, Jaccard, average surface distance (ASD) and 95% Hausdorff distance (95HD), are adopted for comprehensive evaluation. The code will be available at https://github.com/lemoshu/PLIL.Comparison Study. The quantitative results are presented in Table 1. H-Sup denotes the supervised baseline that only Set-HQ is utilized, while HL-Sup denotes that Set-HQ and Set-LQ are mixed for supervised learning. We also include recent SSL methods (UAMT [21], CPS [3], CPCL [20] and URPC [11]), HQ-agnostic noisy label learning (NLL) methods (TriNet [24] and PNL [28]) and HQ-aware NLL methods (Decoupled [10] and MTCL [18]). All the methods are implemented with the same backbone and training protocols to ensure fairness. As observed, H-Sup performs poorly with scarce Set-HQ, yet, HL-Sup even further degrades, implying that our simulated LQ labels have led to serious confirmation bias. Relying on some model assumptions [16], SSL methods ignore the LQ labels and exploit the image information only from Set-LQ. Despite effectiveness, it is still difficult for SSL to accurately propagate voxel-level label information when the HQ labeled data is scarce. For the HQ-agnostic methods, TriNet and PNL show effectiveness in alleviating the negative effects brought by the agnostic LQ labels, yet, even fall behind some SSL methods given the violent simulated label noises, revealing that the HQ-agnostic setting may be sub-optimal. For the HQ-aware scenario, Decoupled [10] and MTCL [18] perform well under both labeling settings, demonstrating the benefits of HQ-aware strategy. Our PLIL relies on the manipulation in the feature space, more HQ labeled data will help the network learn more discriminative representations towards accurate isolation. As observed in the 6-HQ-sample setting, PLIL achieves the Dice of 87.66%, only 3.59% away from the upper bound trained with all 80 HQ labeled data. Despite less-discriminative features learned under the 4-HQ-sample setting, PLIL can still achieve respectable results, demonstrating its robustness. The impact of varying expert labeling budgets is further illustrated in Fig. 2(c) while Fig. 2(a) presents exemplar results of our PLIL and other approaches under the 6-HQ-sample setting. Consistently, the predicted mask of our PLIL fits more accurately with the ground truth. To better understand our method, we visualize the estimated noisy-label selection mask m for a dilated LQ label y l of LA in Fig. 2(b), where it can be observed that most dilated regions are wellcharacterized, further demonstrating the efficacy of our label selection strategy.Ablation Study and Discussions. To further investigate how our method works, we perform an ablation study under the 4-HQ-sample setting (as presented in Table 2) with the following variants:  2, the HQ-agnostic input has interfered with the network training and led to obvious performance degradation, showing the efficacy of our separate strategy. We also observe that the arbitration-based multi-scale voting mechanism enables more reliable isolation due to the consideration of different perspectives of the images. When removing L ls , considerable performance degradation can be observed, revealing that our strategy effectively finds out the clean labeled voxels. Besides the productive guidance provided by the isolated clean labeled voxels, the noisy group exploited by the stability learning can further provide informative clues to boost the performance. Empirically, the noisy labeled regions often appear in the challenging areas, which are more sensitive to the perturbations and therefore exploring their perturbed stability during training is rewarding and can enhance the generalizability of the model [15,19]. However, as a methodological study, we only evaluated the methods with the commonly used simulated LQ noisy labels. As observed, the violent simulated noises lead to serious confirmation bias. Some existing NLL methods cannot handle such violent noises well, but some SSL methods, which discard LQ labels, achieve appealing performance. Thus, further clinical validation with real-world amateur noises is an important future work. Besides, to facilitate practical expert-amateur collaboration, we should further consider two intertwined problems in the future: (i) how to cost-efficiently edu-  "
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,4,Conclusion,"In this work, we proposed a novel Prototypical Label Isolation Learning (PLIL) framework to robustly learn left atrium segmentation from scarce highquality labeled data and massive low-quality labeled data. Taking advantage of our multi-scale voting-based prototypical label isolation and adaptive learning scheme for clean and suspected noisy labeled voxels, our approach can robustly exploit the additional low-quality labeled data (e.g., via cheap crowdsourcing), which enables effective expert-amateur collaboration. Comprehensive experiments on the left atrium segmentation benchmark demonstrated the superior performance of our method as well as the effectiveness of each proposed component."
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Fig. 1 .,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Fig. 2 .,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Table 1 .,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Table 2 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,1,Introduction,"Heart failure is usually characterized by the inability of the heart to supply enough oxygen and blood to other organs of the body [4]. It is a major cause of mortality and hospitalization [14]. Elevated Pulmonary Arterial Wedge Pressure (PAWP) is indicative of raised left ventricular filling pressure and reduced contractility of the heart. In the absence of mitral valve or pulmonary vasculature disease, PAWP correlates with the severity of heart failure and risk of hospitalization [1]. While PAWP can be measured by invasive and expensive Right Heart Catheterization (RHC), simpler and non-invasive techniques could aid in better monitoring of heart failure patients. Cardiac Magnetic Resonance Imaging (MRI) is an effective tool for identifying various heart conditions and its ability to detect disease and predict outcome has been further improved by machine learning techniques [3]. For instance, Swift et al. [17] introduced a machine-learning pipeline for identifying Pulmonary Arterial Hypertension (PAH). Recently, Uthoff et al. [18] developed geodesically smoothed tensor features for predicting mortality in PAH.Cardiac MRI scans contain high-dimensional spatial and temporal features generated throughout the cardiac cycle. The small number of samples compared to the high-dimensional features poses a challenge for machine learning classifiers. To address this issue, Multilinear Principal Component Analysis (MPCA) [11] utilizes a tensor-based approach to reduce feature dimensions while preserving the information for each mode, i.e. spatial and temporal information in cardiac MRI. Hence, the MPCA method is well-suited for analyzing cardiac MRI scans. The application of the MPCA method to predict PAWP might further increase the diagnostic yield of cardiac MRI in heart failure patients and help to establish cardiac MRI as a non-invasive alternative to RHC. Existing MPCA-based pipelines for cardiac MRI [2,17,18] rely on manually labeled landmarks that are used for aligning heart regions in cardiac MRI. The manual labeling of landmarks is a cumbersome task for physicians and impractical for analyzing large cohorts. Moreover, even small deviations in the landmark placement may significantly impact the classification performance of automatic pipelines [16]. To tackle this challenge, we leverage automated landmarks with uncertainty quantification [15] in our pipeline. We also extract complementary information from multimodal data from short-axis, four-chamber, and Cardiac Measurements (CM). We use CM features (i.e., left atrial volume and left ventricular mass) identified in the baseline work by Garg et al. [5] for PAWP prediction.Our main contributions are summarized as follows: 1) Methodology: We developed a fully automatic pipeline for PAWP prediction using cardiac MRI data, which includes automatic landmark detection with uncertainty quantification, an uncertainty-based binning strategy for training sample selection, tensor feature learning, and multimodal feature integration. 2) Effectiveness: Extensive experiments on the cardiac MRI scans of 1346 patients with various heart diseases validated our pipeline with a significant improvement (ΔAUC = 0.1027, ΔAccuracy = 0.0628, and ΔMCC = 0.3917) over the current clinical baseline. 3) Clinical utility: Decision curve analysis indicates the diagnostic value of our pipeline, which can be used in screening high-risk patients from a large population. "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,2,Methods,"As shown in Fig. 1, the proposed pipeline for PAWP prediction comprises three components: preprocessing, tensor feature learning, and performance analysis."
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Cardiac MRI Preprocessing:,"The preprocessing of cardiac MRI contains (1) normalization of scans, (2) automatic landmark detection, (3) inter-subject registration, and (4) in-plane downsampling. We standardize cardiac MRI intensity levels using Z-score normalization [7] to eliminate inter-subject variations. Furthermore, we detect automatic landmarks which is explained in the next paragraph. We perform affine registration to align the heart regions of different subjects to a target image space. We then carry out in-plane scaling of scans by max-pooling at 2, 4, 8, and 16 times and obtain down-sampled resolutions of 128 × 128, 64 × 64, 32 × 32, and 16 × 16, respectively."
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Landmark Detection and Uncertainty-based Sample Binning:,"We utilize supervised learning to automate landmark detection using an ensemble of Convolutional Neural Networks (CNNs) for each modality (short-axis and fourchamber). We use the U-Net-like architecture and utilize the same training regime implemented in [15]. We employ Ensemble Maximum Heatmap Activation (E-MHA) strategy [15] which incorporates an ensemble of five models for each modality. We utilize three landmarks for each modality, with the short-axis modality using the inferior hinge point, superior hinge point, and inferolateral inflection point of the right ventricular apex, and the four-chamber modality using the left ventricular apex and mitral and tricuspid annulus. E-MHA produces an associated uncertainty estimate for each landmark prediction, representing the model's epistemic uncertainty as a continuous scalar value.A minor error in landmark prediction can result in incorrect image registration [16]. To address this issue, we hypothesize that incorrectly preprocessed samples resulting from inaccurate landmarks can introduce ambiguity during model training. For quality control, it is crucial to identify and effectively handle such samples. In this study, we leverage predicted landmarks and epistemic uncertainties to tackle this problem using uncertainty-based binning. To this end, we partition the training scans based on the uncertainty values of the landmarks. The predicted landmarks are divided into K quantiles, i.e., Q = {q 1 , q 2 , ..., q K }, based on the epistemic uncertainty values. We then iteratively filter out training samples starting from the highest uncertain quantile. A sample is discarded if the uncertainty of any of its landmarks lies in quantile q k where k = {1, 2, ..., K}. The samples are discarded iteratively until there is no improvement in the validation performance, as measured by the area under the curve (AUC), for two subsequent iterations.Tensor Feature Learning: To extract features from processed cardiac scans, we employ tensor feature learning, i.e. Multilinear Principal Component Analysis (MPCA) [11], which learns multilinear bases from cardiac MRI stacks to obtain low-dimensional features for prediction. Suppose we have M scans as third-order tensors in the form of {X 1 , X 2 , ..,where P n < I n , and × n denotes a mode-wise product. Therefore, the feature dimensions are reduced from I 1 × I 2 × I 3 to P 1 × P 2 × P 3 . We optimize the projection matrices {U (n) } by maximizing total scatterY m is the mean tensor feature and ||.|| F is the Frobenius norm [10]. We solve this problem using an iterative projection method. In MPCA, {P 1 , P 2 , P 3 } can be determined by the explained variance ratio, which is a hyperparameter. Furthermore, we apply Fisher discriminant analysis to select the most significant features based on their Fisher score [8]. We select the top k-ranked features and employ Support Vector Machine (SVM) for classification.Multimodal Feature Integration: To enhance performance, we perform multimodal feature integration using features extracted from the short-axis, fourchamber, and Cardiac Measurements (CM). We adopt two strategies for feature integration, namely the early and late fusion of features [6]. In early fusion, the features are fused at the input level without doing any transformation. We concatenate features from the short-axis and four-chamber to perform this fusion. We then apply MPCA [11] on the concatenated tensor, enabling the selection of multimodal features. In late fusion, the integration of features is performed at the common latent space that allows the fusion of features that have different dimensionalities. In this way, we can perform a late fusion of CM features with short-axis and four-chamber features. However, we can not perform an early fusion of CM features with short-axis and four-chamber features.Performance Evaluation: In this paper, we use three primary metrics: Area Under Curve (AUC), accuracy, and Matthew's Correlation Coefficient (MCC), to evaluate the performance of the proposed pipeline. Decision Curve Analysis (DCA) is also conducted to demonstrate the clinical utility of our methodology. "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,3,Experimental Results and Analysis,"Study Population : Patients with suspected pulmonary hypertension were identified after institutional review board approval and ethics committee review. A total of 1346 patients who underwent Right Heart Catheterization (RHC) and cardiac MRI scans within 24 hours were included. Of these patients, 940 had normal PAWP (≤ 15 mmHg), while 406 had elevated PAWP (> 15 mmHg). Table 1 summarizes baseline patient characteristics. RHC was performed using a balloon-tipped 7.5 French thermodilution catheter.Cardiac MRI and Measurement: MRI scans were obtained using a 1.5 Tesla whole-body GE HDx MRI scanner (GE Healthcare, Milwaukee, USA) equipped with 8-channel cardiac coils and retrospective electrocardiogram gating. Two cardiac MRI protocols, short-axis and four-chamber, were employed, following standard clinical protocols to acquire cardiac-gated multi-slice steadystate sequences with a slice thickness of 8 mm, a field of view of 48 × 43.2, a matrix size of 512 × 512, a bandwidth of 125 kHz, and TR/TE of 3.7/1.6 ms. Following [5], left ventricle mass and left atrial volume were selected as cardiac measurements."
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Experimental Design:,"We conducted experiments on short-axis and fourchamber scans across four scales. To determine the optimal parameters, we performed 10-fold cross-validation on the training set. From MPCA, we selected the top 210 features. We employed early and late fusion on short-axis and fourchamber scans, respectively, while CM features were only fused using the late fusion strategy. We divided the data into a training set of 1081 cases and a testing set of 265 cases. To simulate a real testing scenario, we designed the experiments such that patients diagnosed in the early years were part of the training set, while patients diagnosed in recent years were part of the testing set. We also partitioned the test into 5 parts based on the diagnosis time to perform different runs of methods and report standard deviations of methods in comparison results. For SVM, we selected the optimal hyper-parameters from {0.001, 0.01, 0.1, 1} using the grid search technique. The code for the experiments has been implemented in Python (version 3.9). We leveraged the cardiac MRI preprocessing pipeline and MPCA from the Python library PyKale [9] and SVM implementation is taken from scikit-learn [12]. "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Uncertainty-Based Sample Binning:,"To improve the quality of training data, we used quantile binning to remove training samples with uncertain landmarks. The landmarks were divided into 50 bins, and then removed one bin at a time in the descending order of their uncertainties. Figure 2 depicts the results of binning using 10-fold cross-validation on the training set, where the performance improves consistently over the four scales when removed bins ≤ 5. Based on the results, we removed 5 bins (129 out of 1081 samples) from the training set, and used the remaining 952 training samples for the following experiments."
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Unimodal Study:,"The performance of three models on single-modality is reported in Table 2, including short-axis (SA), four-chamber (FC), and cardiac measurements (CM), where the CM based unimodal is considered as the baseline. The results demonstrate an improvement of ΔAUC = 0.0800 ΔAccuracy = 0.0527, and ΔMCC = 0.3484 over the baseline obtained by FC based unimodal, which indicates that tensor-based features have a diagnostic value. "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Bi-modal Study:,"In this experiment, we compared the performance of bimodal models. As shown in Table 2, bimodal (four-chamber and CM) produces superior performance (i.e., AUC = 0.8135, Accuracy=0.7925 and MCC = 0.4999) among bi-modal models. Next, we investigated the effect of fusing CM features with short-axis and four-chamber modalities in Fig. 3. It can be observed from these figures that the fusion of CM features enhances the diagnostic power of cardiac MRI modalities at all scales. The bi-modal (four-chamber and CM) model achieved the improvement in the performance (ΔAUC = 0.0035 and ΔMCC = 0.0333) over the unimodal (four-chamber) model."
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Effectiveness of Tri-modal:,"In this experiment, we performed a fusion of CM features with the bi-modal models to create two tri-modal models. The first trimodal is tri-modal late (CM with a late fusion of short-axis and four-chamber) and the second tri-modal is a tri-modal hybrid (CM with an early fusion of shortaxis and four-chamber). As shown in Fig. 4, CM features enhance the performance of bi-modal models and tri-modal hybrid outperforms all. The tri-modal hybrid obtained the best performance (Table 2, where AUC = 0.8327, Accuracy = 0.8038, and MCC = 0.5099) and a significant improvement of ΔAUC = 0.1027, ΔAccuracy = 0.0628, and ΔMCC = 0.3917 over the baseline method.Decision Curve Analysis (DCA) [13,19] on the performance suggests the potential clinical utility of the proposed method. As shown in Fig. 5, the Trimodal model outperformed the baseline method for most possible benefit/harm preferences, where benefit indicates a positive net benefit (i.e. correct diagnosis) and harm indicates a negative net benefit (i.e. incorrect diagnosis). The tri-modal model (the best model) obtained a higher net benefit between decision threshold probabilities of 0.30 and 0.70 which implies that our method has a diagnostic value and can be used in screening high-risk patients from a large population.  Evaluating clinical utility of our method using Decision Curve Analysis (DCA) [19].""Treat All"" means treating all patients, regardless of their actual disease status, while ""Treat None"" means treating no patients at all. Our predictive model's net benefit is compared with the net benefit of treating everyone or no one to determine its overall utility.Feature Contributions: Our model is interpretable. The highly-weighted features were detected in the left ventricle and interventricular septum in cardiac MRI. For cardiac measurements, left atrial volume (0.778/1) contributed more than left ventricular mass (0.222/1) to the prediction."
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,4,Conclusions,"This paper proposed a tensor learning-based pipeline for PAWP classification. We demonstrated that: 1) tensor-based features have a diagnostic value for PAWP, 2) the integration of CM features improved the performance of unimodal and bi-modal methods, 3) the pipeline can be used to screen a large population, as shown using decision curve analysis. However, the current study is limited to single institutional data. In the future, we would like to explore the applicability of the method for multi-institutional data using domain adaptation techniques."
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Fig. 1 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Fig. 2 .Fig. 3 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Fig. 4 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Fig. 5 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Table 1 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Table 2 .,
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,1,Introduction,"Medical imaging technology has revolutionized the field of cardiac disease diagnosis, enabling the assessment of both cardiac anatomical structures and motion, including the creation of 3D models of the heart [5]. Cardiac cine magnetic resonance imaging (cMRI) [16,20] is widely used in clinical diagnosis [14], allowing for non-invasive visualization of the heart in motion with detailed information on cardiac function and anatomy [17]. While cMRI has great potential in helping doctors understand and analyze cardiac function [9,15], the imaging technique has certain drawbacks including low through-plane resolution to accommodate for the limited scanning time, as visualized in Fig. 1. Recently, researchers have approached the problem of cardiac volume reconstruction with learning-based generative models [2]. However, most of the methods suffer from low generation quality, missing key cardiac structures and long generation times. This paper focuses on improving the cardiac model generation quality, while reducing the generation time, aiming to better reconstruct the missing structure of the cardiac model from low through-plane resolution cMRI. Conventional 3D cardiac modeling [12] consists of 2D cardiac image segmentation followed by 3D cardiac volume reconstruction. Recent advances in deep learning methods have shown great success in medical image segmentation [4,6,11,23]. After obtaining 2D labels, the neighboring labels are stacked to reconstruct the 3D model. Nevertheless, due to the low inter-slice spatial cMRI resolution, a significant amount of structural information is lost in the resulting 3D volume. Thus, the interpolation between cMRI slices is necessary. Traditional intensity-based interpolation methods often yield blurring effects and unrealistic results. Conventional deformable model-based method [13] does not need consistency across images of the corresponding cardiac structures, but requires image-based structure segmentation which is nontrivial and hinders their ability to generalize. To overcome these limitations, an end-to-end pipeline based on generative adversarial networks (GANs), DeepRecon, was recently proposed in [2] that utilizes the latent space to interpolate the missing information between adjacent 2D slices. The generative network is first trained and a semantic image embedding in the W + space [1] is computed. Evidently, the acquired semantic latent code is not optimal and needs iterative optimization with segmentation information for improving image qualities. However, even with the optimization step, the generated images still miss details in the cardiac region, which indicates the W + space DeepRecon found does not represent the heart accurately.In order to eliminate the step for optimizing the latent code and improve the image generation quality, we propose a morphology-guided diffusion-based 3D cardiac volume reconstruction method that improves the axial resolution of 2D cMRIs through global semantic and regional morphology latent code interpolation as indicated in Fig. 1. Inspired by [19], we utilize the global semantic latent code to encode the image into a high-level meaningful representation of the image. To improve the cardiac volume reconstruction, our approach needs to focus on the cardiac region. Therefore, we introduce the regional morphology latent code which represents the shapes and locations of LVC, LVM and RVC, which will help generating the cardiac region. The method consists of three parts: an implicit diffusion model, a global semantic encoder and a segmentation network that encodes an image to regional morphology embeddings. The proposed method does not require iteratively fine-tuning the latent codes. Our contributions are: 1) the first diffusion-based method for 3D cardiac volume reconstruction, 2) introducing the local morphology-based latent code for improved conditioning on the image generation process, 3) 8% improvement of left ventricle myocardium (LVM) segmentation accuracy and 35% improvement of structural similarity index compared to previous methods, and 4) improved efficiency by eliminating the iterative step for optimizing the latent code."
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,2,Methods,"Figure 2 demonstrates the structure of our DMCVR approach that learns the global semantic, regional morphology, and stochastic latent spaces from MR images to yield a broad range of outcomes, including generation of high-quality 2D image and high-resolution 3D reconstructed volume. In this section, we will first describe the architecture of our DMCVR method and then elaborate on the latent space-based 3D volume generation which enables 3D volume reconstruction."
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,2.1,DMCVR Architecture,"Our DMCVR is composed of a global semantic encoder E sem , a regional moprphology network (E mor , D mor ) and a diffusion-based generator G. The generating process G is defined as follows: given input x T , sem , mor , which are the stochastic, global semantic and regional morphology latent codes, we want to reconstruct the image x 0 recursively as follows:where θ (x t , t, sem , mor ) is the noise prediction network and f θ is defined as removing the noise from x t or Tweedie's formula [3]:Here, the term α t is a function of t affecting the sampling quality.The forward diffusion process takes the noise x T as input and produces x 0 the target image. Since the change in x T will affect the details of the output images, we can treat x T as the stochastic latent code. Therefore, finding the correct stochastic latent code is crucial for generating image details. Thanks to DDIM proposed by Song et al. [21], it is possible to get x T in a deterministic fashion by running the generative process backwards to obtain the stochastic latent code x T for a given image x 0 . This process is viewed as a stochastic encoder x T = E sto (x 0 , sem , mor ), which is conditioned on sem and mor . This conditioning helps us to remove the iterative optimization step used by previous method. We formulate the inversion process from x 0 to x T as follows:Although using the stochastic latent variables we are able to reconstruct the image accurately, the stochastic latent space does not contain interpolatable high-level semantics. Here we utilize a semantic encoder proposed by Preechakul et al. [19] to encode the global high-level semantics into a descriptive vector for conditioning the diffusion process, similar to the style vector in StyleGAN [10]. The global semantic encoder utilizes the first half of the UNet, and is trained end-to-end with the conditional diffusion model.One drawback of the global semantic encoder is that it encodes the general high-level features, but tends to pay little attention to the cardiac region. This is due to the relatively small area of LVC, LVM and RVC in the cMRI slice. However, the generation accuracy of the cardiac region is crucial for the cardiac reconstruction task. For this reason, we introduce the regional morphology encoder E mor that embeds the image into the latent space containing necessary information to produce the segmentation map of the target cardiac tissues. With this extra morphology information, we are able to guide the generative model to focus on the boundary of the ventricular cavity and myocardium region, which will produce increased image accuracy in the cardiac region and the downstream segmentation task. Here, we do not assume any particular architecture for the segmentation network. However, in our experiments, we utilize the segmentation network MedFormer proposed by Gao et al. [4] for its excellent performance.The training of DMCVR contains the training of the segmentation network and the training of the generative model. We first train the segmentation model with summation of focal loss and dice loss [4]. We utilize the simple loss introduced in [7] for training the conditional diffusion implicit model, where"
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,2.2,3D Volume Reconstruction and Latent-Space-Based Interpolation,"Due to various limitations, the gap between consecutive cardiac slices in cMRI is large, which results in an under-sampled 3D model. In order to output a smooth super-resolution cine image volume, we generate the missing slices by using the interpolated global semantic, regional morphology and stochastic latent codes.For global semantic and regional morphology latent code , since it is similar to the idea of latent code in StyleGAN, we utilize the same interpolation strategies as in the original paper between adjacent slices. Assume that k < j -i, i < j,For interpolating the stochastic latent variable, it is important to consider that the distribution of stochastic noise is high-dimensional Gaussian, as shown in Eq. ( 4). Thus, our stochastic embedding is positioned on a sphere shown in Fig. 2. Using linear interpolation on the stochastic noise deviates from the underlying distribution assumption and causes the diffusion model to generate unrealistic images. Hence, to preserve the Gaussian property of the stochastic latent space, we interpolate the stochastic latent codes over a unit sphere, which can be written as follows: Let k < j -i, i < j and x i T • x j T = cos θ, "
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,3,Experiments,
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,3.1,Experimental Settings,"In this study we use data from the publicly available UK Biobank cardiac MRI data [18], which contains SAX and LAX cine CMR images of normal subjects.LVC, LVM and RVC are manually annotated on SAX images at the end-diastolic (ED) and end-systolic (ES) cardiac phases. We use 808 cases containing 484,800 2D SAX MR slices for training and 200 cases containing 120,000 2D images for testing. To evaluate the 3D volume reconstruction performance, we randomly choose 50 testing 2D LAX cases to evaluate the 3D volume reconstruction task. All models are implemented on PyTorch 1.13 and trained with 4×RTX8000."
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,3.2,Evaluation of the 2D Slice Generation Quality,"We provide peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) [8] to evaluate the similarity between the generated images and the original images. In addition to image quality assessment, we want to consider the segmentation performance on the generated images by using a segmentation network trained on the real training data as the evaluator and segment the testing images generated by DeepRecon 1k , DiffAE which only uses the global semantic latent code as the condition on the DDIM model, and our DMCVR methods. The segmentation accuracy of the evaluator on the generated images can be viewed as a quantitative metric to represent the generation quality of the generated data compared to the cMRI data. We compare segmentation obtained based on three methods against ground truth on the SAX images in Table 1. The Dice coefficient (DICE), volumetric overlap error (VOE), average surface distance (ASD), Hausdorff distance (HD) and average symmetric surface distance (ASSD) [22] are reported for comparison. Our method achieves a PSNR score of 30.504 and SSIM score of 0.982, which is a significant improvement (35% increase in SSIM) compared to Deep-Recon (PSNR: 27.684, SSIM: 0.724) with 1k optimization steps. This indicates that our method generates more realistic image compared to DeepRecon. The segmentation results on the original images in Table 1 provide an upper bound for other results. DMCVR outperforms all other methods in every metric with an 8% increase in LVM segmentation compared to DiffRecon 1k Moreover, by comparing the DiffAE and DMCVR, the introduction of the regional morphology latent code drastically improves the generation results due to the extra information on the shape of LVC, LVM, and RVC. Figure 3 demonstrates the original image and corresponding synthetic images. The white arrow points towards the presence of cardiac papillary muscles. As indicated in the images, DeepRecon 1k (b) cannot effectively recover the information of the papillary muscles from the latent space. However, both diffusion-based (c, d) methods accurately synthesize the information. Our method (d) generates a cleaner image with less artifacts than (c), especially around the LV and RV regions. By comparing the yellow circled area, our method produces image closer to the ground truth compared to DeepRecon 1k . Also, the white circle in Fig. 3 demonstrates the benefits of incorporating regional morphology information. Besides, the generative model used in DeepRecon 1k needs to be trained for 14 days with additional time to iteratively optimize the latent code for each slice. Our method uses 4.8 days for training. Since DDIM inversion does not have test-time optimization as DeepRecon does, DMCVR generates images faster than DeepRecon."
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,3.3,Evaluation of the 3D Volume Reconstruction Quality Through Latent Space Interpolation,"In this section, we exploit the relationship between SAX and LAX images and leverage the LAX label to evaluate the volume reconstruction quality. In cardiac MRI, long axis (LAX) slices typically comprise 2-chamber (2ch), 3-chamber (3ch), and 4-chamber (4ch) views. To evaluate the performance of different interpolation methods on LAX slices, we conducted the following experiments: 1) Nearest Neighbor resampling of short-axis (SAX) volume to each LAX view, 2) Image-based Linear Interpolation, 3) DeepRecon 1k , and 4) our DMCVR. Table 2 shows the computed 2D DICE score between the annotation of different LAX views and the intersection between the corresponding LAX plane and 3D reconstructed volume. Our method outperforms other methods in three categories and has only less than 1% performance degradation compared to DeepRecon 1k but with more stable performance. Figure 4 presents three examples for each LAX view, showing better reconstructed LAX results compared to the original images."
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,4,Conclusion,"Integrating analysis of cMRI holds significant clinical importance in understanding and evaluating cardiac function. We propose a diffusion-model-based volume reconstruction method. Our finding shows that through an interpolatable latent space, we are able to improve the spatial resolution and produce meaningful MR images. In the future, we will consider incorporating LAX slices as part of the generation process to help refine the latent space."
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,,Fig. 1 .,
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,,Fig. 2 .,
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,,Fig. 3 .,
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,,Table 2 .Fig. 4 .,
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,,Table 1 .,
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,,929 12.940 0.250 3.236 0.254,
 DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction,,865 23.636 0.282 3.519 0.267,
Skin Lesion Correspondence Localization in Total Body Photography,1,Introduction,"Evolution, the change of pigmented skin lesions, is a risk factor for melanoma [1]. Therefore, longitudinal tracking of skin lesions over the whole body is beneficial for early detection of melanoma [5]. However, establishing skin lesion correspondences across multiple scans from different patient visits has not been well investigated in the context of full-body imaging.Several techniques have been proposed to match skin lesions across pairs of 2D images [9][10][11][12][13][14]16,17,25]. Early work used geometric constraints imposed by initial matches of skin lesions (manual selection or automatic detection) to align images and further match other skin lesions [10,16,17,25]. Mirzaalian and colleagues published a series of works for establishing lesion correspondence in image space [11][12][13][14]. Li et al. [9] used a CNN to output a 2D vector field for pixel-wise correspondences between the two input images. Though effective at matching skin lesions across pairs of images, the extension of these methods to the context of total body photography (TBP) for longitudinal tracking remains a challenge.Several works have been proposed for tackling the skin lesion tracking problem over the full body [7,8,21,22]. However, they are either only applicable in well-controlled environments or do not extend to the tracking of lesions across scans at different visits. Recently, the concept of finding lesion correspondence using a 3D representation of the human body has been explored in [26] and [2] by using a template mesh. However, accurately deforming a template mesh to fit varying body shapes is challenging when the scanned shape deviates from the template, leading to large errors in downstream tasks such as establishing shape correspondence. Additionally, [26] does not take advantage of texture, while [2] uses texture in a common UV map that may lead to failures when geodesically close locations on the surface are mapped to distant sites in the texture map (e.g. when the two locations are on opposite sides of a texture seam).We propose a novel framework for finding skin lesion correspondence iteratively using geometric and texture information (Fig. 1). We demonstrate the effectiveness of the proposed method in localizing lesion correspondence across scans in a manner that is robust to changes in body pose and camera viewing directions. Our code is available at https://github.com/weilunhuang-jhu/ LesionCorrespondenceTBP3D."
Skin Lesion Correspondence Localization in Total Body Photography,2,Methods,"Given a set of lesions of interest (LOIs) X in the source mesh, we would like to find their corresponding positions Y in the target mesh. Formally, we assume we are given source and target meshes, M 0 and M 1 , with vertex setsWe achieve this by computing a dense correspondence map Φ L, : V 0 → V 1 , initially defined using geometric information and refined using textural information. Then we use that to define a map taking lesions of interest on the source to positions on the target Φ : X → V 1 ."
Skin Lesion Correspondence Localization in Total Body Photography,2.1,Landmark-Based Correspondences,"We define an initial dense correspondence between source and target vertices by leveraging the sparse landmark correspondences [3,6]. We do this by mapping source and target vertices into a high-dimensional space, based on their proximity to the landmarks, and then comparing positions in the high-dimensional space.Concretely, we define maps k : V k → R S , associating a vertex v ∈ V k with an S-dimensional feature descriptor that describes the position of v relative to the landmarks:whereis the geodesic distance function on M k . We use the reciprocal of geodesic distance so that landmarks closer to v contribute more significantly to the feature vector. Given this mapping, we create an initial dense correspondence between the source and target vertices, Φ L, : V 0 → V 1 by mapping a source vertex v ∈ V 0 to the target vertex with the most similar feature descriptor (with similarity measured in terms of the normalized cross-correlation):"
Skin Lesion Correspondence Localization in Total Body Photography,2.2,Texture-Based Refinement,"While feature descriptors of corresponding vertices on the source and target mesh are identical when 1) the landmarks are in perfect correspondence, and 2) the source and target differ by an isometry, neither of these assumptions holds in realworld data. To address this, we use local texture to assign an additional feature descriptor to each vertex and use these texture-based descriptors to refine the coarse correspondence given by Φ L, : V 0 → V 1 . Various texture descriptors have been proposed, e.g. SHOT [20,23,24], RoPS [4], and ECHO [15]. We selected the ECHO descriptor for its better descriptiveness and robustness to noise. Letting k (v) ∈ R N denote the ECHO descriptor of vertex v ∈ V k , our goal is to refine the dense correspondence so that corresponding source and target vertices also have similar descriptors. However, to avoid problems with repeating (local) textures, we would also like the correspondence to stay close to the correspondence defined by the landmarks.We achieve this as follows: To every source vertex v ∈ V 0 we associate a region R v ⊂ V 1 of target vertices that are either close to Φ L, (v) (the corresponding vertex on V 1 as predicted by the landmarks) or have similar geometric feature descriptors:(Given this region, we define the target vertex corresponding to a source as the vertex within the region that has the most similar ECHO descriptor (using the normalized cross-correlation as before).In practice, we compute the ECHO descriptor over three different radii, obtaining three descriptors for each vertex,The selection of three different radii in ECHO descriptors is done to accommodate different sizes of lesions and their surrounding texture, and the values are empirically determined. This gives a mapping Φ L, : V 0 → V 1 defined in terms of the weighted sum of cross-correlations:whereis the texture score of the target vertex v and w i is the weight of the cross-correlation between the ECHO descriptors computed at each radius."
Skin Lesion Correspondence Localization in Total Body Photography,2.3,Iterative Skin Lesion Correspondence Localization Framework,"While each source LOI has a corresponding position on the target mesh as given by Φ L, :, not all correspondences are localized with high confidence when 1) the local texture is not well-preserved across scans and 2) the local region R v does not include the true correspondence. To address this, we adapt our algorithm for computing the correspondence map Φ : X → V 1 by iteratively growing the set of landmarks to include LOI correspondences about which we are confident, similar to the way in which a human annotator would label lesion correspondence (Fig. 2). Iteratively Anchor Confident Correspondences. We iteratively compute correspondence maps Φ k L, :, with the superscript denoting the k th iteration. For each map Φ k L, and every LOI x ∈ X, we determine if we are confident in the correspondence {x, Φ k L, (x)} by evaluating a binary function χ k L : X → {0, 1}. Denoting by X the subset of LOIs about which we are confident, we add the pairs {x , Φ k L, (x )} to the landmark set L and remove the LOI x ∈ X from X. We iterate this process until all the correspondences of LOIs are confidently found or a maximum number of iterations (K) have been performed.Lesion correspondence confidence is measured using three criteria: i) texture similarity, ii) agreement between geometric and textural correspondences, and iii) the unique existence of a similar lesion within a region. To quantify uniqueness, we compute the set of target vertices whose textural descriptor is similar to that of the LOI:and consider the diameter of the set (defined in terms of the mean of the distances of vertices in S δ x from the centroid of S δ x ). Putting this together, we define confidence asx ) < ε5, (6) where the initial values of thresholds ε i are empirically chosen. To further support establishing correspondences, we relax the thresholds ε i in subsequent iterations, allowing us to consider correspondences that are further away and about which we are less confident.Final Correspondence Map. Having mapped every high-confidence LOI to a corresponding target vertex, we must complete the correspondence for the remaining low-confidence LOIs. We note that for a low-confidence LOI x ∈ X, the texture in the source mesh is not well-matched to the texture in the target, for any v ∈ R x . (Otherwise the first term in χ k L would be large.) To address this, we would like to focus on landmark-based similarity. However, by definition of R x , for all v ∈ R x , we know that the landmark descriptors of x and v will all be similar, so that C L, will not be discriminating. Instead, we use a standard transformation to turn distances into similarities. Specifically, we define geometric score between a source LOI x and a target vertex v ∈ R x in terms of the geodesic distance between v and the corresponding position of x in V 1 , as predicted by the landmark descriptors:where σ is the maximum geodesic distance from a vertex within R x to Φ K L, (x). Therefore, for a remaining LOI, we define its corresponding target vertex as the vertex with the highest weighted sum of the geometric and texture scores:where w 1 and w 2 are the weights for combining the scores."
Skin Lesion Correspondence Localization in Total Body Photography,3,Evaluation and Discussion,
Skin Lesion Correspondence Localization in Total Body Photography,3.1,Dataset,"We evaluated our methods on two datasets. The first dataset is from Skin3D [26] (annotated 3DBodyTex [18,19]). The second dataset comes from a 2D Imaging-Rich Total Body Photography system (IRTBP), from which the 3D textured meshes are derived from photogrammetry 3D reconstruction. The number of vertices is on average 300K and 600K for Skin3D and IRTBP datasets respectively. The runtime using 10 iterations is several minutes (on average) on an Intel i7-11857G7 processor. Example data of the two datasets can be found in the supplement."
Skin Lesion Correspondence Localization in Total Body Photography,3.2,Correspondence Localization Error and Success Rate,"Average correspondence localization error (CLE) for individual subjects, defined as the geodesic distance between the ground-truth and the estimated lesion correspondence, is shown in Fig. 3. To interpret CLE in a clinical application, the localized correspondence is successful if its CLE is less than a threshold criterion.We measured the success rate as the percentage of the correctly localized skin lesions over the total number of skin lesion pairs in the dataset.To compare our result to the existing method [26], we compute our success rates with the threshold criterion at 10 mm. As shown in Table 1, the performance of our method is comparable to the previously reported longitudinal accuracy. The qualitative result of the localized correspondence in the Skin3D dataset is shown in Fig. 4. A table of parameters used in the experiments can be found in the supplement. Table 1. Comparison of the success rate on Skin3D dataset. Each metric is computed on a pair of meshes (for one subject) and averaged across paired meshes with the standard deviation shown in brackets. The method Texture radius 50 and Combined radius 50 are defined in Fig. 5.Skin3D [26]  "
Skin Lesion Correspondence Localization in Total Body Photography,3.3,Usage of Texture on 3D Surface,"We believe that the geometric descriptor only provides a coarse correspondence while the local texture is more discriminating. Figure 5 shows the success rate under different threshold criteria for the proposed methods. Since we have two combinations of defining source and target for two scans, we measured the result in both to ensure consistency (Fig. 5(a) and (b)). As expected, we observed that using geometric information with body landmarks and geodesic distances is insufficient for localizing lesion correspondence accurately. However, the correspondence map Φ L, refined with local texture may lead to correspondences with large CLE when using a large region R x . The figure shows the discriminating power and the large-error-prone property of using Φ L, to localize lesion correspondence with one iteration (relatively high success rates under strict criteria and relatively low success rates under loose criteria, compared to the correspondence map combining geometric and texture scores in Eq. 8). The figure also shows the effectiveness of the proposed algorithm when the iterative anchor mechanism is used to localize lesion correspondence, having consistently higher success rates with the criteria within 20 mm.  Iterative algorithm is the proposed algorithm with the anchor mechanism. Shape is the method using Φ 1 L, . Texture radius 25 and texture radius 50 are the methods using Φ 1 L, with ε1 (Eq. 3) selected at 25 mm and 50 mm. Combined radius 25 and combined radius 50 are the methods using Eq. 8 with one iteration."
Skin Lesion Correspondence Localization in Total Body Photography,4,Conclusions and Limitations,"The evolution of a skin lesion is an important sign of a potentially cancerous growth and total body photography is useful to keep track of skin lesions longitudinally. We proposed a novel framework that leverages geometric and texture information to effectively find lesion correspondence across TBP scans. The framework is evaluated on a private dataset and a public dataset with success rates that are comparable to those of the state-of-the-art method.The proposed method assumes that the local texture enclosing the lesion and its surroundings should be similar from scan to scan. This may not hold when the appearance of the lesion changes dramatically (e.g. if the person acquires a tattoo). Also, the resolution of the mesh affects the precision of the positions of landmarks and lesions. In addition, the method may not work well with longitudinal data that has non-isometric deformation due to huge variations in body shape, inconsistent 3D reconstruction, or a dramatic change in pose and, therefore, topology, such as an open armpit versus a closed one.In the future, the method needs to be evaluated on longitudinal data with longer duration and new lesions absent in the target. In addition, an automatic method to determine accurate landmarks is desirable. Note that although we rely on the manual selection of landmarks, the framework is still preferable over manually annotating lesion correspondences when a subject has hundreds of lesions. As the 3D capture of the full body becomes more prevalent with better quality in TBP, we expect that the proposed method will serve as a valuable step for the longitudinal tracking of skin lesions."
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 1 .,
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 2 .,
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 3 .,
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 4 .,
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 5 .,
Skin Lesion Correspondence Localization in Total Body Photography,,,
Skin Lesion Correspondence Localization in Total Body Photography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_25.
Retinal Thickness Prediction from Multi-modal Fundus Photography,1,Introduction,"Retinal thickness map (RTM), generated from optical coherence tomography (OCT) volumes, provides a quantitative representation of various retina pathologic conditions [3]. The ETDRS grid is an array comprising nine values representing the averaged thickness in nine regions in RTM [5]. The RTM and ETDRS grid, are widely employed diagnostic and monitoring techniques for retinal disorders including age-related macular degeneration, glaucoma, and diabetic retinopathy [14], which are prevalent causes of visual impairment worldwide [6]. On the other hand, OCT has been a critical diagnostic tool in ophthalmology due to its exceptional sensitivity and precision in identifying major eye diseases.However, OCT exams are only available to limited patients as it is both costly and time-consuming, which impedes the acquisition of RTM and ETDRS grid. The recent advances in deep learning [20,21] have prompted research efforts aimed at addressing this limitation. There have been attempts to predict centerinvolving macular edema from color fundus photographs (C-FP) [17]. Although these studies showed high sensitivity and specificity, they only provided a binary classification for the presence of macular edema. The lack of quantitative retina thickness prediction results mandated further study.Fundus photography (FP) is widely used to image the retina, which captures the reflected signal of emitted signal from the retinal surface with a flash of light [13]. As the retina is partially-transparent, a minority of light would pass through the surface [19] and reflect back, which might carry information about the retinal thickness. This hypothesis motivates us to explore the connection between the RTM/ETDRS grid and the IR-FP/C-FP, which is rarely explored. Nonetheless, the FPs hold substantial clinical value in facilitating large-scale screening by acquiring RTM and ETDRS grid much faster and more affordable.Recently, Holmberg et al. [10] presented DeepRT, a convolutional neural network (CNN) designed for predicting retinal thickness using only infrared fundus photographs (IR-FP), disregarding C-FP. Exploring the capacity of C-FP to provide depth information has two major advantages: 1) More precise RTM prediction: Different from IR-FP, C-FP is acquired using light of multiple wavelengths that penetrate different depths in the retina [19]. We assume that this can provide richer thickness information, which can lead to more precise RTM prediction when combined with IR-FP; 2) Clinical significance: C-FP is the most commonly used diagnostic tool in ophthalmology, and can be obtained even using a smartphone [7]. The ability to derive thickness information from C-FP alone, without OCT scans, will make C-FP a potential tool for high functioning telemedicine platform which has the ability to diagnose, monitor treatment response, and even screen high-risk patients for diabetic macular edema (DME). In this paper, we explore the capability of IR-FP and C-FP to provide accurate retinal thickness information, with a cohort of patients with DME of different grades. We propose a Multi-Modal Fundus photography enabled Retinal Thickness prediction network (M 2 FRT). It is comprised of two separate encoders, a CNN E C and a Transformer E T , that encode localized information and rich depth information form IR-FP and C-FP respectively. We utilize the features extracted from E C to facilitate the learning process of E T in gathering 2D aligned thickness information via its attention mechanism. The enriched features are subsequently fed into a decoder to predict the RTM.Furthermore, we obtain the ETDRS grid prediction, i.e. nine values representing averaged thickness in the predefined areas in Fig. 1 (c 2 ), solely from the C-FP by processing the features extracted from E T through another lightweight decoder, which has significant clinical implications. To the best of our knowledge, we are the first to demonstrate the benefit of C-FP for RTM prediction and derive the ETDRS grid prediction solely from C-FP."
Retinal Thickness Prediction from Multi-modal Fundus Photography,2,Methodology,"In this study, we exclusively concentrate on DME to explore the predictive capacity of FPs regarding the retinal thickness. The rationale behind this is that, apart from DME, predicting retinal thickness itself has relatively less clinical value. For example, for age-related macular degeneration, the ophthalmologist needs to look for subtle changes in abnormal OCT features (e.g. subretinal fluid, pigmentary epithelial detachments [16]), rather than just the retinal thickness.In standard clinical settings, the ophthalmologist will acquire the C-FP upon patients' arrival. If RTM is deemed necessary for diagnosis, a separate device will capture IR-FP and conduct OCT scanning. Figure 1 (a) illustrates the acquisition process of RTM using OCT, where each B-scan is registered with the 2D positions in IR-FP. The ETDRS grid is an array comprising nine values indicating the average thickness (μm) in nine predefined regions in RTM (Fig. 1 (c 2 )).Dovetailed with the clinical settings, M 2 FRT aims to predict the RTM corresponding to the IR-FP, utilizing enriched depth information from pre-collected C-FP. The RTM requires precise pixel-wise correspondences to the IR-FP, while the ETDRS grid is a regional concept. Therefore, we can manage to derive an ETDRS grid prediction using only easier acquired C-FP, even in the absence of IR-FP, which holds importance within clinical scenarios and telemedicine.As mentioned above, the FPs from the two modalities are captured by different machines. So, the FPs are not registered and have a distinct field of view (FoV). The recent advances in vision Transformers [4,12,18] have inspired us to address this challenge, because the multi-head attention mechanism is location-agnostic, but rather leverages patch embedding and position encoding to introduce positional information."
Retinal Thickness Prediction from Multi-modal Fundus Photography,2.1,Encoder,"The overall pipeline of M 2 FRT is presented in Fig. 1 (b). The notations used for the images in the modality of IR-FP and C-FP are I IR-F P and I C-F P , respectively. The objective is to predict the thickness map M in the FoV of I IR-F P and ETDRS grid G, which represents the central area of the retina and is the major concern in clinical practices.The convolution and concatenation pose ""hard"" operations on the spatial dimensions. Thus, whether we concatenate I IR-F P and I C-F P as input or in the feature space under a CNN backbone, the misalignment of I IR-F P and I C-F P will deteriorate the performance for M prediction. In contrast, the spatial information is ""softly"" incorporated into the Transformer architecture, where the subsequent operations in the feature space are location-agnostic.Therefore, we utilize a CNN encoder E C from U-Net [15] to extract features from I IR-F P , and a Transformer encoder E T from 2D ViT/UNETR [4,8] to extract features from I C-F P . Notably, the deep features extracted by E T are spatially perturbed. M 2 FRT leverages attention mechanisms in E T to gather 2D aligned thickness information from I C-F P , guided by the features extracted from I IR-F P by E C . The extracted multi-level features from I IR-F P and I C-F P are denoted as f IR-F P and f C-F P respectively, as shown in the following equations:(1)"
Retinal Thickness Prediction from Multi-modal Fundus Photography,2.2,Decoder,"M 2 FRT extracts 2D aligned depth information from C-FP, which enrich the depth representations acquired from IR-FP in an end-to-end learning manner.The extracted features are fused by concatenation and passed to the decoder D M to generate the thickness map prediction M , whereWith fine-grained thickness information extracted for the RTM prediction task, the encoded features obtained from E T are ready to be decoded to predict the ETDRS grid using a lightweight decoder D G . In D G , the features from multiple levels are combined using a series of convolutions and concatenations. Then the final prediction for G is generated by a linear projection. The predicted ETDRS grid is denoted as G , where G = D G (f C-F P )."
Retinal Thickness Prediction from Multi-modal Fundus Photography,2.3,Loss Functions,"The loss functions L M 1 and L G 1 are employed in the prediction of the RTM and ETDRS grid using L 1 criteria, respectively, as shown in the following equations,where G (i) and G (i) are the i-th number in the ETDRS grid ground truth G and prediction G . The final loss function is3 Experiments Implementation Details. The M 2 FRT is implemented with PyTorch [2] and MONAI [1], and detailed configurations are in the supplementary material. Random flipping and rotation are utilized for data augmentation. We use the Adam [11] optimizer with (β 1 , β 2 ) = (0.9, 0.999) for training for 300 epoches.The initial learning rate is 0.001 and exponentially decayed with γ = 0.999.Performance Metrics. For the RTM predictions, we use mean absolute error (MAE) and peak signal-to-noise ratio (PSNR) for evaluation in the areas G 1,2,3 as shown in Fig. 1 (c 1 ), where the peak signal is set to 800μm. For the ETDRS grid predictions, we calculate the MAE of the predictions of the nine grids, as shown in Fig. 1 (c 2 ). For the right eye, the grid must be mirrored horizontally, i.e.,The Wilcoxon signed-rank test is employed to compare the performance of M 2 FRT with the baselines."
Retinal Thickness Prediction from Multi-modal Fundus Photography,3.2,Quantitative and Qualitative Evaluations on RTM Predictions,"To better illustrate the problem and our solution, we begin with the most concise design, U-Net [15]. In Table 1, the MAE/PSNR for U-Net with IR-FP as input are 27.84 µm/27.38 dB. By concatenating multi-modal IR-FP and C-FP as input to the U-Net, the performance improved to 25.16 µm/28.36 dB, indicating that C-FP has the potential of containing additional thickness information.However, the multi-modal FPs are unregistered and have a distinct FoV, in which case a mere concatenation of these inputs would diminish the network's capacity to effectively exploit the thickness information from paired 2D positions. A simple solution is to encode the multi-modal FPs with two separated convolutional encoders E C , where the features are deeply fused along the downsampling path. The unregistered problem is eased by the higher-level features with a larger receptive field, and the MAE/PSNR are improved to 24.80 µm/28.54 dB.After all, 2D convolution and concatenation pose a ""hard"" operation to the spatial dimensions, which is still interfered with by the unregistered problem. As shown in Fig. 2, in (b) and (c), there are artifacts in the RTM predictions caused by the annular boundary (grey arrows) and misaligned vessels from C-FP (purple arrows). On the contrary, the attention operations in Transformer are location-agnostic, where the spatial information is more ""softly"" introduced into the network by patch embedding and position encoding [4].Therefore, we employ distinct encoders of a CNN E C and a Transformer E T to IR-FP and C-FP respectively. The attention mechanism in E T is encouraged to gather 2D aligned thickness information from the perturbed patch embeddings, with the guidance from the decoder D M and the L M 1 loss function. With this CNN-Transformer hybrid design, the MAE/PSNR performance are  1, and the network produced the best visual quality and smaller errors in Fig. 2 (f) and (g).Since IR-FP acts as a localizer for the OCT scan and RTM, spatially perturbing the features from IR-FP with E T is not appropriate for the accurate prediction of RTM, and thus not yielding better quantitative results, as shown in Table 1. In Fig. 2 (d), the annular boundary artifacts from C-FP still exist (grey arrows). When both encoders are substituted by E T , in Fig. 2 (e), the 2D localizing information is degraded, in which case, there will be artifacts caused by the patch embedding (brown arrows).Our proposed M 2 FRT utilizes a combination of multi-modal IR-FP and C-FP to predict the RTM. M 2 FRT outperforms the state-of-the-art (SOTA) RTM prediction technique, DeepRT [10], which uses mono IR-FP as input. Besides, methods with multi-modal FPs surpass methods with mono IR-FP as input, especially in the central G 1 area, as shown in Table 1 and pink arrows in Fig. 2. The results demonstrate that C-FP has the ability to provide complementary depth information with IR-FP. The effectiveness of our methodology is validated through the ablation study on the encoders and decoders, as presented in Table 1.Additionally, when E T is guided to gather aligned features for RTM using the attention mechanism, the deep features from E T are ready to be decoded by D G for ETDRS grid predictions, which involves computing the averaged thickness in nine predefined regions. Notably, the ETDRS grid prediction task does not have a significant impact on the performance of the RTM prediction (the last two rows in Table 1), while the ETDRS grid prediction task can benefit from the supervision provided by the RTM prediction task, which will be discussed in Sect. 3.3."
Retinal Thickness Prediction from Multi-modal Fundus Photography,3.3,Quantitative Evaluations on ETDRS Grid Predictions,"Following the clinical settings, we predict the full RTM based on the IR-FP localizer in place of the OCT scanning procedure, which can boost mass screening. We gather enriched thickness information from C-FP and improve the performance with a hybrid CNN-Transformer design, as elaborated in Sect. 3.2.In addition to identifying 2D disease patterns in C-FP, predicting the ETDRS grid solely from C-FP can exploit additional information in the C-FP and hold significant clinical value for rapid diagnosis, especially in the field of telemedicine.To achieve this, we can adopt a conventional learning-based method to predict the nine numbers in the ETDRS grid, i.e. ResNet [9], as shown in Table 2.However, simply approximating the nine numbers will neglect the fine-grained thickness information. To address this issue, following the design in Sect. 3.2, the encoder E T for C-FP is guided by the encoder E C from the IR-FP part for detailed RTM predictions. Therefore, E T has been trained to extract fine-grained depth information from C-FP, which can be decoded for the averaged thickness for ETDRS grid predictions with D G . The fine-grained thickness supervision from the RTM prediction task can benefit the ETDRS grid prediction task, as shown in the last two rows of Table 2. Besides, our proposed M 2 FRT outperforms its ablation and other baselines, as shown in Table 2. We can also observe from Table 1 and 2 that the central thickness in G 1 area is more challenging to predict than the surrounding area for the RTM and ETDRS grid prediction task."
Retinal Thickness Prediction from Multi-modal Fundus Photography,4,Conclusion,"In this paper, we demonstrate the advantages of leveraging multi-modal information from C-FP for RTM prediction with respect to IR-FP, which overcomes the limitations of OCT and has the potential to enhance mass screening. Additionally, we propose a novel method for predicting the ETDRS grids solely from C-FP, which has significant clinical importance for fast diagnosis, telemedicine, etc. Our results indicate that additional fine-grained supervision from the RTM prediction task is beneficial for ETDRS grid prediction, where the ETDRS grid is decoded from the encoder of C-FP by a lightweight decoder during the training procedure of the RTM prediction task. Further research could be conducted for: 1) Predicting RTM of multiple retinal layers simultaneously, and 2) Improving RTM prediction's resolution and detail by acquiring finer OCT as ground truth."
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Fig. 1 .,
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Fig. 2 .,
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Table 1 .,"Data Pre-processing. For IR-FP, we center-crop the area corresponding to the OCT scanning area with a resolution of 544 × 544, and then calculate RTM Enc.: Encoder; Dec.: Decoder. M 2 FRT is comprised of EC , ET , DM , DG. ground truth within. With respect to the B-scans, the retinal thickness is calculated for 31 lines in the 2D IR-FP, and then linearly interpolated to match the resolution of IR-FP. For C-FP, we resize it to 544 × 544 from an original resolution of 3608 × 3608. The dataset is randomly split into training and test datasets at the patient level. The training/test dataset consisted of 657/310 images from 252/109 patients, respectively."
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Table 2 .,
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_55.
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,1,Introduction,"Magnetic resonance imaging (MRI) has emerged as an important tool for assessing brain development in utero [2,4,9,13]. Since manual segmentation is timeconsuming [30] and suffers from high inter-rater variability in quantitative assessment, automatically segmenting brain tissue from MRI data becomes an urgent need [7,10,15]. However, the available annotated fetal brain datasets are limited in number and heterogeneity, hindering the development of automatic strategy. To achieve the unsupervised fetal brain tissue segmentation, registrationbased methods [18,22] use image registration and label fusion to obtain the segmentation result from a set of templates [19,24]. Still, the accuracy of these methods is not sufficient due to the complexity of registration, and they usually underperform on the abnormal fetal brain image. Recently, deep learning (DL) based unsupervised domain adaptation (UDA) methods have shown their advance on medical image segmentation tasks [1,3,8]. UDA methods usually narrow the distribution discrepancy between source and target domains by enforcing image-/feature-level alignment [11,21,25,26]. In addition to inter-domain knowledge transfer, some works [14,29] explored the knowledge from both intermediate domains.However, existing UDA methods in medical imaging mainly focus on the gap from different modalities (e.g., CT and MR), and pay less attention to the domain gap from different centres. Due to motion artifacts [17], it is difficult to collect high-quality fetal brain MR images and is expensive to label the newly collected data voxel-wisely. The above observations motivate us to establish a new UDA problem setting that aims to transfer the segmentation knowledge from the publicly available atlases to unlabeled fetal brain MRIs from new centres.To solve the above UDA task, we propose an Appearance and Structure Consistency (ASC) framework. Consider the fact [26,27] that swapping the lowlevel spectrum between images can exchange their style/color/brightness without changing semantic content, while swapping the higher spectrum introduces unwanted artifacts. Thus, we propose to align appearance by only swapping the low-level spectrum. We develop an appearance consistency regularization based on a frequency-based appearance transformation, which is performed between labeled source data and unlabeled target data. Specifically, the source domain and the source data under the target appearance, are supervised with the same source labels. Then, the target data under the target appearance and source appearance are forced to maintain the same segmentation via dual unsupervised appearance consistency. Considering that significant variances in the shape of abnormal fetal brain tissue can cause difficulties in segmentation, we further constrain structure consistency under different perturbations in the target domain, besides aligning the inter-domain appearance gap. All the above consistency constraints are integrated with a teacher-student framework.The contributions of this work are three-fold: (1) we propose a novel Appearance and Structure Consistency framework for UDA in fetal brain tissue segmentation; (2) we propose to address a practical UDA task adapting publicly available brain atlases to unlabeled fetal brain MR images; (3) experimental results on FeTA2021 benchmark [17] show that the proposed framework outperforms representative state-of-the-art methods."
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2,Methodology,"In the UDA setting, D s = {X s , Y s } M s=1 denotes a set of source domain images (e.g., fetal brain atlases) and corresponding labels, respectively. D t = {X t } N t=1 denotes a set of target domain images (e.g., images from the FeTA benchmark). We aim to learn a semantic segmentation model for target domain data based on the labeled source and unlabeled target domain data. Usually, this goal is achieved by minimizing the domain gap between source domain samples D s and target domain samples D t . Figure 2 depicts the proposed Appearance and Structure Consistency (ASC) framework based on a teacher-student model."
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2.1,Frequency-Based Appearance Transformation,"Atlases are magnetic resonance fetal images with ""average shape"". Domain shifts between the atlases and fetal images are mainly due to the texture, different hospital sensors, illumination or other low-level sources of variability. However, traditional UDA employing GAN to synthetic style-transfer images hardly capture such domain shift. Thus, we align the low-level statistics based on Fourier transformation to narrow the distribution of the two domains. This process is shown in Fig. 3. Taking source data as an example, we compute the Fast Fourier transform (FFT) of each input image to obtain an amplitude spectrum F A and a phase component F P , where the low-frequency part of the amplitude of the source image F A (X s ) is swapped with the amplitude of the target image F A (X t ). Then, the transformed spectral representation of X s and the original phase F P (X s ) are mapped back to the image X sf t by inverse FFT (iFFT). X sf t has the same content as X s and similar appearance to X t . The above process can be formally defined as:where the mask M = I (h,w,d)∈[-βH:βH,-βW :βW,-βD:βD] controls the proportion of the swapped part over the whole amplitude by a parameter β ∈ (0, 1). Here we assume the center of the image is (0, 0, 0). Then we can train a student network with domain alignment images X sf t , the original images X s and the labels Y s by minimizing the dice loss:where P s and P sf t are the prediction of X s and X sf t , respectively."
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2.2,Appearance Consistency,"The above loss function imposes an implicit regularization before and after frequency-based transformation. In other words, source domain image X s and its transformation image X sf t should predict the same segmentation. However, the label of images from the target domain is not available X t in UDA settings.As a replacement, we propose a teacher model for keeping semantic consistency across domain transformation. Specifically, the target domain image X t and its aligned image X tf s are regarded as representations of an object under different domains. Given the inputs of X t and X tf s of teacher and student models, we expect their predictions to be consistent. Further, considering that appearance transformation may break certain semantic information and make the model learn the wrong mapping relationship, we employ a form of dual consistency, which directs the model to focus on invariant information between the two views. f (•) and f (•) represent the outputs of the student model and the teacher model, respectively. Following the conventional consistency learning methods [20], we calculate the appearance consistency loss L app con between the teacher and student networks as:"
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2.3,Structure Consistency,"Although frequency-based transformation and appearance consistency align the two domains' styles, the variance of tissue structure in pathological subjects still brings difficulty to domain alignment, which limits the model's gener alization ability. To this end, we utilise the teacher-student model [20] keeping prediction consistency L str con under structure perturbation [28] to alleviate the above problem. Here structure perturbation is sp for short. To achieve the structure perturbation, we first use a 3D cuboid mask consisting of a single box that randomly covers 25-50% of the image area at a random position, to blend two input images, which are sampled from the same batch. Then we blend the teacher predictions for the input images to produce a pseudo label for the student prediction of the blended image. Such an operation changes the original structural information, reduces the overfitting risk, and increases the robustness of the model to adapt to different structural variations. As appearance transformation doesn't affect the structure information, we add sp to both X t and X tf s to obtain X t,sp and X tf s,sp , which are fed into the teacher-student model and expected their predictions to be consistent. Then, L str con and L app con are combined as L asc :"
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2.4,Overall Training Strategy,"We calculate appearance and structure consistency using the same teacher model. Its model weight θ is updated with the exponential moving average (EMA) of the student model f (θ), i.e., θ t = αθ t-1 + (1 -α)θ t , where α is the EMA decay rate that reflects the influence level of the current student model parameters.Let λ control the trade-off between the supervised loss and the unsupervised regularization loss, the overall loss is:3 Experiment"
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,3.1,Dataset and Pre-processing,"We evaluated our method on the Fetal Brain Tissue Annotation and Segmentation Challenge (FeTA) 2021 benchmark dataset [17], which contains 80 3D T2 MRI volumes with manual segmentations annotation of external cerebrospinal fluid (eCSF), grey matter (GM), white matter (WM), lateral ventricles (LV), cerebellum (CBM), deep grey matter (dGM) and brainstem (BS). The dataset cohort consisted of two subgroups: 31 neurological fetuses and 49 fetuses with abnormal development. Following the general UDA setting [11], the target set was randomly divided into 40 scans for training and 40 scans for testing. A collection from three atlases was used as the source set, including 32 neurotypical fetal brain atlases [6,23] and 15 spina bifida fetal brain atlases [5]. Segmentations for all tissue types are available for all the atlas data. Some examples of slices, segmentations and histogram distribution are shown in Fig. 1. We cropped the foreground region of fetal volumes and reshaped them to 144 × 144 × 144. Before being fed into the network, the input scans were normalized to a zero mean and unit variance."
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,3.2,Implementation Details,"All models were implemented in PyTorch 1.12 and trained with NVIDIA A100 GPU with CUDA 11.3. Following the top-ranked method in the FeTA2021 competition [17], we use SegResNet [16] as the backbone for the teacher/student model. The network parameters were optimized with Adam with the initial learning rate of 1 × 10 -4 . ""Poly"" learning rate policy is applied, where lr = lr init × (1 -epoch epoch total ) 0.9 . The batch size and training epoch were set to 4 (2 from each domain) and 100, respectively. The EMA decay rate α of the teacher model was set to 0.99, and hyperparameters λ were ramped up individually with function λ(t) = γ × e (-5(1-t tmax ) 2 ) , where t, t max and γ were the current step, the last step and weight, respectively. β was set to 0.1. Cutmix [28] was used for target images as the structure perturbation for consistency regularization. We employed the student model prediction as the final result and used the Dice Similarity Coefficient (DSC) scores to evaluate the accuracy of the results. The average results of three runs were reported in all experiments. "
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,3.3,Comparison with the State-of-the-arts,"We implemented several state-of-the-art label-limited segmentation methods for comparison, including Registration-based (SCALE [19]), Unsupervised Domain Adaptatopm (UDA) (FDA [26], OLVA [1] and DSA [8]) and Semi-supervised Learning (CUTMIX [28], ASE-NET [12]) in Table 1. It reports the segmentation performance of UDA of adapting atlas to the fetal brain on FeTA2021, including average DSC for the full set, normal set and abnormal set. The upper bound is given by supervised training which uses fully-labeled target data for model training.It is worth noting that registration-based [19] only successfully segments on the normal set, and GAN-based UDA approach [8] performs worse than the frequency-based approach [26] on the abnormal set. The proposed method is superior to the existing state-of-the-art methods [26,28] and achieves mean Dice of 78.5% over the seven tissue structures, reducing the Dice gap to supervised training to 3.5%. Compared with the baseline model (W/o adaptation), our proposed learning strategy further improves the performance by an average of 3.2% Dice. Visual results in Fig. 4 show our method can perform better in the junction areas of brain tissue."
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,3.4,Ablation Study and Sensitivity Analysis,"The ablation study is shown in Table 2, and all the results boost our method's performance. ""M1"" represents the lower bound that only trains on the source domain data D s . ""M2"" uses the aligned source images X sf t for training. The following component are based on L asc , which is decoupled as L app con(Xt) , L app con(X tf s )and L str con . ""M3"" denotes the appearance consistency loss L app con(Xt) to align distribution from source to target. ""M4"" indicates the dual-view appearance consistency loss to constrain semantic invariance. ""M5"" denotes the structure consistency L str con . We can see that appearance consistency can boost performance on normal and abnormal fetal MRIs, showing that minimizing the appearance gap between the source domain and target domain is effective. Further, we can obtain better results on the abnormal samples by applying the structure consistency loss. Table 3 shows that the performance of our method grows with the increase of the hyper-parameter γ of consistency loss, and achieves the best when γ = 200. Besides, efficiency analysis is shown in the supplementary material."
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,4,Conclusion,"In this paper, we present a novel UDA framework and an atlas-based UDA setting for fetal brain tissue segmentation. Our method integrates appearance consistency encouraging the model to adapt different domain styles to narrow the domain gap and structure consistency making the model robust against the anatomical variations in the target domain. Experiments on the FeTA2021 benchmark demonstrate that our method outperforms the state-of-the-art methods. The proposed novel setting of atlas-based UDA could provide accurate segmentation for the fetal brain MRI data without pixel-wise annotations, greatly reducing the labeling costs."
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Fig. 1 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Fig. 2 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Fig. 3 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Fig. 4 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Table 1 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,±0.2 81.7 ±0.1 78.5 ±0.1,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Table 2 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,±0.2 81.7 ±0.1 78.5 ±0.1,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Table 3 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 31.
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,1,Introduction,"Cataract is the leading cause of blindness worldwide, and cataract surgery is one of the most common operations in health care. Among different cataract surgery techniques, phacoemulsification cataract surgery (PCS) is the standard of care [5,23]. PCS consists of the manual opening of the crystalline lens anterior capsule with forceps, removal of the opacified lens, and implantation of an intraocular lens (IOL) in the remaining capsular envelope to restore visual function.Phacoemulsification needs microsurgical skills, which depend on numerous variables, including the amount of practice, inherent manual dexterity, and previous experience. Statistical analyses have demonstrated significant differences in completion and complication rates among ophthalmologists, with variations observed based on factors such as seniority and experience [14]. During phacoemulsification, a surgical microscope with an integrated video camera is routinely used, providing rich spatiotemporal information [15]. This information presents an excellent opportunity to develop surgical video recognition methods to extract valuable intraoperative information and overlay it on a 2D/3D screen or the microscopic eyepiece, thereby creating an augmented reality (AR) scene.To bridge the experience gap among ophthalmologists, several intraoperative AR-guided systems have been developed. Zhai et al. [25] used two convolutional neural networks (CNNs) to segment the limbus and track the eye's rotation, and subsequently developed an intraoperative guide system for positioning and aligning the IOL. In [16], a multi-task CNN was designed to locate the pupil and recognize the surgical phase in each frame of the surgical microscope video. Nespolo et al. [17] utilized a deep CNN-based method for processing surgical videos, allowing for detecting surgical instruments and tissue boundaries to guide the ophthalmic surgery. Despite the potential of AR-guided phacoemulsification systems, limitations still hinder their clinical implementation. Firstly, the current systems process surgical videos in a frame-wise manner, enabling real-time processing but leading to lost temporal information and incoherent surgical scene recognition. Secondly, the overlaid information during surgery is not categorized by surgical phase, causing visual redundancy for ophthalmologists.Advancements in video spatiotemporal learning, particularly in surgical phase recognition, present a promising opportunity to switch AR scenes to the current surgical phase automatically. Early attempts used a 2D CNN to extract spatial features to predict each video frame's surgical phase [18,22]. However, the lack of temporal information leads to unsatisfactory recognition accuracy. Other studies [11,24] use spatial feature maps of neighboring frames, extracted from CNNs, as an input to Gated Recurrent Unit (GRU) [2] or Long Short-Term Memory (LSTM) [10] to model the temporal dependencies and predict the surgical phase. However, these methods suffer from limited temporal reception field and non-parallel, slow inference. Recent studies focus on modeling long-range temporal relationships. Czempiel et al. [3] introduced a multi-stage temporal convolutional network (TCN) for surgical phase recognition, leveraging causal and dilated convolutions to enable global reception field and online deployment. The current state-of-the-art methods utilize transformer-based models for aggre- gating spatial [4,27] and spatiotemporal features [8,12]. However, focusing solely on global features may result in losing important local temporal dependencies and lead to inaccurate recognition of challenging frames.In this study, we developed a novel intraoperative AR-guide system for PCS. Our contributions are two-fold: (1) We propose an efficient spatiotemporal network for surgical microscope video recognition, consisting of two stages: a multitask learning stage for limbus segmentation and spatial feature extraction, and a temporal pyramid-based spatiotemporal feature aggregation (TP-SFA) module for online surgical phase recognition. (2) We use the limbus and surgical phase information to design phase-specific visual cues that offer real-time intraoperative AR guidance while avoiding distracting the ophthalmologist's attention."
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,2,Methods,"Figure 1 presents an overview of our developed AR-guided system for PCS, which acquires intraoperative video streams from microscope and processes them using the proposed two-stage spatiotemporal network to obtain limbus and surgical phase information. Parameters of intraoperative visual cues are computed using the fitted limbus elliptic parameters and updated according to the recognized surgical phase, providing automatic AR scene switching for ophthalmologists."
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,2.1,Spatiotemporal Network for Microscopic Video Recognition,"Limbus Segmentation and Spatial Feature Extraction. We observe that the region within the limbus displays distinguishable appearances at different phases in surgical microscope videos, whereas other regions like the sclera exhibit similar appearances. We argue that using a limbus region-focused spatial feature extraction network can improve spatiotemporal aggregation. This led us to develop a multi-task network for limbus segmentation and phase recognition in the first stage. As shown in Fig. 1(a), we employ ResNet-50 [9] as the shared backbone to extract spatial features, which are then fed into both the surgical phase recognition and limbus segmentation branches.The surgical phase recognition branch involves a fully connected layer that is directly connected to the global average pooling layer, followed by a softmax layer. To train this branch, we use cross-entropy loss, which is defined aswhere g s is the ground truth binary indicator of phase s, p s is the probability of the input frame belonging to phase s.The limbus segmentation branch incorporates a decoder with upsampling and concatenation, resembling the U-net [20] architecture. To train this branch, we employ a hybrid loss of cross-entropy and dice, which is defined aswhere y c i and p c i are the pixel-level ground truth and prediction result respectively, α is a hyper-parameter to balance the loss. The final loss function for training the first stage is defined aswhere β is a hyper-parameter to balance the loss. After training the first stage, we obtain the spatial feature s t ∈ R 2048 for frame t by outputting the average pooling layer of the spatial feature extractor.Spatiotemporal Features Aggregation. We argue that the surgical phase recognition method used for intraoperative AR should fulfill the following requirements: 1) online recognition for real-time intraoperative guidance, and 2) sufficient stability to avoid distracting ophthalmologists with incorrect phase recognition. As shown in Fig. 1(b), we employ our proposed TP-SFA module in the second stage, which uses multi-scale, causal, and dilated temporal convolutions to model the temporal relationships. We denote the input spatial feature as S 0 ∈ R 2048×T , where T is the sequence length of the video. The first layer of the TP-SFA module is a 1 × 1 convolutional layer that reduces the dimension of S 0 and outputs P 0 ∈ R 32×T . To obtain temporal features with different reception fields, we apply N L dilated layers with different dilation factors on P 0 . Layer k consists of a dilated convolution with a dilation factor of 2 k , followed by a ReLU activation and a 1 × 1 convolution. This can be described as where * denotes the convolutional operator, W 1 is the dilated convolution weights, W 2 is the weights of the 1 × 1 convolution, and b 1 and b 2 are bias vectors. Finally, we concatenate all P k (k = 1, • • • , N L ) together over the temporal dimension, followed by a 1 × 1 convolution, a residual connection with P 0 and another 1 × 1 convolution to adjust the output dimension. This can be described aswhere W 3 and W 4 are the weights of the 1×1 convolution, and b 3 and b 4 are bias vectors. Inspired by [3,7], we connect N tp TP-SFA modules together and use a weighted cross-entropy loss to train the second stage. This can be described aswhere w s is the weight and is inversely proportional to the surgical phase frequencies [6]."
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,2.2,Phase-Specific AR Guidance in PCS,"The proposed spatiotemporal learning network enables real-time limbus segmentation and surgical phase, facilitating the development of our intraoperative AR guidance system for PCS. The limbus contour can be fitted as an ellipse [25,26].To accomplish this, we follow the steps shown in Fig. 1(c), including: 1) identifying the maximum connected region to remove possible mis-segmented regions;2) extracting the contour of the maximum connected region and sampling the contour points; 3) removing contour points near the video boundaries; 4) fitting the remaining contour points to an ellipse and outputting the length and rotation of the long and short axes of the ellipse. We segment PCS into nine phases [19]: incision, rhexis, hydrodissection, phacoemulsification, epinucleus removal, viscous agent (VA) injection, implant setting-up, VA removal, and stitching up. For intraoperative AR guidance, we designed six visual cues, including: 1) fitted limbus ellipse (FLE), extracted from the segmentation results; 2) primary incision curve (PIC), defined as an arc with a length equal to the maximum diameter of the primary incision knife; 3) secondary incision curve (SIC), defined as an arc with a length equal to the maximum diameter of the secondary incision knife; 4) incision guide lines (IGL), with included angles of 95 circ for BIC and 173 circ for SIC, respectively, relative to the reference line; 5) rhexis region (RR), with a diameter equal to half the length of the long axis of the fitted ellipse; and 6) implant reference line (IRL), defined by a horizontal line. Table 1 lists different combinations of intraoperative visual cues that are automatically updated according to the recognized surgical phase."
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3,Experiments and Results,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3.1,Dataset and Implementation Details,"Dataset. We evaluate our methods on CATARACTS [1], a publicly available dataset for cataract surgery. It contains 50 videos with a frame rate of 30 framesper-second (fps) and a total duration of over nine hours. All videos have been subsampled to 1 fps. Each frame has a resolution of 1920 × 1080 pixels and has been annotated in nineteen surgical steps. For the sake of intraoperative guidance convenience, we have reorganized the nineteen fine-grained surgical steps into nine standard phases [19]. Additionally, the limbus region of each frame has been manually delineated by two non-M.D. experts. The dataset is split into 25 cases for training, 5 cases for validation, and 20 cases for test, following [1,12]. Implementation Details. Our network was implemented in PyTorch using two NVIDIA GeForce GTX 3090 GPUs. We initialized the ResNet-50 backbone with weights trained on the ImageNet [21] and implemented random horizontal flip, random crop, random rotation (±20 circ ), and color jitter for data augmentation. The first stage was trained for 100 epochs using Adam optimizer with a learning rate of 5e-5 for the backbone and 5e-4 for the fully connected layer and decoder. The second stage was trained for 50 epochs using Adam optimizer with a learning rate of 1e-4. For hyper-parameters, we set α = 0.6, β = 0.5 and N L =8. "
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3.2,Comparisons with Strong Baselines,"We compare our method with several strong baselines in surgical phase recognition: 1) ResNet-50 [9], a deep residual network that predicts surgical phase in a frame-wise manner; 2) SV-RCNet [11], an end-to-end architecture that utilizes LSTM to learn temporal features; 3) TeCNO [3], a multi-stage temporal convolutional network that models global temporal features; and 4) Trans-SVNet [8], a transformer-based spatiotemporal features aggregation network. Note that we only included comparison methods that support online surgical phase recognition and excluded those that do not. For fair comparison, we use the proposed multi-task learning network in the first stage as the spatial feature extraction backbone for all methods except ResNet-50 [9]. The quantitative comparison results are listed in Table 2, which indicates that our method achieved the best performance among all compared methods. We show the quantitative results with color-coded ribbons in Fig. 2(a). The results indicate that our method can produce a smoother phase prediction compared to ResNet-50 [9] and SV-RCNet [11]. Additionally, our approach surpasses the performance of global feature aggregation-based methods [3,8] in challenging local frames."
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3.3,Ablation Study,"Effect of Multi-task Feature Extractor We performed experiments to evaluate the effect of the multi-task feature extractor. ResNet-50 [9] served as the backbone for all methods. We compared the accuracy of the first stage and the second stage for the single phase recognition task with that of the multi-task  approach. Moreover, we compared the segmentation Dice score of the single segmentation task with that of the multi-task approach. Table 3 shows the results, indicating that the multi-task feature extractor enhances both segmentation and phase recognition performance compared to the single-task approach.Effect of the TP-SFA Module We evaluate the number of the connected TP-SFA modules. The quantitative results are listed in Table 4, which indicates that the second stage achieves the best accuracy when two TP-SFA modules are used. Furthermore, we explore different combinations of the TP-SFA module and a typical causal TCN module [13] and present the results in Table 5. Results show that two connected TP-SFA modules achieve the best performance."
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3.4,AR Guidance Evaluation,"Our method achieves real-time intraoperative processing at a speed of 36 fps. This makes it suitable for meeting the demands of online intraoperative AR guidance, as the acquired microscope video stream has a speed of 30 fps. We show some typical failed scenes in Fig. 2(b). Failed intraoperative AR guidance can result from both mis-recognition of surgical phases and mis-segmentation of the limbus. Mis-recognition of surgical phase may introduce continuous AR scene switching problem and distract the ophthalmologist's attention."
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,4,Conclusion,"We proposed a two-stage spatiotemporal network for online microscope video recognition. Furthermore, we developed a phase-specific intraoperative AR guidance system for PCS. Our developed system has the potential for clinical applications to enhance ophthalmologists' intraoperative skills."
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Fig. 1 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Fig. 2 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 1 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 2 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 3 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 4 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 5 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,1,Introduction,"Anterior segment optical coherence tomography (AS-OCT) is a widely used noninvasive imaging modality for ocular disease [1,2]. It produces high-resolution views of superficial anterior segment structures, such as the cornea, iris, and ciliary body. However, speckle noise inherently exists in AS-OCT imaging systems [3], which can introduce uncertainty in clinical observations and increase the risk of misdiagnosis. AS-OCT despeckling has become an urgent pre-processing task that can benefit clinical studies.To suppress speckle noise in AS-OCT images, commercial scanners [4] generally average repeated scans at the same location. However, this approach can result in artifacts due to uncontrollable movement. As a result, several postprocessing denoising approaches have been developed to reduce speckles, such as wavelet-modified block-matching and 3D filters [5], anisotropic non-local means filters [6], and complex wavelets combined with the K-SVD method [7]. However, these algorithms can lead to edge distortion depending on the aggregation of similar patches. Deep learning has recently been employed for medical image processing, especially, with promising performance for image denoising tasks [8][9][10]. To overcome the limitations caused by the requirement for vast supervised paired data, unsupervised algorithms explore some promising stages to loosen the paired clinical data collection, including cycle consistency loss [11], contrast learning strategies [12], simulated schemes [13], or the Bayesian model [14]. Alternatively, the denoising diffusion probabilistic model (DDPM) can use the averaged image of repeated collections to train the model with excellent performance due to its focus on the noise pattern rather than the signal [15]. Given the prominent pixel-level representational ability for low-level tasks, diffusion models have also been introduced to medical image denoising based on the Gaussian assumption of the noise pattern [16,17].Although previous studies have achieved outstanding performances, deploying AS-OCT despeckling algorithms remains challenging due to several reasons: (1). Gathering massive paired data for supervised learning is difficult because clinical data acquisition is time-consuming and expensive. (2). Speckle noise in AS-OCT images strongly correlates with the real signal, making the additive Gaussian assumption on the speckle pattern to remove noise impractically.(3). Unsupervised algorithms can easily miss inherent content, and structural content consistency are vital for clinical intervention in AS-OCT [18,19]. (4). Existing algorithms focus on suppressing speckles while ignoring the performance improvement of clinical analysis from despeckling results.To address these challenges, we propose a Content-Preserving Diffusion Model for AS-OCT despeckling, named CPDM, which removes speckle noise in AS-OCT images while preserving the inherent content simultaneously. Firstly, we efficiently remove noise via a conditioned noise predictor by truncated diffusion model [16] in the absence of supervised data. We convert the speckle noise into an additive Gaussian pattern by considering the statistical distribution of speckles in AS-OCT to adapt to the reverse diffusion procedure. Secondly, we incorporate the posterior probability distribution in observed AS-OCT images into an iterative reverse stage to avoid getting trapped in artificial artifacts and preserve consistent content. The posterior distribution is regarded as a data fidelity term to constrain the iterative reverse procedure for despeckling. Finally, experiments on the AS-Casia and CM-Casia datasets demonstrate the effectiveness of CPDM compared to state-of-the-art (SOTA) algorithms. Further experiments on ciliary muscle (CM) segmentation and scleral spur (SS) localization verify that the CPDM can benefit clinical analysis."
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,2,The Statistical Characteristic of Speckles,"Speckle noise is inherent in coherent imaging systems [3], as it results from the destructive interference of multiple-scattered waves. As shown in Fig. 1, unlike the additive Gaussian noise Y i = x i + N i (i = 1, ..., n), the multiplicative speckle noise is modeled as Y i = x i N i [20], where Y denotes the noisy image, x is the noise-free image, N is the speckle noise, and i is the pixel index. Moreover, N consists of independent and identically distributed random variables with unit mean, following a gamma probability density function p N [21]:where Γ (•) is the Gamma function and M is the number of multilook [21].To transform the multiplicative noise into an additive one, logarithmic transform [22] is employed on both sides of Eq. 1, as: loge Mw e -e w M . According to the central limit theorem and analyzing the statistical distribution of transformed one in [23,24], W approximately follows a Gaussian distribution. Besides, we can obtain the prior distribution:(2) Diffusion Model. The diffusion model can subtly capture the semantic knowledge of the input image and prevails in the pixel-level representation [15]. As shown in Fig. 2(a), it defines a Markov chain that transforms an image x 0 to white Gaussian noise x T ∼ N (0, 1) by adding random noise in T steps. During inference, a random noise x T is sampled and gradually denoised until it reaches the desired image x 0 . To perfectly recover the image in the reverse sampling procedure, a practicable constraint D KL (q(x t-1 |x t , x 0 ) p θ (x t-1 |x t )) was proposed to minimize the distance between p θ (x t-1 |x t ) and q(x t |x t-1 ) [15]. Thus x t-1 can be sampled as follows:where ε θ is an approximator intended to predict noise ε from x t and I ∼ N (0,1)."
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Truncated Diffusion Model.,"As mentioned in the previous section, speckle noise follows a gamma distribution and can be transformed into a Gaussian distribution via a logarithmic function. This transformation enables matching the Markov chain procedure in the reverse diffusion process. To speed up the sampling process, this work introduces a truncated reverse procedure that can directly obtain satisfying results from posterior sampling [16]. Figure 2(c) illustrates that only the last few reverse diffusion iterations calculated by parameter estimation technique [25] are used to obtain the desired result during despeckling inference. Specifically, following [15], a Markov chain adds Gaussian noise to the data until it becomes pure noise and then gradually removes it by the reverse procedure at the training stage shown in Fig. 2(a). At the despeckling inference stage shown in Fig. 2(b), speckled images are converted into additive Gaussian ones by applying a logarithmic function. Then, the iteration number is determined by estimating the noise levels [16] to achieve an efficient and effective truncated reverse diffusion procedure. Therefore, AS-OCT despeckling can start from noisy image distributions rather than pure noise.CPDM Integrated Fidelity Term. Inspired by the fact that the score-based reverse diffusion process is a stochastic contraction mapping so that as long as the data consistency imposing mapping is non-expansive, data consistency incorporated into the reverse diffusion results in a stochastic contraction to a fixed point [26]. This work adopts the theory into the inverse AS-OCT image despeckling problems, as the iteration steps which impose fidelity term can be easily cast as non-expansive mapping. Accordingly, we can design a fidelity term to achieve data consistency by modeling image despeckling inverse problem. Specifically, invoking the conditional independence assumption, the prior distribution with Eq. 2 can be rewritten as:The Bayesian maximum a posteriori (MAP) formulation leads to the image despeckling optimization with data fidelity and regularization terms.arg minwhere R() is the regularization term, and λ is the regularization parameter.The unconstrained minimization optimization problem can be defined as a constrained formulation by variable splitting method [27]:Motivated by the iterative restoration methods with prior information to tackle various tasks become mainstream, we explore the fidelity term Eq. 4 from the posterior distribution of observed images into the iterative reverse diffusion procedure. The fidelity can guarantee data consistency with original images and avoid falling into artificial artifacts. Moreover, we learn reasonable prior from DDPM reverse recover procedure, which can ensure the flexibility with iterative fidelity term incorporated into the loop of prior generation procedure. As shown in Fig. 2(b), the recovery result obtained from the reverse sampling of DDPM (Eq. 3) can be considered as regularization information of the image despeckling optimization model, and the fidelity term in Eq. 5 can ensure the consistency of the reverse diffusion process with the original image content. Therefore, we can achieve AS-OCT image despeckling by solving Eq. 6 with the ADMM method using variable splitting technique [21]:where the hyperparameter u control the degree of freedom. It is worth mentioning that Eq. 7 is obtained with the trained CPDM model, and Eq. 8 can be solved by the Newton method [28]. Finally, we design an AS-OCT image despeckling scheme by adopting a fidelity term integrated statistical priors to preserve content in the iterative reverse procedure."
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,4,Experiment,"To evaluate the performance of the proposed CPDM for AS-OCT image despeckling, we conduct the comparative experiment and a ablation study in despeckling three evaluations, including despeckling evaluation, subsequent CM segmentation or SS localization.Dataset Preparation. A series of unsupervised methods including generative adversarial networks (GAN) and diffusion models aim at learning the noise distribution rather than the signal. Therefore, we collect images by averaging 16 repeated B-scans as noisy-free data collected from AS-OCT, the CASIA2 (Tomey, Japan). This study obeyed the tenets of the Declaration of Helsinki and was approved by the local ethics committee. AS-Casia dataset contains 432 noisy image and 400 unpaired clean image with the size of 2131 × 1600, which are views of the AS structure, including lens, cornea, and iris. 400 noisy data and 400 clean images were used for training, and the rest were for testing. The SS location in the noisy image is annotated by ophthalmologists.CM-Casia dataset consists of 184 noisy images and 184 unpaired clean data with the size of 1065 × 1465 that show the scope of CM tissue. 160 noisy images and 160 clean data are utilized for training network, with the remaining images reserved for testing. Moreover, ophthalmologists annotated the CM regions on the noisy images.  Implementation Settings. The backbone of our model is a simplified version of that in [15]. The CPDM network was trained on an NVIDIA RTX 2080TI 48GB GPU for 500 epochs, with a batch size of 2, using Adam optimizer. The variance schedule is set to linearly increase from 10 -4 to 6 -3 in T = 1000 steps and the starting learning rate is 10 Comparison on AS-Casia Dataset. We first evaluate the despeckling performance by parameterless index, including contrast-to-noise ratio (CNR) [8], the equivalent number of looks (ENL) [8], and natural image quality evaluator (NIQE) [31]. Then we compare the despeckling results with the SOTA methods by using the SS localization task with trained models in [32]. Concretely, we calculate a euclidean distance (ED) value between the reference and the predicted SS position with despeckled images via trained models. As shown in Table 1, the proposed CPDM achieves promising despeckling results in terms of the best CNR, ENL, NIQE values and the minimum ED error in the SS localization task among all approaches. The visual comparison for denoised images with competing approaches is shown in Fig. 3: the green region has been enlarged to highlight the structure of the anterior lens capsule, which can assist in diagnosing congenital cataracts. It can be observed that the CUT and CycleGAN models oversmooths structures close to flat, the UINT method results in ringing effects while the WBM3D, ANLM and DRDM algorithms retain speckles in the lens structure. Obviously, the proposed CPDM acquires satisfactory quality with fine structure details and apparent grain.Comparison on CM-Casia Dataset. We conduct the experiment of image despeckling and the following CM segmentation task to validate the clinical benefit with CPDM. Specifically, we train a U-Net segmentation model [33] on the CM-Casia dataset and then test the despeckled images of various methods. F1-Score and intersection over union (IoU) index for segmentation were calculated between the despeckled images and reference as reported in Table 1. It can be seen that the proposed CPDM achieves the superior despeckling performance by the highest CNR, ENL, NIQE values and segmentation metrics. Moreover, the segmented CM example of competitive methods is depicted in Fig. 4, in which the CM boundaries reference with the red line, and the yellow line means the segmented results. We can see that NLM, ANLM, CUT, and Speckle2void methods fail to the continuous segmentation results due to insufficient speckle suppres-sion or excessive content loss while the CPDM captures a distinct CM boundary and obtains the highest IoU score. Notably, as a type of smooth muscle, CM has ambiguous boundaries, which are easily affected by speckles, resulting in difficulty distinguishing CM from the adjacent sclera and negative CNR values. Despite these challenges, the proposed CPDM can achieve the best segmentation owing to the speckle reduction while preserving the inconspicuous edge content.Ablation Study. Table 1 shows the ablation study of the proposed CPDM. We compare our method with two variants: ODDM [17] and logDM. The ODDM only considers removing the speckles by hijacking the reverse diffusion process with the Gaussian assumption on speckles. Based on the ODDM, the logDM further transforms speckles to Gaussian distribution by analyzing the statistical characteristics of speckles. Additionally, the CPDM adopts the data fidelity term to regulate the despeckling reverse process by integrating content consistency.From Table 1, we can see that both the logarithmic function and data fidelity term can improve the quality of despeckled images and benefit the subsequent clinical analysis. Consequently, a prominent unsupervised CPDM to AS-OCT image despeckling is acquired with the proposed strategies."
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,5,Conclusions,"Due to the impact of speckles in AS-OCT images, monitoring and analyzing the anterior segment structure is challenging. To improve the quality of AS-OCT images and overcome the difficulty of supervised data acquisition, we propose a content-preserving diffusion model to achieve unsupervised AS-OCT image despeckling. We first analyze the statistical characteristic of speckles and transform it into Gaussian distribution to match the reverse diffusion procedure.Then the posterior distribution knowledge of AS-OCT image is designed as a fidelity term and incorporated into the iterative despeckling process to guarantee data consistency. Our experiments show that the proposed CPDM can efficiently suppress the speckles and preserve content superior to the competing methods. Furthermore, we validate that the CPDM algorithm can benefit medical image analysis based on subsequent CM segmentation and SS localization task."
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Fig. 1 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Fig. 2 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Fig. 3 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Fig. 4 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Table 1 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Acknowledgments,". This work was supported in part by General Program of National Natural Science Foundation of China (Grant No. 82272086), Guangdong Provincial Department of Education (Grant No. 2020ZDZX3043), Shenzhen Natural Science Fund (JCYJ20200109140820699 and the Stable Support Plan Program 20200925174052004), A*STAR Advanced Manufacturing and Engineering (AME) Programmatic Fund (A20H4b0141) and A*STAR Central Research Fund (CRF) ""Robust and Trustworthy AI system for Multi-modality Healthcare""."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,1,Introduction,"Data augmentation (DA) is a key factor in the success of deep neural networks (DNN) as it artificially enlarges the training set to increase their generalization ability as well as robustness [22]. It plays a crucial role in medical image analysis [8] where annotated datasets are only available with limited size. DNNs have already successfully supported radiologists in the interpretation of magnetic resonance images (MRI) for prostate cancer (PCa) diagnosis [3]. However, the DA scheme received less attention, despite its potential to leverage the data characteristic and address overfitting as the root of generalization problems.State-of-the-art approaches still rely on simplistic spatial transformations, like translation, rotation, cropping, and scaling by globally augmenting the MRI sequences [12,20]. They exclude random elastic deformations, which can change the lesion outline but might alter the underlying label and thus produce counterproductive examples for training [22]. However, soft tissue deformations, which are currently missing from the DA schemes, are known to significantly affect the image morphology and therefore play a critical role in accurate diagnosis [6].Both lesion and prostate shape geometrical appearance influence the clinical assessment of Prostate Imaging-Reporting and Data System (PI-RADS) [24]. The prostate constantly undergoes soft tissue deformation dependent on muscle contractions, respiration, and more importantly variable filling of the adjacent organs, namely the bladder and the rectum. Among these sources, the rectum has the largest influence on the prostate and lesion shape variability due to its large motion [4] and the fact that the majority of the lesions are located in the adjacent peripheral prostate zone [1]. However, only one snapshot of all these functional states is captured within each MRI examination, and almost never will be exactly the same on any repeat or subsequent examination. Ignoring these deformations in the DA scheme can potentially limit model performance.Model-driven transformations attempting to simulate organ functions -like respiration, urinary excretion, cardiovascular-and digestion mechanics -offer a high degree of diversity while also providing realistic transformations. Currently, the finite element method (FEM) is the standard for modeling biomechanics [13]. However, their computation is overly complex [10] and therefore does not scale to on-the-fly DA [7]. Recent motion models rely on DNNs using either a FEM model [15] or complex training with population-based models [18]. Motion models have not been integrated into any deep learning framework as an online data augmentation yet, thereby leaving the high potential of inducing applicationspecific knowledge into the training procedure unexploited.In this work we propose an anatomy-informed spatial augmentation, which leverages information from adjacent organs to mimic typical deformations of the prostate. Due to its lightweight computational requirements, it can be easily integrated into common DA frameworks. This technique allows us to simulate different physiological states during the training and enrich our dataset with a wider range of organ and lesion shapes. Inducing this kind of soft tissue deformation ultimately led to improved model performance in patient-and lesion-level PCa detection on an independent test set. "
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2,Methods,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.1,Mathematical Model of the Anatomy-Informed Deformation,"Model-driven spatial transformations simulate realistic soft-tissue deformations, which are part of the physiology, and can highly affect the shape of the prostate as well as the lesions in it. As the computation of state-of-the-art FEM models does not scale to on-the-fly DA, we introduce simplifications to be able to integrate such a biomechanical model as an online DA into the model training:-soft tissue deformation of the prostate is mostly the result of morphological changes in the bladder and rectal space [4,6], -due to the isotropic mechanical behavior of the rectum and the bladder [19],we apply isotropic deformation to them, -we assume similar elastic modulus between the prostate and surrounding muscles [16], allowing us to approximate these tissue classes as homogeneous, -we introduce a non-linear component into the model by transforming the surrounding tissue proportionally to the distance from the rectum and bladder in order to generate realistic deformations [23].Based on them, we define the vector field V for the transformation as the gradient of the convolution between the Gaussian kernel G σ and the indicator function S organ , multiplied by a scalar C to control deformation amplitude and direction:The resulting V serves as the deformation field for an MRI sequence I(x, y, z):It allows us to simulate the distension or evacuation of the bladder or rectal space. We refer to this transformation as anatomy-informed deformation. We make it publicly available in Batchgenerators [9] and integrate it into a nnU-Net trainer https://github.com/MIC-DKFZ/anatomy_informed_DA."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.2,Experimental Setting,"We evaluate our anatomy-informed DA qualitatively as well as quantitatively.First, we visually inspect whether our assumptions in Sect. 2.1 regarding pelvic biomechanics resulted in realistic transformations. We apply either our proposed transformation to the rectum or the bladder, random deformable or no transformation in randomly selected exams and conduct a strict Turing test with clinicians having different levels of radiology expertise (a freshly graduated clinician (C.E.) and resident radiologists (C.M., K.S.Z.), 1.5 -3 years of experience in prostate MRI) to determine if they can notice the artificial deformation.Finally, we quantify the effect of our proposed transformation on the clinical task of patient-level PCa diagnosis and lesion-level PCa detection. We derive the diagnosis through semantic segmentation of the malignant lesions following previous studies [5,11,12,20,21]. Semantic segmentation provides interpretable predictions that are sensitive to spatial transformations, making it appropriate for testing spatial DAs. To compare the performance of the trained models to radiologists, we calculate their performance using the clinical PI-RADS scores and histopathological ground truths. To consider clinically informative results, we use the partial area under the Receiver Operating Characteristic (pAUROC) for patient-level evaluation with the sensitivity threshold of 78.75%, which is 90% of the sensitivity of radiologists for PI-RADS ≥ 4. Additionally, we calculate the F 1 -score at the sensitivity of PI-RADS ≥ 4. Afterward, we evaluate model performances on object-level using the Free-Response Receiver Operating Characteristic (FROC) and the number of detections at the radiologists' lesion level performance for PI-RADS ≥ 4, at 0.32 average number of False Positives per scan. Objects were derived by applying a threshold of 0.5 to the softmax outputs followed by connected component analysis to identify connected regions in the segmentation maps. Predictions with an Intersection over Union of 0.1 with a ground truth object were considered True Positives. To systematically compare the effect of our proposed anatomy-informed DA with the commonly used settings, we create three main DA schemes:1. Basic DA setting of nnU-Net [8], which is an extensive augmentation pipeline containing simple spatial transformations, namely translation, rotation and scaling. This setting is our reference DA scheme. 2. Random deformable transformations as implemented in the nnU-Net [8] DA pipeline extending the basic DA scheme (1) to test its presence in the medical domain. Our hypothesis is that it will produce counterproductive examples, resulting in inferior performance compared to our proposed DA.3. Proposed anatomy-informed transformation in addition to the simple DA scheme (1). We define two variants of it: (a) Deforming only the rectum, as rectal distension has the highest influence among the organs on the shapes of the prostate lesions [4]. (b) Deforming the bladder in addition to the rectum, as bladder deformations also have an influence on lesions, although smaller."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.3,Prostate MRI Data,"774 consecutive bi-parametric prostate MRI examinations are included in this study, which were acquired in-house during the clinical routine. The ethics committee of the Medical Faculty Heidelberg approved the study (S-164/2019) and waived informed consent to enable analysis of a consecutive cohort. All experiments were performed in accordance with the declaration of Helsinki [2] and relevant data privacy regulations. For every exam, PI-RADS v2 [24] interpretation was performed by a board-certified radiologist. Every patient underwent extended systematic and targeted MRI trans-rectal ultrasound-fusion transperineal biopsy. Malignancy of the segmented lesions was determined from a systematic-enhanced lesion ground-truth histopathological assessment, which has demonstrated reliable ground-truth assessment with sensitivity comparable to radical prostatectomy [17]. The samples were evaluated according to the International Society of Urological Pathology (ISUP) standards under the supervision of a dedicated uropathologist. Clinically significant prostate cancer (csPCa) was defined as ISUP grade 2 or higher. Based on the biopsy results, every csPCa lesion was segmented on the T2-weighted sequences retrospectively by multiple in-house investigators under the supervision of a board-certified radiologist. In addition to the lesions, the rectum and the bladder segmentations were automatically predicted by a model built upon nnU-Net [8] trained iteratively on an in-house cohort initially containing a small portion of our cohort. Multiple radiologists confirmed the quality of the predicted segmentations."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.4,Training Protocol,"774 exams were split into 80% training set (619 exams) and 20% test set (155 exams) by stratifying them based on the prevalence of csPCa (36.3%). The MRI sequences were registered using B-spline transformation based on mutual information to match the ground-truth segmentations across all modalities [12,14].As the limited number of exams with csPCa and the small lesion size compared to the whole image can cause instability during training, we adapted the cropping strategy from [21] by keeping the organ segmentations to use the anatomy-informed DA (offsets of ±9 mm axial to the prostate and ±11.25 mm in the axial plane to the rectum and the bladder). The images are preprocessed by the automated algorithm of nnU-Net [8]. We trained 3D nnU-Net models in 5-fold cross-validation with different spatial DA schemes, see Sect. 2.2. The hyperparameter C of the anatomy-informed DA was optimized using validation results, sampled during training with uniform distribution constrained by amplitude values in positive and negative directions of C = {300, 600, 900, 1200, 1500}. C rectum = 1200 and C bladder = 600 were selected for the final models. Compared to the standard nnU-Net settings, we implemented balanced sampling regarding the prevalence of csPCa and reduced the number of epochs to 350 to avoid overfitting. We used Mish activation function, Ranger optimizer, cosine anneal learning rate scheduler, and initial learning rate of 0.001 following [12]. The final models are ensembled and evaluated on the independent test set using bootstrapping with 1000 replications to provide standard deviation and to calculate p-values for the F 1 -score and for the number of detected lesions using two-sided t-test to determine statistical significance."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,3,Results,"The anatomy-informed transformation produced highly realistic soft tissue deformations. Figure 2 shows an example of the transformation simulating rectum distensions with prostate lesions at different distances from the rectum. 92% of the rectum and 93% of the bladder deformation from the randomly picked exams became so realistic that our freshly graduated clinician did not detect them, but our residents noticed 87.5% of the rectum and 25% of the bladder deformations based on small transformation artifacts and their expert intuition. Irregularities resulted from the random elastic deformations can be easily detected, in contrast to our method being challenging to detect its artificial nature. In Table 1 we summarize the patient-level pAUROC and F 1 -scores; and lesion-level FROC results on the independent test set showing the advantage of using anatomy-informed DA. To further highlight the practical advantage of the proposed augmentation, we compare the performance of the trained models to the radiologists' diagnostic performance for PI-RADS ≥ 4, which locate the most informative performance point clinically on the ROC diagram, see Fig. 3.  Extending the basic DA scheme with the proposed anatomy-informed deformation not only increased the sensitivity closely matching the radiologists' patient-level diagnostic performance but also improved the detection of PCa on a lesion level. Interestingly, while the use of random deformable transformation also improved lesion-level performance, it did not approach the diagnostic performance of the radiologists, unlike the anatomy-informed DA.At the selected patient-and object-level working points, the model with the proposed rectum-and bladder-informed DA scheme reached the best results with significant improvements (p < 0.05) compared to the model with the basic DA setting by increasing the F 1 -score with 5.11% and identifying 4 more lesions (5.3%) from the 76 lesions in our test set.The time overhead introduced by anatomy-informed augmentation caused no increase in the training time, the GPU remained the main bottleneck."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,4,Discussion,"This paper addresses the utilization of anatomy-informed spatial transformations in the training procedure to increase lesion, prostate, and adjacent organ shape variability for the task of PCa diagnosis. For this purpose, a lightweight mathematical model is built for simulating organ-specific soft tissue deformations. The model is integrated into a well-known DA framework and used in model training for enhanced PCa detection.Towards Radiologists' Performance. Inducing lesion shape variability via anatomy-informed augmentation to the training process improved the lesion detection performance and increased the sensitivity value towards radiologistlevel performance in PCa diagnosis in contrast to the training with the basic DA setting. These soft tissue deformations are part of physiology, but only one snapshot is captured from the many possible functional states within each individual MR examination. Our proposed DA simulates examples of physiologic anatomical changes that may have occurred in each of the MRI training examples at the same exam time points, thereby aiding the generalization ability as well as the robustness of the network. We got additional, but slight improvements by extending the DA scheme with bladder distensions. A possible explanation for this result is that less than 30% of the lesions are located close to the bladder, and our dataset did not contain enough training examples for more improvements.Realistic Modeling of Organ Deformation. Our proposed anatomyinformed transformation was designed to mimic real-world deformations in order to preserve essential image features. Most of the transformed sequences successfully passed the Turing test against a freshly graduated clinician with prostate MRI expertise, and some were even able to pass against radiology residents with more expertise. To support the importance of realism in DA quantitatively, we compared the performance of the basic and our anatomy-informed DA scheme with that of the random deformable transformation. The random deformable DA scheme generated high lesion shape variability, but it resulted in lower performance values. This could be due to the fact that it can also cause implausible or even harmful image warping, distorting important features, and producing counterproductive training examples. In comparison, our proposed anatomy-informed DA outperformed the basic and random deformable DA, demonstrating the significance of realistic transformations for achieving superior model performance.High Applicability with Limitations. The easy integration into DA frameworks and no increase in the training time make our proposed anatomy-informed DA highly applicable. Its limitation is the need for additional organ segmentations, which requires additional effort from the annotator. However, pre-trained networks for segmenting anatomical structures like nnU-Net [8] have been introduced recently, which can help to overcome this limitation. Additionally, our transformation computation allows certain errors in the organ segmentations compared to applications where fully accurate segmentations are needed. The success of anatomy-informed DA opens the research question of whether it enhances performance across diverse datasets and model backbones."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,5,Conclusion,"In this work, we presented a realistic anatomy-informed augmentation, which mimics typical organ deformations in the pelvis. Inducing realistic soft-tissue deformations in the model training via this kind of organ-dependent transformation increased the diagnostic accuracy for PCa, closely approaching radiologistlevel performance. Due to its simple and fast calculation, it can be easily integrated into DA frameworks and can be applied to any organ with similar distension properties. Due to these advantages, the shown improvements in the downstream task strongly motivate to utilize this model as a blueprint for other applications."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Fig. 1 .,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Fig. 2 .,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Fig. 3 .,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Table 1 .,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_50.
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,1,Introduction,"Achieving complete tumor resection in surgical oncology like breast conserving surgery (BCS) is challenging as boundaries of tumors are not always visible/ palpable [10]. In BCS the surgeon removes breast cancer while attempting to preserve as much healthy tissue as possible to prevent permanent deformation and to enhance cosmesis. The current standard of care for evaluating surgical success is to investigate the resection margins, which refers to the area surrounding the excised tumor. Up to 30% of surgeries result in incomplete tumor resection and require a revision operation [10]. The intelligent knife (iKnife) is a mass spectrometry device that can address this challenge by analyzing the biochemical signatures of resected tissue using the smoke that is released during tissue incineration [3]. Each spectrum contains the distribution of sampled ions with respect to their mass to charge ratio (m/z). Previously, learning models have been used in combination with iKnife data for ex-vivo tissue characterization and real-time margin detection [16,17].The success of clinical deployment of learning models heavily relies on approaches that are not only accurate but also interpretable. Therefore, it should be clear how models reach their decisions and the confidence they have in such decision. Studies suggest that one way to improve these factors is through data centric approaches i.e. to focus on appropriate representation of data. Specifically, representation of data as graphs has been shown to be effective for medical diagnosis and analysis [1]. It has also been shown that graph neural networks can accurately capture the biochemical signatures of iKnife and determine the tissue type. Particularly, Graph Transformer Networks (GTN) has have shown to further enhance the transparency of underlying relation between the graph nodes and decision making via attention mechanism [11].Biological data, specially those acquired intra-opertively, are heterogeneous by nature. While the use of ex-vivo data collected under specific protocols are beneficial to develop baseline models, intra-operative deployment of these models is challenging. For iKnife, the ex-vivo data is usually collected from homogeneous regions of resected specimens under the guidance of a trained pathologist, versus the intra-operative data is recorded continuously while the surgeon cutting through tissues with different heterogeneity and pathology. Therefore, beyond predictive power and explainable decision making, intra-operative models must be able to handle mixed and unseen pathology labels.Uncertainty-aware models in computer-assisted interventions can provide clinicians with feedback on prediction confidence to increase their reliability during deployment. Deep ensembles [15] and Bayesian networks [9] incur high runtime and computational cost both at training and inference time and thus, less practical for real-time computer-assisted interventions. Evidential Deep Learning [18] is another approach that has been proposed based on the evidence framework of Dempster-Shafer Theory [12]. Since the evidential approach jointly generates the network prediction and uncertainty estimation, it seems more suitable for computationally efficient intra-operative deployment. In this paper, we propose Evidential Graph Transformer (EGT), a combination of graph-based feature-level attention mechanism with sample-level uncertainty estimation, to increase the performance and interpretability of surgical margin assessment. This is done by implementing the evidential loss and prediction functions within a graph transformer model to output the uncertainty, intermediate attention, and model prediction. To demonstrate the state-of-theart performance of the proposed approach on mass spectrometry data, the model is compared with different baselines in both cross-validation and prospective schemes on ex-vivo data. Furthermore, the performance of model is also investigated intraoperatively. In addition to the proposed model, we present a new visualization approach to better correlate the graph nodes with the spectral content of the data, which improves interpretability. In addition to the ablation study on the network and graph strictures, we also investigate the metabolic association of breast cancer hormone receptor status."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2,Materials and Methods,"Figure 1 presents the overview of the proposed approach. Following data collection and curation, each burn (spectrum) is converted to a single graph structure. The proposed graph model learns from the biochemical signatures of the tissue to classify cancer versus normal tissue. The uncertainty and intermediate attentions generated by the model are visualized and explored for their association with the biochemical mechanisms of cancer."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.1,Data Curation,"Ex-vivo: Data is collected from fresh breast tissue samples from the patients referred to BCS at Kingston Health Sciences Center over two years. The study is approved by the institutional research ethics board and patients consent to be included. Peri-operatively, a pathologist guides and annotates the ex-vivo pointburns, referred to as spectra, from normal or cancerous breast tissue immediately after excision. In addition to spectral data, clinicopathological details such as the status of hormone receptors is also provided post-surgically. In total 51 cancer and 149 normal spectra are collected and stratified into five folds (4 for cross validation and 1 prospectively) with each patient restricted to one fold only."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Intra-operative:,"A stream of iKnife data is collected during a BCS case (27 min) at Kingston Health Sciences Center. At the sampling rate of 1 Hz, a total of 1616 spectra are recorded. Each spectrum is then labeled based both on surgeons comments during the operation and post-operative pathology report.Preprocessing: Each spectrum is converted to a hierarchical graph as illustrated in Fig. 2. The nodes are generated from a specific subband in each spectrum. Different subband widths (50, 100, 300, and 900 m/z) are used to create different levels of hierarchy (Fig. 2). The edges connect nodes with overlapping subbands within and between levels. As a result, each graph (spectrum) consists of 58 nodes and 135 edges. For details on graph conversion please refer to [2] and [11]. For easier interpretation of nodes with respect to their corresponding subbands, we visualize the graph as a Piano-key plot in Fig. 2, where each key represents a node with m/z range equal to the angular extent of the key. The dark keys show the subband overlaps between adjacent nodes."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.2,Network Architecture and Training,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Graph Transformer Network:,"The GTN consists of a node embedding layer, L Graph Transformer Layers (GTL), a node aggregation layer, multiple dense layers, and a prediction layer [8]. Assume a graph G with N nodes and h i ∈ R d×1 as node features of node i. In each GTL, the H headed attention mechanism updates the features of node i based on all neighboring node features h j that are directly connected to node i via e ij edges. The attention mechanism for node update at layer l + 1 is formulated as:where Q k,l , K k,l , and V k,l are trainable linear weights. The weights w kl ij defines the k-th attention that is paid by node j to update node i at layer l. The concatenation of all H attention heads multiplied by trainable parameters O l generates final attention ĥl+1 i , which is passed through batch normalization and residual layers to update the node features for the next layer. After the last GTL, features from all nodes are aggregated, then passed to the dense layers to construct a final prediction output."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Evidential Graph Transformer:,"Evidential deep learning provides a welldefined theoretical framework to jointly quantify classification prediction and uncertainty modeling by assuming the class probability follows a Dirichlet distribution [18]. We propose to modify the loss and prediction layer of GTN, considering the same assumption, to formulate the Evidential Graph Transformer model. Therefore, there are two mechanisms embedded in EGT: i) node-level attention calculation -via aggregation of neighboring nodes according to their relevance to the predictions, and ii) graph-level uncertainty estimation -via fitting the Dirichlet distribution to the predictions.In the context of surgical margin assessment, the attentions reveal the relevant metabolic ranges to cancerous tissue, while uncertainty helps identify and filter data with unseen pathology. Specifically, the attentions affect the predictions by selectively emphasizing the contributions of relevant nodes, enabling the model to make more accurate predictions. On the other hand, the spread of the outcome probabilities as modeled by the Dirichlet distribution represents the confidence in the final predictions. Combining the two provides interpretable predictions along with the uncertainty estimation.Mathematically, the Dirichlet distribution is characterized by α = [α 1 , ..., α C ] where C is the number of classes in the classification task. The parameters can be estimates as α = f (x i |Θ) + 1 where f (x i |Θ) is the output of the Evidential Graph Transformer parameterized by Θ for each sample(x i ). Then, the expected probability for the c-th class p c and the total uncertainty u for each sample (x i ) can be calculated as p c = αc S , and u = C S , respectively, where S = C c=1 α c . To fit the Dirichlet distribution to the output layer of our network, we use a loss function consisting of the prediction error L p i and the evidence adjustmentwhere λ is the annealing coefficient to balance the two terms. L p i can be crossentropy, negative log-likelihood, or mean square error , while L e i (Θ) is KL divergence to the uniform Dirichlet distribution [18]."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.3,Experiments,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Network/Graph Ablation:,"We explore the hyper-parameters of the proposed model in an extensive ablation study. The attention parameters include the number of attention heads ( 1-15 with step size of 2) and the number of hidden features (7)(8)(9)(10)(11)(12)(13)(14). For the evidential loss, we evaluate the choice of loss function (the 3 previously mentioned), and the annealing coefficient (5-50 with step size of 5). The number of GTLs and dense layers are both fixed at 3. Additionally, we run ablation studies on the graph structure themselves to show the importance of presenting the data as graphs. We try randomizing the edge connections and dropping the nodes with overlapping m/z subbands."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Ex-vivo Evaluation:,"The performance of the proposed network is compared with 3 baseline models including GTN, graph convolution network [14], and non-graph convolution network. Four-fold cross validation is used for comparison of the different approaches, to increase the generalizability (3 folds for train/validation, test on remaining unseen fold, report average test performance). Separate ablation studies are performed for the baseline models to fine tune their structural parameters. All experiments are implemented using PyTorch with Adam optimizer, learning rate of 10 -4 , batch size of 32, and early stopping based on validation loss. To demonstrate the robustness of the model and ensure it is not overfitting, we also report the performance of the ensemble model from the 4-fold cross validation study on the 5th unseen prospective test fold.Clinical Relevance: Hormone receptor status plays an important role in determining breast cancer prognosis and tailoring treatment plans for patients [6]. Here, we explore the correlation of the attention maps generated by EGT with the status of HER2 and PR hormones associated with each spectrum. These hormones are involved in different types of signaling that the cell depends on [5]."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Intra-operative Deployment:,"To explore the intra-operative capability of the models, we deploy the ensemble models of the proposed method as well as the baselines from the cross-validation study to the BCS iKnife stream."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,3,Results and Discussion,"Ablation Study and Ex-vivo Evaluation: According to our ablation study, hyper parameters of 11 attention heads, 11 hidden features per attention head, the cross entropy loss function, and annealing coefficient of 30, result in higher performances when compared to other configurations (370k learnable parameters). The performance of EGT in comparison with the mentioned baselines are summarized in Table 1. As can be seen, the proposed EGT model with average accuracy of 94.1% outperformed all the baselines statistically significantly (maximum p-values of 0.02 in one-tail paired Wilcoxon Signed-Rank test). The lower standard deviation of parameters shows the robustness of EGT compared to other baselines. The regularization term in EGT loss prevents overconfident estimation of incorrect predictions [18] that could lead to superior results, compared to GTN, without overfitting. Lastly, when compared to other state-ofthe-art baselines with uncertainty estimation mechanisms, the proposed Evidential Graph Transformer network (average balanced accuracy of 91.6 ± 4.3% in Table 1) outperforms MC Dropout [9], Deep Ensembles [15], and Masksembles [7] (86.1 ± 5.7%, 88.5 ± 6.8%, and 89.2 ± 5.4% respectively [19]).The estimated probabilities in evidence based models are directly correlated with model confidence and therefore more interpretable. To demonstrate this, Table 1. Average(standard deviation) of accuracy (ACC), balanced accuracy (BAC) Sensitivity (SEN), Specificity (SPC), and the area under the curve (AUC) for the proposed Evidential Graph Transformer in comparison with graph transformer (GTN), graph convolution (GCN), and non-graph convolution (CNN) baselines. the probability of cancer predictions and uncertainty scores for all test samples are visualized in the left plot of Fig. 3. As seen, the higher the uncertainty score (bottom bar plot), the closer the estimated cancer probability is to 0.5 (top bar plot). This information can be provided during deployment to further augment surgical decision making for uncertain data instances. This is demonstrated in the right plot of Fig. 3, where the samples with high uncertainties are gradually disregarded. It can be seen that by not using the network prediction for up to 10% of most uncertain test data, the AUC increases to 1. Providing surgeons with not only the model decision but also a measure of model confidence will improve their intervention decisions. For example, if the model has low confidence in a prediction they can reinforce their decision by other means. The result of our graph structure ablation shows the drop of average ACC to 85.6% by randomizing the edges in the graph (p-value 0.004). Dropping overlapping nodes further decreased the ACC to 82.3% (p-value 0.001). Although the model still trained due to node aggregation, random graph structure acts as noise and affects the performance. Multi-level graphs were shown to outperform other structures for masspect data [Akbarifar 2021] as they preserve the receptive field in the neighborhood of subbands (metabolites).  Clinical Relevance: An appropriate visualization of the attention map for samples can be used to help with this exploration. Accumulating the attentions maps from the cancerous burns based on their hormone receptor status results in the representative maps demonstrated in Fig. 4. The polar bars in this figure show the attention level paid to the nodes in the associated m/z subband. It can be seen that more attention is paid to the amino acids range (100-350 m/z) in HER2 positive breast cancer in comparison to HER2 negative breast cancer, which is in accordance with previous literature that has found evidence for higher glutamine metabolism activity in HER2+ [13]. we have also found that there's more attention in this range for PR negative breast cancer in comparison PR positive, which is in concordance with previous literature demonstrating that these subtypes have higher glutamine metabolic activity [4,5]."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Intra-operative Deployment:,"The raw intra-operative iKnife data (y-axis is m/z spectral range and x-axis is the surgery timeline) along with the temporal reference labels extracted from surgeon's call-outs and pathology report are shown in Fig. 5, top. As seen, the iKnife stream contains spectra from skin cuts, which is considered as an unseen label for the ex-vivo models. The results of deploying the proposed models and baselines are presented in Fig. 5, bottom. When a spectrum is classified as cancer, a red line is overlaid on the timeline. Previous studies showed the similarity between skin and breast cancer mass spectrum that can confuse the binary models. Since our proposed EGT is equipped with uncertainty estimation, this information can be used to eliminate skin spectra from being wrongly detected as cancer. By integrating uncertainty, predictions for such burns are flagged as uncertain so clinicians can compensate for surgical decision making with other sources of information."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,4,Conclusion,"Intra-operative deployment of deep learning solutions requires a measure of interpretability as well as predictive confidence. These two factors are particularly importance to deal with heterogeneity of tissues which represented as mixed or unseen labels for the retrospective models. In this paper, we propose an Evidential Graph Transformer for margin detection in breast cancer surgery using mass spectrometry with these benefits in mind. This structure combines the attention mechanisms of graph transformer with predictive uncertainty. We demonstrate the significance of this model in different experiments. It has been shown that the proposed architecture can provide additional insight and consequently clearer interpretation of surgical margin characterization and clinical features like status of hormone receptors. In the future, we plan to work on other uncertainty estimation approaches and further investigate the graph conversion technique to be more targeted on the metabolic pathways, rather than regular conversion."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 1 .,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 2 .,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 3 .,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 4 .,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 5 .,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,1,Introduction,"Diabetic retinopathy(DR) is a common long-term complication of diabetes that can lead to impaired vision and even blindness as the disease worsen [13,14]. Hence, conducting large-scale screening for early DR is an essential step to prevent visual impairment in patients. Screening fundus images by the ophthalmologist alone is not sufficient to prevent DR on a large scale, and the diagnosis of DR heavily relies on the experience of the ophthalmologist [1]. Therefore, the automatic DR diagnosis on retinal fundus images is urgently needed [3,25]. Recently, in light of the powerful feature extraction and representation capabilities of convolutional neural networks, deep learning technology has developed rapidly in medical image analysis [5,22]. However, leveraging only the image-level grading annotation hinders deep learning algorithms from extracting features of suspicious lesion regions, which further affects the diagnosis of diseases. For these reasons, some previous work [17,19] considers the introduction of pixel-level lesion annotation to improve the model's feature extraction capability for lesion regions. Despite the methods have achieved promising results, the large-scale pixel-level annotation process is time-intensive and error-prone which imposes a heavy burden on the ophthalmologist. To address this problem, contrastive learning(CL) [10,11,23] has received a great deal of attention in medical images, but how to harness the power of CL in the medical applications remains unclear.The challenges mainly lie in: (I) The diagnosis of fundus diseases relies more on local pathological features (haemorrhages, microaneurysms, etc.) than on the global information. How can contrastive learning enable models to extract features of lesion information more effectively on the large datasets with only imagelevel annotation? (II) The false negatives tend to disrupt the feature extraction of contrastive learning [26], resulting in the issue of inaccurate alignment of feature distributions [18] (i.e. similar samples have dissimilar features). How to address the issue of false negatives caused by introducing contrastive learning into automatic disease diagnosis? (III) The performance of contrastive learning benefits from the hard negatives [2,16]. How to effectively exploit hard negatives for improving the quality of the learned feature embeddings?To address the aforementioned issues, we propose the lesion-aware CL framework for DR grading. Specifically, to eliminate false negatives during contrastive learning introduced in automatic disease diagnosis and ensure that samples having similar semantic information stay close in the joint embedding space, we first capture lesion regions in fundus images using a pre-trained lesion detector. Based on the detected regions, we construct a lesion patch set and a healthy patch set, respectively. Then, we develop an encoder and a momentum encoder [6] for extracting the features of positives (lesion patches) and negatives (healthy patches). The introduced momentum encoder enables the contrastive learning to maintain consistency in critical features while creating different perspectives for the positive samples. Secondly, considering the critical role of hard negatives in the contrastive learning, we formulate a two-stage scheme based on knowledge distillation [8,21] to dynamically exploit hard negatives, which further enhances the lesion-aware capability of the diagnosis models, and further improves the quality of the learned feature embeddings. Finally, we fine-tune the proposed framework in the DR grading task to demonstrate its effectiveness.To the best of our knowledge, this is the first work to rethink the potential issues of contrastive learning for medical image analysis. In summary, our contributions can be summarized as follows. (1) A new scheme of constructing positives and negatives is proposed to prevent false negatives from disrupting the extraction of lesion features. This design can be easily extended to other types of medical images with less prominent physiological features to achieve better lesion representation. (2) To enhance the capability of CL in extracting lesion features for medical fundus image analysis and improve the quality of learned feature embeddings, a lesion-aware CL framework is proposed for sufficiently exploiting hard negatives. (3) We evaluate our framework on the large-scale EyePACS dataset for DR grading. The experimental results indicate the proposed method leads to a performance boost over the state-of-the-art DR grading methods. Fig. 1 shows the illustration of the proposed framework. In stage 1, we construct positives and negatives based on a pre-trained lesion detector pre-trained on a auxiliary dataset (IDRiD [15]) with pixel lesion annotation, to avoid the effect of false negatives on the learned feature embeddings while aligning samples with similar semantic features. In stage 2, a dynamically sampling method is developed based on knowledge distillation to effectively exploit hard negatives and improve the quality of the learned feature embeddings. In the last stage, we finetune our model on the downstream DR grading task. Remarkably, to bridge the gap between local patches in the pretext task and global images in the downstream task, we introduce an attention mechanism on the fragmented patches to highlight the contributions of different patches on the grading results."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,2.1,Construction of Positives and Negatives,"In this section, we provide a detailed description regarding the construction of positives and negatives. As opposed to traditional CL working on the whole medical images, it is essential to enable the model to focus more on the lesion regions in the images. Our goal is to eliminate the effect of false negatives on contrastive learning for obtaining a better representation of the lesion features. Specifically, given a training dataset X with five labels (1-4 indicating the increasing severity of DR, 0 indicating healthy). We first divide dataset X into lesion subset X L and healthy subset X H based on the disease grade labels of X. Then, we apply a pre-trained detector f det (•) only on X L and obtain high-confidence detection regions. Finally, the construction process of positives P = {p 1 , p 2 , . . . , p j } and negatives N = {n 1 , n 2 , . . . , n k } can be represented as P = Ω(f det (X L ) > conf) and N = Randcrop(X H ), where conf denotes the confidence threshold of detection results, Ω(•) indicates the operation of expanding the predicted boxes of f det (•) to 128*128 for guaranteeing that the lesions are included as much as possible, and Randcrop(•) indicates randomly cropping images into patches with 128*128 from the healthy images."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,2.2,Dynamic Hard Negatives Mining Enhances Contrastive Learning,"Given the constructed positives P and negatives N , a negatives sampling scheme based on offline knowledge distillation is developed to enable contrastive learning to dynamically exploit hard negatives, and we adjust the update mechanism of the negatives queue(i.e. only enqueue and dequeue N to avoid confusion with P ) to better adapt contrastive learning to the medical image analysis task.Training the Teacher Network. With the positives P , we obtain two views P = {p 1 , p2 , p3 ...p j } and P = {p 1 , p 2 , p 3 ...p j } by data augmentation(i.e. color distortion rotation, cropping followed by resize). Correspondingly, with the negatives N , to increase the diversity of the negatives, we apply a similar data augmentation strategy to obtain the augmented negatives Ñ = {ñ 1 , ñ2 , ñ3 ...ñ k } (where k j). We feed P and P + Ñ to the encoder En(•) and the momentum encoder MoEn(•) to obtain their embeddings Z = {z 1 , ..., z j |z j = En(p j )}, Z = {z 1 , ..., z j |z j = MoEn(p j )} and Z = {z 1 , ..., zk |z k = MoEn(ñ k )}. Then, we calculate the positive and negative similarity matrix by the samples of Z, Z and Z. According to the similarity matrix, the contrastive loss L cl-t of the teacher model training process can be defined as:where sim z j , z j /z k = dot(zj ,z j /z k ) zj 2 z j /z k 2 , τ denotes a temperature parameter, A t p and A t n represent the similarity matrix of positives and negatives, respectively. In order to create a positive sample view different from that of En(•), it should be noted that the parameters θ q of En(•) are updated using gradient descent, while MoEn(•) introduces an extra momentum coefficient m = 0.99 to update its parametersTraining the Student Network. Previous works [2,16] reveal that not all negatives are useful for the contrastive learning. Moreover, the hard negatives may exhibit more semantically similar to the positives than the normal negatives, indicating that hard negatives provide more potentially useful information for facilitating the following DR grading. Meanwhile, the number of hard negatives significantly affects the difficulty of training the model, in other words, the network should be capable of dynamically adjust the optimisation process by controlling the number of hard negatives. In light of the above two points, we formulate and introduce a well-balanced strategy of hard negatives during the training phase of the student model. Specifically, based on the trained teacher model, we first input P and N into both the teacher and student models to generate similarity matrices A t p , A t n and A s p , A s n , respectively. According to the negative similarity matrix A t n produced by the teacher model, we prioritise the negatives that are likely to be confused with the positives in descending order and only select the top δ samples for distillation learning during the student model's training phase. For each negative zk in A t n , the resampled negative set A t n can be defined as:where γ = δ/(cos( πs 2S ) + 1) represents the number of the current hard negatives, s and S denotes the current and maximum training step, respectively. As s increases during the training process, we dynamically adjust the number of hard negatives such that the difficulty of distillation learning proceeds from easy to hard. Based on the index in A t n , the elements at the corresponding positions in A s n are obtained and a resampled negatives similarity matrix A s n is constructed. Hence, the CL loss L cl-s in training process of student can be formulated as:In addition, to improve the quality of embeddings learned by the student model, we leverage the generated similarity matrices to facilitate the richer knowledge distilled from the teacher to the student. Formally, the KL-divergence loss L kd between A t p , A t n and A s p , A s n is represented as follows.where C(•) denotes the matrix concatenation. The final loss of the student model is L = L cl-s + λ 1 L kd , where the λ 1 is a positive parameter controlling the weight of the knowledge distillation loss L kd ."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,2.3,DR Grading Task,"To evaluate the effectiveness of the proposed method, we take the encoder of the pre-trained student model as a backbone and fine-tune it for the downstream DR grading task. Considering that the proposed contrastive learning framework is trained with patches, whereas the downstream grading task relies on entire fundus images, an additional attention mechanism is incorporated to break the gap between the inputs of pretext and downstream tasks. Specifically, we first fragment the entire fundus image into patches x = {x p1 , . . . , x pi }. Then, feature embedding v i of x pi is generated by the encoder. Meanwhile, an attention module with two linear layers is utilized in the DR grading task to obtain the attention weight α i of each patch x pi .where W 1 , W 2 are the parameters of the two linear layers , LayerN orm is the layernorm function. Finally, α i is assigned to the corresponding patch's embedding v i to highlight the contribution of patch x pi , and the predicted results of DR obtained by ŷ = W T 3 • N i=1 α i v i , and W T 3 is parameter of the grading layer."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,3,Experiments,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,3.1,Datasets and Implementation Details,"EyePACS [4]. EyePACS is the largest public fundus dataset which contains 35,126 training images and 53,576 testing images with only image-level DR grading labels. According to the severity of DR, images are classified into five grades: 0 (normal), 1 (mild), 2 (moderate), 3 (severe), and 4 (prolifera-tive).Implementation Details. The proposed framework is implemented by Pytorch on two Tesla T4 Tensor core GPUs. We employ the IDRiD dataset [15] for the pretraining of the lesion detector f det (•). During the sample construction stage, considering the diversity of sizes of the original fundus images, all images are resized to 768 × 768, and the data enhancement strategies include random rotation, flipping and color distortion. During the phase of dynamically mining hard negatives, the Adam optimizer with momentum 0.9 is applied to train and update the parameters of the framework with 800 epochs, the initial learning rate of 1×10 -3 and the batch size of 400. The designed hyper-parameter δ is set to be 4,000 after extra experiments (please refer to the supplementary materials for more details). In the downstream DR grading task, we fine-tune the encoder(i.e. ResNet50) for 25 epochs with an initial learning rate of 1×10 -4 and a batch size of 32. In addition to the normal classification accuracy, we also introduce the quadratic weighted kappa metric to reflect the performance of the proposed method and a range of comparable methods."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,3.2,Comparison with the State-of-the-Art,"In this section, we provide qualitative and quantitative comparisons with various DR grading methods and demonstrate the effectiveness of the proposed method.As shown in Table 1, we conduct a comprehensive comparison of the proposed method with three types of comparable methods: covering the popular backbone network [7], the top two places of Kaggle challenge [4] and the current SOTA DR grading methods [9,10,12,19,20,24]. From the Table 1, it can be observed that our method consistently achieves the best results with respect to both the Kappa and Accuracy. The results show that our framework presents a notably better DR grading performance than the SOTA methods due to improve quality of the learned lesion embeddings by eliminating the false negatives and dynamically mining hard negatives, and in turn enhancing the lesion-awareness of CL, which is beneficial for DR grading. "
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,3.3,Ablation Study,"To more comprehensively evaluate the Lesion-aware CL, we conduct ablation studies to analyze the correlation among DR grading, the construction of positives and negatives(CPN) and dynamic hard negatives mining(DHM). We compare the proposed method with its several variants. The results of ablation study are reported in Table 2. We can draw conclusions from several aspects: (1) CL shows the worst performance and the performance of Lesion-aware CL w/o CPN is obviously degraded compared to Lesion-aware CL (i.e. kappa reduces 1.5% ). The results suggest that CPN is critical for improving the performance when contrastive learning is introduced in fundus images. Without false negatives disrupting the feature extraction procedure of lesions, the model is able to extract a better representation for the regions of lesions and thus achieve better DR grading performance. (2) Lesion-aware CL w/o DHM performs worse than Lesion-aware CL. As opposed to the common CL methods which uses all negative samples, our model takes into account the difference of the negatives with difficulty level. The teacher network is able to dynamically exploit the hard negatives and transfer the learned knowledge to the student, thereby improving the quality of the feature embeddings in subsequent Fig. 2. Visualization results from GradCAM between the four representative methods contrastive learning. Figure 2 shows the visualization results from GradCAM of four representative methods including Resnet50, the common CL methods (MoCo, CL-DR), and the Lesion-aware CL. Two cases with proliferate DR(DR-4) are visualized by four representative models. The intensity of the heatmap indicates the importance of each pixel in the corresponding image for making the prediction. In case 1, both Resnet and typical CL methods focus on the optic disc where has obvious physiological characteristics, while our method focuses more on the lesion regions and less on the structural aspects of the fundus image. In case 2, our method provides a promising perception of the lesion regions than other methods, suggesting that our approach allows the DR grading model to learn better representation of lesion and thus be sensitive to the DR grading."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,4,Conclusion,"In this paper, we propose a novel lesion-aware CL framework for DR grading. The proposed method first overcomes the false negatives problem by reconstructing positives and negatives. Then, to improve the quality of learned feature embeddings and enhance the awareness for lesion regions, we design the dynamic hard negatives mining scheme based on knowledge distillation. The experimental results demonstrate that the proposed framework significantly improves the latest results of DR grading on the benchmark dataset. Furthermore, our approaches are migratable and can be easily applied to other medical image analysis tasks."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,,Fig. 1 .,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,,,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,,Table 1 .,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,,Table 2 .,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,1,Introduction,"Atrial fibrillation (AF) is a cardiac disease characterized by rapid, irregular heartbeats [4]. The disease can lead to stroke and heart failure, and has a mortal-ity rate of almost 20% [5,10,13]. AF is classified as either persistent atrial fibrillation (PeAF), where abnormal heart rhythms occur continuously for more than seven days, or paroxysmal atrial fibrillation (PaAF), where the heart rhythm returns to normal within seven days. Although AF can be treated through a procedure called catheter ablation, PeAF cases have high recurrence rates and often require re-intervention [8]. Accurate knowledge of the disease type is therefore highly valuable for treatment planning and has high prognostic value [22].Clinical studies have discovered a strong relationship between AF and epicardial adipose tissue (EAT), a fat depot layer on the surface of the myocardium that can cause inflammation and disrupt cardiac function [3,15]. Recent works have shown that automatic classification of AF sub-types can be done using CT volumes of the left atrium and surrounding EAT, which can be used to screen for patients with high risk of PeAF. Huber et al. [7] showed that EAT volume, approximated from left-atrium CT images, can be used as a predictor for AF recurrence. Yang et al. [22] trained a random forest model to classify AF subtype based on radiomic features and volume measurements, achieving 85.3% AUC. Although these methods demonstrate the usefulness of radiomic features for AF sub-type classification, such features are generic and not specific to the task, which can limit model performance [12]. Radiomic features also rely on summary statistics such as entropy or homogeneity to obtain global descriptors, and these have limited effectiveness when capturing local feature variations [16].Deep learning has achieved outstanding results on medical imaging analysis tasks, largely due to its ability to learn task-specific features and complex relations between them [17]. Naïvely using deep neural networks (DNNs) to predict AF sub-types from CT volumes yields poor results however due to over-fitting on high-dimensional volume inputs (see results for DNN in Table 1). Existing works have attempted to combine deep and radiomic features through methods such as direct concatenation [2,19], attention modules [14], or contrastive learning between feature types [24]. Although these methods propose different ways of using both approaches, they do not explicitly address the limitations of either approach or explore ways to combine their complementary advantages.In this work, we propose a novel approach to atrial fibrillation sub-type classification from CT volumes by integrating radiomic and deep learning methods. We note that textural radiomic features identified by feature selection methods can serve as an information prior to supplement low-level features from DNNs, since they are designed to capture low-level context and have predictive power [23]. To this end, we locally calculate radiomic features based on patches surrounding each voxel, and perform feature fusion with low-level DNN features. This provides the DNN with pre-defined features known to be relevant to the task to reduce over-fitting, and also allows spatial relations between radiomic features to be learned. Furthermore, we encourage the DNN to learn features complementary to radiomic features to obtain more comprehensive signals and design a novel feature de-correlation loss. The overall framework, which we term Radiomics-Informed Deep Learning (RIDL), is illustrated in Fig. 1. Unlike existing works, our method is designed to directly addresses the limitations of both deep learning and radiomic approaches and achieves state-of-the-art performance on AF sub-type classification. To summarize our key contributions: -We propose a novel radiomics-informed deep learning (RIDL) method for AF sub-type classification from CT volumes, which achieves state-of-the-art results and can be used to screen for patients with high risk of PeAF. -Our method uses a novel approach of fusing locally computed radiomic features with low-level DNN features to improve capturing of local context. -Furthermore, we enforce feature de-correlation using a novel feature-bank design to ensure complementary deep and radiomic features are extracted."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,2,Methodology,"We combine radiomic and deep learning approaches using two novel components: 1) feature fusion of local radiomic features and low-level DNN features to improve local context, 2) encouraging complementary deep and radiomic features through feature de-correlation. These are illustrated in Fig. 2 and explained in detail below. Our datasetincludes N samples of input x i and binary label y i , where 0 indicates PaAF and 1 indicates PeAF. x i has two channels, one consisting of the 3D CT volume centered around the left atrium and the other the binary region-of-interest (ROI) mask indicating EAT. The ROI is obtained through Hounsfield value thresholding such that all voxels valued between -250 and 0 are identified as EAT [7,22]."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,2.1,Feature Fusion of Locally Computed Radiomic Features with Low-Level DNN Features for Improved Local Context,"Under the radiomics pipeline, a large set of features, typically more than a thousand, is first extracted by performing calculations over the volume and ROI input x i . Feature selection methodologies such as mutual information (MI), principal component analysis (PCA), or LASSO regularization, are then used to identify predictive features for classification [23]. Radiomic features are classified into shape, first-order statistical features, and texture features. Texture features are designed to capture local variations and use measures such as Gray-level Co-occurrence Matrices (GLCM) to reflect second-order textural distributions. Conventional statistics such as entropy and correlation are then used to summarize these measures [25], but these tend to be limited in their ability to capture local heterogeneity, such as the varying textures on the surface of a cancer tumor. Although DNN's are more effective at capturing local variations, they can overfit without sufficient data for training [17]. Unlike existing works that naïvely concatenate radiomic and deep features before the classification layer [2,19], we observe that textural features selected through radiomics feature selection algorithms are known to be predictive and can be used as prior knowledge to improve low-level DNN features. Given radiomic feature extractor F r , the global radiomic feature, r g i ∈ R, for input x i is represented by:Our method applies feature calculations locally to cubic patches centered around each voxel, such that features are obtained on a voxel basis and reflect the statistics of the neighbouring region. For a cubic patch with radius p and input x i , the local feature at location (h, w, d), denoted by r p i,(h,w,d) , is obtained by performing R on the cubic patch in x i centered around (h, w, d):where the input of F r is the cubic sub-volume. This process is illustrated in Fig. 2a. Local features can be calculated for multiple texture features and patch size p, which are then concatenated to obtain r l i ∈ R L×H×W ×D , where L is the total number of features used and H, W , and D are original input dimensions. We note that only texture radiomic features are used for local calculation since they are specifically intended to capture local context. r l i is then concatenated with low-level DNN features, z i ∈ R C×H×W ×D , to supplement the DNN with local radiomic features. To effectively fuse the features, we apply a channel attention module, A, following the design in [20]:where z i is the fused feature, ⊕ is channel concatenation, and ⊗ is element-wise multiplication. The learned attention tensor A(r l i ⊕ z i ) has dimensions (C + L) × 1 × 1 × 1 and is broadcasted along the volume dimension, such that attention is applied channel-wise and spatial feature distributions are preserved."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,2.2,Encouraging Complementary Deep and Radiomic Features Through Feature De-Correlation,"Global radiomic features are also included in our model by concatenation with high-level DNN features before the classification layer. Unlike existing approaches however, we encourage our DNN to learn features complementary to radiomic features by enforcing de-correlation between the two. This ensures that different variations are captured, which provides a more comprehensive signal to the classification layer. Accurate approximation of correlation requires large batches sizes however, which requires large GPU memory and can affect model convergence [9]. We instead propose a novel feature-bank implementation with exponential weighting to estimate sample statistics. Every iteration, we save DNN and global radiomic features in feature-bank K, which holds up to N k features in a first-in first-out queue. After a warm-up period, we calculate the sample correlation using an exponential weighting scheme. Given weight parameter w < 1, and the normalized deep feature Z i and radiomic feature R i from K, we calculate feature de-correlation loss L corr as:The first B samples, where B is the batch size, belong to the training sample of the current iteration, and their losses are back-propagated to encourage deep features to have zero correlation with radiomic features. This process is illustrated in Fig. 2b. Although feature banks have been used in techniques such as contrastive learning to address batch size limitations [6,21], we are the first to formulate this technique for feature de-correlation."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,2.3,Overall Framework,"The DNN model uses raw CT volumes concatenated with ROI masks as input.Global and local radiomic features are pre-computed for input into the feature layer. Binary cross-entropy is used for AF sub-type classification loss L cls :where ŷi is the model prediction for sample y i . The model is trained together with feature de-correlation loss L corr and its loss weighting, w corr . To provide further regularization and prevent over-fitting, we perform an additional selfreconstruction task, using loss L rec , which we describe in more detail in the supplementary materials. The overall loss function is then:3 Experiments"
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,3.1,Implementation Details,"Dataset. We use a dataset of 172 patients containing 94 PaAF and 78 PeAF cases collected from the Sun Yat-Sen Memorial Hospital in China. CT volumes are centered on the left atrium and normalized to between -1 and 1. ROI masks for EAT are obtained through Hounsfield value thresholding between -250 and 0. Volumes are resized to the same aspect ratio to ensure consistent dimensions across samples. We use an input size of 96 × 128 × 128 voxels and apply zero padding for smaller volumes. We use five-fold cross-validation and report average test performance across folds. Cross-validation is implemented by splitting the dataset into five equal subsets and using three subsets for training, one subset for validation, and one subset for testing. A rolling scheme is used such that different validation and test subsets are used for each of the five folds. Data acquisition procedures and statistics are given in the supplementary materials.Setup. We use the PyRadiomic package [18] to extract radiomic features from the input volumes and masks. Using the cross-validation splits, we perform feature selection and classification using LASSO regularized logistic regression. LASSO regularization consistently selects four radiomic features as the ones with the most significant predictive power: maximum 3D diameter, Maximum 2D Diameter, Maximum voxel value, and normalized inverse difference of GLCM (glcm Idn). The texture feature glcm Idn is calculated locally for p ∈ {1, 2, 5, 10} to obtain local radiomic features r l i ∈ R 4×96×128×128 . For our DNN network, we use a modified 3D U-Net [1] (abbreviated as m3DUNet) with skip connections between the encoder and decoder removed to enhance bottle-neck feature compression. Bottle-neck features are averaged across spatial dimensions for classification, whilst decoder outputs are used for self-reconstruction regularization. The model is trained using the Adam optimizer with learning rate 10 -4 for 100 epochs and 0.1 decay at 30 epochs. We use batch size B = 1, feature bank size N k = 25, and warm-up period of one epoch. We use w corr = 2 for de-correlation loss weighting, which was chosen based on the validation splits. Mean and standard deviation of ten runs are reported. Additional experiments and details are included in the supplementary materials."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,3.2,Comparison with State-of-the-Art Methodologies,"We compare our method with alternative state-of-the-art approaches based on radiomics, deep learning, and hybrid techniques. Deep and hybrid volume-based classification methods [11,14,24] are adapted to our task since there are no existing works for AF sub-type classification. We use the same encoder for all deep architectures for fair comparison, except for methods that are architecture specific. A naïve feature concatenation method is used as our baseline for the hybrid approach. Radiomic features for the hybrid approach are selected through LASSO regularization as it is the most effective. Results are shown in Table 1.Table 1. Comparison with state-of-the-art methods for radiomic, deep learning, and hybrid approaches. Selector * refers to the feature selection method. Hybrid # methods use radiomic features selected by LASSO regularization, which is the most effective. DNN is a naïve implementation using the m3DUNet model. Baseline † is a naïve hybrid implementation using simple feature concatenation."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Type,"Selector We can see that hybrid methods outperform radiomic and deep methods in general. Our method, RIDL, achieves the best results across all metrics however and improves AUC by 1.1% over the baseline method (86.9% v.s. 85.8%) and 3.5% over the best radiomics approach (86.9% v.s. 83.4%)."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,3.3,Ablation,"Component Analysis. We perform ablation experiments to demonstrate improvements from using local radiomic features, global radiomic features, and feature de-correlation loss. Results are shown in Table 2.We can see that including local radiomic features, improving AUC by up to 1.6% when included with a standard DNN (78.8% v.s. 77.2%). Using feature de-correlation further boosts performance and leads to the best overall results. Effectiveness of Radiomic Feature Selection. To demonstrate the effectiveness of radiomic feature selection as prior knowledge for feature fusion, we compare with results from using features discarded by radiomics feature selection. We randomly select three discarded features to generate local features r l i as input whilst keeping other components constant. Results are shown in Table 3.Table 3. Results using different texture radiomic features for input r l i as displayed by their PyRadiomics key [18]. ""Selected"" indicates whether the feature was selected or discarded by the radiomic feature selection algorithm. We can see that using discarded features leads to worse performance in general. Given the large set of radiomic features, it is possible some discarded features may outperform selected features due to differences in global and local computation. Nevertheless, our results indicate that the radiomic feature selection process serves as an reasonable information prior. Our work is the first to propose fusing locally computed radiomic features with low-level DNN features, and we leave detailed local feature selection methods to future works."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,4,Conclusion,"In this work, we propose a new approach to atrial fibrillation sub-type classification from CT volumes by integrating radiomic and deep learning approaches through a radiomics-informed deep learning method, RIDL. Our method is based on two key ideas: feature fusion of locally computed radiomic features with lowlevel DNN features to improve local context, and encouraging complementary deep and radiomic features through feature de-correlation. Unlike existing hybrid approaches, our method specifically addresses the advantages and limitations of both techniques to improve feature extraction. We achieve state-of-the-art results on AF sub-type classification and outperform existing radiomic, deep learning, and hybrid methods.Future improvements to RIDL can be made by introducing more sophisticated local radiomic features selection methods, given the large set features to choose from. Experiments on larger datasets or alternative tasks can also be done to provide more empirical support, since current results show only slight improvements over baseline. These issues may be addressed in future works. Overall, our method is a novel way of combining radiomic and deep learning approaches, and can be used to improve accuracy of PeAF screening from CT volumes for better preventive care of high-risk patients."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Fig. 1 .,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Fig. 2 .,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,9 ± 0.6 86.3 ± 0.6 74.7 ± 1.5 76.9 ± 1.0,*   
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Table 2 .,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 15.
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,1,Introduction,"Magnetic resonance imaging (MRI) has become an increasingly important tool to investigate prenatal equivocal neurological situations, as it provides excellent anatomical details [1,2]. However, three-dimensional (3D) high-resolution (HR) imaging of the fetal brain is unfeasible due to the unpredictable fetal motion. In clinical practice, T2-weighted (T2w) fast spin echo (FSE) sequences are commonly used to minimize the effects of intra-slice random fetal movements, and multiple orthogonal series are acquired, resulting in several low-resolution (LR) series of two-dimensional (2D) thick slices [3,4]. Nevertheless, the strong anisotropy of the images leading to partial volume effects on small structures within the fetal brain and the remaining inter-slice motion hamper the accurate analysis of 3D imaging biomarkers.Several post-processing techniques have been proposed to combine multiple motion-corrupted LR series and leverage the information redundancy from orthogonal orientations to reconstruct a single, 3D isotropic HR motion-free volume of the fetal brain [5][6][7][8]. These approaches all feature several pre-processing steps (e.g. brain extraction, intensity correction, and harmonization) leading to slice-to-volume registration (SVR) where inter-series inter-slice motion is estimated, followed by super-resolution reconstruction (SRR). This latter step can be framed as an inverse problem of the form minHxx LR 2 + αR(x), (1) where x is the target HR image, x LR the LR series, H an operator describing the motion, blurring and downsampling model estimated from the data, and R the regularization function (e.g., total-variation (TV) [6], first-order Tikhonov [7], etc.). α is a parameter that balances the strength of the regularization term compared to the data fidelity term. Various applications in medical image computing are formulated as inverse problems, and the optimization of regularization parameters has been widely studied in this context [9,10]. Most strategies explicitly rely on reference data, which are not available in the context of fetal MRI, making the setting of the regularization parameter α in a principled and quantitative manner highly challenging. To circumvent the lack of HR data of the fetal brain, several works use HR MR images from newborns as ground truth data and downsample them to simulate the acquisition of LR series that are then reconstructed and compared to the HR image to set the default value of their regularization parameters [5,6]. Alternative approaches consider a leave-one-out approach where the left-out LR series serves as a reference for the quantitative evaluation of the SRR [5,6], or use a volume reconstructed from all available LR series as a reference to which SRR with fewer LR series can be compared [7]. However, all of these works rely on constructing surrogate ground truth images to study the influence of the regularization parameter on the quality of the SRR but do not provide insights on how to adapt it when new input acquisition setting has to be reconstructed. Furthermore, despite well-known differences in image acquisitions protocols, fetal brain SRR MRI studies are still carried out using the default regularization values of the selected pipeline [6][7][8]11].This work proposes the first approach to optimize the setting of the regularization parameter α based on numerical simulations of imaging sequences tailored to clinical ones. We take advantage of a recent Fetal Brain MR Acquisition Numerical phantom (FaBiAN) [12] that provides a controlled environment to simulate the MR acquisition process of FSE sequences, and thus generates realistic T2w LR MR images of the fetal brain as well as corresponding HR volumes that serve as a reference to optimize the parameter α in a data-driven manner, considering both acquisition setting-specific and subject-specific strategies.Our contributions are twofold. First, using synthetic, yet realistic data, we study the sensitivity of the regularization to three common variables in inverse SRR problem in fetal MRI: (i) the number of LR series used as input, (ii) the magnetic field strength which impacts also the in-plane through-plane spatial resolution ratio, and (iii) the gestational age (GA), which leads to substantial changes in brain anatomy. Secondly, we qualitatively illustrate the practical value of our framework, by translating our approach to clinical MR exams. We show that α * estimated by our simulated framework echoes a substantial improvement of image quality in the clinical SRR. To generalize the validity of our findings, we perform our study using two stateof-the-art SRR pipelines, namely MIALSRTK [6] and NiftyMIC [7]."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2,Materials and Methods,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2.1,Simulated Acquisitions,"We use FaBiAN [12,13] to generate T2w MR images of the developing fetal brain derived from a normative spatiotemporal MRI atlas (STA) [14] that features 18 subjects from 21 to 38 weeks of GA. ypical FSE acquisitions are simulated using the extended phase graph (EPG) formalism [15], at either 1.5T or 3T, according to the MR protocol routinely performed at our local hospital for fetal brain examination. All sequence parameters are kept fixed at a given magnetic field strength (at 1.5/3T : TR, 1200/1100ms; TE, 90/101ms; voxel size, 1.1 × 1.1 × 3/0.5 × 0.5 × 3 mm 3 ). Stochastic 3D rigid motion of little-to-moderate amplitude as well as random complex Gaussian noise (mean, 0; standard deviation, 0.15 at 1.5T, respectively 0.0025 at 3T) are applied during k-space sampling to simulate as closely as possible the MR acquisition process. Both realistic fetal movements and noise levels are qualitatively estimated from clinical LR series and set accordingly to match the characteristics of real scans [12,16,17]. More specifically, a maximum of 5% motion-corrupted slices is generated over the whole fetal brain volume with independent translation within a uniform distribution of [-1,1] mm in every direction and 3D rotation within [-2,2]°for little motion, respectively [-3,3] mm and [-5,5]°for moderate motion, to reproduce typical motion patterns. Multiple orthogonal LR series x LR = {x LR,i } i are simulated with a shift of the field-of-view of 1.6mm in the slice thickness direction for series in the same orientation. The amplitude of fetal motion and the number of simulated LR series are further detailed in the experimental settings (Sects. 2.3 and 2.4). A visual comparison between clinical LR series and the corresponding simulated data is available at Fig. 8 in the Supplementary material. A reference HR isotropic volume x HR of the fetal brain is also simulated for each subject, without bias field or motion, to serve as a reference for the quantitative evaluation of the corresponding SRR."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2.2,Super-Resolution Reconstruction Methods,"Two widely adopted reconstruction pipelines, MIALSRTK [6] and NiftyMIC [7], are used to reconstruct 3D isotropic HR images of the fetal brain from orthogonal LR series. For each pipeline, we perform a grid search approach of the regularization parameter space.Remark. Contrary to NiftyMIC [7], MIALSRTK [6] places its regularization parameter λ on the data fidelity term. For the sake of consistency, we will only use the formulation of Eq. 1, with α = 1/λ in the case of MIALSRTK.Quality Assessment. Solving Problem 1 yields a SR-reconstructed image xHR whose quality can be compared against the reference x HR using various metrics. We use two common metrics for SRR assessment [5][6][7], namely the peak signalto-noise ratio (PSNR) and the structural similarity index (SSIM) [18]. The best regularization parameter α is identified as the one maximizing a given performance metric."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2.3,Experiment 1 -Controlled in Silico Environment,"In this first experiment, we study the sensitivity of the parameter α to common variations in the acquisition pipeline.Dataset. For every STA subject, nine LR series (three per anatomical orientation) are simulated at 1.5T and 3T with little amplitude of stochastic 3D rigid motion.Experimental Setting. We define four configurations based on the number of LR series given as input to the SRR pipeline (three or six series) and the magnetic field strength (1.5 or 3T). Note that the inter-magnetic field difference is especially captured in the image resolution, with a through-plane/in-plane ratio of 3.3/1.1 = 3 at 1.5T and 3.3/0.5 = 6.6 at 3T. In each configuration, individual brains are repeatedly reconstructed (n = 3) from a selection of different LR series among the nine series available per subject.The grid of parameters searched for NiftyMIC consists of 10 values geometrically spaced between 10 -3 and 2, plus the default parameter α def = 0.01. For MIALSRTK, we use α ∈ {1/0.75, 1/1.0, 1/1.5, 1/2.0, 1/2.5, 1/3.0, 1/3.5, 1/5.0} (8 values, with default parameter α def = 1/0.75). At the end of the experiment, the best parameter, for either of the pipelines, is referred to as α * 1 .Statistical Analysis. The optimal regularization parameters evaluated for the different SRR configurations are compared using the Wilcoxon rank sum test.The difference between the metrics performance obtained with default or optimal parameters is tested with a paired Wilcoxon rank sum test. The p-value for statistical significance is set to 0.05."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2.4,Experiment 2 -Clinical Environment,"Clinical MR fetal exams are prone to substantial inter-subject variation and heterogeneity. In particular, the number of LR series available for reconstruction, as well as the amplitude of fetal motion, greatly vary from one subject to the other [19]. Therefore, this second experiment has two purposes. First, we translate our findings from the first experiment to clinical data using the best value α * 1 . Secondly, we study an alternative approach to perform a tailored subjectwise regularization tuning by simulating synthetic data for each subject that mimic the clinical acquisitions available. We refer to the obtained value as α * 2 .Dataset. Twenty fetal brain MR exams conducted upon medical indication were retrospectively collected from our institution. All brains were finally considered normal. Fetuses were aged between 21 and 34 weeks of GA (mean ± standard deviation (sd): 29.7 ± 3.6) at scan time. For each subject, at least three orthogonal series were acquired at 1.5T (voxel size: 1.125 × 1.125 × 3 mm 3 ). After inspection, four to nine series (mean ± sd: 6.3 ± 1.5) were considered exploitable for SRR. The local ethics committee approved the retrospective collection and analysis of MRI data and the prospective studies for the collection and analysis of the MRI data in presence of a signed form of either general or specific consent.The same 20 subjects are simulated using exam-specific parameters to mimic as closely as possible the corresponding clinical acquisitions. In particular, we match the number and the orientation of the LR series, as well as the amplitude of fetal motion (from little to moderate), and the GA of each subject.Experimental Setting. We consider the same regularization parameter space as in Experiment 1 (Sect. 2.3), and evaluate both clinical and simulated data on this parameter grid."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Statistical Analysis.,"We compare the similarity between the images reconstructed by MIALSRTK and NiftyMIC using both default and optimized parameters. In this experiment, no reference images are available. Statistical significance of the performance difference is tested using a paired Wilcoxon rank sum test (p < 0.05 for statistical significance)."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,3,Results,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,3.1,Experiment 1 -Controlled in Silico Environment,"Optimal Regularization Parameter. Figure 1 shows the optimal regularization parameters α * 1 of SRR by MIALSRTK and NiftyMIC for each configuration. Table 1. Mean metrics computed across all subjects on images reconstructed using the default regularization parameter α def or the optimal parameter α * 1 respectively, compared to the simulated reference HR volume, for the four configurations studied. † indicates paired Wilcoxon rank sum test statistical significance (p < 0.05). Regardless of the magnetic field strength and the number of LR series used for reconstruction, the optimal regularization parameters that maximize the PSNR and SSIM compared to a synthetic HR volume greatly differ from the default values. For MIALSRTK, α def = 1/0.75, while the optimal range is found between 1/2.25 and 1/4.5. For NiftyMIC, α def = 0.01, whereas the optimal range is found between 0.015 and 0.15. We observe that for both the PSNR and the SSIM, the optimal regularization weight increases with the number of series used in the reconstruction, and decreases with the resolution. This is because changing the number of LR series or the magnetic field strength affects the magnitude of the data fidelity term with respect to the regularization term. When more series are combined in the SRR, a larger regularization parameter must be used to keep the ratio Hxx LR 2 /αR(x) constant.   Quality Improvement. The corresponding mean PSNR and SSIM values, computed across all subjects both with default and optimal regularization parameters, and compared to the reference HR volume are displayed in Table 1. Overall, the quality metrics obtained from the SRR with optimal parameters are significantly improved compared to those obtained with default values. These results strongly suggest that the regularization parameters are highly  sub-optimally set for both SRR pipelines. This is further illustrated on Fig. 2, where we reconstruct simulated data and compare them to the corresponding ground truth image: using a default α, MIALSRTK tends to overly smooth the image, while NiftyMIC reconstructs images that are artificially sharp, enhancing edges beyond what is present on the reference image."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,MIALSRTK,"Gestational Age-Based Analysis. Since the human brain undergoes drastic morphological changes throughout gestation [20], one could expect to adjust α to GA. However, our experiments (cf. Supp. Figure 7) suggest that the α * does not depend on GA, and is in line with the values reported on Fig. 1."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,3.2,Experiment 2 -Clinical Environment,"In this experiment, we compare two differently optimized regularization parameters. First, we use the optimal value α * 1 (from Fig. 1 at 1.5T, and rounded to the closest value on the grid of parameters). Second, we use the optimized regularization parameter α * 2 estimated from the subject-specific simulation. Figure 3 shows the SRR of two subjects with default and optimal parameters using both pipelines. We observe that using the optimal parameters α * 1 and α * 2 makes the reconstructed images more similar. Indeed, the default parameters of MIALSRTK and NiftyMIC promote opposite behaviors, towards smoother (i.e., the default regularization is higher than the optimal one), respectively noisier (the default regularization is lower than the optimal one) images. We quantitatively confirm the similarity of the optimized SRR by computing the PSNR and the SSIM between the reconstructed images from both methods for α def , α * 1 and α * 2 . The results are shown in Fig. 5. The difference between the default and optimized parameters is statistically significant for both metrics. There is however no significant difference between the images reconstructed using the parameters optimized setting-wise (α * 1 ) and subject-wise (α * 2 ). As shown in Fig. 6, the parameters α * 2 optimized based on subject-specific simulations always lie within the range of optimal parameters α * 1 determined in Exp. 1. Beyond more similar images, optimizing the regularization parameter can also matter in terms of the structures that will be visible on the image. On Fig. 4, using the optimal α * 1 allows to delineate the deep gray matter more clearly compared to the α def ."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,4,Discussion and Conclusion,"In this paper, we propose a novel simulation-based approach that addresses the need for automated, quantitative optimization of the regularization hyperparameter in ill-conditioned inverse problems, with a case study in the context of SRR fetal brain MRI. Our estimated regularization weight shows both qualitative and quantitative improvements over widely adopted default parameters. Our results also suggest that subject-specific parameter tuning -which is computationally expensive to run -might not be necessary, but that an acquisition setting-specific tuning, ran only once, might be sufficient in practice.As such, the proposed methodology demonstrates a high practical value in a clinical setting where fetal MR protocols are not standardized, leading to heterogeneous acquisition schemes across centers and scanners. Besides, we show that our simulation-based optimization approach reduces the variability in image quality and appearance between the two SRR pipelines studied. We expect this behavior to contribute to mitigating the domain shift currently inherent to any reconstruction technique, a key challenge in the development of automated tissue segmentation methods [21,22]. Future work will address some limitations of this study. Indeed, we mostly focused on the influence of the main magnetic field strength and the number of available LR series, but the signal-to-noise ratio within LR series may also affect the regularization setting [12]. This aspect could also be tuned within the proposed MR acquisition simulation framework. Moreover, clinical assessment by radiologists of the different SRR would be important to further validate our method. Such an evaluation would allow to compare our approach to other techniques for parameter tuning, which cannot be done quantitatively due to the lack of HR ground truth data."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 1 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 2 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 3 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 4 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 5 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 6 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 32.
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,1,Introduction,"Estimation of patient-specific hemodynamic features in coronary arteries is an essential step in providing personalized and accurate diagnosis and treatment of CAD which is one of the main causes of death in the world [15]. In the assessment of CAD, one of the most important biomarkers is a Fractional Flow Reserve (FFR [3]), measured during an invasive coronary angiography procedure. With developments in CT and CFD, non-invasive approaches for accurate hemodynamic features estimation such as vFFR in patients with suspected ischemic heart disease became possible [9]. Unfortunately, although CFD offers a compelling in-silico replacement for invasive coronagraphy procedures, prolonged vFFR computation times, over many hours, have been a major concern [14].To solve the computation time drawback, AI-based solutions have been proposed to lower the estimation time of vFFR down to minutes, or even seconds at the cost of accuracy [1,22]. Itu et al. [8] propose to compute a set of local and global hand-crafted features from the coronary anatomy to create a representation of a local stenotic segment. Based on this representation, a multilayer perceptron (MLP) model is trained to perform an FFR regression in the stenotic areas. They experiment on a dataset of 12, 000 synthetically generated coronary geometries and evaluated the model on 125 patient-specific anatomical models extracted from CTA, reporting a correlation of 0.729 with invasive FFR on real anatomical models. Wang et al. [24] propose a similar method that utilizes hand-crafted features to feed a recurrent neural network (RNN) to estimate the FFR along the coronary artery. Authors experiment with training on 71 patient-specific anatomies extracted from CTA and report a correlation of 0.686 with invasive FFR measurement. Both approaches report compiling results of the vFFR estimation and showcase a drastic improvement in time consumption: 2.4 s for Itu et al. [8] and 120 s for Wang et al. [24]. The main drawback in both cases is the utilization of hand-crafted features. Additionally, authors do not share details of their features making their work impossible to reproduce [20]. A natural improvement over these methods would be an implicit feature learning method that could be performed on a vessel surface modelled as a mesh or a point cloud. However, there is no such method known to the authors that would tackle the problem of vFFR estimation for coronary arteries from CTA. For a similar task, Li et al. [11] propose a PointNet-like [17] architecture to predict velocity and pressure fields on point cloud geometries containing aorta, coronary arteries and bypass graft. Similarly, Suk et al. [21] utilizes mesh-based neural networks [5,6,23] to estimate wall shear stress along the vessels. Both approaches would be unsuitable for the vFFR estimation from CTA, as it would require the processing of more complex, elongated geometries.In this work, we tackle the problem of estimating hemodynamic features such as pressure drops and vFFR along the coronary arteries extracted from CTA. We propose a novel CenterlinePointNet++ architecture that is tailored towards the processing of complex, elongated structures such as coronary arteries, that can be represented as a surface point cloud and a centerline graph along branches. In our approach, implicit feature extraction is guided by the proposed centerline grouping aggregation. It is a replacement for the commonly used topology-agnostic [18], centerline-agnostic [7,19] or connectivity-based [25] aggregation strategies. To our best knowledge, this is the first method that is tackling the problem of pressure drops and vFFR estimation utilizing a point cloud neural network. We train and evaluate our approach on different scenarios on the dataset of 1, 700 synthetically generated coronary arteries. We test the model capability for two tasks: estimation of pressure drop and vFFR under different GT CFD settings. For each setting, we report an improvement over Euclidean distance based grouping in terms of Mean Absolute Error (MAE) and Normalized Absolute Error (NMAE). We further test the approach performance of vFFR estimation and report a correlation with CFD vFFR of 0.93."
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,2,Method,"In this section, we provide a detailed description of the proposed CenterlinePoint-Net++ architecture (see Fig. 1) for the problem of estimating hemodynamic features. We improve upon the well-known PointNet++ architecture [18] by taking into account a priori knowledge of coronary arteries geometries. We extend its input to accept an additional channel with a centerline graph and change its encoder blocks to the proposed Centerline-Set-Abstraction (CSA) blocks that utilize the centerline graph and the centerline grouping method which facilitates geodesic metric. We refer to the input vessel mesh as M and the point cloud representing its surface that is sampled by extracting all its vertices is denoted as P. The centerline graph, which can be extracted from the mesh with an algorithm of choice is denoted as C = (V C , E C ), where V C is the set of nodes and E C a set of undirected edges. Since the centerline graph is a 3D structure in Euclidean space, we can represent each v ∈ V C as a point with 3D coordinates. The architecture takes as an input a point cloud P and centerline graph C and returns per-point hemodynamic features of choice. The details of the method are described in the following sections."
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,2.1,Encoder,"The encoder is built out of the proposed CSA blocks in a hierarchical manner as it is shown in Fig. 1. It consists of n number of CSA blocks down to the bottleneck. Each CSA i+1 takes as an input a point cloudfrom the previous block and a centerline graph C, where N stands for the number of points, F stands for the number of features, i + 1 is the index of the current CSA block, i is the index of the previous block, and (e) stands for ""encoding"". The CSA block is described in Fig. 2a. In the first step of a CSA block, a representative sampling procedure in form of Farthest Point Sampling [4] (FPS) is utilized to downsample P (e) i and create a representative point cloud R ⊆ P.For each point r ∈ R, a centerline grouping procedure g(r, C) is performed to extract their point neighborhoods G r ⊆ P. In the last step, all extracted neighbourhoods are processed independently with a PointNet [17] to construct neighbourhood feature vectors for all the points in R.Centerline Grouping: We propose a novel centerline-guided point grouping scheme for point cloud structures. Commonly used Euclidean distance based grouping strategies do not take into account an underlying surface manifold formed by the points. Thus they tend to fail when the point cloud topology is complex, by considering points in the close Euclidean distance to be neighbours while their distance along the formed manifold is much larger (see Fig. 3a). By utilizing a centerline-guided point grouping, the geodesic metric is facilitated, and thus the point neighbourhoods are constructed along the manifold (see Fig. 3b). Additionally, grouping along the centerline allows for a robust sequential embedding of the complex vessel trees and thus facilitates the learning of hemodynamic features.For a given point cloud P, its representation R and its centerline C, the centerline grouping procedure g(r, C) is performed independently for each r ∈ R. We define a mapping function M : P → V C , which assigns a closest, in Euclidean distance sense, centerline node v ∈ V C to each point p ∈ P. Since a representative point cloud R is a subset of point cloud P, the mapping is also defined for each point r ∈ R. Due to the utilization of a mapping function M , the grouping procedure can be performed directly on the centerline graph C where the topological structure of the vessel tree can be more accurately represented. Based on the 3D coordinates of centerline nodes V C , the weights of the edges E C are calculated as distances between the connected nodes -since the centerline graph is not-mutable during training, the weights can be pre-computed in the pre-processing stage.Having a mapping function M we can directly work on the centerline graph itself and thus we define a centerline neighbourhood Q v which is computed independently for each v ∈ M [R] and can be expressed with the following equation:where d C is a centerline distance function which returns the length of the weighted shortest path between two given nodes in the centerline C and t is the distance threshold which marks whether nodes should be considered neighbours or not. Having centerline neighbourhoods Q extracted we need to map them back onto the point cloud P to obtain the point neighbourhoods G for each r ∈ R:"
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,2.2,Decoder,"The decoder is built out of the Feature-Propagation (FP) blocks in a similar manner as in PointNet++ [18]. Its architecture is shown in Fig. 2b The FP takes as an input a point cloud P (d) i from the previous block and a reference point cloud P (e) i+1 passed via skip-connection from the respective encoder block, where i + 1 is the index of the current FP block, i is the index of the previous block and (d) stands for ""decoding"". In the first step, the features from the point cloud P "
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,3,Experiments,"In this section, we describe our evaluation of the proposed model architecture in the tasks of pressure drops and vFFR estimation of a synthetic coronary artery geometry with respect to different stenosis severity grades and different biologically relevant ranges of blood flow characteristics. We test our architecture against standard PointNet++ grouping with the same number of adequate layers as a reference due to the impossibility of reproduction [20] of the other two known relevant methods [8,24]. We train CenterlinePointNet++ and PointNet++ with the same set of hyperparameters. Both need approximately 15 s for inference on the RTX 3090 24 GB graphic card. The models are trained for 500 epochs with the batch size of 8 and Adam optimizer with the default constant learning rate of 0.001. For the loss function we use the Mean Squared Error (MSE) which is the mean of per point squared errors."
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,3.1,Dataset,"We utilize a dataset of 1, 700 synthetically generated coronary arteries to train and evaluate the model's performance. We split the data to train, validation and test set of sizes 1,500, 100 and 100, respectively.We generate synthetic coronary arteries using ranges of geometric quantities such as the radii of branches, bifurcation angles, and degree of tapering with distributions similar to other studies [8,13]. On top of that, we model stenotic areas based on Coronary Artery Disease -Reporting and Data System (CAD-RADS) [16] stenoses percentage intervals. Our point clouds are further preprocessed such that each point is described by the position in the 3D space, Euclidean distance to the centerline and geodesic distance to the inlet (first point on the centerline).For the ground-truth (GT) labels generation we use a commercial CFD engine of choice [10] designed for vFFR calculation.The considered simulations for GT are stationary and take up to two hours on a CPU with 16 processes per synthetic coronary artery. We generate labels from a biologically relevant range that aim to simulate a patient under rest, mild exercise and high-intensity exercise conditions [12]. We experiment with three values of input flow Q in : three, five and seven ml/s, and three values of inlet pressure p in : 80, 100 and 120 mmHg. In our experimental setup, one coronary artery is composed of a surface point cloud with 100,000-200,000 points and a centerline graph with 300-600 vertices, depending on the complexity of the vascular tree. The surface point clouds are uniformly downsampled to 20, 000 points during the training process. In the inference setting the per-point results are obtained via a mean aggregation over multiple inference runs, five in our case, of different random splits of a point cloud into chunks of 20, 000 points. The process of projecting the surface features onto the centerline graph is done by averaging the estimated features of the faces' vertices which intersect with the orthogonal plane to the centerline placed for a given centerline node (Table 1)."
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,3.2,Results,"We showcase the comparison between PointNet++ and proposed Centerline-PointNet++ in the task of pressure drop and vFFR estimation for the synthetic dataset under different biologically relevant boundary conditions (BC) in Fig. 4. We report the MAE and NMAE which is MAE normalized by the absolute largest pressure drop in the testing set. The metrics are computed for projected results on a centerline for the evaluation to be more informative. Centerline-PointNet++ outperforms PointNet++ in every combination of input flow Q in and inlet pressure p in for both pressure drop and vFFR estimation.We show a visual comparison between the PointNet++ and CenterlinePoint-Net++ capabilities in the task of the vFFR in Fig. 4 for two example synthetic simulations of a patient under rest with input flow Q in of 3 ml/s and inlet pressure p in of 80 mmHg. Both models tend to achieve good results on non-stenotic and mild-stenotic segments. However, when it comes to larger stenoses, Point-Net++ lacks correct assessment of the stenosis impact, while CenterlinePoint-Net++ estimates correct values. We attribute this robustness towards sequential embedding done along the centerline which follows the natural blood flow in the coronary artery. Both examples in Fig. 4 showcase the common problem of Point-Net++ for vessel trees with close branches. Since the aggregation is done via Euclidean distance, the branches close to each other in this space are considered neighbours. Due to that, the predicted vFFR on one branch tends to spill on  the second branch as seen on the whole left side of the sample in the first row Fig. 4.We evaluate estimated vFFR with respect to the stenosis severity grade and group stenosis grades into the relevant intervals based on CAD-RADS scale [16]. Figure 5 showcases scatter plots for PointNet++ vFFR and CenterlinePoint-Net++ vFFR estimation against GT CFD vFFR with respect to the stenosis severity grade. We report the correlation of 0.88 and 0.93 for PointNet++ and CenterlinePointNet++ vFFR estimation, respectively. Performance in correct stenosis impact assessment seems to deteriorate with the increase of stenosis severity grade, however, only outliers achieve the absolute error above 0.1. CenterlinePointNet++ vFFR estimations achieve smaller variations than Point-Net++ over all classes, and significantly lower MAE for the stenosis severity grade (70-90%). We showcase that for the stenosis severity grade ≤ 70% the median of CenterlinePointNet++ estimated vFFR absolute error is kept under 0.01. Additionally, we evaluate the clinical viability of the proposed method by computing the accuracy of whether to perform intervention or not based on the predicted vFFR. We set the vFFR threshold to the clinically accepted one of 0.8 [2], and report the accuracy of 94.11% and 95.11% for PointNet++ and CenterlinePointNet++, respectively."
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,4,Conclusions,"In this study, we propose a novel point cloud based neural network architecture CenterlinePointNet++ which is tailored towards the analysis of complex vessel trees by incorporating multi-modal input of surface point cloud and centerline graph. We show an improvement in the vFFR time estimation from approx. two hours for a CFD simulation to around 15 s per synthetic coronary artery. Our centerline grouping approach is confronted with PointNet++ in the task of pressure drop and vFFR estimation in synthetic coronary arteries and achieves better results in NMAE and MAE for every set of values of the input flow and pressure of CFD simulation. The evaluation of FFR showcases a correlation of 0.93 with the CFD vFFR. One of the limitations of the method is the fact that the model is trained for the specified set of boundary conditions of underlying CFD simulation. In the future, we plan to expand the approach by incorporating boundary conditions as an additional input to the network. We also aim to conduct a comprehensive study using real patients' geometries for both training and evaluation, while comparing the results with invasive FFR."
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 1 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 2 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 3 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 4 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 5 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Table 1 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 73.
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,1,Introduction,"Population ageing is a huge health burden worldwide as the risk of morbidity and mortality increases exponentially with age [2]. However, great heterogeneity exists across individuals with the same chronological age, indicating chronological age poorly reflects intra-individual variation [10]. A quest for biomarkers that can accurately determine individual-specific, age-related risk of adverse outcomes has been embarked upon. Among the countless potential candidate ageing biomarkers [4,6,12], retinal age has been verified to be one of the most reliable indicators with the advantages of being rapid, non-invasive, and cost-effective [5,16,17].With the advent of technology, deep learning (DL) algorithms have found great applications in retinal age prediction. For example, Liu et al. [9] developed a convolutional neural network (CNN) to estimate retinal age with label distribution learning (LDL) on 12k fundus images from healthy Chinese populations. Zhu et al. [17] trained a CNN regression model on the UK Biobank cohort consisting of ∼70k fundus images. The limitations of these studies include: 1) the use of a single source of data in these studies has underestimated the complexity of data variance in real-world scenarios, which limits the generalizability of retinal age prediction. 2) only snapshot databases are used in these studies and failure track a detailed trail of age-specific changes. 3) outputting a single value with direct regressions [17] ignores the ambiguity of age labels, and using fixed label distribution [9] as ground truth did not consider individual variations. Tackling these shortcomings will improve the generalizability as well as reduce technical errors in the age prediction algorithm, providing a more reliable retinal age estimate.Therefore, in this study, we present an attempt to provide a novel accurate estimate of retinal age by learning adaptive age distribution from multiple cohorts with temporal fundus images available. Instead of learning a model using fixed label distribution as ground truth, we formulate the age estimation as a two-stage LDL task and give an adaptive distribution estimate for individual fundus images. As learning the LDL model with images from different data sources can harm the consistency and ordinality of embedding space, we introduce ordinal constraints to align the image features from different domains. Moreover, to leverage the temporal knowledge from the fundus image sequence, we add a temporal branch to capture the temporal evolution and use this auxiliary information to enhance the predictive performance of our model on snapshot images. We verify our method on a large retinal fundus dataset which consists of approximately 130k images of healthy subjects from the UKB cohort and Chinese cohorts. Extensive experiments prove that our model can achieve lower age prediction errors on multiple cohorts."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,2,Method,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,2.1,Progressive Label Distribution Learning,"As shown in Fig. 1, we formulate the retinal age estimation as a two-stage label distribution learning process. In the first stage, the model uses global features to predict a coarse age distribution on roughly discretized age labels. Each coarse age prediction is associated with a query vector corresponding to an age group. Then, the model performs class attention [13] between the age group query and spatial features to generate fine-level features which are further combined with the coarse age prediction to give refined age predictions.Formally, given a dataset with N imagesThe image encoder first transforms an input image x i into a spatial feature F i ∈ R H×W ×C , then a convolutional projection layer maps F i into the base representation F i ∈ R H×W ×D and the averaged feature f i ∈ R 1×D for the following age distribution learning. We discretize the age classes as ŷi = R (y i /δ d ) * δ d , where R (•) denotes the round operator and δ d is the age bin for tuning the discretization degree. Therefore, the total discretized age class number is. For the coarse-level age estimation, we set a large δ d = 10 which determines the age group queries asThen, we use an FC layer with softmax applied on the f i to calculate the coarse age distribution p i ∈ R 1×C δ d . Different from the previous study [9] using fixed label distribution as ground truth, we directly learn the distribution from training data with discretized age labels:where* ŷc is the expected value of the learned distribution p i , The first term is the cross-entropy loss which helps the model converges in an early training stage, the last two terms encourage the learned distribution to be centered and concentrated at the true age labels.In the refining stage, the mean value of coarse age distribution m i is used to select the age group query from Q coarse to involve the computation of fine-level feature:where GAP (•) is the global averaged pooling and A (•) denote the attention function with θ a as the parameters. The key and value vectors in the atten- tion function come from F i . Finally, we concatenate the f with the mapped coarse age distribution as the final feature embedding to predict the fine-level age distribution on a small age bin of δ d = 1:where f (•) denotes an FC layer with parameters θ f , mlp (•) represents a multilayer perceptron with one hidden layer and the parameter is θ m . The training loss is the same with Eq. 1."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,2.2,Cross-Domain Ordinal Feature Alignment,"Although existing studies [7,14] show that formulating regression as a classification task to learn the label distribution yields better performance, the ordinal information of age relations is lost in feature space. Moreover, when the training data comes from distinct data sources, the domain variance further damages the coherence of the learned features. In Fig. 2 (b) and Fig. 2 (c), we visualize the intermediate feature learned on our fundus image dataset. As can be seen, in Fig. 2 (c) the original model produces scattered and inconsistent features for ordinal age labels, while in 2 (b) the features exhibit a clear gap for different data sources. To address the above issues, we propose to introduce ordinal constraints in the label distribution learning and perform feature alignment to eliminate the domain variance. The key idea of imposing ordinal constraints in embedding space is to construct a set of triplets and enforce the feature distance to be consistent with the relative age gap. Specifically, for each batch of input data {x 1 , ..., x B }, we first compute their pairwise feature distance which outputs a distance matrix D ∈ R B×B . Then, we construct feature triplets and calculate the distance gap by subtracting shifted distance matrix D from the original D. In this case, each sample will have a chance to serve as the anchor to be compared with other samples. We formulate the ordinal constraint as following margin loss:where D [•] denotes metric of Euclidean distance, m is a dynamic margin depends on the relative age difference gap between |ŷ i -ŷj | and |ŷ i -ŷj |. To align features from different data domains, we directly select samples from same class and push them closer in the embedding space by minimizing the intra-class distance on both coarse-level features and fine-level features:where the I c (i, j) is an indicator function."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,2.3,Co-Learning with Temporal Fundus Images,"Compared to merely learning from single snapshot images, temporal data capturing more aging information can further boost retinal age prediction. However, in practice, temporal fundus data can be limited because the individuals are often lost to follow-up. Directly learning a temporal model on these small data usually cause poor generalization. Therefore, we propose to co-train our model on limited temporal imaging data and large-scale snapshot imaging data. Our aim is to use the auxiliary knowledge from temporal data to enhance the performance of our model tested on snapshot images.As illustrated in Fig. 1, the temporal branch consists of an image encoder, a time dimensional attention module (TDA), and a regression head. The temporal image encoder and the regression head are the same as that of the snapshot branch. We first input a fundus image sequence into the temporal branch and extract temporal features F t ∈ R T ×D from the TDA module which performs time dimensional attention to capturing the correlation of local regions across temporal images. At the same time, we input augmented sequential fundus images into the snapshot branch which outputs feature F s1 . Inspired by [15], we encourage the snapshot features to preserve similar relations in temporal features by optimizing the distance correlation loss:where R 2 (•) denotes the distance correlation and the detailed definition refers to [15]. We simple sum L lds , L ord , L align and L dist as the final loss.  Training details : As the biological age is normally developed and assessed in healthy populations where biological age is considered equal to chronological age, the model here is trained on 133895 selected snapshot images of healthy subjects without any report of systemic diseases from the four datasets (shown in Fig. 3). The temporal data is a subset of the snapshot data and consists of 2937 sequences with an average length of 5. We split the dataset into training, validation, and testing set with a ratio of 7:1:2. The standard data augmentation techniques such as random resized cropping, color transformation, and flipping are equally used in all experiments. Each image is resized to a fixed input size of 320 × 320. We use ReseNet-50 [3] as the image encoder for all models and train them using ADAM optimizer with a batch size of 100 and a training epoch of 45 with early stopping. The initial learning rates are set to 1 × 10 -5 and 3 × 10 -4 for the backbone layers and newly added layers, respectively. We divide the learning rate by 10 every 15 epochs. Evaluation metrics : Consistent with previous studies, we consider the mean absolute error (MAE) and the Pearson correlation as measures for assessing the performance of models."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,3,Experiment and Results,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,3.2,Quantitative Results,"Comparative Study: We then compare our model with existing popular regression methods which include both direct regression method, classification-Fig. 5. Result of ablation study on the proposed method. based methods [8,11], and ranking-based method [1]. The Mean-var improves the classification model by adding concentration regularization, the ranking-based method explicitly introduces ordinal information by combing a set of binary classifiers, and the POE methods model uncertainty with probabilistic embeddings. Table 1 shows the detailed comparison results. We denote our method as PLDL. It can be seen that the classification model outperforms the direct regression method on all the data cohorts. This observation is consistent with previous studies [11,14]. The POE-CLS is the best-performed model in all baselines, however, the performance is inferior to our model. When trained only with the snapshot images, our method achieves an MAE of 3.08 and Pearson's R of 0.942.Ablation Study: Here, we give the ablation results of our model to illustrate how different components affect the final performance. Figure 5b shows how the performance changed when adding different components in the proposed method. As can be seen, with only the coarse stage prediction, the model produces an MAE of 3.23 and Pearson's R of 0.939. Performing the refined age stage improves the MAE by ∼ 0.1. Introducing ordinal feature alignment gives a margin improvement in age prediction performance, but the feature space shows a clear improvement (see Fig. 2). At last, modelling the temporal fundus images improves the MAE from 3.08 to 3.03. In Fig. 5a, we give the learning curve of distance correlation and the MAE results on the validation set. As we can see, the snapshot features show a high correlation with the feature from the temporal branch and the MAE also becomes lower when optimizing the L dist . In Fig. 4, we give the detailed MAE distribution over different age labels on each cohort.The results indicate that the model produces high MAE on the tail ages in each cohort. Therefore, a future step to improve our model would be to consider the imbalanced learning techniques or group-wise analysis to reduce the MAE bias."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,3.3,Visualization Results,"In Fig. 6, we illustrate the estimated age distribution for some fundus image samples by our model and the baseline classification model. It can be seen that the refined age distributions are more accurate than the coarse prediction due to more precise discretization. Compared to the estimated age distributions from the baseline model, our method shows a more concentrated age distribution. In Fig. 5a, we visualize the similarity matrix by computing the pair-wise cosine distance between the age group queries. It can be seen that the query vectors for age groups 40∼50, 50∼60, and 60∼70 exhibit a very high similarity which implies that these groups may share more common ageing features."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,4,Conclusion,"In this study, we present a novel accurate modeling of retinal age prediction.Our model is capable of learning adaptive age distribution from multiple cohorts and leveraging temporal knowledge learned from sequencing images to improve age prediction on snapshot image modeling. Our model demonstrated improved performance in four independent datasets, with an overall MAE much lower than previously proposed algorithms."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Fig. 1 .,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Fig. 2 .,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,3. 1,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Fig. 3 .,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Fig. 6 .,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Table 1 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,1,Introduction,"Intraoperative imaging employing a mobile C-arm enables immediate and continuous control during orthopedic and trauma interventions. For optimal fracture reduction and implant placement, correct acquisition of standard views that correspond to a specific C-arm pose relative to the patient is essential [18]. Incorrect standard views can exhibit superimposed anatomical structures, leading to overlooked errors that can result in malunion of fractures, functional impairment, or require revision surgeries. To enable deducing all three dimensions of the trauma case, at least two 2D fluoroscopic views are acquired in two distinct planes usually at right angles to each other. The current manual C-arm positioning procedure results in only 20% surgically relevant acquisitions while the remaining 80% are caused by the iterative positioning process, exposing patients and clinical staff to unnecessary radiation [16]. Recent developments towards robotic C-arms ask for automatic positioning methods. Many state of the art approaches require a patient-specific CT for intraoperative real-time simulation [4,5,8] or 2D-3D registration [2,7,17], external tracking equipment [6,16], manual landmark annotation [1] or do not estimate an optimal pose but reproduce intraoperatively recorded C-arm views employing augmented reality [6,20]. The inherent prior assumptions and severe inference with the clinical workflow hinder broad clinical applicability until today. In contrast to the majority of anatomical regions, the standard planes of the knee are not orthogonal to each other. The a.-p. standard view is characterized by symmetric projection of the joint gap, femoral and tibial condyles. The tibia surface projects line-shaped and the medial half of the fibula head is superimposed by the tibia. In the lateral standard view, both femoral condyles are aligned and the joint gap is maximized. Automatic deep learning-based positioning for standard views involves specific challenges for image understanding due to overlapping anatomical structures, the presence of surgical implants, and changing viewing directions and showed to benefit from extracting semantic information [10]. Inspired by that and considering that standard views of the knee anatomy are characterized by the shape information of the individual projected bones, we propose a complete framework to fully automate the C-arm positioning tasks during knee surgeries (Fig. 1). Our contribution is 4-fold: (1) We propose a novel framework that enables simultaneous automatic standard view classification, laterality classification, in-plane rotation correction, and subsequent view-independent shape-based C-arm positioning to the desired standard view while requiring only a single initial X-ray projection. One pose regression network can handle two distinct standard views of the knee anatomy. A suitable segmentation representation for the knee anatomy is proposed to recognize correct standard views, which explicitly incorporates semantic information to reflect on the actual clinical decision-making of surgeons. Since intraoperative X-rays with reference pose annotations do not exist due to the mobile surgical setup, the proposed framework is solely trained on simulated data with automatically generated pose annotations. (2) We show that the proposed approach outperforms view-specific shape-based and intensity-based pose regression. (3) We show that the proposed shape representation and augmentation strategies aid generalization from simulated training data to real cadaver X-rays. (4) We investigate the importance of individual knee bones on the overall positioning performance for two distinct standard views."
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,2,Materials and Methods,An overview of the complete framework for fully automated C-arm positioning towards desired standard views during knee surgeries is given in Fig. 1. The anterior-posterior (a.-p.) and the lateral standard view showed to be sufficient for various diagnostic entities [3].
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,2.1,Training Data Simulation,"To address the interventional data scarcity problem, simulated training data was generated from a collection of CT and C-arm volumes using a realistic DRR simulation framework [21] complemented with corresponding 2D segmentations. Preprocessing involved the following steps: (1) Field-of-view cropping: Prevents superposition of the other laterality in the projection domain.   [22] with interactive plane positioning. They serve as ground truth pose reference during simulation. (4) 3D automatic bone segmentation: To compute automatic 3D segmentations, a 3D nnU-Net [9] is trained on a subset of 10 manually annotated CTs for the task of multilabel bone segmentation, segmenting the femur, tibia, fibula, and patella. (5) Suitable segmentation representation: The two femur condyles are not distinguishable in the shape-based representation, however, this is relevant for optimal lateral positioning. In an optimal lateral view, femur condyles would overlap. Annotating the condyles as line features would result in an increased manual labeling effort in the projection domain. Alternatively, we propose to incorporate this information in the segmentation by separating the femur annotation symmetrically along the femoral shaft (Fig. 2a). This results in one additional segmentation label for the lateral standard to recognize condyles' congruence and derive the directional pose offset. (6) DRR and mask simulation: DRRs are simulated for varying angulations of orbital and angular rotation α, β ∈ [-40 • , 40 • ] around the defined reference standard. The system parameters are defined according to a Siemens Cios Spin R with 300 mm detector and 1164 mm source-detector distance. Simulation was performed for an increased field-of-view to create in-plane γ augmentations without introducing cut-off regions at the image borders. The DeepDRR simulation framework was extended to allow the forward projection of corresponding masks (Fig. 2b). A set of augmentations is applied to the simulated dataset, to bridge the domain gap from synthetic data to real X-rays (Fig. 3). The in-plane rotation augmentation accounts for variable patient to C-arm alignment, the translation bridges the gap to the validation data where the joint gap is not centered like in the training data, random region dropout reflects superposition artifacts, transparent edge overlays reflect projection artifacts resulting from, e.g., the operating table, and border overlays account for the gamma correction interpolation artifacts. For simulation, a set of 24 CTs was considered, 15 CTs without metal and 9 Carm volumes with metal. The data was divided 60 -20 -20 % for training (15 CTs), validation (5 CTs), test (4 CTs)."
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,2.2,Shape-Based Positioning Framework:,"The proposed shape-based positioning framework was trained jointly for both standard views (Fig. 1). It consists of two modules: The first is responsible for a view classification, in-plane rotation and laterality alignment, directly estimated from the image intensities. Thereby, pose ambiguities and data variation are addressed, simplifying the task for the subsequent module to estimate the optimal C-arm positioning for the desired standard view, employing shape features."
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,(1) Intensity-based multi-task classification and regression module:,"For simultaneous in-plane rotation regression, view recognition, and laterality classification, an EfficientNet-B0 feature extractor [14,19] was extended with two binary classification heads with one output neuron, followed by sigmoid activation, and one regression head, with the same architecture, but 2 outputs, omitting the activation. The in-plane rotation γ is mapped to sin/cos-space to ensure a continuous Loss function during optimization. All training examples were aligned with the same laterality during data simulation which would otherwise result in pose ambiguities. Thus, to train the laterality classifier, the training examples were randomly flipped horizontally with p = 0.5 and the corresponding γ label was adapted accordingly. The weights were optimized using Binary Cross Entropy Loss for the classification heads, and Mean Squared Error for the regression head.(2) Shape-based pose regression: Following surgical characteristics for recognizing correct standard views of the knee, a view-independent shape-based pose regression framework was developed. The architecture is based on a 2D U-Net [12] with two view-specific segmentation heads, because the segmentation labels differ for both views. Considering the results of the view classification, the extracted segmentation map of the corresponding segmentation head are used as input for the pose regression network that outputs the necessary C-arm pose update (α, β, γ, t)∈ R 6 to acquire the desired standard view [11]. To ensure equal number of input channels to the PoseNet for both standard views, a zero channel is appended to the a.-p. multi-label segmentation head output. "
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,2.3,Real X-Ray Test Data:,"Real X-rays for validation were sampled from single Siemens Cios Spin R sequences generated during 3D acquisition of 6 knee cadavers. Preprocessing consisted of (1) definition of 3D standard reference planes, (2) laterality check, (3) sampling of X-rays around the defined reference standards in the interval α, β ∈ [-30 • , 30 • ], and generation of ground truth pose labels. Since the Spin sequences are orbital acquisition sequences, only the orbital rotation is equidistantly covered in the test set, while the angular rotation is constant for all X-rays sampled from the same sequence. The number of sampled X-rays per standard and view may differ, if the reference standard is located close to the edge of the orbital sequence (range: 102-124). "
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,3.1,Importance of Pipeline Design Choices (RQ1),"In an ablation study, the proposed shape-based view-independent pose regression was compared to view-specific direct intensity-based pose regression [11]. Further, the complete pipeline (2-step) is compared to a 1-step segmentationbased approach trained view-specific and view-independent (Fig. 4). Evaluation was performed on the simulated test DRRs and cadaveric X-rays (Fig. 5). Significant performance differences were confirmed by a paired t-test. View-independent vs. view-specific networks: While the view-specific networks perform significantly better (lateral) or comparable (a.-p.) on the simulated data, the view-independent networks perform significantly better (a.-p.) or comparable (lateral) on the real data. 1-step vs. 2-step: The proposed 2-step approach performs significantly better or comparable than a 1-step shape-based pose regression approach on most validation cases (8/12) in viewing direction θ. Regarding the γ rotation, the 2-step approach improves performance across all validation cases. Generalization from DRR to X-ray: The shape-based pose regression network combined with joint view-independent training clearly boosts the performance compared to direct intensity-based pose regression from dθ X-ray a.-p = 12.2±6.8 • , dθ X-ray lateral = 14.4±7.6 • to dθ X-ray a.-p = 7.4±5.0 • , dθ X-ray lateral = 8.4±5.4 • . Figure 6 shows qualitative results on exemplary real test X-rays."
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,3.2,Importance of Individual Bones on Overall Performance (RQ2),"Figure 7 shows the importance of individual segmented bone classes on the overall positioning performance evaluated on the test DRRs (3528 DRRs). The fibula has very little influence on the positioning for both views. The patella is only important for the a.-p. view, while tibia and femur are relevant for both views. The condyle assignment for the lateral view determines the rotation direction for the orbital and angular rotation (α, β). Inverting the assignment of left and right femur condyle results in a sign flip in α, β. "
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,3.3,Accuracy of View and Laterality Classification (RQ3),"The classificator performances were assessed on the synthetic (3528 DRRs, 4 CTs) and real data (1386 X-rays, 6 C-arm scans). The view classificator (a.-p. / lateral) achieved an accuracy of 100% on the test DRRs and 99% on the X-rays. The laterality classificator (left / right) resulted in an accuracy of 98% on the test DRRs and 98% on the X-rays."
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,4,Discussion and Conclusion,"A complete framework for automatic acquisition of standard views of the knee is proposed that can handle several standard views with a single architecture.The complete pipeline is trained on simulated data with automatically generated annotations and evaluated on real intraoperative X-rays. To bridge the domain gap, different augmentation strategies are suggested that address intraoperative confounding factors, e.g., the OR table. View-independent training and multilabel shape features improve the generalization from simulated training to real X-rays and outperform direct intensity-based approaches. View-independent networks result in more training data which showed to improve the generalization from simulated training to real X-rays. The 2-step approach increases robustness and simultaneously automates necessary preprocessing tasks like laterality and standard view recognition, which can be performed with very high accuracy on simulated (100%, 98%) and real data (99%, 98%). The approach is fast and easy to translate into the operating room as it does not require any additional technical equipment. Assuming that the surgeon acquires the initial X-ray Data use declaration: The data was obtained retrospectively from anonymized databases and not generated intentionally for the study. The acquisition of data from living patients had a medical indication and informed consent was not required. The corresponding consent for body donation for these purposes has been obtained."
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 1 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 2 .Fig. 3 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,( 2 ),
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 4 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 5 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 6 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 7 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 45.
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,3,Experiments and Results,"The proposed pipeline was evaluated considering the following research questions:(RQ1) Does the proposed shape-based pipeline outperform view-specific intensity-based and shape-based pose regression? How does it influence the generalization from synthetic to real data? (Sect. 3.1) (RQ2) How do individual bones influence the overall positioning performance? (Sect. 3.2) (RQ3) How accurate is the performance of the view and laterality classification?(Sect. 3.3)Positioning performance was evaluated based on the angle θ = arccos v pred , v gt between the principal rays of the ground truth v gt and predicted pose v pred and the mean absolute error (AE) of in-plane rotation γ. The interrater variation of the reference standard planes defined by two independent raters serves as an upper bound for the reachable accuracy of a C-arm positioning approach trained on the reference annotations. It was assessed in terms of orientation differences θ (θ a.-p. = 4.1 ± 2.6 • , θ lateral = 1.8 ± 1.3 • ). The models were implemented using PyTorch 1.6.0, trained with an 11 GB GeForce RTX 2080 Ti, and optimized with the Adam optimizer with a base learning rate of η = 10 -4 and batchsize 8, pre-trained independently, and jointly fine-tuned until convergence."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,1,Introduction,"Virtual models of cardiac electrophysiology (EP) have demonstrated significant potential in various clinical tasks, such as stratifying the risk for lethal arrhythmias [2] and predicting responses to cardiac resynchronization therapy [18]. Numerical solutions to the governing partial differential equations (PDEs) for cardiac EP (forward problem), however, are difficult to obtain. The inverse estimation of the parameters of these PDEs given observation data (inverse problem) is even more difficult, due to challenges such as the complex relation between the PDE parameters and the observations, and the need to embed a numerical PDE solver within the inverse optimization process.Physics-informed neural networks (PINNs) is a new paradigm for solving both the forward and inverse problems of PDEs [16]. The general idea of PINNs is to train neural networks to satisfy physical laws as described by the PDEs, optimized via the so-called PDE residuals. When partial observations of the PDE solutions are available, this PDE residual can be combined with data-residuals to solve the forward and inverse problems at the same time [4,11,13,15,23]. Despite substantial attention and successes, however, the use of PINN in cardiac EP has been rather limited. To date, there have only been two attempts of PINN-based EP simulation that are limited to 1D/2D [9] or atrial surfaces [17].What fundamental challenges does 3D bi-ventricular EP present for PINNs? First, 3D bi-ventricular EP simulation requires the PDE solutions to be obtained over a complex geometry domain in space and a long duration in time. Second, the PDE solution to bi-ventricular EP -in the form of the spatiotemporal propagation of transmembrane potential (TMP) activation -exhibits sharp spatial and temporal gradients. These characteristics of bi-ventricluar EP simulations present fundamental challenges to the state-of-the-art PINN framework, which has been shown to face potential failure modes when the PDEs are solved over large or complex solution domains with sharp transitions [8,10,14,21,22].In this paper, we present a novel PINN framework to overcome these challenges and enable the forward and inverse solutions to 3D bi-ventricular EP simulations. This is achieved with three key innovations. First, to avoid dealing with higher-order spatial derivatives over the complex 3D geometry, we formulated the PINN over the meshfree representation of the 3D bi-ventricular geometry with a modified PDE residual incorporating the weak form of the original PDE. Second, to enable PINN solutions over the long temporal domain, we present a temporally adaptive and sequential training strategy to guide the PINN to respect the causality of the underlying physics of wave propagation. Finally, we introduce a spatially adaptive training strategy to guide the PINN to exploit the spatiotemporal sparsity of the sharp gradients exhibited in the PDE solution.We experimentally demonstrated that the presented PINN framework was able to enable complete simulation of the bi-ventricular EP activation process that is otherwise not possible with vanilla PINN frameworks. We further conducted detailed ablation studies of the benefits of the presented spatialtemporally adaptive and sequential learning strategies, and demonstrated preliminary feasibility of the presented PINN framework for supporting the inverse parameter estimation of 3D bi-ventricular EP models. These represent an innovative first attempt to enable PINN solutions for 3D bi-ventricular EP applications."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,2,Background: Bi-Ventricular EP Simulations,"Existing ventricular EP models range from macroscopic-level two-variable PDEs to ionic models with tens of variables [5]. In this proof-of-concept study, we consider the two-variable diffusion-reaction Aliev-Panfilov (AP) PDE [1] for its ability to reproduce key excitation features without formidable computation:where u ∈ [0, 1] is the unit-less TMP and v is the recovery current. The diffusion tensor D describes local conductivity anisotropy determined by fiber structures. Parameters {γ, c, e 0 , μ 1 , μ 2 } control the temporal dynamics of u and v."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,3,Methodology,"We present a novel PINN framework to support forward and inverse 3D biventricular EP simulations. As outlined in Fig. 1, it includes three key components: 1) a weak-form PDE residual to bypass the challenges of high-order spatial derivatives over irregular geometry; 2) a novel spatial-temporally adaptive strategy to mitigate PDE propagation failure over large solution domains and accelerate convergence; and 3) a sequential training strategy to enable solutions over the complete bi-ventricular EP activation process."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,3.1,Weak-Form PDE Residual over Meshfree Representations,"In a vanilla PINN framework, we will use a neural network u θ (x, t) to approximate PDE solutions u(x, t) to the AP model. The network will be optimized by the initial, boundary, and PDE residuals (Loss I , Loss B and Loss R ) over a set of pointsj=1 sampled in time, space, and boundary domains:) where λ I , λ B and λ R are the hyperparameters that balance these residuals during training. Because u θ (x, t) lives on the 3D geometry of the ventricles, it is nontrivial to calculate the second-order spatial derivatives.To address this, we utilize the weak form of PDE and spatially discretize it on a bi-ventricular mesh through the Mesh-free method [19] as:where vectors U = [u 1 , u 2 , . . . , u NΩ ] T and V = [v 1 , v 2 , . . . , v NΩ ] T consist of u and v from all N Ω mesh-free points inside the myocardium. Matrices M and K represent numerical approximations of the second-order spatial derivatives in Eq. ( 1), which also automatically incorporate the natural boundary condition [19,24]. We then let the PINN neural network describe the PDE solutions over the discrete ventricular mesh as U θ (t) and V θ (t) as a function of time t, and obtain a modified PDE residual as:)With this modification, PINN no longer needs to deal directly with the second-order spatial derivatives over the 3D ventricular geometry."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,3.2,Spatial-Temporally Adaptive Training Strategies,"A common challenge in PINN training is the failure to propagate PDE solutions over a large solution domain [6,14,20]. Fundamentally different from conven-tional training, only labels of the initial PDE solutions are known in PINN training, from which the correct solutions need to be propagated to the entire solution domain. Before the correct solution arrives, the intermediate (and mostly trivial) solutions also propagate. If dominating the propagation, this will result in a chain reaction that prevents the propagation of correct solutions, leading to propagation failure. This problem is escalated if the propagation gradient is sharp.Temporally Adaptive Training: We argue that the propagation failure is caused by the inability of vanilla PINNs to respect the causality underlying the physical laws of wave propagation. Therefore, we propose temporal weights {β 1 , β 2 , . . . , β NT } to guide PINNs to respect temporal causality during training. The intuition is that, when the correct solution is propagated to time t c , the previous times {t 1 , t 2 , . . . , t c-1 } should all have lower PDE residuals. We thus propose the temporal weights to be:where t is a threshold to be tuned. This will guide the propagation of correct solutions while avoiding obtaining and propagating trivial intermediate solutions."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Spatially Adaptive Training:,"While sharp gradients in PDE solutions are challenging for PINNs [12,14,22], they are sparse both in space and time in ventricular EP activation. We thus propose to exploit these sharp-gradients to focus PINN training on these sparse regions, thus accelerating PINN convergence. This is achieved by spatial weights W S = [w 1 , w 2 , . . . , w NΩ ] defined as:where w h > 1 and s is another threshold to be tuned. With the above strategy, PDE residual is further modified to:Sequential PINN Training: Even when propagation causality and sparse regions of sharp gradients are respected, PINN cannot solve for arbitrarily long time domains because the loss landscape becomes increasingly complex as N T increases. We thus further utilize a sequential learning method where we first uniformly discretize the time domain [0, T ] into n segments, and then train the PINN across these segments sequentially as:where PDE solutions obtained from previous time segments become the initial residual for training the PINN for the next time segment. The complete PDE solution for the entire time domain is obtained at the end of the training."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,3.3,Solving the Forward and Inverse Problems,"In the forward problem, PINNs approximate the solution of the AP model by optimizing the initial residual Loss I and the PDE residual Loss R :In the inverse problem, PINNs utilize partially known solutionsto simultaneously optimize the PINN parameters θ and unknown PDE parameters φ with an additional data loss Loss D :"
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,4,Experiments and Results,"In all experiments, the neural network we used has 5 quadratic residual layers [3] as hidden layers with 512 neurons in each layer, where the input is time t and the output is N Ω (=1862) dimensional vectors U θ and V θ . The quadratic residual layers have a stronger nonlinearity than the fully connected layers [3]. We used Adam optimizer and the learning rate is set at 1 × 10 -3 . The parameters of AP model are fixed to standard values as documented in the literature [1]:c = 8, e 0 = 0.002, μ 1 = 0.2 and μ 2 = 0.3. The hyperparameters of our proposed PINN framework are set as follows: t = 0.001, s = 0.01, w h =4 and T n -T n-1 = 1. In the forward problem λ I : λ R = 10 : 1, and in the inverse problem λ D : λ R = 1 : 1000. All experiments were run on NVIDIA Titan RTX and numerical solution generated through the method in the literature [19].The Effectiveness of Spatial-Temporally Adaptive PINNs: To verify the effectiveness of the spatial-temporally adaptive strategies for PINN training, we solve the forward problem of the AP model on time domain t ∈ [0, 1], t ∈ [0, 5] and t ∈ [0, 10] considering three ablation models: vanilla PINN, PINN + temporal weights, PINN + temporal and spatial weights. Figure 2 shows MSE error between PINN output and numerical solution during the training process. Figure 4 provides examples of how the correct solution, PINN solutions, and spatial weights were propagated over space. As shown, the spatial weights were able to track the ""wavefront"" of the TMP activation -a sparse set of locations where high gradients exist both in space and time. This succeeded in guiding the optimizer to focus more on these regions, thus accelerating the convergence."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,The Effectiveness of Sequential Training:,"We then compared the computation cost and the accuracy of the PDE solution achieved by the presented PINNs with and without sequence learning, across different lengths of time domains as summarized in Table 1. As shown, the sequential training was able to achieve a 1.3-1.8× speedup with increased accuracy. Also note that without sequence  ) throughout the complete ventricular activation (t ∈ [0, 60]), enabling simulation of the complete ventricular activation process as shown in Fig. 4 that is otherwise not possible with the other ablation models.PINN for Supporting PDE Parameter Inference: Finally, we tested the feasibility of the presented PINNs to support parameter estimation of the AP model. We assumed measurements of TMP solutions to be available and considered unknown parameter γ in the AP model (Eq. 1) due to its relatively large influence on the PDE solution [7]. We considered spatially varying values of γ representing infarct tissues, and tested joint optimization of the PINN and the unknown γ for six different cases. The estimated parameter, as shown in Fig. 5, exhibited small MSEs in comparison to the ground truth. "
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,5,Conclusion,"We presented a spatial-temporally adaptive PINN framework to solve the forward and inverse problems of cardiac EP over 3D bi-ventricular models, overcoming the current limitations of PINNs in solving PDEs with sharp gradients/transitions over tricky spatial domains and long time domains. However, our PINN framework still has a much larger computational cost than traditional numerical methods, which is a drawback of PINN itself. Future works will pursue the use of this PINN framework for more complex PDEs of bi-ventricular EP, improve its computational efficiency as well as enable inverse EP using surface measurements such as electrocardiograms (ECGs) in real-data settings."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Fig. 1 .,
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Fig. 2 .Fig. 3 .,
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Fig. 4 .,
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Fig. 5 .,
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Table 1 .,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,1,Introduction,"The thalamus is a critical brain region that relays and modulates information between different parts of the cerebral cortex, and plays a vital role in signal transmission and processing, including pain recognition and reaction [13]. Abnormal fetal thalamus development, which can disrupt serotonin receptor development, may contribute to the development of later neuropsychiatric disorders [12]. Thalamus development is influenced by maternal factors such as diabetes that suppresses thalamus development after 2 weeks of gestation [16]. To further investigate the relationship between maternal factors and fetal thalamus growth, a dataset containing maternal factors, fetal thalamus diameter (FTD), and fetal head circumference measurements (FHC) is needed. FHC measurement is necessary to normalize FTD against gestational age. However, measuring FTD and FHC manually from 2D-US scans is laborious, prone to high inter-observer variability, and complicated by 2D-US images' high signal-to-noise ratio (SNR) nature due to ultrasound wave's lack of penetration power compared to ionizing radiations such as the x-ray, acoustic shadows cast by highly echogenic objects, and unique noise characteristics due to ultrasound reverberation [3,4,9].In recent years, landmark-based detection approaches based on deep learning have been employed to measure biometrics from fetal US images. They were used to detect measurement key points for brain structures in the fetal brains, and bony structures such as length of the femur or dimensions in the pelvic floors [2,7,14]. In these studies, the distance between a pair of landmarks represents the biometry being measured. The current state-of-the-art (SOTA) landmark detection algorithm for measuring biometry in fetal 2D-US images is the Biome-tryNet proposed by Avisdris et al., which was developed to detect landmarks for measuring the FHC and fetal femur length [2]. BiometryNet is based on High-Resolution Net (HRNet) and it has shown great performances in measuring dimensions of fetal skull and femur bone, outperforming other landmark-based methods [10]. BiometryNet uses dynamic orientation determination (DOD) to enforce a consistent orientation between detected landmarks', i.e. the first landmark is always the left/top measurement key point, and the second landmark is always the right/bottom key point [2].However, BiometryNet cannot be directly used to measure FTD and FHC due to two specific difficulties. First being that the ""guitar-shaped"" structure (GsS) by Sridar et al. to measure FTD has similar echogenicity to surrounding brain tissues, resulting in fuzzy boundaries, especially around the wing-tips where measurement landmarks are located [9] (Fig. 1). This difficulty is also observed in 2D-US images of fetal skulls when they appear broken due to unfused bones Fig. 1. Left: 2D-US image of fetal head from the HC18 dataset [11], note part of the skull is not mineralized and has similar echogenicty to the adjacent uterine tissue. Middle: 2D-US image of a fetal femur [1]. Right: 2D-US image of a GsS for measuring FTD, note the gaps in the skull due to unfused bones. and acoustic shadows cast by the skull bones themselves (Fig. 1). Second being that the shape of the GsS resembles the silhouette of a guitar and more complex than that of a skull or femur bone (Fig. 2) [9]. These two difficulties causes uncertainties in the localization of measurement landmarks of FTD, resulting in inaccurate measurement of FTD.To address the above difficulties, we present a novel Swoosh Activation Function (SAF) designed to enhance the regularization of heatmaps produced by landmark detection algorithms. The SAF takes its name from the Nike TM swoosh logo, which resembles its shape (Supplementary Fig. 1). SAF can serve as a regularization term to enforce an optimum mean squared error (MSE) level between a pair of predicted heatmaps (pHs). By doing so, the landmark detection algorithm is compelled to highlight different areas. Additionally, SAF can enforce an optimum MSE between individual pH and a zero matrix to reduce hotspot dispersiveness. Moreover, because SAF does not grow exponentially when the input MSE is higher than the optimum MSE level, it does not hinder algorithm's learning. Consequently, we hypothesize that SAF can enhance landmark detection accuracy and overcome uncertainties arising from the fuzzy edges of GsS by promoting hotspot concentration in pHs."
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2,Method,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2.1,Swoosh Activation Function,"SAF is introduced to optimize pHs by enforcing an optimum MSE between a pair of pHs and a secondary optimum MSE between a predicted heatmap (pH) and a zero matrix (O). We determine the optimum MSE by computing the MSE between a pair of ground truth heatmaps (gHs). Each ground truth heatmap (gH) represents a measurement landmark by a smaller matrix drawn from a Gaussian distribution that is centered at the landmark coordinates with the peak assigned at the value of 1 (Fig. 3.A). Since we determine that gHs represent optimum heatmaps, the MSE between a pair of pHs should approximate the MSE between a pair of gHs. In addition, the secondary optimum MSE between a pH and a zero matrix is half of the MSE between a pair of gHs, since only one Gaussian distribution is being compared. We demonstrate in Fig. 3.B and C how deviations from this optimum MSE value can lead to incorrect and noisy heatmaps. To enforce this pre-determined optimum MSE, we defined SAF as:In Eq. 1, Min represents a function that ensures SAF minimizes to 0, and it is defined as Min = f 1 ab . The coefficient a determines the slope of SAF around the minimum point in Quadrant 1 of the Cartesian coordinate system (Supplementary Fig. 1). The slope of SAF determines its regularization strength. Coefficient b determines the x-axis coordinate of the minimum point where the xcoordinate of the minimum point correspond to the optimum MSE. Coefficient b is be deducted by Supplementary Eq. 1 and the coefficient c is deducted by Supplementary Eq. 2."
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2.2,SAF Regularization,"The loss function consists of the MSE between pH and gH, and three additional SAF regularization terms. We chose SAF as the activation function to control these regularization terms because SAF's output grows exponentially on either side of the minimum point (Supplementary Fig. 1) where the x-axis represents input values of MSE(pH 1 , pH 2 ), MSE(pH 1 , O), and MSE(pH 2 , O), while yaxis represents the output values of SAF which minimize to 0 when input values approximate the predetermined optimum MSE. SAF locks the input MSE to the pre-determined optimum value because deviation from this value would result in a fast increase in the gradient of SAF. The first SAF term regularizes the MSE between a pair of pHs. The next two SAF terms regularize the MSE between each pH and a zero matrix. The equation for the entire loss function is:  "
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2.3,Datasets,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,FTD Dataset:,"The dataset used in this study consists of 1111 2D-US images acquired during the second trimester of pregnancies and confirmed by boardcertified ultrasonographers to be suitable for measuring FTD [8]. No additional ethics approval was required. Spatial constraints were provided by manually added bounding boxes around the GsS, verified by the same ultrasonographers. Pycocotools generated two gHs for each pair of measurement landmarks, with hotspots representing landmarks [6]. 5-fold cross-validation was performed. During training, 100 training samples were randomly held out as the validation set.The intraclass correlation coefficient (ICC) score was computed using IBM TM SPSS TM version 28, with the ICC configuration being Two-Way Random and Absolute Agreement [5].HC18 Dataset: HC18 dataset is available on the Grand Challenge website [11]. We utilized least squared fitting of an ellipse to determine the center, width, height, and angle of rotation of the elliptical ground truth mask. We used trigonometry to determine the coordinates of the landmarks for the major and minor axes of the ellipse. Spatial constraints in the form of bounding boxes were built by the ground truth mask of head circumference published by the dataset author. Predicted landmark points were used for the major and minor axes to calculate the ellipse width and height for testing. The predicted major and minor axes were assumed to be perpendicular, and their point of intersection was used as the center of the ellipse. Finally, we uploaded the results to the Grand Challenge leaderboard and used the mean difference between predicted FHC and ground truth as the performance metric."
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2.4,Experimental Setup,"Training Epochs and Learning Rate. We conducted our experiments using PyTorch version 1.12 on two NVIDIA GTX-1080Ti graphical processing units, each with 11 GB of video memory. For landmark detection training, we used the same learning rate configuration as Avisdris et al. [2] to train both Biom-etryNet with/without SAF. Specifically, we set the initial learning rate to 10 -5 and reduced it using a multi-step learning rate scheduler that scaled the learning rate by a factor of 0.2 at epoch numbers 10, 40, 90, and 150. We trained all BiometryNet models for a total of 200 epochs. For EfficientNet with/without SAF, we set the initial learning rate to 10 -5 and reduced it using a multi-step learning rate scheduler that scaled the learning rate by a factor of 0.1 at epoch number 300 and 350. We trained both models for 400 epochs.Pre-processing. Our pre-processing pipeline included random rotation of ±180 • , random re-scaling of ±5%, resizing to 384×384 pixels without preserving aspect ratio, and normalization using ImageNet-derived mean = (0.485, 0.456, 0.406) and standard deviation = (0.229, 0.224, 0.225) for each color channel. SAF Configuration. Given our dataset configuration where each biometry landmark was represented by a 19 × 19 matrix with values derived from a Gaussian distribution with the center point's value peaked at 1, the MSE between a pair of gHs was 0.0061. This configuration followed the standard implementation used in human pose estimation landmark detection [15]. The secondary optimum MSE between a pH and a zero matrix was halved at 0.00305. We also predetermined the value of Min to be 0.001 to prevent SAF from overpowering the MSE loss between gHs and pHs. We experimented with different values of the coefficient a (1, 4, and 8) to evaluate coefficient a's influence on landmark detection accuracy. We then determined the value of coefficient b using Supplementary Eq. 1, and the value of coefficient c using Supplementary Eq. 2.We conditionally activated SAF when the average of MSE(pH 1 , gH 1 ) and MSE(pH 2 , gH 2 ) was less than 0.0009 because SAF is not bounded and early activation would hinder algorithm learning. The proposed SAF algorithm was evaluated using six model configurations, including Vanilla BiometryNet, Biom-etryNet with SAF with coefficient a values of 1, 4, and 8, an EfficientNet, and the EfficientNet with SAF configured with coefficient a value of 4. The model configurations were trained and tested to verify the usefulness of the proposed SAF using both FTD and HC18 datasets."
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,3,Results,"The results of FTD dataset show that BiometryNet with SAF a1 (Biome-tryNet SAF a1) achieved the highest ICC score at 0.737, surpassing the performance of the vanilla BiometryNet, which scored 0.684. Moreover, BiometryNet with SAF a4 (BiometryNet SAF a4) and BiometryNet with SAF a8 (Biome-tryNet SAF a8) also demonstrated superior ICC scores for FTD measurement, albeit to a lesser degree than BiometryNet SAF a1. The impact of SAF on performance was further observed in the modified EfficientNet, where Efficient-Net SAF a4 achieved a higher ICC score of 0.725, compared to the modified EfficientNet without SAF, which only scored 0.688. We also observed that SAF reduced the similarities between a pair of pHs and the dispersiveness of hotspot in the pH. We display such heatmaps produced by BiometryNet in Fig . 4.A and BiometryNet SAF a1 in Fig . 4.B.For the HC18 dataset, BiometryNet SAF a8 demonstrated the lowest measurement mean difference from the ground truth at 3.86 mm ± 7.74 mm. Additionally, all configurations outperformed the vanilla BiometryNet. The impact on FHC measurement was further observed in the modified EfficientNet, where EfficientNet SAF a4 achieved a lower measurement mean difference at 4.87 mm ± 5.79 mm compared to EfficientNet at 32.76 mm ± 21.01 mm. The FTD dataset ICC scores and HC18 dataset mean differences for each algorithm are presented in Table 1. "
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,4,Discussion,"The main findings of this study are as follows: (1) SAF improved the measurement accuracy of algorithms in both FTD and FHC measurement tasks; (2) SAF regularization is architecture-agnostic, as it improved the measurement accuracy of both BiometryNet and EfficientNet compared to their vanilla forms that do not use SAF; and (3) the optimum configuration of SAF coefficients is taskdependent. For FTD measurement, the most optimum configuration was to use a = 1, while for FHC measurement was with coefficient a = 8.The results demonstrate the performance improvement brought about by SAF regularization, effectively improving the accuracy of landmark detection of fetal biometries in 2D-US images. SAF regularization forces a pair of heatmaps to highlight different areas and reduce the dispersiveness of hotspots in pHs, which results in improved fetal biometry landmark detection accuracy. This is supported by the comparison of heatmaps produced by BiometryNet and BiometryNet SAF a1 displayed in Fig. 4. Moreover, SAF regularization is simple to implement and easy to configure, requiring no modification to the network architecture. Our results also suggest that SAF is highly generalizable as it is architecture-agnostic, improving the performance of both BiometryNet and EfficientNet. SAF is also highly configurable for different tasks via different coefficient configurations. Furthermore, we also suggest that SAF is generalizable to other imaging modalities that also require pair-wise landmark detection. For example, detecting mitral and aortic valves in 2D-US images of heart and detecting cranial sutures in CT images of skull.As part of our future study, we will explore the effectiveness of SAF regularization in other fetal landmark detection tasks, especially those that suffer from similar issues with fuzzy edges and uncertain landmark locations. Additionally, the optimum configuration of SAF coefficients may vary depending on the specific dataset or imaging modality used."
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,5,Conclusion,"Our study demonstrated the effectiveness of SAF as a novel activation function for regularizing heatmaps generated by fetal biometry landmark detection algorithms, resulting in improved measurement accuracy. SAF outperformed the previous state-of-the-art algorithm, BiometryNet, in both FTD and FHC measurement tasks. Importantly, our results showed that SAF is architecture-agnostic and highly configurable for different tasks through its coefficients, making it a generalizable solution for a wide range of landmark detection problems."
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,Fig. 2 .,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,Fig. 3 .,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,Fig. 4 .,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,Table 1 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,1,Introduction,"Brain extraction is the first step in fetal Magnetic Resonance Image (MRI) analysis in advanced applications such as brain tissue segmentation [15] and quantitative measurement [22,24], which is essential for assessing fetal brain development and investigate the neuroanatomical correlation of cognitive impairments [14]. Current research based on Convolutional Neural Network (CNN) [5,19] has achieved promising performance for automatic fetal brain extraction from pixelwise annotated fetal MRI. However, it is labor-intensive, time-consuming, and expensive to collect a large-scale pixel-wise annotated dataset, especially for images with poor quality and large variations. To address these issues, weaklysupervised segmentation methods with image-level supervision [21] are introduced due to their minimal annotation demand. However, learning from imagelevel supervision is extremely challenging since the image-level label only provides the existence of object class, but cannot indicate the information about location and shape that are essential for the segmentation task [1].Prevailing methods learning from image-level labels for segmentation commonly produce a coarse localization of the objects based on Class Activation Maps (CAM) [27]. Due to the weak annotation, the CAMs from the classification network can only provide rough localization and coarse boundaries of objects. To alleviate the problem, a lot of approaches have been proposed, which can be categorized as one-stage and two-stage methods. One-stage methods aim to generate pixel-level segmentation by training a segmentation branch simultaneously with a classification network. For example, Reliable Region Mining (RRM) [26] comprises two parallel branches, in which pixel-level pseudo masks are produced from the classification branch and refined by Conditional Random Field (CRF) to supervise the segmentation branch. Despite their efficiency, one-stage methods commonly achieve inferior segmentation accuracy and incomplete activation of targets, owing to the failure to capture detailed contextual information from image-level labels [2,7].In contrast to one-stage methods, two-stage methods can perform favourably, as they leverage dense labels generated by the classification network to train a segmentation network [12]. For instance, Discriminative Region Suppression (DRS) [9] suppresses the attention on discriminative regions and expands it to adjacent less activated regions. However, these methods leverage the CAMs from the deep layer of the classification network and raise the inherent drawback, i.e., low resolution, leading to limited localization and smooth boundaries of objects. Han et al. [8] proposed multi-layer pseudo supervision to reduce the false positive rate in segmentation results, while the weights for pseudo masks from different layers are constants that cannot be adaptive. Besides, though the quality of CAMs improves, they are still insufficient to provide accurate object boundaries for segmentation. Numerous methods [6,10,11] have been proposed to explore boundary information. For example, Kolesnikov et al. [10] proposed a joint loss function that constrains the global weighted rank pooling and low-level object boundary to expand activation regions. AffinityNet [1] trains another network to learn the semantic similarity between pixels and then propagates the semantics to adjacent pixels via random walk. Nevertheless, these methods use the initial seeds generated from the CAM method, resulting in limited performance when the object-related seeds from CAM are small and sparse. Thus, improving the initial prediction and exploring boundary information are both important for accurate object segmentation.In this work, we propose a novel weakly-supervised method for accurate fetal brain segmentation using image-level labels. Our contribution can be summarized as follows: 1) We design an Uncertainty-weighted Multi-resolution CAM  "
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,2,Method,"An overview of our method is presented in Fig. 1. First, to obtain high-quality pseudo masks, Uncertainty-weighted Multi-resolution CAM (UM-CAM) is produced by fusing low-and high-resolution CAMs from different layers of the classification network. Second, seed points are obtained from the UM-CAM automatically, and used to generate Seed-derived Pseudo Labels (SPL) via Geodesic distance-based Seed Expansion (GSE). The SPL provides more detailed context information in addition to UM-CAM for training the final segmentation model."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,2.1,Psuedo Mask Generation Based on UM-CAM Initial Response via Grad-CAM.,"A typical classification network consists of convolutional layers as a feature extractor, followed by global average pooling and a fully connected layer as the output classifier [3]. Given a set of training images, the classification network is trained with class labels. After training, the Grad-CAM method is utilized to compute the weights α k for the k-th channel of a feature map f at a certain layer via gradient backpropagation from the output node for the foreground class. The foreground activation map A can be obtained from a weighted combination of feature maps and followed by a ReLU activation [20], which is formulated as:where y is classification prediction score for the foreground. i is pixel index, and N is the pixel number in the image."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Multi-resolution Exploration and Integration.,"The localization map for each image typically provides discriminative object parts, which is insufficient to provide supervision for the segmentation task. As shown in Fig. 1(a), the activation maps generated from the shallow layers of the classification network contain high-resolution semantic features but suffer from noisy and dispersive localization. In contrast, activation maps generated from the deeper layers perform smoother localization but lack high-resolution information. To take advantage of activation maps from shallow and deep layers, we proposed UM-CAM to integrate the multi-resolution CAMs by uncertainty weighting. Let us denote a set of activation maps from M convolutional blocks as A = {A m } M m=0 . Each activation map is interpolated to the input size and normalized by its maximum to the range of [0,1], and the normalized activation maps are Â = { Âm } M m=0 . To minimize the uncertainty of pseudo mask, UM-CAM integrates the confident region of multi-resolution CAMs adaptively, which can be presented as the entropy-weighted combination of CAMs:where Âb m and Âf m represent the background and foreground probability of Âm , respectively. w m is the weight map for Âm , and P UM is the UM-CAM for the target."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,2.2,Robust Segmentation with UM-CAM and SPL,"Though UM-CAM is better than the CAM from the deep layer of the classification network, it is still insufficient to provide accurate object boundaries that are important for segmentation. Motivated by [13], we propose a Geodesic distancebased Seed Expansion (GSE) method to generate Seed-derived Pseudo Label (SPL) that contains more detailed context information. The SPL is combined with UM-CAM to supervise the segmentation model, as shown in Fig. 1 (b).Concretely, we adopt the centroid and the corner points of the bounding box obtained from UM-CAM as the foreground seeds S f and background seeds S b , respectively. To efficiently leverage these seed points, SPL is generated via Exponential Geodesic Distance (EGD) transform of the seeds, leading to a foreground cue map P f SP L and a background cue map P b SP L . The values of P b SP L and P f SP L represent the similarity between each pixel and background/foreground seed points, which can be computed as:where P i,j is the set of all paths between pixels i and j. D b (i) and D f (i) represent the minimal geodesic distance between target pixel i and background/foreground seed points, respectively. p is one feasible path and it is parameterized by n ∈ [0, 1]. u (n) = p (n) / p (n) is a unit vector that is tangent to the direction of the path.Based on the supervision from UM-CAM and SPL, the segmentation network can be trained by minimizing the following joint object function:where P p is the prediction of the segmentation network, and λ is a weight factor to balance the supervision of UM-CAM and SPL. L CE is the Cross-Entropy (CE) loss."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,3,Experiments and Results,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,3.1,Experimental Details,"Dataset. We collected clinical T2-weighted MRI data of 115 pregnant women in the second trimester with Single-shot Fast Spin-echo (SSFSE). The data were acquired in axial view with pixel size between 0.5547 mm × 0.5547 mm and 0.6719 mm × 0.6719 mm and slice thickness between 6.50 mm and 7.15 mm. Each slice was resampled to a uniform pixel size of 1 mm × 1 mm. In all experiments, Implementation Details. To boost the generalizability, we applied spatial and intensity-based data augmentation during the training stage, including gamma correction, random rotation, random flipping, and random cropping. For 2D classification, we employed VGG-16 [23] as backbone architecture, in which an additional convolutional layer is used to substitute the last three fully connected layers. The classification network was trained with 200 epochs using CE loss. Stochastic Gradient Descent (SGD) optimizer was used for training with batch size 32, momentum 0.99, and weight decay 5 × 10 -4 . The learning rate was initialized to 1 × 10 -3 . We used UNet [18] as the segmentation network. The learning rate was set as 0.01, and the SGD optimizer was used for training with 200 epochs, batch size 12, momentum 0.9, and weight decay 5 × 10 -4 . The hyper-parameter M and λ were set as 4 and 0.1 based on the best results on the validation set, respectively. We used Dice Similarity Coefficient (DSC) and 95% Hausdorff Distance (HD 95 ) to evaluate the quality of 2D pseudo masks and the final segmentation results in 3D space."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,3.2,Ablation Studies,"Stage1: Quality of Pseudo Masks Obtained by UM-CAM. To evaluate the effectiveness of UM-CAM, we compared different pseudo mask generation strategies: 1) Grad-CAM (baseline): only using CAMs from the last layer of the classification network generated by using Grad-CAM method [20], 2) Average-CAM: fusing multi-resolution CAMs via averaging, 3) UM-CAM: fusing multi-resolution CAMs via uncertainty weighting.  evaluation results of these methods, in which the segmentation is converted from CAMs using the optimal threshold found by grid search method. It can be seen that when fusing the information from multiple convolutional layers, the quality of pseudo masks improves. The proposed UM-CAM improves the average DSC by 4.52% and 1.60% compared with the baseline and Average-CAM, respectively. Figure 2 shows a visual comparison between CAMs obtained by the different methods. It can be observed that there are fewer false positive activation regions of UM-CAM compared with the other methods."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Stage2: Training Segmentation Model with UM-CAM and SPL.,"To investigate the effectiveness of SPL, we compared it with several segmentation models: 1) Grad-CAM (baseline): only using the pseudo mask generated from Grad-CAM to train the segmentation model, 2) UM-CAM: only using UM-CAM as supervision for the segmentation model, 3) SPL: only using SPL as supervision, 4) UM-CAM+SPL: our proposed method using UM-CAM and SPL supervision for the segmentation model. Quantitative evaluation results in the second section of Table 1 show that the network trained with UM-CAM and SPL supervision achieves an average DSC score of 89.76%, improving the DSC by 11.07% compared to the baseline model. Figure 3 depicts a visual comparison between these models. It shows that SPL supervision with context information can better discriminate the fetal brain from the background, leading to more accurate boundaries for segmentation."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,3.3,Comparison with State-of-the-art Methods,"We compared three CAM invariants with our UM-CAM in pseudo mask generation stage, including Grad-CAM++ [4], Score-CAM [25], and Ablation-CAM [17]. Table 2 shows the quantitative results of these CAM variants. It can be seen that GradCAM++, Score-CAM, and Ablation-CAM achieve similar performance, which is consistent with the visualization results shown in Fig. 2. The proposed UM-CAM achieves higher accuracy than existing CAM variants, which generates more accurate boundaries that are closer to the ground truth. We further compared the proposed method with fully supervised method (FullySup) and two state-of-the-art weakly-supervised methods, including DRS [9] that spreads the attention to adjacent non-discriminative regions by suppressing the attention on discriminative regions, and AMR [16] that incorporates a spotlight branch and a compensation branch to dig out more complete object regions. Table 2 lists the segmentation results of these methods. Our proposed method achieves an average DSC of 90.22% and an average HD 95 of 4.04 pixels, which is at least 3.02 pixels lower than the other weakly-supervised methods on the testing set. It indicates that the proposed method can generate segmentation results with more accurate boundaries. Figure 3 shows some qualitative visualization results. The DRS and AMR predictions appear to be coarse and inaccurate in the boundary regions, while our proposed method generates more accurate segmentation results, even similar to those generated from the fully supervised model for some easy samples."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,4,Conclusion,"In this paper, we presented an uncertainty and context-based method for fetal brain segmentation using image-level supervision. An Uncertainty-weighted Multi-resolution CAM (UM-CAM) was proposed to integrate multi-resolution activation maps via uncertainty weighting to generate high-quality pixel-wise supervision. We proposed a Geodesic distance-based Seed Expansion (GSE) method to produce Seed-derived Pseudo Labels (SPL) containing detailed context information. The SPL is combined with UM-CAM for training the segmentation network. The proposed method was evaluated on the fetal brain segmentation task, and experimental results demonstrated the effectiveness of the proposed method and suggested the potential of our proposed method for obtaining accurate fetal brain segmentation with low annotation cost. In the future, it is of interest to validate our method with other segmentation tasks and apply it to other backbone networks."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Fig. 1 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,(,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Fig. 2 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Fig. 3 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Table 1 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Table 2 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,1,Introduction,"Alzheimer's disease (AD) is a progressive and debilitating neurological disorder that affects millions of people worldwide. Although primary detection of AD can be achieved through a combination of cognitive function tests and neuroimaging techniques, such as magnetic resonance imaging (MRI) and cerebrospinal fluid (CSF) analysis [14]. However, these approaches suffer from being invasive, timeconsuming, or expensive, hindering their use in routine clinical practice. The convergence of tissue origin, structural characteristics, and functional mechanisms between the eyes and the brain has been previously reported [18]. For example, patients with AD have significantly decreased blood vessel density in superficial parafoveal and choriocapillaris (CC) [25]. To this end, the automated AD detection using fundus image has emerged as an active research field in the last two years [1,13,20]. Color fundus photography (CFP) has commonly used for AD studies, but the CFP has limitations in capturing the information of deep layer vessels. Optical coherence tomography angiography (OCTA) is an innovative non-invasive technology that generates high-resolution images of depth-resolved retinal microvasculature projections [8], including SVC, DVC, and CC.Studies on clinical biomarkers of OCTA images are mainly based on regional analysis, e.g., the early treatment of diabetic retinopathy study (ETDRS) grid, which divides a target area into 9 regions with three concentric circles and two orthogonal lines, as shown in the right three sub-figures in Fig. 1. The regionbased analysis allows a more specific evaluation of retinal changes and their correlation with AD, which can provide a more nuanced understanding of the disease. Research following ETDRS and IE grid demonstrated the significance of many regions, e.g., in the three sub-regions of nasal-outer, superior-inner, and inferior-inner in inner vascular complexes, which present a substantial decrease in vascular area density and vascular length density for the AD participants [23].Over the past few years, deep-learning-based algorithms have achieved remarkable success in the analysis of medical images. As for AD detection, several methods use an integration of multiple modalities [20,21]. However, these methods rarely follow the clinical region-based analysis routine, which limits their ability to incorporate valuable clinical statistical findings and generate easily interpretable results. To address the above issues, we proposed a novel deep-learning framework to take full advantage of clinical region-based analysis, for AD detection in OCTA images. To obtain a more accurate and interpretable result, we specifically designed an approximate sector convolution, based on the polar transformation and a multi-kernel feature extraction module. The main contributions of the paper can be summarised as follows: (1) Based on the wellknown clinically used ETDRS grids for retinal image analysis, we incorporate the regional importance prior in the training process through a weight matrix, so as to better understand the correlations between retinal structure alterna-  We introduce an approximate sector convolution through polar transformation, to mimic the clinical region-based analysis, by mapping the OCTA image from the Cartesian system to the polar system, as shown in the left two sub-figures in Fig. 1. (3) We further performed the explainability analysis on the well-trained model. The interpretable results showed consistency with the conclusions of the previous clinical studies, indicating that the proposed method can be a potential tool, to investigate the pathological evidence of the relationship between the fundus and AD."
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,2,Methodology,"Figure 1 shows the flowchart of our AD detection method using SVC, DVC, and CC projections of OCTA as input. First, we utilize VAFF-Net [6] to locate the center of the FAZ on SVC. We then transform the original images into the polar coordinates with the FAZ center as the origin. The transformed images are then fed into our Polar-Net, which produces the final detection result and the corresponding region importance matrix."
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,2.1,Polar Coordinate Transformation for OCTA Image,"We introduce a method called polar transformation, to realize region-based analysis. As shown in Fig. 2, the polar transformation converts the region of interest (blue circle) into a polar coordinate system Fig. 2(c), with the center of the FAZ (according to the definition [4] of ETDRS), O c (u o , v o ) as the origin. The original image is represented as points in the Cartesian system p(u, v), and the corresponding points in the polar system are represented by p (θ, r). The relationship between these two coordinate systems is given by the following equations: The width of the transformed image is equal to the distance R, the minimal length from the center O c (u o , v o ) to the edge in the original image, and the height is 2πR. Since the corners are cropped, the outermost pixels of the region of interest are kept in order to preserve the original information as much as possible, and the part near O c is filled by nearest neighbor interpolation. The polar transformation represents the original image in the polar coordinate system by pixel-wise mapping [3], and has the following properties:1) Approximate Sector-Shaped Convolution. Convolution is widely used in convolutional neural networks (CNNs), where the shape of the convolution kernel is always rectangular. However, in the real world, many semantics are non-rectangular, such as circle and sector, which makes the adaptability of the receptive field in CNNs suboptimal. For the polar transformation, the mapping relationship is fixed, enabling us to approximate the sector convolution with a rectangular convolution kernel at a lower computational cost. The mapping relationship shown in Fig. 2(b)(d) explains this, and for the sake of simplicity and clarity, we use ETDRS girds as an example. When we perform convolution with a rectangular kernel along the T I → SI direction on the transformed image, it is equivalent to performing convolution with a sector-shaped kernel counterclockwise around the FAZ center in the original image.2) Equivalent Augmentation. Applying data augmentation to the original image is the same as applying data augmentation in the polar system since the transformation is a pixel-wise mapping [3]. For instance, by changing the start angle and the transformation center O c (u o , v o ), we can realize the drift cropping operation in the polar system. It is analogous to applying various cropping factors for data augmentation by changing the transformation radius R. "
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,2.2,Network Architecture,"In the transformed image, we can extract features around the FAZ. Rectangular features at different scales correspond to different sectoral features in the retina. Therefore, it is critical to extract information across different sizes of the visual field. To this end, we design the Polar-Net. As shown in Fig. 3, it contains several branches, the number of which varies according to the number of projections.Each branch starts with a polar feature extractor module (PFEM) and ends with a residual network. To take full advantage of all the branches, middle fusion is used. To generate the region importance matrix, a polar region importance module (PRIM) is proposed, which follows the residual network. Furthermore, Polar-Net can receive a prior knowledge matrix to utilize clinical knowledge."
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Polar Feature Extractor Module (PFEM):,"To extract shallow features in different views, we propose PFEM, which consists of a multi-kernel atrous convolution module (MKAC), a multi-kernel pooling module (MKPM), and a convolutional block attention module (CBAM) [22]. For each projection x i , MKAC H (•) applies multiple scale atrous convolutions to enlarge the field of view [24], and MKPM G (•) applies a series of max-pooling operations with different pooling kernels to discover microscopic changes, by extracting the most salient feature [9]. Finally, CBAM T (•) is applied to exploit the inter-channel hidden information of features. During feature extraction, W n is used to adaptively adjust the weights of the above processes. The mathematical notation of the above is:(2)"
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Polar Region Importance Module (PRIM):,"To calculate the region importance, we implement PRIM by applying an average pooling after a Grad-CAM [16]. In order to capture the importance of feature map k for class c, we denote a c k as the gradient of the score for class c, with respect to feature map activations A k of the last residual layer. The region importance matrix L c RI is given by:"
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,3,Experiments and Results,"Data Description: An in-house dataset was conducted for this study. It includes 199 images from 114 AD patients and 566 images from 291 healthy subjects. All data were collected with the approval of the relevant authorities and the consent of the patients, following the Declaration of Helsinki. All the patients conform to the standards of the National Institute on Aging and Alzheimer's Association (NIA-AA). The images were captured with a swept-source OCTA (VG200S, SVision Imaging). The images were captured in a 3 × 3 mm 2 area centered on the fovea. We make sure that the images from a single patient will only be used as training or testing sets once. In the cross-validation experiment subset, we sample the categories from each dataset at the same ratio. "
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Model ACC AUROC Kappa,"ResNet-34 [7] 0.8125 ± 0.0267 0.7960 ± 0.0479 0.4909 ± 0.0717 EfficientNet-B3 [17] 0.7942 ± 0.0104 0.7908 ± 0.0157 0.4177 ± 0.0267 ConvNeXt-S [12] 0.7562 ± 0.0138 0.5903 ± 0.0313 0.1660 ± 0.0437 HorNet-S GF [15] 0.7602 ± 0.0113 0.5921 ± 0.0286 0.1738 ± 0.0458 VAN-B6 [5] 0.7707 ± 0.0124 0.6911 ± 0.0298 0.2939 ± 0.0485 ViT-Base [2] 0.7904 ± 0.0183 0.7726 ± 0.0286 0.3641 ± 0.0715 SwinV2-T [11] 0.7601 ± 0.0117 0.7528 ± 0.0343 0.3242 ± 0.0448 MUCO-Net [20] 0 Nvidia RTX 3090 GPUs. We employed Adam as the optimizer with an initial learning rate of 2e-5 and a batch size of 28. We also applied data augmentation by randomly rotating the images by ± 20 • around their centers. The model was trained for 200 epochs. During the transformation, we considered the difference between the left and right eyes and used nearest-neighbor interpolation.The width of the transformed images was resized to 224 pixels. Five-fold crossvalidation was employed to fully utilize the data and make the results more reliable. Since there is no standard way to convert existing prior knowledge into matrices, for prior knowledge, we manually generated a 4 × 2 weight matrix according to the study [23]. The weights were 1 by default. The regions with p-values less than 0.05 had a weight of 1.5, and regions with p-values less than 0.01 had a weight of 2. For the entire DVC, the weight was set to 2."
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Evaluation and Interpretability Assessment:,"We evaluate the performance of the model on the test set using the accuracy score (ACC), area under the receiver operating characteristic (AUROC), and kappa. To evaluate the performance, we compared our method with several state-of-the-art methods in the computer vision field and one in the AD detection field. Table 1 shows that our method outperforms the others in ACC, AUROC, and Kappa, with an improvement of up to 4.07%, 5.63%, and 9.08%, respectively. Prior knowledge did not bring much performance improvement, partly due to the crude method of generating the prior matrices, and partly probably due to the fact that the network's adaptive algorithm may have already learned similar prior knowledge. During the testing phase, we activated PRIM and generated a 4 × 2 importance matrix for the entire testing set. An inverse operation of the polar transformation was applied to generate the importance map.For the sake of simplicity, here we take the EDTRS grid for analysis. As shown in Fig. 4 (a), it can be seen that globally, the importance of CC is highest and DVC is the second. This matches the findings that AD patients have a considerably lower density in choriocapillaris flow [25]. Meanwhile, the significance of DVC coincides with research findings that there is a considerable reduction in vascular area density and other factors in DVC [23]. In the DVC and CC, the parafovea is more important. This may relate to the loss of ganglion cells in the parafoveal retina [19]. For single projection (Fig. 4 (b)), different regions have different importance and the contributions of NI, SI and II (illustrated in Fig. 2) are higher. In summary, we found a pattern that high importance always occurs where there are more micro-vessels, such as the CC, DVC, and the parafovea. This finding coincides with the conclusion that the microvasculature of the brain and retina is significantly decreased in AD patients [25]. Our interpretable results roughly match the clinical study results, because we have made the network follow the clinical analysis method. This also proves the clinical relevance of ours. The minor difference is perhaps because the network unearthed the high-dimensional features that have not yet been discovered clinically.Ablation Study: To evaluate the effectiveness of the polar transformation and Polar-Net, we performed an ablation study. To validate the proposed Polar-Net, we removed the PFEM. The results are shown at the bottom of Table 1. To  ViT-Base [2] w/o trans 0.7904 ± 0.0183 0.7726 ± 0.0286 0.3641 ± 0.0715 w trans 0.7982 ± 0.0177 0.8025 ± 0.0509 0.4235 ± 0.0748 MUCO-Net [20] w/o trans 0.7968 ± 0.0369 0.7773 ± 0.0414 0.3985 ± 0.0789 w trans 0.7916 ± 0.0255 0.7956 ± 0.0500 0.4271 ± 0.0659 validate the transformation, we used the transformed images and the original images respectively. The results are shown in Table 2. All the results showed the effectiveness of the proposed components and modules.Extended Experiment: To further verify our detection method's stability and generalisability, we conducted an additional experiment on a public dataset OCTA-500 [10]. It contains 189 images from 29 subjects with diabetic retinopathy and 160 healthy control. The details of the implementation are the same as the experiments on the in-house dataset. As shown in Table 3, our method achieved the best performances compared to the competitors. "
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,4,Conclusion,"In this paper, we propose a novel framework for AD detection using retinal OCTA images, leveraging clinical prior knowledge and providing interpretable results. Our approach involves polar transformation, allowing for the use of approximate sector convolution and enabling the implementation of the regionbased analysis. Additionally, our framework, called Polar-Net, is designed to acquire the importance of the corresponding retinal region, facilitating the understanding of the model's decision-making process in detecting AD and assessing its conformity to clinical observations. We evaluate the performance of our method on both private and public datasets, and the results demonstrate that Polar-Net outperforms state-of-the-art methods. Importantly, our approach produces clinically interpretable results, providing a potential tool for disease research to investigate the underlying pathological mechanisms. Our work presents a promising approach to using OCTA imaging for AD detection. Furthermore, we highlight the importance of incorporating clinical knowledge into AI models to improve interpretability and clinical applicability."
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Fig. 1 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Fig. 2 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Fig. 3 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Fig. 4 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Table 1 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Table 2 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Table 3 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,1,Introduction,"Breast cancer is the most common cancer and the leading cause of cancer death in women [18]. Early detection of breast cancer allows patients to receive timely treatment, which may have less burden and a higher probability of survival [6]. Among current clinical imaging modalities, magnetic resonance imaging (MRI) has the highest sensitivity for breast cancer detection [12]. Especially, contrastenhanced MRI (CE-MRI) can identify tumors well and has become an indispensable technique for detecting and defining cancer [13]. However, the use of gadolinium-based contrast agents (GBCA) requires iv-cannulation, which is a burden to patients, time consuming and cumbersome in a screening situation. Moreover, contrast administration can lead to allergic reactions and finaly CE-MRI may be associated with nephrogenic systemic fibrosis and lead to bioaccumulation in the brain, posing a potential risk to human health [4,9,[14][15][16]. In 2017, the European Medicines Agency concluded its review of GBCA, confirming recommendations to restrict the use of certain linear GBCA used in MRI body scans and to suspend the authorization of other contrast agents, albeit macrocyclic agents can still be freely used [10].With the development of computer technology, artificial intelligence-based methods have shown potential in image generation and have received extensive attention. Some studies have shown that some generative models can effectively perform mutual synthesis between MR, CT, and PET [19]. Among them, synthesis of CE-MRI is very important as mentioned above, but few studies have been done by researchers in this area due to its challenging nature. Li et al. analyzed and studied the feasibility of using T1-weighted MRI and T2-weighted MRI to synthesize CE-MRI based on deep learning model [11]. Their results showed that the model they developed could potentially synthesize CE-MRI and outperform other cohort models. However, MRI source data of too few sequences (only T1 and T2) may not provide enough valuable informative to effectively synthesize CE-MRI. In another study, Chung et al. investigated the feasibility of using deep learning (a simple U-Net structure) to simulate contrast-enhanced breast MRI of invasive breast cancer, using source data including T1-weighted non-fatsuppressed MRI, T1-weighted fat-suppressed MRI, T2-weighted fat-suppressed MRI, DWI, and apparent diffusion coefficient [5]. However, obtaining a complete MRI sequence makes the examination costly and time-consuming. On the other hand, the information provided by multi-sequences may be redundant and may not contain the relevant information of CE-MRI. Therefore, it is necessary to focus on the most promising sequences to synthesize CE-MRI.Diffusion-weighted imaging (DWI) is emerging as a key imaging technique to complement breast CE-MRI [3]. DWI can provide information on cell density and tissue microstructure based on the diffusion of tissue water. Studies have shown that DWI could be used to detect lesions, distinguish malignant from benign breast lesions, predict patient prognosis, etc [1,3,7,8,17]. In particular, DWI can capture the dynamic diffusion state of water molecules to estimate the vascular distribution in tissues, which is closely related to the contrast-enhanced regions in CE-MRI. DWI may be a valuable alternative in breast cancer detection in patients with contraindications to GBCA [3]. Inspired by this, we develop a multi-sequence fusion network based on T1-weighted MRI and multi-b-value DWI to synthesize CE-MRI. Our contributions are as follows:i From the perspective of method, we innovatively proposed a multi-sequence fusion model, designed for combining T1-weighted imaging and multi-b-value DWI to synthesize CE-MRI for the first time. ii We invented hierarchical fusion module, weighted difference module and multi-sequence attention module to enhance the fusion at different scale, to control the contribution of different sequence and maximising the usage of the information within and across sequences. iii From the perspective of clinical application, our proposed model can be used to synthesize CE-MRI, which is expected to reduce the use of GBCA."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2,Methods,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.1,Patient Collection and Pre-processing,"This study was approved by Institutional Review Board of our cancer institute with a waiver of informed consent. We retrospectively collected 765 patients with breast cancer presenting at our cancer institute from January 2015 to November 2020, all patients had biopsy-proven breast cancers (all cancers included in this study were invasive breast cancers, and ductal carcinoma in situ had been excluded). The MRIs were acquired with Philips Ingenia All MRIs were resampled to 1 mm isotropic voxels and uniformly sized, resulting in volumes of 352 × 352 pixel images with 176 slices per MRI, and subsequent registration was performed based on Advanced Normalization Tools (ANTs) [2]."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.2,Model,"Figure 1 illustrates the structure of the proposed model. First, the reconstruction module is used to automatically encode and decode each input MRI sequence information to obtain the latent representation of different MRI sequences at multi-scale levels. Then, the hierarchical fusion module is used to extract the hierarchical representation information and fuse them at different scales. In each convolutional layer group of the reconstruction module, we use two 3 × 3 filters (same padding) with strides 1 and 2, respectively. The filters are followed by batch normalization, and after batch normalization, the activation functions LeakyReLU (with a slope of 0.2) and ReLU are used in the encoder and decoder, respectively. The l 1 -norm is used as a reconstruction loss to measure the difference between the reconstructed image and the ground truth.Figure 2 shows the detailed structure of the hierarchical fusion module, which includes two sub-modules, a weighted difference module and a multi-sequence attention module. The calculation of the apparent diffusion coefficient (ADC) map is shown in Eq. 1, which provides a quantitative measure of observed diffusion restriction in DWIs. Inspired by ADC, a weighted difference module is designed, in which the neural network is used to simulate the dynamic analysis of the ln function, and the element-wise subtraction algorithm is used to extract the differentiation features between DWIs with different b-values, and finally the features are weighted to obtain weighted feature maps (F DWI , Eq. 2)."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,ADC =ln (S,"where S l and S h represent the image signals obtained from lower b value b l and higher b h , f θ l and f θ h represent the corresponding neural networks for DWI with a lower and higher b value.In the multi-sequence attention module, a channel-based attention mechanism is designed to automatically apply weights (A s ) to feature maps (F concat ) from different sequences to obtain a refined feature map (F concat ), as shown in Eq. 3. The input feature maps (F concat ) go through the maximum pooling layer and the average pooling layer respectively, and then are added element-wise after passing through the shared fully connected neural network, and finally the weight map A s is generated after passing through the activation function, as shown in Eq. 4.where ⊗ represents element-wise multiplication, ⊕ represents element-wise summation, σ represents the sigmoid function, θ fc represents the corresponding network parameters of the shared fully-connected neural network, and AvgP ool and M axP ool represent average pooling and maximum pooling operations, respectively.In the synthesis process, the generator G tries to generate an image according to the input multi-sequence MRI (d 1 , d 2 , d 3 , d 4 , t 1 ), and the discriminator D tries to distinguish the generated image G(d 1 , d 2 , d 3 , d 4 , t 1 ) from the real image y, and at the same time, the generator tries to generate a realistic image to mislead the discriminator. The generator's objective function is as follows:and the discriminator's objective function is as follows:where pro data (d 1 , d 2 , d 3 , d 4 , t 1 ) represents the empirical joint distribution of inputs d 1 (DW I b0 ), d 2 (DW I b150 ), d 3 (DW I b800 ), d 4 (DW I b1500 ) and t 1 (T1weighted MRI), λ 1 is a non-negative trade-off parameter, and l 1 -norm is used to measure the difference between the generated image and the corresponding ground truth. The architecture of the discriminator includes five convolutional layers, and in each convolutional layer, 3 × 3 filters with stride 2 are used. Each filter is followed by batch normalization, and after batch normalization, the activation function LeakyReLU (with a slope of 0.2) is used. The numbers of filters are 32, 64, 128, 256 and 512, respectively."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.3,Visualization,"The T1-weighted images and the contrast-enhanced images were subtracted to obtain a difference MRI to clearly reveal the enhanced regions in the CE-MRI. If the CE-MRI was successfully synthesized, the enhanced region would be highlighted in the difference MRI, otherwise it would not. "
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.4,Experiment Settings,"Based on the ratio of 8:2, the training set and independent test set of the in-house dataset have 612 and 153 cases, respectively. The trade-off parameter λ 1 was set to 100 during training, and the trade-off parameter of the reconstruction loss in the reconstruction module is set to 5. Masks for all breasts were used (weighted by a factor of 100 during the calculation of the loss between generated and real CE-MRI) to reduce the influence of signals in the thoracic area. The batch was set to 8 for 100 epochs, the initial learning rate was 1e-3 with a decay factor of 0.8 every 5 epochs (total run time is about 60 h). Adam optimizer was applied to update the model parameters. MMgSN-Net [11] and the method of Chung et al. [5] were used as cohort models, and all models were trained on NVIDIA RTX A6000 48 GB GPU."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.5,Evaluation Metrics,"Results analysis was performed by Python 3.7. Structural Similarity Index Measurement (SSIM), Peak Signal-to-Noise Ratio (PSNR) and Normalized Mean Squared Error (NMSE) were used as metrics, all formulas as follows:P SNR = 10 log 10 max 2 (y(x), G(x)) where G(x) represents a generated image, y(x) represents a ground-truth image, μ y(x) and μ G(x) represent the mean of y(x) and G(x), respectively, σ y(x) and σ G(x) represent the variance of y(x) and G(x), respectively, σ y(x)G(x) represents the covariance of y(x) and G(x), and c 1 and c 2 represent positive constants used to avoid null denominators."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,3,Results,"First, we compare the performance of different existing methods on synthetic CE-MRI using our source data, The quantitative indicators used include PSNR, SSIM and NMSE. As shown in Table 1, the SSIM of MMgSN-Net [11] and the method of Chung et al. [5] in synthesizing ceT1 MRI is 86.61 ± 2.52 and 87.58 ± 2.68, respectively, the PSNR is 26.39 ± 1.38 and 27.80 ± 1.56, respectively, and the NMSE is 0.0982 ± 0.038 and 0.0692 ± 0.035, respectively. In contrast, our proposed multi-sequence fusion model achieves better SSIM of 89.93 ± 2.91, better PSNR of 28.92 ± 1.63 and better NMSE of 0.0585 ± 0.026 in synthesizing ceT1 MRI, outperforming existing cohort models.MMgSN-Net [11] combined T1-weighted and T2-weighted MRI in their work to synthesize CE-MRI. Here we combined T1-weighted MRI and DWI with a bvalue of 0 according to their method, but the model did not perform well. It may be because their model can only combine bi-modality and cannot integrate the features of all sequences, so it cannot mine the difference features between multiple b-values, which limits the performance of the model. In addition, although the method of Chung et al. [5] used full-sequence MRI to synthesize CE-MRI, it would be advantageous to obtain synthetic CE-MRI images using as little data as possible, taking advantage of the most contributing sequences. They did not take advantage of multi-b-value DWI, nor did they use the hierarchical fusion module to fully fuse the hierarchical features of multi-sequence MRI.  The visualization results of random samples are shown in Fig. 3. It can be seen from the visualization results that after the difference between the generated CE-MRI and the original T1-weighted MRI, the lesion position of the breast is highlighted, the red circle represents the highlighted area, which proves that our method can effectively synthesize contrast-enhanced images, highlighting the same parts as the real enhanced position. See Supplementary Material for more visualization results, including visualizations of breast CE-MRI synthesized in axial, coronal, and sagittal planes."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,4,Conclusion,"We have developed a multi-sequence fusion network based on multi-b-value DWI to synthesize CE-MRI, using source data including DWIs and T1-weighted fatsuppressed MRI. Compared to existing methods, we avoid the challenges of using full-sequence MRI and aim to be selective on valuable source data DWI. Hierarchical fusion generation module, weighted difference module, and multisequence attention module have all been shown to improve the performance of synthesizing target images by addressing the problems of synthesis at different scales, leveraging differentiable information within and across sequences. Given that current research on synthetic CE-MRI is relatively sparse and challenging, our study provides a novel approach that may be instructive for future research based on DWIs. Our further work will be to conduct reader studies to verify the clinical value of our research in downstream applications, such as helping radiologists on detecting tumors. In addition, synthesizing dynamic contrastenhanced MRI at multiple time points will also be our future research direction. Our proposed model can potentially be used to synthesize CE-MRI, which is expected to reduce or avoid the use of GBCA, thereby optimizing logistics and minimizing potential risks to patients."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Fig. 1 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Fig. 2 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Fig. 3 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Table 1 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Table 2 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,.93 ± 2.91 28.92 ± 1.63 0.0585 ± 0.026,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 8.
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,1,Introduction,"Fetal magnetic resonance imaging (MRI) is becoming increasingly common, supplementing ultrasound for clinical decision-making and planning. It has a wide range of functional contrasts, a higher resolution than ultrasound, and can be used from approximately 16 weeks of gestation until birth. The renewed interest in low-field MRI (0.55T) and the increasing availability of commercial low field scanners carries significant advantages for fetal MRI: Low-field MRIs allow a wider bore (widening access to this tool for the increasingly obese pregnant population) while maintaining field homogeneity at the lower field strength, and generally do not require helium for cooling. Low field MRI is especially advantageous for fetal functional imaging (often performed using Echo-Planar-Imaging) as the reduced susceptibility-induced artefacts and longer T2* times allow for longer read-outs and hence more efficient acquisitions. It therefore provides an excellent environment for fetal body T2* mapping [1].T2* maps of the fetus provide an indirect measurement of blood oxygenation levels due to the differing relaxation times of deoxygenated and oxygenated hemoglobin [12,13]. Many fetal body organs including the lungs, kidneys, heart, liver, spleen have changing T2* values throughout gestation [2,15], indicating that quantitative measurements of fetal body T2* organs have the potential to be a clinically useful measurement.However, the wider use of T2* relaxometry in the clinical setting is currently limited by significant methodological barriers such as quality of the data, fetal motion and time-consuming manual segmentations. As a consequence, it has mainly been used in research settings focused on the brain and placenta [3,6,14]. Studies to date have acquired only low-resolution images of the fetal body, which do not allow for proper imaging of small organs such as the adrenal gland, and they are still susceptible to motion artifacts. A lengthy echo time (TE) is required to acquire high-resolution multi-echo sequences. This is both impractical given the unpredictable motion and results in limited SNR on the later TEs. In structural fetal imaging, super-resolution algorithms have been used to transform several low-resolution scans into a single high-resolution volume [5,9,19]. High-resolution reconstructions of fetal T2* maps have only been done as proof of concept in phantoms [10] or in the brain [3]. Once they are acquired, the images require manual reorientation to a standard plane followed by manual segmentation in order to be clinically useful. The segmentations must also be very accurate, as inclusion of other organs or major vessels drastically changes the resulting T2* values. Overall, current state of T2* fetal body organ analysis involves a lengthy acquisition process followed by tedious manual image processing steps, resulting in a barrier to wider adoption of the technique in the clinical setting.Here, we present an automated pipeline for quantitative mean T2* fetal body organs at low field MRI, resulting in normative growth curves from 17-40 weeks.We use a low-resolution dynamic T2* acquisition framework, and then use a novel multi-channel deformable slice-to-volume reconstruction (dSVR) to generate a high-resolution 3D volume of the fetal body and its corresponding T2* map in the standard plane [18][19][20], followed by automatic segmentation of the fetal body organs across a wide range of gestational ages (GA; 17-40 weeks). We generate normative T2* growth curves of ten fetal body organs at low field MRI, which has not been done at higher field strengths in such detail. This pipeline (available: https://github.com/meerkat-tools/Fetal-T2star-Recon) will pave the way for advanced T2* fetal body mapping to become more prevalent in both research and the clinic, potentially allowing further insights into prenatal development, and better screening and diagnostic capacities."
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,2,Methods,"Fetal MRI was acquired as part of an ethically approved study (MEERKAT, REC 21/LO/0742, Dulwich Ethics Committee, 08/12/2021) performed between May 2022 and February 2023 at St Thomas' Hospital in London, UK. Participants for this study were recruited prospectively, with inclusion criteria of a singleton pregnancy, maternal age over 18 years. Exclusion criteria were multiple pregnancies, maternal age <18 years, lack of ability to consent, weight >200 kg, and contraindications for MRI such as metal implants."
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,2.1,Image Acquisition,"The subjects were scanned on a clinical 0.55T scanner (MAGNETOM Free.Max, Siemens Healthcare, Germany) using a 6-element blanket coil and a 9-element spine coil in the supine position. A multi-echo whole-uterus dynamic (timeresolved) gradient echo sequence was acquired in the maternal coronal orientation. The sequence parameters are as follows: Field of View (FOV): 400 × 400 mm; resolution: 3.125 mm × 3.125 mm × 3 mm; TE: [46,120,194] ms; TR: 10,420-18,400 ms; Number of slices: 28-85 Number of dynamics: 15-30; GRAPPA: 2; flip angle 90 • . The acquisition time of the dynamic T2* relaxometry scan with 20 dynamics and 3 echoes was between 4-6 min."
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,2.2,Image Processing,"The acquired images were reviewed to remove dynamics with excess motion. The remaining images were first denoised using MRTRIX3's dwidenoise tool [4,17]. Next, an in-house Python script was used to generate the T2* maps for each dynamic using mono-exponential decay fitting for each voxel [7].Reconstructions. The image generated from the second echo was determined to have the best contract for the fetal body organs (an example of the echos can be seen in the Supplementary Material). This second echo image for each dynamic was then used to create a high-resolution 3D volume with isotropic resolution in a standard atlas space using deformable slice-to-volume (dSVR) registration, with the following non-default parameters: no intensity matching, no robust statistics, resolution = 1.2 mm, cp = [12 5], lastIter=0.015 [19,20]. The resulting transformations are then used to create a high resolution 3D T2* map [18].Fetal Lung Reconstruction Validation. In order to validate that the 3D volume reconstructions do not alter the obtained mean T2* values, mean T2* values from 10 of the acquired dynamics were compared with mean lung T2* value of the corresponding 3D volume. The values were considered to be equal if the mean T2* determined from the 3D volume was within two standard deviations of the mean T2* values sampled from the 10 dynamic scans.Segmentation. Initial segmentations of the second echo 3D volumes were generated with an existing in-house U-Net using the MONAI framework [11] that had been trained on dSVR reconstructions from T2-weighted (HASTE) images of the fetal body acquired at 1.5T and 3T and corresponding manual segmentations for the lungs, thymus, gall bladder, kidney pelvis, kidney parenchyma, spleen, adrenal gland, stomach, bladder, and liver (See Fig. 1). Seven of the initially generated label maps were corrected in detail to create the ground truth.A 3D nnUNet [8] was trained on a Tesla V100 (5 folds, 250 epochs/fold, default nnUnet data augmentation, batch size: 2, kernel size: [3,3,3], Adam opti- mizer, loss function: cross-entropy and Dice, 23 training cases, 4 validation cases, 7 testing cases) using these initial label maps as well as the second echo and the T2* 3D volumes. The final label maps were generated using a semi-automated refinement process, where at the end of each training cycle the cases with the best label maps were chosen, minor corrections were made by fetal anatomy specialists (Intra and inter-observer variability had previously been confirmed [16] before retraining. Two networks were trained, one with only the second echo 3D volumes as input (one channel), and one with both the second echo and the T2* map 3D volumes (two channel). Dice Similarity Coefficients (DSC) were calculated for both the one-and a two-channel networks and a two-sided t-test was performed.Growth Curves. Organ-specific volume and T2* growth curves were created using control cases from the generated label maps and the high-resolution T2* volumes, excluding voxels where the T2* fitting failed. A linear regression analysis was performed in order to determine the relationship between the T2* values and GA.The complete fetal body T2* 3D reconstruction and segmentation processing pipeline can be seen in Fig. 2. "
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,3.1,Fetal Body T2* Reconstruction,"41 subjects had a multi-echo T2* dynamic scan (mean GA: 28.46 ± 6.78 weeks; mean body mass index: 29.0 ± 6.26). Dynamics with motion artifact were removed (mean included: 13.6). All cases underwent 3D volume reconstruction. Both control and pathological cases were used during the iterative training process for the segmentation network. Nine cases were excluded from the creation of growth curves due to a pathology impacting the fetal body. A further two cases were excluded after reconstruction due to excessive motion.Fetal Lung Reconstruction Validation. In the lung reconstruction validation experiment, all cases fell within the required range, and therefore are considered to be equal (Table 2). See Fig. 3 for an example of the individual dynamics and the final 3D volume reconstruction."
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,3.2,Fetal Body T2* Segmentation,The DSC scores of the one-and two-channel networks (Table 1) showed no significant difference. The two-channel network was used to generate the T2* values in the growth curves. 
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,3.3,Growth Curves,"The obtained T2* growth curves (Fig. 4) show a significant increase over gestation in the lungs and liver (R 2 > 0.5) and a significant decrease in the kidney parenchyma and kidney pelvis (R 2 > 0.5). A slight relationship with GA was also found in the spleen, adrenal glands, and gallbladder. No relationship with GA was found in the stomach, bladder, and thymus. The volumetric growth curves for all organs can be found in the Supplementary Material.Table 1. Mean Dice Similarity Coefficients (DSC) for each label. One Channel: Trained using the second echo 3D volume; Two Channel: Trained using both the second echo and the T2* 3D volume. No significant difference was found between the two networks. The data (including images, reconstructions, and segmentations) will be made available from the corresponding author upon reasonable academic request (REC 21/LO/0742). The proposed pipeline overcomes barriers currently hindering wider clinical adaption such as burdensome manual image reorientation and segmentation. It requires users only to acquire the images and review the input scans for motion before starting the pipeline. This automation would allow the pipeline to move advanced fetal image analysis outside of specialist centers and into a more standard workflow. The pipeline worked from 18-40 GA, with only 2 cases (which were <20 weeks) discarded due to motion, and was able to confirm literature trends for all organs except the liver and lungs [2,15], where the opposite trend was observed. This may be due to the different GA ranges included in the literature. The increasing mean T2* values in the lungs may be due to the increasing vascularity, thereby increasing the amount of fetal haemoglobin present. The long T2* values achieved with the low field MRI allows for excellent contrast in the fetal body, which assists in the reconstruction. This pipeline will provide -0.08), spleen (R 2 : 0.36), kidney pelvis (R 2 : -0.52), kidney parenchyma (R 2 : -0.9), bladder (R 2 : -0.07), thymus (R 2 : 0.09), adrenal glands (R 2 : -0.37), and gallbladder (R 2 : -0.40). Two of the graphs in the bottom row (middle, right) display the lung and kidney parenchyma volumes. All organ volumes demonstrated a strong relationship with GA. Blue: Fetuses with normal organs; Red: Fetuses with a pathology potentially impacting body organs.insight into the development of fetal body organs not yet explored, such as the adrenal glands. A more comprehensive study with more cases at every GA is needed to further validate these normative curves.While structural T2-weighted images are more suited for volumetry, the fact that all organs follow the expected growths trend further validates the segmentations. It also indicates that for some of the smaller organs (kidney pelvis, adrenal glands), our network has difficulties in the segmentation step for younger fetuses.Two organs (thymus, adrenal glands) had poor DSC values (below 0.45), indicating that further work on the segmentation network for these difficult organs is required. The thymus has very poor contrast and is difficult to delineate even manually. The network often identified heart tissue as thymus, skewing the T2* values calculated. The adrenal gland is a very small organ, which makes it difficult to segment. The poor DSC for the adrenal gland does not necessarily translate to incorrect average T2* values, as the DSC is a volumetric metric. However, improved DSC scores would allow for more confidence in the calculated T2* values.The proposed multi-organ pipeline can be successfully run across a wide range of GAs, and requires minimal user interaction. The combination of low field fetal MRI, quantitative imaging, and comprehensive image analysis pipelines could potentially make a substantial impact in our understanding of the development of the fetal body throughout gestation, as well as possibly provide clinical prenatal biomarkers."
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Fig. 1 .,
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Fig. 2 .,
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Fig. 3 .,
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Fig. 4 .,
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Table 2 .,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,1,Introduction,"Nasopharyngeal carcinoma (NPC), also known as lymphoepithelioma, is a highly aggressive malignancy that originates in nasopharynx [1]. NPC is characterized by a distinct geographical distribution in Southeast Asia, North Africa, and Arctic [2]. In China, NPC accounts for up to 50% of all head and neck cancers, while in Southeast Asia, NPC accounts for more than 70% of all head and neck cancers [3]. Radiotherapy (RT) is currently the main treatment remedy, which needs precise tumor delineation to ensure a satisfactory RT outcome. However, accurately delineating the NPC tumor is challenging due to the highly infiltrative nature of NPC and its complex location, which is surrounded by critical organs such as brainstem, spinal cord, temporal lobes, etc. To improve the visibility of NPC tumor for precise gross-tumor-volume (GTV) delineation, contrastenhanced MRI (CE-MRI) is administrated through injection of gadolinium-based contrast agents (GBCAs) during MRI scanning. Despite the superior tumor-to-normal tissue contrast of CE-MRI, the use of GBCAs during MRI scanning can result in a fatal systemic disease known as nephrogenic systemic fibrosis (NSF) in patients with renal insufficiency [4]. NSF can cause severe physical impairment, such as joint contractures of fingers, elbows, and knees, and can progress to involve critical organs such as the heart, diaphragm, pleura, pericardium, kidney, liver, and lung [5]. It was reported that the incidence rate of NSF is around 4% after GBCA administration in patients with severe renal insufficiency, and the mortality rate can reach 31% [6]. Currently, there is no effective treatment for NSF, making it crucial to find a CE-MRI alternative for patients at risk of NSF.In recent years, artificial intelligence (AI), especially deep learning, plays a gamechanging role in medical imaging [7,8], which showed great potential to eliminate the use of the toxic GBCAs through synthesizing virtual contrast-enhanced MRI (VCE-MRI) from gadolinium-free sequences, such as T1-weighted (T1w) and T2-weighted (T2w) MRI [9][10][11][12]. In 2018, Gong et al. [11] utilized pre-contrast and 10% low-dose T1w MRI to synthesize the VCE-MRI for brain disease diagnosis using a U-shape model, they found that gadolinium dose is able to be reduced by 10-fold by deep learning while the contrast information could be preserved. Followed by their work, Kleesiek et al. [10] proposed a Bayesian model to explore the feasibility of synthesizing VCE-MRI from contrast-free sequences, their study demonstrated that deep learning is highly feasible to totally eliminate the use of GBCAs. In the area of RT, Li et al. [9] developed a multi-input model to synthesize VCE-MRI for NPC RT. In addition to the advantage of eliminating the use of GBCA, VCE-MRI synthesis can also speed up the clinical workflow by eliminating the need for acquiring CE-MRI scan, which saves time for both clinical staff and patients. However, current studies mostly focus on algorithms development while lack comprehensive clinical evaluations to demonstrate the efficacy of the synthetic VCE-MRI in clinical settings.The clinical evaluation of AI-based techniques is of paramount importance in healthcare. Rigorous clinical evaluations can establish the safety and efficacy of AI-based techniques, identify potential biases and limitations, and facilitate the integration of clinical expertise to ensure accurate and meaningful results [13]. Furthermore, the clinical evaluation of AI-based techniques can help identify areas for improvement and optimization, leading to development of more effective algorithms.To bridge this bench-to-bedside research gap, in this study, we conducted a series of clinical evaluations to assess the effectiveness of synthetic VCE-MRI in NPC delineation, with a particular focus on assessment in VCE-MRI image quality and primary GTV delineation. This study has two main novelties: (i) To the best of our knowledge, this is the first clinical evaluation study of the VCE-MRI technique in RT; and (ii) multiinstitutional MRI data were included in this study to obtain more reliable results. The success of this study would fill the current knowledge gap and provide the medical community with a clinical reference prior to clinical application of the novel VCE-MRI technique in NPC RT."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2,Materials and Methods,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2.1,Data Description,"Patient data was retrospectively collected from three oncology centers in Hong Kong. This dataset included 303 biopsy-proven (stage I-IVb) NPC patients who received radiation treatment during 2012-2016. The three hospitals were labelled as Institution-1 (110 patients), Institution-2 (58 patients), and Institution-3 (135 patients), respectively. For each patient, T1w MRI, T2w MRI, gadolinium-based CE-MRI, and planning CT were retrieved. MRI images were automatically registered as MRI images for each patient were scanned in the same position. The use of this dataset was approved by the Institutional Review Board of the University of Hong Kong/Hospital Authority Hong Kong West Cluster (HKU/HA HKW IRB) with reference number UW21-412, and the Research Ethics Committee (Kowloon Central/Kowloon East) with reference number KC/KE-18-0085/ER-1. Due to the retrospective nature of this study, patient consent was waived. For model development, 288 patients were used for model development and 15 patients were used to synthesize VCE-MRI for clinical evaluation. The details of patient characteristics and the number split for training and testing of each dataset were illustrated in Table 1. Prior to model training, MRI images were resampled to 256*224 by bilinear interpolation [14] due to the inconsistent matrix sizes of the three datasets."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2.2,VCE-MRI Synthesis Network,"The multimodality-guided synergistic neural network (MMgSN-Net) was applied to learn the mapping from T1w MRI and T2w MRI to CE-MRI. The MMgSN-Net was a 2D network. The effectiveness of this network in VCE-MRI synthesis for NPC patients has been demonstrated by Li et al. in [9]. T1w MRI and T2w MRI were used as input and corresponding CE-MRI was used as learning target. In this work, we obtained 12806 image pairs for model training. Different from the original study, which used single institutional data for model development and utilized min-max value of the whole dataset for data normalization, in this work, we used mean and standard deviation of each individual patient to normalize MRI intensities due to the heterogeneity of the MRI intensities across institutions [15]. "
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2.3,Clinical Evaluations,"The evaluation methods used in this study included image quality assessment of VCE-MRI and primary GTV delineation. Two board-certified radiation oncologists (with 8 years' and 6 years' clinical experience, respectively) were invited to perform the VCE-MRI quality assessment and GTV delineation according to their clinical experience. Considering the clinical burden of oncologists, 15 patients were included for clinical evaluations. All clinical evaluations were performed on an Eclipse workstation (V5.0.10411.00, Varian Medical Systems, USA) with the same monitor, and the window/level can be adjusted freely by the oncologists. The results were obtained under the consensus of the two oncologists. (i) Distinguishability between CE-MRI and VCE-MRI. To evaluate the reality of VCE-MRI, oncologists were invited to differentiate the synthetic patients (i.e., image volumes that generated from synthetic VCE-MRI) from real patients (i.e., image volumes that generated from real CE-MRI). Different from the previous studies that utilized limited number (20-50 slices, axial view) of 2D image slices for reality evaluation [9,10], we used 3D volumes in this study to help oncologists visualize the inter-slice adjacent information. The judgement results were recorded, and the accuracy of each institution and the overall accuracy were calculated."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Image,"(ii) Clarity of tumor-to-normal tissue interface. The clarity of tumor-normal tissue interface is critical for tumor delineation, which directly affects the delineation outcomes. Oncologists were asked to use a 5-point Likert scale ranging from 1 (poor) to 5 (excellent) to evaluate the clarity of tumor-to-normal tissue interface. Paired two-tailed t-test (with a significance level of p = 0.05) was applied to analyses if the scores obtained from real patients and synthetic patients are significantly different. (iii) Veracity of contrast enhancement in tumor invasion risk areas. In addition to the critical tumor-normal tissue interface, the areas surrounding the NPC tumor will also be considered during delineation. To better evaluate the veracity of contrast enhancement in VCE-MRI, we selected 25 tumor invasion risk areas according to [16], including 13 high-risk areas and 12 medium-risk areas, and asked oncologists to determine whether these areas were at risk of being invaded according to the contrast-enhanced tumor regions. The 13 high-risk areas include: retropharyngeal space, parapharyngeal space, levator veli palatine muscle, prestyloid compartment, Tensor veli palatine muscle, poststyloid compartment, nasal cavity, pterygoid process, basis of sphenoid bone, petrous apex, prevertebral muscle, clivus, and foramen lacerum. The 12 medium-risk areas include foramen ovale, great wing of sphenoid bone, medial pterygoid muscle, oropharynx, cavernous sinus, sphenoidal sinus, pterygopalatine fossa, lateral pterygoid muscle, hypoglossal canal, foramen rotundum, ethmoid sinus, and jugular foramen. The areas considered at risk of tumor invasion were recorded.The Jaccard index (JI) [17] was utilized to quantitatively evaluate the results of recorded risk areas from CE-MRI and VCE-MRI. The JI could be calculated by:where R CE and R VCE represents the set of risk areas that recorded from CE-MRI and corresponding VCE-MRI, respectively. JI measures similarity of two datasets, which ranges from 0% to 100%. Higher JI indicates more similar of the two sets.(iv) Efficacy in primary tumor staging. A critical RT-related application of CE-MRI is tumor staging, which plays a critical role in treatment planning and prognosis prediction [18]. To assess the efficacy of VCE-MRI in NPC tumor staging, oncologists were asked to determine the stage of the primary tumor shown in CE-MRI and VCE-MRI. The staging results from CE-MRI were taken as the ground truth and the staging accuracy of VCE-MRI was calculated.Primary GTV Delineation. GTV delineation is the foremost prerequisite for a successful RT treatment of NPC tumor, which demands excellent precision [19]. An accurate tumor delineation improves local control and reduce toxicity to surrounding normal tissues, thus potentially improving patient survival [20]. To evaluate the feasibility of eliminating the use of GBCA by replacing CE-MRI with VCE-MRI in tumor delineation, oncologists were asked to contour the primary GTV under assistance of VCE-MRI. For comparison, CE-MRI was also imported to Eclipse for tumor delineation but assigned as a different patient, which were shown to oncologists in a random and blind manner.To mimic the real clinical setting, contrast-free T1w, T2w MRI and corresponding CT of each patient were imported into the Eclipse system since sometimes T1w and T2w MRI will also be referenced during tumor delineation. Due to both real patients and synthetic patients were involved in delineation, to erase the delineation memory of the same patient, we separated the patients to two datasets, each with the same number of patients, both two datasets with mixed real patients and synthetic patients without overlaps (i.e., the CE-MRI and VCE-MRI from the same patient are not in the same dataset).When finished the first dataset delineation, there was a one-month interval before the delineation of the second dataset. After the delineation of all patients, the Dice similarity coefficient (DSC) [21] and Hausdorff distance (HD) [22] of the GTVs delineated from real patients and corresponding synthetic patients were calculated to evaluate the accuracy of delineated contours.Dice Similarity Coefficient (DSC). DSC is a broadly used metric to compare the agreement between two segmentations [23]. It measures the spatial overlap between two segmentations, which ranges from 0 (no spatial overlap) to 1 (complete overlap). The DSC can be expressed as:where C CE and C VCE represent the contours delineated from real patients and synthetic patients, respectively."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Hausdorff Distance (HD).,"Even though DSC is a well-accepted segmentation comparison metric, it is easily influenced by the size of contours. Small contours typically receive lower DSC than larger contours [24].Therefore, HD was applied as a supplementary to make a more thorough comparison. HD is a metric to measure the maximum distance between two contours. Given two contours C CE and C VCE , the HD could be calculated as:where d(x, C VCE ) and d(y, C CE ) represent the distance from point x in contour C CE to contour C VCE and the distance from point y in contour C VCE to contour C CE . (i) Distinguishability between CE-MRI and VCE-MRI. The overall judgement accuracy for the MRI volumes was 53.33%, which is close to a random guess accuracy (i.e., 50%). For Institution-1, 2 real patients were judged as synthetic and 1 synthetic patient was considered as real. For Institution-2, 2 real patients were determined as synthetic and 4 synthetic patients were determined as real. For Institution-3, 2 real patients were judged as synthetic and 3 synthetic patients were considered as real. In total, 6 real patients were judged as synthetic and 8 synthetic patients were judged as real. (ii) Clarity of tumor-to-normal tissue interface. The overall clarity scores of tumorto-normal tissue interface for real and synthetic patients were 3.67 with a median of 4 and 3.47 with a median of 4, respectively. No significant difference was observed between these two scores (p = 0.38). The average scores for real and synthetic patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for Institution-1, Institution-2, and Institution-3, respectively. 5 real patients got a higher score than synthetic patients and 3 synthetic patients obtained a higher score than real patients. The scores of the other 7 patient pairs were the same. (iii) Veracity of contrast enhancement in tumor invasion risk areas. The overall JI score between the recorded tumor invasion risk areas from CE-MRI and VCE-MRI was 74.06%. The average JI obtained from Institution-1, Institution-2, and Institution-3 dataset were similar with a result of 71.54%, 74.78% and 75.85%, respectively. In total, 126 risk areas were recorded from the CE-MRI for all of the evaluation patients, while 10 (7.94%) false positive high risk invasion areas and 9 (7.14%) false negative high risk invasion areas were recorded from VCE-MRI. (iv) Efficacy in primary tumor staging. A T-staging accuracy of 86.67% was obtained using VCE-MRI. 13 patient pairs obtained the same staging results. For the Institution-2 data, all synthetic patients observed the same stages as real patients."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,3,Results and Discussion,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,3.1,Image Quality of VCE-MRI,"For the two T-stage disagreement patients, one synthetic patient was staged as phase IV while the corresponding real patient was staged as phase III, the other synthetic patient was staged as I while corresponding real patient was staged as phase III.   "
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,3.2,Primary GTV delineation,"The average DSC and HD between the C CE and C VCE was 0.762 (0.673-0.859) with a median of 0.774, and 1.932 mm (0.763 mm-2.974 mm) with a median of 1.913 mm, respectively. For Institution-1, Institution-2, and Institution-3, the average DSC were 0.741, 0.794 and 0.751 respectively, while the average HD were 2.303 mm, 1.456 mm, and 2.037 mm respectively. Figure 2 illustrated the delineated primary GTV contours from an average patient with the DSC of 0.765 and HD of 1.938 mm. The green contour shows the primary GTV that delineated form the synthetic patient, while the red contour was delineated from corresponding real GBCA-based patient. "
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,4,Conclusion,"In this study, we conducted a series of clinical evaluations to validate the clinical efficacy of VCE-MRI in RT of NPC patients. Results showed the VCE-MRI has great potential to provide an alternative to GBCA-based CE-MRI for NPC delineation."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Figure 1,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Fig. 1 .,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Fig. 2 .,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Table 1 .,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Table 2,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Table 2 .,quality evaluation results of VCE-MRI: (A) Distinguishability between CE-MRI and VCE-MRI; (B) Clarity of tumor-to-normal tissue interface; (C) Veracity of contrast enhancement in risk areas; and (D) T-staging. Abbreviations: Inst: Institution; C.A.: Center-based average; O.A.: Overall average; Syn: Synthetic.
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,1,Introduction,"Fluorescein Angiography (FA) is a commonly utilized imaging modality for detecting and diagnosing fundus diseases. It is widely used to image vascular structures and dynamically observe the circulation and leakage of contrast agents in blood vessels. Recently, the emergence of Ultra-Wide-angle Fundus (UWF) imaging has enabled its combination with FA and Scanning Laser Ophthalmoscopy (SLO), namely UWF-FA and UWF-SLO. The UWF-FA imaging enables simultaneous and high-contrast angiographic images of all 360 • C of the mid and peripheral retina [1,4,21]. However, both FA and UWF-FA require injecting a fluorescent dye (i.e., sodium fluorescein) into the anterior vein of the patient's hand or elbow, which then passes through the blood circulation to the fundus blood vessels. Some patients may experience adverse reactions such as vomiting and nausea during or after the examination. Moreover, it is not suitable for patients with serious cardiovascular and other systemic diseases.Cross-modality medical image generation provides a new method for solving the aforementioned problems. Multi-scale feature maps from different input modalities usually have similar structures. Hence, different contrasts can be merged to generate target images based on multimodal deep learning to provide more information for diagnosis [15]. Recently, the generative adversarial networks (GANs) [5] and their variants have made breakthroughs in this field. The idea of PatchGAN [12] was proposed to synthesize clearer images. Liu et al. [14] proposed an end-to-end multi-input and multi-output deep adversarial learning network for MR image synthesis. Xiao et al. [22] proposed a Transfer-GAN model by combining transfer learning and GANs to generate CT highresolution images. By merging multi-scale generators, these networks can explore fine and coarse features from images [10]. Kamran et al. [9] proposed a semisupervised model called VTGAN introducing transformer module into discriminators, helping the synthesis of vivid images. However, previous methods yielded lower-resolution situations and most discriminators can only take squared inputs (width equal to height) [18]. Moreover, misaligned data and lower attention on disease-related regions as well as the correctness of synthesized lesions remain significant issues. Furthermore, the highly non-linear relationship between different modalities makes the mapping from one modality to another difficult to learn [23].In this paper, we present the Ultra-Wide-Angle Transformation GAN (UWAT-GAN), a supervised conditional GAN capable of generating UWF-FA from UWF-SLO. To address the image misalignment issue, we employ an automated image registration method and integrate the idea of pix2pixHD [20] to use multi-scale discriminators. In addition, we use the multi-scale generators to synthesize high-resolution images as well as improve the ability to capture tiny vascular lesion areas and employ multiple new weighted losses on different scales of data to optimize model training. For evaluation metrics, we use Fréchet Inception Distance (FID) [6], Kernel Inception Distance (KID) [11], Inception Score (IS) [2] and Learned Perceptual Image Patch Similarity (LPIPS) [24] to quantify the quality of images. Finally, we compare UWAT-GAN with the state-of-the-art image synthesis frameworks [3,7,20] for qualitative assessment and conduct an ablation study. Our main contributions are summarised as follows:1). To the best of our knowledge, we present the first study to synthesize UWF-FA from UWF-SLO, overcoming the limitations of UWF-FA imaging. 2). We propose a novel UWAT-GAN utilizing multi-scale generators and multiple new weighted losses on different data scales to synthesize high-resolution images with the ability to capture tiny vascular lesion areas. 3). We assess the performance of the UWAT-GAN on a clinical in-house dataset and adopt an effective preprocessing method for image sharpening and registration to enhance the clarity of vascular regions and tackle the misalignment problem. 4). We demonstrate the superiority of the proposed UWAT-GAN against the state-of-the-art models through extensive experiments, comparisons, and ablation studies."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2,Methodology,"We propose a supervised conditional GAN for synthesizing UWF-FA from UWF-SLO images, as illustrated in Fig. 1. In order to achieve the desired outcome, we propose the concept of a fine-coarse level generator in whole architecture (Sect."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,2.1) and a fusion module works on result of both level generators (Sect. 2.2).,"Then the Attention Transmit Module is put forward to improve the U-Net-like architecture (Sect. 2.3). Additionally, we provide a comprehensive description of up-down sampling process and architecture of multi-scaled discriminators (Sec. 2.4). Eventually, we discuss the proposed loss function terms and their impacts in detail (Sect. 2.5). "
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.1,Overall Architecture,"UWF-FA has global features such as eyeballs and long-thick blood vessels, as well as local features such as small lesions and capillary blood vessels. However, generating images with both global and local features using a single generator is challenging. To address this issue, we devise two different levels of generators.The coarse generator Gen C extracts global information and generates a result based on this information, while the fine generator Gen F extracts local information. The results of global and local information can be used, alternately, as a reference for each other. Hence, this allows the extraction and utilization of both global and local information. In Fig. 1, the original image is fed into the entire model. After down-sampling, the image is passed into Gen C . Then, we extract a patch from the original image as an input to Gen F as described in Sect. 2.3. Both generators share the down-sample residual block and attention concatenated modules. Note that both generators generate a UWF-FA image and pass it to the discriminators. However, the output of Gen F is the one we considered in the later experiments."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.2,Patch and Fusion Module,"In Fig. 1, Gen F receives a randomly cropped patch as the input instead of the original image. This is because directly inputting the original image would occupy a large amount of memory and significantly reduce the training speed. Therefore, we only feed one cropped patch of the original image to Gen F in each step. In Fig. 2(A), a fusion block takes both patches from Gen F and Gen C . To get the same region from Gen F and Gen C , we resized the images to the same size, and cropped the patches from the same position. These two patches are concatenated at the depth level and passed into a two-layer fusion operation."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.3,Attention Transmit Module,"In Fig. 2, the attention module is designed based on U-Net-like [16] architecture, whose sampling process can provide more information to the decoder. Whereas, when synthesizing UWF-FA from UWF-SLO, the information density of the source image is low. For instance, the eye sockets in the periphery of the image are sparsely distributed blood vessels in some areas. Therefore, completely passing the graphs from the down-sampling process to the up-sampling process is not appropriate. Subsequently, images that pass through Attention Transmit Module can first extract useful information so that the decoder uses this information, efficiently. The multi-head attention [19] and the CNN-Attention blocks are shown in Fig. 3. "
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.4,Generator and Discriminator Architectures,"After conducting multiple experiments, we choose three down-sample layers for Gen F and two down-sample layers for Gen C . In addition, each generator includes an initial block, down-sample block, up-sample block, residual block, and attention transmit module, which are shown in Fig. 3. The initial block contains a reflection padding, a 2D convolution layer, and the Leaky-Relu activation function. The down/up-sampling blocks consist of a 2D convolution/transposed layer and the activation function combined with normalization. Additionally, the multi-scale discriminator in pix2pixHD [20] is employed to evaluate the output of the generator. For generator Gen X , the first discriminator D X1 takes the original and generated image P 1 while the second discriminator D X2 takes the down-sampled version of P 1 . Although theoretically, a multi-level discriminator can be applied by generating an image pyramid for an image, we use D C1 and D C2 for Gen C , and D F for Gen F in our framework."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.5,Proposed Loss Function,"Denote the two generators Gen C and Gen F as G C and G F , the three discriminators as D C1 , D C2 , and D F , and the paired variables {(c i , x i )}, where c represents the distribution of original input as a condition and x represents the distribution of ground truth (i.e., real UWF-FA image). Given the conditional distribution c, we aim to maximize the loss of D C1 , D C2 , and D F while minimizing the loss of Gen C and Gen F using the following objective function:where L cGAN is given by:We adopt the feature mapping (FM) loss [20] in our framework. Firstly, we collect the target images and their translated counterparts as a pair of images. Then, we split the discriminators into multiple layers and obtain the output from each layer. Denote D (i) as the ith-layer to extract the feature, the loss function is then defined as:where T is the total number of layers and N i represents each layer's number of elements. (e.g., convolution, normalization, Leaky-Relu means three elements). Minimizing this loss ensures that each layer can extract the same features from the paired images. Additionally, we use the perceptual loss [8] in our framework, which is utilized by a pretrained VGG19 network [17], to extract the features from the paired images and it is defined as:where N represents the total number of layers, M i denotes the elements in each layer, and V i is the ith-layer of the VGG19 network. The final cost function is as follows:where λ F MC , λ V GGC , λ F MF , λ V GGF indicate adjustable weight parameters.3 Experiments and Results"
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,3.1,Data Preparation and Preprocess,"In our experiments, we utilized an in-house dataset of UWF images obtained from a local hospital, comprising UWF-FA and UWF-SLO images. The UWF-SLO are in 3-channel RGB format, whereas the UWF-FA images are in 1-channel format. Each image pair was collected from a unique patient. However, from a clinical perspective, images taken with an interval of more than one day or those with noticeable fresh bleeding were excluded. Additionally, images that contain numerous interfering factors affecting their quality were also discarded. After the quality check, we have 70 paired images with the size of 3900 × 3072, of which 70% were randomly allocated for training and 30% for testing, respectively. Furthermore, we employed image sharpening through histogram equalization to enhance the clarity of images. We then utilized automated image registration software, i2k Retina Pro, to register each pair of images which changed the image size. To standardize the size of each image, we resized the registered images to 2432 × 3702. Subsequently, we randomly cropped the resized images with a size of 608 × 768 into different patches. And 50 patches could be obtained for each image. Finally, we adopted data augmentation using random flip and rotation to increase the number of training images from 49 pairs to 1960 pairs."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,3.2,Implementation Details,"All our experiments were conducted on the PyTorch 1.12 framework and carried out on two Nvidia RTX 3090Ti GPUs. Our model was trained from scratch to 200 epochs. The parameters were optimized by the Adam optimizer algorithm [13] with learning rate α = 0.0002, β 1 = 0.5 and β 2 = 0.999. We used a batch size of 2 to train our model and set λ F MF = λ F MC = λ V GGF = λ V GGC = 10 (Eq. 5)."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,3.3,Comparisons,"We first compared the performance of our model with some state-of-the-art GAN-based models including: Pix2pix [7], Pix2pixHD [20] and StarGAN-v2 [3]. For a fair comparison, we took the default parameters of the open-source codes of the competing methods, ensuring that the data volume matched the number of training cycles. We used the F ID(↓), KID(↓), LP IP S(↓) and IS(↑) to evaluate the generated UWF-FA. Table 1 shows the generation performance of different methods. Overall, our method achieves the best in all metrics compared to other models. The Pix2Pix attained the worst performance in all evaluation metrics while PixPixHD and StarGAN had comparable performance. In general, our method outperformed the competing methods and improved FID, KID, IS, and LPIPS by at least 24.47%, 39.95%, 3.59%, and 14.04%, respectively. Although StarGAN-v2 yielded the second-best performance, it is still less comparable with the proposed UWAT-GAN due to the lesion generation module which could capture tiny image details and improve overall performance (see Fig. 4).  "
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,3.4,Ablation Study,"To evaluate the significance of the attention transmit module proposed in UWAT-GAN, we trained our model with and without this module, namely M A and M NA , respectively. Unlike the generated images of M A , we found that M NA was not so distinctive as some vessels were missing and some interference of eyelashes was incorrectly considered as vessels. In Fig. 4, we showed the original pair of UWF-SLO and UWF-FA images and the generated images with M A and M NA . It is clear that the proposed method can generate good images and preserve small details. It becomes more distinctive when the attention module was used, as shown in the enlarged view of the red rectangle. It is also obvious that the FID and KID scores were improved by 22.25% and 18.46%, respectively."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,4,Discussion and Conclusion,"To address the potential adverse effects of fluorescein injection during FA, we propose UWAT-GAN to synthesize UWF-FA from UWF-SLO. Our method can generate high-resolution images and enhance the ability to capture small vascular lesions. Comparison and ablation study on an in-house dataset demonstrate the superiority and effectiveness of our proposed method. However, our model still has a few limitations. First, not every pair of images can be registered since some paired images may have fewer available features, making registration difficult. Second, our model's accuracy in synthesizing very tiny lesions is not optimal, as some lesions cannot be well generalized. Third, the limited size of our dataset is relatively small and may affect the model performance. In the future, we aim to expand the size of our dataset and explore the use of the object detection model, especially for small targets, to push our model pay more attention to some lesions. After further validation, we aim to adopt this method as an auxiliary tool to diagnose and detect fundus diseases."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Fig. 1 .,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Fig. 2 .,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Fig. 4 .,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Table 1 .,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Clinical Applications -Vascular,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,1,Introduction,"Lung cancer is the leading cause of cancer death in the United States, and early detection is key to improving survival rates. CT lung cancer screening is a lowdose CT (LDCT) scan of the chest that can detect lung cancer at an early stage, when it is most treatable. However, the current workflow for performing CT lung scans still requires an experienced technician to manually perform pre-scanning steps, which greatly decreases the throughput of this high volume procedure. While recent advances in human body modeling [4,5,12,13,15] have allowed for automation of patient positioning, scout scans are still required as they are used by automatic exposure control system in the CT scanners to compute the dose to be delivered in order to maintain constant image quality [3].Since LDCT scans are obtained in a single breath-hold and do not require any contrast medium to be injected, the scout scan consumes a significant portion of the scanning workflow time. It is further increased by the fact that tube rotation has to be adjusted between the scout and actual CT scan. Furthermore, any patient movement during the time between the two scans may cause misalignment and incorrect dose profile, which could ultimately result in a repeat of the entire process. Finally, while minimal, the radiation dose administered to the patient is further increased by a scout scan.We introduce a novel method for estimating patient scanning parameters from non-ionizing 3D camera images to eliminate the need for scout scans during pre-scanning. For LDCT lung cancer screening, our framework automatically estimates the patient's lung position (which serves as a reference point to start the scan), the patient's isocenter (which is used to determine the table height for scanning), and an estimate of patient's Water Equivalent Diameter (WED) profiles along the craniocaudal direction which is a well established method for defining Size Specific Dose Estimate (SSDE) in CT imaging [8,9,11,18]. Additionally, we introduce a novel approach for updating the estimated WED in real-time, which allows for refinement of the scan parameters during acquisition, thus increasing accuracy. We present a method for automatically aborting the scan if the predicted WED deviates from real-time acquired data beyond the clinical limit. We trained our models on a large collection of CT scans acquired from over 60, 000 patients from over 15 sites across North America, Europe and Asia. The contributions of this work can be summarized as follows:-A novel workflow for automated CT Lung Cancer Screening without the need for scout scan -A clinically relevant method meeting IEC 62985:2019 requirements on WED estimation. -A generative model of patient WED trained on over 60, 000 patients.-A novel method for real-time refinement of WED, which can be used for dose modulation"
Automated CT Lung Cancer Screening Workflow Using 3D Camera,2,Method,"Water Equivalent Diameter (WED) is a robust patient-size descriptor [17] used for CT dose planning. It represents the diameter of a cylinder of water having the same averaged absorbed dose as the material contained in an axial plane at a given craniocaudal position z [2]. The WED of a patient is thus a function taking as input a craniocaudal coordinate and outputting the WED of the patient at that given position. As WED is defined in an axial plane, the diameter needs to be known on both the Anterior-Posterior (AP) and lateral (Left-Right) axes noted respectively W ED AP (z) and W ED L (z). As our focus here is on lung cancer screening, we define 'WED profile' to be the 1D curve obtained by uniformly sampling the WED function along the craniocaudal axis within the lung region.Our method jointly predicts the AP and lateral WED profiles. While WED can be derived from CT images, paired CT scans and camera images are rarely available, making direct regression through supervised learning challenging. We propose a semi-supervised approach to estimate WED from depth images. First, we train a WED generative model on a large collection of CT scans. We then train an encoder network to map the patient depth image to the WED manifold. Finally, we propose a novel method to refine the prediction using real-time scan data."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.1,WED Latent Space Training,"We use an AutoDecoder [10] to learn the WED latent space. Our model is a fully connected network with 8 layers of 128 neurons each. We used layer normalization and ReLU activation after each layer except the last one. Our network takes as input a latent vector together with a craniocaudal coordinate z and outputs W ED AP (z) and W ED L (z), the values of the AP and lateral WED at the given coordinate. In this approach, our latent vector represents the encoding of a patient in the latent space. This way, a single AutoDecoder can learn patient-specific continuous WED functions. Since our network only takes the craniocaudal coordinate and the latent vector as input, it can be trained on partial scans of different sizes. The training consists of a joint optimization of the AutoDecoder and the latent vector: the AutoDecoder is learning a realistic representation of the WED function while the latent vector is updated to fit the data.During training, we initialize our latent space to a unit Gaussian distribution as we want it to be compact and continuous. We then randomly sample points along the craniocaudal axis and minimize the L1 loss between the prediction and the ground truth WED. We also apply L2-regularization on the latent vector as part of the optimization process."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.2,Depth Encoder Training,"After training our generative model on a large collection of unpaired CT scans, we train our encoder network on a smaller collection of paired depth images and CT scans. We represent our encoder as a DenseNet [1] taking as input the depth image and outputting a latent vector in the previously learned latent space. Our model has 3 dense blocks of 3 convolutional layers. Each convolutional layer (except the last one) is followed by a spectral normalization layer and a ReLU activation. The predicted latent vector is then used as input to the frozen AutoDecoder to generate the predicted WED profiles. We here again apply L2regularization on the latent vector during training."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.3,Real-Time WED Refinement,"While the depth image provides critical information on the patient anatomy, it may not always be sufficient to accurately predict the WED profiles. For example, some patients may have implants or other medical devices that cannot be guessed solely from the depth image. Additionally, since the encoder is trained on a smaller data collection, it may not be able to perfectly project the depth image to the WED manifold. To meet the strict safety criteria defined by the IEC, we propose to dynamically update the predicted WED profiles at inference time using real-time scan data. First, we use our encoder network to initialize the latent vector to a point in the manifold that is close to the current patient. Then, we use our AutoDecoder to generate initial WED profiles. As the table moves and the patient gets scanned, CT data is being acquired and ground truth WED can be computed for portion of the body that has been scanned, along with the corresponding craniocaudal coordinate. We can then use this data to optimize the latent vector by freezing the AutoDecoder and minimizing the L1 loss between the predicted and ground truth WED profiles through gradient descent. We can then feed the updated latent vector to our AutoDecoder to estimate the WED for the remaining portions of the body that have not yet been scanned and repeat the process.In addition to improving the accuracy of the WED profiles prediction, this approach can also help detect deviation from real data. After the latent vector has been optimized to fit the previously scanned data, a large deviation between the optimized prediction and the ground truth profiles may indicate that our approach is not able to find a point in the manifold that is close to the data. In this case, we may abort the scan, which further reduces safety risks. Overall flowchart of the proposed approach is shown in Fig. 1."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3,Results,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.1,Data,"Our CT scan dataset consists of 62, 420 patients from 16 different sites across North America, Asia and Europe. Our 3D Camera dataset consists of 2, 742 pairs of depth image and CT scan from 2, 742 patients from 6 different sites across North America and Europe acquired using a ceiling-mounted Kinect 2 camera. Our evaluation set consists of 110 pairs of depth image and CT scan from 110 patients from a separate site in Europe."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.2,Patient Preparation,"Patient positioning is the first step in lung cancer screening workflow. We first need to estimate the table position and the starting point of the scan. We propose to estimate the table position by regressing the patient isocenter and the starting point of the scan by estimating the location of the patient's lung top.Starting Position. We define the starting position of the scan as the location of the patient's lung top. We trained a DenseUNet [7] taking the camera depth image as input and outputting a Gaussian heatmap centered at the patient's lung top location. We used 4 dense blocks of 4 convolutional layers for the encoder and 4 dense blocks of 4 convolutional layers for the decoder. Each convolutional layer (except the last one) is followed by a batch normalization layer and a ReLU activation. We trained our model on 2, 742 patients using Adaloss [14] and the Adam [6] optimizer with a learning rate of 0.001 and a batch size of 32 for 400 epochs. Our model achieves a mean error of 12.74 mm and a 95 th percentile error of 28.32 mm. To ensure the lung is fully visible in the CT image, we added a 2 cm offset on our prediction towards the outside of the lung. We then defined the accuracy as whether the lung is fully visible in the CT image when using the offset prediction. We report an accuracy of 100% on our evaluation set of 110 patients. Third and fourth columns show the performance of our model with real-time refinement every 5 cm and 2 cm respectively. Ground truth is depicted in green and our prediction is depicted in red. While the original prediction was off towards the center of the lung, the real-time refinement was able to correct the error.Isocenter. The patient isocenter is defined as the centerline of the patient's body. We trained a DenseNet [1] taking the camera depth image as input and outputting the patient isocenter. Our model is made of 4 dense blocks of 3 convolutional layers. Each convolutional layer (except the last one) is followed by a batch normalization layer and a ReLU activation. We trained our model on 2, 742 patients using Adadelta [16] with a batch size of 64 for 300 epochs. On our evaluation set, our model outperforms the technician's estimates with a mean error of 5.42 mm and a 95 th percentile error of 8.56 mm compared to 6.75 mm and 27.17 mm respectively. Results can be seen in Fig. 2."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.3,Water Equivalent Diameter,"We trained our AutoDecoder model on our unpaired CT scan dataset of 62, 420 patients with a latent vector of size 32. The encoder was trained on our paired CT scan and depth image dataset of 2, 742 patients. We first compared our method against a simple direct regression model. We trained a DenseUNet [7] taking the camera depth image as input and outputting the Water Equivalent Diameter profile. We trained this baseline model on 2, 742 patients using the Adadelta [6] optimizer with a learning rate of 0.001 and a batch size of 32. We Table 1. WED profile errors on our testing set (in mm). 'w' corresponds to the portion size of the body that gets scanned before updating the prediction (in cm). Top of the table corresponds to lateral WED profile, bottom corresponds to AP WED profile. Updating the prediction every 20 mm produces the best results."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Method (lateral),"Mean then measured the performance of our model before and after different degrees of real-time refinement, using the same optimizer and learning rate. We report the comparative results in Table 1.We observed that our method largely outperforms the direct regression baseline with a mean lateral error 40% lower and a 90 th percentile lateral error over 30% lower. Bringing in real-time refinement greatly improves the results with a mean lateral error over 40% and a 90 th percentile lateral error over 20% lower than before refinement. AP profiles show similar results with a mean AP error improvement of nearly 40% and a 90 th percentile AP error improvement close to 30%. When using our proposed method with a 20 mm window refinement, our proposed approach outperforms the direct regression baseline by over 60% for lateral profile and nearly 80% for AP.Figures 3 highlights the benefits of using real-time refinement. Overall, our approach shows best results with an update frequency of 20 mm, with a mean lateral error of 15.93 mm and a mean AP error of 10.40 mm. Figure 4 presents a qualitative evaluation on patients with different body morphology.Finally, we evaluated the clinical relevancy of our approach by computing the relative error as described in the International Electrotechnical Commission (IEC) standard IEC 62985:2019 on Methods for calculating size specific dose estimates (SSDE) for computed tomography [2]. The Δ REL metric is defined as:where:-Ŵ ED(z) is the predicted water equivalent diameter -W ED(z) is the ground truth water equivalent diameter z is the position along the craniocaudal axis of the patient. IEC standard states the median value of the set of Δ REL (z) along the craniocaudal axis (noted Δ REL ) should be below 0.1. Our method achieved a mean lateral Δ REL error of 0.0426 and a mean AP Δ REL error of 0.0428, falling well within the acceptance criteria."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,4,Conclusion,"We presented a novel 3D camera based approach for automating CT lung cancer screening workflow without the need for a scout scan. Our approach effectively estimates start of scan, isocenter and Water Equivalent Diameter from depth images and meets the IEC acceptance criteria of relative WED error. While this approach can be used for other thorax scan protocols, it may not be applicable to trauma (e.g. with large lung resections) and inpatient settings, as the deviation in predicted and actual WED would likely be much higher. In future, we plan to establish the feasibility as well as the utility of this approach for other scan protocols and body regions.1 "
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Clinical Applications -Musculoskeletal,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Fig. 1 .,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Fig. 2 .,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Fig. 3 .,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Fig. 4 .,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,1,Introduction,"In-silico trials (ISTs) use computational modelling and simulation techniques with virtual twin or patient models of anatomy and physiology to evaluate the safety and efficacy of medical devices virtually [22]. Virtual patient populations (VPs), distinct from virtual twin populations, comprise plausible instances of anatomy and physiology that do not represent any specific real patient's data (as in the case of the latter, viz. virtual twins). In other words, VPs comprise synthetic data that help expand/enrich the diversity of anatomical and physiological characteristics that can be investigated within an IST for a given medical device. A key aspect of patient recruitment in real clinical trials used to assess device performance and generate regulatory evidence for device approval is the clear definition of inclusion and exclusion criteria for the trial. These criteria define the target patient population considered appropriate/safe to assess the performance of the device of interest. Consequently, it is desirable to enable the controlled synthesis of VPs that may be used for device ISTs, in a manner that emulates the imposition of trial inclusion and exclusion criteria.Virtual populations can be considered to be parametric representations of the anatomy sampled from a generative model. Traditional statistical shape models (SSMs), based on methods such as principal component analysis (PCA), have been widely explored in the past decade [8,9,15]. Recent studies focus on deep learning-based generative models due to their automatic and powerful hierarchical feature extraction [3,7]. For instance, Bonazzola et al. [3] used a graph convolutional variational auto-encoder (gcVAE) to learn latent representations of 3D left ventricular meshes and used the learnt representations as surrogates for cardiac phenotypes in genome-wide association studies. Dou et al. [7] proposed learning the shape representations of multiple cardiovascular anatomies using gcVAE independently and then assembling them into complete wholeheart anatomies termed virtual heart chimaeras. Other studies have investigated conditional-generative models for synthesis of VPs of anatomies. For example, Beetz et al. [1] employed a conditional VAE (cVAE), conditioned on gender and cardiac phase, to allow the synthesis of VPs from biventricular anatomies. In subsequent work [2,12], they extended their method to a multidomain VAE to model biventricular anatomies at multiple times (across the cardiac cycle), using patient-specific electrocardiogram (ECG) signals as additional conditioning information (in addition to patient demographic data and standard clinical measurements) to guide the synthesis. All aforementioned methods model the latent space in the VAEs/cVAEs as a multivariate Gaussian distribution with a diagonal covariance matrix. This limits the flexibility afforded to the cVAE, as the Gaussian distribution, being unimodal, is a poor approximation to multimodal latent posterior distributions. This in turn limits the overall variability in anatomical shape that can be captured by standard VAEs and cVAEs.In this study, we address the limitations of the state-of-the-art conditional generative models used to synthesise VPs of anatomical structures. In particular, we propose a method to relax the constraint on modelling the latent distribution as a unimodal multivariate Gaussian, to boost the flexibility of the generative model, and to enable conditional synthesis of diverse and plausible VPs generation. Recent advances in normalising flows [14,16,21] introduce a new solution for this limitation by leveraging a series of invertible parameterized functions to transform the unimodal distribution to a multimodal one. Motivated by this technique, we propose the first conditional flow VAE (parameterised as a graphconvolutional network) for the task of controllable synthesis of VPs of anatomy. The contributions are as follows: (i) we introduce normalising flows to learn a multimodal latent posterior distribution by transforming the latent variables from a simple unimodal distribution. This helps the generative model capture greater anatomical variability from the observed real population, leading to the synthesis of more diverse VPs; (ii) we condition the flow-based VAE on patient demographic data and clinical measurements. This enables conditional synthesis of plausible VPs (given relevant covariates/conditioning information as inputs), which reflect the observed correlations between nonimaging patient information and anatomical characteristics in the real population."
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,2,Methodology,"In this study, we propose a cVAE model equipped with normalising flows for controllable synthesis of VPs of cardiovascular anatomy. A schematic of the proposed conditional flow VAE network architecture is shown in Fig. 1. We employ normalising flows in the latent space of the cVAE to transform the initial Gaussian posterior to a complex multimodal distribution."
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Demographic Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Mesh Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Normalizing Flow,Planner Planner
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,… Demographic Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Concatenate,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Latent Flow Model,Conv Norm
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Demographic Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Conv ReLU Norm Residual,Basic Block 
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Mesh Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Conditional Variational Autoencoder:,"A VAE is a probabilistic generative model/network [11] that comprises an encoder and a decoder network branch. The encoder learns a mapping from the input data to a low-dimensional latent space that abstracts the semantic representations from the observations, and the decoder reconstructs the original data from the low-dimensional latent representation. The latent space from which the observed data is generated is given by approximating the posterior distribution of the latent variables using variational inference. The VAE network is trained by maximising the evidence lower bound (ELBO), which is a summation of the expected log-likelihood of the data and the Kullback-Leibler divergence between the approximate posterior and some assumed prior distribution over the latent variables (typically a multivariate Gaussian distribution). Despite its effectiveness in capturing some of the observed variability in the training population (e.g. of anatomical shapes or images), VAEs do not provide any control over the generation process and hence cannot guarantee that the generated population anatomical shapes are representative of target patient populations with specific inclusion/exclusion criteria. Controllable synthesis of anatomical VPs is essential for constructing meaningful cohorts for use in ISTs. Conditional VAE [18] is a VAE-variant that uses additional covariates/conditioning information in addition to the input data (e.g. anatomical shapes) to learn a conditional latent posterior distribution (conditioned on the covariates), enabling controllable synthesis of VPs during inference (given relevant covariates/conditioning information as input).Our conditional flow VAE (cVAE-NF) is a graph-convolutional network which takes as input a triangular surface mesh representation of an anatomical structure of interest, i.e., the Left Ventricle (LV) in this study, and its associated covariates/conditioning variables, i.e., the patient demographic data and clinical measurements, such as gender, age, weight, blood cholesterol, etc., and outputs the reconstructed surface mesh. Each mesh is represented by a list of 3D spatial coordinates of its vertices and an adjacency matrix defining vertex connectivity (i.e. edges of mesh triangles). The encoder and decoder contain five residual graph-convolutional blocks, respectively. Each block comprises two Chebyshev graph convolutions, each of which is followed by batch normalisation and ELU activation. A residual connection is added between the input and the output of each graph-convolutional block. Hierarchical mesh down/up-sampling operations proposed in CoMA [13] are adopted after each block to capture the global and local shape context. The VAE model is conditioned on covariates by scaling the hidden representations in the encoder similar to adaptive instance normalization [10] given the covariates as input to generate the scaling factor, and by concatenating the covariates with the latent variables before decoding.Flexible Posterior Using Normalizing Flow: Vanilla cVAEs model the approximate posterior distribution using Gaussian distributions with a diagonal covariance matrix. However, such a unimodal distribution is a poor approximation of the complex true latent posterior distribution in most real-world applications (e.g. for shapes of the LV observed across a population), limiting the anatomical variability captured by the model. In this study, we introduce normalising flows to construct a flexible multi-modal latent posterior distribution by applying a series of differentiable, invertible/diffeomorphic transformations iteratively to the initial simple unimodal latent distribution. As shown in Fig. 2, a two-dimensional Gaussian distribution can be transformed into a multi-modal distribution by applying several normalising flow steps to the former. Consider an invertible and smooth mapping function f : R d → R d with inverse f -1 = g, and a random variable z with distribution q(z). The transformed variable z = f (z) follows a distribution given by:where the det ∂f ∂z is the Jacobian determinant of f . Therefore, we can obtain a complex multi-modal density by composing multiple invertible mappings to transform the initial, simple and tractable density sequentially, as follows,The specific mathematical formulation of the normalising flow function is important and must be chosen with care to allow for efficient gradient computation during training, scalable inference, and efficiency in computing the determinant of the Jacobian. In this study, we leverage the planar flow in [16] as a basic unit of our latent normalising flow net. Specifically, each transformation unit is given by,where w ∈ R d , u ∈ R d and b ∈ R are learnable parameters; h(•) is a smooth element-wise non-linear function with derivative h (•) (we use tanh in our study) and z denotes the latent variables sampled from the posterior distribution. Therefore, we could compute the log determinant of the Jacobian term in O(D) time as follows:Finally, the network is trained by optimizing the modified ELBO based on Eq. 3:where, ln p(x|c) is the marginal log-likelihood of the observed data x (i.e. here x represents an LV graph/mesh), conditioned on the covariates of interest (i.e. patient demographics and clinical measurements) c; i is the steps of the normalizing flows. p(x|z i , c) is the likelihood of data parameterised by the decoder network, which reconstructs/predicts x given the latent variables z i , transformed by latent (planar) normalising flows, and the conditioning variables c; KL(q(z 0 |x) p(z i )) is the Kullback-Leibler divergence of the approximate posterior initial q(z 0 |x, c) from the prior, p(z) = N (z | 0, I)."
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,3,Experimental Setup and Results,"Data: In this study, we created a cohort of 2360 triangular meshes of the left ventricle (LV) based on a subset of cardiac cine-MR imaging data available from the UK Biobank (UKBB) by registering a cardiac LV atlas mesh [17] in manual contours (as described in [23]). We randomly split the data set into 422/59/1879 for training, validation, and testing, respectively. All meshes have the same and fixed graph topology, sharing the same edges and faces but differing in the position of vertices; i.e. there is pointwise correspondence across all shapes. We used 14 covariates available for the same subjects in UKBB as conditioning variables for our model, including, gender, age, height, weight, pulse, alcohol drinker status, smoking status, HbA1c, cholesterol, C-reactive protein, glucose, high-density lipoprotein cholesterol (HDL), insulin-like growth factor 1 (IGF-1), and low-density lipoprotein (LDL) cholesterol. These covariates were chosen because they are known cardiovascular risk factors."
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Implementation Details:,"The framework was implemented using PyTorch on a standard PC with a NVIDIA RTX 2080Ti GPU. We trained our model using the AdamW optimizer with an initial learning rate of 1e-3 and batch size of 16 for 1000 epochs. The feature number for each graph convolutional block in the encoder was 16, 32, 32, 64, 64, and in reverse order in the decoder. The latent dimension was set at 16. The down/up-sampling factor was four, and we used a warm-up strategy [19] to the weight of the KL loss to prevent model collapse."
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Evaluation Metrics:,"We compared our model (cVAE-NF) with a traditional PCA-based SSM, two generative models without conditioning information including a vanilla VAE and a VAE with normalising flow (VAE-NF) and the vanilla cVAE. Comparison of the vanilla cVAE can also validate the performance of existing approaches [1,2] because they are built on the cVAE with different covariates and basic units in the network. We evaluated the performance of all methods using three different metrics: 1) the reconstruction error, which evaluates the generalisability of the trained model to reconstruct/represent unseen shapes, using the distance between the reconstructed mesh with the ground truth/original mesh; 2) the specificity error, which measures the anatomical plausibility of the virtual cohorts synthesised, using the distance between the generated meshes and its nearest neighbour in the unseen real population [6]; and 3) the variability in the left ventricular volume in the synthesised cohorts, to assess the diversity of the instances generated in terms of a clinically relevant cardiac index. The variability in LV volume was quantified as the standard deviation of the volumes of LV blood pools (BPVols). The Euclidean distance was used to evaluate all three metrics. Additionally, we measured the activity of the latent dimension using the statistic A = Cov x (E z∼q(z|x) [z]) of the observations x [4]. A higher activity score indicates that a given latent dimension can capture greater population-wide shape variability. The results of our method are presented in Table 1. Our model outperforms the cVAE in terms of reconstruction error and the amount of volume variability captured in the synthesised VP (the reference volume variability for the real UKBB population was 33.38 mm 3 ). However, the cVAE achieved lower specificity errors than our model. This indicates that our method is better at capturing the population's shape variability, but it also creates some instances that are further away from the real population, resulting in higher specificity errors. We attribute this to the normalising flow's ability to learn a more flexible approximate posterior latent distribution of the observed shapes than the cVAE. This is also seen when comparing the performance of VAE and VAE-NF, where the latter can synthesise significantly more diverse VPs (e.g. it improves the volume variability from 3.00 to 16.03). Figure 3 shows the variability captured in each latent dimension. We observe that VAE-NF has higher activity scores in all latent dimensions compared to vanilla cVAE. The normalising flow allows for the approximation of multimodal latent distributions in the generative model, resulting in greater shape variability. Although PCA outperforms our method in terms of generalisation error and volume variability captured, it does not allow for controllable synthesis of VPs based on relevant patient demographic information and clinical measurements, making it less useful for our application of synthesising VPs for use in ISTs.It is essential to capture the distribution of clinically relevant biomarkers (e.g. BPVol) in the synthesised virtual populations (VPs) based on the specified covariates/conditioning information available for real patients, in order to effectively replicate the inclusion/exclusion criteria used during trial design in ISTs. For example, the BPVol of women is known to be lower than that of men [20].To verify this, we generated VPs using cVAE and our method, conditioned on real patient data (covariates) from the UK Biobank. Figure 3 summarises the BPVol distribution for both genders in the synthesised VPs and the real UKBB population, and the former accurately reflects the known trend of women having lower BPVol than men. Compared to cVAE, our model generates a VP that more closely matches the distribution of the volume of the LV blood pool observed in the real population. We also visualised the effect of manipulating individual attributes on two real patients in Fig. 4. We selected two representative attributes that are significantly associated with BPVol and myocardial volume (MyoVol): weight and age. We observe that BPVol and MyoVol of the  LV are positively correlated with the weight of the patients (as expected). On the other hand, increasing the individual's age results in a smaller BPVol, but an increased MyoVol (as visualised in Fig. 4), which is known to be due to cardiac hypertrophy caused by aging [5]."
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,4,Conclusion,"We proposed a conditional flow VAE model for the controllable synthesis of VPs of anatomy. Our approach was demonstrated to increase the flexibility of the learnt latent distribution, resulting in VPs that captured greater variability in the LV shape than the vanilla cVAE. Furthermore, our model was able to model the relationship between covariates/conditional variables and the shape of the LV, and synthesise target VPs that fit the desired criteria (in terms of demographics of the patient and clinical measurements) and closely matched the real population in terms of a clinically relevant biomarker (LV BPVol). These results suggest that our approach has potential for the controllable synthesis of diverse, yet plausible, VPs of anatomy. Future work will focus on modelling the whole heart and exploring the impact of individual covariates on VP synthesis in more detail."
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Fig. 1 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Fig. 2 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Fig. 3 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Fig. 4 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Table 1 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 14.
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,1,Introduction,"Accurate segmentation of the optic cup and optic disc in fundus images is essential for the cup-to-disc ratio measurement that is critical for glaucoma screening and detection [6]. Although deep neural networks have achieved great advances in medical image segmentation, they are susceptible to data with domain shifts, such as those caused by using different scanning devices or different hospitals [24]. Unsupervised domain adaptation [11] is proposed to transfer knowledge to the target domain with access to the source and target data while not requiring any annotation in the target domain. Recently, source-free unsupervised domain adaptation (SF-UDA) has become a significant area of research [5,10,14,15,19,20], where source data is inaccessible due to privacy or intellectual property concerns.Existing SF-UDA solutions can be categorized into four main groups: batch normalization (BN) statistics adaptation [16,17,23], approximating source images [9,26], entropy minimization [2], and pseudo-labeling [3,25]. BN statistics adaptation methods aim to address the discrepancy of statistics between different domains. For example, [16,17] update low-order and high-order BN statistics with distinct training objectives, while [23] adapts BN statistics to minimize the entropy of the model's prediction. Approximating source images aims to generate sourcelike images. For example, [26] first attains a coarse source image by freezing the source model and training a learnable image, then refines the image via mutual Fourier Transform. The refined source-like image provides a representation of the source data distribution and facilitates domain alignment during the adaptation process. For another instance, [9] learns a domain prompt to add to a target domain image so that the sum simulates the source image. Entropy minimization methods aim to produce more confident model predictions. For example, [2] minimizes output entropy with a regularizer of class-ratio. The class-ratio is estimated by an auxiliary network that is pre-trained on the source domain. For pseudo-labeling [12,29], erroneous pseudo-labels are either discarded or corrected. For example, [3] identifies low-confidence pseudo-labels at both the pixel-level and the class-level. On the other hand, [25] performs uncertainty-weighted soft label correction by estimating the class-conditional label error probability. However, all of these methods overlook context relations, which can enhance adaptation performance without the need to access the source data.We observe in our experiments (see Fig. 1 (a)) that domain gaps can result in the source model making context-inconsistent predictions. For neighboring patches of an image with similar visual appearance, the source model can yield vastly different predictions. This phenomenon can be explained by the observation in [15] that target data shifts in the feature space, causing some data points to shift across the boundary of the source domain segmentor. The issue of context inconsistency motivates us to utilize context relations in refining pseudo-labels. Moreover, it is observed in our experiments (as shown in Fig. 1(b)) that target features produced by the source model still form clusters, meaning that the features of target data points with the same class are closely located. This discovery led us to calculate context relations from feature distances; see Fig. 1(c).In this paper, we present a novel context-aware pseudo-label refinement (CPR) framework for source-free unsupervised domain adaptation. Firstly, we develop a context-similarity learning module, where context relations are computed from distances of features via a context-similarity head. This takes advantage of the intrinsic clustered feature distribution under domain shift [27,28], where target features generated by the source encoder are close for the same class and faraway for different classes (see Fig. 1 (b)). Secondly, context-aware revision is designed to leverage adjacent pseudo-labels for revising bad pseudo-labels, with aid of the learned context relations. Moreover, a calibration strategy is proposed, aiming to mitigate the negative effect brought about by the inaccurate learned context relations. Finally, the refined pseudo-labels are denoised with consideration of model knowledge and feature distribution [3,13] to select reliable pseudo-labels for domain adaptation. Experiments on cross-domain fundus image segmentation demonstrate our proposed framework outperforms the state-of-the-art source-free methods [3,25,26]."
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,2,Method,"Figure 2 illustrates our SF-UDA framework via context-aware pseudo-label refinement. In this section, we first introduce the context-similarity learning scheme. Next, we propose the pseudo-label refinement strategy. Finally, we present the model training with the denoised refined pseudo-labels."
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,2.1,Context-Similarity Learning,"In the SF-UDA problem, a source modelf s is typically trained with a supervision loss of cross-entropy. Also an unlabeled dataset {x i t } nt i=1 from the target domain D t is given, where x i t ∈ D t . SF-UDA aims to learn a target model f t : X t → Y t with only the source model f s and the target dataset {x i t } nt i=1 . In our fundus segmentation problem, y i ∈ {0, 1} H×W ×C , where C is the number of classes and C = 2 because there are two segmentation targets, namely optic cup and optic disc."
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Architecture of Context-Similarity Head.,"Although the target features generated by source encoder do not align with the source segmentor, features of the same classes tend to be in the same cluster while those of different classes are faraway, as shown in Fig. 1 (b). This indicates the source feature encoder is useful for computing context relations. Therefore, we freeze the source encoder and add an additional head to the encoder for learning context semantic relations, motivated by [1]. A side benefit of freezing the source encoder is the training time and required memory can be reduced, as backward propagation is not needed on the encoder. Specifically, the feature map f sim is first obtained, where a 1 × 1 convolution is applied for adaptation to the target task. Then the semantic similarity between coordinate i and coordinate j on the feature map is defined as(Computing similarities between every pair of coordinates in a feature map is computationally costly. Thus, for each coordinate i, only similarities with coordinates j lying within the circle of radius r are considered in our implementation.Training of Context-Similarity Head. Given a target image x t , initial pseudolabels and uncertainty mask can be obtained from the source model f s and x t , following previous work [3] as:(2), ω ∈{foreground (fg), background (bg)},In Eq. 2, Monte Carlo Dropout [8] is performed with K forward passes through the source model, thereby calculating pseudo-label ŷv and uncertainty u v for the vth pixel. Equation 3 first extracts the class-wise prototypes z ω from the feature map f l,v of the layer before the last convolution, then uncertainty mask m v is calculated by combining the distance to prototypes and uncertainty u v . A pseudo-label for the v-th pixel is reliable ifBinary similarity label is then obtained. For two coordinates i and j, similarity label S * ij is 1 if pseudo-labels ŷi = ŷj , and 0 otherwise. Note only reliable pseudolabels are considered to provide less noisy supervision.The context-similarity head is trained with S * . To address the class imbalance issue, the loss of each type of similarity (fg-fg, bg-bg, fg-bg) is calculated and aggregated [1] as 4)"
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,2.2,Context-Similarity-Based Pseudo-Label Refinement,"Context-Aware Revision. The trained context-similarity head is utilized to refine the initial coarse pseudo-labels. Specifically, context-similarities S ij are computed by passing the target image through the source encoder and the trained head. Then the refined probability for the i-th coordinate is updated as the weighted average of the probabilities in a local circle around the i-th coordinate aswhere p re i is the revised probability and d(•) is the Euclidean distance. β ≥ 1, in order to highlight the prominent similarities and ignore the smaller ones. By combining neighboring predictions based on context relations, revised probabilities are more robust. Equation 5 is performed iteratively for t rounds, since revised probabilities can be used for further revision.Calibration. The probability update by Eq. 5 might be hurt by inaccurate context relations. We observe that for some classes (optic cup for fundus segmentation) with worse pseudo-labels, the context-similarity for ""fg-bg"" is not learned well. Consequently, the probability of background incorrectly propagates to that of foreground, making the probability of foreground lower. To tackle this issue, the revised probability is calibrated asThe decreased probability is rectified by the maximum value in the image, considering the maximum probability (e.g., in the center of a region) after calibration of a class should be close to 1."
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,2.3,Model Adaptation with Denoised Pseudo Labels,"The refined pseudo-labels can be obtained byHowever, noisy pseudo-labels inevitably exist. The combination of model knowledge and target feature distribution shows the best estimation of sample confidence [13]. To this end, reliable pseudo-labels are selected at pixel-level and class-level [3] asin which γ low and γ high are two thresholds for filtering out pseudo-labels without confident probabilities. d fg v and d bg v are the distances to feature prototypes as computed in Eq. 3. The final label selection mask is the intersection of m v,p and m v,c , i.e., m v = m v,p •m v,c . The target model is trained under the supervision of pseudolabels selected by m v , with cross-entropy loss:"
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,3,Experiments,"Datasets. For a fair comparison, we follow prior work [3] to select three mainstream datasets for fundus image segmentation, i.e., Drishti-GS [22], RIM-ONE-r3 [7], and the validation set of REFUGE challenge [18]. These datasets are split into 50/51, 99/60, and 320/80 for training/testing, respectively.Implementation Details and Evaluation Metrics. Following prior works [3,24,25], our segmentation network is MobileNetV2-adapted [21] DeepLabv3+ [4]. The context-similarity head comprises two branches for optic cup and optic disc, respectively. Each branch includes a 1×1 convolution and a similarity feature map. The threshold γ for determining pseudo-labels is set to 0.75, referring to [24]. The radius r in Eq. 5, the β in Eq. 5 and the iteration number t are set to 4, 2 and 4 respectively. The two thresholds for filtering out unconfident refined pseudo-labels are empirically set as γ low = 0.4 and γ high = 0.85, respectively. Each image is pre-processed by clipping a 512 × 512 optic disc region [24]. The same augmentations as in [3,25] are applied, including Gaussian noise, contrast adjustment, and random erasing. The Adam optimizer is adopted with learning rates of 3e-2 and 3e-4 in the context-similarity learning stage and the target domain adaptation stage respectively. The momentum of the Adam optimizer is set to 0.9 and 0.99. The batch size is set to 8. The context-similarity head is trained for 16 epochs and the target model is trained for 10 epochs. The implementation is carried out via PyTorch on a single NVIDIA GeForce RTX 3090 GPU. For evaluation, we adopt the widely used Dice coefficient and Average Surface Distance (ASD).Comparison with State-of-the-Arts. Table 1 shows the comparison of our method with the state-of-the-art SF-UDA methods. Besides three SOTA methods, i.e., DPL [3], FSM [26], and U-D4R [25], we also report the adaptation result without adaptation and the result with fully supervised learning (denoted as ""upper bound""). The results show that our approach achieves clear improvements over the previous methods, owing to the proposed pseudo-label refinement scheme which takes advantage of the feature distribution property under domain shift to learn context relations and utilizes valuable context information to rectify pseudolabels. Figure 3 (a) shows a qualitative comparison.Ablation Study on Different Modules. Table 2 provides a quantitative analysis to investigate the function of each module. Each component shows its importance in improving the adaptation performance. Particularly, without our pseudolabel refinement, an obvious decrease of segmentation performance can be witnessed, revealing its necessity. Without calibration, the segmentation performance  Ablation Study on Pseudo-label Refinement. Ablation study is conducted to verify the effectiveness of the pseudo-label refinement strategy. As shown in Table 3, after refinement, the quality of the pseudo-label is clearly promoted, leading to more accurate supervision for target domain adaptation. For the pseudolabel of optic disc which originally has high accuracy, our refinement scheme encouragingly achieves a boost of 3.5%, showing the robustness of our refinement scheme for different quality of initial pseudo-labels. Without calibration, the accu- racy of the pseudo-label of optic cup is substantially dropped, indicating it is an indispensable part of the overall scheme. Figure 3 (b) visualizes an example of the evolution of the pseudo-label. As can be seen, the context-inconsistent region is clearly improved."
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,4,Conclusion,"This work presents a novel SF-UDA method for the fundus image segmentation problem. We propose to explicitly learn context semantic relations to refine pseudolabels. Calibration is performed to compensate for the wrong revision caused by inaccurate context relations. The performance is further boosted via the denoising scheme, which provides reliable guidance for adaptation. Our experiments on crossdomain fundus image segmentation show that our method outperforms the stateof-the-art SF-UDA approaches."
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Fig. 1 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Fig. 2 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Fig. 3 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Table 1 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Table 2 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Table 3 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 58.
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,1,Introduction,"Quantitative T 1 mapping is a magnetic resonance imaging (MRI) technique that allows for the precise measurement of intrinsic longitudinal relaxation time in myocardial tissue [13]. ""Native"" T 1 mapping, acquired without administration of a paramagnetic contrast agent, has been found to be sensitive to the presence of myocardial edema, iron overload, as well as myocardial infarcts and scarring [12]. It is increasingly recognized as an indispensable tool for the assessment of diffuse myocardial diseases such as diffuse myocardial inflammation, fibrosis, hypertrophy, and infiltration [13].The derivation of accurate T 1 maps necessitates a sequential acquisition of registered images, where each pixel characterizes the same tissue at different timepoints (Fig. 1). However, the inherent motion of the heart, respiration, and spontaneous patient movements can introduce substantial distortions in the T 1 maps, ultimately impeding their reliability and clinical utility, and potentially leading to an erroneous diagnosis. [14]. Echo-triggering is a well-established approach to mitigate the effects of cardiac motion. Conversely, breath-hold sequences such as the Modified Look-Locker Inversion recovery (MOLLI) sequence and its variants [11] are commonly employed to suppress motion artifacts associated with respiration. However, the requirement for subjects to hold their breath places practical constraints on the number of images that can be acquired [11], as well as on the viability of the technique for certain patient populations who cannot tolerate breath-holding. Further, inadequate echo-triggering due to cardiac arrhythmia may lead to unreliable T 1 maps, compromising the diagnosis.Alignment of the images obtained at different time-points via image registration can serve as a mitigation for residual motion and enable cardiac T 1 mapping with free-breathing sequences such as the slice-interleaved T 1 (STONE) sequence [15]. Yet, the intrinsic complexity of the image data, including contrast inversion, partial volume effects, and signal nulling for images acquired near the zero crossing of the T 1 relaxation curve, presents a daunting task in achieving registration for these images. Zhang et al. [18] proposed to perform motion correction in T 1 mapping by maximizing the similarity of normalized gradient fields in order to address the intensity differences across different time points. El-Rewaidy et al. [5] employed a segmentation-based approach in which the residual motion was computed by matching manually annotated contours of the myocardium to the different images. Xue et al. [16] and Tilborghs et al. [14] proposed an iterative approach in which the signal decay model parameters are estimated and synthetic images are generated. Then, image registration used the predicted images to register the acquired data. Van De Giessen et al. [6] used directly the error on the exponential curve fitting as the registration metric to spatially align images obtained from a Look-Locker sequence.Deep-learning methods have been also proposed for motion correction by image registration as a pre-processing step in quantitative cardiac T 1 mapping [2,7,10]. A recent study by Yang et al. [17] introduced a sequential process to address the contrast differences between images. Initially, their approach aimed to separate intensity changes resulting from different inversion times from the fixed anatomical structure. However, this method heavily relied on the perfect disentanglement of the anatomical structure from the contrast. Moreover, the registration is performed exclusively between the disentangled anatomical images, overlooking the adherence of the signal along the inversion time axis to the signal decay model. Nevertheless, these methods do not account directly for the signal decay model, therefore they may produce physically-unlikely deformations.In this work, we introduce PCMC-T1, a physically-constrained deep-learning model for simultaneous motion correction and T 1 mapping from free-breathing acquisitions. Our network architecture combined an image registration module and an exponential T 1 signal decay model fitting module. The incorporation of the signal decay model into the network architecture encourages physicallyplausible deformations along the longitudinal relaxation axis.Our PCMC-T1 model has the potential to expand the utilization of quantitative cardiac T 1 mapping to patient populations who cannot tolerate breathholding by enabling automatic motion-robust accurate T 1 parameter estimation without additional manual annotation of the myocardium."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,2,Method,"We formulate the simultaneous motion correction and signal relaxation model estimation for qMRI T 1 mapping as follows:where N is the number of acquired images, M 0 , T 1 are the exponential signal relaxation model parameters, φ i is the i'th deformation field, Φ = {φ} N -1 i=0 , I i is the i'th original image, and t i is the i'th timestamp. However, direct optimization of this problem can be challenging and time-consuming [6]. The second encoder-decoder generates parametric maps and motion-free synthetic images. The main goal of our network is to minimize the discrepancy between the registered images and the motion-free synthetic images, aiming for physically plausible deformations along the longitudinal relaxation axis. Additionally, we optionally promote anatomically consistent deformation fields by introducing a segmentation loss (c)."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,2.1,Model Architecture,"To overcome this challenge, we propose PCMC-T1, a DNN architecture that simultaneously predicts the deformation fields and the exponential signal relaxation model parameters. Figure 2 summarizes the overall architecture of our model. It includes two U-Net-like encoder-decoder modules that are operating in parallel. Skip connections are connecting between the encoder and the decoder of each model. The first encoder-decoder module is a multi-image deformable image registration module based on the voxelmorph architecture [3], while the second encoder-decoder module is the qMRI signal relaxation model parameters prediction module.The input of the DNN is a set of acquired images {I i |i = 0 . . . N -1}, stacked along the channel dimension. The first encoder-decoder is an extension of the pair-wise VoxelMorph model [3] for registration of multiple images. The encoder is a U-Net-like encoder consisting of convolutional and downsampling layers with an increasing number of filters. The decoder output splits into multiple separated heads of convolutional layers and integration layers that produce a specific defor-mation field {φ i |i = 0 . . . N -1} for each timestamp i. Skip connections are used to propagate the learned features into the deformation field prediction layers. A spatial warping layer is used to align the acquired images I i to the synthetics images generated from the signal relaxation model parameters predictions (S i ): R i = I i • φ i . The specific details of the architecture are as in Balakrishnan et al. [3] and the details of the integration layer are as in Dalca et al. [4].The second encoder-decoder has a similar architecture. It has two output layers representing the exponential signal relaxation model parameters: T 1 and M 0 . The predicted parameters maps are then used, along with the input's timestamps {t i |i = 0 . . . N -1}, as input to a signal generation layer. This layer generates a set of motion-free images {S i |i = 0 . . . N -1} computed directly from the estimated parametric maps (M 0 , T 1 ) at the different inversion times using the signal relaxation model [15]:"
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,2.2,Loss Functions,"We encourage predictions of physically-plausible deformation fields by coupling three terms in our loss function as follows:The first term (L fit ) penalizes for differences between the model-predicted images generated by the model-prediction decoder and the acquired images warped according to the deformation fields predicted by the registration decoder. Specifically, we use the mean-squared-error (MSE) between the registered images {R i |i = 0 . . . N -1} and the synthetic images {S i |i = 0 . . . N -1}:where S i are the images generated with the signal model equation (Eq. 2), and the registered images are the output of the registration module. This term encourages deformation fields that are physically plausible by means of a signal relaxation that is consistent with the physical model of T 1 signal relaxation.The second term (L smooth ) encourages the model to predict realistic, smooth deformation fields Φ by penalizing for a large l 2 norm of the gradients of the velocity fields [3]:where Ω is the domain of the velocity field and p are the voxel locations within the velocity field. In addition, we encourage anatomically-consistent deformation fields by introducing a segmentation-based loss term (L seg ) as a third term in the overall loss function [3]. This term can be used in cases where the left ventricle (LV)'s epicardial and endocardial contours are available during training. Specifically, the segmentation loss function is defined as follows:where Seg i , (i ∈ 0, . . . , N -1) is the i th binary segmentation mask of the myocardium, Seg r is the binary segmentation mask of the fixed image, and r is the index of the fixed image. This term can be omitted in cases where the segmentations of the myocardium are not available."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,2.3,Implementation Details,"We implemented our models in PyTorch. We experimentally fixed the first timepoint image, and predict deformation fields only for the rest of the time points. We optimized our hyperparameters using a grid search. The final setting for the loss function parameters were: λ 1 = 1, λ 2 = 500, λ 3 = 70000. We used a batch size of 8, ADAM optimizer with a learning rate of 2 • 10 -3 . We trained the model for 300k iterations. We used the publicly available TensorFlow implementations of the diffeomorphic VoxelMorph [4] and SynthMorph [9] as baseline methods for comparison. We performed hyper-parameter optimization for baseline methods using a grid search. All experiments were run on an NVIDIA Tesla V100 GPU with 32G RAM."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,3,Experiments and Results,
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,3.1,Data,"We used the publicly available myocardial T 1 mapping dataset [1,5]. The dataset includes 210 subjects, 134 males and 76 females aged 57 ± 14 years, with known or suspected cardiovascular diseases. The images were acquired with a 1.5T MRI scanner (Philips Achieva) and a 32-channel cardiac coil using the ECG-triggered free-breathing imaging slice-interleaved T 1 mapping sequence (STONE) [15]. Acquisition parameters were: field of view (FOV) = 360 × 351[mm 2 ], and voxel size of 2.1 × 2.1 × 8[mm 3 ]. For each patient, 5 slices were acquired from base to apex in the short axis view at 11 time points. Additionally, manual expert segmentations of the myocardium were provided as part of the dataset [5]. We cropped the images to a size of 160 × 160 pixels for each time point. We normalized the images using a min-max normalization. "
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,3.2,Evaluation Methodology,"Quantitative Evaluation: We used a 5-fold experimental setup. For each fold, we divided the 210 subjects into 80% as a training set and 20% as a test set.We conducted an ablation study to determine the added value of the different components in our model. Specifically, we compared our method using a few variations, including a multi-image registration model with a mutual-informationbased loss function (REG-MI) [8], and our method (PCMC-T1) without the segmentation loss term. We used two state-of-the-art deep-learning algorithms for medical image registration including the pairwise probabilistic diffeomorphic VoxelMorph with a mutual-information-based loss [4], and pairwise SynthMorph [9], as well as with T 1 maps produced from the acquired images directly without any motion correction step. We quantitatively evaluated the T 1 maps produced by our PCMC-T1 model in comparison to T 1 maps produced after applying deep-learning-based image registration as a pre-processing step. We used the R 2 of the model fit to the observed data in the myocardium, the Dice score, and Hausdorff distance values of the myocardium segmentations as the evaluation metrics.Clinical Impact: We further assessed the clinical impact of our method by conducting a semi-quantitative ranking of the T 1 maps for the presence of motion artifacts by an expert cardiac MRI radiologist (3 years of experience) who was blinded to the methods used to generate the maps. We randomly selected 29 cases (5 slices per case) from the test set with their associated T 1 maps. The radiologist was asked to rank each slice with 1 in case of a good quality map without visible motion artifacts and with 0 otherwise. We computed overall patient scores by summing the slice grades. The maximum grade per subject was 5 for cases in which no motion artifacts were present in all slices and 0 for cases in which motion artifacts were present in all slices. We assessed the statistical significance with the repeated measures ANOVA test; p<0.05 was considered significant."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,3.3,Results,"Quantitative Evaluation: Table 1 summarizes our results for the test sets across all folds, encompassing a total of 210 patients. Our PCMC-T1 approach achieved the best result in terms of R 2 with the smallest variance. Although PCMC-T1 without the segmentation loss (L seg ) achieved a higher R 2 result compared to PCMC-T1 with the segmentation loss, it degraded the Dice value, representing over-fitted predictions. On the other hand, the slightly higher Dice score and Hausdorff distance values obtained by baseline methods compared to PCMC-T1 suggest bias of these methods toward the registration of the segmentation maps rather than producing deformation fields that are consistent with the signal relaxation model. The balanced result of PCMC-T1 indicates an improvement in the physical plausibility of the deformations produced by PCMC-T1 by means of signal relaxation and anatomical consistency.Clinical Impact: Figure 3 presents several representative cases. Although the Dice score of the baseline methods is higher compared to this of PCMC-T1, the quality of the maps produced by PCMC-T1 is better. The rightmost column of Fig. 3. Representative T1 maps computed with the different approaches. Our approach (PCMC-T1) demonstrates a clearer delineation between the blood and the muscle with a reduced partial volume effect, resulting in a more homogeneous mapping of the myocardium.Table 1 summarizes the results of the clinical impact assessment of our PCMC-T1 approach. Our PCMC-T1 received the highest quality score compared to the baseline methods. The difference in the radiologist grading was statistically significant (p 10e -5 ). The improvement in the radiological evaluation suggests that PCMC-T1 provides a balanced result that is not overly biased toward the segmentations or toward the signal relaxation model."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,4,Conclusions,"We presented PCMC-T1, a physically-constrained deep-learning model for motion correction in free-breathing T 1 mapping. Our main contribution is the incorporation of the signal decay model into the network architecture to encourage physically-plausible deformations along the longitudinal relaxation axis. We demonstrated a quantitative improvement by means of fit quality with comparable Dice score and Hausdorff distance. We further assessed the clinical impact of our method by conducting a qualitative evaluation of the T 1 maps produced by our method in comparison to baseline methods by an expert cardiac radiologist. Our PCMC-T1 model holds the potential to broaden the application of quantitative cardiac T 1 mapping to patient populations who are unable to undergo breath-holding MRI acquisitions by enabling motion-robust accurate T 1 parameter estimation. Further, the proposed physically-constrained motion robust parameter estimation approach can be directly extended to quantitative T2 mapping as well as to additional qMRI applications."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,,Fig. 1 .,
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,,Fig. 2 .,
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,,,
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,,Table 1 .,
TabAttention: Learning Attention Conditionally on Tabular Data,1,Introduction,"Many clinical procedures involve collecting data samples in the form of imaging and tabular data. New deep learning (DL) architectures fusing image and nonimage data are being developed to extract knowledge from both sources of information and improve predictive capabilities [9]. While concatenation of tabular and imaging features in final layers is widely used [8,11], this approach limits the interaction between them. To facilitate better knowledge transfer between these modalities more advanced techniques have been proposed. Duanmu et al. [5] presented the Interactive network in which tabular features are passed through a separate branch and channel-wise multiplied with imaging features at different stages of Convolutional Neural Network (CNN). Pölsterl et al. [16] proposed a Dynamic Affine Feature Map Transform (DAFT) to shift and scale feature maps conditionally on tabular data. In [6], Guan et al. presented a method for transforming tabular data and processing them together with 3D feature maps via VisText self-attention module. The importance of the attention mechanism on DL models' performance has been extensively studied [23]. Convolutional Block Attention Module (CBAM) [25] has been shown to improve the performance of DL models on high dimensional data [24,26]. Despite these advances, few studies have explored the potential of incorporating attention maps with imaging and tabular data simultaneously.We develop such a solution and as an example of application, we use fetal birth weight (FBW) prediction from ultrasound (US) data. It is a challenging task requiring clinicians to collect US videos of fetal body parts and fetal biometry measurements. Currently, abdominal circumference (AC), head circumference (HC), biparietal diameter (BPD), and femur length (FL) are used to estimate FBW with heuristic formulae [7]. The predicted weight is the indicator of perinatal health prognosis or complications in pregnancy and has an impact on the method of delivery (vaginal or Cesarean) [17]. Unfortunately, the current approach to FBW estimation is often imprecise and can lead to a mean absolute percentage error (MAPE) of 10%, even if performed by experienced sonographers [20]. An ensemble of Machine Learning algorithms was proposed by Lu et al. [12] for solving this task. CNNs are applied for fetal biometry measurements estimation from US standard planes [1] or US videos [15]. Tao et al. [21] approach this problem with a recurrent network utilizing temporal features of fetal weight changes over weeks concatenated with fetal parameters. P lotka et al. [14] developed BabyNet, a hybrid CNN with Transformer layers to estimate FBW directly from US videos. Recent studies show that there is a strong correlation between the image features of the abdominal plane and the estimated fetal weight, indicating that it can serve as a dependable indicator for evaluating fetal growth [3]. We utilize the US videos of the abdomen (imaging data) and biometry measurements with other numerical values (tabular data) during our experiments.In this work, we introduce TabAttention, a novel module designed to enhance the performance of CNNs by incorporating tabular data. TabAttention extends the CBAM to the temporal dimension by adding a Temporal Attention Module (TAM) that leverages Multi-Head Self-Attention (MHSA) [23]. Our method utilizes pooled information from imaging feature maps and tabular data (represented as tabular embeddings) to generate attention maps through Channel Attention Module (CAM), Spatial Attention Module (SAM), and TAM. By incorporating tabular data, TabAttention enables the network to better identify what, where, and when to focus on, thereby improving performance. We evaluate our method on the task of estimating FBW from abdominal US  videos and demonstrate that TabAttention is at least on par with existing methods, including those based on tabular and/or imaging data, as well as clinicians.The main contributions of our work are: 1) the introduction of TabAttention, a module for conditional attention learning with tabular data, 2) the extension of CBAM to the temporal dimension via the TAM module, and 3) the validation of our method on the FBW estimation task, where we demonstrate that it is competitive with state-of-the-art methods."
TabAttention: Learning Attention Conditionally on Tabular Data,2,Method,"In this section, we introduce the fundamental components of the TabAttention module. We detail the development of CBAM augmented with a Temporal Attention Module. Then, we elaborate on how TabAttention leverages tabular embeddings to modulate the creation of attention maps and outline how the module can be seamlessly incorporated into the residual block of ResNet. Figure 1 presents the overview of the TabAttention module. Given US video sequence S ∈ R T0×1×H0×W0 of height H 0 , width W 0 and frame number T 0 as the input, 3D CNN produces intermediate temporal feature maps S ∈ R T ×C×H×W where C is the number of channels. In our setting, the CBAM block generates T 1D channel attention maps M c ∈ R T ×C×1×1 and T 2D spatial attention maps M s ∈ R T ×1×H×W . We create attention maps separately for every temporal feature map as the information of what is meaningful and where it is important to focus on might change along the temporal dimension. To account for the temporal changes and focus on when is the informative part we add TAM which infers temporal attention map M t ∈ R T ×1×1×1 . Intermediate temporal feature maps S are refined with attention maps in the following way:Here O denotes the output of the module and ⊗ is an element-wise multiplication during which attention maps are broadcasted along all unitary dimensions.  In general, attention maps are computed based on information aggregated by average-and max-pooling along specified dimensions which are then passed through shared layers for refinement (Fig. 2). Then, these refined descriptors are passed through the sigmoid function to create final attention maps. To account for the tabular information during attention maps computing, we embed the input tabular data T ab ∈ R D , where D is the number of numerical features, with two linear layers and Rectified Linear Unit (ReLU) activation in between. The tabular data is embedded to the size of pooled feature maps. The embedding is passed through shared layers in the same way as pooled feature maps. Therefore, the attention maps are computed conditionally on tabular data. Thus, the output of TabAttention O t is computed as follows:(2) Channel Attention Module. We follow the design of the original CBAM [25]. We split temporal feature maps into T feature maps F i where i ∈ 1, ..., T so that each of them is passed through CAM separately. To compute the channel attention (M c ), we aggregate the spatial information through average-and maxpooling to produce descriptors (F c avgi , F c maxi ∈ R C×1×1 ). We pass the tabular data through a multi-layer perceptron (MLP embc ) with one hidden layer (of size R C z , where z is the reduction ratio set to 16) and ReLU activation to embed it into the same dimension as spatial descriptors. Then, both descriptors, with tabular embedding are passed through the shared network which is MLP with a hidden activation size of R C z and one ReLU activation. After the MLP is applied, the output vectors are element-wise summed to produce the attention map. We concatenate attention maps of all feature maps to produce M c :Spatial Attention Module. After splitting the temporal feature maps, we average-and max-pool them along channel dimension to produce feature descriptors (F s avgi , F s maxi ∈ R 1×H×W ). We pass the tabular data through MLP embs with one hidden layer of size R H×W 2and ReLU activation to embed it into the same dimension as spatial descriptors. We reshape this embedding to the size of feature descriptors and concatenate it with them. We pass the following representation through a 2D convolution layer and the sigmoid activation:Temporal Attention Module. We create temporal descriptors by averageand max-pooling temporal feature maps along all non-temporal dimensions (F t avgi , F t maxi ∈ R T ×1×1×1 ). We embed tabular data with MLP embt with one hidden layer of size R T 2 into the same dimension. We concatenate created vectors and treat them as the embedding of the US sequence which we pass to the MHSA layer (with 2 heads). We create the query (Q), key (K) and value (V) with linear layers and an output size of d (4). We add relative positional encodings [19] r to K. After passing through MHSA, we squash the refined representation with one MLP layer and sigmoid function to create a temporal attention map M t :TabAttention can be integrated within any 3D CNN (or 2D CNN in case TAM is omitted). As illustrated in Fig. 2, we add TabAttention between the first ReLU and the second convolution in the residual block to integrate our module with 3D ResNet-18."
TabAttention: Learning Attention Conditionally on Tabular Data,3,Experiments and Results,"This section describes the dataset used and provides implementation details of our proposed method. We benchmark the performance of TabAttention against several state-of-the-art methods and compare them to results obtained by clinicians. Additionally, we conduct an ablation study to demonstrate the significance of each key component utilized in our approach.Dataset. This study was approved by the Ethics Committee of the Medical University of Warsaw (Reference KB.195/2021) and informed consent was obtained for all subjects. The multi-site dataset was acquired using international standards approved by [18]. The dataset consists of 92 2D fetal US video scans captured in the standard abdominal plane view. These scans were collected from 92 pregnant women (31.89 ± 4.76 years), across three medical centers, and obtained as part of routine US examination done less than 24 h before delivery. This allowed us to obtain the real ground truth which was baby weight soon after birth. Five experienced sonographers (14.2 ± 4.02 years of experience) acquired the data using a single manufacturer device (General Electric) and several models (GE Voluson E6, S8, P8, E10, and S10). The abdominal fetal US videos (5-10 seconds, 13-37 frames per second) were saved in the DICOM file format.We resized the pixel spacing to 0. Implementation Details. We use 3D ResNet-18 as our base model. We implement all experiments with PyTorch and train networks using NVIDIA A100 80GB GPU for 250 epochs with a batch size of 16 and an initial learning rate chosen with grid search from the set of {1 × 10 -2 , 1 × 10 -3 , 1 × 10 -4 }. To minimize the Mean Squared Error loss function, we employ the Adam [10] optimizer with L2 regularization of 1 × 10 -4 and cosine annealing learning rate scheduler. To evaluate the reliability of the regression algorithm, we conduct five-fold cross-validation (CV) and ensure that each patient's data is present in only one fold. To ensure similar birth weight distribution in all folds, we stratify them based on the assignment of data samples into three bins: < 3000 g g, > 4000 g g, and in-between. The input frames are of size 128 × 128 pixels. We follow the approach presented in [14], we set the number of input frames to 16 and average per-patient predictions of all 16 frame segments from the single video. Throughout the training process, we employ various data augmentation techniques such as rotation, random adjustments to brightness and contrast, the addition of Gaussian noise, horizontal flipping, image compression, and motion blurring for every batch. We standardize all numerical features to a mean of 0 Comparison with State-of-the-Art Methods. We compare TabAttention with several methods utilizing tabular data only (Linear Regression [13],XGBoost [4]), imaging data only (3D ResNet-18 [22], BabyNet [14]), both types of data (Interactive [5], DAFT [16]), and Clinicians. The predictions of Clinicians were achieved using Hadlock III [7] formula and AC, HC, BPD, FL measurements. The comparison of results from the five-fold CV is presented in Table 1.TabAttention achieves the lowest MAE, RMSE and MAPE (170 ± 26, 225 ± 37, 5.0 ± 0.8 respectively) among all tested methods. Our approach outperforms clinically utilized heuristic formulae, machine learning, and image-only DL methods (two-tailed paired t-test p-value < 0.05). Results of TabAttention are also best compared with all DL models utilizing tabular and imaging modalities, however, the difference does not reach statistical significance with a p-value around 0.11.Ablation Study. We conduct ablation experiments to validate the effectiveness of key components of our proposed method (Table 2). We employ 3D ResNet-18 as the baseline model. The integration of TAM or CBAM with attention maps learned conditionally on tabular data into the 3D ResNet-18 architecture improves the predictive performance of the network. Subsequently, the incorporation of full TabAttention further enhances its capabilities. "
TabAttention: Learning Attention Conditionally on Tabular Data,4,Discussion and Conclusions,"In this work, we present a novel method, TabAttention, that can effectively compete with current state-of-the-art image and/or tabular-based approaches in estimating FBW. We found that it outperformed Clinicians achieving mMAPE of 5.0% vs. 5.9% (p-value < 0.05). A key advantage of our approach is that it does not require any additional effort from clinicians since the necessary data is already collected as part of standard procedures. This makes TabAttention an alternative to the heuristic formulas that are currently used in clinical practice. We should note that while TabAttention achieved the lowest metrics among the DL models we evaluated, the differences between our approach and other DL methods using tabular data were not statistically significant, partly due to the small performance change. This small difference in the performance is likely caused by the fact that the tabular features used in TabAttention are mainly derived from the same modality (i.e. US scans), so they do not carry additional information, but instead can be considered as refined features already present in the scans. To develop TabAttention, we used tabular data as a hint for the network to learn attention maps and gain additional knowledge about essential aspects presented in the scans. This approach significantly improved the performance of baseline methods and demonstrated its practical applicability. Accurate estimation of FBW is crucial in determining the appropriate delivery method, whether vaginal or Cesarean. Low birth weight (less than 2500 g g) is a major risk factor for neonatal death, while macrosomia (greater than 4000 g) can lead to delivery traumas and maternal complications, such as birth canal injuries, as reported by Benacerraf et al. [2]. Thus, precise prediction of FBW is vital for very low and high weights. Notably, in this respect, our method is robust to outliers with high or low FBW since there is no correlation between true FBW and absolute prediction error (Pearson correlation coefficient of -0.029).This study has limitations. Firstly, a relatively small study cohort was used, which may affect the accuracy and generalization of the results. To address this, future work will include a larger sample size by using additional datasets. Secondly, our dataset is limited to only Caucasian women and may not be rep-resentative of other ethnicities. It is important to investigate the performance of our method with datasets from different ethnic groups and US devices to obtain more robust and generalizable results. Lastly, our method relies on fetal biometry measurements that are subject to inter-and intra-observer variabilities. This variability could potentially affect the network's performance and influence the measurements' quality. Future studies should consider strategies to reduce measurement variabilities, such as standardized protocols or automated measurements, to improve the accuracy of the method.To summarize, we have introduced TabAttention, a new module that enables the conditional learning of attention on tabular data and can be integrated with any CNN. Our method has many potential applications, including serving as a computer-aided diagnosis tool for various clinical workflows. We have demonstrated the effectiveness of TabAttention on the FBW prediction task, utilizing both US and tabular data, and have shown that it outperforms other methods, including clinically used ones. In the future, we plan to test the method in different clinical applications where imaging and tabular data are used together."
TabAttention: Learning Attention Conditionally on Tabular Data,,Fig. 1 .,
TabAttention: Learning Attention Conditionally on Tabular Data,,Fig. 2 .,
TabAttention: Learning Attention Conditionally on Tabular Data,,Fig. 3 .,
TabAttention: Learning Attention Conditionally on Tabular Data,,,
TabAttention: Learning Attention Conditionally on Tabular Data,,Table 1 .,
TabAttention: Learning Attention Conditionally on Tabular Data,,Table 2 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,1,Introduction,"Medical phrase grounding (MPG) is the task of associating text descriptions with corresponding regions of interest (ROIs) in medical images. It enables machines to understand and interpret medical findings mentioned in medical reports in the context of medical images, which is crucial in medical image analysis and radiological diagnosis. Figure 1 illustrates how an MPG system facilitates the radiological diagnosis process. Radiologists first review the medical images (e.g., X-rays, CT scans, and MRI scans) to find out possible abnormalities and then write a report that summarizes their findings. Then, given the image and report, the MPG system can help doctors to locate and link ROIs to the corresponding phrases in the reports, which reduces the time of the diagnostic process and improves the quality of risk stratification and treatment planning. In this paper, we study the MPG problem and focus on a typical setting to learn the grounding between Chest X-ray images and medical reports. As far as we know, there are only a few related works on the medical phrase grounding problem. (This is probably because medical annotations of grounding data require specialized expertise and are time-consuming and expensive to be collected.) Benedikt et al. [1] made use of text semantics to improve biomedical vision-language processing. They first evaluated the grounding performance of self-supervised biomedical vision-language models by proposing an MPG benchmark. However, their focus is on vision-language pre-training rather than addressing the MPG problem. Qin et al. [19] proposed to transfer the knowledge of general vision-language models for detection tasks in medical domains. The key idea is to guide the vision-language model through hand-crafted prompting of visual attributes such as color, shape, and location that may be shared between natural and medical domains. This approach fails to consider unique characteristics in radiological images and reports and is inapplicable to MPG for radiological images.Although visual grounding has been well studied for natural images [3,4,[22][23][24], it is non-trivial to apply these approaches to radiological images. Specifically, MPG requires learning specialized visual-textual features so that the model can identify medical findings with subtle differences in texture and shape and interpret the relative positions mentioned in the medical reports. In contrast, general grounding methods often rely on visual features that are useful for object detection or classification but not specific to medical images, leading to inaccurate region-phrase correlations and thus sub-optimal results. In addition, many grounding models for general domains are too heavy to be trained with limited annotated data, which is common. Such heavy model structures are generally difficult to be trained with limited annotated data.In this work, we propose MedRPG, an end-to-end approach for MPG. MedRPG has a lightweight model architecture and explicitly captures the finding-specific correlations between ROIs and report phrases. Specifically, we propose to stack a few vision-language transformer layers to encode both the medical images and report phrases and directly predict the box coordinates of desired medical findings. Compared to general grounding methods with heavy model architectures, this design is more robust against overfitting for MPG with limited training data. To locate nuanced medical findings with better regionphrase correspondences, we further propose Tri-attention Context contrastive alignment (TaCo). TaCo seeks context alignment to learn finding-specific representations that jointly align the region, phrase, and box prediction under the same context of a vision-language transformer encoder. It pulls both the features and attention outputs close together for semantically relevant region-phrase pairs while pushing those of irrelevant pairs far away. This encourages the alignment between regions and phrases at both feature and attention levels, leading to enhanced finding-identification ability and reduced spurious region-phrase correlations. Experimental results on three medical datasets demonstrate that our MedRPG is more effective in localizing medical findings, achieves better regionphrase correspondences, and significantly outperforms general visual grounding approaches on the MPG task."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,2,Proposed Method,"The MPG problem can be defined as follows: Given a radiological image I associated with medical phrases T written by specialist radiologists, MPG aims to locate the described findings and then output a 4-dim bounding box (bbox) coordinates b = (x, y, w, h), where (x, y) is the box center coordinates and w, h are the box width and height, respectively."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,2.1,Model Architecture,"Figure 2 illustrates the framework of our method. Given image I and phrase T, we first leverage the Vision Encoder and Language Encoder to generate the image and text embeddings. Next, we concatenate the multi-modal feature embeddings and append a learnable token (named [REG] token), and then feed them into a lightweight Vision-Language Transformer to encode the intra and inter-modality context in a common semantic space. Finally, the output state of the [REG] token is employed to predict the 4-dim bbox via a grounding head. Additionally, to ensure a consistent representation of medical findings across modalities, we introduce TaCo, which aligns the context of region and phrase embeddings at both the feature and attention levels.Vision Encoder: Following the common practice [2,4,13], the visual encoder starts with a CNN backbone, followed by the visual transformer. We choose the ResNet-50 [9] as the CNN backbone. The visual transformer includes 6 stacked transformer encoder layers [5]. Given a radiological image I ∈ R 3×W ×H , it is fed into the CNN backbone to obtain the high-level deep features. Next, we apply a 1 × 1 ConV layer to project the deep features into a C v -dimensional subspace. Finally, we exploit the visual transformer to mine the long-range visual relations and further output the visual features, where N v is the number of visual tokens and f n v ∈ R Cv is the n-th token of F v . Language Encoder: We leverage pre-trained language models such as BERT [14] as the language encoder, which includes 12 transformer encoder layers. Given a medical phrase T, we first utilize the BERT tokenizer to convert it into a sequence of tokens. Next, we follow the common practice to append a [CLS] token at the beginning as the global representation of the input medical phrases and append a [SEP] token at the end, and then pad the sequence to a fixed length. Finally, we use BERT to encode the tokens into the text embeddings, where N l is the number of text tokens, C l is the feature dimensions, and f n l ∈ R C l is the n-th token of F l . Vision-Language Transformer: After the individual vision and language encoding, we obtain F v and F l . To capture the correspondence between the image and phrase embeddings, we first project them into the common space (channel= C vl ) and then fed them into a Vision-Language Transformer (VLT), together with an extra learnable [REG] token, which is further used to predict the bbox:where ϕ v (•) and ϕ l (•) denote the project functions for vision and language tokens, respectively. r∈R C vl is the [REG] token and VLT(•) denotes the VLT encoder with learnable position embeddings. H ∈ R C vl ×N vl (whereis the output of VLT that consist of three parts: vision embeddings, and [REG] embedding h reg . To perform the final grounding results, we further feed h reg into a 3-layer MLP to predict the final 4-dim box coordinates b = MLP(h reg ). Given the grounding-truth box b 0 , we leverage smooth L1 loss [7] and GIoU [20] loss which are popular in grounding and detection tasks to optimize our model:where L box is the box loss. Φ l1 and Φ giou are the smooth L1 and GIoU loss functions, respectively. λ is the trade-off parameter."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,2.2,Tri-Attention Context Contrastive Alignment,"Medical findings often share subtle differences in texture and brightness level due to the low contrast of the medical images, which makes it challenging for the MPG methods to capture accurate region-phrase correlations. To identify nuanced medical findings with better region-phrase correspondences, we propose the Tri-attention Context Contrastive Alignment (TaCo) strategy to learn finding-specific representations with accurate region-phrase correlations by explicitly aligning relevant regions and phrases at both feature and attention levels.Feature-Level Alignment. The feature-level alignment aims to make visual and textual embeddings with the same semantics meaning to be similar. To this end, given the bbox b 0 related to a given phrase query, we first obtain the positive ROI embeddings) by aggregating visual embeddings H v within the bbox b 0 . Next, we randomly select K bbox {b k } K k=1 that have low IoUs with b 0 (i.e., regions that are irrelevant to the given phrase query) and obtain K negative region embeddingsLet h cls be the features of the input phrases. We want to make the positive ROI embedding h box 0 close to the corresponding phrase embedding h cls whereas negative region embeddings {h box k } K k=1 far away. This is achieved by exploiting the InfoNCE [8,17] loss as:where L fea denotes the feature-level alignment loss, τ is a temperature hyperparameter and '•' represents the inner (dot) product.Attention-Level Alignment. In addition to the feature-level alignment, we also consider attention-level alignment, which encourages the attention outputs of VLT for relevant region-phrase pairs to be similar. To realize this, we extract the attention weight A ∈ R N vl ×N vl from the last multi-head attention layer of VLT. We denote a reg , a cls and {a box k } K k=0 as the attention weights for the embeddings of the [REG] token, the [CLS] token, and the K + 1 bboxes, respectively, where a box k = Pool(A, b k ). Given the k-th bbox embedding, we calculate the joint attention weights of bbox, [CLS], and [REG] embeddings and then further product H to get the triple-attention context pooling c k as follows:where t k represents the joint attention weights, t (j) k denotes the j-th element of t k , H[:, j] denotes the j-th column of H, and Norm(•) is the L2 normalization operation to constrain the sum of the squared weights to be equal to 1. Such triple-attention context pooling c k characterizes the contextual dependencies among regions, phrases, and box predictions in the VLT. Intuitively, the box prediction of certain medical findings should be made based on its relevant regions and phrases rather than irrelevant ones. Therefore, the attention outputs c 0 for relevant region-phrase pairs should be similar to their individual embeddings h box 0 and h cls , leading to attention-level alignment.Table 1. Grounding results on MS-CXR [1], ChestX-ray8 [21], and the in-house datasets with respect to Acc and mIoU. ). This leads to the TaCo loss as follows:Finally, we combine the TaCo loss ( 5) and box loss (2) to get the overall loss functions of MedRPG:where L MedRP G denotes total loss for MedRPG and μ is the trade-off parameter."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,3,Experiments,"Dataset. Our experiments are conducted on two public datasets, i.e., MS-CXR [1], ChestX-ray8 [21], and one in-house datase 1 . MS-CXR is sourced from MIMIC-CXR [11,12] and consists of 1,153 samples of Image-Phrase-BBox triples.We pre-process MS-CXR to make sure a given phrase query corresponds to only one bounding box, which results in 890 samples from 867 patients. ChestX-ray8 is a large-scale dataset for diagnosing 8 common chest diseases, of which 984 images with pathology are provided with hand-labeled bounding boxes. Due to the lack of finding-specific phrases from medical reports, we use category labels as the phrase queries to build the Image-Report-BBox triples for our task. Our in-house dataset comprises 1,824 Image-Phrase-BBox samples from 635 patients, including 23 categories of chest abnormalities with more complex phrases. For a fair comparison, all datasets are split into train-validation-test sets by 7:1:2 based on the patients.Evaluation Metrics. To evaluate the quality of the MPG task, we follow the standard protocol of nature image grounding [4] to report Acc(%), where a predicted region will be regarded as a positive sample if its intersection over union (IoU) with the ground-truth bounding box is greater than 0.5. Besides, we also report mIoU (%) metric for a more comprehensive comparison. Baselines. We compare our MedRPG with SOTA methods for general visual grounding, such as RefTR [15], TransVG [4], VGTR [6], and SeqTR [24]. We choose their official implementations for a fair comparison. Since the medical datasets are too small to train a data-hungry transformer-based model from scratch, we initialize our MedRPG (encoders) from the general grounding models pre-trained on natural images. The compared methods share the same settings. We also compare two self-supervised biomedical vision-language processing methods BioViL [1] and GLoRIA [10] which pre-trained on MIMIC-CXR [11]. Implementation Details. The experiments are conducted on the PyTorch [18] platform with an NVIDIA RTX 3090 GPU. The input image size is 640×640.The channel numbers C v , C l , and C vl are 256, 768, and 256. The sample number K is set to 5. The trade-off parameter λ in Eq. 2 and μ in Eq. 6 are set to 1 and 0.05, respectively. The base learning rates for the vision encoder, language encoder, and vision-language transformer are set to 1×10 -5 , 1×10 -5 , and 5×10 -5 , respectively. We train our MedRPG model by the AdamW [16] optimizer for 90 epochs with a learning rate dropped by a factor of 10 after 60 epochs. For all the baselines and MedRPG, we select the best checkpoint for testing based on validation performance and report the average performance metrics computed by repeating each experiment with three different random seeds. Experimental Results. Table 1 provides the grounding results on the MS-CXR, ChestX-ray8, and in-house datasets. As can be seen, our MedRPG consistently achieves the best performance in all cases. In particular, we note that lightweight models like TransVG and our MedRPG generally perform better, which indicates lightweight models are more applicable for MPG. Despite this, our method still outperforms TransVG by a margin of 6.1% in Acc on MS-CXR.This can be attributed to the proposed TaCo strategy in learning finding-specific representations and improving region-phrase alignment. On ChestX-ray8, all methods get degraded results due to the lack of position cues in the phrase queries. Nevertheless, our method still outperforms the second-best method by 4.3% in Acc and 1.6% in mIoU. On the in-house dataset, our method is still the best even when there exist much more types of findings to be grounded. Note that the self-supervised methods BioViL and GloRIA achieve very poor results compared to other methods on all three datasets.Ablation Study. We conduct ablative experiments on the MS-CXR dataset to verify the effectiveness of each component in MedRPG. Table 2 shows the quantitative results of each combination. To verify how the vision and language modalities contribute to the MPG performance, we perform MedRPG with either image or test inputs. As expected, MedRPG can only achieve poor results under the unimodal setting. Next, we consider the inputs with both images and phrases and observe a significant improvement in performance compared to MedRPG trained from a single modality. Then, we equip MedRPG with feature-level alignment and gain the improvement of 0.6% in Acc and 0.5% in mIoU, which suggests it is helpful but still not good enough to learn the accurate region-phrase correspondences. Finally, with the proposed TaCo, MedRPG further gains a significant improvement by 3.8% in Acc. This shows that TaCo is effective in improving the MPG performance with better region-phrase correlations. In addition, we study the impact of hyper-parameters (trade-off parameter μ and number of negative samples K). Table 3 shows two metrics of our MedRPG method with varying hyper-parameters onni the MS-CXR dataset. As can be seen, our method is not very sensitive to hyper-parameter choices. Additional analysis of the model's confidence intervals and its ability to generalize can be found in the supplementary material.Qualitative Results. In Fig. 3, we show the box predictions and attention maps obtained by MedRPG with and without TaCo to demonstrate the effectiveness of TaCo in identifying abnormal medical findings and capturing region-phrase correlations. For instance, in Case 1, pneumothorax is present in an uncommon location (i.e., lower left lung) and the phrase does not provide an accurate location cue. Without TaCo, MedRPG overfits the upper lung regions where pneumothorax appears more frequently. In contrast, with TaCo, the model can better learn pneumothorax representations and identify the corresponding ROI even without accurate location information. In other cases, although the method without TaCo can also roughly find the location of the medical findings, MedRPG with TaCo can obtain more focused attention maps on the medical findings. It suggests that TaCo is effective in reducing spurious region-phrase correlations, leading to more accurate and interpretable bbox predictions."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,4,Conclusion,"This study introduces MedRPG, a lightweight and efficient method for medical phrase grounding. A novel tri-attention context contrastive alignment (TaCo) is proposed to learn finding-specific representations and improve region-phrase alignment in feature and attention levels. Experimental results show that MedRPG outperforms existing visual grounding methods and achieves more consistent correlations between phrases and mentioned regions."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Fig. 1 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Fig. 2 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Fig. 3 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Table 2 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Table 3 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_35.
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,1,Introduction,"Intelligible speech is produced by the intricate three-dimensional structure of the tongue, composed of localized functional units [26]. These functional units, when measured using tagged magnetic resonance imaging (MRI), exhibit cohesive displacements and derived quantities that serve as intermediate structures linking tongue muscle activity to tongue surface motion, which in turn facilitates the production of speech. A framework based on sparse non-negative matrix factorization (NMF) with manifold regularization can be used to estimate the functional units given input motion features, which yields a set of building blocks (or basis vectors) and a corresponding sparse weighting map (or encoding) [27]. The building blocks can form and dissolve with remarkable speed and agility, yielding highly coordinated patterns that vary depending on the specific speech task at hand. The corresponding weighting map can then be used to identify the cohesive regions and reveal the underlying functional units [25]. As such, by elucidating the relationship between the weighting map and intelligible speech, we can gain valuable insights for the development of speech motor control theories and the treatment of speech-related disorders.Despite recent advances in cross-modal speech processing, translating between varied-size of wide 2D weighting maps and high-frequency 1D audio waveforms remains a challenge. The first obstacle is the inherent heterogeneity of their respective data representations, compounded by the tendency of losing pitch information in audio [1,6]. By contrast, transforming a 1D audio waveform into a 2D spectrogram provides a rich representation of the audio signal's energy distribution over the frequency domain, capturing both pitch and resonance information along the time axis [9,12]. Second, the input sizes of the weighting maps vary between 20 × 5,745 and 20 × 11,938, while the output spectrogram has a fixed size for each audio section. Notably, fully connected layers used in [1] require fixed size input, while the possible fully convolution neural networks (CNN) can have varied output sizes and unstable performance [23]. Third, modeling global correlations for the long column dimension of the weighting map and the lack of spatial local neighboring relationships in the row dimension presents further difficulties for conventional CNNs that rely on deep hierarchy structure for expanding the reception field [2,21]. Furthermore, the limited number of training pairs available hinders the large model learning process.To address the aforementioned challenges, in this work, we propose an endto-end translator that generates 2D spectrograms from 2D weighting maps via a heterogeneous plastic light transformer (PLT) encoder and a 2D CNN decoder. The lightweight backbone of PLT can efficiently capture the global dependencies with a wide matrix input in every layer [14]. Our PLT module is designed with directional product relative position bias and single-level spatial pyramid pooling to enable flexible global modeling of weighting maps with variable sizes, producing fixed-size spectrograms without information loss or dimension expansion due to cropping, padding, or interpolation for size normalization. To deal with a limited number of training samples, we explore pair-wise utterance consistency as prior knowledge with Maximum Mean Discrepancy (MMD) [8] in a disentangled latent space as an additional optimization objective. Additionally, a generative adversarial network (GAN) [10] can be incorporated to enhance the realism of the generated spectrograms.The main contributions of this work are three-fold: Both quantitative and qualitative evaluation results demonstrate superior synthesis performance over comparison methods. Our framework has the potential to support clinicians and researchers in deepening their understanding of the interplay between tongue movements and speech waveforms, thereby improving treatment strategies for patients with speech-related disorders."
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,2,Methods,
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,2.1,Preprocessing,"During the training phase, we are given M pairs of synchronized tagged MRI sequences t i and audio waveforms a i , i.e., {t i , a i } M i=1 . First, we apply a non-linear transformation using librosa to convert a i into mel-spectrograms, denoted as s i with the function S : a i → s i . This transformation uses a Hz-scale to emphasize human voice frequencies ranging from 40 to 1000 Hz Hz, while suppressing highfrequency instrument noise. Second, for each tagged MRI sequence t i , we use a phase-based diffeomorphic registration method [29] to track the internal motion of the tongue. This allows us to generate corresponding weighting maps denoted as H i , which are based on input motion features X i , including the magnitude and angle of each track, by optimizing the following equation.where λ and η denote the weights associated with the manifold and sparse regularizations, respectively, and Tr(•) represents the trace of a matrix. The graph Laplacian is denoted by L."
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,2.2,Encoding Variable Size H i with Plastic Light-Transformer,"Directly modeling correlations among any two elements in a given weighting mapThe recent efficient vision transformers (ViTs) [5,14,20,20,32] usually adopt a local patch design to compute local self-attention and correlate patches with CNNs. Specifically, the input is divided into, each of which is flattened to a token vector with a length of d = P x × P y [7]. The local self-attention is then formulated with a complexity of O(N i d 2 = X i Y i d) as follows:where vectorsNi×d are produced by the linear projections of query (W q ), key (W k ), and value (W v ) branches, respectively [5,7,32]. The global correlation of ViTs with CNN [5,31,32] or window shifting [20], however, may not be efficient for our wide matrix H i , which lacks explicit row-wise neighboring features and may have a width that is too long for hierarchical convolution modeling. To address these challenges, we follow the lightweight ViT design [14], which uses a global embedding G ∈ R T ×d with T N i randomly generated global tokens as the anchor for global information aggregation Ĝi . The aggregation is performed with attention of G q , H k i , H v i , which is then broadcasted with attention of H q i , Ĝk i , Ĝv i to leverage global contextual information [14]. While LightViT backbones have been shown to achieve wide global modeling within each layer [14], they are not well-suited for our variable size input and fixed size output translation. Although the self-attention scheme used in ViTs does not constrain the number of tokens, the absolute patch-position encoding in conventional ViTs [7] can only be applied to a fixed N i [32], and the attention module will keep the same size of input and output. Notably, the number of tokens N i will change depending on the size of X i × Y i . As such, in this work, we resort to the directional product relative position bias [28] to add R i ∈ R Ni×Ni , where element r a,b = p δ x a,b ,δ y a,b is a trainable scalar, indicating the relative position weight between the patches a and b 2 . We set the offset of patch position in 1 The bottom-right boundary is padded with 0 to ensure Xi%Px = 0 and Yi%Py = 0. 2 a learnable matrix p ∈ R (2Px-1)×(2Py -1) is initialized with trunc normal , where Px = 20 and Py = 12000 20 = 600 are the maximum patch dimensions in our task.x and y directions δ x a,b = x ax b + P x , δ y a,b = y ay b + M y as the index in p. Furthermore, the product relative position bias utilized in this work can distinguish between vertical or horizontal offsets, whereas the popular cross relative position bias [28] in computer vision tasks does not need to differentiate between time and spatial neighboring relationships in two dimensions.Therefore, for global attention, we can aggregate the information of local tokens by modeling their global dependencies with"
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Ĝi,"Then, these global dependencies are broadcasted to every local token:By adding H local i and H global i , each token can benefit from both local and global features, while maintaining linear complexity with respect to the input size. This brings noticeable improvements with negligible FLOPs increment. However, the sequentially proportional patch merging used in [5,14,32] still generates output sizes that vary with input sizes. Therefore, we utilize the single-level Spatial Pyramid Pooling (SSPP) [13] to extract a fixed-size feature for arbitrary input sizes. As illustrated in Fig. 1, the output of our channel-wise SSPP module with 20 × 256 bins has the size of 20 × 256 × d, which can be a token merging scheme that adapts to the input size. Therefore, the final output of a layer is given byWe cascade four PLT layers with SSPP as our encoder to extract the feature representation f i ∈ R 8×8×d . For the decoder, we adopt a simple 2D CNN with three deconvolutional layers to synthesize the spectrogram si ."
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,2.3,Overall Training Protocol,"We utilize the intermediate pairs of {H i , s i } M i=1 to train our translator T , which consists of a PLT encoder and a 2D CNN decoder. The quality of the generated spectrograms si is evaluated using the mean square error (MSE) with respect to the ground truth spectrograms s i :Additionally, we utilize the utterance consistency in the latent feature space as an additional optimization constraint. Specifically, we propose to disentangle f i into two parts, i.e., utterance-related f u i and subject-related f s i . In practice, we split the utterance/subject-related parts channel-wise using tensor slicing method. Following the idea of deep metric learning [19], we aim to minimize the discrepancy between the latent features f u i and f u j of two samples t i and t j that belong to the same utterance. Therefore, we use MMD [8] as an efficient discrepancy loss L MMD = γMMD(f u i , f u j ), where γ = 1 or 0 for same or different utterance pairs, respectively.Of note, the f s i is implicitly encouraged to incorporate the subject-related style of the articulation other than f u i with a complementary constraint [16,18] for reconstruction. Therefore, the decoder, which takes f s i conditioned on f u i can be considered as the utterance-conditioned spectrogram distribution modeling. This approach follows a divide-and-conquer strategy [3,17] for each utterance and can be particularly efficient for relatively few utterance tasks.A GAN model can be further utilized to boost the realism of si . A discriminator D is employed to differentiate whether the mel-spectrogram is real s i = S(a i ) or generated si = T (H i ) with the following binary cross-entropy loss:In adversarial training, the translator T attempts to confuse D by optimizingOf note, T does not involve real spectrograms in log(D(s i )) [24]. Therefore, the overall optimization objectives of our translator T and discriminator D are expressed as:where β and λ represent the weighting parameters. Notably, only T is utilized in testing, and we do not need pairwise inputs for utterance consistency. Recovering audio waveform from mel-spectrogram can be achieved by the well-established Griffin-Lim algorithm [11] in the Librosa toolbox."
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,3,Experiments and Results,"For evaluation, we collected paired 3D tagged MRI sequences and audio waveforms from a total of 29 subjects, while performing the speech words ""a souk"" or ""a geese,"" with a periodic metronome-like sound as guidance [15,30]. The tagged-MRI sequences consisted of 26 frames, which were resized to 128 × 128.The resulting H matrix varied in size from 20 × 5,745 to 20 × 11,938 (we set one dimension to a constant value of 20.) The audio waveforms had varying lengths between 21,832 to 24,175. To augment the dataset, we employed a sliding window technique on each audio, allowing us to crop sections with 21,000 time points, resulting in 100 audio waveforms. Then, we utilized the Librosa library to convert all audio waveforms into mel-spectrograms with a size of 64 × 64. For our evaluation, we utilized a subject-independent leave-one-out approach. For the data augmentation of the H matrix, we randomly drop the column to round Y i to the nearest hundred, e.g., 9,882 to 9,800, generating 100 versions of H. We utilized the leave-one-out evaluation, following a subject-independent manner. In our implementation, we set P x = 1 and P y = 20, i.e., d = 20. Our encoder consisted of four PLT encoder layers with SSPP, to extract a feature f i with the size of 8 × 8 × 20. Specifically, the first 8 × 8 × 4 component was set as the utterance-related factors, and the remaining 16 channels were for the subject-specific factors. Then, the three 2D de-convolutional layers were applied as our decoder to generate the 64 × 64 mel-spectrogram. The activation units in our model were rectified linear units (ReLU), and we normalized the final output of each pixel using the sigmoid function. The discriminator in our model consisted of three convolutional layers and two fully connected layers, and had a sigmoid output. A detailed description of the network structure is provided in the supplementary material, due to space limitations.Our model was implemented using PyTorch and trained 200 epochs for approximately 6 h on a server equipped with an NVIDIA V100 GPU. Notably, the inference from a H matrix to audio took less than 1 s, depending on the size of H. Also, the pairwise utterance consistency and GAN training were only applied during the training phase and did not affect inference. For our method and its ablation studies, we consistently set the learning rates of our heterogeneous translator and discriminator to lr T = 10 -3 and lr D = 10 -4 , respectively, with a momentum of 0.5. The loss trade-off hyperparameters were set as β = 0.75, and we set λ = 1.It is important to note that without NMF, generating intelligible audio with a small number of subjects using video-based audio translation models, such as Lip2AudSpect [1], is not feasible. As an alternative, we pre-processed the input by cropping, padding with zeros, or using bi-cubic interpolation to obtain a fixed-size input H. We then compared the performance of our encoder module with conventional CNN or LightViT [14].Figure 2 shows a qualitative comparison of our PLT framework with CNN and LightViT [14] using bi-cubic interpolation. We can observe that our generated spectrogram and the corresponding audio waveforms demonstrate superior alignment with the ground truth. It is worth noting that the CNN model or the CNN-based global modeling ViTs [5,31] require deep models to achieve large receptive fields [2,21]. Moreover, the interpolation process adds significant computational complexity for both CNN and LightViT, making it difficult to train on a limited dataset. In Fig. 3(a), we show that our proposed PLT framework achieves a stable performance gain along with the training and outperforms CNN with the crop, which lost the information of some functional units.Following [1], we used 2D Pearson's correlation coefficient (Corr2D) [4], and Perceptual Evaluation of Speech Quality (PESQ) [22] as our evaluation metrics to measure the synthesis quality of spectrograms in the frequency domain, and waveforms in the time domain, respectively. The numerical comparisons of different encoder structures with conventional CNN or LightViT with different crop or padding strategies and our PLT framework are provided in Table 1. The standard deviation was obtained from three independent random trials. Our framework outperformed CNN and lightViT consistently. In addition, the synthesis performance was improved by pair-wise disentangled utterance consistency MMD loss and GAN loss, as demonstrated in our ablation studies. Furthermore, it outperformed the in-directional cross relative position bias [28], since two dimensions in the weighting map indicate time and spatial relationship, respectively. Notably, even though CNN with SSPP can process varied size inputs, it suffers from limited long-term modeling capacity [2,21] and unstable performance [23]. The sensitivity analysis of our loss weights are given in Fig. 3(b) and (c), where the performance was relatively stable for β ∈ [0.75, 1.5] and λ ∈ [1,2]. "
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,4,Conclusion,"This work aimed to explore the relationship between tongue movements and speech acoustics by translating weighting maps, which represent the functional units of the tongue, to their corresponding audio waveforms. To achieve this, we proposed a deep PLT framework that can handle variable-sized weighting maps and generated fixed-sized spectrograms, without information loss or dimension expansion. Our framework efficiently modeled global correlations in wide matrix input. To improve the realism of the generated spectrograms, we applied pairwise utterance consistency with MMD constraint and adversarial training. Our experimental results demonstrated the potential of our framework to synthesize audio waveforms from weighting maps, which can aid clinicians and researchers in better understanding the relationship between the two modalities."
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Fig. 1 .,
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Fig. 2 .,
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Fig. 3 .,
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Table 1 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,1,Introduction,"High myopia (HM) has become a global concern for public health, with its markedly growing prevalence [10] and its increased risk of irreversible vision loss and even blindness [14,16,25,27]. In brief, excessive axial elongation in HM eyes will produce mechanical stretching on the posterior segment of eyeballs, leading to various structural changes and HM-related complications, e.g., myopic maculopathy (MM), and consequently, functional changes, resulting in vision loss.Accurate quantification of vision loss is integral to the early detection and timely treatment for MM and other HM-related complications [16]. Currently, the diagnosis of vision loss is made on the basis of visual field (VF) sensitivity by standard automated perimetry, which is a systematic metric and gold standard to quantify visual function [19]. However, measuring VF is prohibitively time-consuming and subjective as it highly requires patients' concentration and compliance during the test [12].Conversely, imaging techniques, such as fundus photography (a.k.a., fundus), provide a relatively objective and robust measurement of the retinal morphology, which likely corresponds to the VF with an underlying ""structure-function relationship"" [27,32]. Actually, fundus is most commonly used for the diagnosis and evaluation of HM and its complications, in particular in rural and developing regions, with its lower cost and convenience of acquisition [17].Therefore, utilizing machine learning models to estimate VF from fundus becomes a promising and feasible alternative for HM subjects in clinical practice. To the best of our knowledge, there is no existing approach to estimate VF from fundus. Some studies have been proposed to estimate the global indices (e.g., mean deviation) of VF from fundus [3,11], and others estimate VF using retinal thickness [4,18,28,30]. It is worth mentioning that, all these studies were conducted for the glaucoma population [3,4,11,18,28,30], in which most cases of visual abnormality or defect were likely glaucomatous. However, MM and other HM-related complications may lead to non-glaucomatous vision loss.Actually, estimating VF with conventional regression [18] using fundus fails to predict local vision loss in our HM population, producing stationary nonsense predictions. As shown in Fig. 1, these predictions from regression exhibit a relatively similar and consistent pattern in most HM subjects, failing to capture/learn the The reason for such failure lies in regression's inability to learn high-entropy feature representations [31], which is further confirmed by measuring the entropy of feature representations, as marked in blue in Fig. 2a.To tackle this challenge, we propose a novel method for estimating VF for HM using fundus, namely VF-HM. In general, VF-HM incorporates VF properties and is additionally regularized by an auxiliary task, thereby learning relatively high-entropy feature representations (see the orange line in Fig. 2a). In detail, we formulate VF estimation as an ordinal classification problem, where each VF point is interpreted as an ordinal variable rather than a continuous one, given that any VF point is a discrete integer with a relative ordering. Besides, we introduce an auxiliary task for MM severity classification to assist the generalization of VF estimation, because MM is strongly associated with vision loss in HM [7,16,21,32] and its symptom can be observed from the fundus directly. As a result, VF-HM significantly outperforms conventional regression and accurately predicts vision loss (see Fig. 1).Our contributions are summarized as follows:-We propose a novel method, VF-HM, for estimating VF from fundus for HM. VF-HM more accurately detects the local vision loss and significantly outperforms conventional regression by 16.61% in the MAE metric on a real dataset. -VF-HM is the first work for VF estimation using fundus for HM, allowing for more convenient and cost-efficient detection of vision loss in HM, which could be useful for not only clinics but also large-scale vision screenings."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,2,Problem Formulation,"Let D = {(x i , m i )} denote the training set, where x i ∈ X denotes the fundus, m i ∈ M denotes its corresponded VF. And A = {(x i , y i )} denotes the auxiliary set, where y i ∈ Y denotes the MM severity category of a given x i . The objective is to learn a model f : X -→ M by utilizing both D and A. The novelty of this formulation is additionally utilizing the auxiliary set to improve the model's generalization. And challenges mainly come from the following two aspects. First, how to design the model f , as mentioned earlier, conventional regression fails to predict local vision loss. Second, how to properly utilize the auxiliary set to assist the generalization of f , as the auxiliary information is not always helpful during the learning progress, i.e., sometimes may interfere [5,6,22].3 Proposed Method: VF-HMIn this section, we first present an overview of the proposed method. Then, we introduce the details of different components."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,3.1,Overview,"We present an overview of the proposed method in Fig. 2b. Specifically, the primary task (denoted by T pri ) is the VF estimation and the auxiliary task is MM classification (denoted by T aux ). Then, our method aims to solve T pri with the assistance of T aux . We propose to parameterize the solution for T pri and T aux by two neural networks: f (•; θ, φ) and g(•; θ, ψ), where they share the same backbone θ and have their own task-specific parameters φ and ψ. Thereafter, the overall objective function is formulated as follows:where L pri and L aux denote the loss function for T pri and T aux , respectively. λ ∈ (0, 1] is a hyper-parameter to control the importance of L aux ."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,3.2,Primary Task: VF Estimation,"The overall interest is only the primary task T pri , which is parameterized by f (•; θ, φ) : X -→ M. Specifically, we formulate T pri as an ordinal classification (aka, rank learning) problem, where each VF point m j i represents an ordinal variable/rank rather than a continuous one. Such a formulation incorporates the distinct properties of VF, which include: 1) Discretization:i , there is a relative order among VF values. To achieve this goal, we extend the ordinal variable/rank into binary labels [2,13], i.e., m j i = [r j,1 i , ..., r j,K-1 i ] T where r j,k i ∈ {0, 1} indicates whether m j i exceeds k-th rank or not. To ensure rank-monotonic and guarantee prediction consistency, we utilize the ordinal bias [2]. In detail, the task-specific parameter φ contains independent bias for each ordinal variable. Thereafter, T pri can be solved by the binary cross-entropy loss, which is defined as follows:where L BCE (•) denotes the binary cross-entropy loss In addition, we propose to reuse the features from different blocks, as they contain distinct spatial information. Specifically, we propose Multi-scale Feature Fusion (MFF) for aggregating features from different blocks. As highlighted in orange in Fig. 2b, MFF aggregates features from all blocks at the last in an addition operation. The detailed implementation is reported in Sect. 4.2."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,3.3,Auxiliary Task: MM Classification,"The auxiliary task T aux is introduced only to assist the generalization of T pri . Specifically, T aux is to predict MM severity category y i from fundus x i , which is parameterized by g(•; θ, ψ) : X -→ Y. MM is highly correlated to vision loss [7,16,21,32], and its symptom can be observed from the fundus directly. According to its increasing severity, MM can be classified into five categories [26], i.e., C 0 ≺ C 1 ... ≺ C 4 . Therefore, we also interpret the MM category as the ordinal variable/rank. Similar to the label extension in T pri , we extend the MM category into binary labels y i = [r 1 , r 2 , r 3 , r 4 ] T . The loss function L aux for solving T aux is also the binary cross-entropy, which is defined as follows:However, the T aux is not always helpful for T pri because of the negative transfer [5,6,22]. The negative transfer refers to a problem that sometimes T aux becomes harmful for T pri . Specifically, let ∇ θ L denote the gradient of Eq. ( 1) in terms of the shared parameters θ, and it can be decomposed as follows:T aux becomes harmful for T pri , when the cosine similarity between ∇ θ L pri and ∇ θ L aux becomes negative [6], i.e., cos(∇ θ L aux , ∇ θ L pri ) < 0. Negative transfer is observed in our setting when optimizing Eq. ( 1) directly, as illustrated in Fig. 3a. Following [6], we mitigate negative transfer by refining ∇ θ L aux . Specifically, we adapt the weighted cosine similarity to refine ∇ θ L aux , which is defined as follows:"
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4,Experiments,"In this section, we conduct experiments on a clinic-collected real-world dataset to evaluate the performance of our proposed method1 . "
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4.1,The Studied Data,"The studied data comes from a HM population, including 75 patients, each with diagnosis information for both eyes. For each eye, there are one fundus, VF, and MM severity category. Specifically, the fundus is captured in colorful mode, the VF is measured in the 24-2 mode with 52 effective points, and MM category is labeled by registered ophthalmologists. Besides, 34 patients (i.e., 68 eyes) have SD-OCT scans in the macular region. For these SD-OCT scans, we extract the retinal thickness with the pre-trained model [20] in order to compare our method to conventional regression using retinal thickness. According to whether the eye has SD-OCT scans or not, we divide the whole data into a training set and a test set. Specifically, the training and test data contain 68 eyes (from 34 patients) and 82 eyes (from 41 patients), respectively. It is worth mentioning that the training data and test data do not have the same patient. Besides, in the following K-fold cross-validation experiments, we split the training data based on the patient's ID to ensure that there is no information leakage."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4.2,Experimental Setup,"Data Pre-processing. We choose the left eye pattern as our base. For fundus, VF and retinal thickness are not in the left eye pattern, we convert them using the horizontal flip.Data Augmentation. Following [1], we consolidate a set of data augmentations for both fundus and retinal thickness, respectively. The details are reported in the supplementary material. Different from applying all [1] augmentations during training, we utilize the TrivialAugment [15] instead, which randomly selects one from the given data augmentations, generating more diverse augmented data.Evaluation Methods. For quantitative evaluation, we utilize three metrics [3,4,18,29,33]: RMSE, MAE and SMAPE. For qualitative evaluation, we visualize two representative predictions on the test set, and more visualized results are presented in the supplementary material. Baseline Methods. We mainly compare our method to conventional regression that estimates VF from fundus. Besides, for a more comprehensive comparison, we also compare our approach to conventional regression using different retinal thicknesses. In detail, we consider three variants: (a) the combination of GCIPL, RNFL and RCL [33], (b) the combination of GCIPL and RNFL [18], (c) only RNFL [4]. Due to the limited data, we compare our method to conventional regression using the above thickness by K-fold cross-validation on training data.Implementation Details. We utilize the ResNet-18 [8] as the backbone. For the regression baseline, we use only one linear layer at last. For our method, we use the combination of Conv2D, BatchNorm2D and ReLU as the classification head for T pri . For the MFF, we utilize the above classification head to aggregate features from different blocks. Note that the features from earlier blocks have relatively large features, thus we use AdaptiveAvgPooling2D to perform downsampling first. For T aux , we use only one linear layer as the classifier. For a fair comparison, we train all methods with the same training configurations. Specifically, we train the models with 80 training epochs and the SGD optimizer, where the batch size is set to 32, the learning rate is set to 0.01, momentum is set to 0.9 and L2 weight decay is set to 1e -4 . Besides, we utilize a cosine learning rate decay [9] to adjust the learning rate per epoch. Finally, we fix all input resolutions to 384 × 384 for both training and evaluation. All experiments are run independently with four seeds: 0, 1, 2, and 3. As for hyper-parameters, we search them on training data with K-fold cross-validation."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4.3,Experimental Results,"Main Results. Table 1 reports the performance of our method and baselines. In general, our method achieves the best performance compared to these baselines. Specifically, compared to conventional regression using fundus, our method outperforms it by 13.79% and 16.61% according to the RMSE and MAE metric on test data. Besides, our method achieves better performance than baselines using different retinal thicknesses. Visualization of Predictions. As shown in Fig. 1, we visualize predictions from methods using fundus on two representative cases. Specifically, conventional regression fails to predict local vision loss, as its predictions share a similar and consistent pattern for both cases. In contrast, predictions from our method are more precise, revealing the local vision loss. More visualized results are presented in the supplementary material."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4.4,Ablation Study,"To get a better understanding of the effectiveness of the main components in our proposed method, we conduct a series of ablation studies.Effectiveness of Main Components. We first examine the effectiveness of the main components by ablating them. The results are reported in Table 2. In general, we can observe that all components can improve performance except AUX. Specifically, AUX denotes solely introducing the auxiliary task, which brings a degradation, because of the existence of negative transfer. Meanwhile, with the help of Eq. ( 5), the negative transfer can be mitigated. Besides, we observe these main components allow the model to learn high-entropy feature representations, thereby improving the model's performance [31]. More details are reported in the supplementary material.Impact of Hyper-parameter λ. We study the impact of the hyper-parameter λ with K-fold cross validation on training data. We choose λ ∈ {1.0, 0.1, 0.01, 0.001, 0.0001}. According to the results shown in Fig. 3b, we observe that λ = 0.1 achieves the best performance.Different Methods for Mitigating the Negative Transfer. We consider three alternatives to refine the auxiliary gradient for mitigating the negative transfer: (1) weighted cosine (WC) similarity [6] (2) unweighted cosine (UC) similarity [6] (3) projection (P) [22]. For a fair comparison, we set λ = 0.1, then conduct experiments on training data with K-fold cross-validation. As shown in Fig. 3c, and we observe that (1) WC achieves the best performance."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,5,Conclusion,"In this work, we propose VF-HM for estimating VF from fundus for HM, which is the first work for VF estimation in HM; and it provides a more convenient and cost-effective way to detect HM-related vision loss. The major limitations include: first, our sample size is limited; second, we utilize both eyes from one patient as two independent inputs, which ignores their similarity; third, we only include the MM severity as the auxiliary information. Future work could be conducted as follows. First, collecting more data from different clinical sites. Second, modeling the relationship between both eyes from the same patient [33].Third, exploring more auxiliary information. Besides, studying how to adapt our method to different domains is a crucial problem [24], as we seek to improve the generalizability. In addition, exploring VF prediction with the missing modalities [23]: either fundus or thickness is another interesting direction."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Fig. 1 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Fig. 2 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Fig. 3 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Table 1 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Table 2 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_61.
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,1,Introduction,"Automatic airway labeling aims to assign the corresponding anatomical names to the branches in airway trees. The identification of peripheral branches plays an essential role in bronchoscopic navigation. Figure 1 illustrates the hierarchical nomenclature [11] from lobar to subsegmental levels in the bronchial tree. In the first level, the naming is based on the five lung lobes, and similarly, the subtrees are further divided according to the 18 lung segments. The nomenclature of subsegmental bronchi is more complex, which contains six classes (a, b, c, a+b, a+c, and b+c) for the subtrees and their common branches.Various methods have been proposed for airway labeling [2,6,7,9,10,14,16,21,22] in recent years. Several works [2,6,16] adopted graph-matching-based algorithms to clarify the candidate trees based on a reference tree. However, the performance is limited at the subsegmental level due to individual variation. Deep learning methods [14,21,22] are also developed for this task. Tan et al. [14] proposed a multi-task U-Net [12] with a structure-aware graph convolutional network (GCN) [5] to segment the airway tree semantically. Yu et al. [21] converted the airway tree from the image to the graph space and designed a multi-stage framework with hypergraph neural networks (HGNN) for node classification. There are two main challenges for these learning-based methods. First, 127 classes are included in the nomenclature up to subsegmental level. The annotation is quite time-consuming and labor-intensive. Classification for over one hundred categories with limited training data is an extremely hard problem. Second, the inter-individual differences regarding the location, direction, length, and diameter of branches become increasingly significant from the lobar order to the subsegmental level. Figure 1(d) demonstrates the t-distributed stochastic neighbor embedding (t-sne) results of the learned features of RB10 branches in two examples. The distribution of RB10b in the first case is overlapped with the distribution of RB10c in the second example. Such individual variation dramatically affects the generalization ability of models.To resolve the first problem, in this work, we propose to fully use the latent relationships within the tree structure and airway nomenclature by a novel network named AirwayFormer. The tree structure provides inherent message transmission roads, while both global and local information is critical for anatomical labeling. To this end, we first adopt a neighborhood information encoding module based on the transformer block to aggregate both global and local features within the tree structure. Another structural relationship appears in the nomenclature of bronchial trees. As shown in Fig. 1(a), (b), and (c), the subsegmental bronchi RB10a belong to RB10 in the segmental level and the right lower lobe bronchi (RLL) in the lobar level, respectively. The consistency between classes at different levels is an important cue in this task. In previous works, Yu et al. [21] introduced the classification results of the current stage to the next stage as extra constraints. However, the inference is only single-directional, where the prediction is not used to refine the classification in the upper level. To achieve a bi-directional refinement, a U-shape layout is designed to explore the correspondence between different nomenclature levels. For the second challenge, an ideal solution is that the model can adaptively adjust itself to deal with individual variation. Especially in subsegmental level, the same category in different cases may have distinct features. Actually, the nomenclature of subsegmental bronchi depends on their relative positions to the sibling branches within a segment. Based on this prior knowledge, we design a novel generator that learns to capture the relative relationships by predicting the weights of the last fully connected layer. More specifically, the weight for a subsegmental class is generated based on the feature representation of the corresponding segmental class. As illustrated in Fig. 1, compared with using fixed weights in the classifier, the predicted weights dynamically adjust the decision boundaries according to the characteristics of the segmental bronchi in each case, alleviating the overfitting problem caused by individual variation.Our main contributions can be summarized as follows: (1) AirwayFormer is proposed for accurate airway labeling up to subsegmental level by exploiting the latent structural relationships. (2) A weight generator is designed to mitigate the overfitting caused by individual variation via adaptive decision boundary adjustment. ( 3) With extensive experiments on the public dataset, our method achieves state-of-the-art results in lobar, segmental, and subsegmental levels."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,2,Method,"An overview of our proposed method is illustrated in Fig. 2. Following previous work [21], each branch is regarded as a node in a graph space with well-designed 20-dimensional features. Five transformer blocks are used to classify these nodes hierarchically. The weight matrix of the subsegmental classifier is dynamically updated according to the segmental features. We denote the function of the  m-th transformer blocks asdenotes the input feature of the corresponding transformers. The node number and feature dimension are denoted by n and d, respectively. The number of categories of the m-th classifier is c m . More details about our method are introduced in the following sections."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,2.1,Structure-Aware Transformers,"We propose a transformer-based model named AirwayFormer to exploit the structural relationships in airway labeling. The tree structure is introduced into the self-attention calculation, while the class consistency in the hierarchical nomenclature is encoded in the U-shape layout. Self-attention module in transformers enables nodes to aggregate messages from a global scale while neglecting the local structure prior. Recent studies [1,8,20] showed that incorporating graph information into vanilla transformers can achieve competitive performance. As the labeling of peripheral branches needs both global and local information, we adopt a neighborhood information encoding (NIE) module, which integrates the structural prior into the selfattention mechanism. Given an airway tree with n branches, each branch can be seen as a node v. The parent bronchus and children bronchus is adjacent. Then we adopt a distance function ψ(v i , v j ) to measure the spatial relation between nodes v i and v j . Here, ψ(v i , v j ) is defined as the shortest path distance (SPD) between v i and v j . Finally, a codebook C m is used to convert the SPD matrix D ∈ R n×n to learnable scale parameters, which can serve as a graph bias term of the attention map A ∈ R n×n :Here, Q ∈ R d×d and K ∈ R d×d are trainable parameters. Matrix D encodes the neighborhood information of the tree structure, and the network can adaptively concentrate on the local structure by optimizing C m .To simultaneously utilize the bi-directional correspondence between different levels, AirwayFormer adopts a U-shape layout. Concretely, the network performs nodes classification hierarchically from lobar level to subsegmental level and then back to lobar level:(3)Here, G m ∈ R n×d is the output feature of the m-th transformer blocks, and CAT (•) denotes the concatenate operation. Z m (•) is the m-th classifier consisting of a simple linear layer while P m ∈ R n×cm is the prediction result. The advantages of this design are two-fold. First, the output of coarser level is used as the input of finer level, which is helpful for the network to learn the corresponding relationship of the categories from coarse to fine. Second, the results of coarser level can be directly deduced from the predictions of finer level. Fine-grained features feedback to coarser level reduces the classification difficulty of coarser level and promotes the labeling performance. Besides, transformers conducting labeling of the same level are connected directly using highways. The same-level features can prevent performance degradation and ameliorate training stability. We further stop gradient back-propagation from segmental level to subsegmental level. The reason is that the severe distribution overlap in subsegments makes the feature learning in this level markedly different from others. This is further discussed in the supplementary materials."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,2.2,Boundary-Adaptive Classifier,"Individual variations cause the distribution of adjacent classes to intersect, especially in subsegmental level. Although most subsegmental bronchi in the same tree are separable according to their relative positions, the inter-individual differences seriously affect the generalization performance of the model. To introduce case-specific information, we propose a boundary-adaptive classifier that adopts a novel generator to predict case-specific weights for each subsegmental category. Let G k ∈ R n×d denote the subsegmental output feature. We first use the next segmental level feature G k+1 ∈ R n×d and prediction P k+1 ∈ R n×c k+1 to obtain coarser representation for each segmental class:Here, S k+1 ∈ R n×c k+1 denotes the probability matrix of the segmental level while α is a learnable cluster parameter. H = (h 1 , h 2 , ..., h c k+1 ) T ∈ R c k+1 ×d are representations for c k+1 segmental categories. However, the segmental prediction P k+1 is not always perfect and may contain some classification errors. To avoid potential error propagation and obtain better class representations, these cluster centers are refined using a vanilla transformer. Specifically, we take H as query vectors and G k+1 as key and value vectors:After getting the enhanced segmental class representations, a generator network T (•) is used to produce classifier weights for subsegmental categories. Since each segment contains six subsegmental bronchi, T (•) transforms each segmental center into the seven (including itself) classifier weights:Here, Gelu(•) denotes the GELU activation function [3]. A bottleneck architecture is used to limit the number of parameters: V ∈ R d×b and U ∈ R b×7×d are down-projection and up-projection learnable matrices respectively where b d. Finally, the weight matrix of the subsegmental classifier can be obtained by stacking w i ∈ R 7×d .We use cross-entropy loss with labeling smoothing [13] as the loss function for each task. Then the total loss function can be formulated aswhere γ m is the weight of the m-th loss function."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,3,Experiments and Results,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,3.1,Dataset and Implementation Details,"We evaluated our method on the public Airway Tree Labeling (ATL) Dataset1  [21]. The dataset contains 104 labeled bronchial trees from CT scans whose slice thickness ≤ 0.67 mm and spatial resolution ranges from 0.78 mm to 0.82 mm. The annotation is three-level, including six lobar bronchi, 19 segmental bronchi, and 127 subsegmental bronchi. We conducted 4-fold cross-validation on the dataset. We set the label smoothing hyperparameter σ of the loss function to 0.02. The weight of loss function γ m was set to 1. More experiments about hyperparameters can be found in the supplementary materials. We stacked two transformer blocks for airway labeling of each level. The model was trained using Adam optimizer (β 1 = 0.9, β 2 = 0.999) with a learning rate of 5e -4 for 800 epoches. Predictions of the last lobar and segmental classifiers were used to be the final results of these levels. Four evaluation metrics were used: (a) accuracy (ACC), (b) precision(PR), (c) recall (RC), and (d) F1 score (F1). All the networks were implemented in PyTorch framework with a GeForce GTX TITAN XP GPU.  "
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,3.2,Evaluations,"Table 1 compares the quantitative results of our proposed method with other methods. GNNs methods such as GCN [5] and GAT [18] only aggregate information from neighborhoods, while DAGNN [15] gradually performs message passing and updating from root nodes to the end. These methods merely encode the local structure of airway trees and are seriously disturbed by distribution overlapping in subsegments. Thus, they fail to achieve satisfactory performance, especially in subsegmental level. In HGNN methods, HyperGCN [19] and UniSAGE [4] use hyperedges to represent subtrees. TNN [21] further proposes a sub-network for subtrees to exchange information and outperforms other HGNNs methods. SGNet [14] uses CNNs to conduct semantic segmentation directly on CT images, which can be easily affected by class imbalance. Conventional methods [2,6,16] usually adopt graph-matching-based algorithms, but the reference tree cannot meet all situations due to individual variations. Thus, their performance drops a lot in the subsegmental level. Vanilla Transformer [17] achieves the same effect as TNN owing to the global-scale self-attention module while lacking the utilization of structural prior. Compared to all the above methods, our approach improves the accuracy by more than 2% and 4% respectively in segmental and subsegmental levels, demonstrating the effectiveness of the proposed method.Qualitative results are displayed in Fig. 3 to demonstrate the superior performance of our model at different levels. In segmental labeling, transformer lacks the local structural prior, in which way the sibling categories cannot be classified well. The problem is more severe in subsegments. TNN adopts a subtree interaction module to learn relative information, but the correspondence between different nomenclature level is not fully utilized. By contrast, our method can perform accurate airway labeling both in segmental and subsegmental levels. We further conducted ablation studies to verify the effectiveness of each component of AirwayFormer. Table 2 shows the results. Vanilla transformer performing bronchi labeling for each level independently is used as the baseline. The U-shape layout encodes the correspondence of nomenclature and improves the results in all three levels. NIE module introduces the local structure prior to selfattention calculation and also promotes the labeling performance, especially in the subsegmental level. Combining the two modules, the performance is further ameliorated. The weight generator network dynamically adjusts the classification weights and improves the subsegmental results. When applying the three modules simultaneously, the network demonstrates the most powerful ability for airway labeling. The experimental results indicate that these proposed modules do contribute to the satisfactory performance of our method."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,4,Conclusion,"This paper presented a transformer-based method named AirwayFormer for airway anatomical labeling. A U-shape layout integrating graph information is used to exploit the latent relationships fully. Meanwhile, a weight generator that produces the dynamic decision boundaries is designed to capture the relative relationships between sibling categories. Extensive experiments showed that our proposed method achieved superior performance in the bronchi labeling of all three levels, leading to an efficient clinical tool for intra-operative navigation."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Fig. 1 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Fig. 2 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Fig. 3 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Table 1 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,98.8 98.2 98.5 95.7 95.9 95.9 95.9 86.0 83.5 84.5 84.0,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Table 2 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 37.
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,1,Introduction,"Cataract has been the leading cause of vision loss. As the only treatment option, cataract surgery is one of the most commonly performed surgeries worldwide. Nevertheless, not all patients achieve complete visual recovery after surgery, which can be due to various factors, such as pre-existing eye conditions, surgical complications, and postoperative inflammation. The ability to accurately predict visual recovery can help clinicians identify high-risk patients and provide appropriate interventions to improve their visual outcomes.Over the past decades, many efforts have been made to predict the Best Corrected Visual Acuity (BCVA) after cataract surgeries. Most of them are based on traditional approaches like retinometer [8,11,13] or visual electrophysiology [2,5,14]. These methods require specialized expertise to perform and are subject to significant variability in their results. Even though some computeraided approaches have been proposed, most of them use traditional machine learning algorithms [1] and focus on single-modal data [9,15,16]. While they can be effective to some extent, they often have limited predictive power due to the reliance on a single source of information. In addition, traditional machine learning methods (e.g., linear regression, decision trees, and support vector machines) may not be able to capture complex relationships between different modalities, such as clinical data and imaging data. What's more, clinical scenarios are more complex. For example, the multimodal data may be incomplete due to medical conditions, as shown in Fig. 1. Therefore, there is a need for more sophisticated computer-aided techniques that can integrate multiple sources of data and leverage the strengths of each to improve predictive accuracy as well as address the challenging modality-missing problem.Transformers [12], which are a type of neural network architecture originally developed for natural language processing tasks, have recently shown For instance, ""two images"" denotes the samples with two image modalities. We can see that only one-third of cases have complete multimodal images.great promise in computer vision applications, such as image classification [4], object detection [3], and segmentation [20]. Transformers have the ability to learn from large amounts of data and can capture complex patterns and relationships in the data, which makes them well-suited for analyzing multimodal learning. Motivated by the tremendous success of Transformers in multimodal learning [10,17,18], we propose a new framework that utilizes incomplete multimodal data for predicting BCVA after cataract surgeries. In particular, our framework contains three stages: modality-specific feature extraction, attentional feature fusion, and visual acuity prediction. Firstly, for each input modality, a pre-trained efficient transformer will be used to extract features. To better leverage the clinical diagnosis keywords, an auxiliary classification loss is added to the image transformer. And to extract text features more efficiently, we apply a CLIP-like [10] input to combine discrete clinical words (e.g., age, sex, and preoperative visual acuity) into sentences. Secondly, we apply an attentional transformer to fuse multimodal features. Specifically, to address the issue of missing modalities, we introduce modality embeddings and attentional masks to prevent the interference of missing modalities with the remaining modalities. Finally, a prediction head takes the fused features as input to predict the BCVA with the Mean Square Error (MSE) loss.The main contributions of this work can be summarized as follows: (1) We develop a novel framework that uses multimodal data to predict BCVA as well as tackle the complex modality-missing issue. (2) An auxiliary classification loss is adopted to extract more comprehensive pathological features for images. Also, discrete textual words are combined into sentences to better fit the text transformer input. (3) Extensive experiments are conducted to prove the effectiveness of our method. The compared methods include incomplete multimodal learning approaches and monomodal BCVA prediction methods. We also analyze the importance of each component of our method."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2,Method,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2.1,Framework Overview,"As shown in Fig. 2, our framework contains three parts: modality-specific encoder, multimodal fusion network, and BCVA prediction head. During feature extraction, we take pre-trained transformers as the backbone, i.e., ViT [4] as the image encoder, and CLIP [10] as the text encoder. After that, a crossmodal transformer is used to fuse features from multiple modalities, in which an attentional mask is added to tackle the missing modalities. Finally, a fully connected (FC) layer is used as the prediction head."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2.2,Monomodal Feature Extraction,"Text Encoder. The text encoder is a pre-trained CLIP [10] model. The discrete physiological information is combined into a sentence format that benefits the Fig. 2. Pipeline of the proposed framework. The modality-specific encoders utilize vanilla multi-head self-attention. In contrast, the multimodal fusion network employs masked multi-head self-attention. Notably, the fusion network takes features (i.e., tokens) from all modalities as input, whereas each auxiliary classification network receives features from a single modality as input.text encoder. For instance, the text ""male, 67 years old, preoperative visual acuity 0.52 logMAR"" will be combined into the sentence ""A 67-year-old male patient with preoperative visual acuity of 0.52 logMAR"". By combining the text in this way, the physiological texts of all patients are fed into the text transformer in a unified manner. Compared with directly concatenating the texts and inputting them into the model, the combined sentences are more in line with the real scene and are easier to be understood by the model to extract key semantic information. Moreover, the CLIP text encoder is trained on a large text corpus, enabling excellent generalization performance. Thus, during the training of the overall model, the weights of the text encoder can be fixed.Image Encoder. The image encoder adopts ViT [4]. Since ViT is trained on natural images and may not be directly applicable to medical images, it can not be fixed during the training. However, the pre-trained weights can be utilized to expedite convergence. In addition, there are diagnostic keywords given by ophthalmologists for each image in the dataset. It is not appropriate to directly use the CLIP text encoder to extract medical features from these keywords since CLIP is trained on natural language texts. To this end, we introduce an auxiliary classification loss in the image encoder. Specifically, for each input image, a multilabel classification network is incorporated after the image encoder to predict the diseases contained in the image. The classification network is composed of an average pooling layer and a fully connected layer. For simplicity, we adopt the binary cross-entropy loss as the auxiliary classification loss as Eq. ( 1).where W i equals 1 if the i-th modality is available and equals 0, otherwise. By adding the classification loss, the final loss to train the whole model is:where L MSE is the mean square error loss between the predicted BCVA and the ground truth, α is a hyper-parameter and set to 0.5."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2.3,Multimodal Feature Fusion,"We use a cross-modal Transformer as the multimodal fusion network. The obtained modality-specific tokens are projected into the same dimension and concatenated into an input sequence. In contrast to the modality-specific Transformer, besides adding positional embeddings to all input tokens, we also add learnable modality-specific embeddings to all tokens indicating the modality information.Complete Multimodal Learning. In the collected dataset, all cases have corresponding text modality, and almost all cases have corresponding OCT modality. Therefore, we built a complete multimodal prediction model based on the text modality and OCT modality. In such a situation, only the OCT encoder and text encoder will be preserved, while the SLO encoder and Ultrasound encoder will be dropped. Since transformers can handle sequences of any input length, we don't need to make any changes to the fusion network. Using complete multimodal learning, we can compare our methods to other BCVA prediction approaches which do not consider the incomplete multimodal scenario."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Incomplete Multimodal,"Learning. Not all cases have complete modalities for the three image modalities (i.e., OCT, SLO, and Ultrasound). For the missing modalities, one possible way is to simply represent them by 0 values [18]. However, the 0 values will be regarded as noise by the model as they do not contain any useful information. To avoid model degradation, we add attentional masks in the vanilla self-attention to exclude the interactions among missing modalities and available modalities.The proposed attentional masks are easy to implement. As shown in Eq. ( 3), self-attention in transformers is mainly matrix multiplication.in which, Q, K, and V are queries, keys, and values obtained from tokens, respectively. d z is the projection dimension. To avoid interactions between irrelevant tokens, the masked self-attention is computed as:in which, M is the mask matrix. For each element in M , it will be 0 if the interactions should be included, or it will be negative infinity to avoid unnecessary interactions. By adding negative infinity, the results of softmax will be very close to 0. Figure 3 shows an example of masks when modalities are missing. "
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2.4,Implementation Details,"To validate the effectiveness of the proposed method, we have conducted extensive experiments implemented with Pytorch and 8×RTX 3090 GPUs. The input images are resized to 224 × 224. The Adam optimizer is adopted with an initial learning rate of 0.001 and β 1 = 0.9, β 2 = 0.99. The mini-batch size is set to 32. We train the model for 100 epochs in total, and the learning rate will be decayed by 0.1 every 20 epochs. Besides, we randomly split all samples to 80% for training and 20% for testing. All experiments are conducted with 5-fold cross-validation to produce more solid results."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,3,Experiments,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,3.1,Datasets and Evaluation Metrics,"The collected dataset consists of 1960 patients (2685 eyes) having cataract surgeries at Aier eye hospital of Wuhan University. The collected modalities are texts and images. The images contain 2635 Optical Coherence Tomography (OCT), 2615 Ultrasound, and 988 Scanning Laser Ophthalmoscopy (SLO). The textual information includes sex, age, preoperative and postoperative visual acuity. For each image, three ophthalmologists will label it to obtain the diagnosis (i.e., clinical diagnosis keywords) of 14 retinal diseases. The retinal diseases include normal, vitreous opacity, posterior staphyloma, stellate vitreous degeneration, pathological myopia changes, retinal atrophy, macular degeneration, epiretinal membrane, ellipsoid band partially missing, retinoschisis, retinal hemorrhage, macular edema, macular hole, and retinitis pigmentosa. We use Mean Absolute Error (MAE), Symmetric Mean Absolute Percentage Error (SMAPE), and prediction accuracy as the metrics. For simplicity, we consider predictions to be accurate if the prediction errors are within ± 0.10 logMAR."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,3.2,Quantitative Performance,"We have compared the results on the collected dataset with other approaches.The compared methods include state-of-the-art methods which aim to predict BCVA using OCT like CTT-Net [15], Wei et al. [16], and other algorithms considering incomplete multimodal learning such as Huang et al. [6], Ma et al. [7], and Zhao et al. [19]. For the former methods, we compare our method with them using the complete data. As for the latter approaches, they are not proposed for BCVA prediction. Therefore, we finetune them so that they can be applied to incomplete BCVA prediction data. From Table 1, we can see that our proposed framework achieves the best performance. Specifically, we have improved CTT-Net [15] and shown a sharp rise compared to Wei et al. [16] on the complete dataset. Even though CCT-Net uses both text and oct modalities, the utilization of text is still limited. Wei et al. [16] directly ignores the textual information and only uses some simple frameworks to predict BCVA, thus achieving the worst results. When using incomplete data, the performance is also improved greatly, and we still achieve the best performance. Huang et al. [6] try to apply image synthesis to solve the modality-missing problem. However, the collected images in our dataset are not  [19] propose to learn common representations of all modalities, and this idea works in cases of minorly missing modalities but not in our dataset. Ma et al. [7] achieve similar performance to our proposed method due to their robust design. The results show that our model is capable of extracting modality-specific features as well as fusing them in an effective way. Besides, the performance also shows the robustness of our proposed method. Note that almost all results on the incomplete multimodal dataset outperform results on the complete multimodal dataset. This is due to the incomplete multimodal dataset will always contain OCT and text modalities, thus having more information in the input. Figure 4 shows the distribution of preoperative, predictive, and postoperative visual acuity. We can see that the predicted visual acuity largely overlaps with the true postoperative visual acuity, demonstrating the effectiveness of our method."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,3.3,Ablation Study,"As shown in Table 2, the proposed framework mainly benefits from the auxiliary classification loss and attentional fusion mask. Our analysis is as follows: Images provide more valuable information than text, making the auxiliary classification loss more effective than text combining. In missing multimodal learning, feature fusion takes precedence, and the masked self-attention mechanism contributes the most. Additionally, we conducted experiments to evaluate each modality's effectiveness. The results can be seen in the supplementary material."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,4,Conclusion,"In this paper, we present a new framework for BCVA prediction on the collected incomplete multimodal dataset. We take full advantage of multimodal information through our framework. The text modality is better utilized through the combination of the words. Moreover, image modality is explored effectively by the auxiliary classification loss. The attentional mask addresses the modalitymissing issue. Extensive experiments have proved the effectiveness and superiority of our method."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Fig. 1 .,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Fig. 3 .,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Fig. 4 .,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,"specific encoder Multimodal Fusion Network 12 Layers, C=768 6 Layers, C=384 Tokens Fused Features",
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Predicted Diseases Predicted Diseases Predicted Diseases Predicted BCVA Auxiliary Classification Network,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Table 1 .,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Table 2 .,"aligned, and image synthesis may not work in such a situation.Zhao et al.  "
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Acknowledgements,. This work is partially supported by Bingtuan Science and Technology Program (No. 2022DB005 and 2019BC008) and Key Research and Development Program of Hubei Province (2022BCA009).
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 69.
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",1,Introduction and Related Work,"Skin diseases are a major public health concern that impacts millions of people worldwide. The first step towards diagnosis and treatment of skin diseases often involves visual inspection and analysis of the lesion by dermatologists or other medical experts. However, this process is often subjective, time-consuming, costly, and inaccessible for many people, especially in low-resource communities or remote areas. It is estimated that around 3 billion people lack adequate access to dermatological care [1]. In the United States, only about one in three patients with skin disease are evaluated by a dermatologist, their average wait time exceeds 38 days while representing a cost of $75 billion on the healthcare system [2,3]. Therefore, there exists a growing need for automated methods that can assist dermatologists, especially those in low-resource environments, in attending to skin lesions accurately and efficiently.Skin lesion semantic segmentation and malignancy classification are essential for providing accurate and explainable diagnosis information for patients with skin diseases, and recently Artificial Intelligent (AI) systems have led the stateof-the-art for these tasks. However, these systems are commonly based on data and training methods that are prone to racial biases [4][5][6]. Some of the main challenges facing AI systems that can lead to bias are:-Data scarcity: Annotated medical images are often scarce and expensive to obtain due to privacy issues, cost, and expert availability. This limits the amount of data available for training Deep Learning (DL) models, which may result in overfitting, especially in a medical context, as shown in [7]. -Class imbalance: The distribution of different types of skin lesions is often imbalanced in real-world datasets. For example, melanoma cases may be rare than basal cell carcinoma cases; this could then be exacerbated by datasets that are primarily sourced from light-skinned populations [8]. This class imbalance can introduce biases in modeling. -Data diversity: The appearance and morphology of skin lesions can vary across different individuals due to factors such as age, gender, and ethnicity [9]. A dataset can be large but not necessarily diverse. This lack of diversity in the data may lead to poor generalization [4]. -Base models: Some recent works on dermatology images stem from transferlearning models designed for ImageNet [10], which may be overly large for smaller dermatologic datasets [11,12]. Tuning these massive encoders could lead to overfitting. -Lack of diverse studies: A recent review of 70 dermatological AI studies between 2015 and 2020 found that only 17 studies included ethnicity descriptors, and only 7 included skin tone descriptors [13]. This could lead to under-specification of model performance for different ethnicities.Denoising Diffusion Probabilistic Models (DDPMs) have been introduced [14] as a new form of generative modeling. DDPMs have achieved state-of-the-art performance in image synthesis [15] and are effectively applied in colorization [16], super-resolution [17], segmentation [18], and other tasks. In the medical domain, recent work has presented results for DDPM-based anomaly detection [19] and segmentation [20], but these are limited to MRI, CT, and ultrasonography not natural smartphone-captured images of dermatology conditions. To our knowledge, none have explored segmentation and malignancy classification in this context from DDPM-based embeddings without re-training and evaluated performance on diverse dermatology images.We introduce the FEDD framework, a denoising diffusion-based approach trained on small, skin tone-balanced, Diverse Dermatology Images (DDI) [4] subsets for skin lesion segmentation and malignancy classification that outperforms state-of-the-art across a diverse spectrum of skin tones and malignancy  "
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",2,Description of Data,"The most commonly used dataset to train and evaluate fairness in dermatology AI is Fitzpatrick17k [8,21,22] thanks to its large size of nearly 17,000 images. However it contains a significant skin tone imbalance (3.6 times more light than dark skin toned samples) and greater than 30% disease label noise [8]. Further, the samples are not biopsied and visual inspection itself can be an unreliable way of diagnosis without the use of histopathological information [23]. Recently, DDI [4] was published as a dermatological image dataset with Fitzpatrick-scale [24] scores for all images, classifying them as light (I-II), medium (III-IV), or dark (V-VI) in skin tone. While at a lower skin tone resolution (3-point instead of 6-point), these labels are reviewed by two board-certified dermatologists. It also includes a mix of rare and common benign and malignant skin conditions, all of which are confirmed via biopsy. The dataset contains visually ambiguous lesions that would be difficult to visually diagnose but represent the kind of lesions that are seen in clinical practice. DDI is somewhat balanced between skin tones, with about 16% more information for medium skin tones; however, it is not balanced between malignant and benign classes. In total, the dataset contains 656 samples. Details and distribution of diagnosis are shown in Fig. 1.We draw 4 balanced subsets of DDI for training, each representing approximately 5% (10 samples per skin tone), 10% (20 samples per skin tone), 15% (30 samples per skin tone), and 20% (40 samples per skin tone) of DDI. The smaller training sets are subsets of the larger ones, this is to say 5% ⊆ 10% ⊆ 15% ⊆ 20% ⊆ DDI. For classification we draw validation and test sets, each containing 30 samples (10 samples per skin tone). Further, we test model checkpoints trained on each DDI subset on all remaining DDI images (476 samples), accuracy results from this larger test set are reported on the paper text and on Table 3 of the supplementary materials. For segmentation, we test on 198 additionally annotated samples. This is due to DDI including disease labels suitable for malignancy classification. For segmentation however, samples need to be semantically labeled and some samples may be difficult to correctly annotate, leading to discarding; for example if the target lesion is ambiguous, blurry, partially visible or occluded. We annotated the dataset and all masks underwent a secondary quality review. We define 5 classes: lesion, skin, marker, ruler, and background. We opted to label these classes as many images include a ruler or markings to denote the lesion of focus. Visualizations of our ground truth labels are shown in Fig. 2. Details on the annotation protocol, including skip criteria, can be found in the supplementary materials. We release our annotation work (a total of 378 annotated DDI images) as part of our contributions."
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",3,Approach,"The UNet architecture was introduced for diffusion [14] and found to improve generative performance [25] over other denoising score-matching architectures.Recent work [15] has extensively ablated the diffusion UNet architecture by increasing depth while reducing the width, increasing the number of attention heads and applying it at different resolutions, applying BigGAN [26] residual blocks, and introducing AdaGN for injecting timestep and class embedding onto residual blocks, obtaining state-of-the-art for image synthesis. From this work, we designate the unconditional image generation model as our backbone and freeze its weights. This network is an ImageNet-trained DDPM with 256 × 256 input and output resolution."
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",3.1,"Lesion, Marker, Ruler, Skin, and Background Segmentation","We obtain image encodings from blocks on the decoder side of the DDPM-UNet architecture, then apply bi-linear up-sampling up to some target output resolution (256 × 256 in this case) and concatenate them before feeding them to MLPs for per-pixel classification following [18]. However, we use fewer blocks, a single-time step, and 5 MLPs. This is to avoid overfitting and because results from [18] describe how different blocks at different timesteps perform differently depending on the target data. To understand which blocks are most promising for dermatology images, we obtain a sample of feature encodings at different blocks and perform K-Means clustering shown in the supplementary materials. We selected the blocks which clustered semantically meaningful areas (e.g. lesion, skin, and ruler). We identify block 6 and block 8 at timestep 100 as the most promising and use this setup for the rest of our segmentation experiments."
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",3.2,Malignancy Classification,"For classification, we down-sample block encodings using a combination of 2D and 1D max pooling operations until the feature vectors are one-dimensional and of size 512. We note that the total number of pooling operations varies depending on the sampled block, as deeper blocks are smaller with more channels, while shallower blocks are larger with fewer channels. The vectors are then passed onto a 3-layer MLP of size 64, 32, and 1. We include batch normalization and dropout between each layer with 50% and 25%. This classification network is trained to predict the malignancy of the input image from the down-sampled feature vector. A summary of our approach is shown in Fig. 3."
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",4,Results and Discussion,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",4.1,"Lesion, Marker, Ruler, Skin, and Background Segmentation","Our segmentation results are evaluated by Intersection over Union (IoU) performance and are compared against other architectures pre-trained on ImageNet: DenseNet121 [27], VGG16 [28], ResNet50 [29], and two other smaller networks, EfficientNetB0 [30] and MobileNetV2 [31]. These architectures are configured as UNets and tasked with segmenting the input images. We observe FEDD outperforms all other architectures across all subsets of DDI on our validation and test sets. Importantly, other architectures (particularly the smaller networks) close the performance gap as the amount of training data increases, showcasing that FEDD's efficiency is most prevalent in very small data scenarios.We further compare the FEDD's IoU performance on light and dark skin tone images to showcase algorithmic fairness. We note that all architectures show similar performance for both skin tones, suggesting that our balanced data subsets play a larger role in fairness than the choice of neural network. Finally, we plot test set performance when only considering the lesion class split between light and dark tones. We find that FEDD's performance is significantly better at segmenting the lesion class compared to other architectures. We believe this is due to the greater morphology variation of different skin lesions being harder to learn than other more consistent targets like the ruler. Since DDI contains a diversity of skin conditions, FEDD's efficiency becomes very useful for highquality lesion segmentation of this morphologically changing class. These results are shown in Fig. 4.In Fig. 5, we visualize predicted segmentation masks between FEDD and the next best IoU-performing architecture, EfficientNetB0. We observe that FEDD achieves significantly better segmentation masks at lower fractions of labeled data across all skin tones. With a larger percentage of labeled data, EfficientNet begins to produce similar results to FEDD, but FEDD comparatively outputs higher-quality segmentations with fewer segmentation artifacts and false positives. The skin lesions themselves, which appear in different sizes, locations, and morphologies, are also most accurately segmented by FEDD. "
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",4.2,Malignancy Classification,"We ablate the performance of each individual block per timestep in the classification context as, to the best of our knowledge, it has not been done before. It is also not entirely intuitive which block depth and timestep combination would produce the best representations for classification, as well as how that performance varies as we introduce more data. We train FEDD's classifier on block embeddings produced between timestep 0 and 1000 of the backward diffusion process. We then record the accuracy of the classifier per block in increments of 50 timesteps. Figure 6 describes these results. While noisy, given the small amount of test data, the general pattern is that the earlier time-steps (later in the reverse diffusion de-noising process) allow for higher classification accuracy. This is likely due to the quality of the DDPM sample increasing as more noise is removed. Another finding is that as the classifier is shown more data, the shallower blocks begin to perform better. The best performing blocks are shown to be: block 4 at 5% DDI, block 6 at 10% DDI, block 12 at 15% DDI, and block 14 at 20% DDI. We attribute this to the fact that shallower blocks of the UNet decoder capture finer detail of the reconstructed image while deeper blocks capture lower-resolution detail. This coarse data is more generic and thus more generalizable than the finer features in later blocks. As we increase the amount of data, the classifier has enough information to learn from the finer details of later blocks, boosting performance.We select the best-performing block and timestep combination at each fraction of data for the rest of our experiments. The previous classification state-of-Fig. 6. Classification accuracy of each DDPM UNet decoder is shown. Later steps in the reverse diffusion process produce the highest quality embeddings. When less data is available (top row), earlier blocks of the UNet decoder perform best. When more data is available (bottom row), the later, shallower blocks of the decoder perform best. Fig. 7. ROC-AUC scores (left) show FEDD outperforms other methods, including an ensemble of dermatologists. Accuracy per method is also higher (right).the-art on the DDI dataset is reported on [4] as the DeepDerm [11] architecture pre-trained on HAM10000 [32] and fine-tuned on DDI. We compare FEDD performance against this setup as well as other commonly used classifiers on each of our DDI subsets. We measure Receiver Operating Characteristic Area Under the Curve (ROC-AUC) at the best threshold for each method, F1 scores, and classification accuracy. We observe FEDD obtains a higher ROC-AUC than any other method at every level of data. It also surpasses the dermatologist ensemble performance reported by [4]. FEDD also reports the best accuracy, however, it does not see improvement at 10% or 15% of DDI compared to 5% and 20%, as shown in Fig. 7. When observing ROC curves for FEDD, we see it meets or exceeds the ensemble of dermatologists even at the smallest subset of DDI. We divide F1 scores between the light and dark skin tones finding that FEDD does not always obtain the best F1 performance at larger subsets of data, namely 15% and 20% of DDI. This result suggests that purpose-built classification networks could have a performance advantage over diffusion embeddings applied toward classification when allowed to train on larger amounts of data. Detailed F1 results are shown in table form on the supplementary materials."
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",5,Conclusion,We introduce the FEDD framework for skin lesion segmentation and malignancy classification that outperforms state-of-the-art methods and an ensemble of board-certified dermatologists across a diverse spectrum of skin tones and malignancy conditions under limited data scenarios. Our proposed methodology can improve the diagnosis and treatment of skin diseases while maintaining fair segmentation outcomes for under-represented skin tones and accurate malignancy predictions for rare malignancy conditions. We freely release our code and annotations to encourage further research around dermatological AI fairness.
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 1 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 2 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 3 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 4 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 5 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 26.
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Clinical Applications -Fetal Imaging,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,1,Introduction,"Although these days smart healthcare devices such as smartwatches can be used to monitor a single-lead electrocardiogram (ECG) for Lead I and detect cardiovascular diseases (CVDs) such as atrial fibrillation (AF), multichannel ECGs such as twelve-lead signals are still required to diagnose more complex CVDs such as left and right bundle branch blocks (LBBBs and RBBBs) or myocardial infarction. To proactively deal with such intricate CVDs, therefore, one may need to undergo a 12-lead ECG measurement at a hospital and utilize 12-lead ECG-based deep learning algorithms for predicting CVDs [3,4,15,18], which can be a cumbersome process in everyday life. It is neither plausible to train a prediction model using only single-lead ECGs measured by smart devices as it is not possible to correctly label complex CVDs for them in the first place. To address this problem, in this paper, we propose a novel generative adversarial network (GAN) [13], called EKGAN, that can faithfully reconstruct 12-lead ECGs only from single-lead ones.Although the ECG synthesis problem is not new, most previous studies have focused on utilizing it for data augmentation purpose as it is difficult to collect a sufficient amount of labeled ECGs (with CVDs) for developing prediction models. For example, many researchers have focused on synthesizing realistic ECGs using variants of autoencoders and GANs [5,6,[9][10][11]14,22,23]. These methods can be useful for training prediction models, but it is unclear how they can be leveraged with commonly available wearable devices that can measure only single-lead ECGs. Meanwhile, another line of work has focused on reconstructing the corresponding (missing) ECGs from only a few actual lead signals [2,7,17,21], usually generating 12 leads from three leads including Lead I and II. We note here that if we know Lead I and II, then all six limb leads can be derived by Willem Einthoven's law [8] and Goldberger's law [12]. Thus, their reconstruction problem indeed reduces to the problem of reconstructing six precordial leads.Our work differs significantly from previous studies, as we generate all 11 remaining leads simultaneously from only a single lead. Thus, our method can be used to bridge commonly available wearable devices that can measure only Lead I and high-performance deep learning-based prediction models using 12lead ECGs. To the best of our knowledge, our work is the first to reconstruct all 12 leads simultaneously from a real single lead. By using the approach in [6], one can also generate all 12 leads, but only incrementally. Our proposed method EKGAN employs two generators and one 1D U-Net [19] discriminator to capture CVD-specific characteristics and correlation patterns between Lead I and the remaining leads from ECG training data. Experimental results show that the reconstruction performance of EKGAN outperforms other representative generative models such as Pix2Pix [16], CycleGAN [24] and CardioGAN [20] under various metrics. We also evaluated the practical applicability of our method by applying an existing 12-lead ECG-based CVD prediction model [18] to the reconstructed ECGs. To this end, we consider AF, LBBB, and RBBB, among which LBBB and RBBB require multichannel ECGs to detect. Moreover, three cardiologists examined reconstructed ECGs to see if they accurately reflected the important CVD-related characteristics of the original ECGs. All the results confirm the effectiveness and usefulness of our method, thus enabling preventive healthcare with smart wearable devices for CVDs. For reproducibility, the source code of EKGAN is available at https://github.com/knu-plml/ecg-recon."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,2,Methods,"In this section, we introduce a novel conditional GAN for ECG reconstruction, called EKGAN, which is based on Pix2Pix [16] but employs an additional label generator G L and uses a 1D U-Net discriminator instead of a PatchGAN discriminator. Its overall structure is shown in Fig. 1. The inference generator G I takes a Lead I signal and generates corresponding 12-lead signals, while the label generator G L takes the same input as G I and simply returns the same signal. The whole purpose of G L is to enable G I 's encoder to learn the important characteristics of Lead I. To this end, the latent vector produced by G I 's encoder is approximated to that of G L 's encoder so that G I 's decoder can produce more detailed ECGs that are closely correlated with the input Lead I signals. The discriminator D distinguishes 12-lead ECGs generated by G I and the original ECGs. The whole process is repeated, adversarially learning G I and D. "
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,2.1,Generator,"Inference Generator. The inference generator G I is based on Pix2Pix's 2D U-Net generator and consists of an encoder and a decoder (Fig. 1). It takes an input of size (16, 512, 1): 16 for 12 replicated signals of the input single lead plus 4 zero padding, 512 for the length of the ECG signal, and 1 for the channel size. More specifically, to generate 12-lead signals from single-lead signals, an input single-lead signal of length 512 is copied 12 times and two rows of zeros are added to both the top and bottom. Each original 12-lead ECG is arranged in the order of Lead I, II, III, aVR, aVL, aVF, and V1-V6, and is also zero padded. The encoder consists of five blocks each of which consists of convolution, batch normalization, and Leaky ReLU layers (an exception is the first block which excludes batch normalization). The numbers of convolution filters are 64, 128, 256, 512, 1024, while the kernel size is set to (2,4). The stride size is (2, 2) except for the last block whose stride size is (1,2). The decoder is the inverse of the encoder and takes the encoder output as input. It also consists of five blocks each of which consists of concatenation, deconvolution, batch normalization, and ReLU layers (exceptions are the first block, which excludes concatenation, and the last block, which only uses concatenation and deconvolution). The numbers of deconvolution filters are 512, 256, 128, 64, 1, while the kernel size is set to (2,4). The stride size is (2, 2) except for the first block whose stride size is (1,2). From the second block, the output of the previous decoder block and that of the corresponding encoder block are concatenated and used as input."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Label,"Generator. An autoencoder-based model like U-Net [19] should create a latent vector in the encoder that represents the features of input data well. Then, the decoder should be learned to generate a target-like output from the latent vector. When training a U-Net, however, as we only use the reconstruction loss between the original and generated data, we cannot accurately determine how well the latent vector captures the essential features of the input (because there is no ground truth for the latent vector). In this paper, as a workaround, we use a label generator G L which takes Lead I signals and returns the same signals. Accordingly, the latent vector produced by G L 's encoder would represent the features of the input accurately and thus can be used as ground truth for G I 's encoder. By doing so, G I 's decoder can produce a 12-lead ECG that is not only realistic but also closely associated with the input Lead I signal. The structure of G L is similar to that of G I , but it does not incorporate concatenation between the encoder and decoder. We experimentally validate the effectiveness of the label generator in Sect. 3."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,2.2,Discriminator,"In 12-lead ECGs, each lead has its own characteristics and thus it is important for a discriminator to analyze each lead signal individually at the pixel level. If a standard 2D convolution-based discriminator is used, as for image-to-image translation, which uses 2D patches, then the unique characteristics of each lead signal may be intermixed with others, which may in turn degrade the reconstruction quality. To prevent this problem, instead, we use a 1D U-Net discriminator D, which has the same layer architecture as G L . D takes as input either G I 's output or an original 12-lead ECG, which is concatenated with G I 's input. In the encoder, the numbers of convolution filters are 32, 64, 128, 256, 512, kernel sizes are 64, 32, 16, 8, 4, and stride sizes are 4, 4, 4, 2, 2. The decoder has the inverse structure of the encoder as in the two generators, except that it uses a sigmoid activation function at the last layer."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,2.3,Loss,"The objective of EKGAN is similar to that of other conditional GANs except that it uses the label generator G L to train the inference generator G I . Let us write e i for 12 replicated ECG segments from the input single-lead ECG for G I and e o for the corresponding ground-truth 12-lead ECG segments. In addition, let z i and z l be the latent vectors produced by the encoders of G I and G L , respectively. We use the following adversarial loss and L1 losses:where the last one is the latent vector loss for the inference generator.Then, the objective of EKGAN is defined as:where λ and α control the relative importance of each function. Note that L L1 (G L ) is used solely for training G L and thus not included in the objective equation. Through a grid search, we determined λ = 50 and α = 1, and these values have been used in the following experiments unless otherwise stated."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,3,Evaluation,"This section introduces our dataset and its preprocessing. Then, we extensively evaluate EKGAN in terms of its reconstruction performance. We also assess its applicability using an existing prediction model with proven performance [18] and with three cardiologists."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,3.1,Experimental Setup,"Datasets. To develop and evaluate EKGAN, we used about 326,000 ECGs collected from Ewha Womans University Mokdong and Seoul Hospitals between May 23, 2017, and November 30, 2022. Specifically, we first selected ECGs with LBBB, RBBB, and AF from the dataset and randomly selected normal sinus rhythm (NSR) ECGs in a 1:1 ratio to reduce the bias of the generative models.For the label information, we simply used the interpretation result of the ECG machine. Next, for a test set for CVD multi-label classification and cardiologists' examination, we randomly chose 100 ECGs for each disease and 300 NSR ECGs. Then, the remaining dataset was randomly divided into train and validation sets in an 8:2 ratio. Table 1 shows the configuration of the final dataset. We note here that since AF and NSR may coexist with LBBB and RBBB, the number of classes is different from the total number of data. The generative models were trained using the train set and evaluated using the validation set. The CVD prediction model was trained using the train and validation sets where the latter was used for hyperparameter tuning. Finally, 12-lead ECGs were generated by using both the validation and test sets, and their quality was evaluated by using the predictive model, i.e., by comparing the classification performance with the original 12-lead ECGs and the generated ones, and examined by three cardiologists. amplitudes may be ignored when predicting CVD. More specifically, for each signal, we first apply min-max normalization to [-1, 1] and then a band-pass filter with lower and upper cutoff frequencies [0.05, 150] as some fine details and important characteristics disappear when using a high lower cutoff frequency or a low upper cutoff frequency [1]. Finally, we downsample 4,096 lengths to 512 lengths."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Training and Test.,"All experiments were conducted on a workstation with an NVIDIA RTX 8000 and using TensorFlow 2.8. For comparisons, we implemented not only EKGAN but also Pix2Pix [16], CycleGAN [24], and CardioGAN [20] with minor modifications so that they can be applied to ECG data. In particular, for unpaired training of CycleGAN and CardioGAN, the input and label data were separated and shuffled independently. Moreover, due to the cycle consistency loss, Lead I of the label data was replaced with zero padding. All models were trained for 10 epochs, with the learning rate of 1e-4 until 5 epochs after which weight decay of 0.95 was applied per epoch. The kernel-initializer was sampled from a normal distribution N (0, 0.02 2 )."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,3.2,Reconstruction Performance,"Table 2 shows the reconstruction performance of EKGAN and other methods for the validation data in terms of root mean square error (RMSE), mean absolute error (MAE), percentage root mean square difference (PRD), maximum mean discrepancy (MMD), and mean absolute error for heart rates (MAE HR ). For all metrics, EKGAN significantly outperforms other methods, confirming the effectiveness of its label generator and 1D U-Net discriminator for pixel-level learning of ECG signals. Meanwhile, CycleGAN and CardioGAN, which are based on unpaired training, are not suitable for 12-lead ECG reconstruction.Figure 2 shows examples of reconstructed 12-lead ECGs by various methods from the Lead I, where EKGAN produces the most faithful reconstruction of the original ECG.  Table 3 shows the performance of different variants of EKGAN, that is, showing the effectiveness of our 1D discriminator and label generator. 'EKGAN w/o 1D discriminator' uses Pix2Pix's PatchGAN discriminator instead of our 1D U-Net discriminator. 'EKGAN w/o label generator' excludes a label generator and uses only an inference generator. We observe that the use of 1D discriminator is more effective than the use of a label generator, but using both results in a greater improvement over Pix2Pix for reconstructing 12-lead ECG signals. "
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,3.3,Reconstruction Quality Evaluation,"To evaluate the applicability and quality of ECG signals reconstructed by EKGAN, we use a highly accurate prediction model for CVDs [18]. The model can predict six diseases by analyzing a 12-lead ECG, among which we choose LBBB, RBBB, and AF. We choose LBBB and RBBB as they require analysis of multi-lead ECGs and AF as it can be already predicted using commonly available wearable devices. This allows us to indirectly check if EKGAN is able to generate diverse ECG signals capturing different characteristics for each disease. Table 4 shows the multi-label classification results for the test set. Since the dataset used in [18] differs from ours, the performances using their datasets and ours are also slightly different, but both seem to perform well. We observe that the F1-scores when using the dataset reconstructed by EKGAN are comparable to those when using the original dataset and consistently better than those when using the ones reconstructed by Pix2Pix.Table 5 shows the concordance rate between the test ECGs and the corresponding reconstructed ones by EKGAN, evaluated by three cardiologists. We randomly shuffled the original and reconstructed ECGs, and each cardiologist reviewed every ECG if it exhibited LBBB, RBBB, AF, or NSR. Then, for each case, the concordance rate is calculated as the ratio of pairs of original and corresponding reconstructed ECGs such that a cardiologist's read result coincides among all data pairs. We note here that since each ECG must exclusively include either AF or NSR, their concordance rates are the same. The results confirm that the reconstructed ECGs by EKGAN effectively capture the important characteristics of the original ones."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,4,Conclusion,"This paper studies a novel problem of reconstructing 12-lead ECGs from singlelead ECGs. To address this problem, we propose a novel conditional GAN, called EKGAN, based on Pix2Pix, which consists of two generators and one 1D U-Net discriminator. Experimental results show that EKGAN significantly outperforms other representative generative models such as Pix2Pix, CycleGAN, and Car-dioGAN, and is able to reconstruct 12-lead ECGs that faithfully capture the essential characteristics of the original 12-lead ECGs useful for predicting CVDs. Therefore, we expect that numerous deep learning models based on 12-lead ECGs with proven performance could be applied to smart healthcare devices that can measure only single-lead signals. It would be also interesting to investigate if our method is applicable to more complex CVDs such as acute myocardial infarction, which require a more detailed analysis of 12-lead ECGs by cardiologists."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Fig. 1 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Fig. 2 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Table 1 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Table 2 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,proposed) 0.32 0.25 6.95 0.04 × 10 -3 20.51Table 3 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Table 4 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Table 5 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 18.
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,1,Introduction,"Placenta Accreta Spectrum (PAS) occurs when the placenta becomes abnormally adherent to the myometrium rather than the uterine decidua [15]. The primary risk of PAS is hemorrhage and even life-threatening associated complications [15]. Particularly, it can lead to excessive blood loss and transfusions of blood products at the delivery [9]. Prenatal screening helps identify women with potential PAS and allows for appropriate delivery planning, which is critical for better clinical outcomes. Meanwhile, Magnetic Resonance Imaging (MRI) features high resolution and sensitivity, playing an increasingly important role in prenatal screening [12]. MRI is often used after an inconclusive ultrasonic examination to assess the invasive condition of PAS [9]. A detailed background illustration is included in Section A of supplementary material.In the field of computer-aided PAS diagnosis for prenatal screening, some work based on traditional machine learning has been proposed [7,[11][12][13] and achieved promising progress. In these works, radiomics features were first extracted from the Region of Interest (ROI, placenta) of raw images and a classifier was then fitted between the features and clinical outcomes. In [12], Random Forests (RF), K Nearest Neighbor (KNN), Naive Bayes (NB), and Multi-Layer Perception (MLP) were compared. Statistical analysis and multivariate logistic regression were employed in [13]. The features are mostly extracted only on placenta ROI with Pyradiomics [17] or predefined rules, ignoring the context information such as the location of the placenta and its interaction with nearby organs that are important in PAS diagnosis [10].On the other hand, deep learning stands out in terms of automatic deep feature extraction [24] and end-to-end learning fashion [3]. Deep learning has been recently explored for image-based PAS diagnosis [20,23]. A deep dynamic convolution neural network with autoencoder training manner was proposed in [20] to extract deep features. In [23] the encoder of a placenta segmentation network was employed to capture the semantic features. To enhance such deep features with the awareness of the focal area, attention mechanisms [25] are considered in this paper. The ROI relevant to the prediction task can then be automatically localized, leading to more comprehensive features that capture both semantic and context information. Moreover, prior knowledge about the focal area can be incorporated by annotating a limited number of images with ROIs. That is more efficient than directly including the localization task in addition to the prediction task, which requires extra pixel-level annotation of each image.Another gap to fill is the inherent diversity of MRI sequences. There are T1weighted and T2-weighted MRI scans, upon which three planes with multiple slices are further present. Therefore, each patient corresponds to dozens of slice images. The physicians usually need to inspect a large portion of these images in order to identify the focus of suspected PAS and reach the patient-level diagnosis. For the existing work, a single (or few) slice image is selected to represent the information of a patient, which can lead to potentially biased results. Multi-Instance Learning (MIL), as a potential solution, has been widely adopted in lesion classification and localization [19,21,22]. Some recent works, such as slice attention transformer and convolutional transformer-based MIL, have studied the problem of patient as bag and images as instances [6,8,16]. It remains to explore how MIL can benefit the computer-aided PAS diagnosis.To deal with the issues above, a novel end-to-end Hierarchical Attention and Contrastive Learning Network (HACL-Net) is proposed under the formulation of a multi-instance problem. Slice-level spatial residual attention with prior knowledge is designed to extract context-aware deep semantic features from individual MRI slices of a patient. Patient-level attention-based pooling is then applied to aggregate these features into patient representation for PAS prediction. To make such features more discriminative, a plug-and-play contrastive loss is further included. Extensive experiments with ablation study show the proposed network can achieve state-of-the-art performance on a real clinical dataset involving 359 distinct patients."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,2,Methodology,"Consider each patient bag P i includes K i valid 2D MRI slices from axial, sagittal, and coronal views (possibly both T1-weighted and T2-weighted scans exist). In the training dataset, there is an associated binary label Y i indicating whether PAS is reported at delivery. Denotei is the j-th MRI slice of i-th patient. Note that the number K i can vary with different patients. The proposed HACL-Net aims to learn the relationship between X i and Y i in a weekly-supervised and end-to-end manner (Fig. 1). The slice-level attention module is designed to extract context-aware deep semantic features from each MRI slice. Then the slice-wise features are fused in a taskspecific way via the patient-level attention module, after which being connected to FC layer for PAS prediction. A plug-and-play contrastive learning module is presented to facilitate discriminative patient-wise features between classes."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,2.1,Slice-Level Spatial Residual Attention,"Trunk Branch. The trunk branch aims to extract global semantic information from each MRI slice that is important for the PAS prediction task. It is composed of the top convolution layer, batch normalization layer, and ReLU layer from ResNet18. For each grayscale MRI slice X (j) i ∈ R 256×256×1 , a preliminary feature map T (j) i ∈ R 128×128×64 is generated. Note that to realize the spatial residual attention with the mask branch, shallow features are first derived since they can be aligned with the context features. Deep features will be extracted later after the spatial residual attention layer.Mask Branch. The mask branch aims to extract the context information that indicates the location of ROI. Unlike the traditional implementation where a general bottom-up top-down network is employed, here the U-Net dedicated for medical image segmentation [14] is considered because the placenta area accounts for most of the ROI. Automatic segmentation of placenta with U-Net has proved effective in recent years [4]. The original network consists of 4 downsampling and 4 upsampling modules. To preserve the context information surrounding the placenta, a partial U-Net is constructed, as shown in Fig. 2. The last upsampling module to generate the single-channel mask (R 256×256×1 ) is removed and the intermediate feature map M (j) i ∈ R 128×128×64 is obtained as the output of mask branch. Note that the entire U-Net is first pre-trained with a small number of placenta annotated MRI slices. Then the partial U-Net gets fine-tuned with a small learning rate in the end-to-end prediction task."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Spatial Residual Attention and Deep Feature Extraction. The mask branch output M (j),"i has been aligned with the trunk branch output T (j) i in terms of the field of view. Therefore, M (j) i is multiplied in an element-wise manner with T (j) i as the attention mechanism. Besides, to avoid potential performance drop with attention, residual learning is further considered [18]. The shallow features after spatial residual attention become Hi . Note that the attention is not performed directly on deep features because the ROI considered has explicit physical meanings instead of being ""hidden"".The remaining layers of pre-trained ResNet18 (excluding the top three layers used previously) are employed to further extract deep features, as shown in Fig. 3. These layers are fine-tuned in end-to-end learning so that task-specific deep features can be obtained. For each patient bag, a set of slice-wise deep features} are eventually generated, where F (j) i ∈ R 512 . These features capture the context-aware semantic information of MRI slices."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,2.2,Patient-Level Attention-Based Pooling,"Due to the MIL nature of the studied problem, MRI slices in a patient bag contribute unevenly to the PAS diagnosis. There are no detailed annotations to reflect the role of each slice. Therefore, patient-level attention mechanism is further proposed to aggregate the deep features from each slice in a task-specific manner, i.e., deriving the patient-wise features G i based on a set of slice-wise features F i . Some recent work [5,22] has proved that attention-based pooling is useful for feature aggregation in MIL problems. It shows certain similarities with channel attention which has been widely applied in image classification. Besides, such aggregators can be easily embedded in the network as a trainable layer. Concretely, the attention-based pooling layer works as follows:where the calculation of attention weight a j involves trainable parameters w ∈ R 64×1 and V ∈ R 64×512 . Such weight quantifies the degrees of ""activation"" toward the final prediction for each MRI slice. Note that the weight calculation resembles the softmax operation, and can deal with the varying number of slices for different patients unless the variation is not significant. The aggregated patient-wise features G i are then mapped to an output vector Ŷi with an FC layer. Binary Cross Entropy (BCE) is used as the classification loss function,"
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,2.3,Contrastive Learning for Discriminative Deep Features,"For the studied problem, the MRI slices of a positive patient (with PAS) that reflect the clues about PAS can account for only a small portion of all slices, while most ones look normal. Meanwhile, some slices of a negative patient (without PAS) can manifest suspected patterns. Therefore, the extracted patient-wise deep features should be more discriminative under the existence of inherent noises. In addition to aggregating the slice-wise features with attention, contrastive learning is further considered to realize that [2]. Specifically, the representation G i needs to keep certain distances between positive and negative patients while getting relatively closer for patients in the same class. Since the proposed network does not require self-supervised learning, a plug-and-play contrastive loss is employed to avoid additional capacity. Concretely, to make up each batch, a positive patient and a negative patient are first sampled from the dataset. Then another patient is randomly picked up. It is thus guaranteed that a matching pair and a non-matching pair exist, respectively. Assume the third patient is positive, the patient-wise features are denoted as G ia , G in , G ip . The triplet loss [1] is calculated between these feature maps (G ia is the anchor, G in is the non-matching sample and G ip is the matching sample). The triplet loss is used as the contrastive loss:where c is the positive margin constant that quantifies the distance. It is added to the BCE losses of the samples in a batch: L total = L e + L c ."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,3,Experiment,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,3.1,Dataset and Implementation Details,"The experiments are conducted on a real clinical dataset collected from a large obstetrics and gynecology hospital. The dataset involves 359 distinct patient subjects with a total of 15,186 MRI slices. A 5-fold cross-validation is performed and the average metrics values with standard deviations over the folds are reported.To pre-train the U-Net in the mask branch, 185 MRI slices are annotated with placenta by the experts. The ResNet18 is pre-trained on ImageNet. HACL-Net is trained end to end with Adam optimizer and batch size N b = 3 for 100 epochs.The learning rate of the slice-level attention module and final FC layer is set as 10 -6 and 10 -4 respectively, while that of the rest parts is set as 10 -5 . PyTorch 1.10.1, and an NVIDIA GeForce RTX 2080 Ti GPU with CUDA 11.3 are used to train our method. More details can be found in Section B of supplementary.The code is publicly available on https://github.com/LouieBHLu/HACL-Net."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,3.2,Performance Comparison of PAS Diagnosis,"Section C of supplementary material illustrates the effect of different selections of trunk branch. Table 1 shows the performance of the pre-trained baseline U-Net HACL-Net 84.2 ± 0.9 84.9 ± 0.6 79.2 ± 5.9 86.9 ± 2.8 compared with other three common networks. It turns out such network choice does not have a significant impact. The placenta area can be accurately captured by U-Net, which justifies the usage of Partial U-Net in the mask branch. Visual ROI prediction can be found in Section D of supplementary material.To evaluate the performance of machine learning methods with Pyradiomics features and deep learning methods [12,20,23], the slice-wise features are aggregated with simple mean to derive the patient-wise features. The accuracy, area under the ROC curve (AUC), sensitivity, and specificity are employed as the metrics. As shown in Table 2, HACL-Net achieves the best 84.9% AUC and 79.2% sensitivity with 23.1% and 44.2% improvement respectively over the second-best approach. The outcomes are reflected in Fig. 4, where the compared methods suffer from very low sensitivity and high specificity. Since only a small portion of MRI slices have clues about a patient's being positive, traditional aggregation of slices tends to dilute the effective information. As the features are extracted from a segmentation network in [23], the sensitivity is relatively improved but with highly degraded specificity. In general deep features even perform worse than ROI-based radiomics features, partially due to the increased complexity of input is not properly explained by the clinical outcome of PAS."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,3.3,Ablation Study,"Table 3 and Fig. 5 illustrate the effect of each module in HACL-Net. For experiments without patient-level attention, slice-wise features are also aggregated   with simple mean to derive patient-level features. The combination of mask branch and trunk branch achieves 15.4% AUC improvement over a single trunk branch, indicating the gain of additional context information. A 12.3% AUC improvement is witnessed when patient-level attention is used as an aggregator instead of mean. Contrastive learning manifests 5.7% AUC and 16.3% sensitivity improvement with 11.8% specificity drop as a trade-off. It is believed that sensitivity can be more important from a clinical perspective, and the trade-off can be tuned according to specific demands. Moreover, the softmax activation is replaced with sigmoid, and different confidence thresholds are used. As the threshold gets lower, a non-monotonic trend of sens-spec trade-off is observed, while the optimal balance lies around 0.4. That reveals the potential effect of class imbalance, i.e., fewer positive cases. To this end, contrastive learning can help separate the decision boundary between two classes. Sigmoid results differ from softmax due to the explicitly set decision threshold. The predicted scores between positive and negative patients can be close for hard samples, leading to large deviations in sigmoid results. Figure 6 shows feature maps of a positive "
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,4,Conclusion,"This paper presents a novel end-to-end HACL-Net for MRI-based computeraided PAS diagnosis that facilitates more efficient prenatal screening. The proposed network utilizes slice-level spatial residual attention to extract contextaware deep semantic features, and aggregate them as a comprehensive representation of a patient with patient-level attention-based pooling. Moreover, a contrastive loss is added to improve the discriminating power of learned patient-wise features. Extensive experiments on a real clinical dataset show that HACL-Net achieves superior performance, with great potential for clinical applications."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 1 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 2 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 3 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 4 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 5 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 6 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Table 1 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Table 2 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Table 3 .,70.7 ± 2.7 53.9 ± 5.2 12.5 ± 10.9 95.2 ± 4.4 77.7 ± 0.6 66.9 ± 1.5 35.0 ± 4.1 94.7 ± 1.0 80.7 ± 4.1 79.2 ± 5.2 62.9 ± 1.9 98.7 ± 1.3 84.2 ± 0.9 84.9 ± 0.6 79.2 ± 5.9 86.9 ± 2.8
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 29.
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,1,Introduction,"The inception of deep neural networks has revolutionized the landscape of medical image segmentation [14,24]. This tremendous success, however, is conditioned on the assumption that the training and testing data are drawn from the same distribution. Unfortunately, in real-world clinical scenarios, due to different acquisition protocols or various imaging modalities, domain shift is widespread between training (i.e., source domain) and testing (i.e., target domain) datasets [15]. This distribution gap usually degenerates the model performance on the target domain. To achieve reliable performance across different domains, a straightforward way is manually labeling some target data and fine-tuning the pretrained model on them [13]. However, obtaining expert-level annotation data in the medical imaging domain incurs significant time and expense [22]. Recently, unsupervised domain adaptation (UDA) has been widely investigated to reduce domain gap through transferring the knowledge learned from a rich-labeled source domain to an unlabeled target domain [4,7,17,19]. Existing UDA methods typically require sharing source data during adaptation, and enforce distribution alignment to diminish the domain discrepancy between source and target domains. This requirement limits the application of UDA methods when source domain data are not accessible. Hence, some very recent works have started to explore a more practical setting, source-free domain adaptation (SFDA), that adapts a pre-trained source model to unlabeled target domains without accessing any source data [1,5,6,12,20,21]. Among these methods, [5] and [20] focus on generating reliable pseudo labels for target domain data by developing various denoising strategies. Unavoidably, these self-training methods depends heavily on initial probability maps produced by the source model, which are considerably unreliable when the domain discrepancy is large (e.g., CT and MRI). To relieve the issues caused by noisy pseudo labels, Bateson et al. [1] proposed a prior-aware entropy minimization method to minimize the label-free entropy loss for target predictions. Furthermore, unlike the above self-adaption methods, Yang et al. [21] utilized the statistic information stored in the batch normalization layer of the source model and mutual Fourier Transform to synthesize the source-like image. However, the quality of the generated image is still influenced by the domain discrepancy."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Background,"In this work, we propose a novel SFDA framework for cross-modality medical image segmentation. Our framework contains two sequentially conducted stages, i.e., Prototype-anchored Feature Alignment (PFA) stage and Contrastive Learning (CL) stage. As previous works [12] noted, the weights of the pre-trained classifier (i.e., projection head) can be employed as the source prototypes during domain adaptation. That means we can characterize the features of each class with a source prototype and align the target features with them instead of the inaccessible source features. To that end, during the PFA stage, we first provide a target-to-prototype transport to ensure the target features get close to the corresponding prototypes. Then, considering the trivial solution that all target features are assigned to the dominant class prototype (e.g., background), we add a reverse prototype-to-target transport to encourage diversity. However, although most target features have been assigned to the correct class prototype after PFA, some hard samples with high prediction uncertainty still exist in the decision boundary (see Fig. 1(a→b)). Moreover, we observe that those unreliable predictions usually get confused among only a few classes instead of all classes [18]. Taking the unreliable pixel in Fig. 1(b,c) for example, though it achieves similar high probabilities on the spleen and left kidney, the model is pretty sure about this pixel not belonging to the liver and right kidney. Inspired by this, we use confusing pixels as the negative samples for those unlikely classes, and then introduce the CL stage to pursue a more compact target feature distribution. Finally, we conduct experiments on a cross-modality abdominal multi-organ segmentation task. With only a source model and unlabeled target data, our method outperforms the state-of-the-art SFDA and even achieves comparable results with some classical UDA approaches."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,2,Methods,"We are first provided a segmentation model M s trained on N s labeled samples {(x s n , y s n )} Ns n=1 from the source domain D s , and an unlabeled dataset with N t samples {x t m } Nt m=1 from the target domain D t , where x s , x t ∈ R H×W ×D , y s n ∈ R H×W , H and W are the height and width of the samples. The goal of SFDA is to adapt the source model M s with only unlabeled x t to predict pixel-wise label y t for the target domain data. In general, the segmentation model consists of two parts: 1) a feature extractor F θ :that projects pixel feature into the semantic label space with C classes.In the SFDA task, the source classifier φ s encounters a domain shift problem when classifying the target domain feature. To tackle this challenge, we propose a novel SFDA framework mainly including two stages, shown in Fig. 2. We will elaborate on the details in the following. Given a target image, we first use M t 0 to make a prediction, and separate the pixels into query one and negative ones for each class based on their reliability (entropy). Then, features of query pixels come from F t θ (query samples), while features of negative pixels are from F t 0 θ (negative samples), when minimizing LCL."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,2.1,Prototype-Anchored Feature Alignment,"Since source data is not available, explicit feature alignment that directly minimizes the domain gap between the source and target data like many UDA methods [4,8] is inoperative. As shown by previous methods [12], the weightsof the source domain classifier φ s can be interpreted as the source prototypes, which characterize the features of each class. Thus, we introduce a bi-directional transport cost to align the target features with these prototypes instead of the unaccessible source features.Following [23], given a mini-batch {x t m } M m=1 with M images, we first adopt the cosine distance d(µ c , f t m,i ) = 1 -µ c , f t m,i to define a point-to-point transport cost between f t m,i and µ c , where •, • is the cosine similarity. Then, a conditional distribution π θ µ c | f t m,i specifying the probability of transporting from f t m,i to µ c can be constructed as,where τ is the temperature parameter, and p (µ c ) is the prior distribution (i.e., class proportion) over the C classes for the target domain. As the true class distribution is unavailable in the target domain, we use the EM algorithm to infer p (µ c ) instead of using a uniform prior distribution (see more details in [16]). Note that in Eq. 1, a target point is more likely to be transported to the class prototypes closer to it or those with higher class propotion.With the conditional distribution and point-to-point transport cost, we can derive the target-to-prototype (T2P) expected cost of moving the target features in this mini-batch to source prototypes,In this target-to-prototype direction, we assign each target pixel to the prototypes according to their similarities and the class distribution. However, like many entropy minimization methods [1,2], optimizing target-to-prototype cost alone may result in degenerate trivial solutions, biasing the prediction towards a single dominant class [16]. To avoid mapping most of the target features to only a few prototypes, we add a prototype-to-target (P2T) transport cost in the opposite direction, which ensures that each prototype can be assigned to some target features. Similarly, we have:Then, combining the conditional transport cost in these two directions, we define the total prototype-anchored feature alignment (PFA) loss:Similar to [6], we initialize the adaptation model M t0 with the pre-trained source model M s and fix the weights of the classifier during adaptation."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,2.2,Contrastive Learning Using Unreliable Predictions,"After the PFA stage, the clusters of target features are shifted towards their corresponding source prototypes, which brings remarkable improvements for the initial noisy prediction (see Fig. 3(b)). To further improve the compactness of the target feature distribution, previous self-training methods mainly focus on strengthening the reliability of pseudo labels by developing denoising strategies [5,20], but discard those low-confidence predictions. However, such contempt for unreliable predictions may result in information loss. For example, in Fig. 1(c), the probability of the unreliable pixel hovers between spleen and left kidney, yet is confident enough to indicate the categories it does not belong to.With this intuition, we denote p t m,i as the softmax probabilities generated by model M t0 for the target data x t m,i . Then, for each class c, we construct three components, named query samples, positive prototypes, and negative samples, to explore those unreliable predictions as [18]."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Query Samples.,"During training, we employ the per-pixel entropy as uncertainty metric [18], and sample the pixels with low entropy (reliable pixel) in the current mini-batch as query candidates. We denote the set of features of all query pixels for class c as P c ,where H(•) is the entropy of the input probabilities and γ c is the entropy threshold for class c. Here we set γ c as the α c -th percentile of all the entropy values of pixels assigned a pseudo label c.Positive Prototypes. The positive prototype is the same for all query pixels from the same class. Instead of using the center of query samples like [18], we set them the same as the previous source prototype, which is denoted as z + c = µ c . Negative Samples. For a query sample from class c, its qualified negative samples should satisfy: 1) unreliable; 2) highly probable not belong to class c. Therefore, we introduce the pixel-level category order O t m,i = argsort(p t m,i ). For example, we have O t m,i (arg max p t m,i ) = 1 and O t m,i (arg min p t m,i ) = C. Thus, we can use O t m,i (c) to define the set of all negative samples:where r l is the low rank threshold and is set to 3 in our task.With the above definition, we have the pixel-level contrastive loss as:where K is the number of query samples, and z c,k ∈ P c denotes the k-th query sample from class c. Each query sample is paired with a positive prototype z + c and N negative samples z - c,k,j ∈ N c ."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,3,Experiments and Results,
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,3.1,Experimental Setup,"Datasets and Evaluation Metrics. We evaluate our SFDA approach on a cross-modality abdominal multi-organ segmentation task. For the abdominal datasets, we obtain 20 MRI volumes from the 2019 CHAOS Challenge [10] and 30 CT volumes from MICCAI 2015 [11], respectively. Both datasets are under the Creative Commons Attribution 4.0 International license and involve segmentation masks for the following abdominal organs: liver, right kidney, left kidney and spleen. We complete adaptation experiments both in the ""MRI to CT"" direction and in the ""CT to MRI"" direction. For the ""MRI to CT"" direction, we take the MRI modality to train the source model and vice verse. Both modalities are randomly divided into 80% for domain adaptation training and 20% for evaluation. For both datasets, we discard the axial slices that do not contain foreground and crop out the non-body region [3]. The value range in CT volumes is first clipped to [-125, 275]. Then min-max normalization has been performed on both datasets to normalize the intensity value to [0, 1]. After that, all the MRI and CT volumes are uniformly resized to 256 × 256 in axial plane. Due to the large variance in the slice thickness of CT and MRI modality, we split the volume into slices for the model training.For the evaluation, two main metrics, dice similarity coefficient (Dice) and average symmetric surface distance (ASSD) are used to quantitatively evaluate the segmentation results [4,15]. Implementation Details. We adopt classic U-Net structure for the segmentation model as the previous work [1]. The source segmentation model is trained in a fully-supervised manner for 10k iterations. During adaptation, we use Adam optimizer with the learning rate 1 × 10 -4 and a weight decay of 5 × 10 -4 . The temperature τ and batch size is set as 0.1 and 16, respectively. In PFA stage, we freeze the classifier and optimize F t0 θ for 200 iterations. In CL stage, we empirically set hyper-parameters α c = 80, K = 64, and N = 256 for all classes. All experiments are conducted with PyTorch on a single NVIDIA RTX 3090 GPU of 24 GB memory. Data augmentation such as random cropping, rotation, and brightness are adopted for source domain training and target domain adaptation."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,3.2,Results of Source-Free Domain Adaptation,"Comparision with Other Methods. In our experiments, ""no adaptation"" lower bound denotes learning a model on the source domain and directly test on the target domain without adaptation. And ""supervised"" upper bound means training and testing in the same target domain. We compared our methods with recent SFDA methods all designed for medical image segmentation scenarios, including a denoised pseudo-labeling approach (DPL) [5], a prior-aware entropy minimization approach (AdaMI) [1], a fourier style mining approach (FSM) [21], and a feature map statistics-guided approach [9]. We also considered top-performing UDA methods (i.e., SIFA [4], DAG-Net [19]). For a fair comparison, we utilized the same backbone for these methods [1,4,5,21] and reimplemented them according to their official codes. Note that we reported the results of methods [9,19] from papers, since their official codes were not released.The quantitative evaluation results are presented in Table 1. Compared to the upper and lower bounds in both directions, a huge performance gap can be observed due to the severe domain shifts between MRI and CT modalities. In ""MRI to CT"" direction, our method remarkably outperforms all other SFDA approaches on the right kidney and spleen, achieving the highest average Dice because, without PFA, the source model prediction is too noisy to sample the qualified query and negative pixels for contrastative learning. We also study the impact of different uncertainty percentile α c in Fig. 4(b). This parameter has a certain impact on performance, and we find α c = 80% achieves the best performance for most organs. Large α c may introduce low-confidence query samples for supervision, and small α c will drop some informative negative samples."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,4,Conclusion,"In this paper, we propose a novel two-stage framework to address the source-free domain adaptation problem in medical image segmentation. We first introduce a bi-directional transport cost to encourage the alignment between target features and source class prototypes in the prototype-anchored feature alignment stage. Also, a contrastive learning stage using unreliable predictions is further devised to learn a more compact target feature distribution. Sufficient experiments on the cross-modality abdominal multi-organ segmentation task validate the effectiveness and superiority of our method against other strong SFDA baselines, even some classical UDA approaches."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Fig. 1 .,
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Fig. 2 .,
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Fig. 3 .,
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 1.Q. Yu et al. ✔ 70.1 ± 6.9 52.9 ± 14.2 65.7 ± 12.5 70.9 ± 13.2 64.9 ± 8. of 86.1% and the lowest average ASSD of 1.4. Moreover, compared with recent UDA methods, our method obtains competitive results on average Dice and ASSD, which may be due to the use of unreliable predictions. As for ""CT to MRI"" direction, our method similarly shows great superiority on most organs as well, achieving the best performance in terms of both the average Dice (89.2%) and ASSD (1.3) among all SFDA methods. Figure 3(a) shows the segmentation results obtained by existing and our methods in both modalities. As observed, DPL is prone to amplify the initial noisy regions since it directly discards the unreliable pixels in self-training. For comparison, our method substantially rectificate the uncertain regions from the initial prediction, and details are shown in Fig. 3(b).Ablation Study. In Fig. 4(a), we verify the effectiveness of the proposed two SFDA stages by removing each stage while keeping the other. The consecutive two stage adaptation leads to the best performance, while the drop in Dice is more significant if we remove the PFA stage. This result is not surprising"
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,1,Introduction,"Bone shapes vary significantly across the world's population [1], making the design of personalized medical plates for repairing fractures challenging. Yet patient-specific implants would provide major benefits for surgeons and patients: surgery time would be reduced and treatment outcomes improved. While some plates are malleable enough to be contoured during surgery to improve overall fitness, these are more prone to fatigue failure when compared to rigid precontoured plates. But anatomically pre-contoured plates only fit a subset of the population [2][3][4], causing malalignment at the fracture site [5]. Even with the extra contouring step during the surgery, the plates do not fit all bone shapes [6].Our work contributes several key steps to create personalized plates: (i) we first design an optimal PHILOS 3D plate shape for the humerus bone, fulfilling the clinical constraints given by a senior surgeon; (ii) we propose an anatomyaware transfer of the plate shape into any new bone, resulting in a personalized plate shape; (iii) we propose a method to position an arbitrary plate on an arbitrary bone according to clinical constraints, allowing surgical compatibility to be evaluated; (iv) we leverage the compatibility assessment to obtain a compact set of plates that accommodates a given population of bones; (v) we validate our methodology with extensive experiments on ex-vivo and 3D-printed bones, demonstrating the relevance of the designed personalized plate shapes for the clinical setting; (vi) we make available for research purposes the humerus and plate 3D models, as well as the plate extraction and fitting code 1 .The creation of surgical plates consists of several tasks, which have been partially addressed by the existing state of the art. In Table 1 of Sup. Mat. we provide a schematic comparison of how our approach goes beyond existing methods.Most methods start with a base plate or template that is designed for a specific bone surgery (tibia, humerus, clavicle, etc.) [2,4,[6][7][8][9][10][11][12][13]. One important step is how to position a given plate on a given bone. Existing approaches are either manual, semi-automatic [2,3,7,11], or fully automatic [4,9,10,12,14]. Our fully automatic positioning strategy differs from the state of the art as it considers the surrounding anatomy; i.e. to minimize the risk of radial nerve damage, the plate is twisted around the bone so that the proximal end is fixed on the lateral side of the humeral head, and the distal end of the plate to the ventral surface of the humerus [15,16].Once the plate is positioned, one needs to evaluate if the plate can be used in surgery. Some works compute plate-to-bone metrics [2,6,13,17], but do not propose a binary decision criterion stating whether the plate is valid for surgery or not. We argue that this binary decision, also provided by existing works [3,4,[7][8][9]11,12,14], is important, as aggregate distance numbers can be misleading; e.g. one part of the plate could have a perfect fit but another part could make the plate not suitable for surgery. In our work, we define a three section-based criterion that takes into account the two plate-to-bone fixation regions and the middle plate section that transitions from one fixation region to the other. In addition, we perform a case study evaluation with an expert surgeon, in which the validity of the numerical criterion is confirmed.Once the plate is positioned on a bone, and the fit evaluated, one can further consider the question of how to deform the plate to fulfill the fit criterion. For instance, [6] aims to develop a plate shape to reduce in-situ plate manipulation. They approximate the plate and target bone surfaces by planar sections and measure the bending necessary for each section of the plate to fit the bone surface. In contrast, our approach is similar to the one of [3], in which the plate shape is not deformed after evaluating the fit, but rather directly extracted from the shape of an individual bone. We propose a plate transfer strategy that takes into account the matching of anatomic regions. One advantage over existing methods is that our approach simultaneously ensures both: (i) a personalized plate shape matching the bone's shape and (ii) its proper placement.Prior work proposes a set of plate shapes that, together, can accommodate a range of patients [8,9]. This set is created by manually modifying an original plate to improve its plate-to-bone distance. We propose an automatic approach in which we first create many plates, and then, with a greedy algorithm, select the plate set that accommodates the most bones."
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,2,Method,"Dataset. To create and evaluate our plate designs, we use a dataset consisting of 97 3D meshes of humerus bones, divided into two groups (A and B). Group A has 54 bones scanned with a FARO laser scanner (25 females and 29 males, 50% Black and 50% White, age range 17 and 45). Group B has 43 Computed Tomography (CT) scans of bones from autotomized body donors. The CT volumes were segmented and cleaned to reconstruct a mesh of the bone. Left-side humerus scans were mirrored along the z-axis to work with right-side humeri only. Plate Design. Given a bone, our goal is to automatically generate an optimal plate shape and determine its position on the bone. The optimality of our plate design is defined by an experienced surgeon, who established fixation points, areas to avoid, and plate-to-bone distance tolerances. These choices were validated by a second expert surgeon. A plate fulfilling these constraints on a bone is considered optimal for surgery. To obtain the plate design we 3D printed 7 bone meshes from our dataset with diverse shapes and asked the surgeon to annotate each bone with a marker. The surgeon indicated the bone areas that should be in contact with an ideal plate and the areas to be avoided (Fig. 1 left). From these annotations, we designed an ideal plate mesh P T contoured to a humerus template bone mesh T (Fig. 1 middle). This plate has similar dimensions as the actual INTEOS PROXIMAL HUMERAL PLATE 3.5. Following the surgeon's advise, the plate should not be in contact with the bone at the bone neck level, hence, an offset was added in this region. The fit criteria between a plate and a bone was defined such that the distance between them should not exceed 2mm at the fixation points, located at the head P h and the shaft P s , and should be less than 5mm in the other regions, with the exception of the plate neck, which must not be in direct contact with the bone (Fig. 1 "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,right).,"Bone Registration for Optimal Plate Transfer. Once the optimal plate is defined on the template bone, the goal is to register any new bone to the template, by preserving the relevant anatomic regions for the plate fixation. To do so, we adapt the registration technique designed for vertebrae [18] to the humerus.We first register each bone of set A by optimizing their Eqs. 1, 2 and 3 from [18]. Given these initial registrations we build a statistical shape model using principal component analysis (PCA). This model is parameterized with the shape vector β, and constructs a bone mesh T(β): an array of size N b × 3 containing the mesh vertex positions.Second, we register the whole dataset using the learned bone shape model by minimizingwhere S denotes the scanned bone mesh, t and r are, respectively, the 3D rigid translation and rotation applied to the bone mesh vertices, and F is a 3D pervertex offset applied to each mesh vertex. E p2m (S, .) is the point-to-triangle distance between the scan vertices and the mesh triangles. The function Δ(T) is the mesh Laplacian operator, which is used to regularize the offsets F. We minimize Eq. ( 1) in three successive steps: with respect to (t, r), β, and F. For each scan S i we obtain its corresponding bone registrationDue to the elongated shape of the humeri and their axial rotation similarity, the registrations B i can contain sliding, i.e. the same vertex does not correspond precisely to the same anatomic location on two different bones. To correct this, we perform a second pass of registration using smooth shells [19] to deform the initial template T to the surface of B i and we obtain the final bone registrations B i matching the scan S i (see Fig. 2 left). Note that smooth shells [19] can not directly register T to the scans S i , and the intermediate meshes B i are required.To validate the bone correspondences we define anatomic regions on the bone template T, and transfer them to all registrations B i on a per-vertex index basis. Figure 2 shows that the annotated anatomic parts are faithfully preserved. Plate Extraction. As the registration process preserves anatomic regions, given a bone registration B i , we can extract a new optimal plate P i . For each vertex of the designed plate P T we compute its offset to the template bone T. By applying these offsets to the new bone B i , we obtain the personalized plate P i .Positioning a Plate on a Bone. Now that we can create personalized plates, we want to study if a created plate can be used for surgery on another bone. For that, given an arbitrary plate P i and bone registration B j , we need to position the plate and determine if it fits. We automate the plate positioning by minimizing the distance between the plate and bone fixation points. We start by computing an initial 3D rigid transformation r 0 , t 0 by optimizing Eq. 2 in Sup. Mat. and then obtaining the final positioning r f , t f by optimizing Eq. 3 in Sup. Mat. These equations enforce the fixation areas of the plate P i h to match B j h and P i s to match B j s (see Fig. 1 right), while avoiding plate-to-bone inter-penetrations. Once the plate is positioned on the bone, we evaluate the binary fit criteria (see Fig. 1 right) to conclude whether the plate shape is fit for surgery or not."
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Plate Shapes Set.,"A single plate design can not accommodate the whole population due to its morphological variance [1]. Thus we propose to build a set of plate shapes with a greedy algorithm. The algorithm is summarized in pseudocode in Alg. 1 of Sup. Mat. Given a bone dataset, we start by creating as many personalized plates as bones. Then, for each plate we use the previous optimization to determine how many bones it accommodates, i.e how well a plate shape generalizes to different bones. We then select the plate that fits most bones and remove the plate and the fitted bones from the current sets. We iterate until no bones are left. The result is an ordered plate-set that accommodates the input bone dataset."
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,3,Experiments,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,3.1,Numerical Evaluations,"Bone Registration Accuracy. We validate that our registrations B i accurately match the scans S i by computing the mean distance (MD) between each registration to the closest vertex in the scan. We obtain a MD of 0.08 mm (std=0.04) for group A, and a MD of 0.27 mm (std=0.55) for group B. For both sets, the registered bones match the scans with sub-millimeter accuracy. In addition, all distances are less than 1mm on the surface where the plate is positioned (Fig. 3 "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,left).,"Plate Set Coverage. One application of our design is that it can generate a plate set that could be preprinted in the hospital and be readily available for immediate use. Using our greedy algorithm on our bone dataset, a set P N =5 S made of the first 5 plates accommodates already 51.04% of the bones and P N =10 S accommodates 73.96% (Fig. 3 middle). In Sec. 4 we discuss how this coverage could be improved."
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,3.2,Ex-Vivo Evaluations,"Comparison with State-of-the-Art Plates. We CT-scanned an isolated bone (not included in the original dataset) and asked a surgeon to manually contour a state-of-the-art plate, as done in clinical practice. We compare it to the custom plate P C and the best set plate P S by computing the plate-to-bone distances (Fig. 3 right). The proposed plates are clearly closer to the bone than the handcontoured plate. This proximity is known to be beneficial for bone recovery [20]. While P S is less accurate than the custom plate -and shorter, as it was generated from a shorter bone -the surgeon considered the plate and its placement as suitable. This experiment reveals the difficulty of closely fitting the bone by manually bending a plate and the benefit of the proposed design.Ex-vivo Surgery. We performed an ex-vivo experiment on a cadaveric arm, mimicking an actual minimally-invasive surgical operation setting. One goal was to test whether the designed 3D plates can be properly inserted along the bone under the muscles. We CT-scanned 3 cadaveric arms (not included in the original dataset), reconstructed the bone scans and registered them to obtain B 1 , B 2 and B 3 . For each bone we generated and 3D printed their custom plate P 1,2,3 C and the best fitting set plate P 1,2,3 S . The plates were drilled and coated with metallic paint to be visible on the CT scan. During the experiment, the plates were inserted in the cadaver arm, as it would be done in a standard operation, and fixed to the humerus with screws. For the 3 bones, the surgeon estimated that the fit of both the custom and set plates was satisfactory. He noted that the surface roughness of the 3D print and metallic coating was too high, making the plate hard to slide across the humerus into place. We CT-scanned each plate screwed to the bone and used the scans to asses the fit quality. Figure 4 left shows the plate-to-bone distances for each bone. The plate P 1  C fits closely on the proximal side near the sulcus bicipitalis. For P 2 C and P 3  C the fits are not as good as P 1 C . This was credited to a handling/alignment issue raised by the surgeon. Consequently, greater deviation in alignment is seen on the distal end of the bone on the lateral side. There is also a greater distance between the ventral part of the plate and the sulcus bicipitalis. The plate is too lateral on the humerus. In each case, the surgeon found the custom plate better. The limit in terms of fit was mainly attributed to the operative process rather than to the plate shape itself. The surgeon was satisfied with the 6 fits and noted that the 3D-printed plates were less stiff than the commercial plates he usually uses, making them harder to manipulate."
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,3.3,Evaluations with 3D-Printed Bones,"To further evaluate our design, we generated 7 bones {B i } i∈ [4;10] . B 7 is the mean bone shape and the 6 others are generated by uniformly sampling the bone PCA space B (β ∈ {-2, 2}) in the first 3 PCA dimensions. For each bone, we 3D printed its custom plate, P C , and the best set plate, P S , from the plate-set.First, we presented pairs of bones and plates to the surgeon and asked whether they were fit for surgical use. Then, we asked them to position the plates on the 3D bones to evaluate our positioning strategy. Last, to evaluate the adequacy of a plate set, we presented one bone to the surgeon and asked them to choose the best plate from the set.Ready-for-Surgery Evaluation. For each bone B i , the surgeon considered both plates as candidates and evaluated if their shape was suitable for surgery. The surgeon stated that 100% of the plates were fit for surgery and that the custom plates P C were always clearly better fitting than the plates P S from the set.Positioning Evaluation. To evaluate the plate positioning computed by our algorithm, we compare it to the surgeon's positioning. We asked the surgeon to position the set plates P S on each bone, secured them with blue tack and scanned the whole with a FARO hand scanner. Figure 4 right shows the overlay of both positions of the used plate for each bone. For most bones, the placement of the plate by the surgeon and the algorithm were very similar. The differences can be explained as there are multiple plate positions that fit. Both placements (orange and green), even in the most dissimilar case (first bone in Fig. 4 right) were considered correct by the surgeon.Choosing from the Plate Set. Our algorithm iteratively positions the plates P i S from i = 0 to i = N and picks the best-fitting plate, so we asked the surgeon to perform a similar trial-and-error task to evaluate the relevance of our plate set.Out of the 7 bones, the surgeon only chose the same plate as our algorithm for two of them (B 7 and B 10 ). For the other cases, the surgeon compared his selection to the algorithms' choice and confirmed that both plates were fit for surgery: different plates with slightly different placements can work for the same bone. Most interestingly, some plates selected by the surgeon did not fit the bone according to the strict numeric fit criteria. We discuss this finding in the next section. The surgeon noted that the plates of our set were always preferred over the state-of-the-art plates."
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,4,"Conclusions, Take-Aways and Future Work","Conclusion. We propose to automatically generate a custom humerus plate that specifically matches the 3D shape of a bone and specifically meets the requirements of a surgeon for minimally invasive surgery. We also generate a set of plate shapes that accommodates a given bone population. We extensively evaluate our approach on cadaverous arms and 3D printed bones.Experimental Takeaways. All the proposed plates (individual and set plate) match the shape of the bones, while the individual ones are considered the best by the surgeon. Furthermore, our experiments show the importance of evaluating on 3D-printed bones and ex-vivo arms. With the former, the surgeon can easily assess the plate-to-bone fit. With the latter, the insertion procedure can affect the plate placement. Moreover, our results on 3D printed bones argue for a relaxation of the theoretical fitting constraints: the plate-to-bone distance could be higher in some areas while still obtaining a proper surgical fit. Loosening the tolerances would allow more bones to be fit with the same plates and potentially the number of plates required in the plate set could be further reduced.Future Work. To reach actual clinical use, two elements need consideration: (i) the current method takes as input the shape of a healthy bone, whereas patients have a fracture. One could perform a CT scan of the other arm and generate the symmetric bone or leverage methods that reconstruct a full bone from partial observations [18]. (ii) Our work focuses on the geometric design of the plate but does not consider the physical properties of the plate, such as thickness and stiffness. Future work should optimize these physical properties for the stresses that the plate must endure, e.g. using finite element methods."
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Fig. 1 .,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Fig. 2 .,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Fig. 3 .,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Fig. 4 .,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 46.
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,1,Introduction,"Age-related macular degeneration (AMD) is the leading cause of blindness in the elderly, affecting nearly 200 million people worldwide [24]. Patients with early stages of the disease exhibit few symptoms until suddenly converting to the late stage, at which point their central vision rapidly deteriorates [12]. Clinicians currently diagnose AMD, and stratify patients, using biomarkers derived from optical coherence tomography (OCT), which provides high-resolution images of Fig. 1. Our method finds common patterns of disease progression in datasets of longitudinal images. We partition time series into sub-trajectories before introducing a clinically motivated distance function to cluster the sub-trajectories in feature space. The clusters are then assessed by ophthalmologists on their interpretability and ability to capture the progression of AMD.the retina. However, the widely adopted AMD grading system [7,13], which coarsely groups patients into broad categories for early and intermediate AMD, only has limited prognostic value for late AMD. Clinicians suspect that this is due to the grading system's reliance on static biomarkers that are unable to capture temporal dynamics which contain critical information for assessing progression risk.In their search for new biomarkers, clinicians have annotated known biomarkers in longitudinal datasets that monitor patients over time and mapped them against disease progression [2,16,19]. This approach is resource-intensive and requires biomarkers to be known a priori. Others have proposed deep-learningbased methods to discover new biomarkers at scale by clustering OCT images or detecting anomalous features [17,18,23]. However, these approaches neglect temporal relationships between images and the obtained biomarkers are by definition static and cannot capture the dynamic nature of the disease.Our Contribution: In this work, we present a method to automatically propose biomarkers that capture temporal dynamics of disease progression in longitudinal datasets (see Fig. 1). At the core of our method is the novel strategy to represent patient time series as trajectories in a latent feature space. Individual progression trajectories are partitioned into atomic sub-sequences that encode transitions between disease states. Then, we identify population-level patterns of AMD progression by clustering these sub-trajectories using a newly introduced distance metric that encodes three distinct temporal criteria. In experiments involving 160,558 retinal scans, four ophthalmologists verified that our method identified several candidates for temporal biomarkers of AMD. Moreover, our clusters demonstrated greater prognostic value for late-stage AMD when compared to the widely adopted AMD grading system."
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,2,Related Work,"Current AMD Grading Systems: Ophthalmologists' current understanding of progression from early to late AMD largely involves drusen, which are subretinal lipid deposits. Drusen volume increases until suddenly stagnating and regressing, which often precedes the onset of late AMD [16]. The established AMD grading system stratifies early and intermediate stages solely by the size of drusen in a single OCT image [1,6,7,10]. Late AMD is classified into either choroidal neovascularisation (CNV), identified by subretinal fluid, or geographic atrophy, signalled by progressive loss of photoreceptors and retinal thinning. The degree of atrophy can be staged using cRORA (complete retinal pigment epithelium and outer retinal atrophy), which measures the width in μm of focal atrophy in OCT [13]. Grading systems derived from these biomarkers offer limited diagnostic value and little to no prognostic capability.Tracking Evolution of Known Biomarkers: Few research efforts have aimed at quantifying and tracking known AMD biomarkers, mostly drusen, over time [16,19]. More work has explored the disease progression of Alzheimer's disease (AD), which offers a greater array of quantitative imaging biomarkers, such as levels of tau protein and hippocampal volume. Young et al. [25] fit an event-based model that rediscovers the order in which these biomarkers become anomalous as AD progresses. Vogel et al. [21] find four distinct spatiotemporal trajectories for tau pathology in the brain. However, this only works if biomarkers are known a priori and requires manual annotation of entire time series.Automated Discovery of Unknown Biomarkers: Prior work for automated biomarker discovery in AMD explores the latent feature space of encoders trained for image reconstruction [18,23], segmentation [27] or generative adversarial networks [17]. However, these neural networks are prone to overfit to their specific task and lose semantic information regarding the disease. Contrastive methods [3,8,26] encode invariance to a set of image transformations, which are uncorrelated with disease features, resulting in a more expressive feature space. However, all aforementioned methods group single images acquired at one point in time, and in doing so neglect temporal dynamics. The one work that tackles this challenge, and the most related to ours, categorises the time-dependent response of cancer cells to different drugs, measured by the changing distance in contrastive feature space from healthy controls [5]."
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3,Materials and Methods,
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.1,OCT Image Datasets,"We use two retinal OCT datasets curated in the scope of the PINNACLE study [20]. We first design and test our method on a Development dataset, which was collected from the Southampton Eye Unit. Afterwards, we test our method on a second independent Unseen dataset, which was obtained from Moorfields Eye Hospital. All images were acquired using Topcon 3D OCT devices (Topcon Corporation, Tokyo, Japan). After strict quality control, the Development dataset consists of 46,496 scans of 6,236 eyes from 3,456 patients. Eyes were scanned 7.7 times over 1.9 years on average at irregular time intervals. The Unseen dataset is larger, containing 114,062 scans of 7,253 eyes from 3,819 patients. Eyes were scanned 16.6 times over 3.5 years on average. A subset of 1,031 longitudes was labelled using the established AMD grading protocols derived from known imaging biomarkers. Early AMD was characterised by small drusen between 63-125µm in diameter. We also recorded CNV, cRORA (≥ 250µm and <1000µm), cRORA (≥ 1000µm) [13] and healthy cases with no visible biomarkers. Visual acuity scores, which measured the patient's functional quality of vision using a LogMAR chart, are available at 83,964 time points."
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.2,Self-supervised Feature Space Using Contrastive Learning,"We use the BYOL contrastive loss [8] in Eq. 1 to train a ResNet50 (4x) model f over each batch of twice transformed images xwhere the output of the momentum updated 'teacher' network f is passed through a stop-gradient, so that only the student network f is updated. As several of the contrastive transformations designed for natural images are inapplicable to medical images, such as solarisation, colour shift and greyscale, we use the set tailored for retinal OCT images by Holland et al. [9]. Models were trained on the entire dataset for 120,000 steps using the Adam optimiser with a momentum of 0.9, weight decay of 1.5 • 10 -6 and a learning rate of 5 • 10 -4 . After training f , we first remove the final linear layer before projecting all labelled images to the feature space of 2048 dimensions.Fig. 2. Illustration of sub-trajectory distance functions which each encode temporal criteria for similarity (see Eq. 4). We illustrate clusters assignments, denoted by colour, resulting from three combinations of φ and λ."
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.3,Extracting Sub-trajectories via Partitioning,"Naively clustering whole time series of patients ignores two characteristics of longitudinal data. Firstly, individual time series are not directly comparable as patients enter and leave the study at different stages of their overall progression.Secondly, longer time series can record multiple successive transitions in disease stage. Inspired by TRACLUS [11], the state of the art in trajectory clustering, we adapt their partition-and-group framework by assuming that trajectories can be partitioned into a common set of sub-trajectories that capture singular transitions between progressive states of the disease.For each eye, we first form piecewise-linear trajectories by linking points in feature space that were derived from consecutively acquired OCT images. We then extract sub-trajectories by finding all sequences of images spanning 1.0 ± 0.5 years of elapsed time within each trajectory. Next, to avoid oversampling trajectories with a shorter time interval between images, we randomly sample at most one sub-trajectory in every 0.5-year time interval."
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.4,Sub-trajectory Distance Functions and Clustering,"In order to find common patterns of disease progression among sub-trajectories we cluster them. To this end we introduce a new distance function between subtrajectories that incorporates three distinct temporal criteria (see Fig. 2). The first, formulated in Eq. 2, matches two sub-trajectories, U and V , of patients who progress between the same start and end states:Since all sub-trajectories cover a similar temporal duration, D transition also differentiates between fast and slow progressors and stable periods of no progression. However, by ignoring intermediary images, this metric does not respect the disease pathway along which patients progress. To incorporate this, we include a second metric that measures path dissimilarity, calculated using dynamic time warping (DTW) [4,14,15]. DTW finds the optimal temporal alignment between two time series before computing their distance. This re-alignment allows us to match sub-trajectories that traverse the same disease states in the same order, irrespective of the rate of change between states. We combine D transition with DTW using a λ ∈ R, 0 ≤ λ ≤ 1 coefficient so the overall distance between U and V isThe third and final temporal criteria is to match time series that progress in the same relative direction, regardless of absolute disease states. We weight the contribution of this with φ ∈ R, 0 ≤ φ ≤ 1 in Eq. 4:Spectral Clustering: As the non-linearity of D subtraj prohibits the use of k-means for clustering, we instead use spectral clustering [22] to group similar sub-trajectories. Hereby, we construct an affinity matrix A encoding the negative of the distance D subtraj between all pairs of sub-trajectories. Using A, we group sub-trajectories into K clusters. "
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.5,Qualitative and Quantitative Evaluation of Clusters,"Initially, we tune the hyperparameters, λ, φ and K, on the Development dataset by heuristically selecting values that result in higher uniformity between subtrajectories within each cluster. Two teams of two ophthalmologists then review 20 sub-trajectories from distinct patients in each cluster, interpreting and summarising any consistently observed temporal dynamics. Next, using the same hyperparameters we apply the method directly to the Unseen dataset. The ophthalmologists then review these clusters and confirm whether they capture the same temporal biomarkers observed in the Development dataset.In addition to the qualitative evaluation, we also validate the utility of our clusters as biomarkers that stratify risk of disease progression. We test this by predicting the time until conversion to late AMD and its subtypes, CNV and cRORA. Additionally, we predict current visual acuity. For prediction, each sub-trajectory is characterised by a vector of size K that encodes proportional similarity to each cluster. This vector is then used by a Lasso linear regression model. Similarly, we fit an equivalent linear regression model to the static biomarkers from the established grading system detailed in Sect. 3.1. We also include a demographic baseline using age and sex. We also add a temporally agnostic baseline that clusters only single time points. Finally, to demonstrate the performance gap between our interpretable approach and black-box supervised learning algorithms, we include a fully supervised deep learning baseline by fitting an SVR directly to the feature space. Each experiment uses 10-fold cross validation on random 80/20 partitions, while ensuring a patient-wise split. Finally, we repeat the entire method, starting from sub-trajectory extraction, followed by clustering and then regression experiments, using 7 random seeds and report the means and standard deviations. "
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,4,Experiments and Results,"Sub-trajectory Clusters are Candidate Temporal Biomarkers: By first applying our method to the Development dataset we found that using λ = 0.75, φ = 0.75 and K = 30 resulted in the most uniform and homogeneous clusters while still limiting the total number of clusters to a reasonable amount. Achieving the same cluster quality with smaller values of φ required many more clusters in order to encode all combinations of possible start and end disease states. The expert ophthalmologists remarked that many of the identified clusters capture dynamics that have already been linked to the progression of AMD, even though they are not currently included in any clinical grading system. Using the same hyperparameters our method generalised to the Unseen dataset which yielded clusters with equivalent dynamics and quality (see Fig. 3). Ophthalmologists identified clusters capturing the same variants of temporal progression in both datasets. They named these as 'rapid growth of drusen pigment epithelial detachments (PED)', 'regression of drusen PED', 'development of subretinal fluid ', 'development of intraretinal fluid ', 'development of hypertransmission' and 'stable state' (no signs of progression at each disease state).Sub-trajectory Clusters Predict Conversion to Late AMD: Next, we validated that our clusters are predictive of progression to late AMD. Our subtrajectory clusters were comparable to, and in some cases outperformed, the current widely adopted grading system in predicting risk of conversion (see Table 1). In all tasks the standard biomarkers are only marginally more indicative of risk than the patient's age and sex. This experiment confirms that our clusters are related to disease progression."
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,5,Discussion and Conclusion,"Motivated to improve inadequate grading systems for AMD that do not incorporate temporal dynamics we developed a method to automatically propose biomarkers that are time-dependent, interpretable, and predictive of conversion to late-stage AMD. We applied our method to two large longitudinal datasets, cataloguing 3,218 total years of disease progression. The found time-dependent clusters were subsequently interpreted by four ophthalmologists. They found them to capture distinct patterns of disease progression that have been previously linked to AMD, but are not currently included in clinical grading systems. Furthermore, we experimentally demonstrated that the found clusters predict conversion to late-stage AMD on par with the established grading system.In the future, biomarkers identified by our method can be further refined by clinicians. We will also use the full volumetric image to model progression dynamics outside the macular. As late stage patients were overrepresented in our datasets, we also intend to apply our method to datasets with greater numbers of patients progressing from earlier disease stages. Ultimately, we envision that proposals from our method may inform the next generation of grading systems for AMD that incorporate the temporal dimension intrinsic to this dynamic disease."
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,,Fig. 3 .,
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,,,
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,,Table 1 .,
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_68.
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,1,Introduction,"Diffuse glioma is the most common and aggressive primary brain tumors in adults, accounting for more deaths than any other type [7]. Pathology diagnosis is the gold standard for diffuse glioma but is usually time-consuming and highly depends on the expertise of senior pathologists [13]. Hence, automatic algorithms based on histology whole slide images (WSIs) [15], namely digital pathology, promise to offer rapid diagnosis and aid precise treatment.Recently, deep learning has achieved success in diagnosing various tumors [2,21]. Most methods are mainly predicting histology based on WSI, less concerning molecular markers. However, the paradigm of pathological diagnosis of glioma has shifted to molecular pathology, reflected by the 2021 WHO Classification of Tumors of the Central Nervous System [14]. The role of key molecular markers, i.e., isocitrate dehydrogenas (IDH) mutations, co-deletion of chromosome 1p/19q and homozygous deletion (HOMDEL) of cyclin-dependent kinase inhibitor 2A/B (CDKN), have been highlighted as major diagnostic markers for glioma, while histology features that are traditionally emphasized are now considered as reference, although still relevant in many cases. For instance, in the new pathology scheme, glioblastoma is increasingly diagnosed according to IDH mutations, while previously its diagnosis mostly relies on histology features, including necrosis and microvascular proliferation (NMP). 1  However, the primary approaches to assess molecular markers include gene sequencing and immuno-staining, which are time-consuming and expensive than histology assessment. As histology features are closely associated with molecular alterations, algorithm predicting molecular markers based on histology WSIs is feasible and have clinical significance. Moreover, under the new paradigm of integrating molecular markers with histological features into tumor classification, it is helpful to model the interaction of histology and molecular makers for a more accurate diagnosis. Therefore, there is an urgent need for developing novel digital pathology methods based on WSI to predict molecular markers and histology jointly and modeling their interactions for final tumor classification, which could be valuable for the clinically relevant diagnosis of diffuse glioma.This paper proposes a deep learning model (DeepMO-Glioma) for glioma classification based on WSIs, aiming to reflect the molecular pathology paradigm. Previous methods are proposed to integrate histology and genomics for tumour diagnosis [3,10,20]. For instance, Chen et al. [3] proposed a multimodal fusion strategy to integrate WSIs and genomics for survival prediction. Xing et al. [20] devised a self-normalizing network to encode genomics. Nevertheless, most existing approaches of tumor classification only treat molecular markers as additional input, incapable to simultaneously predict the status of molecular markers, thus clinically less relevant under the current clinical diagnosis scheme. To jointly predict histology and molecular markers following clinical diagnostic pathway, we propose a novel hierarchical multi-task multi-instance learning (HMT-MIL) framework based on vision transformer [4], with two partially weight-sharing parts to jointly predict molecular markers and histology.Moreover, multiple molecular markers are needed for classifying cancers, due to complex tumor biology. To reflect real-world clinical scenarios, we formulate predicting multiple molecular markers as a multi-label classification (MLC) task. Previous MLC methods have successfully modeled the correlation among labels [12,22]. For example, Yazici et al. [22] proposed an orderless recurrent method, while Li et al. designed a label attention transformer network with graph embedding. In medical domain, Zhang et al. [25] devised a dual-pool contrastive learning for classifying fundus and X-ray images. Despite success, when applied to predicting multiple molecular markers, most existing methods may ignore the co-occurrence of molecular markers, which have intrinsic associations [23]. Hence, we propose a co-occurrence probability-based, label-correlation graph (CPLC-Graph) network to model the co-occurrence of molecular markers, i.e., intra-omic relationship.Lastly, we focus on modeling the interaction between molecular markers and histology. Specifically, we devise a novel inter-omic interaction strategy to model the interaction between the predictions of molecular markers and histology, e.g., IDH mutation and NMP, both of which are relevant in diagnosing glioblastoma. Particularly, we design a dynamical confidence constraint (DCC) loss that constrains the model to focus on similar areas of WSIs for both tasks. To the best of our knowledge, this is the first attempt to classify diffuse gliomas via modeling the interaction of histology and molecular markers.Our main contributions are: (1) We propose a multi-task multi-instance learning framework to jointly predict molecular markers and histology and finally classify diffuse glioma, reflecting the new paradigm of pathology diagnosis. (2) We design a CPLC-Graph network to model the intra-omic relationship of multiple molecular markers. (3) We design a DCC learning strategy to model the inter-omic interaction between histology and molecular markers for glioma classification."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,2,Preliminaries,"Database: We use publicly available TCGA GBM-LGG dataset [6]. Following [15], we remove the WSIs of low quality or lack of labels. Totally, we include Training Labels: Original lables for genomic markers and histology of WSIs are obtained from TCGA database [6]. According to the up-to-date WHO criteria [14], we generate the classification labels for each case as grade 4 glioblastoma (defined as IDH widetype), oligodendroglioma (defined as IDH mutant and 1p/19q co-deletion), grade 4 astrocytoma (defined as IDH mutant, 1p/19q non co-deletion with CDKN HOMDEL or NMP), or low-grade astrocytoma (other cases)."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,3,Methodology,"Figure 1 illustrates the proposed DeepMO-Glioma. As shown above, the upto-date WHO criteria incorporates molecular markers and histology features. Therefore, our model is designed to jointly learn the tasks of predicting molecular markers and histology features in a unified framework. DeepMO-Glioma consists four modules, i.e., stem, genomic marker prediction, histology prediction and cross-omics interaction. Given the cropped patches {X i } N 1 as the input, DeepMO-Glioma outputs 1) the status of molecular markers, including IDH mutation lidh ∈ R 2 , 1p/19q co-deletion l1p/19q ∈ R 2 and CDKN HOMDEL lcdkn ∈ R 2 , 2) existence of NMP lnmp ∈ R 2 and 3) final diagnosis of diffuse gliomas lglio ∈ R 4 . Of note, the final diagnosis of diffuse gliomas is generated via a decision tree-based logical function with the input of predicted molecular markers and histology, consistent with the up-to-date WHO criteria."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,3.1,Hierarchical Multi-task Multi-instance Learning,"To extract global information from input {X i } N  1 , we propose a hierarchical multi-task multi-instance learning (HMT-MIL) framework for both histology and molecular marker predictions. Different from methods using one [24] or several [3,20] representative patches per slide, HMT-MIL framework can extract information from N = 2,500 patches per WSI via utilizing the MIL learning paradigm with transformer blocks [4] embedded. Note for WSIs with patch number< N, we adopt a biological repeat strategy for dimension alignment. "
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,3.2,"Co-occurrence Probability-Based, Label-Correlation Graph","In predicting molecular markers, i.e., IDH, 1p/19q and CDKN, existing MLC methods based on label correlation may ignore the co-occurrence of the labels. In the genomic marker prediction module, we proposed a co-occurrence probabilitybased, label-correlation graph (CPLC-Graph) network and a label correlation (LC) loss for intra-omic modeling of the co-occurrence probability of the three markers.1) CPLC-Graph network: CPLC-Graph (Fig. 2) is defined as G = (V, E), where V indicates the nodes, while E represents the edges. Given the intermediate features in predicting the three molecular markers subnetsas input nodes, we construct a co-occurrence probability based correlation matrix A ∈ R 3×3 to reflect the relationships among each node feature, with a weight matrix W g ∈ R C×C to update the value of F in . Formally, the output nodes F mid ∈ R 3×C are formulated by a single graph convolutional network layer asIn (1), δ(•) is an activation function and p(F in i |F in j ) denote the probability of the status of i-th marker given the status of j-th marker. Besides, residual structure is utilized to generate the final output F out of CPLC-Graph network, defined aswhere α is a graph balancing hyper-parameter."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,2) LC loss:,"In order to fully exploit the co-occurrence probability of different molecular markers, we further devise the LC loss that constrains the similarity between any two output molecular markers F out i and F out j to approach their correspondent co-occurrence probability A j i . Formally, the LC loss is defined asIn (2), MSE denotes the function of mean square error, while D i,j cos is the cosine similarity of features F out i and F out j ."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,3.3,Dynamical Confidence Constraint,"In the cross-omics interaction module, we design a dynamical confidence constraint (DCC) strategy to model the interaction between molecular markers and histological features. Taking IDH and NMP as an example, the final outputs for IDH widetype2 and NMP predictions can be defined as lwt = N n=1 ω n wt f n wt and lnmp = N n=1 ω n nmp f n nmp , respectively. Note that f n wt and ω n wt are values of the extracted feature and the corresponding decision weight of n-th patch, respectively. We then reorder [ω n wt ] N n=1 to [ω n wt ] N n=1 based on their values. Similarly, we obtain [ω n nmp ] N n=1 for NMP confidence weights. Based on ordered confidence weights, we constrain the prediction networks of histology and molecular markers to focus on the WSI areas important for both predictions, thus modeling inter-omic interactions. Specifically, we achieve the confidence constraint through a novel DCC loss focusing on top K important patches for both prediction. Formally, the DCC loss in m-th training epoch is defined as:where S(ω k wt , ωnmp ) is the indicator function taking the value 1 when the k-th important patch of IDH widetype is in the set of top K m important patches for NMP, and vice versa. In addition, to facilitate the learning process with DCC loss, we adopt a curriculum-learning based training strategy dynamically focusing on hard-to-learn patches, regarded as the patches with higher decision importance weight, as patches with lower confidence weight, e.g., patches with fewer nuclei, are usually easier to learn in both tasks. Hence, K m is further defined asIn ( 4), K 0 and m 0 are hyper-parameters to adjust L DCC in training process."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,4,Experiments and Results,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,4.1,Implementation Details,"The proposed DeepMO-Glioma is trained on the training set for 70 epochs, with batch size of 8 and initial learning rate of 0.003 with Adam optimizer [11] together with weight decay. Key hyper-parameters are listed in Table 1 of supplementary material. All hyper-parameters are tuned to achieve the best performance over the validation set. All experiments are conducted on a computer with an Intel(R) Xeon(R) E5-2698 CPU @2.20 GHz, 256 GB RAM and 4 Nvidia Tesla V100 GPUs. Additionally, our method is implemented on PyTorch with Python environment. * We modified baselines to the MIL setting, since they are not originally designed for MIL. "
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,4.2,Performance Evaluation,"1) Glioma classification. We compare our model with five other state-of-theart methods: CLAM [15], TransMIL [16], ResNet-18 [5], DenseNet-121 [8] and VGG-16 [17]. Note CLAM [15] and TransMIL [16] are MIL framework, while others are commonly-used image classification methods, set as our baselines.The left panel of Table 1 shows that DeepMO-Glioma performs the best, achieving at least 6.1%, 13.1%, 3.1% and 11.0% improvement over other models in accuracy, sensitivity, specificity and AUC, respectively, indicating that our model could effectively integrate molecular markers and histology in classifying diffuse gliomas.2) Predictions of genomic markers and histology features. From the left panel of Table 2, we observe that DeepMO-Glioma achieves the AUC of 92.0%, 88.1%, 77.2% and 94.5% for IDH mutation, 1p/19q co-deletion, CDKN HOMDEL and NMP prediction, respectively, considerably better than all the comparison models. Figure 3 plots the ROCs of all models, demonstrating the superior performance of our model over other comparison models.3) Network interpretability. An additional visualization experiment is conducted based on patch decision scores to test the interpretability of our method. Due to the page limit, the results are presented in supplementary Fig. 1."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,4.3,Results of Ablation Experiments,"1) CPLC-Graph network. The right panels of Table 1 shows that, by setting graph balancing weight α to 0 for the proposed CPLC-Graph, the accuracy, sensitivity, specificity and F 1 -score decreases by 7.8%, 29.0%, 3.6% and 21.6%, respectively. Similar results are observed for the prediction tasks of molecular markers and histology (Table 2). Also, the ROC of removing the CPLC-Graph network is shown in Fig. 3. These results indicate the utility of the proposed CPLC-Graph network.2) LC loss. The right panels of Table 1 shows that the performance after removing LC loss decreases in all metrics, causing a reduction of 6.1%, 15.0%, 4.3% and 9.8%, in accuracy, sensitivity, specificity and F 1 -score, respectively. Similar results for the tasks of molecular marker and histology prediction are observed in the right panel of Table 2 with ROCs in Fig. 3, indicating the effectiveness of the proposed LC loss.3) DCC loss. From Table 1, we observe that the proposed DCC loss improves the performance in terms of accuracy by 9.1%. Similar results can be found for sensitivity, specificity and F 1 -score. From Table 2, we observe that the AUC decreases 2.9%, 2.9%, 0.5% and 2.8% for the prediction of IDH, 1p/19q, CDKN and NMP, respectively, when removing the DCC loss. Such performance is also found in comparing the ROCs in Fig. 3, suggesting the importance of the DCC loss for all the tasks."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,5,Summary,"The paradigm of pathology diagnosis has shifted to integrating molecular makers with histology features. In this paper, we aim to classify diffuse gliomas under up-to-date diagnosis criteria, via jointly learning the tasks of molecular marker prediction and histology classification. Inputting histology WSIs, our model incorporates a novel HMT-MIL framework to extract global information for both predicting both molecular markers and histology. We also design a CPLC-Graph network and a DCC loss to model both intra-omic and inter-omic interactions. Our experiments demonstrate that our model has achieved superior performance over other state-of-the-art methods, serving as a potentially useful tool for digital pathology based on WSIs in the era of molecular pathology."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Fig. 1 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,"2 ,",
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Fig. 2 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Fig. 3 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Table 1 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Table 2 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_52.
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,1,Introduction,"Depiction of white matter fiber tracts is of paramount importance for brain characterization in health and disease. Diffusion-weighted magnetic resonance imaging (dMRI) is the method of choice to study axon bundles that connect different brain regions. Several models have been proposed to map the 4-dimensional diffusion signal to objects such as tensors or fiber orientation distribution functions (FODs) [23,32], which can be further processed to compute metrics such as tract orientation and apparent fiber density [16,26]. Model-based FODs are the mathematical frameworks of choice for microstructure estimation. In fact, FODs accurately describe the underlying microstructure by a deformed sphere in which different radii correspond to different intra-voxel fibers. Moreover, without FODs, tracking stops prematurely and favors shorter fiber tracts [8]. Standard FOD estimation methods [1,15,23,27,32] process voxels individually and thus do not exploit correlations between neighboring voxels. As a result, these methods demand dMRI measurements with multiple b-values and a high number of gradient directions to account for the response function of each tissue type [15].These acquisitions require prolonged scans that are not affordable for newborn and fetal subjects because of the sensitivity of these cohorts and the increased risk of motion. Acquisitions have to be fast to freeze in-plane motion; yet data dropout rates are high in these cohorts because of motion artifacts. Reconstructing FODs in developing brains has been performed [4,5,7,30] using high-quality datasets and rich information including several gradient directions, higher and/or multiple b-values, and high signal-to-noise ratio (3 T magnetic field strength). Additionally, the datasets were acquired in a controlled and uniform research setting with healthy volunteers, which can hardly be reproduced in the clinical environment. Moreover, and in contrast to adult brains, anisotropy increases in white matter fibers of developing brains because of increased water volume and poor alignment of the fibers [6]. Gray matter on the other hand, during early gestational weeks, is highly anisotropic because of the complexity of the formation of cell bodies, glial cells, and the different neuronal structures [6]. This dynamic period for microstructure [2,9] makes FOD estimation a more challenging task. Adaptive learning-based methods can be leveraged to learn from high-quality datasets and exploit this knowledge in clinical routine acquisitions.Deep learning models, first suggested in [12], promise to overcome the error accumulation of suboptimal processing steps that are characteristic of standard estimation techniques. This end-to-end learning paradigm has been then applied in dMRI for several purposes [13,18,22,24,25]. The authors in [24] have accurately predicted tensor maps with six diffusion measurements. In [22], a 2D convolutional neural network (CNN) was used to predict the orientation of the fibers in a classification approach whereas [25] deployed a 3D CNN to predict FODs using a small neighborhood of the diffusion signal. In [18], a feedforward neural network was used to predict the FODs and found that 44 directions can be sufficient. However, the network does not exploit neighboring voxels correlations. A more recent work [13] used a Transformer-CNN block to first map 200 to 60 directions and the latter to FODs. However, for uncooperative cohorts such as neonates or fetuses, this number of measurements is unrealistic to acquire.To the best of our knowledge, no learning-based method to predict FODs has been reported for newborn and fetal brains. In this work, we demonstrate that a deep convolutional neural network with a large field of view (FOV) can accurately estimate FODs using only 6-12 diffusion-weighted measurements. Our contribution is three-fold. We first show that a deep learning method can achieve an accuracy level that is comparable with the agreement between the state-ofthe-art methods, while drastically reducing the number of measurements, for developing brains. We then show a low agreement between state-of-the-art methods in terms of different metrics using data from a highly controlled setting, namely the developing Human Connectome Project (dHCP). This addresses the need to build reproducible and reliable pipelines for white matter characterization [29], particularly for the developing brain. Finally, we demonstrate the generalizability of our method on two clinical datasets of fetuses and newborns that were acquired in completely different settings than those used in the training data. "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2,Methodology,
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.1,Paradigm,"The method is based on directly learning a mapping between the raw diffusion signal and the FOD in a supervised manner (Fig. 1). Our model inputs are respectively 6 single-shell (b = 1000 s/mm 2 ) measurements for the neonate network (DL n ) and 12 single-shell (b = 400 s/mm 2 ) measurements for the fetal inference network (DL f ), trained on pre-term subjects (as in [17,19]). To be independent on gradient directions, projection of the signal onto spherical harmonics basis (SH) (SH-L max order 2 and 4 respectively for the two networks) was performed to predict the FOD represented in the SH basis (SH-L max order 8). To train the model, these target coefficients are estimated from 300 multi-shell measurements using MSMT-CSD [15]. These measurements are distributed over 3 shells of {400, 1000, 2600} s/mm 2 with 64, 88 and 128 samples, respectively, and 20 b0 (b = 0 s/mm 2 ) images. The few measurements used as input to the model were based on the scheme in [31], whereby the gradient directions minimized the condition number of the diffusion tensor reconstruction matrix."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.2,Data Processing,"dHCP Newborns -We have selected two subsets from the developing Human Connectome Project (dHCP) dataset (1) 100 subjects (weeks: [32.1, 44.7], mean: 40, standard deviation: 2.4) and ( 2) 68 pre-term subjects (weeks: [29.3, 37.0], mean: 34.9, standard deviation: 1.8). The data was acquired with a 3T Philips Achieva scanner in a multi-shell scheme (b ∈ {0, 400, 1000, 2600} s/mm 2 ) [14] and was denoised, motion and distortion corrected [3]. It has a final resolution of 1.17×1.17×1.5 mm 3 in a field of view of 128×128×64 voxels. We have upsampled the data to 1 mm isotropic resolution to account for network isotropic 3D patches. We have additionally normalized the input data by b0. A white matter mask was generated using the union of the White Matter and the Brainstem labels provided by the dHCP, and the voxels where Fractional Anisotropy (FA) was higher than 0.25. A resampling of the dHCP labels from T2-w resolution (0.5 mm 3 isotropic) to 1 mm 3 resolution was performed.Clinical Newborns and Fetuses -Acquisitions of 8 neonates ([38.1, 39.4, 40.1, 40.4, 40.7, 40.9, 41.8, 42] weeks), were performed during natural sleep at 3T (Siemens Trio and Skyra). Five b0 images and 30 b = 1000 s/mm 2 were acquired. The TR-TE were 3700-104 ms and voxel size was 2 mm isotropic. Eight fetal subjects ( [24, 25, 26.3, 26.6, 26.7, 26.9, 29.4, 38.7] gestational weeks, GW) were scanned using a 3T Siemens Skyra MRI scanner (TR = 3000-4000 ms, TE = 60 ms) with one b0 and 12 diffusion-sensitized images at b = 500 s/mm 2 . All subjects were processed for noise [34] and bias field inhomogeneities [33]. Rigid registration to a T2 atlas [11] was performed and b-vectors were rotated accordingly for fetal data. The different volumes were upsampled to 1 mm 3 and normalized by b0. The studies were approved by the institutional review board committee."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.3,Training,"Two networks, DL n and DL f (see Subsect. 2.1 above), were trained using Adam optimizer [21] to minimize the 2 norm loss function between the predicted 45 SH coefficients and the ground truth FOD SH coefficients generated using the 300 directions and the 4 b-values ({0, 400, 1000, 2600} s/mm 2 ), i.e.We used 70% of the subjects for training, 15% for validation, and 15% for testing. We used the number of FOD peaks (extracted from Dipy [10]) to balance patch selection per batch. The central voxel of each patch was constrained to be in the generated white matter mask and to be 1 peak in 2  3 of the batch and more than one peak in 1  3 of the batch. This condition implicitly guarantees the non selection of empty patches. The patch size was empirically set to 16 3 voxels. The batch size was set to 27 for DL n and 9 for DL f , and the initial learning rate to 10 -4 and was decreased by 0.9 whenever the validation loss did not improve after one epoch. The total number of training epochs was 10000 and a dropout rate of 0.1 was used in all layers to reduce overfitting and improve generalization. In DL f , Gaussian noise injection (mean = 0, sigma = 0.025) was applied as well as small rotations (uniformly from [-5 • , +5 • ]) to make the model robust to minor uncorrected movements due to small differences in FOV and fetal head motion."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.4,Evaluation of dHCP Newborns,"Comparison with State-of-the-Art Methods -In addition to comparing our network (DL n ) prediction with FODs estimated using MSMT-CSD of 300 directions (considered as ground truth, GT), we have assessed the agreement between two mutually exclusive subsets extracted from the ground truth (gold standards 1 and 2, respectively GS1 and GS2). Each subset contains 150 directions (b ∈ {0, 400, 1000, 2600} s/mm 2 ) with respectively 10, 32, 44, and 64 measurements (half measurements of GT data). GS1 and GS2 subsets can be considered as independent high-quality scans, and differences in terms of subsequent metrics can be considered as an upper bound error for the different methods deployed. Furthermore, we have computed three state-of-the-art methods: (1) Constrained spherical deconvolution (CSD) [32] using the 128 gradient directions of the highest shell, i.e. b = 2600 s/mm 2 and 20 b0 images; (2) Constant Solid Angle ODF (Q-Ball) model [1] that we refer to as CSA and (3) the Sparse Fascicle Model (SFM) [27] model for which we have used the default regularization parameters. We also compared our method with the multilayer perceptron (MLP) in [18], which has been shown to outperform the method of [25].Error Metrics -Quantitative validation was performed based on the number of peaks, the angular error and the apparent fiber density (AFD) [26]. The number of peaks was generated from the FOD predicted by the network and the ones estimated by the different methods (GT, GS1, GS2, CSD, CSA and SFM) using the same parameters (mean separation angle of 45 • , a maximum number of 3 peaks and relative peak threshold of 0.5). The conservative choice of these parameters was guided by [28] which shows the limitations of current dMRI models at depicting multiple number of peaks and low angular crossing fibers. We have compared these models in terms of confusion matrices, and the agreement rate (AR). AR is defined for each number of peaks p as: AR = Ap ΣDp where A p is the percentage of voxels on which both methods agree on p number of peaks and D p the percentage of voxels where at least one of the two methods predicts p and the other p where p = p . For the voxels containing the same number of peaks, we have computed the angular error with respect to the GT, as well as between GS1 and GS2. For voxels with multiple fibers, we have first extracted corresponding peaks between the two methods by computing the minimum angle between all configurations (4 for 2 peaks and 9 for 3 peaks); we then removed these peaks and recursively apply the same algorithm. We have also compared AFD, that is defined as the FOD amplitude. AFD was extensively demonstrated as a biologically plausible measure that is not only sensitive to the fiber partial volume fraction but also to fiber density or membrane permeability [26]. Statistical validation using paired t-test corrected for multiple comparisons with Bonferroni method was performed between the errors of the different methods with respect to GT and the difference between GS1 and GS2."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.5,Evaluation of Clinical Datasets,"DL f was tested on fetal volumes whereas DL n was tested on the clinical newborn dataset. Due to the lack of ground truth for both clinical datasets, we qualitatively assess the network predictions with 12 and 6 measurements, as compared to CSD using all available measurements (SH-L max order 4 and 8), respectively for fetuses and newborns."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,3,Results,"The networks consistently learned a mapping between the six/twelve diffusion measurements and the ground truth FOD constructed with 300 measurements across 4 b-values, as evaluated on the independent test data (Figs. 2 and3)."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,3.1,In-Domain Quantitative Evaluation in Newborns dHCP,"Number of Peaks -We first observe a low agreement (AR) between the two gold standard acquisitions (GS1 vs. GS2), that is more pronounced for multiple fibers voxels. For instance, 1-peaks AR is 80.4%, 30.3% for 2-peaks and 27.9% for 3-peaks. SFM achieves a relatively high 1-peaks agreement with the GT of 83% and the lowest with multiple fibers voxels (10% and 3.5% for 2-and 3-peaks, respectively). In contrast, CSD estimates a high number of multiple fibers (16.5% and 5.9% for 1-and 2-peaks respectively) and achieves the lowest 1-peaks AR with 11.7%. In fact, the latter is biased towards multiple peaks estimation with more than 90% of the voxels modeled as either two or three peaks. This might be explained by the high b-value (b = 2600 s/mm 2 ) containing high levels of noise. Our method, DL n , achieves an agreement for 1-, 2-and 3-peaks of respectively 79%, 16% and 3% that is globally the closest to the agreement between the gold standards when compared to other methods. We believe that the relatively low agreement for multiple intravoxel fiber orientations is due to their incongruence across GT subjects, and hence the absence of a Table 1. Mean angular error, agreement rate on number of peaks and Apparent Fiber Density (AFD) error between GT (MSMT-CSD) and the different methods. ΔGS refers to GS1 and GS2 agreements. The number of measurements (Nm) and the b-values used are also reported. All results were statistically significant compared to ΔGS (p ≤9e -10  for angular error, except SFM three fibers, and p ≤4.5e -3 for AFD error). consistent pattern to be learned by the neural network. In fact, this is supported by the modest agreement between the two gold standards (ΔGS), in which both the subjects and the number of measurements are the same, only the gradient directions vary and already result in a drop of 70% in multiple fibers depiction."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Method b-values (s/mm,"It is worth noting that the agreement between the different methods (CSD vs. CSA, SFM vs. CSA, CSD vs. DL n , etc.) was also low. The confusion matrices for ΔGS agreement and the different methods can be found in Table 1 and the comparison with [18] in Section 3 of Supplementary materials.Angular Error -The agreement in terms of the number of peaks does not guarantee that the fibers follow the same orientation. Table 1 shows the angular error and the agreement rate (AR) in numbers of peaks for the different configurations. In GS1 and GS2, the angular difference increases almost linearly for one, two and three fibers. Our learning model achieves an error rate that is comparable (although statistically different, p ≤9e -10 ) to GS1 and GS2. SFM and CSA achieve a higher error rate for single and two fiber voxels, whereas CSD achieves the lowest. This is because of the low AR and hence the error is computed among a small subset of common voxels between the GT and CSD as shown in Table 1. It is worth mentioning that using 15 directions instead of 6 as input to the network did not improve the results; and in general, these angular errors are higher than those reported for adult data, such as the Human Connectome Project as in [13]. We hypothesize this can be due to immature and high variability of the developing brain anatomy.Apparent Fiber Density -The last column in Table 1 shows the differences between AFD averaged over the 15 test subjects. Our model achieves the closest error rate of 0.27 (±0.03) to the GT compared with the gold standards difference of 0.2 (±0.025), in terms of mean and standard deviation. The other methods have an increased error rate compared to DL n with factors of around 2.5, 4.5 and 9.5-fold for SFM, CSD and CSA respectively. Results were statistically significant (p ≤4.5e -3 ) compared to the agreement between the gold standard models. "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,3.2,Generalizability to Clinical Acquisitions (newborns and fetuses),"DL f successfully generalized to fetal data as can be seen in Fig. 3 (right) for two subjects. Callossal fibers are clearly delineated on the top and bottom subjects.The radial coherence of cortical plate at early gestation [20] is also highlighted on the same panels. Similarly, DL n generalized to the new newborn dataset (Fig. 3, left), despite differences in scanner and protocol. Both cortico-spinal tract and corpus callosum are shown in the bottom subject. As opposed to CSD that overestimated false positive crossing fibers, likely due to residual noise, the deep learning method trained on MSMT-CSD directly produced low amplitude FODs in isotropic or non-consistent regions. The results for six other subjects can be found in Figure S2 in Supplementary materials. "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,4,Conclusion,"We have demonstrated how a deep neural network can successfully reconstruct high angular multi-shell FODs from a reduced number (6 to 12) of diffusion measurements. The substantially lower number of samples is compensated by learning from high-quality training data and by exploiting the spatial neighborhood information. The network was quantitatively evaluated on the dHCP dataset which was acquired in a highly controlled setting that cannot be reproduced in clinical settings. We showed that our method relying on six measurements can be leveraged to reconstruct plausible FODs of clinical newborn and fetal brains. We compared our model to commonly used methods such as CSD and MSMT-CSD between two gold standard datasets. The results exhibit low agreements between the different methods, particularly for multiple fiber orientations, despite using high angular multi-shell data. This highlights the need to build robust and reproducible methods for microstructure estimation in developing brains."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Fig. 1 .,
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Fig. 2 .,
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Fig. 3 .,
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 28.
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_48.
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,1,Introduction,"Esophageal cancer is a significant contributor to cancer-related deaths globally [3,15]. One effective treatment option is radiotherapy (RT), which utilizes high-energy radiation to target cancerous cells [4]. To ensure optimal treatment outcomes, both the cancerous region and the adjacent organ-at-risk (OAR) must be accurately delineated, to focus the high-energy radiation solely on the cancerous area while protecting the OARs from any harm. Gross tumor volume (GTV) represents the area of the tumor that can be identified with a high degree of certainty and is of paramount importance in clinical practice.In the clinical setting, patients may undergo a second round of RT treatment to achieve complete tumor control when initial treatment fails to completely eradicate cancer [16]. However, the precise delineation of the GTV is laborintensive, and is restricted to specialized hospitals with highly skilled RT experts. The automatic identification of the esophagus presents inherent challenges due to its elongated soft structure and ambiguous boundaries between it and adjacent organs [12]. Moreover, the automatic delineation of the GTV in the esophagus poses a significant difficulty, primarily attributable to the low contrast between the esophageal GTV and the neighboring tissue, as well as the limited datasets.Recently, advances in deep learning [21] have promoted research in automatic esophageal GTV segmentation from computed tomography (CT) [18,19]. Since the task is challenging, Jin et al. [9,10] improve the segmentation accuracy by incorporating additional information from paired positron emission tomography (PET). Nevertheless, such approaches require several imaging modalities, which can be both costly and time-consuming, while disregarding any knowledge from previous treatment or anatomical understanding. Moreover, the correlation between the first and second courses of RT is rarely investigated, where detailed prior tumor information naturally exists in the previous RT planning.In this paper, we present a comprehensive study on accurate GTV delineation for the second course RT. We proposed a novel prior Anatomy and RT information enhanced Second-course Esophageal GTV segmentation network (ARTSEG). A region-preserving attention module (RAM) is designed to effectively capture the long-range prior knowledge in the esophageal structure, while preserving regional tumor patterns. To the best of our knowledge, we are the first to reveal the domain gap between the first and second courses for GTV segmentation, and explicitly leverage prior information from the first course to improve GTV segmentation performance in the second course.The medical images are labeled sparsely, which are isolated by different tasks [20]. Meanwhile, an ideal method for automatic esophageal GTV segmentation in the second course of RT should consider three key aspects: 1) Changes in tumor volume after the first course of RT, 2) The proliferation of cancerous cells from a tumor to neighboring healthy cells, and 3) The anatomical-dependent Our training approach leverages multi-center datasets containing relevant annotations, that challenges the network to retrieve information from E1 using the features from E2. The decoder D utilizes the prior knowledge obtained from I1 and G1 to generate the mask prediction. Our training strategy leverages three datasets that introduce prior knowledge to the network of the following three key aspects: 1) Tumor volume variation, 2) Cancer cell proliferation, and 3) Reliance of GTV on esophageal anatomy.nature of GTV on esophageal locations. To achieve this, we efficiently exploit knowledge from multi-center datasets that are not tailored for second-course GTV segmentation. Our training strategy does not specific to any tasks but challenges the network to retrieve information from another encoder with augmented inputs, which enables the network to learn from the above three aspects. Extensive quantitative and qualitative experiments validate our designs."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,2,Network Architecture,"In the first course of RT, a CT image denoted as I 1 is utilized to manually delineate the esophageal GTV, G 1 . During the second course of RT, a CT image I 2 of the same patient is acquired. However, I 2 is not aligned with I 1 due to soft tissue movement and changes in tumor volume that occurred during the first course of treatment. Both images I 1/2 have the spatial shape of H × W × D.Our objective is to predict the esophageal GTV G 2 of the second course. It would be advantageous to leverage insights from the first course, as it comprises comprehensive information pertaining to the tumor in its preceding phase. Therefore, the input to encoder E 1 consists of the concatenation of I 1 and G 1 to encode the prior information (features f d 1 ) from the first course, while encoder E 2 embeds both low-and high-level features f d 2 of the local pattern of I 2 (Fig. 1),where the spatial shape of, with 2 d+4 channels. Region-Preserving Attention Module. To effectively learn the prior knowledge in the elongated esophagus, we design a region-preserving attention module (RAM), as shown in Fig. 1. The multi-head attention (MHA) [17] is employed to gather long-range informative values in f d 1 with f d 2 as queries and f d 1 as keys. The features f d 1/2 are reshaped to HW D 2 3d × C before passed to the MHA, where C is the channel dimension. The attentive features f d A can be formulated as:Since MHA perturbs the positional information, we preserve the tumor local patterns by concatenating original features to the attentive features at the channel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ 1×1×1 to squeeze the channel features (named as RAM), as shown in the following equations,where the lower-level features from both encoders are fused by concatenation.The decoder D generates a probabilistic prediction) with skip connections (Fig. 1). We utilize the 3D Dice [14] loss function,"
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3,Training Strategy,"The network should learn from three aspects: 1) Tumor volume variation: the structural changes of the tumor from the first to the second course; 2) Cancer cell proliferation: The tumor in esophageal cancer tends to infiltrate into the adjacent tissue; 3) Reliance of GTV on esophageal anatomy: The anatomical dependency between esophageal GTV and the position of the esophagus. Medical images are sparsely labeled which are isolated by different tasks [20], and are often inadequate. In this study, we use a paired first-second course GTV dataset S p , an unpaired GTV dataset S v , and a public esophagus dataset S e .In order to fully leverage both public and private datasets, the training objective should not be specific to any tasks. Here, we denote G 1 /G 2 as prior/target annotations respectively, which are not limited only to the GTV areas. As shown in Fig. 1, our strategy is to challenge the network to retrieve information from augmented inputs in E 1 using the features from E 2 , which can incorporate a wide range of datasets that are not tailored for second-course GTV segmentation."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.1,Tumor Volume Variation,"The differences in tumor volume between the first and second courses following an RT treatment can have a negative impact on the state-of-the-art (SOTA) learning-based techniques, which will be discussed in Sect. 4.2. To adequately monitor changes in tumor volume and integrate information from the initial course into the subsequent course, a paired first-second courses dataset S p = {i 1  p , i 2 p , g 1 p ; g 2 p } is necessary for training. In S p , i 1 p and i 2 p are the first and second course CT images, while g 1 p and g 2 p are the corresponding GTV annotations."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.2,Cancer Cell Proliferation,"The paired dataset S p for the first and second courses is limited, whereas an unpaired GTV dataset S v = {i v ; g v } can be easily obtained in a standard clinical workflow with a substantial amount. S v lacks its counterpart for the second course, in which i v /g v are the CT image and the corresponding annotation for GTV. To address this, we apply two distinct randomized augmentations, P 1 , P 2 , to mimic the unregistered issue of the first and second course CT. The transformed data is feed into the encoders E 1/2 as shown in the following equations:, P 1 (g e ), P 2 (i e ), P 2 (g e ), when i e , g e ∈ S e .(4)The esophageal tumor can proliferate with varying morphologies into the surrounding tissues. Although not paired, S v contains valuable information about the tumor. Challenging the network to query information within GTV will enhance the capacity to retrieve pertinent information for the tumor positions."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.3,Reliance of GTV on Esophageal Anatomy,"To make full use of the datasets of relevant tasks, we incorporate a public esophagus segmentation dataset, denoted as S e = {i e ; g e }, where i e /g e represent the CT images and corresponding annotations of the esophagus structure. By augmenting the data as described in Eq. ( 4), S e challenges the network to extract information from the entire esophagus, which enhances the network's embedding space with anatomical prior knowledge of the esophagus. Similarly, data from the paired S p is also augmented by P 1/2 to increase the network's robustness.In summary, our training strategy is not dataset-specific or target-specific, thus allowing the integration of prior knowledge from multi-center esophageal GTV-related datasets, which effectively improves the network's ability to retrieve information for the second course from the three key aspects stated in Sect. 3."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4,Experiments,
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.1,Experimental Setup,"Datasets. The paired first-second course dataset, S p , is collected from Sun Yat-Sen University Cancer Center (Ethics Approval Number: B2023-107-01), comprising paired CT scans of 69 distinct patients from South China. We collected the GTV dataset S v from MedMind Technology Co., Ltd., which has CT scans from 179 patients. For both S p and S v , physicians annotated the esophageal cancer GTV in each CT. The GTV volume statistics (cm 3 , mean ± std.) in S v is 40.60 ± 29.75, and is 83.70 ± 55.97/71.66 ± 49.36 for the first/second course RT in S p respectively. Additionally, we collect S e from SegTHOR [12], consisting of CT scans and esophagus annotations from 40 patients who did not  Implementation Details. The CT volumes from the first and second course in S p are aligned based on the center of the lung mask [8]. The CT volumes are applied with a windowing of [-100, 300] HU, and resampled to 128 3 , with a voxel size of 1.2 × 1.2 × 3 mm 3 . The augmentations P 1/2 involve a combination of random 3D resized cropping, flipping, rotation in the transverse plane, and Gaussian noise. We employ the Adam [11] optimizer with (β 1 , β 2 , lr) = (0.9, 0.999, 0.001) for training for 500 epoches. The network is implemented using PyTorch [2] and MONAI [1], and detailed configurations are in the supplementary material. Experiments are performed on an NVIDIA RTX 3090 GPU with 24GB memory.Performance Metrics. Dice score (DSC), averaged surface distance (ASD) and Hausdorff distance (HSD) are used as metrics for evaluation. The Wilcoxon signed-rank test is used to compare the performance of different methods."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.2,Domain Gap Between the First and Second Course,"As previously mentioned, the volume of the tumors changes after the first course of RT. To demonstrate the presence of a domain gap between the first and second courses, we train SOTA methods with datasets S train p and S v , by feeding the data sequentially into the network. We then evaluate the models on S test p . The results presented in Table 1 indicate a performance gap between GTV segmentation in the first and second courses, with the latter being more challenging. Notably, the paired first-second course dataset S test p pertains to the same group of patients, thereby ensuring that any performance drop can be attributed solely to differences in courses of RT, rather than variations across different patients.Figure 2 illustrates the reduction in the GTV area after the initial course of RT, where the transverse plane is taken from the same location relative to the vertebrae (yellow lines). The blue arrows indicate that the networks failed to track these changes and produced false predictions in the second course of RT. This suggests that deep learning-based approaches may not rely solely on the identification of malignant tissue patterns, as doctors do, but rather predict highrisk areas statistically. Therefore, for accurate second-course GTV segmentation, we need to explicitly propagate prior information from the first course using dual encoders in ARTSEG, and incorporate learning about tumor changes."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.3,Evaluations of Second-Course GTV Segmentation Performance,"Combination of Various Datasets. Table 2 presents the information gain derived from multi-center datasets using quantified metrics for segmentation performance. We first utilize a standard ARTSEG (w/o RAM) as an ablation network. When prior information from the first course is explicitly introduced using S p , ARTSEG outperforms other baselines for GTV segmentation in the second course, which reaches a DSC of 66.73%. However, in Fig. 3, it can be observed that the model failed to accurately track the GTV area along the esophagus (orange arrows) due to the soft and elongated nature of the esophageal tissue, which deforms easily during CT scans performed at different times.By subsequently incorporating S e for structural esophagus prior knowledge, the DSC improved to 69.42%. Meanwhile, the esophageal tumor comprises two primary regions, the original part located in the esophagus and the extended part that has invaded the surrounding tissue. As shown in Fig. 3, identifying the tumor proliferation into the surrounding tissue without comprehensive knowledge of tumor morphology can be challenging (blue arrows). To address this, incorporating S v to comprehensively learn the tumor morphology is required.When S v is incorporated for learning tumor proliferation, the DSC improved to 72.64%. We can observe from Case 2 in Fig. 3 that the network has a better understanding of the tumor proliferation with S v , while it still fails to track the GTV area along the esophagus as pointed by the orange arrow. Therefore, S v and S e improve the network from two distinct aspects and are both valuable. Our proposed training strategy fully exploits the datasets S p , S v , and S e , and  further improve the DSC to 74.54% by utilizing comprehensive knowledge of both the tumor morphology and esophageal structures.Region-Preserving Attention Module. Although introducing the esophageal structural prior knowledge using S e can improve the performance in DSC and ASD (Table 2), the increase in HSD (38.22 to 47.89 mm; 21.71 to 27.00 mm) indicates that there are outliers far from the ground truth boundaries. This may be attributed to the convolution that cannot effectively handle the long-range knowledge of the esophagus structure. The attention mechanism can effectively capture the long-range relationship as shown recently in [13].However, there is no performance gain with MHA as shown in Table 2, and the HSD further increased to 27.33 mm. We attribute the drawback is due to the location-agnostic nature of the operations in MHA, where the local regional correlations are perturbed.To tackle the aforementioned problem, we propose RAM which involves the concatenation of the original features with attention outputs, allowing for the preservation of convolution-generated regional tumor patterns while effectively comprehending long-range prior knowledge specific to the esophagus. Finally, our proposed ARTSEG with RAM achieves the best DSC/HSD of 75.26%/19.75 mm, and outperforms its ablations as well as other baselines, as shown in Table 2.Limitations. For the method's generalizability, analysis of diverse imaging protocols and segmentation backbones are inadequate. Besides, ARTSEG requires more computational resources due to its dual-encoder and attention design."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,5,Conclusion,"In this paper, we reveal the domain gap between the first and second courses of RT for esophageal GTV segmentation. To improve the accuracy of GTV declination in the second course, we explicitly incorporated the naturally existing prior information from the first course. Besides, to efficiently leverage prior knowledge contained in various medical CT datasets, we train the network in an information-querying manner. We proposed RAM to capture long-range prior knowledge in the esophageal structure, while preserving the regional tumor patterns. Our proposed ARTSEG incorporates prior knowledge of the tumor volume variation, cancer cell proliferation, and reliance of GTV on esophageal anatomy, which enhances the GTV segmentation accuracy in the second course RT. Our future research includes accurate delineation for multiple targets in the second course and knowledge transferring through the time series of multiple courses."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Fig. 1 .,
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Fig. 2 .,
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Fig. 3 .,
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Table 1 .,ASD (mm) ↓ DSC (%) ↑ ASD (mm) ↓ mean ± std. med. mean ± std. med. mean ± std. med. mean ± std. med. UNETR [7] 59.77 ± 20.24 62.90 10.57 ± 14.66 7.06 53.03 ± 17.62* 55.17 11.29 ± 11.44 8.42 Swin UNETR [6] 60.84 ± 19.74 64.07 10.29 ± 17.78 6.67 57.04 ± 20.16* 60.73 9.76 ± 15.43 6.21 DenseUnet [19] 63.95 ± 18.23 68.11 8.94 ± 13.82 6.04 55.35 ± 18.59* 58.54 9.84 ± 6.91* 8.54 3D U-Net [5] 66.73 ± 17.21 69.86 8.04 ± 16.83 4.19 57.50 ± 19.49* 62.62 9.14 ± 12.03* 6.09
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Table 2 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,1,Introduction,"Lung cancer is one of the most fatal diseases worldwide, and early diagnosis of the pulmonary nodule has been identified as an effective measure to prevent lung cancer. Deep learning-based methods for lung nodule classification have been widely studied in recent years [9,12]. Usually, the malignancy prediction is often formulated as benign-malignant binary classification [9,10,19], and the higher classification performance and explainable attention maps are impressive. Most previous works employ a learning paradigm that utilizes cross-entropy loss between predicted probability distributions and ground-truth one-hot labels. Furthermore, inspired by ordered labels of nodule progression, researchers have turned their attention to ordinal regression methods to evaluate the benignunsure-malignant classification task [2,11,13,18,21], where the training set additionally includes nodules with uncertain labels. Indeed, the ordinal regressionbased methods are able to learn ordered manifolds and to further enhance the prediction accuracy.However, the aforementioned methods still face challenges in distinguishing visually similar samples with adjacent rank labels. For example, in Fig. 1(a), since we conduct unimodal contrastive learning and map the samples onto a spherical space, the false positive nodule with a malignancy score of 2.75 has a closer distance to that with a score of 4.75, and the false negative one should not be closer to that of score 2.5. To address this issue, we found that the text attributes, such as ""subtlety"", ""sphericity"", ""margin"", and ""lobulation"", annotated by radiologists, can exhibit the differences between these hard samples. Therefore, we propose leveraging text annotations to guide the learning of visual features. In practice, this also aligns with the fact that the annotated text information represents the direct justification for identifying lesion regions in the clinic. As shown in Fig. 1, this text information is beneficial for distinguishing visually similar pairs, while we conduct this behavior by applying contrastive learning that pulls semantic-closer samples and pushes away semantic-farther ones.To integrate text annotations into the image-domain learning process, an effective text encoder providing accurate textual features is required. Fortunately, recent advances in vision-language models, such as contrastive languageimage pre-training (CLIP) [16], provide us with a powerful text encoder pre-trained with text-based supervisions and have shown impressive results in downstream vision tasks. Nevertheless, it is ineffective to directly transfer CLIP to medical tasks due to the data covariate shift. Therefore, in this paper, we pro- pose CLIP-Lung, a framework to classify lung nodules using image-text pairs. Specifically, CLIP-Lung constructs learnable text descriptions for each nodule from both class and attribute perspectives. Inspired by CoCoOp [20], we propose a channel-wise conditional prompt (CCP) module to allow nodule descriptions to guide the generation of informative feature maps. Different from CoCoOp, CCP constructs specific learnable prompts conditioned on grouped feature maps and triggers more explainable attention maps such as Grad-CAM [17], whereas CoCoOp provides only the common condition for all the prompt tokens. Then, we design a textual knowledge-guided contrastive learning based on obtained image features and textual features involving classes and attributes. Experimental results on LIDC-IDRI [1] dataset demonstrate the effectiveness of learning with textual knowledge for improving lung nodule malignancy prediction.The contributions of this paper are summarized as follows.1) We propose CLIP-Lung for lung nodule malignancy prediction, which leverages clinical textual knowledge to enhance the image encoder and classifier. 2) We design a channel-wise conditional prompt module to establish consistent relationships among the correlated text tokens and feature maps. 3) We simultaneously align the image features with class and attribute features through contrastive learning while generating more explainable attention maps. "
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,2,Methodology,"m=1 is the set of attribute embeddings, where each element a m ∈ R d×1 is a vector representing the embedding of an attribute word such as ""spiculation"". Then, for a given sample {I i , y i }, our aim is to learn a mapping f θ : I i → y i , where f is a deep neural network parameterized by θ. CLIP-Lung. In Fig. 2(a), the training framework contains an image encoder f θ and a text encoder g φ . First, the input image I i is fed into f θ and then generates the feature maps. According to Fig. 2(b), the feature maps are converted to channel-wise feature vectors f θ (I i ) = F t,: and then to learnable tokens l t . Second, we initialize the context tokens l t and add them with l t to construct the learnable prompts, where T is the number of context words. Next, the concatenation of the class token and l t + l t is used as input of text encoder yielding the class features g φ (c k ) = C k,: , note that C k,: is conditioned on channel-wise feature vectors F t,: . Finally, the attribute tokens a m are also fed into the text encoder to yield corresponding attribute features g φ (a m ) = A m,: . Note that the vectors F t,: , l t,: , l t,: , and C k,: are with the same dimension d = 512 in this paper. Consequently, we have image feature F ∈ R T ×d , class feature C ∈ R K×d , and attribute feature A ∈ R M ×d to conduct the textual knowledge-guided contrastive learning."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,2.2,Instance-Specific Attribute Weighting,"For the attribute annotations, all the lung nodules in the LIDC-IDRI dataset are annotated with the same eight attributes: ""subtlety"", ""internal structure"", ""calcification"", ""sphericity"", ""margin"", ""lobulation"", ""spiculation"", and ""texture"" [4,8], and the annotated value for each attribute ranges from 1 to 5 except for ""calcification"" that is ranged from 1 to 6. In this paper, we fix the parameters of a pre-trained text encoder so that the generated eight text feature vectors are the same for all the nodules. Therefore, we propose an instance-specific attribute weighting scheme to distinguish different nodules. For the i-th sample, the weight for each a m is calculated through normalizing the annotated values:where v m denotes the annotated value for a m . Then the weight vectors of the i-th sample is represented as. Hence, the elementwise multiplication w i • A i is unique to I i ."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,2.3,Channel-Wise Conditional Prompt,"CoCoOp [20] firstly proposed to learn language contexts for vision-language models conditioned on visual features. However, it is inferior to align context words with partial regions of the lesion. Therefore, we propose a channel-wise conditional prompt (CCP) module, in Fig. 2(b), to split latent feature maps into T groups and then flatten them into vectors F t,: . Next, we denote h(•) as a context network that is composed of a multi-layer perceptron (MLP) with one hidden layer, and each learnable context token is now obtained by l t = h(F t,: ). Hence, the conditional prompt for the t-th token is l t +l t . In addition, CCP also outputs the F t,: for image-class and image-attribute contrastive learning."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,2.4,Textual Knowledge-Guided Contrastive Learning,"Recall that our aim is to enable the visual features to be similar to the textual features of the annotated classes or attributes and be dissimilar to those of irrelevant text annotations. Consequently, we accomplish this goal through contrastive learning [3,5,7]. In this paper, we conduct such image-text contrastive learning by utilizing pre-trained CLIP text encoder [16]. In Fig. 2, we align F ∈ R T ×d with C ∈ R K×d and A ∈ R M ×d , i.e., using class and attribute knowledge to regularize the feature maps.Image-Class Alignment. First, the same to CLIP, we align the image and class information by minimizing the cross-entropy (CE) loss for the sample {I i , y i }:whereand "" "" denotes concatenation, i.e., C k,: is conditioned on learnable prompts l t + l t . σ(•, •) calculates the cosine similarity and τ is the temperature term. Therefore, L IC implements the contrastive learning between channel-wise features and corresponding class features, i.e., the ensemble of grouped image-class alignment results. Image-Attribute Alignment. In addition to image-class alignment, we further expect the image features to correlate with specific attributes. So we conduct image-attribute alignment by minimizing the InfoNCE loss [5,16]:Hence, L IA indicates which attribute the F t,: is closest to since each vector F t,: is mapped from the t-th group of feature maps through the context network h(•). Therefore, certain feature maps can be guided by specific annotated attributes. Class-Attribute Alignment. Although the image features have been aligned with classes and attributes, the class embeddings obtained by the pre-trained CLIP encoder may shift in the latent space, which may result in inconsistent class space and attribute space, i.e., annotated attributes do not match the corresponding classes, which is contradictory to the actual clinical diagnosis. To avoid this weakness, we further align the class and attribute features:and this loss implies semantic consistency between classes and attributes.Finally, the total loss function is defined as follows:where α and β are hyperparameters for adjusting the losses and are set as 1 and 0.5, respectively. L CE denotes the cross-entropy loss between predicted probabilities obtained by the classifier and the ground-truth labels. Note that during the inference phase, test images are only fed into the trained image encoder and classifier. As a result, CLIP-Lung does not introduce any additional computational overhead in inference."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,3,Experiments,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,3.1,Dataset and Implementation Details,"Dataset. LIDC-IDRI [1] is a dataset for pulmonary nodule classification or detection based on low-dose CT, which involves 1,010 patients. According to the annotations, we extracted 2, 026 nodules, and all of them were labeled with scores from 1 to 5, indicating the malignancy progression. We cropped all the nodules with a square shape of a doubled equivalent diameter at the annotated center, then resized them to the volume of 32 × 32 × 32. Following [9,11], we modified the first layer of the image encoder to be with 32 channels. According to existing works [11,18], we regard a nodule with an average score between 2.5 and 3.5 as unsure nodules, benign and malignant categories are those with scores lower than 2.5 and larger than 3.5, respectively. In this paper, we construct three sub-datasets: LIDC-A contains three classes of nodules both in training and test sets; according to [11], we construct the LIDC-B, which contains three classes of nodules only in the training set, and the test set contains benign and malignant nodules; LIDC-C includes benign and malignant nodules both in training and test sets.Experimental Settings. In this paper, we apply the CLIP pre-trained ViT-B/16 as the text encoder for CLIP-Lung, and the image encoder we used is ResNet-18 [6] due to the relatively smaller scale of training data. The image encoder is initialized randomly. Note that for the text branch, we froze the parameters of the text encoder and updated the learnable tokens l and l during training. The learning rate is 0.001 following the cosine decay, while the optimizer is stochastic gradient descent with momentum 0.9 and weight decay 0.00005. The temperature τ is initialized as 0.07 and updated during training. All of our experiments are implemented with PyTorch [15] and trained with NVIDIA A100 GPUs. The experimental results are reported with average values through five-fold cross-validation. We report the recall and F1-score values for different classes and use ""±"" to indicate standard deviation."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,3.2,Experimental Results and Analysis,"Performance Comparisons. In Table 1, we compare the classification performances on the LIDC-A dataset, where we regard the benign-unsure-malignant We argue that this is due to the indistinguishable textual annotations, such as similar attributes of different nodules. In addition, we verify the effect of textual branch of CLIP-Lung using MV-DAR [12] on LIDC-A dataset. The obtained accuracy values with and without the textual branch are 58.9% and 57.3%, respectively, demonstrating the effectiveness of integrating textual knowledge. Table 2 presents a performance comparison of CLIP-Lung on the LIDC-B and LIDC-C datasets. Notably, CLIP-Lung obtains higher evaluation values other than recalls of benign class. This disparity is likely attributed to the similarity in appearances and subtle variations in text attributes among the benign nodules. Consequently, aligning these distinct feature types becomes challenging, resulting in a bias towards the text features associated with malignant nodules.  Visual Features and Attention Maps. To illustrate the influence of incorporating class and attribute knowledge, we provide the t-SNE [14] and Grad-CAM [17] results obtained by CLIP, CoCoOp, and CLIP-Lung. In Fig. 3, we can see that CLIP yields a non-compact latent space for two kinds of nodules.CoCoOp and CLIP-Lung alleviate this phenomenon, which demonstrates that the learnable prompts guided by nodule classes are more effective than fixed prompt engineering. Unlike CLIP-Lung, CoCoOp does not incorporate attribute information in prompt learning, leading to increased false negatives in the latent space. From the attention maps, we can observe that CLIP cannot precisely capture spiculation and lobulation regions that are highly correlated with malignancy. Simultaneously, our CLIP-Lung performs better than CoCoOp, which demonstrates the guidance from textual descriptions such as ""spiculation"".Ablation Studies. In Table 3, we verify the effectiveness of different loss components on the three constructed datasets. Based on L IC , L IA and L CA improve the performances on LIDC-A, indicating the effectiveness of capturing fine-grained features of ordinal ranks using class and attribute texts. However, they perform relatively worse on LIDC-B and LIDC-C, especially the L IC + L CA . That is to say, L IA is more important in latent space rectification, i.e., image-attribute consistency. In addition, we observe that L IC +L IA performs better than L IA +L CA , which is attributed to that L CA regularizes the image features indirectly."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,4,Conclusion,"In this paper, we proposed a textual knowledge-guided framework for pulmonary nodule classification, named CLIP-Lung. We explored the utilization of clinical textual annotations based on large-scale pre-trained text encoders. CLIP-Lung aligned the different modalities of features generated from nodule classes, attributes, and images through contrastive learning. Most importantly, CLIP-Lung establishes correlations between learnable prompt tokens and feature maps using the proposed CCP module, and this guarantees explainable attention maps localizing fine-grained clinical features. Finally, CLIP-Lung outperforms compared methods, including CLIP on LIDC-IDRI benchmark. Future work will focus on extending CLIP-Lung with more diverse textual knowledge."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Fig. 1 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Fig. 2 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Fig. 3 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,2.1 Overview Problem Formulation. In,"i=1 is the corresponding class label set and y i ∈ {1, 2, . . . , K}, and K is the number of"
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Table 1 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Table 2 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Table 3 .,LIC LIA LCA LIDC-A LIDC-B LIDC-C 56.8 ± 0.6 86.8 ± 0.7 88.2 ± 0.6 59.4 ± 0.4 86.8 ± 0.6 86.7 ± 0.4 58.1 ± 0.2 85.7 ± 0.6 87.5 ± 0.5 56.9 ± 0.3 84.7 ± 0.4 84.0 ± 0.7 60.9 ± 0.4 87.5 ± 0.5 89.5 ± 0.4
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Acknowledgements. This work was supported in part by,"Natural Science Foundation of Shanghai (No. 21ZR1403600), National Natural Science Foundation of China (Nos. 62101136 and 62176059), China Postdoctoral Science Foundation (No. 2022TQ0069), Shanghai Municipal of Science and Technology Project (No. 20JC1419500), and Shanghai Center for Brain Science and Brain-inspired Technology."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,1,Introduction,"Atrial fibrillation (AF) has been one of the most common types of cardiovascular diseases and is closely related to the left atrium (LA) [18]. Beside this chamber structure, there is a finer anatomy termed with the left atrial appendage (LAA). The majority of strokes due to AF result from clots existing in the LAA [13]. A common measure for treatment is anticoagulant therapy. However, many patients have contraindications to this type of therapy. A more effective and feasible stroke prevention procedure is the left atrial appendage closure, which can avoid most of the drawbacks by anticoagulant therapy [12]. And the size of occlusion devices designed for patients is strongly associated with the anatomical interface between the LA and LAA [13]. Thus, enhancing the understanding for the structure of the LA and LAA is beneficial to carry out treatments for strokes due to AF. A normal pre-surgery imaging is Cardiac Computed Tomography (Cardiac CT), which is a popular physical inspection for diagnoses [18]. Thus, automatic and accurate segmentation of the LA and LAA from Cardiac CT images is essential to provide support for the diagnosis and treatment of various cardiovascular diseases.Till now, many researches have focused on the automatic segmentation of the LA [24]. Compared with that, the LAA has not been sufficiently researched, particularly the relative relations between the LA and LAA [27]. In our work, we aim to design an automatic method for the correlation modeling between the LA and LAA, then give a quantitative and qualitative evaluation of segmentation performance. The LA and LAA both have large anatomical variations [9,27]. Besides, there are uncertain boundaries for the structure of the LAA, especially the interface between the LA and LAA. In contrast, cardiac tissues like the right atrium (RA), right ventricle (RV) and left ventricle (LV) have explicit boundaries, which can be easily and finely segmented [28]. As shown in Fig. 1, some cases show poor segmentation results on the uncertain boundary between the LA and LAA. There are many related works on the refinement for boundary segmentation, which can be grouped into three categories. The first strategy attempts to exert a strong loss constraint on boundaries via the multitask learning paradigm [4]. Then some researchers apply a complex post-process to the segmentation of coarse boundaries, such as [26]. All the methods above mainly emphasize on refining predicted masks for high-quality images with clear boundaries, are not applicable for ambiguous or unclear boundaries [23] in the LA and LAA segmentation. Instead, the third strategy truly works, with the mechanism of enhancing deep features representing uncertain boundaries. Lee et al. [11] proposed a novel boundary-preserving block (BPB) with the ground-truth structure information indicated by experts. Xie et al. [23] used the confidence map to evaluate the uncertainty of each pixel to enhance the segmentation of ambiguous boundaries. Furthermore, some loss functions [10,25] are specifically designed for the segmentation of uncertain boundaries.Diffusion is a physical model aimed at minimizing the spatial concentration difference [15] and is widely used in computer vision [21,22]. In our work, we detailedly explain the process of refining uncertain boundaries based on diffusion theory. Then we propose a semantic guidance module based on differential operators to refine features from ambiguous boundaries, which is called semantic difference module (SDM). Here we introduce semantic information from deeper layers to guide the diffusion process. As a result of fuzzy boundaries of the LAA region and CT imaging noise, there exists a disconnection between the LA and LAA as shown in Fig. 1. Thus, we design another connectivity-refined network (CRN) combined with the connectivity loss, to deal with the connectivity of two regions. The contributions of our work are listed as follows:(1) We introduce a new LA & LAA CT dataset. And as far as we are concerned, this is the first work based on deep neural networks, to model relative relations between the LA and LAA. (2) We propose a novel semantic difference module based on diffusion theory to deal with the segmentation of uncertain boundaries. (3) We apply a connectivity-refined network with the connectivity loss to refine coarse masks, then achieve the connectivity between the LA and LAA. (4) Our proposed network achieves state-of-the-art segmentation performance on the LA and LAA. Specifically, SDM outperforms other methods related to refining the segmentation of uncertain boundaries."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,2,Methodology,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,2.1,Preliminaries,"Diffusion is a physical phenomenon, in which molecules spread from regions with higher concentrations toward regions with lower concentrations [15,16]. Then the whole system tends to be balanced. For a feature vector F to be smoothed, the diffusion process can be modeled as the following partial differential equation:where D is the diffusivity function determining the diffusion speed along each direction, ∇ is the gradient operator. In our application, the stable state of F (t) will show a more accurate localization for uncertain boundaries of the LAA. Linear isotropic diffusion (D is equal to a constant) cannot be applied to complex scenes, because the diffusion velocity is the same in all directions. For a spatial-dependent function D = D(x, y, z), the process is linear anisotropic. However, if we aim to extract refined boundary features, adopting linear diffusion processes will smooth both backgrounds and the edges. A more feasible solution is to devise complex diffusion functions D = D(F ) with nonlinear characteristics [21,22]. As a result, the diffusion process exerts more smoothing to regions parallel to boundaries compared to regions vertical to these edges.Detailedly, given a feature F where regions of uncertain boundaries are not highlighted, it is updated by the diffusion process in infinite time. The diffusion adjacent to ambiguous boundaries should be restrained, while the diffusion far away from boundaries is promoted. And the final state of the diffused feature will accurately localize uncertain boundaries of the LAA. "
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,2.2,Semantic Difference Module,"To localize fuzzy boundaries of the LAA, especially the interface between the LA and LAA, we propose the semantic difference module (SDM) to refine boundary features from these regions. Motivated by the diffusion process, we formulate the process of enhancing boundary features as solving a second-order partial differential equation. Due to the fact that semantic information is required to guide the localization of uncertain boundaries, we introduce the deep feature G from the precedent decoder layer to the diffusion process in each SDM.Here we adopt ∇G as the semantic guidance map. And the square term h(|∇G| 2 ) is deployed as function D to model nonlinear characteristics of the diffusion process, where h is a projection function. In terms of [15], Eq. 1 can be approximately solved via iterative updates as depicted by the following equations:where p is the index of feature maps, δ p is the local neighborhood centered at p, λ and ν are weighting coefficients. Indeed, F t p -F t p is the differential information of original feature F t at point p, representing abundant boundary information, which contains complicated boundary features of anatomies as shown in Fig. 2.c, including the RA RV, etc. However, predicted boundaries between the LA and LAA are not accurate enough only with the diffusion process. Thus, the semantic difference guidance |G p -G p | 2 is introduced to generate refined boundary feature F t+1 . F t+1 will diffuse into the stable state as t increases, which can highlight boundaries between the LA and LAA, and suppress the activation on other boundary regions. And refined feature F t+1 is attained by fusing the original feature F t with enhanced boundary feature F t+1 .We design the semantic difference module based on Eq. 3 as illustrated by Fig. 2. Here we make an improvement on the calculation of differential maps. Motivated by the fact that there exists an anisotropic distribution for our LA and LAA dataset in x, y and z dimensions, traditional edge operator cannot finely extract the differential map of feature F . Therefore, we propose a learnable boundary operator, which bears different values in each position of the kernel. As shown in Fig. 2, we fix the center value as -1 to maintain the difference attribute of the edge kernel. The revised description of enhanced boundary feature is calculated by Eq. 4.where α p and β p refer to the learnable edge operator for feature F and semantic feature G respectively. And ω p means a vanilla 3 × 3 × 3 convolution kernel."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,2.3,Connectivity-Refined Network,"Due to the CT imaging noise, some cases show the phenomenon that the LAA is separated from the LA. To deal with disconnections between the LA and LAA, we propose the second-stage network called connectivity-refined network (CRN).Inspired by the metric of 95% Hausdorff distance (HD 95 ) [7], we figure out that a poor connectivity between the LA and LAA will bring a large HD 95 value for the LAA segmentation, which is not what we expected. Therefore, we adopt another distance constraint loss called connectivity loss L c , besides the per-pixel Dice loss L d in the training process of CRN. Specifically, we choose coarse predictions from validation datasets from the first stage, concatenated with original images as the input of CRN. We firstly localize the predicted LAA region via its unique label. Then to make the region of LAA connected with the LA, we can only focus on the voxels most adjacent to the boundary interface. For the training efficiency, we locate the predicted LAA region with the approximately minimum external cube C (Please refer to supplementary material for more details about the algorithm). Four vertexes V i (i=1, 2, 3, 4) of C neighbored with the LA are selected to calculate the connectivity loss, which is indeed an improved minimal distance from vertexes to the surface of LA. Here we note the point set in the LA surface as P , and each point from P is noted as P j .where D means the Euclidean distance, σ is the sigmoid function. S is a scaling coefficient, and we set it as 20 according to the ablation study on this hyperparameter (Please refer to supplementary material for more quantitative results). Besides, λ is set as 1 if the training epoch reaches more 300 epochs, or it is 0. When there is no LAA predictions in a cropped patch, L c is equal to 0. On the ground that cases with a poor connectivity need to be refined in the training process of CRN, we increase sampling ratios of the whole LAA region from coarse masks of validation datasets. By cropping patches containing the boundary interface as network inputs, CRN will better learn the connectivity prior from the mapping between coarse predictions and expert annotations.In the inference stage, final decoded features F d are applied with the softmax operator to attain predicted masks. However, different channels of F d bear different maximum values, which will affect segmentation performance. Thus, we propose the concept of channel calibration (CC). Before per-pixel selecting maximum values between channels, we uniform the maximum value of F d channel by channel."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,3,Experiment,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,3.1,Experimental Settings,"Dataset. To evaluate the performance of our network, we conduct experiments on a new dataset, containing accurate annotations of LA and LAA provided by multiple experts. In detail, we collect 80 CT scans from 80 patients, which are split into 45/15/20 for training, validation and testing cases in Stage 1. In Stage 2, 50 predictions of the validation dataset in Stage 1 are generated by various models, in which there are 30 predicted masks with disconnections. Then we randomly split them as 35/15 for training and validation. Moreover, we choose the Dice score and HD 95 as quantitative metrics. Implementation Details. In the first stage, we choose the vanilla 3D UNet as our baseline model, trained for 2000 epochs. And we utilize a combination of cross entropy loss and Dice loss followed by [8]. For the second stage, CRN is trained for 500 epochs, which is a smaller 3D UNet with only 1.92M parameters.And we utilize Dice loss and connectivity loss as illustrated by Eq. 6. We train all models using AdamW optimizer. With the linear warm-up strategy, the initial learning rate is set as 5e-4 with a cosine learning rate decay scheduler, and weight decay is set as 1e-5. The size of cropped patches is 160 × 160 × 192. All models are implemented based on Pytorch and trained on 2 NVIDIA Tesla V100 GPUs."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,3.2,Experimental Results,"Table 1 consists of two sub-figures (a) and (b). According to Table 1.a, our proposed network outperforms classic CNNs and recent Transformer-based models. Specifically, our model shows superior to powerful nnUNet [8] on four metrics, and there exist 0.69% increase on the Dice score of LAA, 0.54 mm decrease on the HD 95 of LA. Compared with nnUNet, our two-stage model requires less computational cost and is free from complicated multi-model ensembles. However, the Dice score of our model on LA is inferior to nnUNet. We argue that our model is aimed at improving the segmentation mask of LAA, which bears a different structure from LA. And the iterative optimization process of CRN in Fig. 2 shows the refinement for LAA segmentation. More results about the generalization of CRN can be found in supplementary material. Besides, Swin UNETR [17] takes the lead in Transformer-based models, which is not as good as our model because Transformer-based models are data-hungry [6].Another phenomenon worth to mention is that our model shows an ordinary performance on the HD 95 of LAA, which is owing to the appearance of outliers.And not only our model but other CNNs and Transformer-based models suffer from the existence of outliers in predictions (More visualization results can be found in supplementary material). We will address this issue in the future research. In Table 1.b, we choose 3D UNet as the baseline model. SDM shows better segmentation performance compared with other methods on improving the segmentation of uncertain boundaries. And Fig. 2   "
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,3.3,Ablation Studies,"Table 2 shows ablation studies on key components and on the detailed structure of SDM. From Table 2.a, SDM, CRN and CC can all boost segmentation performance of the baseline model. Besides, we visualize qualitative results in Fig. 3.For the first row, our model give a more accurate localization for the boundary interface, which proves the effectiveness of SDM. The second row is a strong proof that CRN can effectively improve the connectivity between LA and LAA.Then we probe into the efficacy of connectivity loss L c by removing it from CRN, which results in a 0.51mm increase on the HD 95 of LAA, which is reasonable because L c is indeed a distance regularization on LAA boundaries. In Table 2.b, we investigate the significance of each component in SDM.(1) Without learnable difference kernels for differential maps of feature F and semantic feature G, the Dice score for LAA declines sharply, which reveals we need to focus on the anisotropic distribution of this dataset. (2) In SDM, we adopt a residual block by fusing the original feature F and the boundary feature. With F removed, there is some details and texture information missing, resulting in a performance drop. (3) Finally, semantic guidance from deeper features is deployed to guide the extraction for uncertain boundaries between LA and LAA. Besides, some false boundaries are restrained, which is illustrated in Fig. 2."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,4,Conclusion,"In this paper, we introduce a new CT dataset on LA and LAA, then carry out the segmentation task. Detailedly, we explain the refined process for the segmentation of uncertain boundaries via diffusion theory. Based on this, we apply SDM to successfully improve the segmentation for uncertain boundaries between LA and LAA. Then CRN with the connectivity loss can deal with the poor connectivity between two structures. Detailed quantitative and qualitative results have demonstrated the efficacy of two proposed elements."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Fig. 1 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Fig. 2 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Fig. 3 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Table 1 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,both sides of '|' represent evaluation metrics for each stage of our model,"). (b) Segmentation performance including our SDM and other methods related to deal with uncertain boundaries (LA: Left Atrium, LAA: Left Atrial Appendage. Bold: the best, Underlined numbers: the second best. '-'"
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,indicates that loss func- tions do not change Parameters and FLOPs of the baseline model).,(a) Segmentation benchmark on LA & LAA Model Params(M) FLOPs(T) Dice score (%) ↑ HD95 (mm) ↓
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,130 81.85 95.96 88.91 ±6.9 8.87 6.63 7.75 ±2,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Table 2 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 12.
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,1,Introduction,"Pulmonary diseases pose significant health risks, and computed tomography (CT) analysis of pulmonary airways and vessels has become a valuable clinical tool for revealing tomographic patterns [10,25]. Precise representation of the airway tree is essential for quantifying morphological changes, diagnosing respiratory disorders such as bronchial stenosis, acute respiratory distress syndrome, idiopathic pulmonary fibrosis, chronic obstructive pulmonary disease (COPD), obliterative bronchiolitis, and pulmonary contusion, as well as for virtual bronchoscopy and endobronchial navigation in surgery [3,10]. Furthermore, accurate modeling pulmonary arteries and veins improves computer-aided diagnosis of pulmonary embolism, chronic pulmonary hypertension [11,17], and lobectomy/segmentectomy [12,26,27].In recent years, deep learning methods have spawned research on airway and vessel segmentation. Convolutional neural networks (CNNs) have been widely employed in various existing studies to learn robust and discriminative features for automatic airway/artery/vein segmentation [4,[8][9][10]14,15,25]. However, accurately reconstructing complete airway or vessel tree branches remains a major challenge. Current state-of-the-art segmentation models, such as nnU-Net [5], still suffer from inadequate precision due to the minute scale and scattered spatial distribution of peripheral bronchi and vessels, which causes a severe class imbalance between the foreground and background, leading to degraded segmentation accuracy. The implications of such degraded performance can have negative consequences on clinical judgments and diagnoses, as it can lead to disconnections of pulmonary tubular structures of airways or vessels, as depicted in Fig. 1, potentially impeding accurate medical assessments.In this paper, we formulate the problem of disconnected pulmonary tubular structures as a key point detection task. The primary objective is to repair the topology structures of two disconnected components by accurately identifying the centers of the disconnected parts located at both ends of the components. Endpoints corresponding to the broken centerline of the pulmonary tubular structure are treated as two key points. The identification of these key points is critical in recognizing disconnections in pulmonary tubular structures for diagnosing pulmonary diseases, which has significant research implications.To address this issue, we propose a training data synthesis pipeline that generates disconnected data from complete pulmonary structures. We further explore the training strategy and thus build a strong basline based on 3D-UNet to predict the key points that can bridge disconnected components. Our contributions can be briefly summarized as follows:-A novel formulation of a practical research problem: We have formulated the problem of pseudo disconnection pulmonary tubular structures as a key point detection task, which is a significant contribution to the field as it has not been extensively explored before. "
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2,Method,"In this section, we present a comprehensive analysis of our approach for detecting pulmonary tubular interruptions as a keypoint detection task. We start by formulating the problem, followed by a description of the data simulation process used to construct the dataset. The dataset construction process is explained in detail to provide insight into the methods used for generating realistic data samples. We then introduce the simple two-channel 3D-UNet, and describe its architecture, key features, training objective, and implementation details."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2.1,Problem Formulation,"Segmentation of thoracic tubular structures, such as airways and vessels, from lung Computed Tomography (CT) scans is vital for diagnosing pulmonary diseases. Over the years, various deep learning-based segmentation methods have demonstrated the potential of Convolutional Neural Networks (CNNs) in handling this task. However, accurately segmenting pulmonary airways, arteries, and veins without interruption remains challenging due to the unique properties of the thoracic tubular structure. The trachea and blood vessels constitute only a small fraction of the whole thoracic CT image, which leads to severe class imbalance between the tubular foreground and background, hindering 3-D CNNs learning from sufficient supervisory signals [15]. Moreover, airways and vessels are complex tree-like structures with numerous bifurcations and branches of various sizes and lengths, making it difficult for CNNs to capture fine-grained patterns without encountering memory/parameter explosion and overfitting [10]. Segmentation networks often produce unsatisfactory predictions, resulting in disconnection or interruption of the estimated tubular structure, which could affect clinicians' judgment in clinical practice. Therefore, identifying the location of disconnections is of great research importance. In this paper, we have formulated the problem as a key point detection task, with the two endpoints of the interrupted centerline of the tubular structure serving as the two key points. We aim to use neural networks to predict the location of the disconnection part of vessels/airways, which has significant research implications. Keypoint detection is a popular computer vision technique for identifying object parts in images, with applications ranging from face recognition, pose estimation to medical landmark detection [13,16,18,23]. Heatmap regression has emerged as a standard approach for keypoint detection, where ground-truth heatmaps are generated for each keypoint using a Gaussian kernel [7,24]. The network outputs multi-channel heatmaps, with each channel corresponding to a specific keypoint. Our work adopts this approach for detecting two keypoints located at the endpoints of interrupted airway/vessel."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2.2,Training Data Synthesis,"We generated synthetic data from lung CT scans with carefully annotated pulmonary airways, arteries, and veins, as no public medical dataset was available for the task at hand. The synthetic data simulates the scenario of vascular/trachea disconnection and serves as a benchmark dataset for the keypoint detection task. To generate the data, binary masks of the tubular structures were extracted from 800 CT scans [6], and VesselVio software [2] was used to identify the centerlines of binarized airway/vessel volumes and create tree-like graphs. Random sampling was performed to select a branch of the vessel or airway, and two keypoints were sampled along the pre-extracted centerline. The keypoints were then subjected to morphological operations (from SimpleITK Python library [1,22]) to create near-true vascular disconnections. The resulting keypoints were labeled KP 1 and KP 2 , and the data was visualized in Fig. 1. It is important to note that discontinuities in real-world scenarios are mainly observed in thinner blood vessels. Due to the random sampling process and the prevalence of small branches within the entire tubular structure, the generated discontinuities are predominantly manifested in small blood vessels. Including the subfigures in Fig. 1(b) aims to clearly illustrate the visual appearance of these generated discontinuities."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2.3,The Keypoint Detection Network,"The framework and training pipeline of our network are depicted in Fig. 2. In the following sections, we will introduce each component in detail.Data Sampling. The generated raw data is too large for training the network due to the high-resolution nature of CT scans, which have dimensions of 512 × 512 for the x-y plane and variable dimensions for the z plane. Directly feeding the entire 3-D volume into the network can cause significant memory overhead and slow down the training convergence, especially with limited computing resources. Therefore, we crop a subvolume with a size of 80 × 80 × 80 around where the disconnection occurs from the original volume. Specifically, since the location of interrupted blood vessels cannot be known in advance and the small connected component where KP 2 is located can be found using morphological operations, we randomly select a point in that small object as the center point of our subvolume. This approach also serves as a new form of data augmentation. For each selected branch in an original volume, we randomly crop one subvolume for training purposes and three subvolumes for validation and testing.Network Design. We propose an encoder-decoder network that is based on the widely used 3D U-Net architecture. As depicted in Fig. 2, the inputs to the network are obtained by cropping subvolumes of the same size as the original volume. The first input contains only KP 1 and its connected component in the whole volume but is presented in a subvolume view. The second input exclusively comprises the small vessel/airway segment of KP 2 . The output heatmaps of the two keypoints correspond to the KP 1 input and KP 2 input, respectively, which avoids learning ambiguity. The 3D U-Net is a neural network architecture that features three encoder and three decoder stages. Each stage includes a convolution block and a downsampling or upsampling layer. The convolution block consists of two convolution layers, each using a kernel size of 3 × 3 × 3, followed by batch normalization and rectified linear unit (ReLU) activation.The network receives two binarized subvolumes I ∈ R 2×D×H×W as inputs, where D, H, W represent the spatial dimensions of the cropped volume. In this study, we decided to set the output heatmaps to the same size as the inputs, without downsampling, in order to avoid the loss of coordinate accuracy.In the neural network design phase, we prioritized formulating the problem, constructing an open-source dataset, and proposing a comprehensive training and testing pipeline. We refrained from incorporating sophisticated modules, such as attention mechanisms, transformer blocks, or distillation, and fine-tuning hyper-parameters. Hence, we do not delve into detailed network architecture design in this paper. However, we obtained promising results using a simple two-channel 3D-UNet model and explored various training techniques. Our work lays a solid foundation for future researchers to improve upon our findings by incorporating advanced techniques and innovative modules.Loss Function. We adopt the state-of-the-art keypoint detection framework to represent the problem as heatmap estimation, where the coordinate with the highest confidence in each heatmap of H ∈ R k×D×H×W corresponds to the location of the kth keypoint. The ground-truth heatmaps are generated by placing a 3D Gaussian kernel at the center of each ground-truth keypoint location. For simplicity, we define the Keypoint Mean-Squared Error (KMSE) loss function as follows:where H k and Ĥk refer to the ground-truth and the predicted heatmaps for the kth keypoint, and K is fixed to 2 in our study. To reduce memory cost, we limit the size of subvolumes to 80 × 80 × 80, which may result in invisible keypoints if the branch is long and the two keypoints are too far apart. Here, V k indicates the visibility of the ground truth keypoint, where V k = 1 and δ(V k ) = 1 if the keypoint is visible, and vice versa.Implementation Details. During the training phase, we employ a sampling strategy that randomly crops one volume, which introduces data augmentation, mitigates overfitting, and ensures training convergence. To reduce testing time and enable fair comparisons between models, we generate and save three random crops for each vessel branch during validation and testing. The size of the groundtruth heatmaps is 80 × 80 × 80, and the sigma of the 3D Gaussian kernel used to generate them is set to 2.5. All networks were trained using AdamW optimizer with a learning rate of 0.0001 and beta hyperparameters of 0.5 and 0.999. The training was performed on a single NVIDIA 3090ti GPU with a batch size of 16.PyTorch framework was used for implementation, and early stopping strategy was adopted to prevent overfitting. To speed up training, we initialize the artery and vessel models with the trained airway model. We combined artery and vein training data to increase the training samples and reduce the training time."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2.4,Model Inference,"Models trained using our proposed training paradigm may not be directly applicable to real-world data due to several assumptions made during training. Specifically, during training, we assume that the interrupted segmentation mask consists of only two continuous components representing KP 1 and KP 2 , and that the location of KP 2 is known a priori, which is used to randomly crop subvolumes. Additionally, we limit the subvolume's size to ensure efficient training.However, in real-world scenarios, the location of KP 1 and KP 2 components is unknown, and there may be small disconnected objects and noises scattered throughout the volume's entire original size. The only prior knowledge available is that KP 1 is located in the volume's largest connected component (i.e., the main vessel/airway), and KP 2 is in one of the small isolated components. To address this issue, we have developed an algorithm that bridges the gap between model training and inference, and accurately predicts disconnections in realworld situations. The pseudo-code of the inference algorithm is detailed in the supplementary materials."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,3,Experiments,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,3.1,Datasets,"A total of 800 CT scans with annotations of pulmonary airways, arteries, and veins are utilized to construct our dataset. The CT scans from multiple medical centers are manually annotated by a junior radiologist and confirmed by a senior radiologist [6]. The data is divided into training, validation, and test subsets with a ratio of 7:1:2. Each CT scan is pre-processed into three binarized volumes of airways, arteries, and veins. Subsequently, 30 distinct branches per volume were randomly selected for each binarized volume under specific criteria to create 30 volumes with vascular interruptions. The Pulmonary Tree Repairing (PTR) dataset includes 3D models represented by binarized ground-truth segmentation masks, centerlines, disconnected volumes, and a corresponding json file for each subject. The json file contains comprehensive information, such as the coordinates of bifurcations, endpoints, and all points along each branch, capturing diverse characteristics specific to each blood vessel. Note that the keypoint detection of airways, arteries, and veins disconnection is treated as three independent tasks, with each task having a dataset size of 800 × 30. The results are optimized on the validation set and reported on the test set."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,3.2,Evaluation Metrics,"Based on the definition of Object Keypoint Similarity (OKS) in pose estimation tasks, we have adapted this metric to align with the features of our dataset. Our modifications to the OKS are reflected in the following metric formulations:Here d k is the Euclidean distance between the predicted keypoint and the ground-truth keypoint, along with the vessel volume S of the corresponding branch. To maintain a consistent scale for OKS, we have introduced λ, a constant which we set to 0.2. OKS k refers to the OKS of kth keypoint (k = 2 in our study).where OKS i denotes the OKS of ith sample and V k is the visibility flag.where we use standard AP τ , which measures prediction precision of a model given a specific threshold τ . In order to provide a comprehensive and nuanced evaluation of the model's performance, we report average precision across various thresholds. Specifically, we report AP 50 (AP at τ = 0.5), AP 75 (AP at τ = 0.75), AP (the mean AP across 10 τ positions, where τ = {0.5, 0.55, ..., 0.95}), AP S (for small vessels with edge radius within the range of (0, 2]), AP M (for medium vessels with edge radius within the range of (2, 3]), AP L (for large vessels with edge radius greater than 3), AP k1 (AP for KP 1 ), AP k2 (AP for KP 2 ), E d (mean "
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,3.3,Results,"To analyze the performance of our methods on topology repairing of disconnected pulmonary airways and vessels, we report several methods on the proposed PTR dataset, as shown in Table 1. The keypoint heatmap visualization is provided in the supplementary materials.The study demonstrates that the two-channel 3D-UNet model surpasses the performance of the one-channel counterpart on airway and vessel segmentation tasks. Specifically, the two-channel model yields significant improvements in AP of approximately 7%, 9%, and 15% for airway, artery, and vein tasks, respectively. Additionally, the two-channel model achieves the highest performance on all evaluation metrics for all three tasks. These results suggest that the separation of KP 1 and KP 2 components as two-channel input can effectively improve their interaction in multiple feature levels, leading to improved performances. This is likely due to the high correlation between these two keypoints throughout the topological structure. However, detecting KP 1 was significantly challenging due to the random selection of cropping center points during data sampling, leading to a weaker performance for E d and AP metrics. Additionally, the sparse distribution of keypoints on small pulmonary vessels posed a considerable challenge for capturing subtle features. Notably, the two-channel networks exhibited superior performance over one-channel methods by a substantial margin, which emphasizes the advantages of separating the two components. In the future work, it will be beneficial to design models that capture this characteristic."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,4,Conclusion,"In this study, we introduce a data-driven post-processing approach that addresses the challenge of disconnected pulmonary tubular structures, which is crucial for the diagnosis and treatment of pulmonary diseases. The proposed approach utilizes the newly created Pulmonary Tree Repairing (PTR) dataset, comprising 800 complete 3D models of pulmonary structures and synthetic disconnected data. A two-channel simple yet effective neural network is trained to detect keypoints that bridge disconnected components, utilizing a training data synthesis pipeline that generates disconnected data from complete pulmonary structures. Our approach yields promising results and holds great potential for clinical applications. While our study primarily focuses on addressing the disconnection issue, we recognize that more complex scenarios, such as handling multiple disconnected components, distinguishing between arteries and veins, and implementing our method in real-world settings, require further investigation in future work. Point or implicit representations [19][20][21] learning the geometric structures have high potentials in this application."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Fig. 1 .,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Fig. 2 .,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,-An effective yet simple baseline with efficient 3D-UNet: We,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Table 1 . Keypoint detection performance on the PTR dataset.,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Acknowledgment. This research was supported by Australian Government Research,"Training Program (RTP) scholarship, and supported in part by a Swiss National Science Foundation grant."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 36.
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,1,Introduction,"Skin cancer is a serious and widespread form of cancer that requires early detection for successful treatment. Computer-aided diagnosis systems (CAD) using deep learning models have shown promise in accurate and efficient skin lesion diagnosis. However, recent research has revealed that the success of these models may be a result of overly relying on ""spurious cues"" in dermoscopic images, such as rulers, gel bubbles, dark corners, and hairs [3][4][5]29], which leads to unreliable diagnoses. When a deep learning model overfits specific artifacts instead of learning the correct dermoscopic patterns, it may fail to identify skin lesions in real-world environments where the artifacts are absent or inconsistent.To alleviate the artifact bias and enhance the model's generalization ability, we rethink the problem from the domain generalization (DG) perspective, where a model trained within multiple different but related domains are expected to perform well in unseen test domains. As illustrated in Fig. 1, we define the domain labels based on the types of artifacts present in the training images, which can provide environment-aware prior knowledge reflecting a range of noisy contexts. By doing this, we can develop a DG algorithm to learn the generalized and robust features from diverse domains.Previous DG algorithms learning domain-invariant features from source domains have succeeded in natural image tasks [2,17,19], but cannot directly apply to medical images, in particular skin images, due to the vast cross-domain diversity of skin lesions in terms of shapes, colors, textures, etc. As each domain contains ad hoc intrinsic knowledge, learning domain-invariant features is highly challenging. One promising way is, as suggested in some recent works [7,24,32], exploiting multiple learnable domain experts (e.g., batch norm statistic, auxiliary classifiers, etc.) to capture domain-specific knowledge from different source domains individually. Still, two significant challenges remain. First, previous work only exploits some weak experts, like the batch norm, to capture knowledge, which naturally hampers the capability of capturing essential domain-specific knowledge. Second, previous methods such as [30] focused on learning domain knowledge independently while overlooking the rich cross-domain information that all domain experts can contribute collectively for the target domain prediction.To overcome the above problems, we propose an environment-aware prompt vision transformer (EPVT) for domain generalization of skin lesion recognition. On the one hand, inspired by the emerging prompt learning techniques that embed prompts into a model for adaptation to diverse downstream tasks [12,26,31], we construct different prompt vectors to strengthen the learning of domainspecific knowledge for adaptation to diverse domains. Then, the self-attention mechanism of the vision transformer (ViT) [8] is adopted to fully model the relationship between image tokens and prompt vectors. On the other hand, to encourage cross-domain information sharing while preserving the domain-specific knowledge of each domain prompt, we propose a domain prompt generator based on low-rank weights updating. The prompt generator enables multiple domain prompts to work collaboratively and benefit from each other for generalization to unknown domains. Additionally, we devise a domain mixup strategy to resolve the problem of co-occurring artifacts in dermoscopic images and mitigate the resulting noisy domain label assignments.Our contributions can be summarized as: (1) We resolve an artifacts-derived biasing problem in skin cancer diagnosis using a novel environment-aware prompt learning-based DG algorithm, EPVT; (2) EPVT takes advantage of a ViTbased domain-aware prompt learning and a novel domain prompt generator to improve domain-specific and cross-domain knowledge learning simultaneously;(3) A domain mixup strategy is devised to reduce the co-artifacts specific to dermoscopic images; (4) Extensive experiments on four out-of-distribution skin datasets and six biased ISIC datasets demonstrate the outperforming generalization ability and robustness of EPVT under heterogeneous distribution shifts."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2,Method,"In domain generalization (DG), the training dataset D train consists of M source domains, denoted as D train = {D k |k = 1, ..., M }. Here, each source domain D k is represented by n labeled instances {(x k j , y k j )} n j=1 . The goal of DG is to learn a model G : X → Y from the M source domains so that it can generalize well in unseen target domains D test . The overall architecture of our proposed model, EPVT, is shown in Fig. 2a. We will illustrate its details in the following sections."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2.1,Domain-Specific Prompt Learning with Vision Transformer,"To enable the pre-trained vision transformer (ViT) to capture knowledge from different domains, as shown in Fig. 2a, we define a set of M learnable domain prompts produced by a domain prompt generator (introduced in Sect. 2.2), denoted as, where d is the same size as the feature embedding of the ViT and each prompt P m corresponds to one domain (i.e. dark corners). To incorporate these prompts into the model, we follow the conventional practice of visual prompt tuning [12], which prepends the prompts P D into the first layer of the transformer. Particularly, for each prompt P m in P D , we extract the domain-specific features as:where F is the feature encoder of the ViT, X 0 denotes the class token, E 0 is the image patch embedding, F m is the feature extracted by ViT with the m-th prompt, and 0 is the index of the first layer. Domain prompts P D are a set of learnable tokens, with each prompt P m being fed into the vision transformer along with the image and corresponding class tokens from a specific domain.Through optimizing, each prompt becomes a domain expert only responsible for the images from its own domain. By the self-attention mechanism of ViT, the model can effectively capture domain-specific knowledge from the domain prompt tokens."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2.2,Cross-Domain Knowledge Learning,"To facilitate effective knowledge sharing across different domains while maintaining its own parameters of each domain prompt, we propose a domain prompt generator, as depicted in Fig. 2b. Our approach is inspired by model adaptation and multi-task learning techniques used in natural language processing [13,26]. Aghajanyan et al. [1] have shown that when adapting a model to a specific task, the updates to weights possess a low intrinsic rank. Similarly, each domain prompt P m should also have a unique low intrinsic rank when learning knowledge from its own domain. To this end, we decompose each P m into a Hadamard product between a randomly initialized shared prompt P * and a rank-one matrix P k obtained from two randomly initialized learnable vectors u k and v k , which is:where P m represents the domain-specific prompt, computed by Hadamard product of P * and P k . Here, P * ∈ R s×d is utilized to learn general knowledge, with s and d representing the dimensions of the prompt vector and feature embedding respectively. On the other hand, P k is computed using domain-specific trainable vectors: u k ∈ R s and v k ∈ R d . These vectors capture domain-specific information in a low-rank space. The decomposition of domain prompts into rank-one subspaces ensures that the model effectively encodes domain-specific information. By using the Hadamard product, the model can efficiently leverage cross-domain knowledge for target domain prediction."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2.3,Mitigating the Co-artifacts Issue,"The artifacts-based domain labels can provide domain information for dermoscopic images. However, a non-trivial issue arises due to the possible cooccurrence of different artifacts from other domains within each domain. To address this issue, we employ a domain mixup strategy [27,28]. Instead of assigning a hard prediction label (""0"" or ""1"") to each image, in each batch, we mix every image using two randomly selected images from two different domains. This allows us to learn a flexible margin relative to both domains. We then apply the cross-entropy loss to the corresponding labels of bot images, as shown in Fig. 2c and can be represented by the following equation:where x mix = λx k i +(1-λ)x q j ; x k i and x q j are samples from two different domains k and q, and y k i and y q j are the corresponding labels. This strategy can overcome the challenge of ambiguous domain labels in dermoscopic images and improve the performance of our model."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2.4,Optimization,"So far, we have introduced L mixup in Eq. 3 for optimizing our model. However, since our goal is to generalize the model to unseen environments, we also need to take advantage of each domain prompt. Instead of assigning equal weights to each domain prompt, we employ an adapter [30] that learns the linear correlation between the domain prompts and the target image prediction. To obtain the adapted prompt for inference in the target domain, we define it as a linear combination of the source domain prompts:where A represents an adapter containing a two-layer MLP with a softmax layer, and w m denotes the learned weights.To train the adapter A, we simulate the inference process for each image in the source domain by treating it as an image from the pseudo-target domain.Specifically, we first extract features from the ViT: Fm (x) = F ([X 0 , E 0 ]). Then we calculated the adapted prompt P adapted for the pseudo-target environment image x using the adapter A: P adapted = A( Fm (x)). Next, we extract features from ViT using the adapted prompt: Fm (x) = F ([ Fm (x), P adapted , E 0 ]). Finally, the classification head H is applied to predict the label y: y = H( Fm (x)). Additionally, the inferece process is the same as the simulated inference process and our final prediction will be conditioned on the adapted prompt P adapted .To ensure that the adapter learns the correct linear correlation between the domain prompts and the target image, we use the domain label from source domains to directly supervise the weights w m . We also use the cross-entropy loss to maintain the model performance with the adapted prompt:(5) where Fm (x) is the obtained feature map conditioned on the adapted prompt P adapted , and H is the classification head. The total loss is then defined as L total = L mixup + L adapted ."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,3,Experiments,"Experimental Setup: We consider two challenging melanoma-benign classification settings that can effectively evaluate the generalization ability of our model in different environments and closely mimic real-world scenarios. (1) Outof-distribution evaluation: The task is to evaluate the model on test sets that contain different artifacts or attributes compared to the training set. We train and validate all algorithms on ISIC2019 [6] dataset, following the split of [3]. We use the artifacts annotations from [3] and divide the training set of ISIC2019 into five groups: dark corner, hair, gel bubble, ruler, and clean, with 2351, 4884, 1640, 672, and 2796 images, respectively. We evaluate models on four out-of-distribution (OOD) datasets, including Derm7pt-Dermoscopic [14], Derm7pt-Clinical [14], PH2 [18], and PAD-UFES-20 [21]. It's worth noting that ISIC2019, Derm7pt-Dermoscopic, and PH2 are dermoscopic images, while Derm7pt-Clinical and PAD are clinical images. (2) Trap set debiasing: We train and test our EPVT with its baseline on six trap sets [3] with increasing bias levels, ranging from 0 (randomly split training and testing sets from the ISIC2019 dataset) to 1 (the highest bias level where the correlation between artifacts and class label is in the opposite direction in the dataset splits). More details about these datasets and splits are provided in the complementary material.Implementation Details: For a fair comparison, we train all models using ViT-Base/16 [8] backbone pre-trained on Imagenet and report the ROC-AUC with five random seeds. Hyperparameter and model selection methods are crucial for domain generalization algorithms. We conduct a grid search over learning rate (from 3e -4 to 5e -6 ), weight decay (from 1e -2 to 1e -5 ), and the length of the prompt (from 4 to 16, when available) and report the best performance of all models. We employ the training-domain validation set method [11] for model selection. After the grid search, we use the AdamW optimizer with a learning rate of 5e -6 and a weight decay of 1e -2 . The batch size is 130, and the length of the prompt is 10. We resize the input image to a size of 224 × 224 and adopt the standard data augmentation like random flip, crop, rotation, and color jitter. An early stopping with the patience of 22 is set and with a total of 60 epochs for OOD evaluation and 100 epochs for trap set debiasing. All experiments are conducted on a single NVIDIA RTX 3090 GPU.Out-of-Distribution Evaluation: Table 1 presents a comprehensive comparison of our EPVT algorithm with existing domain generalization methods. The results clearly demonstrate the superiority of our approach, with the best performance on three out of four OOD datasets and remarkable improvements over the ERM algorithm, especially achieving 4.1% and 8.9% improvement on the PAD and PH2 datasets, respectively. Although some algorithms may perform similarly to our model on one of the four datasets, none can consistently match the performance of our method across all four datasets. Particularly, our approach showcases the highest average performance, with a 2.05% improvement over the second-best algorithm across all four datasets. These findings highlight the effectiveness of our algorithm in learning robust features and its strong generalization abilities across diverse environments."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Ablation Study:,"We perform ablation studies to analyze each component of our model, as shown in Table 2. We set our baseline as the Empirical Risk Minimization (ERM) algorithm, and we gradually add P (prompt [12]), A (Adapter), M (Mixup), and G (domain prompt generator) into the model. Firstly, we observe that the baseline model with prompt only improves the average performance by  0.1%, showing that simply combining prompt does not very helpful for domain generalization. When we combine the adapter, the model's average performance improves by 1.37%, but it performs worse than ERM on PAD dataset. Subsequently, we added domain mixup and domain prompt generator to the model, resulting in significant further improvements in the model's average performance by 1.32% and 1.69%, respectively. The consistently better performance than the baseline on all four datasets also highlights the importance of addressing coartifacts and cross-domain learning for DG in skin lesion recognition.Trap Set Debiasing: In Fig. 3a, we present the performance of the ERM baseline and our EPVT on six biased ISIC2019 datasets. Each point on the graph represents an algorithm that is trained and tested on a specific bias degree split.The graph shows that the ERM baseline performs better than our EPVT when the bias is low (0 and 0.3). However, this is because ERM relies heavily on spurious correlations between artifacts and class labels, leading to overfitting on the training set. As the bias degree increases, the correlation between artifacts and class labels decreases, and overfitting the train set causes the performance of ERM to drop dramatically on the test set with a significant distribution difference. In contrast, our EPVT exhibits greater robustness to different bias levels. Notably, our EPVT outperforms the ERM baseline by 9.4% on the bias 1 dataset.Prompt Weights Analysis: To verify whether our model has learned the correct domain prompts for target domain prediction, we analyze and plot the results in Fig. 3b and3c. Firstly, we extract the features of each domain from our training set and extract the feature from one target dataset, Derm7pt-Clin.We then calculate the Frechet distance [9] between each domain and the target dataset using the extracted feature, representing the domain distance between them. The results are recorded in Fig. 3b. Next, we record the learned weights of each domain prompt in Fig. 3c; it shows that our model assigns the highest weight to the ""dark corner"" group, as the domain distance between ""dark corner"" and Derm7pt-Clin is the closest, as shown in Fig. 3b. This suggests that they share the most similar domain information. Further, the ""clean"" group is assigned the smallest weight as the domain distance between them is the largest, indicating that their domains are significantly different and contain less useful information for target domain prediction. In summary, we observe a negative correlation between domain distance and the prompt's weights, indicating that our model can learn the correct knowledge from different domains precisely."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,4,Conclusion,"In this paper, we propose a novel DG algorithm called EPVT for robust skin lesion recognition. Our approach addresses the co-artifacts problem using a domain mixup strategy and cross-domain learning problems using a domain prompt generator. Compared to other competitive domain generalization algorithms, our method achieves outstanding results on three out of four OOD datasets and the second-best on the remaining one. Additionally, we conducted a debiasing experiment that highlights the shortcomings of conventional training using empirical risk minimization, which leads to overfitting in dermoscopic images due to artifacts. In contrast, our EPVT model effectively reduces overfitting and consistently performs better in different biased environments."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Fig. 1 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Fig. 2 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Fig. 3 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Table 1 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Table 2 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_24.
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,1,Introduction,"Computer-aided Diagnosis (CAD) systems have achieved success in many clinical tasks [5,6,12,17]. Most CAD studies were developed on regular and selected datasets in the laboratory environment, which avoided the problems (data noise, missing data, etc.) in the clinical scenarios [3,6,9,13,18]. In a real clinical scenario, the clinicians generally synthesize all aspects of information, and conduct consultations with Multidisciplinary Team (MDT), to accurately diagnose and plan the treatment [9,10,13]. Real-world studies have received increasing attention [11,16], and it is challenging for the CAD in the real-world scenarios as: 1) Consistent with the clinical workflow, CAD needs to consider multidisciplinary information to obtain multidimensional diagnosis; 2) Due to information collection, storage and manual evaluation, there are missing and noisy medical data. This phenomenon is especially common in rare tumors like pancreatic neuroendocrine neoplasms (pNENs).In order to overcome above challenges, some studies [3,9,13,18] used multilabel method because of the following advantages: 1) The input of the model is only a single modality such as images, which is easy to apply clinically; 2) The model learns multi-label and multi-disciplinary knowledge, which is consistent with clinical logic; 3) Multi-label simultaneous prediction, which meets the need of clinical multi-dimensional description of patients. For the above advantages, multi-label technology is suitable for real-world CAD. The previous multi-label CAD studies were designed based on simple parameter sharing methods [9,15,20] or Graph Neural Network (GNN) method [2]. The former implicitly interacts with multi-label information, making it difficult to fully utilize the correlation among labels; And the latter requires the use of word embeddings pre-trained on public databases, which is not friendly to many medical domain proper nouns. The generalizability of previous multi-label CAD studies is poor due to these disadvantages. In addition, none of the current multi-label CAD studies have considered the problem of missing labels and noisy labels.Considering these real-world challenges, we propose a multi-label model named Self-feedback Transformer (SFT), and validate our method on a realworld pNENs dataset. The main contributions of this work are listed: 1) A transformer multi-label model based on self-feedback mechanism was proposed, which provided a novel method for multi-label tasks in real-world medical application; 2) The structure is flexibility and interactivity to meet the needs of realworld clinical application by using four inference modes, such as expert-machine combination mode, etc.; 3) SFT has good noise resistance, and can maintain good performance under noisy label input in expert-assisted mode."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,2,Method,"Transformer has achieved success in many fields [4,19]. Inspired by DETR [1] and C-Tran [8], we propose a multi-label model based on transformer and selffeedback mechanism. As shown in Fig. 1,1) image is embedded by Convolutional Neural Network (CNN) firstly; 2) then all labels are embedded and combined with their state embeddings; 3) finally, all embeddings are fed into a transformer, and the output label tokens are fed into Fully Connection (FC) layers for final predictions. Based on this network, we further introduce a self-feedback strategy, which allows the label information (including the missing labels) to be reused iteratively for enhancing the utilization of labels."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,2.1,Transformer-Based Multi-label Model,"Image Embeddings F . Given input image x ∈ R L×W ×H , the feature vector k ∈ R C is extracted by a CNN after Global Average Pooling (GAP), where the output channel C = 256. Then k is split along the channel dimension into N (N = 8) sub-feature vectors F = {f 1 , f 2 , . . . , f N }, f i ∈ R d , d = C/N for tokenization. We choose 3D VGG8, a simple CNN with 8 convolution layers.Label Embeddings L. In order to realize the information interaction among labels, and between labels and image features, we embed labels by an embedding layer. Each image x has M labels, and all labels are embedded into a vector set L = {l 1 , l 2 , . . . , l M }, l i ∈ R d by the learnable embedding layer of size d × M ."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Soft State Embeddings S.,"There is a correlation between labels, e.g. the lesions with indistinct borders tend to be malignant. Therefore, we hypothesize that the states (GT values) of some labels can be a context for helping predict the remaining labels. We use a soft state embedding method. Specifically, we first embed the positive and negative states into s p and s n , both ∈ R d , and then the final state embedding s i is the weighted sum of s p and s n as shown in Equation (1). The state weight w p i and w n i is the true label value (eg. w p i = 1.0 when label is positive), where w p i + w n i = 1. For labels with continuous values such as age, the value normalized to 0 ∼ 1 is w p i . The s i is set as a zero vector for unknown label. l i = s i + l i is the final label embedding.Multi-label Inference with Transformer Encoder. In a transformer, each output token is the integration of all input tokens. Taking advantage of this structure, we use a transformer encoder to integrate image embeddings and label embeddings, and used the output label tokens to predict label value. Specifically, embedding setare the input tokens, the attention value α and output token e are computed as follows:where e i is from E, W q , W k and W v are weight matrices of query, key and value, respectively, W r and W o are transformation matrices, and b 1 and b 2 are bias vectors. This update procedure is repeated for L layers, where the e i are fed to the successive transformer layer. Finally, all e i which are label output tokens are fed into M independent FC layers for predicting value of each label. The states of unknown labels cannot provide context, thus, the information interaction between known labels and unknown labels may be weaken. To overcome this problem, we propose a Self-feedback Strategy (SFS) inspired by Recurrent Neural Networks (RNN) to enhance the interaction of labels."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,2.2,Self-feedback Strategy,"Training Progress and Loss Function. As shown in Fig. 2, at time point t = 0, the state embedding is initialized to s GT i by Ground Truth (GT) value, and the initial label embedding l initial i is computed byis combined with f i as the initial input, and then the output predicted value is converted into state embedding s 0 i by Equation (1). When t>0, the label embedding l t i is updated iteratively by l t i = s t-1 i + l i , and then fed into the transformer T times. For classification and regression labels, we use focal crossentropy loss and L2 loss respectively, and use the method in [7] to auto-weight the loss value of each label. The backpropagation of gradients and parameter updates are performed immediately after calculating the loss at each time point t. In the regular inference phase, the state of all labels is initialized as unknown.Label Mask Strategy. To avoid predicting with labels' own input state, we use a Label Mask Strategy (LMS) during training phase to randomly mask a certain proportion a of known labels, which causes the labels' states to be embedded as zero vectors. Meanwhile, only the loss of the masked known label is calculated."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3,Experiments and Results,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.1,Dataset and Evaluation,"Real-World pNENs Dataset. We validated our method on a real-world pNENs dataset from two centers. All patients with arterial phase Computed Tomography (CT) images were included. The dataset contained 264 and 28 patients in center 1 and center 2, and a senior radiologist annotated the bounding boxes for all 408 and 28 lesions. We extracted 37 labels from clinical reports, including survival, immunohistochemical (IHC), CT findings, etc. Among them, 1)RECIST drug response (RS), 2)tumor shrink (TS), 3)durable clinical benefit (DCB), 4)progression-free survival (PFS), 5)overall survival (OS), 6)grade (GD), 7)somatostatin receptor subtype 2(SSTR2), 8)Vascular Endothelial Growth Factor Receptor 2 (VEFGR2), 9)O6-methylguanine methyltransferase (MGMT), 10)metastatic foci (MTF), and 11)surgical recurrence (RT) are main tasks, and the remaining are auxiliary tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics features of them were extracted, of which 162 features were selected and binarized as auxiliary tasks because of its statistically significant correlation with the main labels. The label distribution and the overlap ratio (Jaccard index) of lesions between pairs of labels are shown in Fig. 3. It is obvious that the real-world dataset has a large number of labels with randomly missing data, thus, we used an adjusted 5-fold cross-validation. Taking a patient as a sample, we chose the dataset from center 1 as the internal dataset, of which the samples with most of the main labels were used as Dataset 1 (219 lesions) and was split into 5 folds, and the remaining samples are randomly divided into the training set Dataset 2 (138 lesions) and the validation set Dataset 3 (51 lesions), the training set and the validation set of the corresponding folds were added during cross-validation, respectively. All samples in Center 2 left as external test set. Details of each dataset are in the Supplementary Material. Dataset Evaluation Metrics. We evaluate the performance of our method on the 10 main tasks for internal dataset, and due to missing labels and too few SSTR2 labels, only the performance of predicting RT, PFS, OS, GD, MTF are evaluated for external dataset. We employ accuracy (ACC), sensitivity (SEN), specificity (SPC), F1-score (F1) and area under the receiver operating characteristic (AUC) for each task, and compute the mean value of them (e.g. mAUC)."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.2,Implementation Details,"The CT window width and window level were adjusted to 310 and 130 HU refer to [17], and the image inside the bounding box was cropped and scaled to 128×128×64 pixels. The numbers of convolutional kernels of VGG8 are [3,32,32,64,64,128,128,256]. Transformer encoder contained 2 layers and 8 heads. Layer normalization was used in transformer. The LMS a was set as 0.5, and the training feedback times T was 4. We used Adam optimiser, and used cosine annealing to reduce the learning rate from 1e-4 to 1e-12 over all 200 epochs. Our method was implemented in Pytorch, using an NVIDIA RTX TITAN GPU."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.3,Comparison and Ablation Experiments,"We compared our method with Single-task(ST), Parameters Sharing (PS), ML-Decoder [14] and C-tran [8]. Specifically, a ST model is trained using single label, and PS model uses a FC layer followed by the CNN to predict all labels. It should be noted that the CNN backbone of each method was replaced as 3D VGG8 to ensure fair comparison. In the ablation experiment, we removed the LMS and SFS to analyze their impact. The AUC of each main label is shown in Fig. 4, and the average performance is shown in Table 1. It can be seen that multi-label models is better than that of ST due to using the relationship among labels, and SFT outperform other methods on most tasks and the average performance. The ablation experiments results showed that removing the LMS and SFS components causes performance degradation, indicating the necessity of them.   "
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.4,Performance of Different Inference Modes,"As shown in the Fig. 5, we designed 4 inference modes: 1) Regular, only input images; 2) Expert-assisted (EA), certain information is provided by clinicians; 3) Self-feedback (SF), iteratively inference T times by using prediction; 4) Expertmachine Combination (EMC), expert-assisted and self-feedback inference are both performed. Only the auxiliary labels states were input in EA mode. The results is shown in Table 2. Both SF and EA perform better than regular mode, and EMC outperforms other modes with a mAUC of 0.72 (0.82 on external dataset). We tested the SFT with and without SFS training under different feedback times T in SF mode, and results (Fig. 6.) showed that the performance of SFT with SFS training increases gradually with the increase of T , while the SFT without SFS training has a general trend of decreasing performance after continuous iteration.   "
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.5,Analysis of Noise Resistance,"To explore the noise resistance of SFT in EA mode, we selected 20, 40, 60, 80, and 100 percent of the known labels respectively, and further negated 0, 20, 40, 60, 80, 100 percent of the selected labels to simulate noisy labels. The way to negate a label is to change the label value x to 1x. As shown in Fig. 7, as the noise ratio increases, the performance shows a decreasing trend, and the performance decreases slightly when the noise ratio ≤ 40 %. Finally, it can be observed that the SFT using the SFS training strategy is relatively less affected by noise. When using 100 percent labels, the mAUC of the internal dataset decreased from 0.71 (noise ratio = 0.0) to 0.53 (noise ratio = 1.0), a decrease of 0. "
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,4,Conclusion,"We proposed a novel model SFT for multi-label prediction on real-world pNENs data. The model integrates label and image informations based on a transformer encoder, and iteratively uses its own prediction based on a self-feedback mechanism to improve the utilization of missing labels and correlation among labels. Experiment results demonstrated our proposed model outperformed other multilabel models, showed flexibility by multiple inference modes, and had a certain ability to maintain performance when the input context noise was less than 40%."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 1 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 2 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 3 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 4 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 5 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 6 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 7 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Table 1 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Table 2 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_49.
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,1,Introduction,"Placenta accreta spectrum (PAS) refers to the abnormal invasion of trophoblast cells into the myometrium at different depths of infiltration [10]. With the rising trend of advanced maternal age and cesarean section delivery, the incidence of PAS has increased steadily in recent years [5]. Undiagnosed PAS may lead to massive obstetric hemorrhage and hysterectomy [21]. Therefore, accurate detection of PAS prenatally is critical for appropriate treatment planning. Magnetic resonance imaging (MRI) provides valuable position information of placenta and can be an important complementary imaging method when ultrasound (US) diagnosis is inconclusive [1,15]. Since manually identifying PAS on MR images is time-consuming and labor-intensive, automated detection of PAS is significant in clinical practice.However, due to the lack of open-source dataset, the research on computeraided diagnosis of PAS is very limited. Previous studies [17,18,20,29] mainly focused on the classification task but accurate location cannot be provided. Moreover, existing object detection methods are designed for natural images [3,19,23,30] or specific diseases [16,22,27], with no consideration for the characteristics of PAS. Given that the abnormal invasion usually occurs near the uteroplacental interface, the geometric information of lesion regions is highly correlated with the shape of placenta, thereby causing significant variation in the aspect ratios of PAS bounding boxes. Furthermore, MRI in most cases is used after suspecting an abnormality on US, this by itself raises the difficulty of accurate labeling [19].To address the above issues, we propose a novel geometry-adaptive PAS detection method, which utilizes the shape prior of placenta to predict highquality PAS bounding boxes and further refines inaccurate annotations. The prior knowledge is mainly reflected in the aspect ratio of lesion boxes. Specifically, to take advantage of the geometry prior, we design a Geometry-adaptive Label Assignment (GA-LA) strategy and a Geometry-adaptive RoI Fusion (GA-RF) module. The GA-LA strategy dynamically selects positive proposals by calculating the optimal IoU threshold for each lesion. The GA-RF module fuses multi-scale RoI features from different pyramid layers. To reduce the impact of lesions with large aspect ratio, the module generates fusion weights through the geometry distribution of proposals. Furthermore, in order to alleviate the reliance on accurate annotations, we construct a Lesion-aware Detection Head (LA-Head), which leverages the geometry-guided predictions to iteratively refine bounding box labels by multiple instance learning. To the best of our knowledge, this is the first work to automatically detect PAS disorders on MR images. The contributions of this paper can be summarized as follows: (1) A Lesion-aware Detection Head (LA-Head) is designed, which employs a new multiple instance learning approach to improve the robustness to inaccurate annotations. (2) A flexible Geometry-adaptive Label Assignment (GA-LA) strategy is proposed to select positive PAS candidates according to the shape of lesions. (3) A statisticbased Geometry-adaptive RoI Fusion (GA-RF) module is developed for aggregating multi-scale features based on the geometry distribution of proposals."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,2,Method,"The central idea of our geometry-adaptive network is to refine inaccurate bounding boxes with geometry-guided predictions. To this end, we propose a Geometryadaptive Label Assignment (GA-LA) strategy and a Geometry-adaptive RoI Fusion (GA-RF) module to introduce geometry prior to detection. Then we develop a Lesion-aware Detection Head (LA-Head) to achieve the refinement of inaccurate annotations using multiple instance learning.The overview of our method is illustrated in Fig. 1. Given an input image, Feature Pyramid Network (FPN) [12] extracts image features and then Region Proposal Network (RPN) [19] generates a set of region proposals. After obtaining the candidate boxes, RoIAlign [7] maps them to feature pyramid levels and extracts each proposal as a fixed-size feature map. Then the GA-RF module aggregates the multi-scale representations of each level through the statistical geometric information. Based on the fused RoI feature map, the LA-Head predicts and refines the classification and localization of lesions. During training, the GA-LA strategy assigns label to each proposal according to the shape of corresponding ground-truth box, making the model directly trainable."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,2.1,Geometry-Adaptive Label Assignment,"The static label assignment strategy adopted by Faster R-CNN predefines the IoU threshold τ to match objects and background to each anchor, but ignores the different shapes of PAS regions. To relieve the problem, we propose a GA-LA strategy to dynamically calculate the IoU threshold τ i for each lesion bounding box g i according to its aspect ratio r i . A previous study demonstrated that the aspect ratio is larger, the detection performance is better with a low IoU threshold [9,26]. Therefore, the value of τ i should be inversely proportional to r i . We design a simple but effective weighting factor and compute τ i as below:where α is a hyper-parameter. For each lesion g i , the proposals with an IoU greater than or equal to the threshold τ i are selected as positive samples. The labeled proposals are then used to train the network."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,2.2,Geometry-Adaptive RoI Fusion,"RoIAlign maps each proposal to a single feature pyramid level, which fails to leverage multi-scale information from other levels. Some works [6,14] have attempted to integrate RoI features, but they do not consider the geometric characteristics of PAS bounding boxes. Hence, we design a GA-RF module to embed geometry prior into the representations of proposals. Given a proposal, the mapped RoI features {f l i ∈ R c×h×w } L l=1 in different levels are concatenated in channel dimension, where L is the number of levels. We define the proposal whose aspect ratio is greater than R as a hard sample. The number of hard samples distributed to each level is formulated as geometry prior knowledge s i ∈ R 1×4 , which is used to generate fusion factor ω ∈ R 1×4 . The fused feature f i ∈ R cL×h×w is the weighted sum of all feature levels and expressed as follows:where {N l } L l=1 is the number of proposals with large aspect ratio in each layer. In this way, we take full advantage of the multi-scale information and the geometry distribution prior to enrich the feature representation for PAS prediction."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,2.3,Lesion-Aware Detection Head,"Motivated by [2,25], we present a Lesion-aware Detection Head (LA-Head) to use high-quality proposals to iteratively refine the lesion box labels.In this work, we regard PAS detection as a multiple instance learning (MIL) problem. In the standard paradigm for object detection, a MIL method treats an image as a bag and proposals in the image as instances [13,28]. Different from previous studies, we treat an object as a bag B i and proposals corresponding to the object as instances P i = {p j i } N j=1 , where N is the number of instances. Each bag has a label y i ∈ {1, -1}, where y i = 1 denotes an inaccurate ground-truth box containing at least one lesion candidate and y i = -1 denotes a background box without lesions.As shown in Fig. 1(c), our lesion-aware MIL method can be separated into three alternative parts: detector μ(θ d ), instance selector φ(θ s ) and instance classifier ψ(θ c ), where θ d , θ s and θ c are parameters to be learned. First, the detector generates lesion instances based on proposal features. Then for each instance p j i , the instance selector computes the confidence score φ(p j i ; θ s ). Considering that the classification scores are not strongly correlated with localization quality [11], we select the most positive instance p j * i as follows:where j * is the index of the instance with the highest score. To leverage the classification information of predicted bounding boxes, we fuse the most positive instance p j * i and the ground-truth instance g i to obtain a high-quality positive instance for training. The final selected instance is defined as below:where δ s is the weighting factor. Intuitively, the weight assigned to p * i should be higher when φ(p j * i ; θ s ) is larger, and the weight assigned to g i should have a lower bound γ to ensure that g i can provide prior knowledge. Thus δ s is calculated as:where β and γ are hyper-parameters. With the selected instances, instance classifier is trained to classify other instances as positive or negative. Then these proposals can serve as refined bounding box annotations for the detector. Furthermore, the detector generates instances to train the detection head and highquality proposals can improve the detection performance. Thus this enables the self-feedback relationship between the detector and the LA-Head.During training, instance selector, instance classifier and detector are jointly optimized based on the loss function:where L s is the loss of instance selector which is defined as hinge loss:The second term L c is the loss of instance classifier and denoted as follows:where ψ(p j i ; θ c ) is the probability that p j i contains lesions, and y j i is the label of p j i and calculated as follows:where τ * i is the dynamic IoU threshold of p j i . The third term L d is defined as:where 1(x) is the indicator function, set to 1 if x = 1; otherwise, set to 0. We adopt the smooth L 1 loss as the loss function L reg for regression."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,3,Experiments and Results,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,3.1,"Dataset, Evaluation Metrics and Implementation Details","Dataset. Owing to the lack of open-source PAS dataset, our experiments are performed on a private dataset. We collected 110 placenta MRI scans of different patients upon the approval of Xiangya Hospital of Central South University. All T2-weighted image volumes were sliced along the sagittal plane. Two experienced radiologists with 20 and 14 years of clinical experience in medical imaging and PAS diagnosis selected the slices with PAS and manually annotated the lesion bounding boxes using LabelImg [24]. We finally obtain 312 2D MR images with a resolution of 640 × 640 px. To verify the robustness of our network, we simulate inaccurate annotations by perturbing clean bounding box labels. With the noise rate λ, we randomly shift and scale the ground-truth box {x i , y i , w i , h i } as follows:where Δx, Δy, Δw, and Δh follow the uniform distribution U (-λ, λ).Evaluation Metrics. We adopt Average Precision (AP) and Sensitivity to evaluate the detection performance. In detail, AP is calculated over IoU threshold ranges from 0.25 to 0.95 at an interval of 0.05. Sensitivity denotes the proportion of correct prediction results in all ground-truths and is computed with an IoU threshold of 0.25 at 1 false positive per image.Implementation Details. The proposed network is implemented with MMDetection 2.4.0 [4] and Pytorch 1.6 on NVIDIA GeForce RTX 1080Ti. We took FPN with ResNet50 [8] as backbone. The framework was trained using the SGD optimizer, where the initial learning rate, momentum, and weight decay were 5e -4, 0.9, and 1e -3, respectively. We adopt a batch size of 1 and the epochs of 120.The hyper-parameters are set as α = 2, β = 0.2, γ = 0.8 and R = 2."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,3.2,Ablation Study,"The results of ablative experiments are listed in Table 1. Faster R-CNN is set as the baseline. We first explore the impact of GA-LA and GA-RF under accurate annotations. Compared with the baseline, two components bring 3.4% and 5.0% mAP gains separately. Another 3.7% mAP improvement is achieved when applied together. This finding indicates that geometry information of lesions can behave as effective prior for PAS detection. We then add LA-Head and conduct experiments under inaccurate annotations. It outperforms the baseline by at least 1.3% mAP, which demonstrates that the classification information of predictions is beneficial to alleviate the impact of noisy bounding box labels. We subsequently combine all key designs and obtain the optimal 30.3% and 26.8% mAP under 10% and 20% noise levels, outperforming the baseline by 6.1% and 5.8% respectively. The results reveal that high-quality predictions with geometry guidance can provide precise supervision for LA-Head. In addition, our method brings more obvious improvement under high annotation noise, which further proves its robustness to inaccurate annotations."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,3.3,Comparison with State-of-the-Art Methods,"We compare the geometry-adaptive network with seven object detection methods on both clean and noisy datasets. Faster R-CNN, Cascade R-CNN [3] and Dynamic R-CNN [30] are anchor-based methods, while FCOS is an anchor-free detector. ATSS [31] and SA-S [9] are dynamic label assignment strategies. Note that SA-S also uses the object shape information to select samples. AugFPN [6] is a variant of FPN and contains a soft RoI selection module. The quantitative results are reported in Table 2. We first analyze the results of experiments under clean data. Compared with the general anchor-based and anchor-free methods, our method outperforms them by a large margin, thereby verifying that higher-quality predictions are generated under the guidance of geometry information. Compared with dynamic label assignment strategies, the improvement of mAP by 5.9%, 2.6% and sensitivity by 9.6%, 1.4% demonstrated that our GA-LA strategy is simple but effective. Although SA-S also considers the shape of objects, the proposed method still achieves superior performance, likely because our model benefits from the two-stage structure. Compared with the feature fusion method, our approach performs favorably against heuristicguided RoI selection of AugFPN. An intuitive explanation is that the optimal feature may be difficult to obtain using heuristic-guided method. Meanwhile our GA-RF module can adaptively generate representation according to the geometry prior of PAS lesions. We then analyze experimental results under 10% noise level data. Our geometry-adaptive network achieves consistent performance improvements compared with other state-of-the-art detectors. The result reveals that the high-quality predictions can serve as precise supervision signals for learning on inaccurate annotations. Figure 2 provides the visualization results of Faster R-CNN, Dynamic R-CNN, SA-S and AugFPN. The examples show that our method can generate PAS bounding boxes with more accurate shape and localization, which is consistent with the previous analysis. "
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,4,Conclusion,"In this paper, we present a geometry-adaptive network for robust PAS detection. We point out that the geometry prior missing problem and inaccurate annotations could deteriorate the performance of detectors. To solve the problem, a Geometry-adaptive Label Assignment strategy (GA-LA) and a Geometryadaptive RoI Fusion (GA-RF) module are proposed to fully utilize the geometry prior of lesions to predict high-quality proposals. Moreover, a Lesion-aware Detection Head (LA-Head) is developed to alleviate the impact of inaccurate annotations by leveraging the classification information of predicted boxes. The experimental results under both clean and noisy labels demonstrate that our method surpasses other state-of-the-art detectors."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Fig. 1 .,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Fig. 2 .,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Table 1 .,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Table 2 .,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Clinical Applications -Breast,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,1,Introduction,"Breast cancer is a serious health problem with high incidence and wide prevalence for women throughout the world [1,2]. Regular screening and early detection are crucial for effective diagnosis and treatment, and hence for improved prognosis and survival rate [3,4]. Clinical researches have shown that ultrasound imaging is an effective tool for screening breast cancer, due to its critical characteristics of non-invasiveness, non-radiation and inexpensiveness [5][6][7]. In clinical diagnosis, delineating tumor regions from background is a crucial step for quantitative analysis [8]. Manual delineation always depends on the experience of radiologists, which tends to be subjective and time-consuming [9,10].Therefore, there is a high demand for automatic and robust methods to achieve accurate breast tumor segmentation. However, due to speckle noise and shadows in ultrasound images, breast tumor boundaries tend to be blurry and are difficult to be distinguished from background. Furthermore, the boundary and size of breast tumors are always variable and irregular [11,12]. These issues pose challenges and difficulties for accurate breast tumor segmentation in ultrasound images.Various approaches based on deep learning have been developed for tumor segmentation with promising results [13][14][15][16][17][18][19]. Su et al. [13] designed a multi-scale U-Net to extract more semantic and diverse features for medical image segmentation, using multiple convolution sequences and convolution kernels with different receptive fields. Zhou et al. [14] raised a deeply-supervised encoder-decoder network, which is connected through a series of nested and dense skip pathways to reduce semantic gap between feature maps. In [15], a multi-scale selection and multi-channel fusion segmentation model was built, which gathers global information from multiple receptive fields and integrates multi-level features from different network positions for accurate pancreas segmentation. Oktay et al. [16] proposed an attention gate model, which is capable of suppressing irrelevant regions while highlighting useful features for a specific task. Huang et al. [17] introduced a UNet 3+ for medical image segmentation, which incorporates low-level and high-level feature maps in different scales and learns full-scale aggregated feature representations. Liu et al. [18] established a convolution neural network optimized by super-pixel and support vector machine, segmenting multiple organs from CT scans to assist physicians diagnosis. Pei et al. [19] introduced channel and position attention module into deep learning neural network to obtain contextual information for colorectal tumors segmentation in CT scans. However, although these proposed models have achieved satisfactory results in different medical segmentation tasks, their performances are limited for breast tumor segmentation in ultrasound images due to the low image contrast and blurry tissue boundary.To address these challenges, we present, to the best of our knowledge, the first work to adopt multi-scale features collected from large set of clinical ultrasound images for breast tumor segmentation. The main contributions of our work are as follows: (1) we propose a well-pruned simple but effective network for breast tumor segmentation, which shows remarkable and solid performance on large clinical dataset; (2) our large pretrained model is evaluated on two additional public datasets without fine-tuning and shows extremely stabilized improvement, indicating that our model has outstanding generalizability and good robustness against multi-site data data."
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,2,Method,"We demonstrate the architecture of our proposed network in Fig. 1. It is based on the UNet [20] backbone. In order to collect multi-scale rich information for tumor tissues, we propose to use GAN [21]-based multi-scale deep supervision. In particular, we apply similarity constraint for each stage of the UNet decoder to obtain consistent and stable segmentation maps. Instead of using Dice score in the final layer of UNet, we also use Dice loss on each of the decoder stages. Besides, we integrate an adversarial loss as additional constraint to penalize the distribution dissimilarity between the predicted segmentation map and the ground truth. In the framework of GAN, we take our segmentation network as the generator and a convolutional neural network as the discriminator. The discriminator consists of five convolution layers with the kernel sizes of 7 × 7, 5 × 5, 4 × 4, 4 × 4 and 4 × 4. Therefore, we formulate the overall loss for the generator, namely the segmentation network, aswhere SN represents the segmentation network and CN represents the involved convolutional network. θ SN and θ CN refer to the parameters in the segmentation and convolutional network, respectively. P (n) represents the segmentation maps obtained from the n-th stage in the segmentation network, and G refers to the corresponding ground truth. p SN (proba) denotes the distribution of probability maps. CN θCN (SN θSN (P (2) )) represents the probability for the input of CN coming from the predicted maps rather than the real ones. The parameters α 1 is empirically set as 1, α 2 , α 3 , α 4 are set as 0.1, and β 1 , β 2 , β 3 and β 4 are set as 0.05. It should be noted that, in UNet, there are 4 stages and hence we employ 4 CNNs for each of them without sharing their weights. Meanwhile, the adversarial loss for each of the CNN is defined as:where p CN (truth) denotes the distribution of original samples. CN θCN (G) represents the probability for the input of CN coming from the original dataset.In the implementation, we update the segmentation network and all the discriminators alternatingly in each iteration until both the generator and discriminators are converged."
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3,Experiments,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.1,Dataset and Implementation Details,"We collected 10927 cases for this research from Yunnan Cancer Hospital. Each scan is with resolution of 1 × 1 mm 2 and size of 512 × 480. The breast tumors of each case are delineated by three experienced experts. Five-fold cross validation is performed on the dataset in all experiments to verify our proposed network. For external validation, we further test our model on two independent publicly-available datasets collected by STU-Hospital (Dataset 1) [22] and SYU-University (Dataset 2) [23]. In order to comprehensively evaluate segmentation efficiency of our model, Dice Similarity Coefficient (DSC), Precision, Recall, Jaccard, and Root Mean Squared Error (RMSE) are used as evaluation metrics in this work. Our proposed algorithm is conducted on PyTorch, and all experiments are performed on NVIDIA Tesla A100 GPU. We use Adam optimizer to train the framework with an initial learning rate of 10 -4 . The epochs to train all models are 100 and the batch size in training process is set as 4."
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.2,Comparison with State-of-the-Art Methods,"To verify the advantages of our proposed model for breast tumor segmentation in ultrasound images, we compare our deep-supervised convolutional network with the state-ofthe-art tumor segmentation methods, including DeepRes [24], MSUNet [13], UNet++ [14], SegNet [25], AttU-Net [16], U 2 -Net [26] and UNet 3+ [17]. The comparison experiments are carried on a large-scale clinical breast ultrasound dataset, and the quantitative results are reported in Table 1. It is obvious that our proposed model achieves the optimal performance compared with other segmentation models. For example, our method obtains a DSC score of 80.97%, which is 5.81%, 9.13%, 7.77%, 4.13%, 7.51%, 2.19%, and 3.83% higher than other seven models. These results indicate the effectiveness of the proposed model in delineating breast tumors in ultrasound images.Representative segmentation results using different methods are provided in Fig. 2. The probability maps predicted by our model are more consistent with the ground truth, especially in the tiny structures which are difficult to capture. This verifies the superior ability of the proposed model in maintaining detailed edge information compared with state-of-the-art methods.  "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.3,Ablation Studies,"We demonstrate the efficacy of the deep supervision strategy using ablation studies. Four groups of frameworks (Stage I, Stage II, Stage III and Stage IV) are designed, with the numerals denoting the level of deep supervision counting from the last deconvolutional layer.We test these four frameworks on the in-house breast ultrasound dataset, and verify their segmentation performance using the same five evaluation criteria. The evaluation metrics from all cases are presented by the scatter plots in Fig. 3. The obtained quantitative results are shown in Table 2, where Stage IV model achieves the optimal DSC, Precision, Recall, and Jaccard. All these results draw a unanimous conclusion on the relationship between these four frameworks. That is, the segmentation ability of the proposed Stage IV is ameliorated from every possible perspective. Moreover, Stage IV obtains minimal RMSE compared with other three models (0.68 mm vs 0.84 mm, 0.82 mm, 0.75 mm), which means better matching of the predicted maps from Stage IV with the corresponding ground truth. All these comparison results verify the superiority of deep supervision for breast tumor segmentation in ultrasound images.  "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,4,Conclusion,"In this paper, we have developed a large pre-trained model for breast tumor segmentation from ultrasound images. In particular, two constraints are proposed to exploit both image similarity and space correlation information for refining the prediction maps. Moreover, our proposed deep supervision strategy is used for quality control at each decoding stage, optimizing prediction maps layer-by-layer for overall performance improvement. Using a large clinical dataset, our proposed model demonstrates not only state-of-the-art segmentation performance, but also the outstanding generalizability to new ultrasound data from different sites. Besides, our large pre-trained model is general and robust in handling various tumor types and shadow noises in our acquired clinical ultrasound images. This also shows the potential of directly applying our model in real clinical applications."
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Fig. 1 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Fig. 2 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Fig. 3 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Table 1 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Table 2 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,97 ± 0.88 81.64 ± 1.30 82.19 ± 1.61 68.83 ± 1.38 0.68 ± 0.06 3.4 Performance on Two External Public Datasets In,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Table 3 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,1,Introduction,"Mitral Regurgitation. Mitral regurgitation (MR) [7] is a valvular heart disease in which the mitral valve does not close completely during systole when the left ventricle contracts, causing regurgitation -leaking of blood backwards -from the left ventricle (LV), through the mitral valve, into the left atrium (LA) -Fig. 1. MR can be caused by either organic or functional mechanisms [6], with organic MR leading to atrial and annular enlargement and functional MR increasing atrial pressure. As MR progresses, it may cause arrhythmia, shortness of breath, heart palpitations and pulmonary hypertension [14]. Left undiagnosed and untreated, MR may cause significant hemodynamic instability and congestive heart failure which can lead to death [17], while acute MR usually necessitates immediate medical intervention [22]. Thus, early detection and assessment of MR are crucial for optimal treatment outcomes, with the best short-term and long-term results obtained in asymptomatic patients operated on in advanced repair centers with low operative mortality (<1%) and high repair rates (≥80-90%) [7]. MR Diagnosis. MR is often only detected following symptom onset. Among patients with asymptomatic MR, quantitative grading of mitral regurgitation is a powerful indicator for clinical treatment such as immediate cardiac surgery [8]. Clinically, MR is usually diagnosed with doppler echocardiography, with cardiovascular magnetic resonance (CMR) subsequently used to assess the MR severity and to accurately quantify the regurgitant volume, one of the indicators of severity [20]. Most studies that have evaluated CMR for assessing the mitral regurgitant volume use the difference between left ventricular stroke volume (LVSV) and forward stroke volume (FSV). LVSV is usually estimated with the short-axis (SA) view CMR -a 4-D tensor -while FSV is most commonly determined by aortic phase-contrast velocity-encoding images [20]. This diagnosis and assessment process requires continuous involvement from expert clinicians along with specific order and post-processing for the phase-contrast images of the proximal aorta or main pulmonary artery during the acquisition of the CMR data. The associated expense with this standard diagnostic procedure thus poses an obstacle to the large-scale screening for MR in the general population.Towards Machine Learning for MR Diagnosis. Although quantitatively assessing mitral regurgitant volume requires specific CMR imaging sequences and expert analysis, four-chamber (4CH) CMR images provide a comprehensive view of all four heart chambers, including the mitral valve as it opens and closes, as shown in Fig. 1. Thus, we propose to train a model that uses 4CH CMR to automatically diagnose MR, making wide screening possible. As training data, we use the long axis 4CH CMR imaging data from the UK Biobank [1], from over 30,000 subjects, out of which N = 704 were labeled by an expert cardiologist. While the 4CH view has the potential to identify MR when the regurgitant jet is visible, the imaging is not accompanied by comprehensive annotations or diagnoses of diseases/conditions for individual patients. This is in contrast to Zhang et al. [27], where tens of thousands of annotated color doppler echocardiography images are available for MR assessments. To overcome this difficulty, we rely on weakly supervised and unsupervised methods. Weakly supervised deep learning has proved successful in detecting other heart pathologies. Specifically, Fries et al. [9] proposed a weakly supervised deep learning method (CNN-LSTM) to classify aortic valve malformation from the aortic valve cross section CMR present in the UK Biobank, wherein the critical feature of the aortic valve opening shape was easily extracted from the aortic valve cross section CMR imaging data. Meanwhile, Vimalesvaran et al. [21] proposed a deep learning based pipeline for detecting aortic valve pathology using 3CH CMR imaging from three hospitals. The data set was fully annotated with landmarks, stenotic jets and regurgitant jets. Unlike these prior two studies, we faced the challenge of extracting complex mitral valve regurgitant features from 4CH CMR images with no annotations for landmarks, regurgitant jets or easily extractable features, and only a small amount of binary MR labels. To the best of our knowledge, this is the first study on identifying MR using the 4CH CMR imaging data in an automated pipeline.Our Approach. We propose an automated five stage pipeline named Cardio-vascular magnetic resonance U-Net localized Self-Supervised Predictor (CUSSP). Our approach incorporates several different preexisting neural network architectures in the pipeline, discussed in Sect. 2.3, to address the challenges inherent to the MR classification task. Specifically, we use a U-Net [18] to perform segmentation of the heart chambers, which we then use to localize the area around the mitral valve. We apply histogram equalization to enhance the appearance of the valve. We then use a Barlow Twins [26] network to learn, without supervision, representations of the blood flow around the valve, and a Siamese network [25] to learn differences between instances of MR and non-MR.During training, CUSSP leverages a large amount of unlabeled CMR images, and minimal supervision, in the form of a comparatively small set of MR labels manually annotated by cardiologist. However, at test time CUSSP is fully automated.  "
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,2,Methods,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,2.1,Segmentation of the Cardiac Magnetic Resonance Images,"The CMR imaging data from the UK Biobank that is relevant to MR detection includes long-axis 2-chamber (2CH) view and long-axis 4-chamber (4CH) view, which are all shown in Fig. 2. In addition, the short-axis view CMR provides accurate description of the left ventricle. Both long-axis views and short-axis view are used to estimate heart measurements relevant to the MR detection task, while only the long-axis 4CH view is used for the deep learning models.As a pre-processing step, we performed semantic segmentation on the CMR imaging data, using masks (Fig. 2) generated by a U-Net [18] segmentation model to highlight regions of interest to MR classification. U-Net is currently the leading model architecture for medical imaging segmentation, with various U-Net variants developed for different applications. TernausNet [12] is a U-Net variant that reshapes the U-Net encoder to match the VGG11 architecture, allowing it to use pre-trained VGG11 [19] model weights for faster convergence and improved segmentation results. While most medical imaging segmentation models are trained using supervised learning, weakly supervised segmentation methods such as VoxelMorph augmented segmentation [28], ACNN [16], CCNN [13], graph-based unsupervised segmentation [15], and GAN-based unsupervised segmentation [23,24] also produce comparable segmentation results. For the segmentation of the 4CH, 2CH, SA, and aorta view CMR imaging dataset from the UK Biobank, Bai et al. [2] offer a supervised segmentation model.We manually labeled 100 CMR images for each view and trained a supervised segmentation model with the TernausNet [12] architecture. Then, segmentation outputs, shown in Fig. 2, are used to compute measurements of cardiac structure and function for the four chambers of the heart, as summarized in Table 1. The short-axis view CMR segmentation output is used to estimate the left ventricle and right ventricle measurements, while the long-axis 4CH view and 2CH view outputs are used to estimate the left atrium and right atrium measurements. Specifically, the left atrial volume is estimated using the biplane method with segmentation of both the 2CH and 4CH view, while the right atrial volume is estimated using single plane method with segmentation of the 4CH view."
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,2.2,Baseline Models,"We first considered a random forest (RF) classifier [3] trained for MR classification on the tabular heart measurements in Table 1. We divided the 18 features by body surface area (BSA) prior to training the RF.Next, we developed a deep learning baseline model for MR classification, a weakly supervised CNN-LSTM, following the principles in Fries et al. [9] and operating on the 4CH CMR imaging data. Fries et al. [9] used CMR imaging sequences from the UK Biobank, however, the objective of their work was the identification of aortic valve malformations. Their proposed deep learning architecture -CNN-LSTM -used DenseNet [11] as the CNN of choice to encode CMR imaging frames and the LSTM to encode embeddings of all frames within each sequence for a final classification of aortic valves into tricuspid (normal) and bicuspid (pathological). We point out that our MR classification problem is considerably more challenging due to the lack of direct view of the mitral valve in the CMR imaging data. Moreover, the flow information provided from the 4CH view CMR imaging data is difficult to learn and encode in the model, an issue which we alleviated in the CUSSP framework.The CNN-LSTM pipeline, shown in Fig. 3, includes an image segmentation model, and an image classification model. It uses the 4CH CMR from the UK Biobank. The CMR data is center-cropped using the center of mass of the CMR imaging frames. The resulting sequence provided to the CNN-LSTM, which generates probabilistic labels of MR for the sample.In the CNN-LSTM model architecture, the CNN serves as the frame encoder, which encodes each frame of each sequence into a representation vector. The model uses DenseNet-121 pre-trained on ImageNet as the CNN. To better learn  the attention span of the frame encoder, we added an attention layer to the DenseNet-121 after the first convolutional layer. After the bi-directional LSTM, a multi-layer perceptron (MLP) performs the final classification."
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,2.3,The CUSSP Framework,"Conceptualization. To better encode the blood flow information relevant to MR classification from the 4CH CMR view, we investigated self-supervised representation learning methods which can leverage all the unlabeled CMR sequences present in the UK Biobank. Typically, self-supervised representation learning for visual data involves maximizing the similarity between representations of various distorted versions of a sample. Among the many self-supervised architectures, SimCLR [5], SwAV [4], and BYOL [10], we chose Barlow Twins [26], since it does not require large batches. With the labeled data, our siamese network compares the representation differences between classes by sampling two inputs from different classes as performed in [25]. Thus, our CUSSP MR classification pipeline takes advantage of both self-supervised and supervised representation learning.Test-Time Pipeline. Our CUSSP method consists of five main steps, shown in Fig. 4, the first four for pre-processing, and the last one for prediction, with three stages using network components trained for MR classification. The preprocessing of the CMR imaging sequence is shown in Fig. 8 in the Appendix. We used the segmentation model in Sect. 2.1 to locate the mitral valve and determine the orientation of the left ventricle using the contours and centers of the heart chambers derived from the segmentation output. We then cropped a square patch with the mitral valve at its center positioned horizontally. After cropping, we applied histogram equalization to the patch with the pixel intensity range of the left atrium. The resulting patches are used by the downstream networks.Training Process. The first step involves training a representation encoder in a Barlow Twins network using over 30,000 unlabeled pre-processed sequences. We chose Barlow Twins for its versatility and robustness to distortions such as blurring, different sizes of the relevant areas and intensity variations, which are common in CMR images. We used a ResNet-18 model pretrained on ImageNet as an encoder. Its output vector is 512-dimensional. We selected ResNet-18 because we found that more complex encoders would easily memorize the limited labeled dataset, reducing the effectiveness of the embeddings. The projector network has three fully connected layers, all with 2048 output units. After training the encoder with the unlabeled dataset, it is fine-tuned in a siamese network using a comparatively smaller labeled set, as indicated in Sect. 3. During training, two sequences are sampled from the labeled dataset, with the first being non-MR and the second being either MR or non-MR. The two sequences are passed through the representation encoder to obtain embeddings, which are then used to calculate the contrastive loss. The model is trained to maximize contrastive loss when the two samples are non-MR and MR and to minimize it when both are non-MR.Contrastive learning helps because it uses the very limited labels in our possession to obtain representations focused on encoding differences between classes.Once the representation encoder is fine-tuned in the siamese network, it is combined with a 3-layer multi-layer perceptron (MLP) network to form a classifier, which is trained on the same labeled dataset. To improve computation efficiency and training accuracy, we also tested the framework using a smaller window of 25 frames, since MR occurs between diastole and systole. The code is available at https://github.com/Information-Fusion-Lab-Umass/CUSSP UKB MR."
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,3,Experiments and Discussion,"Experimental Setup. 4CH CMR images were used to conduct experiments with both the CNN-LSTM method and the CUSSP method. We used a total of 704 labeled sequences, with 525 sequences selected for the training set, including 452 labeled as non-MR and 73 labeled as MR. The remaining 179 sequences were used for testing, with 154 labeled as non-MR and 25 labeled as MR. Considering the substantial class imbalance, we opted to use F1 score as our primary evaluation metric, along with precision and recall.Random Forest Classification Results. The random forest model is trained with 10-fold cross validation, with a random search over a parameter grid of 10-100 estimators, 2-16 depth, 2-8 min samples. The optimal hyper-parameter setting found is: 20 estimators, log2 max features, max depth of 9 and a minimum of 2 samples per leaf node. The best results obtained are presented in Table 2. "
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,CUSSP Classification Results,"Fig. 5. The ROC AUC curve and the precisionrecall curve of CUSSP-SIAM-25 from Table 2. The annotated coordinates on the precision-recall curve plot are (recall, precision, F1-score, threshold).We evaluated various configurations of the CUSSP model, to determine the relative benefits of different components. In the first configuration, the ResNet18 model was combined with a 3-layer MLP to train a classifier using the labeled training set after being trained in the Barlow-Twins network with the unlabeled dataset. During the classifier training, the cross-correlation loss from the Barlow-Twins network and the cross-entropy loss from the binary classification were weighted using three different configurations. For CUSSP-1 the crosscorrelation loss has a weight of 0.9, while the cross-entropy loss has a weight of 0.1. For CUSSP-2, the weights are 0.5 and 0.5, while for CUSSP-3 they are 0.1 and 0.9, respectively. Both CUSSP-1 and CUSSP-3 outperform CUSSP-2, though the performance is low, indicating the importance of fine-tuning, described below.In the second scenario, we fine-tuned the encoder with a siamese network to enhance the quality of the encoded representations after training the Barlow Twins network. To prevent overfitting of the model and to limit its capacity, we froze the parameters of all layers except the last block of the ResNet18 encoder when training the siamese network and the classifier. The resulting model, CUSSP-SIAM, showed a significant improvement in performance. In the final configuration CUSSP-SIAM-25, the number of frames in the training sequences was truncated from 50 frames to the 25 frames that correspond to the interval when mitral regurgitation occurs. The results are summarized in Table 2, while the ROC-AUC curve for CUSSP-SIAM-25 are shown in Fig. 5.CUSSP attains an F1 score of 0.69, and an ROC AUC of 0.88, outperforming the CNN-LSTM approach. This is dues to CUSSP's focus on the area around the valve to capture the blood flow through the valve, combining the advantages of Barlow Twins and contrastive learning. Meanwhile, the CNN-LSTM relies on attention, which does not seem to work as well. Additionally, using only the frames relevant to the task reduces the number of parameters and makes the model sample-efficient. This is essential for attaining high performance in the low label setting. In the future, we aim to use the pipeline on the large unlabeled dataset to scan for and adjudicate more MR cases.In conclusion, we present the first automated mitral regurgitation classification system using CMR imaging sequences. The CUSSP model we developed, trained with limited supervision, operates on 4CH CMR imaging sequences and attains an F1 score of 0.69 and an ROC AUC of 0.88, opening up the opportunity for large-scale screening for MR."
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Fig. 1 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Fig. 2 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Fig. 3 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Fig. 4 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Table 1 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Table 2 .,"CNN-LSTM Classification Results.We conducted experiments on the DenseNet-LSTM classification model using various input image sizes, attention layer configurations, and masks. The best CNN-LSTM model attains a F1-score of 0.44, shown in Table2, with further information on the performance under other settings summarized in the Appendix."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,1,Introduction,"Color fundus imaging can detect and monitor various ocular diseases, including agerelated macular degeneration (AMD), glaucoma, and pathological myopia (PM) [29]. In remote and under-developed areas, color funduscopy imaging has become increasingly accessible, allowing healthcare professionals to identify and manage these ocular diseases before they progress to irreversible stages. However, interpreting these images can be challenging for inexperienced or untrained personnel, necessitating the transfer of data to urban healthcare facilities where specialists can make more accurate diagnoses [3,21]. Compressing and decompressing the data without losing spatial information can be utilized in this scenario by the super-resolution algorithm.In a similar manner, color funduscopy imaging has found applications beyond the confines of the planet. For example, astronauts onboard the International Space Station (ISS) utilize this imaging to identify spaceflight-associated neuro-ocular syndrome (SANS) [10]. This condition can occur due to prolonged exposure to microgravity. It affects astronauts during long-duration spaceflight missions and can present with asymmetric/unilateral/bilateral optic disc edema and choroidal folds, which are easily identifiable by Color fundus images [11]. With the low bandwidth communication between ISS and terrestrial station [19], it becomes harder for experts to conduct early diagnosis and take preventive measures. So, super-resolution techniques can be vital in this adverse scenario. Medical experts can visualize and analyze these changes and act accordingly [18].Image super-resolution and compression using different upsampling filters [6,20,23], has been a staple in image processing for a long time. Yet, those conventional approaches required manually designing convolving filters, which couldn't adapt to learning spatial and depth features and had less artifact removal ability. With the advent of deep learning, convolutional neural network-based super-resolution has paved the way for fast and low-computation image reconstructions with less error [4,25,26]. A few years back, attention-based architectures [17,22,27,28] were the state-of-the-art for any image super-resolution tasks. However, with the introduction of shifted window-based transformer models [8,13], the accuracy of these models superseded attention-based ones with regard to different reconstruction metrics. Although swin-transformer-based models are good at extracting features of local patches, they lose the overall global spatial and depth context while upsampling with window-based patch merging operations.Our Contributions: Considering all the relevant factors, we introduce the novel Swin-FSR architecture. It incorporates low-frequency feature extraction, deep feature extraction, and high-quality image reconstruction modules. The low-frequency feature extraction module employs a convolution layer to extract low-level features and is then directly passed to the reconstruction module to preserve low-frequency information. Our novelty is introduced in the deep feature extraction where we incorporated Depthwise Channel Attention block (DCA), improved Residual Swin-transformer Block -(iRSTB), and Spatial and Channel Attention block (SCA). To validate our work, we compare three different SR architectures for four Fundus datasets: iChallenge-AMD [5], iChallenge-PALM [7], G1020 [1] and SANS. From Fig. 2 and Fig. 3 , it is apparent that our architecture reconstructs images with high PSNR and SSIM."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2,Methodology,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2.1,Overall Architecture,"As shown in Fig. 1, SwinFSR consists of four modules: low-frequency feature extraction, deep patch-level feature extraction, deep depth-wise channel attention, and highquality (HQ) image reconstruction modules. Given a low-resolution (LR) image I LR R H×W ×C . Here, H = height, W = width, c = channel of the image. For lowfrequency feature extraction, we utilize a 3×3 convolution with stride= 1 and is denoted as H LF , and it extracts a feature F LF R H×W ×Cout with which illustrated in Eq. 1.It has been reported that the convolution layer helps with better spatial feature extraction and early visual processing, guiding to more steady optimization in transformers [24]. Next, we have two parallel branches of outputs as denoted in Eq. 2. Here, H DCA and H iRST B are two new blocks that we propose in this study, and they both take the low-feature vector as F LF input and generate two new features namely, F SF and F P LF . We elaborate this two blocks in Subsect. 2.2 and 2.3Finally, we combine all three features from our previous modules, namely, F LF , F SF , and F P LF and apply a final high-quality (HQ) image reconstruction module to generate a high-quality image I HQ as given in Eq. 3.where H REC is the function of the reconstruction block. Our low-frequency block extracts shallow features, whereas the two parallel depth-wise channel-attention and improved residual swin-transformer blocks extract spatially and channel-wise dense features extracting lost high-frequencies. With these three parallel residual connections, SwinFSR can propagate and combine the high and low-frequency information to the reconstruction module for better super-resolution results. It should be noted that the reconstruction module consists of a 1 × 1 convolution followed by a Pixel-shuffle layer to upsample the features."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2.2,Depth-Wise Channel Attention Block,"For super-resolution architectures, channel-attention [2,15,28] is an essential robust feature extraction module that helps these architectures achieve high accuracy and more visually realistic results. In contrast, recent shifted-window-based transformers architecture [12,13] for super-resolution do not incorporate this module. However, a recent work [8] utilized a cross-attention module after the repetitive swin-transformer layers.One of the most significant drawbacks of the transformer layer is it works on patch-level tokens where the spatial dimensions are transformed into a linear feature. To retain the spatial information intact and learning dense features effectively, we propose depthwise channel attention given in Eq. 4.Here, δ is ReLU activation, and φ is Sigmoid activation functions. The regular channelattention utilizes a 2D Conv with 1 × 1 × C weight vector C times to create output features 1×1×C. Given that adaptive average pooling already transforms the dimension to 1 × 1, utilizing a spatial convolution is redundant and shoots up computation time. To make it more efficient, we utilize depth-wise attention, with 1 × 1 weight vector applied on each of the C features separately, and then the output is concatenated to get our final output 1 × 1 × C. We use four DCA blocks in our architecture as given in Fig. 1."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2.3,Improved Residual Swin-Transformer Block,"Swin Transformer [16] incorporates shifted windows self-attention (SW-MSA), which builds hierarchical regional feature maps and has linear computation complexity compared to vision transformers with quadratic computation complexity. Recently, Swin-IR [13] adopted a modified swin-transformer block for different image enhancement tasks such as super-resolution, JPEG compression, and denoising while achieving high PSNR and SSIM scores. The most significant disadvantage of this block is the Multi-layer perceptron module (MLP) after the post-normalization layer, which has two linear (dense) layers. As a result, it becomes computationally more expensive than a traditional 1D convolution layer. For example, a linear feature output from a swin-transformer layer having depth D, and input channel, X in and output channel, X out will have a total number of parameters, D × X in × X out . Contrastly, a 1D convolution with kernel size, K = 1, with the same input and output will have less number of parameters, 1 × X in × X out . Here, we assign bias, b = 0. So, the proposed swin-transformer block can be defined as Eq. 5 and is illustrated in Fig. 1 as STLc block.Here, σ is Layer-normalization and ConvMLP has two 1D convolution followed by GELU activation. To capture spatial local contexts for patch-level features we utilize a patch-unmerging layer in parallel path and incorporate SCA (spatial and channel attention) block. The block consists of a convolution (k = 1, s = 1), a dilated convolution (k = 3, d = 2, s = 1) and a depth-wise convolution (k = 1, s = 1) layer. Here, k= kernel, d =dilation and s =stride. Moreover all these features are combined to get the final ouptut. By combining repetitive SCA, ST Lc blocks and a identity mapping we create our improved reisdual swin-transformer block (iRSTB) illustrated in Fig. 1. In Swin-FSR, we incorporate four iRSTB blocks."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2.4,Loss Function,"For image super resolution task, we utilize the L1 loss function given in Eq. 6. Here, I RHQ is the reconstructed output of SwinFSR and I HQ is the original high-quality image.3 Experiments"
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.1,Dataset,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,Data use Declaration and Acknowledgment:,"The AMD and PALM dataset were released as part of REFUGE Challenge, PALM Challenge. The G1020 was published as technical report and benchmark [1]. The authors instructed to cite their work [5,7,14] for usage. The SANS data is privately held and is provided by the National Aeronautics and Space Administration(NASA) with Data use agreement 80NSSC20K1831."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.2,Hyper-parameter,"We utilized L1 loss for training our models for the super-resolution task. For optimizer, we used Adam [9], with learning rate α = 0.0002, β 1 = 0.9 and β 2 = 0.999. The batch size was b = 2, and we trained for 200 epochs for 8 h with NVIDIA A30 GPU. We utilize PyTorch and MONAI library monai.io for data transformation, training and testing our model. The code repository is provided in this link. "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.3,Qualitative Evaluation,"We compared our architecture with some best-performing CNN and Transformer based SR models, including RCAN [28], ELAN [27], and SwinIR [13] as illustrated in Fig. 2 and Fig. 3. We trained and evaluated all four architectures using their publicly available source code on the four datasets. SwinIR utilizes residual swin-transformer blocks with identity mapping for dense feature extractions. In contrast, the RCAN utilizes repetitive channel-attention blocks for depth-wise dense feature retrieval. Similarly, ELAN combines multi-scale and long-range attention with convolution filters to extract spatial and depth features. In Fig. 2, we illustrate ×2 reconstruction results for all four architectures. By observing, we can see that our model's vessel reconstruction is more realistic for ×2 factor samples than other methods. Specifically for AMD and SANS, the degeneration is noticeable. In contrast, ELAN and RCAN fail to accurately reconstruct thinner and smaller vessels.In the second experiment, we show results for ×4 reconstruction for all SR models in Fig. 3. It is apparent from the figure that our model's reconstruction is more realist than other transformer and CNN-based architectures, and the vessel boundary is sharp, containing more degeneration than SwinIR, ELAN and RCAN.. Especially for AMD , G1020 and PALM, the vessel edges are finer and sharper making it easily differentiable. In contrast, ELAN and RCAN generate pseudo vessels whereas SwinIR fails to generate some smaller ones. For SANS images, the reconstruction is much noticable for the ×4 than ×2. "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.4,Quantitative Bench-Marking,"For quantitative evaluation, we utilize Peak Signal-to-Noise-Ratio (PSNR) and Structural Similarity Index Metric (SSIM), which has been previously employed for measuring similarity between original and reconstructed images in super-resolution tasks [13,27,28]. We illustrate quantitative performance in Table . 1 between SwinFSR and other state-of-the-art methods: SwinIR [13], RCAN [28], and ELAN [27]. Table . 1 shows that SwinFSR's overall SSIM and PSNR are superior to other transformer and CNN-based approaches. For ×2 scale reconstruction, SwinIR achieves the secondbest performance. Contrastly, for ×4 scale reconstruction, RCAN outperforms SwinIR while scoring lower than our SwinFSR model for PSNR and SSIM.Table 1. Quantitative comparison on AMD [5], PALM [7,14], G1020 [1] "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.5,Clinical Assessment,"We carried out a diagnostic assessment with two expert ophthalmologists and test samples of 80 fundus images (20 fundus images per disease classes: AMD, Glaucoma, Pathological Myopia and SANS for both original x2 and x4 images, and superresolution enhanced images). Half of the 20 fundus images were control patients without disease pathologies; the other half contained disease pathologies. The clinical experts were not provided any prior pathology information regarding the images. And each of the experts was given 10 images with equally distributed control and diseased images for each disease category. The accuracy and F1-score for original x4 images are as follows, 70.0% and 82.3% (AMD), 75% and 85.7% (Glaucoma), 60.0% and 74.9% (Palm), and 55% and 70.9% (SANS). The accuracy and F1-score for original x2 are as follows, 80.0% and 88.8% (AMD), 80% and 88.8% (Glaucoma), 70.0% and 82.1% (Palm), and 65% and 77.4% (SANS). The accuracy and F1-score for our model Swin-FSR's output from ×4 images are as follows, 90.0% and 93.3% (AMD), 90.0% and 93.7% (Glaucoma), 75.0% and 82.7% (Palm), and 75% and 81.4% (SANS). The accuracy and F1-score for Swin-FSR's output from ×2 images are as follows, 90.0% and 93.3% (AMD), 90.0% and 93.7% (Glaucoma), 80.0% and 85.7% (Palm), and 80% and 85.7% (SANS).We also tested SWIN-IR, ELAN, and RCAN models for diagnostic assessment, out of which SWIN-IR upsampled images got the best results. For x4 images, the model's accuracy and F-1 score are 80% and 87.5% (AMD), 85.0% and 90.3% (Glaucoma), 70.0% and 80.0% (Palm), and 70% and 76.9% (SANS). For x2 images, the model's accuracy and F-1 score are 80% and 87.5% (AMD), 80% and 88.8% (Glaucoma), 70.0% and 80.0% (Palm), and 75% and 81.4% (SANS). Based on the above observations, our model-generated images achieves the best result."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.6,Ablation Study,"Effects of iRSTB, DCA, and SCA Number: We illustrate the impacts of iRSTB, DCA, and SCA numbers on the model's performance in Supplementary Fig. 1 (a),(b), and (C). We can see that the PSNR and SSIM become saturated with an increase in any of these three hyperparameters. One drawback is that the total number of parameters grows linearly with each additional block. Therefore, we choose four blocks for iRSTB, DCA, and SCA to achieve the optimum performance with low computation cost.Presence and Absence of iRSTB, DCA, and SCA Blocks: Additionally, we provide a comprehensive benchmark of our model's performance with and without the novel blocks incorporated in Supplementary Table 1. Specifically, we show the performance gains with the usage of an improved residual swin-transformer block (iRSTB) and depth-wise channel attention (DCA). As the results illustrate, by comprising these blocks, the PSNR and SSIM reach higher scores."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,4,Conclusion,"In this paper, we proposed Swin-FSR by combining novel DCA, iRSTB, and SCA blocks which extract depth and low features, spatial information, and aggregate in image reconstruction. The architecture reconstructs the precise venular structure of the fundus image with high confidence scores for two relevant metrics. As a result, we can efficiently employ this architecture in various ophthalmology applications emphasizing the Space station. This model is well-suited for the analysis of retinal degenerative diseases and for monitoring future prognosis. Our goal is to expand the scope of this work to include other data modalities."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,Fig. 1 .,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,Fig. 2 .,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,Fig. 3 .,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,1,Introduction,"Retinal fundus images and Optical Coherence Tomography (OCT) are common 2D and 3D imaging techniques used for eye disease screening. Multimodality learning usually provides more complementary information than unimodality learning [3,4,31]. This motivates researchers to integrate multiple modalities to improve the performance of eye disease screening. Current multimodality learning methods can be roughly classified into early, intermediate, and late fusion, depending on the fusion stage [2]. For multimodality ophthalmic image learning, recent works have mainly focused on the early fusion [10,15,23] and intermediate fusion stages [3,4,16,27]. Early fusion-based approaches integrate multiple modalities directly at the data level, usually by concatenating the raw or preprocessed multimodality data. Hua et al. [10] combined preprocessed fundus images and wide-field swept-source optical coherence tomography angiography at the early stage and then extracted representational features for diabetic retinopathy recognition. Intermediate fusion strategies allow multiple modalities to be fused at different intermediate layers of the neural networks. He et al. [9] extracted different modality features with convolutional block attention module [28] and modality-specific attention mechanisms, then concatenated them to realize the multimodality fusion for retinal image classification. However, few studies have explored multimodality eye disease screening at the late fusion stage. Furthermore, the above methods do not adequately assess the reliability of each unimodality, and may directly fuse an unreliable modality with others. This could lead to screening errors and be challenging for real-world clinical safety deployment. To achieve this goal, we propose a reliable framework for the multimodality eye disease screening, which provides a confidence (uncertainty) measure for each unimodality and adaptively fuses multimodality predictions in principle.Uncertainty estimation is an effective way to provide a measure of reliability for ambiguous network predictions. The current uncertainty estimation methods mainly include Bayesian neural networks, deep ensemble methods, and deterministic-based methods. Bayesian neural networks [18,21,22] learn the distribution of network weights by treating them as random variables. However, these methods are affected by the challenge of convergence and have a large number of computations. The dropout method has alleviated this issue to a certain extent [12]. Another uncertainty estimation way is to learn an ensemble of deep networks [14]. Recently, to alleviate computational complexity and overconfidence [25], deterministic-based methods [17,19,25,26,32] have been proposed to directly output uncertainty in a single forward pass through the network. For multimodal uncertainty estimation, the Trusted Multi-view Classification (TMC) [8] is a representative method that proposes a new paradigm of multiview learning by dynamically integrating different views at the evidence level. However, TMC has a limited ability to detect Out-Of-Distribution (OOD) samples [11]. This attributes to TMC is particularly weak in modeling epistemic uncertainty for each single view [12]. Additionally, the fusion rule in TMC fails to account for conflicting views, making it unsuitable for safety-critical deployment [30]. To address these limitations, we propose EyeMoSt, a novel evidential fusion method that models both aleatoric and epistemic uncertainty in unimodality, while efficiently integrating different modalities from a multi-distribution fusion perspective.In this work, we propose a novel multimodality eye disease screening method, called EyeMoSt, that conducts Fundus and OCT modality fusion in a reliable manner. Our EyeMoSt places Normal-inverse Gamma (NIG) prior distributions over the pre-trained neural networks to directly learn both aleatoric and epistemic uncertainty for unimodality. Moreover, Our EyeMoSt introduces the Mixture of Student's t (MoSt) distributions, which provide robust classification results with global uncertainty. More importantly, MoSt endows the model with robustness under heavy-tailed property awareness. We conduct sufficient experiments on two datasets for different eye diseases (e.g., glaucoma grading, age-related macular degeneration, and polypoid choroidal vasculopathy) to verify the reliability and robustness of the proposed method. In summary, the key contributions are as follows: 1) We propose a novel multimodality eye disease screening method, EyeMoSt, which conducts reliable fusion of Fundus and OCT modalities.  "
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,2,Method,"In this section, we introduce the overall framework of our EyeMoSt, which efficiently estimates the aleatoric and epistemic uncertainty for unimodality and adaptively integrates Fundus and OCT modalities in principle. As shown in Fig. 1 (a), we first employ the 2D/3D neural network encoders to capture different modality features. Then, we place multi-evidential heads after the trained networks to model the parameters of higher-order NIG distributions for unimodality. To merge these predicted distributions, We derive the posterior predictive of the NIG distributions as Student's t (St) distributions. Particularly, the Mixture of Student's t (MoSt) distributions is introduced to integrate the distributions of different modalities in principle. Finally, we elaborate on the training pipeline for the model evidence acquisition.Given a multimodality eye datasetand the corresponding label y i , the intuitive goal is to learn a function that can classify different categories. Fundus and OCT are common imaging modalities for eye disease screening. Therefore, here M = 2, x i 1 and x i 2 represent Fundus and OCT input modality data, respectively. We first train 2D encoder Θ of Res2Net [7] and 3D encoder Φ of MedicalNet [5] to identify the feature-level informativeness, which can be defined as Θ x i 1 and Φ x i 2 , respectively."
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,2.1,Uncertainty Estimation for Unimodality,"We extend the deep evidential regression model [1] to multimodality evidential classification for eye disease screening. To this end, to model the uncertainty for Fundus or OCT modality, we assume that the observe label y i is drawn from a Gaussian N y i |μ, σ 2 , whose mean and variance are governed by an evidential prior named the NIG distribution:where Γ -1 is an inverse-gamma distribution, γ m ∈ R, δ m > 0, α m > 1, β m > 0 are the learning parameters. Specifically, the multi-evidential heads will be placed after the encoders Θ and Φ (as shown in Fig. 1 (a)), which outputs the prior NIG parameters p m = (γ m , δ m , α m , β m ). As a result, the aleatoric (AL) and epistemic (EP) uncertainty can be estimated by the E σ 2 and the Var [μ], respectively, as:Then, given the evidence distribution parameter p m , the marginal likelihood is calculated by marginalizing the likelihood parameter:Interacted by the prior and the Gaussian likelihood of each unimodality [1], its analytical solution does exist and yields an St prediction distribution as:with "
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,2.2,Mixture of Student's t Distributions (MoSt),"Then, we focus on fusing multiple St Distributions from different modalities. How to rationally integrate multiple Sts into a unified St is the key issue. To this end, the joint modality of distribution can be denoted as:In order to preserve the closed St distribution form and the heavy-tailed properties of the fusion modality, the updated parameters are given by [24]. In simple terms, we first adjust the degrees of freedom of the two distributions to be consistent. As shown in Fig. 1 (b), the smaller values of degrees of freedom (DOF) v has heavier tails. Therefore, we construct the decision value τ m = v m to approximate the parameters of the fused distribution. We assume that multiple St distributions are still an approximate St distribution after fusion. Assuming that the degrees of freedom of τ 1 are smaller than τ 2 , then, the fused St distribution St (y i ; u F , Σ F , v F ) will be updated as:More intuitively, the above formula determines the modality with a stronger heavy-tailed attribute. That is, according to the perceived heavy-tailed attribute of each modality, the most robust modality is selected as the fusion modality. Finally, the prediction and uncertainty of the fusion modality is given by:"
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,2.3,Learning the Evidential Distributions,"Under the evidential learning framework, we expect more evidence to be collected for each modality, thus, the proposed model is expected to maximize the likelihood function of the model evidence. Equivalently, the model is expected to minimize the negative log-likelihood function, which can be expressed as:Then, to fit the classification tasks, we introduce the cross entropy term L CE m :where λ is the balance factor set to 0.5. For further information on the selection of hyperparameter λ, please refer to Supplementary S2. Similarly, for the fusion modality, we first maximize the likelihood function of the model evidence as follows:) Complete derivations of Eq. 8 are available in Supplementary S1.2. Then, to achieve better classification performance, the cross entropy term L CE m is also introduced into Eq. 8 as below:Totally, the evidential learning process for multimodality screening can be denoted as:In this paper, we mainly consider the fusion of two modalities, M = 2."
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,3,Experiments,"Datasets: In this paper, we verify the effectiveness of EyeMoSt on the two datasets. For the glaucoma recognition, We validate the proposed method on the GAMMA [29] dataset. It contains 100 paired cases with a three-level glaucoma grading. They are divided into the training set and test set with 80 and 20 respectively. We conduct the five-fold cross-validation on it to prevent performance improvement caused by accidental factors. Then, we test our method on the in-house collected dataset, which includes Age-related macular degeneration (AMD) and polypoid choroidal vasculopathy (PCV) diseases. They are divided into training, validation, and test sets with 465, 69, and 70 cases respectively. More details of the dataset can be found in Supplementary S2. Both of these datasets are including the paired cases of Fundus (2D) and OCT (3D).  Training Details: Our proposed method is implemented in PyTorch and trained on NVIDIA GeForce RTX 3090. Adam optimization [13] is employed to optimize the overall parameters with an initial learning rate of 0.0001. The maximum of epoch is 100. The data augmentation techniques for GAMMA dataset are similar to [3], including random grayscaling, random color jitter, and random horizontal flipping. All inputs are uniformly adjusted to 256 × 256 and 128 × 256 × 128 for Fundus and OCT modalities. The batch size is 16."
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Compared Methods and Metrics:,"We compare the following six methods: For different fusion stage strategies, a) B-EF Baseline of the early fusion [10] strategy, b) B-IF Baseline of the intermediate typical fusion method, c) M 2 LC [28] of the intermediate fusion method and the later fusion method d) TMC [8] are used as comparisons. B-EF is first integrated at the data level, and then passed through the same MedicalNet [5]. B-IF first extracts features by the encoders (same with us), and then concatenates their output features as the final prediction. For the uncertainty quantification methods, e) MCDO (Monte Carlo Dropout) employs the test time dropout as an approximation of a Bayesian neural network [6]. f) DE (Deep ensemble) quantifies the uncertainties by ensembling multiple models [14]. We adopt the accuracy (ACC) and Kappa metrics for intuitive comparison with different methods. Particularly, expected calibration error (ECE) [20] is used to compare the calibration of the uncertainty algorithms."
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Comparison and Analysis:,"We reported our algorithm with different methods on the GAMMA and in-house datasets in Table 1. First, we compare these methods under the clean multimodality eye data. Our method obtained competitive results in terms of ACC and Kappa. Then, to verify the robustness of our model, we added Gaussian noise to Fundus or OCT modality (σ = 0.1/0.3) on  the two datasets. Compared with other methods, our EyeMoSt maintains classification accuracy in noisy OCT modality, while comparable in noisy Fundus modality. More generally, we added different Gaussian noises to Fundus or OCT modality, as shown in Fig. 2. The same conclusion can be drawn from Fig. 2. This is attributed to the perceived long tail in the data when fused. The visual comparisons of different noises to the Fundus/OCT modality on the in-house dataset can be found in Supplementary S2. To further quantify the reliability of uncertainty estimation, we compared different algorithms using the ECE indicator.As shown in Table 1 and Fig. 2, our proposed algorithm performs better in both clean and single pollution modalities. The inference times of the uncertaintybased methods on two modalities on the in-house dataset are 5.01 s (MCDO), 8.28 s (DE), 3.98 s (TMC), and 3.22 s (Ours). It can be concluded that the running time of EyeMost is lower than other methods. In brief, we conclude that our proposed model is more robust and reliable than the above methods.Understanding Uncertainty for Unimodality/Multimodality Eye Data: To make progress towards the multimodality ophthalmic clinical application of uncertainty estimation, we conducted unimodality and multimodality uncertainty analysis for eye data. First, we add more Gaussian noise with varying variances to the unimodality (Fundus or OCT) in the GAMMA and in-house datasets to simulate OOD data. The original samples without noise are denoted as in-distribution (ID) data. Figure 3 (a) shows a strong relationship between uncertainty and OOD data. Uncertainty in unimodality images increases positively with noise. Here, uncertainty acts as a tool to measure the reliable unimodality eye data. Second, we analyze the uncertainty density of unimodality and fusion modality before and after adding Gaussian noise. As shown in Fig. 3 (b), take adding noise with σ = 0.1 to the Fundus modality on the GAMMA dataset as an example. Before the noise is added, the uncertainty distributions of unimodality and fusion modality are relatively concentrated. After adding noise, the uncertainty distribution of the fusion modality is closer to that of the modality without noise. Hence, EyeMoSt can serve as a tool for measuring the reliable modality in ophthalmic multimodality data fusion. To this end, our algorithm can be used as an out-of-distribution detector and data quality discriminator to inform reliable and robust decisions for multimodality eye disease screening."
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,4,Conclusion,"In this paper, we propose the EyeMoSt for reliable and robust screening of eye diseases using evidential multimodality fusion. Our EyeMoSt produces the uncertainty for unimodality and then adaptively fuses different modalities in a distribution perspective. The different NIG evidence priors are employed to model the distribution of encoder observations, which supports the backbones to directly learn aleatoric and epistemic uncertainty. We then derive an analytical solution to the Student's t distributions of the NIG evidence priors on the Gaussian likelihood function. Furthermore, we propose the MoSt distributions in principle adaptively integrates different modalities, which endows the model with heavy-tailed properties and is more robust and reliable for eye disease screening. Extensive experiments show that the robustness and reliability of our method in classification and uncertainty estimation on GAMMA and in-house datasets are competitive with previous methods. Overall, our approach has the potential to multimodality eye data discriminator for trustworthy medical AI decision-making. In future work, our focus will be on incorporating uncertainty into the training process and exploring the application of reliable multimodality screening for eye diseases in a clinical setting."
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Fig. 1 .,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,2 ),
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,1,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Fig. 2 .,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Fig. 3 .,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Table 1 .,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Acknowledgements,". This work was supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-TC-2021-003), A*STAR AME Programmatic Funding Scheme Under Project A20H4b0141, A*STAR Central Research Fund, the Science and Technology Department of Sichuan Province (Grant No. 2022YFS0071 & 2023YFG0273), and the China Scholarship Council (No. 202206240082)."
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 56.
