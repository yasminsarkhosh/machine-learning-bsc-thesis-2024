Paper Title,Header Number,Header Title,Text
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,1,Introduction,"Surgical scene reconstruction using stereo endoscopes is crucial to Robotic-Assisted Minimally Invasive Surgery (RAMIS). It aims to recover a 3D model of R. Zha and X. Cheng-Equal contribution. Fig. 1. 3D meshes extracted from EndoNeRF [26] and our method. EndoNeRF cannot recover a smooth and accurate surface even with post-processing filters.the observed tissues from a stereo endoscope video. Compared with traditional 2D monitoring, 3D reconstruction offers notable advantages because it allows users to observe the surgical site from any viewpoint. Therefore, it dramatically benefits downstream medical applications such as surgical navigation [21], surgeon-centered augmented reality [18], and virtual reality [7]. General reconstruction pipelines first estimate depth maps with stereo-matching [5,6,13] and then fuse RGBD images into a 3D model [12,14,17,23,28]. Our work focuses on the latter, i.e., how to accurately reconstruct the shape and appearance of deforming surfaces from RGBD sequences.Existing approaches represent a 3D scene in two ways: discretely or continuously. Discrete representations include point clouds [12,14,23,28] and mesh grids [17]. Additional warp fields [9] are usually utilized to compensate for tissue deformation. Discrete representation methods produce surfaces efficiently due to their sparsity property. However, this property also limits their ability to handle complex high-dimensional changes, e.g., non-topology deformation and color alteration resulting from cutting or pulling tissues.Recently, continuous representations have become popular with the blossoming of neural fields, i.e., neural networks that take space-time inputs and return the required quantities. Neural-field-based methods [19,20,22,[25][26][27] exploit deep neural networks to implicitly model complex geometry and appearance, outperforming discrete-representation-based methods. A good representative is EndoNeRF [26]. It trains two neural fields: one for tissue deformation and the other for canonical density and color. EndoNeRF can synthesize reasonable RGBD images with post-processing filters. However, the ill-constrained properties of the density field deter the network from learning a solid surface shape. Figure 1 shows that EndoNeRF can not accurately recover the surface even with filters. While there have been attempts to parameterize other geometry fields, e.g., occupancy fields [19,20] and signed distance function (SDF) fields [25,27], they hypothesize static scenes and diverse viewpoints. Adapting them to surgical scenarios where surfaces undergo deformation and camera movement is confined is non-trivial.We propose EndoSurf: neural implicit fields for Endoscope-based Surf ace reconstruction, a novel neural-field-based method that effectively learns to represent dynamic scenes. Specifically, we model deformation, geometry, and appearance with three separate multi-layer perceptrons (MLP). The deformation network transforms points from the observation space to the canonical space. The  geometry network represents the canonical scene as an SDF field. Compared with density, SDF is more self-contained as it explicitly defines the surface as the zero-level set. We enforce the geometry network to learn a solid surface by designing various regularization strategies. Regarding the appearance network, we involve positions and normals as extra clues to disentangle the appearance from the geometry. Following [25], we adopt unbiased volume rendering to synthesize color images and depth maps. The network is optimized with gradient descent by minimizing the error between the real and rendered results. We evaluate EndoSurf quantitatively and qualitatively on public endoscope datasets. Our work demonstrates superior performance over existing solutions, especially in reconstructing smooth and accurate shapes."
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,2,Method,
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,2.1,Overview,"Problem Setting. Given a stereo video of deforming tissues, we aim to reconstruct the surface shape S and texture C. Similar to EndoNeRF [26], we take as inputs a sequence of frame dataHere T stands for the total number of frames. I i ∈ R H×W ×3 and D i ∈ R H×W refer to the ith left RGB image and depth map with height H and width W . Foreground mask M i ∈ R H×W is utilized to exclude unwanted pixels, such as surgical tools, blood, and smoke. Projection matrix P i ∈ R 4×4 maps 3D coordinates to 2D pixels. t i = i/T is each frame's timestamp normalized to [0, 1]. While stereo matching, surgical tool tracking, and pose estimation are also practical clinical concerns, in this work we prioritize 3D reconstruction and thus take depth maps, foreground masks, and projection matrices as provided by software or hardware solutions.Pipeline. Figure 2(a) illustrates the overall pipeline of our approach. Similar to [25,26], we incorporate our EndoSurf network into a volume rendering scheme. Specifically, we begin by adopting a mask-guided sampling strategy [26] to select valuable pixels from a video frame. We then cast 3D rays from these pixels and hierarchically sample points along the rays [25]. The EndoSurf network utilizes these sampled points and predicts their SDFs and colors. After that, we adopt the unbiased volume rendering method [25] to synthesize pixel colors and depths used for network training. We tailor loss functions to enhance the network's learning of geometry and appearance. In the following subsections, we will describe the EndoSurf network (cf. Sect. 2.2) and the optimization process (cf. Sect. 2.3) in detail."
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,2.2,EndoSurf: Representing Scenes as Deformable Neural Fields,"We represent a dynamic scene as canonical neural fields warped to an observed pose. Separating the learning of deformation and canonical shapes has been proven more effective than directly modeling dynamic shapes [22]. Particularly, we propose a neural deformation field Ψ d to transform 3D points from the observed space to the canonical space. The geometry and appearance of the canonical scene are described by a neural SDF field Ψ s and a neural radiance field Ψ r , respectively. All neural fields are modeled with MLPs with position encoding [16,24]. Neural SDF Field. The shape of the canonical scene is represented by a neural field Ψ s (x c ) → (ρ, f ) that maps a spatial position x c ∈ R 3 to its signed distance function ρ ∈ R and a geometry feature vector f ∈ R F with feature size F .In 3D vision, SDF is the orthogonal distance of a point x to a watertight object's surface, with the sign determined by whether or not x is outside the object. In our case, we slightly abuse the term SDF since we are interested in a segment of an object rather than the whole thing. We extend the definition of SDF by imagining that the surface of interest divides the surrounding space into two distinct regions, as shown in Fig. 2 (b). SDF is positive if x falls into the region which includes the camera and negative if it is in the other. As x approaches the surface, the SDF value gets smaller until it reaches zero at the surface. Therefore, the surface of interest S is the zero-level set of SDF, i.e.,Compared with the density field used in [26], the SDF field provides a more precise representation of surface geometry because it explicitly defines the surface as the zero-level set. The density field, however, encodes the probability of an object occupying a position, making it unclear which iso-surface defines the object's boundary. As a result, density-field-based methods [16,26] can not directly identify a depth via ray marching but rather render it by integrating the depths of sampled points with density-related weights. Such a rendering method can lead to potential depth ambiguity, i.e., camera rays pointing to the same surface produce different surface positions (Fig. 2(b)).Given a surface point p c ∈ R 3 in the canonical space, the surface normal n c ∈ R 3 is the gradient of the neural SDF field Ψ s : n c = ∇ Ψs (p c ). Normal n o of the deformed surface point p o can also be obtained with the chain rule.Neural Radiance Field. We model the appearance of the canonical scene as a neural radiance field Ψ r (x c , v c , n c , f ) → c c that returns the color c c ∈ R 3 of a viewpoint (x c , v c ). Unlike [16,26], which only take the view direction v c and feature vector f as inputs, we also feed the normal n c and position x c to the radiance field as extra geometric clues. Although the feature vector implies the normal and position information, it is validated that directly incorporating them benefits the disentanglement of geometry, i.e., allowing the network to learn appearance independently from the geometry [25,27]."
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,2.3,Optimization,"Unbiased Volume Rendering. Given a camera ray r(h) = o o + hv o at time t in the observed space, we sample N points x i in a hierarchical manner along this ray [25] and predict their SDFs ρ i and colors c i via EndoSurf. The color Ĉ and depth D of the ray can be approximated by unbiased volume rendering [25]:where))/φ(ρ i ), 0) and φ(ρ) = (1 + e -ρ/s ) -1 . Note that s is a trainable standard deviation, which approaches zero as the network training converges.Loss. We train the network with two objectives: 1) to minimize the difference between the actual and rendered results and 2) to impose constraints on the neural SDF field such that it aligns with its definition. Accordingly, we design two categories of losses: rendering constraints and geometry constraints:where λ i=1,••• ,6 are balancing weights. The rendering constraints include the color reconstruction loss L color and depth reconstruction loss L depth :where M (r), { Ĉ, D}, {C, D} and R are ray masks, rendered colors and depths, real colors and depths, and ray batch, respectively.We regularize the neural SDF field Ψ s with four losses: Eikonal loss L eikonal , SDF loss L sdf , visibility loss L visible , and smoothness loss L smooth .(4) Here the Eikonal loss L eikonal [10] encourages Ψ s to satisfy the Eikonal equation [8]. Points x are sampled from the canonical space X . The SDF loss L sdf restricts the SDF value of points lying on the ground truth depths D to zero. The visibility loss L visible limits the angle between the canonical surface normal and the viewing direction v c to be greater than 90 • . The smoothness loss L smooth encourages a surface point and its neighbor to be similar, where is a random uniform perturbation."
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,3,Experiments,
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,3.1,Experiment Settings,"Datasets and Evaluation. We conduct experiments on two public endoscope datasets, namely ENDONERF [26] and SCARED [1] (See statistical details in the supplementary material). ENDONERF provides two cases of in-vivo prostatectomy data with estimated depth maps [13] and manually labeled tool masks. SCARED [1] collects the ground truth RGBD images of five porcine cadaver abdominal anatomies. We pre-process the datasets by normalizing the scene into a unit sphere and splitting the frame data into 7:1 training and test sets.Our approach is compared with EndoNeRF [26], the state-of-the-art neuralfield-based method. There are three outputs for test frames: RGB images, depth maps, and 3D meshes. The first two outputs are rendered the same way as the training process. We use marching cubes [15] to extract 3D meshes from the density and SDF fields. The threshold is set to 5 for the density field and 0 for the SDF field. See the supplementary material for the validation of threshold selection. Five evaluation metrics are used: PSNR, SSIM, LPIPS, RMSE, and point cloud distance (PCD). The first three metrics assess the similarity between the actual and rendered RGB images [26], while RMSE and PCD measure depth map [5,6,14] and 3D mesh [3,4] reconstruction quality, respectively. Implementation Details. We train neural networks per scene, i.e., one model for each case. All neural fields consist of 8-layer 256-channel MLPs with a skip connection at the 4th layer. Position encoding frequencies in all fields are 6, Fig. 3. 2D rendering results on the dynamic case ""ENDONERF-cutting"" and static case ""SCARED-d1k1"". Our method yields high-quality depth and normal maps, whereas those of EndoNeRF exhibit jagged noise, over-smoothed edges (white boxes), and noticeable artifacts (white rings).except those in the radiance field are 10 and 4 for location and direction, respectively. The SDF network is initialized [2] for better training convergence. We use Adam optimizer [11] with a learning rate of 0.0005, which warms up for 5k iterations and then decays with a rate of 0.05. We sample 1024 rays per batch and 64 points per ray. The initial standard deviation s is 0.3. The weights in Eq. 2 are λ 1 = 1.0, λ 2 = 1.0, λ 3 = 0.1, λ 4 = 1.0, λ 5 = 0.1 and λ 6 = 0.1. We train our model with 100K iterations for 9 h on an NVIDIA RTX 3090 GPU."
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,3.2,Qualitative and Quantitative Results,"As listed in Table 1, EndoSurf yields superior results against EndoNeRF. On the one hand, EndoSurf produces better appearance quality than EndoNeRF by ↑ 0.571 PSNR, ↑ 0.025 SSIM, and ↑ 0.020 LPIPS. On the other hand, EndoSurf dramatically outperforms EndoNeRF in terms of geometry recovery by ↓ 0.190 RMSE and ↓ 1.625 PCD. Note that both methods perform better on ENDON-ERF than on SCARED. This is because ENDONERF fixes the camera pose, leading to easier network fitting. Figure 3 shows the 2D rendering results. While both methods synthesize highfidelity RGB images, only EndoSurf succeeds in recovering depth maps with a smoother shape, more details, and fewer artifacts. First, the geometry constraints in EndoSurf prevent the network from overfitting depth supervision, suppressing rough surfaces as observed in the EndoNeRF's normal maps. Second, the brutal post-processing filtering in EndoNeRF cannot preserve sharp details (white boxes in Fig. 3). Moreover, the texture and shape of EndoNeRF are not disentangled, causing depth artifacts in some color change areas (white rings in Fig. 3).Figure 4 depicts the shapes of extracted 3D meshes. Surfaces reconstructed by EndoSurf are accurate and smooth, while those from EndoNeRF are quite noisy. There are two reasons for the poor quality of EndoNeRF's meshes. First, the density field without regularization tends to describe the scene as a volumetric fog rather than a solid surface. Second, the traditional volume rendering causes discernible depth bias [25]. In contrast, we force the neural SDF field to conform to its definition via multiple geometry constraints. Furthermore, we use unbiased volume rendering to prevent depth ambiguity [25].We present a qualitative ablation study on how geometry constraints can influence the reconstruction quality in Fig. 5. The Eikonal loss L eikonal and SDF loss L sdf play important roles in improving geometry recovery, while the visibility loss L visible and smoothness loss L smooth help refine the surface. "
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,4,Conclusion,"This paper presents a novel neural-field-based approach, called EndoSurf, to reconstruct the deforming surgical sites from stereo endoscope videos. Our approach overcomes the geometry limitations of prior work by utilizing a neural SDF field to represent the shape, which is constrained by customized regularization techniques. In addition, we employ neural deformation and radiance fields to model surface dynamics and appearance. To disentangle the appearance learning from geometry, we incorporate normals and locations as extra clues for the radiance field. Experiments on public datasets demonstrate that our method achieves state-of-the-art results compared with existing solutions, particularly in retrieving high-fidelity shapes."
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,,Fig. 2 .,
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,,,
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,,Fig. 4 .,
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,,Fig. 5 .,
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,,,
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,,Table 1 .,
EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 2.
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,1,Introduction,"Surgical treatment remains to this day the main approach to correct severe spinal deformities, which consists of rectifying the alignment of the spine over several Supported by the Canada Research Chairs and NSERC Discovery Grant RGPIN-2020-06558.vertebral bodies with intrinsic forces. However major limiting factors remain the loss in spinal mobility, as well as the increased risk of pain and osteoarthritis in years after surgery [1]. Furthermore, prone positioning during spinal surgery combined with hip extension can greatly affect post-operative outcomes of sagittal alignment and balance, particularly in degenerative surgical cases [2]. Recent literature has demonstrated the importance to preserve the lumbar positioning, as well as the normal sagittal alignment once surgery has been completed [5,7]. Therefore, by optimizing the intra-op posture, surgeons can help to reduce long-term complications such as lower back pain and improve spinal mobility. A predictive model anticipating the upright 3D sagittal alignment of patients during spine surgery [15], such as anterior vertebral tethering (AVT), may not only have an impact on patients to preserve their post-operative balance, but also provide information about patients at risk of degenerative problems in the lumbar region of the spine following surgery [24].To interpret the positioning effects during surgery, identifying features leading to reliable predictions of shape representations in postural changes is a deciding factor as shown in [14], which estimated correlations with geometrical parameters. However, with the substantial changes during surgery such as the prone positioning of the spine and global orientation of the pelvic area, this becomes very challenging. Previous classification methods were proposed in the literature to estimate 3D articulated shape based on variational auto-encoders [19] or with spatio-temporal neural networks [10], combining soft tissue information of the pre-operative models. The objective here is to address these important obstacles by presenting a forecasting framework used in the surgical workflow, describing the spine shape changes within an implicit neural representation, with a clinical target of estimating Cobb angles within 5 • [14].Recently, Neural Radiance Fields (NeRF) [12] have been exploited for synthetic view generation [23]. The implicit representation provided by the parameters of the neural network captures properties of the objects such as radiance field and density, but can be extended to 3D in order to generate volumetric views and shapes from images [4], meshes [16] or clouds of points [9]. As opposed to traditional non-rigid and articulated registration methods based on deep learning for aligning pre-op to intra-op 2D X-rays [3,26], kernel methods allow to map input data into a different space, where subsequently simpler models can be trained on the new feature space, instead of the original space. They can also integrate prior knowledge [22], but have been mostly focused on partial shapes [21]. However, due to the difficulty to apply neural fields in organ rendering applications in medical imaging, and the important variations in the environment and spine posture/shape which can impact the accuracy of the synthesis process, their adoption has been limited. This paper presents an intra-operative predictive framework for spine surgery, allowing to forecast on-the-fly the standing geometry of the spine following corrective AVT procedures. The proposed forecasting framework uses as input the intra-operative 3D spine model generated from a multi-view Transformer network which integrates a pre-op model (Sect. 2.1). The predictive model is based on neural field representations, capturing the articulation variations in a disentangled latent space intra-operative positioning in the prone position. The method regresses a kernel function (Sect. 2.2) based on neural splines to estimate an implicit function warping the spine geometry between various poses. The inference process is regularized with a piecewise geodesic term, capturing the natural evolution of spine morphology (Sect. 2.3). As an output, the framework produces the geometry of the spine at the first-erect examination in a standing posture following surgery, as illustrated in Fig. 1."
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,2,Methods,
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,2.1,Intra-operative 3D Model from Multi-view X-Rays,"The first step of the pipeline consists of inferring a 3D model of the spine in the OR prior to instrumentation, using as input pair of orthogonal C-arm acquisitions I = {I 1 , I 2 } and previously generated pre-op 3D model [6], capturing the spine geometry in a lying prone position on the OR table. We use a multi-view 3D reconstruction approach based on Transformers [20]. The Transformer-based framework integrates both a bi-planar Transformer-based encoder, which combines information and relationships between the multiple calibrated 2D views, and a 3D reconstruction Transformer decoder, as shown in Fig. 2. The 3D-reconstruction Transformer decodes and combines features from the biplanar views generated by the encoder, in order to produce a 3D articulated mesh model with probabilistic outputs for every spatial query token. The decoder's attention layers captures the 2D-3D relationship between the resulting grid node of the mesh and input 2D X-rays. Attention layers in the 3D network on the other hand analyzes correlations between 3D mesh landmarks to learn a 3D representation. A conditional prior using the pre-op model is integrated at the input of the 3D module to inject knowledge about the general geometry. By combining these modules (2D-2D, 2D-3D, 3D-3D) into a unified framework, feature correlations can be processed simultaneously using the attention layers in the encoder/decoder networks, generating as an output shape model S = {s 1 , . . . , s m } with s m as a vertebral mesh with a specific topology for the vertebral level m with an associated articulation vector:representing the m inter-vertebral transformations T i between consecutive vertebrae i and i+1, each consisting of 6 DoF with translation and rotation, expressed with recursive compositions. To accommodate with the Transformer's inability to explore and synthesize multi-view associations at deeper levels, the divergence decay is slowed down within the self-attention layers by augmenting the discrepancy in multi-view embeddings."
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,2.2,Learnable Shape Deformation Kernel,"We use a training set S of N spine models from a surgical population, where each spine model is articulated into pre-operative, prone and first-erect poses (K = 3), leading to a training set of L = N × K spine models from a population of scoliotic patients. We denote S n,k as the spine model n given a particular shape articulation k, where n ∈ {1, . . . , N}, and k ∈ {1, . . . , K}. We also define s i ∈ R 3 as a sample 3D point from a spine shape in S.We first train a kernel based on neural splines using the embeddings generated with a neural network [21], in order to map spine input points from the prone to the first-erect position. Input points s i ∈ S, associated to a shape code φ ∈ R D in latent space, with D indicating the dimensionality, are associated with a feature vector ρ(s i |S, Ω), where Ω conditions the parameters of the neural network ρ using the dataset S. Based on the learned feature points of prone and upright spine models, the data-related kernel is given by:with [c : d] concatenating feature vectors c and d, representing the features of sample points i and j taken from a mesh model, respectively, and K NS representing Neural Spline kernel function. A Convolutional Occupancy Network [17] is used to generate the features from sampled points in both spaces. Once the space around each input point is discretized into a volumetric grid space, PointNet [18] is used in every grid cell which contains points, to extract non-zero features from each 3D cell. These are then fed to a 3D FCN, yielding output features of same size.The neural network's implicit function is determined by finding coefficients α j associated for each point s j , such that:which solves a 2L × 2L linear system to obtain the standing articulations (y j ), with G(S, Ω) as the gram matrix, such that G(S, Ω) ij = K S,Ω (s i , s j ), with s i and s j are the input sample points of the spine models, and regularized by the λ parameter using the identity matrix I. For new feature points s, the regressed function is evaluated using α in the following term:which represents the ShapeNet function and maps feature points s from the original prone embedding onto the standing embedding."
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,2.3,Articulation SDF,"Each training shape S n,k is also associated with an articulation vector field, ξ ∈ R d . The shape embedding φ n is common across all instances n with various articulation vectors. We then train ArticulationNet, where for each instance, the shape code is maintained and each individual code for shape is updated. The articulation vector y i defined in Eq.( 1) is used to define ξ i . The shape function f s trained from kernel regression is integrated into an articulation network [13] using the signed distance function (A-SDF) which is determined by using s ∈ R 3 as a shape's sample point. We recall that φ and ξ are codes representing the shape and inter-vertebral articulation, respectively.The A-SDF for the spine shape provided by f θ is represented with an autodecoder model, which includes the trained shape kernel function f s , while f a describes ArticulationNet:with v ∈ R representing a value of the SDF on the output mesh, where the sign specifies if the sampled spine point falls within or outside the generated 3D surface model, described implicitly with the zero level-set f θ (.) = 0.At training, the ground-truth articulation vectors ξ are used to train the parameters of the model and the shape code. Sample points s are combined with the shape code φ to create a feature vector which is used as input to the shape embedding network. A similar process is used for the articulation code ξ, which are also concatenated with sample points s. This vector is used as input to the articulation network in order to predict for each 3D point s the value of the SDF. Hence, the network uses a fully connected layer at the beginning to map the vector in the latent space, while a classification module is added in the final hidden layer to output the series of vertebrae. We define the loss function using the L1 distance term, regressing the SDF values for the M number of 3D points describing each spine shape using the function f θ :where s m ∈ S is a sample point from the shape space, and v m is the SDF value used as ground-truth for m ∈ {1, . . . , M}. The second loss term evaluates the vertebral constellation and alignment with the inter-vertebral transformations:with p m identifying the vertebral level for sample s m and CE represents the cross-entropy loss. This classification loss enables to separate the constellation of vertebral shapes s m .To ensure the predicted shape is anatomically consistent, a regularization term L r is used to ensure generated models fall near a continuous piecewisegeodesic trajectory defined in a spatio-temporal domain. The regularization term integrates pre-generated embeddings capturing the spatiotemporal changes of the spine geometry following surgery [11]. The piecewise manifold is obtained from nearest points with analogue features in the training data of surgical spine patients in the low-dimensional embedding, producing local regions N (s i ) with samples lying within the shape change spectrum. A Riemannian domain with samples y i is generated, with i a patient model acquired at regular time intervals. Assuming the latent space describes the overall variations in a surgical population and the geodesic trajectory covers the temporal aspects, new labeled points can be regressed within the domain R D , producing continuous curves in the shape/time domain. The overall loss function is estimated as:with β p and β θ weighting the parts and regularization terms, respectively. During training, a random initialization of the shape codes based on Gaussian distributions is used. An optimization procedure is then applied during training for each shape code, which are used for all instances of articulations. Hence the global objective seeks to minimize for all N × K shapes, the following energy:At test time, the intra-operative spine model (obtained in 2.1) is given as input, generating the articulated standing pose, with shape and articulation vectors, using back-propagation. The shape and articulation codes φ and ξ are randomly initialized while the network parameters stay fixed. To overcome divergence problems and avoid a local minima for Eq. ( 9), a sequential process is applied where first articulated codes are estimated while shape outputs are ignored, allowing to capture the global appearance. Then, using the fixed articulated representation, the shape code is re-initialized and optimized only for φ."
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,3,Experiments,"A mono-centric dataset of 735 spine models was used to train the articulated neural kernel field forecasting model. The dataset included pre-, intra-and postoperative thoraco/lumbar 3D models. Pre-and post-op models were generated from bi-planar X-rays using a stereo-reconstruction method (EOS system, Paris, France), integrating a semi-supervised approach generating vertebral landmarks [6]. Each patient in the cohort underwent corrective spine surgery, with a main angulation range of [35 For the neural field model, batch size was set at 32, with a dropout rate of 0.4, a learning rate of 0.003, β p = 0.4, β θ = 0.5, λ = 0.25. The AdamW optimizer was used for both models [8]. Inference time was of 1.2 s on a NVIDIA A100 GPU. We assessed the intra-operative modeling of the spine in a prone position using the Transformer based framework on a separate set of 20 operative patients. Ground-truth models with manually annotated landmarks on pairs of C-arm images were used as basis of comparison, yielding a 3D RMS error of 0.9 ± 0.4 mm, a Dice score (based on overlap of GT and generated vertebral meshes) of 0.94 ± 0.3 and a difference of 0.9 • ± 0.3 in the main spine angulation. These are in the clinical acceptable ranges to work in the field. Finally, the predicted standing spine shape accuracy was evaluated on a holdout set of 81 surgical patients who underwent minimally invasive corrective spine surgery. The cohort had an initial mean angulation of 48 • , and immediate followup exam at 2 weeks. Results are presented in Table 1. For each predicted standing spine model, errors in 3D vertebral landmarks, IoU measures and Chamfer distance, as well as differences in Cobb and lordosis angles were computed for the inferred shapes, and were compared to state-of-the art spatio-temporal and neural fields models. Ablation experiments were also conducted, demonstrating Table 1. 3D RMS errors (mm), IoU (%) Chamfer distance, Cobb angle difference ( o ) and lordosis angle difference ( o ) for the proposed articulated neural kernel field (A-NKF) method, compared to ST-ResNet [25], Convolutional Occupancy Network [17], DenseSDF [16], NeuralPull [9] and ST-Manifold [10]. The ground-truth standing 3D spine models obtained at follow-up from bi-planar X-rays were used as basis for comparison. Bottom rows shows the ablation experiments. statistically significant improvements (p < 0.05) with the kernel and regularization modules to the overall accuracy. We finally evaluated the performance of the model by measuring the 3D RMS errors versus the input size of points sampled from each shape and tethered vertebral levels. We can observe limitations when using a sparser set of samples points at 250 points with a mean error of 2.8 mm, but a significant increase in performance of the kernel-based neural fields using denser inputs for each model (n=1000)."
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,4,Conclusion,"In this paper, we proposed an online forecasting model predicting the first-erect spine shape based on intra-operative positioning in the OR, capturing the articulated shape constellation changes between the prone and the standing posture with neural kernel fields and an articulation network. Geometric consistency is integrated with the network's training with a pre-trained spine correction geodesic trajectory model used to regularize outputs of ArticulationNet. The model yielded results comparable to ground-truth first-erect 3D geometries in upright positions, based on statistical tests. The neural field network implicitly captures the physiological changes in pose, which can be helpful for planning the optimal posture during spine surgery. Future work will entail evaluating the model in a multi-center study to evaluate the predictive robustness and integrate the tool for real-time applications."
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,,Fig. 1 .,
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,,Fig. 2 .,
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,,Fig. 3 .,
Intra-operative Forecasting of Standing Spine Shape with Articulated Neural Kernel Fields,,,
ConTrack: Contextual Transformer for Device Tracking in X-Ray,1,Introduction,"Tracking of interventional devices plays an important role in aiding surgeons during catheterized interventions such as percutaneous coronary interventions (PCI), cardiac electrophysiology (EP), or trans arterial chemoembolization (TACE). In cardiac image-guided interventions, surgeons can benefit from visual guidance provided by mapping vessel information from angiography (Fig. 1b) to fluoroscopy (Fig. 1a) [6,10] for which the catheter tip is used as an anchor point representing the root of the vessel tree structure. This visual feedback helps in reducing the contrast usage [7] for visualizing the vascular structures and it can also aid in effective placements of stents or balloons. Recently, deep learning-based siamese networks have been proposed for medical device tracking [1,4,5]. These networks achieve high frame rate tracking, but are limited by their online adaptability to changes in target's appearance as they only use spatial information. Cycle Ynet [5] uses the cycle consistency of a sequence and relies on a semi-supervised learning approach by doing a forward and a backward tracking. In practice, this method suffers from drifting for long sequences and cannot recover from misdetections because of the single template usage. The closest work related ours is [6], they use a convolutional neural network (CNN) followed by particle filtering as a post processing step. The drawback of this method is that, it does not compensate for the cardiac and respiratory motions as there is no explicit motion model for capturing temporal information. A similar method adds a graph convolutional neural network for aggregating both spatial information and appearance features [3] to provide a more accurate tracking but its effectiveness is limited by its vulnerability to appearance changes and occlusion resulting from detection techniques. Optical flow based network architectures [8] utilize keypoint tracking throughout the entire sequence to estimate the motion of the whole image. However, such approaches are not adapted for tracking a single point, such as a catheter tip.For general computer vision applications, transformer [9] based-trackers have achieved state-of-the-art performance [2,11,12]. Initially proposed for natural language processing (NLP), Transformers learn the dependencies between elements in a sequence, making it intrinsically well suited at capturing global information. Thus, our proposed model consists of a transformer encoder that helps in capturing the underlying relationship between template and search image using self and cross attentions, followed by multiple transformer decoders to accurately track the catheter tip.To overcome the limitations of existing works, we propose a generic, end-toend model for target object tracking with both spatial and temporal context. Multiple template images (containing the target) and a search image (where we would identify the target location, usually the current frame) are input to the system. The system first passes them through a feature encoding network to encode them into the same feature space. Next, the features of template and search are fused together by a fusion network, i.e., a vision transformer. The fusion model builds complete associations between the template feature and search feature and identifies the features of the highest association. The fused features are then used for target (catheter tip) and context prediction (catheter body). While this module learns to perform these two tasks together, spatial context information is offered implicitly to provide guidance to the target detection. In addition to the spatial context, the proposed framework also leverages the temporal context information which is generated using a motion flow network. This temporal information helps in further refining the target location.Our main contributions are as follows: 1) Proposed network consists of segmentation branch that provides spatial context for accurate tip prediction; 2) Temporal information is provided by computing the optical flow between adjacent frames that helps in refining the prediction; 3) We incorporate dynamic templates to make the model robust to appearance changes along with the initial template frame that helps in recovery in case of any misdetection; 4) To the best of our knowledge, this is the first transformer-based tracker for real-time  device tracking in medical applications; 5) We conduct numerical experiments and demonstrate the effectiveness of the proposed model in comparison to other state-of-the-art tracking models."
ConTrack: Contextual Transformer for Device Tracking in X-Ray,2,Methodology,"Given a sequence of consecutive X-ray images {I t } n t=0 and an initial location of the target catheter tip x 0 = (u 0 , v 0 ), our goal is to track the location of the target x t = (u t , v t ) at any time t, t > 0. The proposed model framework is summarized in Fig. 2. It consists of two stages, target localization stage and motion refinement stage. First, given a selective set of template image patches and the search image, we leverage the CNN-transformer architecture to jointly localize the target and segment the neighboring context, i.e., body of the catheter. Next, we estimate the context motion via optical flow on the catheter body segmentation between neighboring frames and use this to refine the detected target location. We detail these two stages in the following subsections."
ConTrack: Contextual Transformer for Device Tracking in X-Ray,2.1,Target Localization with Multi-template Feature Fusion,"To identify the target in the search frame, existing approaches build a correlation map between the template and search features. Limited by definition, the template is a single image, either static or from the last frame tracked result. A transformer naturally extends the bipartite relation between template and search images to complete feature associations which allow us to use multiple templates. This improves model robustness against suboptimal template selection which can be caused by target appearance changes or occlusion.Feature Fusion with Multi-head Attention. In the encoding stage, given a set of template image patches centered around the target {T ti } ti∈H and current frame I s as the search image, we aim to determine the target location by fusing information from multiple templates. H is the set containing historically selected frames for templates. This can be naturally accomplished by multi-head attention (MHA). Specifically, let us denote the ResNet encoder by θ, given the feature map of the search image θ(I s ) ∈ R C×Hs×Ws , and the feature maps of the templates {θ(T ti )}}, we use 1 × 1 convolutions to project and flatten them into d-dimensional vector query, key and value embedding, q s , k s , v s for the search image features and {q ti }, {k ti }, {v ti } for templates features respectively. The attention is based on the concatenated vectors,where Q = Concat(q s , q t1 , q t2 , ..., q tn ),The definition of MHA then follows [9].Joint Target Localization and Context Segmentation. In the decoding stage, we follow [12] and adjust the transformer decoder to a multi-task setting.As the catheter tip represents a sparse object in the image, solely detecting it suffers from class imbalance issue. To guide the catheter tip tracking with spatial information, we incorporate additional contextual information by simultaneously segmenting the catheter body in the same frame. Specifically, two object queries (e 1 , e 2 ) are employed in the decoder, where e 1 defines the position of the catheter tip, and e 2 defines the mask of the catheter body. As is illustrated in Fig. 2 (b), we first calculate similarity scores between decoder and the encoder output via dot product. We then use element-wise product between the similarity scores and the encoder features to promote regions with high similarity. After reshaping the processed features to d × H s × W s , an encoder-decoder structured 6-layer FCN is attached to process the features to probability maps with the same size as the search image. A combination of the binary cross-entropy and the dice loss is then used,where x i , m i represent the ground truth annotation of the catheter tip and mask, xs i , ms i are predictions respectively. Here we use sup-script ""s"" to denote the predictions from this spatial stage. G(x i ; μ, σ) := exp(-x iμ 2 /σ 2 ) is the smoothing function that transfers dot location of x i to probability map. λ * bce , λ * dice ∈ R are hyperparameters that are empirically optimized."
ConTrack: Contextual Transformer for Device Tracking in X-Ray,2.2,Localization Refinement with Context Flow,"In interventional procedures, one common challenge for visual tracking comes from occlusion. This can be caused by injected contrast medium (in the angiographic image) or interferring devices such as sternal wires, stent and additional guiding catheters. If the target is occluded in the search image, using only spatial information for localization is inadequate. To address this challenge, we impose a motion prior of the target to further refine the tracked location. As the target is a sparse object, this is done via optical flow estimation of the context."
ConTrack: Contextual Transformer for Device Tracking in X-Ray,,Context Flow Estimation.,"Obtaining ground truth optical flow in real world data is a challenging task and may require additional hardware such as motion sensors. As such, training a model for optical flow estimation directly in the image space is difficult. Instead, we propose to estimate the flow in the segmentation space, i.e., on the predicted heatmaps of the catheter body between neighboring frames. We use the RAFT [8] model for this task. Specifically, given the predicted segmentation maps m t-1 and m t , we first use a 6-block ResNet encoder g θ to extract the features g θ (m t-1 ), g θ (m t ) ∈ R H f ×W f ×D f . Then we construct the correlation volume pyramid {C i } 3 i=0 , whereHere corr(g θ (m t-1 ), g θ (m t )) ∈ R H f ×W f ×H f ×W f stands for correlation evaluation:which can be computed via matrix multiplication. Starting with an initial flow f 0 = 0, we follow the same model setup as [8] to recurrently refine the flow estimates to f k = f k-1 + f with a gated recurrent unit (GRU) and a delta flow prediction head of 2 convolutional layers. Given the tracked tip result from the previous frame xt-1 , we can then predict the new tip location at time t by warpping with the context flow xf t = f k (x t-1 ). Here we use sup-script ""f "" to denote the prediction by flow warpping.We note here that since the segmentations of the catheter body are sparse objects compared to the entire image, computation of the correlation volume and subsequent updates can be restricted to a cropped sub-image which reduces computation cost and flow inference time. As the flow estimation is performed on the segmentation map, one can simply generate synthetic flows and warp them with the existing catheter body annotation to generate data for model training.Refinement with Combined Spatial-temporal Prediction. Finally, we generate a score map with combined information from the spatial localization stage and the temporal prediction by context flow,Here α is a positive scalar. It helps the score map to promote coordinates that are activated jointly on both the spatial prediction xs t and the temporal prediction xf t . Finally, we forward the score map through a refinement module to finalize the prediction. The refinement module consists of a stack of 3 convolutional layers. Similar to the spatial localization stage, a combination of the binary cross-entropy and the dice loss is used as the final loss."
ConTrack: Contextual Transformer for Device Tracking in X-Ray,3,Experiments and Results,"Dataset. Our study uses an internal dataset of X-ray sequences captured during percutaneous coronary intervention procedures, featuring a field of view displaying the catheter within the patient's heart. The test dataset is divided into two primary categories: fluoroscopic and angiographic sequences. Fluoroscopic sequences are real-time videos of internal movements captured by low-dose Xrays without radiopaque substances, while angiographic sequences display blood vessels in real-time after the introduction of radiopaque substances.We further separate the test dataset into a third category, ""devices"", presenting a unique challenge for both fluoroscopic and angiographic sequences. In these cases, devices such as wires can obscure the catheter tip and have a similar appearance to the catheter, making tracking more challenging.The dataset includes frames annotated with the coordinates of the catheter tip and, in some cases, a catheter body mask annotation. For training and validation, we use 2,314 sequences consisting of 198,993 frames, of which 44,957 are annotated. As the model training only requires image pairs, i.e.templates and search images, in order to reduce annotation effort, a nonadjacent subset of frames in each sequence is annotated. Their neighboring unannotated frames are also used to provide flow estimation, as is shown in Fig. 2(c). For testing, we use 219 sequences consisting of 17,988 frames, all annotated. The test dataset split is as follows: Fluoro (i.e., fluoroscopy), consisting of 94 sequences, 8,494 frames, from 82 patients; Angio (i.e., angiography), consisting of 101 sequences, 6,904 frames, from 81 patients; and devices, consisting of 24 sequences, 2,593 frames, from 10 patients. All frames undergo a preprocessing pipeline with resampling and padding to size of 512 × 512 with 0.308 mm isotropic pixel spacing.Training. The template frame is of size 64 × 64. The search frame is of size 160 × 160. With this, the inference speed reaches 12 fps. We train our model for 300 epochs using a learning rate of 0.0001.Comparison Study. We compare the proposed approach with existing arts and summarize the results in Table 1. The proposed approach achieves best performance in all testing dataset. In contrast to our method, SiameseRPN [4], STARK [12] and MixFormer [2] focus on spatial localization of the target. Temporal information is being incorporated only with the setting of multi-templates thus target motion modeling is limited. While such approaches can achieve good performance with low median errors (∼2 mm), the high 5-7 mm standard deviations indicate the stability issues, especially in data with devices where occlusions are present. Cycle Ynet [5] uses cycle-consistency loss for motion learning directly on the target. As catheter tip is a sparse object, our approach leverages the motion information of the neighboring context which provide more robust guidance for target location refinement.Overall, ConTrack outperforms all other methods, with a median tracking error of less than 1.08 mm. Our model is particularly effective at tracking the catheter tip when other devices are in the field of view, where all other methods tend to underperform. Compared to Cycle Ynet on all test datasets, our model is 45% more accurate, with an average distance of less than 1mm between the prediction and ground truth. Further, we show the accuracy distributions in Fig. 3. It can be seen that the proposed approach shows superior performance to all other approaches in various percentiles.Ablation Study. We conduct an ablation study to investigate the effectiveness of different model components. Results are summarized in Table 2. Our ablation study revealed three key findings: 1) The addition of the mask segmentation branch improved tracking performance on Fluoro, where the device appearance remains consistent and there is no occlusion. However, when there are distractors, the results are less accurate; 2) The inclusion of a mask segmentation enabled the estimation of motion. The resulting flow helped to stabilize tracking in the presence of distractors; and 3) Multiple templates were employed to better handle changes in appearance. The combined model showed the best performance in dataset of angiography and data with devices, while yielding similar results in dataset of fluoroscopy. Despite our framework's incorporation of various temporal and spatial contexts, catheter tracking remains a challenging task, particularly in cases where other devices or contrast agents obscure the catheter tip and create visual similarities with the catheter itself. Nonetheless, our results demonstrate the promise of ConTrack as a valuable tool for enhancing catheter tracking accuracy.  "
ConTrack: Contextual Transformer for Device Tracking in X-Ray,4,Conclusion,"Device tracking is an important task in interventional procedures. In this paper, we propose a generic model framework, ConTrack, that leverages both spatial and temporal information of the surrounding context for accurate target localization and tracking in X-ray. Through extensive experimentation on large datasets, our approach demonstrated superior tracking performance, outperforming other state-of-the-art tracking models, especially in challenging scenarios where occlusions and distractors are present. Current approach has its limitations. Motion estimation is learned from neighboring two frames and thus target historical trajectory information is missing. Further, transformer-based model training require large amount of annotated data, which is challenging to collect in interventional applications. Finally, throughout the paper we follow established setups and focus on the development on the tracking model with manual initialization. In general, long-term visual tracking with automatic (re-)initialization is a challenging problem and require a system of approaches. A safe and automatic system of device and anatomy tracking is of great clinical relevance and will be an important future work for us.Disclaimer. The concepts and information presented in this paper/presentation are based on research results that are not commercially available. Future commercial availability cannot be guaranteed."
ConTrack: Contextual Transformer for Device Tracking in X-Ray,,Fig. 1 .,
ConTrack: Contextual Transformer for Device Tracking in X-Ray,,Fig. 2 .,
ConTrack: Contextual Transformer for Device Tracking in X-Ray,,Fig. 3 .,
ConTrack: Contextual Transformer for Device Tracking in X-Ray,,Table 1 .,
ConTrack: Contextual Transformer for Device Tracking in X-Ray,,Table 2 .,
ConTrack: Contextual Transformer for Device Tracking in X-Ray,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 65.
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,1,Introduction,"In cardiology, endovascular microcatheters are widely used to provide sensing or interventional imaging for diagnostic and therapeutic applications. One example is a pressuresensing microcatheter, which measures blood pressure waveforms within coronary arteries to assess the severity of a stenosis and thereby to guide decisions about stent deployment. A ""rapid exchange"" microcatheter has a lumen in its distal section that allows it to be delivered over a guidewire positioned within the patient's vasculature. Rapidexchange microcatheters are typically guided to their target destination with fluoroscopic (X-ray) imaging. The use of fluoroscopic guidance has several disadvantages, including exposure of the patient and clinician to X-rays, back pain experienced by practitioners from wearing heavy X-ray protective aprons, and the need for X-ray imaging systems that are not always available in resource-constrained environments. Across a wide range of cardiovascular applications, it is of significant interest to explore alternatives to fluoroscopic guidance of microcatheters.Ultrasound (US) tracking is an emerging method for localizing medical devices within the body that involves ultrasonic communication between the device and an external imaging system. This method can be performed in ""receive-mode"" with an ultrasound sensor in the device that receives transmissions from the imaging probe; the time delays between transmission and reception are processed to obtain estimates of the sensor position in 2D [1,2] and 3D [3,4]. In reciprocal ""transmit-mode"" US tracking, the imaging probe receives transmissions from the device [5]. Fiber optic receivers are well suited to receive-mode US tracking: they have broad bandwidth for compatibility with different imaging probes and for high tracking resolution, they are largely omnidirectional, and their small lateral dimensions and flexibility are well suited to integration into minimally invasive cardiovascular devices such as microcatheters. Previous studies with fiber optic receivers have been focused on tracking needles, for instance in the contexts of peripheral nerve blocks and fetal medicine [2,6].US tracking of microcatheters would potentially enable ultrasound imaging to be used in place of X-ray imaging for guidance, particularly in applications where there are unobstructed ultrasonic views of the vasculature; these devices typically have very low echogenicity due to their small dimensions and polymeric construction. To the authors' knowledge, US microcatheter tracking has not previously been performed. This endeavor leads to several questions that are addressed in this study: first, how can a fiber optic sensor that was originally developed for blood pressure sensing be adapted to obtain concurrent ultrasound signals; second, how does spatial resolution depend on the angular orientation and spatial position of microcatheter; third, how does this combination perform within a clinically realistic environment? In this study, we address these questions, with validation in a porcine model in vivo."
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,2,Materials and Methods,
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,2.1,Ultrasonic Tracking and Concurrent Pressure Sensing System,"The ultrasonic tracking system comprised three components: a clinical US imaging system (SonixMDP, Ultrasonix, Canada) with an external 2-D linear array probe (L14-5; 128 elements), a coronary microcatheter with the integrated pressure/ultrasound sensor (as described in Sect. 2.2), and an US tracking console. The tracking console interrogated the fiber optic sensor in the microcatheter to receive transmissions from the array probe, and obtained processed B-mode US images and two triggers (start of each B-mode frame; start of each A-line) from the US imaging system.The US signals received by the fiber optic sensor were parsed according to the onsets of the A-lines using the acquired triggers, and concatenated to create a 2-D tracking image of the sensor that was inherently co-registered to the corresponding B-mode US images. Envelope detection of the US signals was performed with a Hilbert transform. To localize the sensor, a region of interest (9 mm × 9 mm) was selected from the tracking image, which was centered on the maximum value. After zeroing values less than 70% of this maximum value (an empirically-obtained threshold value), the sensor position was calculated as the location of the center of mass within this region. The coordinates of the catheter tip were then superimposed on the acquired US images frame-by-frame to show the tracked position of the catheter tip.Automatic electronic focusing of the B-mode images was performed in order to improve the lateral tracking accuracy [7]. With this method, the estimated axial (depth) coordinate of the estimated sensor position was relayed to the US imaging system. In this way, the sensor was maintained in the electronic focus of transmissions from the imaging probe without operator intervention."
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,2.2,Sensor and Microcatheter,"The sensor comprised a cylindrical capillary structure (diameter: 250 µm; length: 1 mm) with an inorganic membrane at the distal end. This membrane was separated by an air gap from the distal end of a single mode fiber, thereby creating a low-finesse Fabry-Pérot (F-P) cavity. As the blood pressure increases, the membrane is deflected inward, and viceversa; this deflection is measured using phase-sensitive low-coherence interferometry with a broadband light source [8]. The sensor was integrated within a rapid-exchange coronary microcatheter (minimum/maximum diameters: 0.6 mm/0.9 mm), designed for deployment over a coronary guidewire (diameter: 0.014 = 0.36 mm). The sensor was positioned on one side of the microcatheter in the rapid-exchange region, which allowed for a guidewire to pass through the central lumen (Fig. 1). Reference pressure data were obtained from the aorta with a fluid-line and an external pressure sensor, acquired by the console synchronously with the sensor data. The sensor, microcatheter, and pressure sensing console were provided by Echopoint Medical (London, UK).For US tracking, the sensor was interrogated concurrently with a wavelength that was continuously tuned so that the change of reflectivity with membrane deflection was maximized [9]. Light from this source was distinct from that of the broadband source used for pressure sensing, thereby avoiding optical cross-talk. The two light sources were combined using a fiber optic coupler (50/50; Thorlabs, UK). With this arrangement, invasive pressure measurements and ultrasound reception were obtained concurrently."
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,2.3,Relative Tracking Accuracy,"The relative tracking accuracy of the system was evaluated on the benchtop with the microcatheter immersed in a water tank. With the sensor and the surrounding region of Fig. 1. Schematic of the ultrasound (US) tracking and intravascular pressure sensing system. The rapid exchange microcatheter was progressed over a guidewire within the artery; it contained a fiber optic sensor for receiving ultrasound transmissions (Tx) from the US imaging probe that was also used for concurrent intravascular pressure sensing via a fiber optic coupler (FOC). G: guidewire; MC: microcatheter. the microcatheter held stationary within the imaging plane, the US imaging probe was translated in steps in the lateral and axial positions with two motorized linear translation stages (MTS50/M-Z-8, Thorlabs, UK) arranged orthogonally. At each step, 100 tracking images were acquired; a digital frequency filter (low-pass, 4th-order Butterworth; 4 MHz cut-off) was applied for noise rejection. The corresponding estimated changes in sensor position relative to the starting position were averaged. These changes were subtracted from the actual changes effected by the linear translation stages to measure the relative tracking accuracy."
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,2.4,Impact of Microcatheter Orientation,"To determine the extent to which the hyperechoic guidewire within the central lumen of the microcatheter shadows ultrasound transmissions by the imaging probe, the signal-tonoise ratio (SNR) of the tracking signals was measured for different axial orientations. These orientations included 0°, where the sensor has a smaller depth than the guidewire and one could expect an absence of shadowing, and 180°, where the sensor has a larger depth so that the guidewire is directly above it and one could expect maximal shadowing. The catheter tip was positioned on a mount at different orientations, with the sensor in the imaging plane at a depth of the 4.5 cm. At each orientation angle, the tracking signals were recorded and the mean SNR was estimated over 100 US tracking frames. For the SNR calculation, the signal was taken as the maximum within the tracking frame; the noise was estimated as the standard deviation from deeper regions of the frame for which signals were visually absent."
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,2.5,In Vivo Validation,"An initial clinical assessment of the system was performed with a swine model in vivo. All procedures on animals were conducted in accordance with U.K. Home Office regulations and the Guidance for the Operation of Animals (Scientific Procedures) Act (1986). Ethics approval was provided by the joint animal studies committee of the Griffin Institute and the University College London, United Kingdom.Following arterial access, a coronary guidewire was positioned into the right femoral artery, with guidance from the external ultrasound imaging probe. The microcatheter was subsequently inserted over the guidewire and 250 US tracking frames were acquired, while the microcatheter was moved inside the artery. The microcatheter was then removed and the guidewire was positioned in a renal artery with fluoroscopic guidance. A guide catheter delivered over the guidewire allowed for injections of radio-opaque contrast for locating the renal artery. With the guide catheter removed, the microcatheter was then advanced over the guidewire into the renal artery and another 250 tracking frames were acquired with the US imaging probe mechanically fixed in position. Concurrent blood pressure data were obtained from the renal artery."
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,3,Results and Discussion,"The relative spatial locations of the tracked and actual sensor positions in water were in good agreement for all sensor positions (Fig. 2a,b). To measure the relative axial accuracy, the depth of the sensor relative to the imaging probe (Z) was varied from 18 to 68 mm, corresponding to relative positions of 0 to 50 mm, whilst its lateral position (Y) was held in the center of the imaging plane (Y = 0). Conversely, to measure lateral accuracy, Y was varied from 7 to 33 mm (full range: 0 to 38 mm), corresponding to relative positions of 0 to 26 mm, whilst Z was held constant at 68 mm. The accuracy, defined here as the absolute difference between estimated and actual relative positions, was finer in the Y dimension (<0.6 mm) than in the Z dimension (<0.9 mm).The SNR of the US tracking signals varied with the orientation of the microcatheter. The axial orientation was changed from 0°to 270°in steps of 90°(Fig. 2c). The SNR was a maximum with the sensor head at 0°(SNR: 202) and a minimum at 180°(SNR: 107) (Fig. 2d). These differences in SNR can be attributed to partial US shadowing of the metallic guidewire at an orientation of 180°. Despite these shadowing effects, the SNR was sufficiently high to permit tracking at all orientations.When the microcatheter was in the femoral artery of the swine (depth range: 10 to 20 mm) and also in the imaging plane, SNR values as high as 72 were obtained. As the microcatheter was advanced along the guidewire inside the artery, the tracking images each had singular locations from which strong signals were obtained, which corresponded to received transmissions from the US imaging probe (Fig. 3a,b,c). During the experiment, strong visual correspondences between the estimated positions and the B-mode ultrasound images were observed.When the microcatheter was in the renal artery (depth range: 30 to 35 mm), SNR values as high as 19 were obtained (Fig. 3d,e). This lower maximum SNR value relative to the one obtained from the femoral artery can be attributed in part to the greater depth of the microcatheter and corresponding larger ultrasound attenuation, although slight Fig. 2. Measurements of relative tracking accuracy in the axial (a) and lateral (b) directions, performed with the microcatheter and the ultrasound imaging probe in water. With a motorized translation, the axial position was increased from 18 to 68 mm, corresponding to the plotted relative axial positions of 0 to 50 mm. Likewise, the lateral position was increased from 7 to 33 mm, corresponding to the plotted relative lateral positions of 0 to 26 mm. The signal-to-noise ratio (SNR) of the tracking images was measured as a function of the angular orientation of the microcatheter (c). The region of the microcatheter (MC) containing the sensor (S) was within the imaging plane; in this cross-sectional view, the out-of-plane dimension is denoted as ""x"". US Tx: ultrasound transmission; G: guidewire. The SNR varied as a function of the angular orientation, assuming a maximum when the sensor was facing the imaging probe (0°) and a minimum at 180°. out-of-plane deviations from the imaging plane could also be responsible. Whilst there was axial spread in the tracking images (Fig. 3d), its impact on tracking accuracy was mitigated with the use of center of mass for sensor position estimation. The pressure trace signal recorded by the microcatheter from the renal artery was in good agreement with the reference fluid column measurement from the aorta (Fig. 3f).To the authors' knowledge, this is the first study in which US tracking of a rapidexchange microcatheter was performed, and also the first in which concurrent ultrasound and invasive blood pressure measurements were obtained with a single fiber optic sensor. The multimodal ultrasound/pressure sensing capability that was achieved with one optical fiber could be critically important for vessels with small lumens, to minimize the complexity, size, and flexibility of the device. With its diminutive size, this sensor could be readily incorporated into a wide range of cardiovascular devices and could find widespread utility in cardiovascular medicine. In addition to intracoronary sensing, it could be used for US tracking during endovascular repair of the tricuspid valve, where visualizing therapeutic devices with a transesophageal probe is very challenging, and during renal denervation procedures. This tracking technology is compatible with other types of US transducers than the one used here, such as curvilinear and phased array probes. Future optimizations to the sensor could include reflective surfaces on the distal end of the fiber and the pressure-sensing membrane to increase the ultrasound sensitivity via an increase in optical finesse. Upstream, diffuse delivery of light and temperature measurements from the sensor will enable invasive flow measurements [10], and optically-absorbing nanocomposite coatings applied to the distal end of the membrane could be used for concurrent optical ultrasound imaging [11]. Reproducibility of the sensor and device will be an important area of focus to broaden the range of applications for clinical translation.The system presented here has the advantage of providing tracking images in which signals derive solely from the sensor, which are inherently co-registered with the Bmode US images. The sub-mm tracking accuracy of the system was similar to those previously obtained with receive-and transmit-mode US tracking of medical needles [1,2]. A key observation made in this study was that high sensitivity to ultrasound transmissions could be achieved even when the microcatheter was oriented with the sensor on the opposite side of the guidewire from the imaging probe. In future studies, significant signal attenuation from adipose tissue and sound speed heterogeneities could play a confounding role. Additionally, artifacts might arise from reflections of ultrasound transmissions from strongly echoic structures such as bones or implanted medical devices, which could give rise to multiple objects in tracking images. These artifacts could be potentially mitigated using deep learning approaches developed for ultrasonic tracking [12,13] and photoacoustic imaging [14]. Mechanical resonance of the sensor membrane, which can lead to axial spread in the tracking images, could be mitigated with frequency filtering. Ultimately, the advantages of US tracking relative to other tracking methods, including automatic image-based device detection, stereo camera tracking, electromagnetic tracking and magnetic sensing depend on a multitude of factors such as size and cost requirements. As demonstrated with an in vivo model in this study, ultrasonic tracking is a promising method for guiding endovascular interventions and many other minimally invasive procedures."
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,,Fig. 3 .,
Ultrasonic Tracking of a Rapid-Exchange Microcatheter with Simultaneous Pressure Sensing for Cardiovascular Interventions,,,
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,1,Introduction,"Orthognathic surgery corrects facial skeletal deformities that may cause functional and aesthetic impairments. In orthognathic surgical procedures, the jaws are cut into several bony segments and repositioned to desired locations to achieve an ideal alignment. Surgeons plan the movement of osteotomized bony segments before surgery to obtain the best surgical outcome. While surgeons do not operate on the soft tissue of the face during the procedure, it is passively moved by the underlying bone, causing a change in facial appearance. To visualize the proposed outcome, simulations of the planned procedure may be performed to predict the final facial tissue appearance. Current simulation techniques use the finite element method (FEM) to estimate the change of the facial tissue caused by the movement of the bony segments [2,5,6,8,9]. Subjectspecific FEM models of the facial tissue are created from preoperative Computed Tomography (CT) imaging. The planned bony displacement is fed to the model as an input boundary condition. Previous studies divided the input bony displacement into smaller, incremental steps to improve model accuracy [7]. While incremental FEM simulations are very accurate, they require significant computational time, sometimes approaching 30 min to perform a single simulation [7]. This restricts the utility of FEM since surgeons may try several iterations before arriving at an optimal surgical plan, and simulations must be performed quickly to be useful in a clinical pipeline. Previous studies have implemented deep learning (DL) methods to shorten soft tissue simulations. Physics-informed neural networks (PINNs) learn the constitutive model of soft tissues, however, these models do not generalize well to new settings [1,11,15]. Other studies learned a general biomechanical model by training a UNet-style network on FEM simulations, but these methods are limited to grid-like structures and cannot represent irregular meshes used in FEM [12,14]. PointNet++ has also been used to learn biomechanical deformation based on point cloud data representing nodes within a mesh [10]. While PointNet++ can represent nodes in an irregular mesh, it does not capture the connections (edges) between nodes. PhysGNN utilizes edge information by learning biomechanical deformation based on graphs created from FE meshes [16]. However, the main limitation of all the aforementioned DL methods is that they perform ""single-step"" simulations (Fig. 1), which are not ideal for modeling non-linear materials such as facial soft tissue, especially when large deformation (> 1mm) is involved [7]. Incremental simulations help deal with the non-linearity of soft tissue by simulating several transitional states, providing a more realistic and traceable final postoperative prediction.A simplistic approach to performing incremental simulations using DL is to break a large simulation into smaller steps and perform them in sequential order, but independently. However, this does not allow the model to correct errors, causing them to accumulate over incremental steps. One previous work implemented a data augmentation strategy to minimize network error in incremental simulations of soft tissue [17]. However, the network used was not designed to utilize temporal information or capture temporal trends across incremental steps, such as deformation that follows a general trajectory, which can be utilized to improve incremental predictions. While another previous work incorporated long short-term memory into a convolutional neural network (CNN) to learn across multiple timesteps, the use of a CNN means the network is limited to representing regular grids without integration of edge information [4]. Therefore, a method is needed which can perform incremental simulations using both spatial and temporal information while accepting irregular mesh geometry in the form of graphs. There are several technical challenges to combining spatial and temporal information in a deep learning network. First, it can be challenging to capture spatiotemporal features that effectively represent the data, especially in cases where the objects change shape over time. Second, spatiotemporal networks are often computationally complex and prone to overfitting, which limits the model's clinical utility. For this reason, an explainable spatiotemporal network, i.e. one that learns temporal trends from already extracted spatial features, while minimizing computational complexity is needed.In this study, we hypothesize that utilizing both spatial and temporal information for incremental simulations can improve facial prediction accuracy over DL networks that perform single-step simulations or incremental simulations while only considering spatial information. Therefore, the purpose of this study is to introduce a spatiotemporal deep learning approach for incremental biomechanics simulation of soft tissue deformation. We designed a network that learns spatial features from incremental simulations and aggregates them across multiple incremental simulations to establish sequential continuity. The contributions of this work are (1) a method for combining spatial and temporal learning in an incremental manner (2) a method for performing incremental simulations while preserving knowledge from previous increments. Our proposed method successfully implements spatiotemporal learning by observing temporal trends from spatial features while minimizing computational complexity. "
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,2,Method,"The Spatiotemporal Incremental Mechanics Modeling (SIMM) method predicts the incremental deformation of a three-dimensional (3D) soft tissue mesh based on incrementally planned input displacement. The SIMM network consists of two main operations: 1) spatial feature encoding and 2) temporal aggregation of spatial features (Fig. 2). The SIMM method is designed to first encode spatial features using a graph neural network optimized for spatial tasks, then observe how these features change over time using a temporal aggregation mechanism. SIMM is designed to be explainable and efficient by using the already extracted spatial features to observe temporal trends."
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,2.1,Data Representation,"The SIMM method uses an incremental simulation scheme, shown in Fig. 2. SIMM is designed to accept input data in graph format that represents volumetric meshes used in FEM. In the input graphs, vertices correspond to nodes in an FE mesh and the edges correspond to the connections between nodes within an element [13]. For this reason, we will refer to our input data as ""meshes"" consisting of ""nodes"" and ""edges"" in the rest of this manuscript. In the SIMM method, the planned bony displacement d B for a surgical procedure is broken into smaller steps and applied incrementally. The incremental planned bony displacement dand geometric information from an input facial mesh mF are fed to the spatial sub-network to predict the incremental facial deformation d (t) F,s . The geometric information from the input facial mesh consists of an adjacency matrix a F and a set of edge weights w (t) F . The adjacency matrix is a binary matrix describing the connections between nodes. The edge weights are calculated as the inverse of the Euclidean distance between nodes, providing the network with spatial information. Spatial features x s are extracted in each GNN layer and aggregated using jumping-knowledge connections [18]. In the temporal sub-network, the stack of encoded spatial features [x F,gt . The spatiotemporally predicted facial deformation is then used to update the facial mesh for the subsequent timestep. After all incremental steps are applied (at t = N ), the final predicted deformation is used to calculate the predicted postoperative facial mesh m N F ."
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,2.2,Network,"Spatial sub-network: We used a PhysGNN network to learn the spatial features [16]. PhysGNN consists of six GNN layers that extract features in an increasing neighborhood around a given node. PhysGNN also uses jumpingknowledge connections which aggregate information from the first three and last three GNN layers. The jumping-knowledge connections use an aggregation function to fuse features across the GNN layers using the equation:where y s is the spatially aggregated features, and x (l) are the features from layers 0 to l. f aggr can be any form of aggregation function, such as concatenation.In this work, we use a long-short-term memory (LSTM) attention aggregation mechanism to achieve optimal results, which has been demonstrated in our ablation study (Sect. 3.4) [3,18]."
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,,Temporal sub-network:,"To capture temporal trends, we implemented additional aggregation layers to process features across a sequence of incremental timepoints. Instead of aggregating features across several spatial ""neighborhoods"" as seen in Eq. ( 1), these additional aggregation layers aggregate features from a given spatial neighborhood across several sequential timepoints (t), as seen in Eq. ( 2):where y t is the temporally aggregated features and x (t) are the features to be aggregated across timepoints (t) to (t -N ). We hypothesized that including more than one previous timepoint in the aggregation layer may improve the network performance (see Sect. 3.4). We used the same LSTM-attention aggregation mechanism as used in the PhysGNN spatial sub-network.Training Strategy: The SIMM network produces two predictions of the incremental tissue deformation, one from the spatial sub-network and the other after spatiotemporal aggregation (Fig. 2). We used incremental FEM simulations as ground truth while training our network. The loss was calculated as the meansquared error between the predicted deformation and the ground truth FEM deformation. We use the following equation to calculate the loss of the network:where l SIMM is the total loss of the SIMM network, l s is the loss of the spatial sub-network predicted deformation dF,s , and l st is the loss of the spatiotemporal predicted deformation d (t) F,st . The combined loss was used to train the network to ensure the spatial sub-network still learns to adequately encode the spatial features."
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,3,Experiments and Results,
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,3.1,Dataset,"The SIMM method was evaluated on a dataset of incremental FEM simulations from 18 subjects using a leave-one-out cross-validation (LOOCV). The subjects were chosen from our digital archive of patients who underwent doublejaw orthognathic surgery (IRB# PRO00008890). FEM meshes were generated from CT scans and FE simulations were performed using an existing clinical pipeline [7]. Boundary conditions including bony movements and sliding effect were applied to the FEM mesh. FEM simulation was performed to simulate the facial change with at least 200 incremental steps to have enough training data. The total number of incremental steps varied, depending on the stability of each incremental simulation step. Meshes for all subjects consisted of 3960 nodes and 2784 elements. The incremental bony displacement information was fed to the network as an input feature vector for each node. For facial nodes assumed to move together with the underlying moving bony surface, the input displacement vector was equal to the bony surface displacement d (t) B , which describes the boundary condition between the moving bony structures and the facial soft tissue. For facial nodes not on the surface of a moving bony segment, or that are on the surface of a fixed bony segment, the input displacement vector was initialized as all zeros. To increase the number of simulations for training, we further split the simulation for a given subject into several ""sub-simulations"" consisting of a subset of the incremental timesteps. The timesteps for each sub-simulation were determined by using a maximum deformation threshold, d max . For example, starting from timepoint 0, the first incremental timepoint with a maximum deformation exceeding the threshold would be included in the sub-simulation (Fig. S1). The starting point of each sub-simulation was unique, however, all sub-simulations ended on the postoperative face as the last incremental step, allowing for a direct comparison between all sub-simulations on the postoperative mesh for a given subject. We used a maximum deformation threshold of 0.5mm when splitting each subject's simulation into sub-simulations, which was chosen based on an ablation study (see Sect. 3.4). The number of sub-simulations for each subject can be found in Tab. S1. "
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,3.2,Implementation Details and Evaluation Metrics,"All networks were trained in pytorch using the Adam optimizer with an initial learning rate of 5e-3 for 100 epochs on an NVIDIA Tesla V100. We trained all networks in leave-one-out cross-validation across all 18 subjects. The loss was calculated as the mean squared error between the network-predicted and FEM-predicted deformation vectors for each node within the facial mesh. The final accuracy was calculated as the Euclidean distance between the networkpredicted and FEM-predicted node coordinates on the final postoperative face. The distribution of the Euclidean distances was not found to be normal using the Kolmogorov-Smirnov test, so statistical significance between methods was tested using the Wilcoxon signed-rank test and a p-value of 0.05."
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,3.3,Comparison with Baselines,"We compared our SIMM method with two baseline methods: 1) a single-step method, and 2) an incremental method. For the single-step method, we trained a PhysGNN network to predict the total facial deformation in a single step. The same sub-simulations used in the incremental and SIMM methods (see Sect. 3.1) were used to train the single-step network, however, all intermediate timepoints between the first and final timepoints were removed. For the incremental method, we used PhysGNN to perform incremental simulations. The incremental method used the prediction from timepoint t-1 to update the edge weights in timepoint t, similar to the feedback loop in SIMM (Fig. 2). However, no temporal aggregation or separate spatiotemporal prediction was used.Our SIMM method achieved a mean error of 0.42 mm on all subjects (Table 1) with subject-specific mean error between 0.23 and 0.77 mm (Tab. S1). In comparison, the single-step method achieved a mean error of 0.44 mm on all subjects (Table 1) with subject-specific mean error between 0.30 and 0.91 mm (Tab. S1). The incremental method achieved a mean error of 0.47 mm on all subjects (Table 1) with subject-specific mean error between 0.25 and 1.00 mm (Tab. S1). Statistical analysis showed SIMM performed significantly better than the singlestep and incremental methods, while the single-step and incremental methods were not significantly different."
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,,Single-step,"Incremental SIMM 332 nodes >1mm error 230 nodes >1mm error 130 nodes >1mm error Fig. 3. Error of a) the single-step method, b) incremental method, and c) SIMM method for subject 9"
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,3.4,Ablation Studies,"We also performed ablation studies to investigate the effects of several key components in the SIMM method. First, when splitting subject simulations into sub-simulations, we varied the maximum deformation threshold d max to 1.0, 0.5, and 0.1 mm. We found a d max of 0.5 achieved the best performance in the incremental method, although the best d max may change for different cases (Tab. S2). Second, we investigated the effect of the type of aggregation mechanism used in the PhysGNN network of the incremental method by replacing the LSTM-attention aggregation mechanism with a concatenation aggregation mechanism. The mean error increased to 1.25 mm when using a concatenation aggregation. Third, we tried increasing the number of previous timepoints in the spatial features stack to be used in the temporal aggregation layers from 1 previous timepoint to 5 previous timepoints. We hypothesized that including multiple previous timepoint in the aggregation layer may improve performance.The mean error increased to 0.44 mm when using 5 previous timepoints."
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,4,Discussions and Conclusions,"The SIMM method achieved a lower mean error than the single-step and incremental methods, as seen in the quantitative results (Table 1). The results of the incremental method suggest the network accumulates errors across incremental steps, as seen in a plot of the predicted deformation vectors over time (Fig. S2). Figure 3 shows an example of the improvement of SIMM over the single-step and incremental methods. The results of the ablation study showed concatenation aggregation did not perform as well as LSTM-attention aggregation, which is consistent with other studies that investigated aggregation methods in GNNs [3,18]. Our ablation study also demonstrated that increasing the number of previous timepoints in the temporal feature aggregation did not improve network performance. We think this result is likely caused by over-smoothing the temporal information when aggregating features from many previous timepoints. One limitation of our SIMM method is that it requires incremental FEM simulations to be trained. This greatly increases training requirements over single-step methods, which can feasibly be trained only on preoperative and postoperative facial data without performing incremental FEM simulations. However, this is true of any incremental simulation method. Once SIMM is trained, the inference simulation time is considerably faster than FEM, which can take several minutes to perform an incremental simulation. The SIMM method could be easily extended to biomechanical simulations of many other types of soft tissues and clinical applications.In conclusion, we successfully created a spatiotemporal incremental mechanics modeling (SIMM) method for simulating incremental facial deformation for surgical planning. The SIMM method shows potential to improve simulation accuracy over methods based on spatial information alone, suggesting the importance of spatiotemporal learning for incremental simulations."
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,,Fig. 1 .,
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,,Fig. 2 .,
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,,,
Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change,,Table 1 .,*  
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,1,Introduction,"A difficulty faced by surgeons performing endoscopic pituitary surgery is identifying the areas of the bone which are safe to open. This is of particular importance during the sellar phase as there are several critical anatomical structures within close proximity of each other [9]. The sella, behind which the pituitary tumour is located, is safe to open. However, the smaller structures surrounding the sella, behind which the optic nerves and internal carotid arteries are located, carry greater risk. Failure to appreciate these critical parasellar neurovascular structures can lead to their injury, and adverse outcomes for the patient [9,11]. The human identification of these structures relies on visual clues, inferred from the impressions made on the bone, rather than direct visualisations of the structures [11]. This is especially challenging as the pituitary tumour often compresses; distorts; or encases the surrounding structures [11]. Neurosurgeons utilise identification instruments, such as a stealth pointer or micro-doppler, to aid in this task [9]. However, once an identification instrument is removed, identification is lost upon re-entry with a different instrument, and so the identification can only be used in referenced to the more visible anatomical landmarks. Automatic identification from endoscopic vision may therefore aid surgeons in this effort while minimising disruption to the surgical workflow [11]. This is a challenging computer vision task due to the narrow camera angles enforced by minimally invasive surgery, which lead to: (i) structure occlusions by instruments and biological factors (e.g., blood); and (ii) image blurring caused by rapid camera movements. Additionally, in this specific task there are: (iii) numerous small structures; (iv) visually similar structures; and (v) unclear structure boundaries. Hence, the task can be split into two sub-tasks to account for these difficulties in identification: (1) the semantic segmentation of the two larger, visually distinct, and frequently occurring structures (sella and clival recess); and (2) the centroid detection of the eight smaller structures (Fig. 1).To solve both tasks simultaneously, PAINet (Pituitary Anatomy Identification Network) is proposed. This paper's contribution is therefore:1. The automated identification of the ten critical anatomical structures in the sellar phase of endoscopic pituitary surgery. To the best of the authors' knowledge, this is the first work addressing the problem at this granularity. 2. The creation of PAINet, a multi-task neural network capable of simultaneously semantic segmentation and centroid detection of numerous anatomical structures within minimally invasive surgery. PAINet uniquely utilises two loss functions for improved performance over single-task neural networks due to the increased information gain from the complementary task."
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,2,Related Work,"Encoder-decoder architectures are the leading models in semantic segmentation and landmark detection [4], with common architectures for anatomy identification including the U-Net and DeepLab families [6]. Improvements to these models include: adversarial training to limit biologically implausible predictions [14]; spatial-temporal transformers for scene understanding across consecutive frames [5]; transfer learning from similar anatomical structures [3]; and graph neural networks for global image understanding [2]. Multi-task networks improve on the baseline models by leveraging common characteristics between sub-tasks, increasing the total information provided to the network [15], and are effective at instrument segmentation in minimally invasive surgery [10].The most clinically similar works to this paper are: (1) The semantic segmentation of 3-anatomical-structures in the nasal phase of endoscopic pituitary surgery [12]. Here, U-Net was weakly-supervised on centroids, outputting segmentation masks for each structure. Training on 18-videos (367-images), the model achieved statistically significant results (P < 0.001) on the hold-out testing dataset of 5-videos (182-images) when compared to a location prior baseline model [12]. (2) The semantic segmentation of: 2-zones (safe or dangerous); and 3-anatomical-structures in laparoscopic cholecystectomy [7]. Here, two PSPNets were fully-supervised on 290-videos (2627 images) using 10-fold cross-validation, achieving 62% mean intersection over union (MIoU) for the 2-zones; and 74% MIoU for the 3-structures [7]. These works are extended in this paper by increasing the number of anatomical structures and the identification granularity."
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,3,Methods,"PAINet: A multi-task encoder-decoder network is proposed to improve performance by exchanging information between the semantic segmentation and centroid detection tasks. EfficientNetB3, pre-trained on ImageNet, is used as the encoder because of its accuracy, computational efficiency and proven generalisation capabilities [13]. The decoder is based on U-Net++, a state-of-the-art segmentation network widely used in medical applications [16]. The encoderdecoder architecture is modified to output both segmentation and centroid predictions by sending the decoder output into two separate layers: (1) a convolution for segmentation prediction; and (2) an average pooling layer for centroid prediction. Different loss functions were minimised for each sub-task (Fig. 2). Ablation studies and granular details are provided below. The priority was to find the optimal sella segmentation model, as it is required to be opened to access the pituitary tumour behind it, indicating the surgical ""safe-zone"".Semantic Segmentation: First, single-class sella segmentation models were trialed. 8-encoders (pre-trained convolution neural networks) and 15-decoders were used, with their selection based off architecture variety. Two loss functions were also used: (1) distribution-based logits cross-entropy; and (2) region-based Jaccard loss. Boundary-based loss functions were not trialed as: (1) the boundary of the segmentation masks are not well-defined; and (2) in the cases of split structures (Fig. 1c), boundary-based loss functions are not appropriate [8]. The decoder output is passed through a convolution layer and sigmoid activation.For multi-class sella and clival recess segmentation, the optimal single-class model was extended by: (1) sending through each class to the loss function separately (multi-class separate); and (2) sending both classes through together (multi-class together). An extension of logits cross-entropy, logits focal loss, was used instead as it accounts for data imbalance between classes.Centroid Detection: 5-models were trialed: 3-models consisted of encoders with a convolution layer and linear activation; and 2-models consisted of encoderdecoders with an average pooling layer and sigmoid activation with 0.3 dropout. Two distance-based loss functions were trialed: (1) mean squared error (MSE); and (2) mean absolute error (MAE). Loss was calculated for all structures simultaneously as a 16 dimensional output (8 centroids × 2 coordinates) and set to 0 for a structure if ground-truth centroids of that structure was not present."
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,4,Experimental Setup,"Evaluation Metrics: For sella segmentation the evaluation metric is intersection over union (IoU), as commonly used in the field [4,8]. For multi-class segmentation, the model that optimises clival recess IoU without reducing the previously established sella IoU is chosen. Precision and recall are also given.For centroid detection, the evaluation metric is mean percentage of correct keypoints (MPCK) with the threshold set to 20%, indicating the mean number of predicted centroids falling within 144 pixels of the ground-truth centroid. This is commonly used in anatomical detection tasks as it ensures the predictions are close to the ground-truth while limiting overfitting [1]. MPCK-40% and MPCK-10%, along with the mean percentage of centroids that fall within their corresponding segmentation mask (mean percentage of centroid masks (MPCM)) are given as secondary metrics. For multi-task detection, MPCK-20% is optimised such that sella IoU does not drop from the previously established optimal IoU.Network Parameters: 5-fold cross-validation was implemented with no holdout testing. To account for structure data imbalance, images were randomly split such that the number of structures in each fold is approximately even. Images from a singular video were present in either the training or validation dataset.Each model was run for with a batch size of 5 for 20 epochs, where the epoch with the best primary evaluation metric on the validation dataset was kept. The optimising method was Adam with varying initial learning rates, with a separate optimiser for each loss function during multi-task training.All images were scaled to 736 × 1280 pixels for model compatibility, and training images were randomly augmented within the following parameters: shift in any direction by up to 10%; zooming in or out about the image center by up to 10%; rotation about the image center clockwise or anticlockwise by up to π/6; increasing or decreasing brightness, contrast, saturation, and hue by up to 10%.The code is written in Python 3.8 using PyTorch 1.8.1, run on a single NVIDIA Tesla V100 Tensor Core 32-GB GPU using CUDA 11.2, and is available at https://github.com/dreets/pitnet-anat-public. For PAINet, a batch size of 5 utilised 29-GB and the runtime was approximately 5-min per epoch. Valuation runtime is under 0.1-s per image and therefore a real-time overlay on-top of the endoscope video feed is feasible intra-operatively."
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,5,Dataset Description,"Images: Images come from 64-videos of endoscopic pituitary surgery where the sellar phase is present [9], recorded between 30 Aug 2018 and 20 Feb 2021 from The National Hospital of Neurology and Neurosurgery, London, United Kingdom. All patients have provided informed consent, and the study was registered with the local governance committee. A high-definition endoscope (Hopkins Telescope, Karl Storz Endoscopy) was used to record the surgeries at 24 frames per second (fps), with at least 720p resolution, and stored as mp4 files. 10-images corresponding to 10-s of the sellar phase immediately preceding sellotomy were extracted from each video at 1 fps, and stored as 720p png files. Video upload and annotation was performed using Touch Surgery TM Enterprise. Annotations: Expert neurosurgeons identified 10-anatomical-structures as critical based on the literature (Fig. 1a) [9,11]. 640-images were manually segmented to obtain ground-truth segmentations. A two-stage process was used: (1) two neurosurgeons segmented each image, with any differences settled through discussion; (2) two consultant neurosurgeons independently peer-reviewed the segmentations. Only visible structures were annotated (Fig. 1b); if the structures were occluded, the segmentation boundaries were drawn around these occlusions (Fig. 1c); and if an image is too blurry to see the structures no segmentation boundaries were drawn -this excluded 5 images (Fig. 1d). The center of mass of each segmentation mask was defined as the centroid.The sella is present in all 635-images (Fig. 3). Other than the clival recess, the remaining 8-structures are found in less than 65% of images, with planum sphenoidal found in less than 25% of images. Moreover, the area covered by these 8-structures are small, with several covering less than 10% of the total area covered by all structures in a given image. Furthermore, most smaller structures boundaries are ambiguous as they are hard to define even by expert neurosurgeons. This emphasizes the challenge of identifying smaller structure in computer vision, and supports the need for detection and multi-task solutions."
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,6,Results and Discussion,"Quantitative evaluation is calculated for: single-class sella segmentation (Table 1); single-class, multi-class, and PAINet 2-structures segmentation (Table 2); multi-class, and PAINet 8-structures centroid detection (Tables 3 and4).The optimal model for single-class sella segmentation achieved 65.4% IoU, utilising an EfficientNetB3 encoder; U-Net++ decoder; Jaccard loss; and a 0.001 initial learning rate. Reductions in IoU are seen when alternative parameters are used, highlighting their impact on model performance. Using the optimal sella model configuration, 53.4% IoU is achieved for singleclass clival recess segmentation. Extending this to multi-class and PAINet training improves both sella and clival recess IoU to 66.1% and 54.1% respectively.The optimal model for centroid detection achieves 51.7% MPCK-20%, with minor deviations during model parameter changes. This model, ResNet18 with MSE loss, outperforms the more sophisticated models, as these models over-learn image features in the training dataset. However, PAINet leverages the additional information from segmentation masks to achieve an improved 53.2%.The per structure PCK-20% indicate performance is positively correlated with the number of images where the structure is present. This implies the limiting factor is the number of images rather than architectural design.   Qualitative predictions of the best performing model, PAINet, are displayed in Fig. 4. The segmentation predictions look strong, with small gaps from the ground-truth. However, this is expected as structure boundaries are not welldefined. The centroid predictions are weaker: in (a) the planum sphenoidale (grey) is predicted within the segmentation mask; in (b) three structures are within their segmentation mask, but the left optic-carotid recess (orange) is predicted in a biologically implausible location; and in (c) this is repeated for the right carotid (pink) and no structures are within their segmentation masks."
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,7,Conclusion,"Identification of critical anatomical structures by neurosurgeons during endoscopic pituitary surgery remains a challenging task. In this paper, the potential of automating anatomical structure identification during surgery was shown. The proposed multi-task network, PAINet, designed to incorporate identification of both large prominent structures and numerous smaller less prominent structures, was trained on images of the sellar phase of endoscopic pituitary surgery. Using 635-images from 64-surgeries annotated by expert neurosurgeons and various model configurations, the robustness of the PAINet was shown over single task networks. PAINet achieved 66.1% (+0.7%) and 54.1% IoU (+0.7%) for sella and clival recess segmentation respectively, a higher performance than other minimally invasive surgeries [7]. PAINet also achieved 53.2% MPCK-20% (+1.5%) for detection of the remaining 8-structures. The most important structures to identify and avoid, the carotids and optic protuberances, have high performance, and therefore demonstrate the success of PAINet. This performance is greater than similar studies in endoscopic pituitary surgery for different structures [12] but lower than anatomical detection in other surgeries [1]. Collecting data from more pituitary surgeries will support incorporating anatomy variations and achieving generalisability. Furthermore, introducing modifications to the model architecture, such as the use of temporal networks [5], will further boost performance required for real-time video clinical translation."
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,,Fig. 1 .,
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,,Fig. 2 .,
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,,Fig. 3 .,
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,,Fig. 4 .,
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,,Table 1 .,
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,,Table 2 .,
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,,Table 3 .,1 ± 3.7 56.2 ± 4.7 26.4 ± 4.8 06.8 ± 1.6PAINetU-Net++ EfficientNetB3 MSE 53.2 ± 5.9 58.0 ± 6.9 39.6 ± 3.2 13.4 ± 2.9
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,,Table 4 .,PCK-20% 68.3 ± 9.2 37.6 ± 4.8 53.9 ± 7.3 27.6 ± 2.4 76.1 ± 2.8 72.0 ± 9.1 34.8 ± 3.0 55.4 ± 8.5
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_45.
Intraoperative CT Augmentation for Needle-Based Liver Interventions,1,Introduction,"Needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave, laser, cryoablation) have a great potential for local curative tumor control [1], with comparable results to surgery in the early stages for both primary and secondary cancers. Furthermore, as it is minimally invasive, it has a low rate of major complications and procedure-specific mortality, and is tissue-sparing, thus, its indications are growing exponentially and extending the limits to more advanced tumors [3]. CT-guidance is a widely used imaging modality for placing the needles, monitoring the treatment, and following up patients. However, it is limited by the exposure to ionizing radiation and the need for intravenous injection of contrast agents to visualize the intrahepatic vessels and the target tumor(s).In standard clinical settings, the insertion of each needle requires multiple check points during its progression, fine-tune maneuvers, and eventual repositioning. This leads to multiple CT acquisitions to control the progression of the needle with respect to the vessels, the target, and other sensible structures [26]. However, intrahepatic vessels (and some tumors) are only visible after contrast-enhancement, which has a short lifespan and dose-related deleterious kidney effects. It makes it impossible to perform each of the control CT acquisitions under contrast injection. A workaround to shortcut these limitations is to perform an image fusion between previous contrasted and intraoperative noncontrasted images. However, such a solution is only available in a limited number of clinical settings, and the registration is only rigid, usually deriving into bad results. In this work, we propose a method for visualizing intrahepatic structures after organ motion and needle-induced deformations, in non-injected images, by exploiting image features that are generally not perceivable by the human eye in common clinical workflows.To address this challenge, two main strategies could be considered: image fusion and image processing techniques. Image fusion typically relies on the estimation of rigid or non-rigid transformations between 2 images, to bring into the intraoperative image structures of interest only visible in the preoperative data. This process is often described as an optimization problem [9,10] which can be computationally expensive when dealing with non-linear deformations, making their use in a clinical workflow limited. Recent deep learning approaches [11,12,14] have proved to be a successful alternative to solve image fusion problems, even when a large non-linear mapping is required. When ground-truth displacement fields are not known, state-of-the-art methods use unsupervised techniques, usually an encoder-decoder architecture [7,13], to learn the unknown displacement field between the 2 images. However, such unsupervised methods fail at solving our problem due to lack of similar image features between the contrasted (CCT) and non-contrasted (NCCT) image in the vascular tree region (see Sect. 3.3).On the other hand, deep learning techniques have proven to be very efficient at solving image processing challenges [15]. For instance, image segmentation [16], image style transfer [17], or contrast-enhancement to cite a few. Yet, segmenting vessels from non-contrasted images remains a challenge for the medical imaging community [16]. Style transfer aims to transfer the style of one image to another while preserving its content [17][18][19]. However, applying such methods to generate a contrasted intraoperative CT is not a sufficiently accurate solution for the problem that we address. Contrast-enhancement methods could be an alternative. In the method proposed by Seo et al. [20], a deep neural network synthesizes contrast-enhanced CT from non contrast-enhanced CT. Nevertheless, results obtained by this method are not sufficiently robust and accurate to provide an augmented intraoperative CT on which needle-based procedures can be guided.In this paper we propose an alternative approach, where a neural network learns local image features in a NCCT image by leveraging the known preoperative vessel tree geometry and topology extracted from a matching (undeformed) CCT. Then, the augmented CT is generated by fusing the deformed vascular tree with the non-contrasted intraoperative CT. Section 2 presents the method and its integration in the medical workflow. Section 3 presents and discusses the results, and finally we conclude in Sect. 4 and highlight some perspectives."
Intraoperative CT Augmentation for Needle-Based Liver Interventions,2,Method,"In this section, we present our method and its compatibility with current clinical workflows. A few days or a week before the intervention, a preoperative diagnostic multiphase contrast-enhanced image (MPCECT) is acquired (Fig. 1, yellow box). The day of the intervention, a second MPCECT image is acquired before starting the needle insertion, followed by a series of standard, non-injected acquisitions to guide the needle insertion (Fig. 1, blue box). Using such a noncontrasted intraoperative image as input, our method performs a combined non-rigid registration and augmentation of the intraoperative CT by adding anatomical features (mainly intrahepatic vessels and tumors) from the preoperative image to the current image. To achieve this result, our method only requires to process and train on the baseline MPCECT image (Fig. 1,red box). An overview of the method is shown in the Fig 2 and the following sections describe its main steps.  "
Intraoperative CT Augmentation for Needle-Based Liver Interventions,2.1,Vessel Map Extraction,"We call Vessel Map (VM) the region of interest defining the vascular tree in the NCCT. Since vascular structures are not visible in non-contrasted images, the extraction of this map is done by segmenting the CCT and then using this segmentation as a mask in the NCCT. Mathematical morphology operators, in particular a dilation operation [23], are performed on the segmented region of interest to slightly increase its dimensions. This is needed to compensate for segmentation errors and the slight anatomical motion that may exist between the contrasted and non-contrasted image acquisitions. In practice, the acquisition protocols limit the shift between the NCCT and CCT acquisitions, and only a few sequential dilation operations are needed to ensure we capture the true vessel fingerprint in the NCCT image. Note that the resulting vessel map is not a binary mask, but a subset of the image limited to the volume covered by the vessels."
Intraoperative CT Augmentation for Needle-Based Liver Interventions,2.2,Data Augmentation,"The preoperative MPCECT provides a couple of registered NCCT and CCT images. This is obviously not sufficient for training purposes, as they do not represent the possible soft tissue deformation that may occur during the procedure. Therefore, we augment the data set by applying multiple random deformations to the original images. Random deformations are created by considering a predefined set of control points for which we define a displacement field with a random normal distribution. The displacement field of the full volume is then obtained by linearly interpolating the control points' displacement field to the rest of the volume. All the deformations are created using the same number of control points and characteristics of the normal distributions."
Intraoperative CT Augmentation for Needle-Based Liver Interventions,2.3,Neural Network,"Predicting the vascular tree location in the deformed intraoperative NCCT is done using a U-net [5] architecture. The neural network takes as input the preoperative vessel map and the intraoperative NCCT, and outputs the intraoperative vessel map. Our network learns to find the image features (or vessel fingerprint) present in the vessel map, in a given NCCT assuming the knowledge of its geometry, topology, and the distribution of contrast from the preoperative MPCECT. The architecture of our network is illustrated in Fig. 3. It consists of a four layers analysis (left side) and synthesis (right side) paths that provide a non-linear mapping between low resolution input and output images. Both paths include four 3 × 3 × 3 unpadded convolutions, each followed by a Leaky Rectified Linear Unit (LeakyReLU) activation function. The analysis includes a 2 × 2 × 2 max pooling with a stride of 1, while the synthesis follows each convolution by a 2 × 2 × 2 up-convolution with a stride of 1. Shortcut connections from layers of equal resolution in the analysis path provide the essential high-resolution features to the synthesis path. In the last layer, a 1 × 1 × 1 convolution reduces the number of output channels to one, yielding the vessel map in the intraoperative image. Our network is trained by minimizing the mean square error between the predicted and ground truth vessel map. Training details are presented in Sect. 3. "
Intraoperative CT Augmentation for Needle-Based Liver Interventions,2.4,Augmented CT,"Once the network has been trained on the patient-specific preoperative data, the next step is to augment and visualize the intraoperative NCCT. This is done in 3 steps:-The dilatation operations introduced in Sect. 2.1 are not reversible (i.e. the segmented vessel tree cannot be recovered from the VM by applying the same number of erosion operations). Also, neighboring branches in the vessel tree could end up being fused, thus changing the topology of the vessel map. Therefore, to retrieve the correct segmented (yet deformed) vascular tree, we compute a displacement field between the pre-and intraoperative VMs. This is done with the Elastix library [21,22]. The resulting displacement field is applied on the preoperative segmentation to retrieve the intraoperative vessel tree segmentation. This is illustrated in Fig. 4. -The augmented image is obtained by fusing the predicted intraoperative segmentation with the intraoperative NCCT image. The augmented vessels are displayed in green to ensure the clinician is aware this is not a true CCT image (see Fig. 5). -It is also possible to add anatomical labels to the intraoperative augmented CT to further assist the clinician. To achieve this objective, we compute a graph data structure from the preoperative segmentation. We first extract the vessel centerlines as described in [4]. To define the associated graph structure, we start by selecting all branches with either no parent or no children. The branch with the highest radius is then selected as the root edge. An oriented graph is created using a Breadth First Search algorithm starting from the root edge. Nodes and edges correspond respectively to vessel tree bifurcations and branches. We use the graph structure to associate each anatomical label (manually defined) with a Strahler [6] graph ordering. The same process is applied to the predicted intraoperative segmentation. This makes it possible to correctly map the preoperative anatomical labels (e.g. vessel name) and display them on the augmented image.Fig. 4. This figure illustrates the different stages of the pipeline adopted to generate the VM and show how the vessel tree topology is retrieved from the predicted intraoperative VM by computing a displacement field between the preoperative VM and the predicted VM. This field is applied to the preoperative segmentation to get the intraoperative one."
Intraoperative CT Augmentation for Needle-Based Liver Interventions,3,Results and Discussion,
Intraoperative CT Augmentation for Needle-Based Liver Interventions,3.1,Dataset and Implementation Details,"To validate our approach, 4 couples of MPCECT abdominal porcine images were acquired from 4 different subjects. For a given subject, each couple corresponds to a preoperative and an intraoperative MPCECT. We recall that an MPCECT contains a set of registered NCCT and CCT images. These images are then cropped and down-sampled to 256 × 256 × 256, and the voxels intensities are scaled between 0 and 255. Finally, we extract the VM from each MPCECT sample and apply 3 dilation operations, which demonstrated the best performance in terms of prediction accuracy and robustness on our data. We note that public data sets such as DeepLesion [24], 3Dircadb-01 [25] and others do not fit our problem since they do not include the NCCT images. Aiming at a patientspecific prediction, we only train on a ""subject"" at a time. For a given subject, we generate 100 displacement fields using the data augmentation strategy explained above with 50 voxels for the control points spacing in the three spatial directions and a standard deviation of 5 voxels for the normal distributions. The resulting deformation is applied to the preoperative MPCECT and its corresponding VM. Thus, we end up with a set of 100 triplets (NCCT, CCT and VM). Two out of the 100 triplets are used for each training batch, where one is considered as the pre-operative MPCECT and the other as the intraoperative one. This makes it possible to generate up to 4950 training and validation samples. The intraoperative MPCECT of the same subject is used to test the network. Our method is implemented in Tensorflow 2.4, on a GeForce RTX 3090. We use an Adam optimizer (β 1 = 0.001, β 2 = 0.999) with a learning rate of 10 -4 . The training process converges in about 1,000 epochs with a batch size of 1 and 200 steps per epoch."
Intraoperative CT Augmentation for Needle-Based Liver Interventions,3.2,Results,"To assess our method, we use a dice score to measure the overlap between our predicted segmentation and the ground truth. Being a commonly used metric for segmentation problems, Dice aligns the nature of our problem as well as the clinical impact of our solution. We ha performed tests on 4 different (porcine) data sets. Results are reported in Table 1. The method achieved a mean dice score of 0.81. An example of a subject intraoperative augmented CT is illustrated in Fig. 5, where the three images correspond respectively to the initial non injected CT, the augmented CT without and with labels. Figure 6 illustrates the results of our method for Subject 1. The green vessels correspond to the ground truth intraoperative segmentation, the orange ones to the predicted intraoperative segmentation and finally the gray vessel tree corresponds to the preoperative CCT vessel tree. Such results demonstrate the ability of our method to perform very well even in the presence of large deformations.  Qualitative Assessment: To further demonstrate the value of our method, we have asked two clinicians to manually segment the NCCT images in the intraoperative MPCECT data. Their results (mean and standard deviation) are reported in Table 1. Our method outperforms the results of both clinicians, with an average dice score of 0.81 against 0.51 as a mean for the clinical experts."
Intraoperative CT Augmentation for Needle-Based Liver Interventions,3.3,Ablation Study and Additional Results,"Vessel Map: We have removed the VM from the network input to demonstrate its impact on our results. Using the data of the Subject 1, a U-net was trained to segment the vessel tree of the intraoperative NCCT image. The network only managed to segment a small portion of the main portal vein branch. Thus, achieving a dice score of 0.16 vs 0.79 when adding the preoperative VM as additional input. We also studied the influence of the diffusion kernel applied to the initial segmentation. We have seen, on our experimental data, that 3 dilation operations were sufficient to compensate for the possible motion between NCCT and CCT acquisitions."
Intraoperative CT Augmentation for Needle-Based Liver Interventions,,Comparison with VoxelMorph:,"The problem that we address can be seen from different angles. In particular, we could attempt to solve it by registering the preoperative NCCT to the intraoperative one and then applying the resulting displacement field to the known preoperative segmentation. However, state-of-the-art registration methods such as VoxelMorph [7] and others do not necessarily guarantee a diffeomorphic [8] displacement field that ensures the continuity of the displacement field inside the parenchyma where the intensity is quite homogeneous on the NCCT. To assess this assumption, a VoxelMorph1 network was trained on the Subject 1 of our porcine data sets. We trained the network with both MSE and smoothness losses during 100 epochs and given a batch of size 4. Results are illustrated below in Fig. 7. While the VoxelMorph network accurately registers the liver shape, the displacement field is almost null in the region of vessels inside the parenchyma. Therefore, the preoperative vessel segmentation is not correctly transferred into the intraoperative image. "
Intraoperative CT Augmentation for Needle-Based Liver Interventions,4,Conclusion,"In this paper, we proposed a method for augmenting intra-operative NCCT images as a means to improve needle CT-guided techniques while reducing the need for contrast agent injection during tumor ablation procedures, or other needle-based procedures. Our method uses a U-net architecture to learn local vessel tree image features in the NCCT by leveraging the known vessel tree geometry and topology extracted from a matching CCT image. The augmented CT is generated by fusing the predicted vessel tree with the NCCT. Our method is validated on several porcine images, achieving an average dice score of 0.81 on the predicted vessel tree location. In addition, it demonstrates robustness even in the presence of large deformations between the preoperative and intraoperative images. Our future steps will essentially involve applying this method to patient data and perform a small user study to evaluate the usefulness and limitations of our approach.Aknowledgments. This work was partially supported by French state funds managed by the ANR under reference ANR-10-IAHU-02 (IHU Strasbourg). The authors would like to thank Paul Baksic and Robin Enjalbert for proofreading the manuscript."
Intraoperative CT Augmentation for Needle-Based Liver Interventions,,Fig. 1 .,
Intraoperative CT Augmentation for Needle-Based Liver Interventions,,Fig. 2 .,
Intraoperative CT Augmentation for Needle-Based Liver Interventions,,Fig. 3 .,
Intraoperative CT Augmentation for Needle-Based Liver Interventions,,Fig. 5 .Fig. 6 .,
Intraoperative CT Augmentation for Needle-Based Liver Interventions,,Fig. 7 .,
Intraoperative CT Augmentation for Needle-Based Liver Interventions,,Table 1 .,
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,1,Introduction,"Healthy and cancerous soft tissue display different elastic properties, e.g. for breast [19], colorectal [7] and prostate cancer [4]. Different imaging modalities can be used to detect the biomechanical response to an external load for the characterization of cancerous tissue, e.g., ultrasound, magnetic resonance and optical coherence elastography (OCE). The latter is based on optical coherence tomography (OCT), which provides excellent visualization of microstructures and superior spatial and temporal resolution in comparison to ultrasound or magnetic resonance elastography [8]. One common approach for quantitative OCE is to determine the elastic properties from the deformation of the sample and the magnitude of a quasi-static, compressive load [10]. However, due to the attenuation and scattering of the near-infrared light, imaging depth is generally limited to approximately 1 mm in soft tissue. Therefore, OCE is well suited for sampling surface tissue and commonly involves bench-top imaging systems [26], e.g. in ophthalmology [21,22] or as an alternative to histopathological slice examination [1,16]. Handheld OCE systems for intraoperative assessment [2,23] have also been proposed. While conventional OCE probes have been demonstrated at the surface, regions of interest often lie deep within the soft tissue, e.g., cancerous tissue in percutaneous biopsy.Taking prostate cancer as an example, biomechanical characterization could guide needle placement for improved cancer detection rates while reducing complications associated with increased core counts, e.g. pain and erectile dysfunction [14,18]. However, the measurement of both the applied load and the local sample compression is challenging. Friction forces superimpose with tip forces as the needle passes through tissue, e.g., the perineum. Furthermore, the prostate is known to display large bulk displacement caused by patient movement and needle insertions [20,24] in addition to actual sample compression (Fig. 1, left). Tip force sensing for estimating elastic properties has been proposed [5] but bulk tissue displacement of deep tissue was not considered. In principle, compression and tip force could be estimated by OCT. Yet, conventional OCE probes typically feature flat tip geometry [13,17].To perform OCE in deep tissue structures, we propose a novel bevel tip OCE needle design for the biomechanical characterization during needle insertions. We consider a dual-fiber setup with temporal multiplexing for the combined load and compression sensing at the needle tip. We design an experimental setup that can simulate friction forces and bulk displacement occurring during needle biopsy (Fig. 1). We consider tissue-mimicking phantoms for surface and deep tissue indentation experiments and compare our results with force-position curves externally measured at the needle shaft. Finally, we consider how the obtained elasticity estimates can be used for the classification of both materials."
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,2,Methods,"In the following, we first present our OCE needle probe and outline data processing for elasticity estimates. We then present an experimental setup for simulating friction and bulk displacement and describe the conducted surface and deep tissue indentation experiments."
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,2.1,OCE Needle for Deep Tissue Indentation,"Our OCE needle approach is illustrated in Fig. 2. It consists of an OCT imaging system, a time-division multiplexer and our OCE needle probe. The needle features two single-mode glass fibers (SMF-28, Thorlabs GmbH, GER) embedded into a bevel tip needle. The forward viewing fiber (Fiber 1) images sample compression while the load sensing fiber (Fiber 2) visualizes the displacement of a reference epoxy layer that is deformed under load. We cleave the distal ends of both fibers to enable common path interference imaging. The outer diameter of the OCE needle prototype is 2.0 mm. We use a spectral domain OCT imaging system (Telesto I, Thorlabs GmbH, GER) with a center wavelength λ 0 of 1325 nm to acquire axial scans (A-scans) at a sampling rate of 91.3 kHz. A solid state optical switch (NSSW 1x2 NanoSpeed TM , Agiltron, USA), a 100 kHz switch driver (SWDR DC-100KHz NS Driver, Agiltron, USA) and a microcontroller (Arduino TM Mega 2560, Arduino, USA) alternate between the two fibers every second A-scan. Compared to spatial multiplexing [17], our temporal multiplexing maximizes the field-of-view and signal strength while effectively halving the acquisition frequency. with the force F , the area A, initial sample length L 0 and assuming incompressibility, quasi-static loading and neglecting viscoelasticity. However, the indentation with our bevel tipped needle will not result in uniform stress and we hypothesize instead that the elasticity is only relative to the applied tip force F T and the resulting local strain l . To obtain a single parameter for comparing two measurements, we assume a linear relationin the context of this work. To detect strain (Fiber 1) and applied force (Fiber 2), we consider the phase φ of the complex OCT signals for fiber i at time t and depth z. The phase shift between two A-scans is proportional to the depth dependent displacement δu i (z, t)assuming a refractive index n of 1.45 and 1.5 for tissue (Fiber 1) and epoxy (Fiber 2), respectively. We obtain the deformation u i (z, t) from the unwrapped phase and perform spatial averaging to reduce noise. For fiber 1, we employ a moving average with a window size of 0.1 mm. We estimate local strain based on the finite difference along the spatial dimension over an axial depth Δz of 1 mm.For fiber 2, we calculate the mean ū2 (t) over the entire depth of the epoxy. We assume a linear coefficient a F to model the relation between the applied tip force F T and the mean deformation ū2 of the reference epoxy layer.F T (t) = a F * ū2 (t).(5)"
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,2.3,Experimental Setup,"We build an experimental setup for surface and deep tissue indentations with simulated force and bulk displacement (Fig. 1). For deep tissue indentations, different tissue phantoms are stacked on a sample holder with springs in between.For surface measurements, we position the tissue phantoms separately without additional springs or tissue around the needle shaft. We use a motorized linear stage (ZFS25B, Thorlabs GmbH, GER) to drive the needle while simultaneously logging motor positions. An external force sensor (KD24s 20N, ME-Meßsysteme GmbH, GER) measures combined axial forces. We consider two gelatin gels as tissue mimicking materials for healthy and cancerous tissue. The two materials (Mat. A and Mat. B) display a Young's modulus of 53.4 kPa and 112.3 kPa, respectively. Reference elasticity is determined by unconfined compression experiments of three cylindrical samples for each material according to Eq. 1, using force and position sensor data (See supplementary material). The Young's modulus is obtained by linear regression for the combined measurements of each material. We calibrate tip force estimation (Fiber 2) by indentation of silicone samples with higher tear resistance to ensure that no partial rupture has taken place. We then determine a linear fit according to Eq. 5 and obtain a F = 174.4 mN mm -1 from external force sensor and motor position measurements (See supplementary material)."
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,2.4,Indentation Experiments,"In total, we conduct ten OCE indentation measurements for each material. Three surface measurements with fixed samples and seven deep tissue indentations with simulated friction and bulk displacement. For each indentation, we place the needle in front of the surface or deep tissue interface and acquire OCT data while driving the needle for 3 mm (Fig. 1). As the beginning of the needle movement might not directly correspond to the beginning of sample indentation, we evaluate OCE measurements only if the estimated tip force is larger than 50 mN.To further ensure that measurements occur within the pre-rupture deformation phase [6,15], only samples below 20 % local strain are considered. A visualization of the OCE acquisition window from an example insertion with surface rupture and post-rupture cutting phase [6,15] is shown in Fig. 3. We evaluate external needle shaft measurements of relative axial force and relative motor position with the same endpoint obtained from local strain estimates. We perform linear regression to determine the slopes E OCE [mN % -1 ] and E EXT [mN mm% -1 ] from tip-force-strain and axial-force-position curves, respectively. As we can consider surface measurements as equivalents to the known elasticity, we regard the relative error (RE) of the mean value obtained for deep indentations, with respect to the average estimate during surface indentations. We report the RE  for both OCE and external measurements and material A and B, respectively. Finally, we consider the measured elasticities for the biomechanical classification of the material. We report the area under the receiver operating characteristic (AUROC) and area under the precision recall curve (AUPRC) for both external and OCE sensing."
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,3,Results,"The OCE measurements for surface and deep tissue indentations are displayed in Fig. 4a. In comparison, external force-position curves are shown in Fig. 5a. The resulting estimates E OCE and E EXT are shown in Fig. 4b and Fig. 5b, respectively. It can be seen that OCE measurements result in separation of both materials while an overlap is visible for external sensors. The sample elasticities and relative error are also accumulated in Table 1. Biomechanical characterization based on the OCE estimates allows complete separation between materials, with AUROC and AUPRC scores of 1.00 (See supplementary material). External measurements do not enable robust discrimination of materials and yielded AUROC and AUPRC scores of only 0.85 and 0.861, respectively."
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,4,Discussion and Conclusion,"We demonstrate our approach on two tissue mimicking materials that have similar elastic properties as healthy and cancerous prostate tissue [5,11]. The con-  4b and the AUROC and AUPRC scores of 1. Note that the high errors for external measurements at the needle shaft are systematic, as friction and bulk displacement are unknown. In contrast, our probe does not suffer from these systematic errors. Moreover, considering the standard deviation for OCE estimates, improved calibration of our dual-fiber needle probe is expected to further improve performance. Deep learning-based approaches for tip force estimation could provide increased accuracy and sensitivity compared to the assumed linear model [3]. Weighted strain estimation based on OCT signal intensity [26] could address the underestimation of local strain during segments of low signal-to-noise-ratio (See supplementary material). We are also currently only considering the loading cycle and linear elastic models for our approach. However, soft-tissue displays strong non-linearity in contrast to the mostly linear behavior of gelatin gels. Compression OCE theoretically enables the analysis of non-linear elastic behavior [26] and future experiments will consider non-linear models and unloading cycles better befitting needletissue-interaction [15,25]. Interestingly, our needle works with a beveled tip geometry that allows insertion into deep tissue structures. During insertion, tip force estimation can be used to detect interfaces and select the pre-rupture deformation phase for OCE estimates (Fig. 3). This was previously not possible with flat tip needle probes [9,13,17]. While the cylindrical tip is advantageous for calculating the Young's modulus, it has been shown that the calculation of an equivalent Young's modulus is rarely comparable across different techniques and samples [4,12]. Instead, it is important to provide high contrast and high reproducibility to reliably distinguish samples with different elastic properties. We show that our dual-fiber OCE needle probe enables biomechanical characterization by deriving quantitative biomechanical parameters as demonstrated on tissue mimicking phantoms. Further experiments need to include biological soft tissue to validate the approach for clinical application, as our evaluation is currently limited to homogeneous gelatin. This needle probe could also be very useful when considering robotic needle insertions, e.g., to implement feedback control based on elasticity estimates."
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,,Fig. 1 .Fig. 2 .,
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,,Fig. 3 .,
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,,Fig. 4 .,
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,,Fig. 5 .,
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,,Table 1 .,
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 58.
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,1,Introduction,"Understanding the movement of surgical instruments and the surgeon's hands is essential in computer-assisted interventions and has various applications, including surgical navigation systems [27], surgical skill assessment [10,15,23] and robot-assisted surgeries [8]. With the rising interest in using head-mounted Mixed Reality (MR) devices for such applications [1,7,21,28], estimating the 3D pose of hands and objects from the egocentric perspective becomes more important. However, this is more challenging compared to the third-person viewpoint because of the constant self-occlusion of hands and mutual occlusions between hands and objects. While the use of deep neural networks and attention modules has partly addressed this challenge [13,18,19,22], the lack of egocentric datasets to train such models has hindered progress in this field. Most existing datasets that provide 3D hand or hand-object pose annotations focus on the third-person perspective [11,20,29]. FPHA [9] proposed the first egocentric hand-object video dataset by attaching magnetic sensors to hands and objects. However, the attached sensors pollute the RGB frames. More recently, H2O [17] proposed an egocentric video dataset with hand and object pose annotated with a semi-automatic pipeline, based on 2D hand joint detection and object point cloud refinement. However, this pipeline is not applicable to the surgical domain because of the large domain gap between the everyday scenarios in [9,17] and surgical scenarios. For instance, surgeons wear surgical gloves that are often covered with blood during the surgery process, which presents great challenges for vision-based hand keypoint detection methods. Moreover, these datasets focus on large, everyday objects with distinct textures, whereas surgical instruments are often smaller and have featureless, highly reflective metallic surfaces. This results in noisy and incomplete object point clouds when captured with RGB-D cameras. Therefore, in a surgical setting, the annotation approaches proposed in [11,17] are less stable and reliable. Pioneer work in [14] introduces a small synthetic dataset with blue surgical gloves and a surgical drill, following the synthetic data generation approach in [13]. However, being a single-image dataset, it ignores the strong temporal context in surgical tasks, which is crucial for accurate and reliable 3D pose estimation [17,24]. Surgical cases have inherent task-specific information and temporal correlations during surgical instrument usage, such as cutting firmly and steadily with a scalpel. Moreover, it lacks diversity, focusing only on one unbloodied blue surgical glove and one instrument, and only provides low-resolution image patches.To fill this gap, we propose a novel synthetic data generation pipeline that goes beyond single image cases to synthesize realistic temporal sequences of surgical tool manipulation from an egocentric perspective. It features a body motion capture module to model realistic body movement sequences during artificial surgeries and a hand-object manipulation generation module to model the grasp evolution sequences. With the proposed pipeline, we generate POV-Surgery, a large synthetic egocentric video dataset of surgical activities that features surgical gloves in diverse textures (green, white, and blue) with various bloodstain patterns and three metallic tools that are commonly used in orthopedic surgeries.In summary, our contributions are:-A novel, easy-to-use, and generalizable synthetic data generation pipeline to generate temporally realistic hand-object manipulations during surgical activities. -POV-Surgery: the first large-scale dataset with egocentric sequences for hand and surgical instrument pose estimation, with diverse, realistic surgical glove textures, and different metallic tools, annotated with accurate 3D/2D handobject poses and 2D hand-object segmentation masks. -Extensive evaluations of existing state-of-the-art (SOTA) hand pose estimation methods on POV-Surgery, revealing their shortcomings when dealing with the unique challenges in surgical cases from egocentric view. -Significantly improved performance for SOTA methods after fine-tuning them on the POV-Surgery training set, on both our synthetic test set and a real-life test set."
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,2,Method,"We focus on three tools commonly employed in orthopedic procedures -the scalpel, friem, and diskplacer -each of which requires a unique hand motion. The scalpel requires a side-to-side cutting motion, while the friem uses a quick downward punching motion, similar to using an awl. Finally, the diskplacer requires a screwing motion with the hand. Our pipeline to capture these activities and generate egocentric hand-object manipulation sequences is shown in Fig. 1. "
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,2.1,Multi-view Body Motion Capture,"To capture body movements during surgery, we used four temporally synchronized ZED stereo cameras on participants during simulated surgeries. The intrinsic camera parameters were provided by ZED SDK and the extrinsic parameters between the four different cameras were calibrated with a chessboard. We adopt the popular [5] [6] module for SMPLX body reconstruction. OpenPose [2] with hand -and face-detection modules is first used to detect 2D human skeletons with a confidence threshold of 0.3. The 3D keypoints are obtained via triangulation with camera pose, regularized with bone length. The SMPLX body meshed is optimized by minimizing the 2D re-projection and triangulated 3D skeleton errors. Moreover, we enforce a large smoothness constraint, which regularizes the body and hand pose by constraining the between-frame velocities. It vastly reduces the number of unrealistic body poses. There are two critical differences between the surgical tool and everyday object grasping: surgical tools require to be held in specific poses. Moreover, a surgeon would hold firmly and steadily with a particular pose for some time span during surgeries."
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,2.2,Hand-Object Manipulation Sequence Generation,"To address this issue, we generate each instrument manipulation sequence by firstly modeling the key poses that are surgically plausible, and then interpolating in between to model pose evolution. The key pose generation pipeline is shown in Fig. 2. The part highlighted in blue is the pose generation component based on GrabNet [25]. We provide the 3D instrument models to GrabNet with arbitrary initial rotation and sample from a Gaussian distribution in latent space to obtain diverse poses. 500 samples are generated for scalpel, diskplacer, and friem, respectively, followed by manual selection to get the best grasping poses as templates. With a pose template as prior, we perform the re-sampling near it to obtain diverse and similar hand-grasping poses as key poses for each sequence. To improve the plausibility of the grasping pose and hand-object interactions, inspired by [16], an optimization module is adopted for post-processing, with the overall loss function defined as:L penetr , L contact , and L keypoint denote the penetration loss, contact loss, and keypoint loss, respectively. And α, β, γ are object-specific scaling factors to balance the loss components. For example, the weight for penetration is smaller for the scalpel than the friem and diskplacer to account for the smaller object size. The penetration loss is defined as the overall penetration distance of the object into the hand mesh:where P in denotes the vertices from the object mesh which are inside the hand mesh, and V i denotes the hand vertex. The P o in is defined as the dot product of the vector from the hand mesh vertices to their nearest neighbors on the object mesh. To encourage hand-object contact, a contact loss is defined to minimize the distance from the hand mesh to the object mesh.where V and P denote vertices from the hand and object mesh, respectively. In addition, we regularize the optimized hand pose by the keypoint displacement, which penalizes hand keypoints that are far away from the initial hand keypoints:where K is the refined hand keypoint position and k is the source keypoint position. After the grasping pose refinement, a small portion of the generated hand poses are still unrealistic due to the poor initialization. To this end, a post-selection technique similar to [13,26] is further applied to discard the unrealistic samples with hand-centric interpenetration volume, contact region, and displacement simulation.For each hand-object manipulation sequence, we select 30 key grasping poses, hold on, and interpolate in between to model pose evolution within the sequence. The number of frames for the transition phase between every two key poses is randomly sampled from 5 to 30. The interpolated hand poses are also optimized via the pose refinement module with the source keypoint in L keypoint defined as the interpolated keypoints between two key poses."
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,2.3,Body and Hand Pose Fusion and Camera Pose Calculation,"In previous sections, we individually obtained the body motion and hand-object manipulation sequences. To merge the hand pose into the body pose to create a whole-body grasping sequence, we established an optimization-based approach based on the SMPLX model. The vertices to vertices loss is defined as:where v is the vertices in the target grasping hand, v is the vertices in the SMPLX body model, with M being the vertices map from MANO's right hand to SMPLX body. R and T are the rotation matrix and translation vector applied to the right hand. The right-hand pose of SMPLX, R, and T are optimized with the Trust Region Newton Conjugate Gradient method (Trust-NCG) for 300 iterations to obtain an accurate and stable whole-body grasping pose. R and T are then applied to the grasped object. The egocentric camera pose for each frame is calculated with head vertices position and head orientation. Afterwards, outlier removal and moving average filter are applied to the camera pose sequence to remove temporal jitterings between frames. We use blender [3] and bpycv packages to render the RGB-D sequences and instance segmentation masks. Diverse textures and scenes of high quality are provided in the dataset: it includes 24 SMPLX textures featuring blue, green, and white surgical gloves textures with various blood patterns and a synthetic surgical room scene created by artists. The Cycle rendering engine and de-noising post-processing are adopted to produce high-quality frames. POV-Surgery provides clean depth maps for depth-based methods or point-cloud-based methods, as the artifact of real depth cameras can be efficiently simulated via previous works as [12]. A point cloud example generated from an RGB-D frame with added simulated Kinect depth camera noise is provided in Fig. 3. The POV-Surgery dataset consists of 36 sequences with 55,078 frames in the training set and 17 sequences with 33,161 frames in the testing set, respectively. Three bloodied glove textures and one scene created from a room scanning of a surgery room are only used in the testing set to measure generalizability. Fig. 3 shows the ground truth data samples and the dataset statistics."
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,2.4,Rendering and POV-Surgery Dataset Statistics,
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,3,Experiment,"Fig. 4. Qualitative results of METRO [18], SEMI [19], and HANDOCCNET [22] on the test of of POV-Surgery. The FT denotes fine-tuning. We show the 2D re-projection of the predicted 3D hand joints and object control bounding box overlayed on the input image.We evaluate and fine-tune two state-of-the-art hand pose estimation methods: [18,22], and one hand-object pose estimation [19] method on our dataset with provided checkpoints in their official repositories. 6 out of 36 sequences from the training set are selected as the validation set for model selection. We continue to train their checkpoints on our synthetic training set, with a reduced learning rate (10 -5 ) and various data augmentation methods such as color jittering, scale and center jittering, hue-saturation-contrast value jittering, and motion blur for better generalizability. Afterwards, we evaluate the performance of those methods on our testing set. We set a baseline for object control point error in pixels: 41.56 from fine-tuning [19]. The hand quantitative metrics are shown in Table 1 and qualitative visualizations are shown in Fig. 4, where we highlight the significant performance improvement for existing methods after fine-tuning them on the POV-Surgery dataset.To further evaluate the generalizability of the methods fine-tuned on our dataset, we collect 6,557 real-life images with multiple surgical gloves, tools, and backgrounds as the real-life test set. The data capture setup with four stereo cameras is shown in Fig. 5. We adopt a top-down-based method from [4] with manually selected hand bounding boxes for 2D hand joint detection. [5] is used to reconstruct 3D hand poses from different camera observations. We project the hand pose to the egocentric camera view and manually select the frames with accurate hand predictions to obtain reliable 2D hand pose ground truth. We show quantitative examples of the indicated methods and the PCP curve in Fig. 5.After fine-tuning on our synthetic dataset significant performance improvements are achieved for SOTA methods on the real-life test set. Particularly, we observe a similar performance improvement for unseen purple-texture gloves, showing the effectiveness of our POV-Surgery dataset towards the challenging egocentric surgical hand-object interaction scenarios in general. "
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,4,Conclusion,"This paper proposes a novel synthetic data generation pipeline that generates hand-tool manipulation temporal sequences. Using the data generation pipeline and focusing on three tools used in orthopedic surgeries: scalpel, diskplacer, and friem, we propose a large, synthetic, and temporal dataset on egocentric surgical hand-object pose estimation, with 88,329 RGB-D frames and diverse bloody surgical gloves patterns. We evaluate and fine-tune three current stateof-the-art methods on the POV-Surgery dataset. We prove the effectiveness of the synthetic dataset by showing the significant performance improvement of the SOTA methods in real-life cases with surgical gloves and tools."
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,,Fig. 1 .,
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,,Fig. 2 .,
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,,Fig. 3 .,
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,,Fig. 5 .,
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,,,
POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities,,Table 1 .,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,1,Introduction,"Navigating surgical instruments in vitreoretinal procedures requires extreme manual dexterity. Surgeons need to manipulate fine structures, such as an Epiretinal Membrane (ERM), which can measure up to only 60µm [16], or carefully placing a microsurgical cannula within the on average 250µm [6] thick retina for intra or subretinal injections.Accurate distance perception regarding the surgical instruments and the retina is of utmost importance for punctuated surgical action. Any unintentional contact with the retina could cause severe damage to important retinal cells. In conventional procedures, surgeons rely only on visual guidance from an operating microscope that only allows a top view. An endo-illumination probe is inserted into the eye to visualize the operating area. When surgical instruments are introduced, they cast a shadow onto the retina, relative to the position of the illumination probe. Such shadows provide one of the most essential cognitive cues to perceive the distance between the instrument and the retina and therefore are important for performing surgical action. Figure 1 shows frames of a video sequence during retinal membrane peeling. While the instrument approaches the retina, the distance to its shadow gradually decreases until the instrument tip coincides with the tip of its shadow at the retina.Besides performing a procedure through this conventional stereoscopic view, recently, other imaging modalities have also become available. Technical advances have led to the integration of Swept-Source Optical Coherence Tomography (SS-OCT) into surgical microscopes [2,3,7], enabling near video-rate, depth-resolved imaging. Such 4D OCT systems have the potential to generate advanced visualizations of surgical interactions and improved treatments. While previous studies have demonstrated the feasibility of 4D OCT-guided surgery [1,9], some important perceptual cues that are available in the microscopic images are not present in OCT, such as the instrument shadow generated by the illumination probe. The absence of such perceptual cues increases the burden for surgeons to adapt to 4D OCT. While instrument shadows still exist in OCT, they are generated by the OCT laser and occur when the laser beam is blocked by the instrument such that structures below the instrument are fully obscured in the imaging modality. This OCT shadow is fixed in perspective and always occurs directly beneath the instrument. Hence, it does not provide the same intuitive cognitive cues, especially when viewing the volume from a top view, which is the usual view during ophthalmic surgery. To enable perceptually similar features as generated by the illumination probe, advanced visualization techniques need to be explored. Such visualizations could provide cues that improve the general usability of 4D-OCT without deviating from the standard workflow and steepen the learning curve for surgeons.In this paper, we propose Semantic Virtual Shadows (SVS), a concept that integrates such visual cues for perceptual distance estimation in 4D OCT. By identifying shadow-casting and shadow-receiving objects in the OCT volume, we augment the shadow of surgical instruments on the retina. In particular, instrument-specific shadows can be generated in one case on the retinal surface, and in other cases, exclusively on deep-seated layers below the surface. Precomputing a semantic shadow volume texture prior to direct volume rendering (DVR) enables real-time performance and more flexibility in the rendering approach. We demonstrate the versatility of SVS by proposing two visualization approaches for different scenarios in retinal surgery, including retinal membrane peeling and subretinal injection."
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,2,Related Work,"So far, only a few works have addressed the integration of perceptual cues into OCT volume rendering algorithms for interactive surgical visualizations in 4D OCT. Among these, approaches to color voxels based on distance information have been explored as a means of conveying spatial perceptual understanding. In an early study [1], a color transfer function was applied to each voxel, based on the distance to the center of mass of the volume. In a later work, [14] proposed a visualization that leverages a perceptually linear color map, which is anchored by a retinal layer segmentation, thus encoding distance information to an anatomical reference depth in a color map. They furthermore employ shadow rays [10] to enhance surface structures. However, their method is limited to local shadow rays to achieve real-time performance. There are only a few works that deal with the generation of a shadow for distance estimation in 4D OCT. Only in [5] do the authors propose to define a virtual light direction orthogonal to the OCT A-scans and employ shadow rays to create a shadow projection on an external plane positioned next to the volume. Such visualizations, however, are perceptually similar to 2D lateral volume projections, do not convey correct distance cues in presence of curved structures, and require removing the gaze from the surgical area to perceive instrument distance.As opposed to previous works, we integrate an instrument-specific shadow augmented directly in the rendered OCT volume. Hence, our method aims to provide surgeons with familiar cognitive cues for perceptual distance estimation, as described in Sect. 1. Additionally, identifying shadow-casting and shadowreceiving objects allows the generation of instrument shadows on a specific retinal layer below the surface, which is not possible with previous approaches due to the self-shadowing of the retina."
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,3,Methodology,"We define a Semantic Virtual Shadow (SVS) as a shadow that is only generated by identified shadow-casting objects and is explicitly cast onto identified shadow-receiving objects. In our application, the primary use case of SVS is to generate visual cues for spatial distance perception. Taking into consideration shadow-casting and shadow-receiving objects as well as a virtual light source, a semantic shadow volume V s can be constructed as a 3D texture that assigns a shadowing factor to each voxel associated with a shadow-receiving object in the OCT volume. This precomputed V s is directly consumed by the direct volume rendering (DVR) to generate the instrument shadow augmentation. An overview of the proposed method is shown in Fig. 2.While in theory any segmentation method could be employed to identify, we combine efficient volume processing with a learning based approach to approximate the segmentations and achieve the required processing rates for 4D OCT. For our interventional scenarios, we define the instrument as the shadow-casting object. Inspired by [15], to segment the instrument we first generate a 2D average projection image along the OCT A-scan direction (Z-axis) usingfrom which A-scans containing parts of the instrument signal can be identified. We train a U-net style [8] architecture with a ResNet34 [4] backbone to generate a 2D binary instrument segmentation map M sc . Since the OCT signal is blocked at the instrument surface, the A-scans corresponding to identified pixels in M sc contain only instrument information. Therefore the instrument can be identified by thresholding the voxel intensities of A-scans obtained by M sc . We further define the retinal layer, on which the SVS is generated as the shadow-receiving object. Due to their terrain-like surface properties, the retinal layer surface can be defined by a 2D depth map M sr , indicating the relative surface position for each A-scan in the volume. Further, a distance parameter d r is introduced, defining the extent of the shadow-receiving object. To generate V s , a shadowing factor is calculated for each voxel p associated with a shadow-receiving object.Given the position of the virtual light source p l and L(p) = p l -p |p l -p| defining the light direction, V s (p) is calculated with:(sampling volume intensity values I(p) along the light direction through the entire volume. Note that, in our equations, the z axis corresponds to the axial Ascan direction. We precompute V s prior to volume raymarching using a compute shader program, enabling high update rates and flexibility in the design of the DVR algorithm that directly consumes V s . In the following, we propose two specific visualization approaches that integrate SVS according to equation ( 2) with different shadow-receiving objects, demonstrating the versatility and advantages of the method for different vitreoretinal procedures.  "
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,3.1,Shadow Augmentation on Surface Structures,"During surgical phases, in which the instrument is located above the retinal surface, unintentional contact needs to be avoided. By defining the retinal surface as the shadow-receiving object, the SVS generates cognitive cues for surface distance perception. We use the following 2D projection for M sr :where t s = 0.17 is an empirically chosen threshold. We render the OCT volume using classic Phong shading to visualize surface structure details while integrating V s to augment the instrument-specific shadow C(p) = C P hong (p) • V s (p). The top row of Fig. 3 shows frames of a volume sequence with Phong shading, while in the bottom the same volumes are rendered with both Phong shading and SVS."
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,3.2,Shadow Augmentation on Subsurface Structures,"In intra-and subretinal injection procedures, once reached the retinal surface, surgeons need to carefully guide the cannula to the injection target without damaging the underlying Retinal Pigment Epithelium (RPE) which is a comparably deep-seated layer in OCT. We propose to augment an instrument shadow explicitly on the RPE and visualize the retinal surface semi-transparent. Compared to the previous section, we define the RPE as the shadow-receiving object. Since in OCT imaging, the RPE typically is a hyper-reflective layer [17], M sr can be efficiently approximated by:During volume raymarching, we apply Phong shading models to visualize the instrument and render the retinal surface semi-transparent while preserving surface highlights, as presented in [12]. As previously, Eq. ( 3) is used to obtain the surface depth. For the RPE and underlying structures, we integrate V s and apply an RGB color map C RGB (p). During volume raymarching, this leads to the following convention, integrated via a piece-wise linear opacity function:Here, C P hongT ool (p) and C P hongILM (p) are the Phong shading models for the instrument and the retinal surface, respectively. We refer to Fig. 4 and in particular to our supplementary video demonstrating visualization examples."
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,4,Experiments and Results,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,4.1,Implementation and Comparative Time Profiling,"Our instrument segmentation network was trained on 2D projection images generated from OCT volumes using Eq. 1. In total 330 OCT volumes of resolution 391 × 391 × 720 (spiral scanning) acquired from a model eye and 160 volumes of resolution 128 × 512 × 1024 (linear scanning) acquired from ex-vivo porcine eyes were used to generate a data set of 3928 images, including data augmentation strategies. The axial projection images were manually labeled by two biomedical engineers. We used Pytorch 1.13 for model training and TensorRT 8.4 for optimization. We implemented our method using C++ and OpenGL 4.6 (Windows 10 with Intel Core i7-8700K @3.7 GHz, Nvidia RTX 3090Ti), employing a compute shader to generate the 2D projection images and V s , and a fragment shader for DVR. Table 1 shows the inference times of SVS compared to baseline shadow rays (SR), which were previously integrated for OCT visualization [14]. For ablation, we also compare SVS to only using a shadow volume buffer without semantic information (SB). The visualization parameters were adjusted in all methods to achieve a similar visual outcome. The volumes were rendered at a resolution of 3840 × 2160. To further demonstrate the potential of SVS to support surgical tasks, our method was integrated into the 4D SS-OCT system presented in [2] with a volume acquisition rate of 10Hz. Surgical actions with a forceps in a phantom eye model are demonstrated in our supplementary material. "
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,4.2,User Study,"To evaluate the effect of SVS for distance perception compared to baseline DVR in 4D OCT guided surgery, we conducted a user study with 12 biomedical experts (10 male, 2 female) familiar with OCT. We conducted the study in a virtual environment simulating 4D OCT using the method proposed in [11], which was displayed using stereo rendering in a VR (HTC VIVE Pro) headset. A 3D input device (3D Systems Geomagic Touch) was used to control a calibrated virtual surgical tool, where a motion scaling of 4 : 1 was applied to not affect the results by the limitations of manual dexterity. Prior to the study, participants were asked to perform vision tests to ensure normal visual acuity, including stereo vision, contrast vision, and normal visual field. The study data was anonymized and performed in accordance with the declaration of Helsinki.Users were asked to familiarize themselves with the interaction in the virtual environment before the study, reducing the impact of a learning curve on the study results. For each trial, we randomly positioned a small target point close to the retina, simulating anatomical structures that could be targeted in a surgical scenario. Participants were asked to move the instrument tip to the target and press a button to confirm the positioning. Each uses performed 10 trials, each in the following three variants: (i) baseline DVR with Phong shading as in [13], (ii) (SV S T ool ) treating the instrument as a shadow-casting object and the retinal surface as a shadow-receiving object, and (iii) SV S T oolT arget with both the instrument and the target as shadow-casting objects, as the target represents an anatomical structure that could theoretically be segmented in OCT. We provide examples of the study variants in our supplementary material. During the study, the order of the trial variants and the position of the virtual light source was randomized, while the view onto the retina was fixed at 10 • C from the axial  OCT direction. Over the progression of each trial, we extracted the distance error to the target and to the retina. With baseline DVR, a final mean targeting error of 0.29 mm (±0.18) was achieved, while with SV S T ool and SV S T oolT arget , mean errors of 0.19 mm (±0.15) and 0.12 mm (±0.07) where achieved respectively. After identifying unequal variances in the distributions, a Kruskal-Wallis test obtained significant differences between the methods (p < 0.001). Figure 5 shows the distance error to the target in axial OCT direction measured over the trial progression, along with a 95% confidence interval of the error values.With SVS, users approached with faster convergence to the target and less error variance toward the end of the trial. In addition, Fig. 6 shows the results of the NASA-TLX survey. Statistically significant differences (p < 0.001) in all categories could be obtained based on the ANOVA test, while equal variances were found in all categories."
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,5,Discussion and Conclusion,"The results of our user study indicate that the proposed method is able to generate effective perceptual cues for distance estimation in 4D OCT. This is reflected in improved targeting performance, as well as a high acceptance of the generated perceptual cues. Figure 5 suggests more robust targeting in both SV S T ool and SV S T oolT arget variants, implying higher confidence in the approach. Preliminary qualitative feedback from four clinicians confirmed the intuitiveness of SVS and its strong integration potential without changing the existing clinical workflow. An essential component of SVS is the identification of shadow-casting and shadow-receiving objects in the volume. Compared to previous methods, SVS is able to generate perceptual cues exclusively on subsurface structures. Precomputing a semantic shadow volume texture was shown to achieve processing rates suitable for 4D OCT systems with volume acquisition rates of 24.2Hz [7]. In the future, prior knowledge of the instrument's geometrical model could be integrated to enable more accurate shadow representations. We also plan to investigate optimal virtual light source positions and envision employing the illumination probe to control the virtual light for more intuitive interactions.In conclusion, the proposed SVS augments visual cues that are naturally not present in OCT, but essential in microscopic vitreoretinal surgery. The flexibility of our approach enables object-specific shadow generation not only on surface structures but also exclusively on subsurface structures, supporting various surgical procedures. We provided a general definition of object-specific shadow generation and demonstrated our method on a 4D SS-OCT system for live display. In our user study, SVS was shown to provide intuitive and effective visual cues for targeted instrument maneuvers."
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,,Fig. 1 .,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,,Fig. 2 .,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,,Fig. 3 .,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,,Fig. 4 .,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,,Fig. 5 .,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,,Fig. 6 .,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,,Table 1 .,
Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_39.
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,1,Introduction,"Spinal nerves play a crucial role in the body's sensory, motor, autonomic, and other physiological functions. Injuries to these nerves carry an extremely high risk and may even lead to paralysis. Minimally invasive endoscopic surgery is a common treatment option for spinal conditions, with great care taken to avoid damage to spinal nerves. However, the safety of such surgeries still heavily relies on the experience of the doctors, and there is an urgent need for computers to provide effective auxiliary information, such as real-time neural labeling and guidance in videos. Ongoing studies using deep learning methods to locate spinal nerves in endoscopic videos can be classified into two categories based on the level of visual granularity: coarse-grained and fine-grained tasks.Coarse-grained vision task focuses on object detection of spinal nerve locations. In this task, object detection models applied to natural images are widely transferred to endoscopic spinal nerve images. Peng Cui et al. [1] used the Yolov3 [2] model to transfer training to the recognition of spinal nerves under endoscopy, which has attracted widespread attention. Sue Min Cho et al. [3] referred to Kaiming's work and achieved certain results using RetinaNet [4] for instrument recognition under spinal neuro-endoscopic images. Fine-grained tasks require semantic segmentation of spinal nerves, which provides finer contours and better visualization of the position and morphology of nerves in endoscopic view, leading to greater clinical significance and surgical auxiliary value. However, there are still very few deep learning-based studies on fine-grained segmentation of nerves, one important reason is a lack of semantic segmentation models suitable for spinal nerve endoscopy scenarios. The endoscopic image has the characteristics of blur, blisters, and the lens movement angle is not large, which is quite different from the natural scene image. Therefore, a segmentation model that performs well under natural images may not still be applicable under endoscopic images. Another reason is medical data involves ethical issues such as medical privacy, and the labeling of pixel-level data also relies on professional doctors. Furthermore, endoscopic images of the inside of the spine are often only available during surgery, which is much more difficult than obtaining image datasets from ordinary medical examinations. These lead to the scarcity of labeled data, and ultimately it is difficult to drive the training of neural network models.In response to the above two problems, our contribution is as follows:• We innovatively propose inter-frame and channel attention modules for the spinal neural segmentation problem. These two modules can be readily inserted into popular traditional segmentation networks such as Unet [5], resulting in a segmentation network proposed in this paper, called Frame-Unet (FUnet, Fig. 1). The purpose of the inter-frame attention module is to capture the highly similar context between certain adjacent frames, which are characterized by the slower movement of the lens and high similarity of information such as background, elastic motion, and texture between frames. Moreover, we devised a channel self-attention module with global information to overcome the loss of long-distance dependent information in traditional convolutional neural networks. FUnet achieved leading results in many indicators of the dataset we created. Furthermore, FUnet was verified on similar endoscopic video datasets (such as polyps), and the results demonstrate that it outperforms others, confirming our model's strong generalization performance instead of overfitting to a single dataset. • We propose the first dataset on endoscopic spinal nerve segmentation from endoscopic surgeries, and each frame is finely labeled by professional labelers. The annotated results are also rechecked by professional surgeons."
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,2,Method,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,2.1,Details of Dataset,"The dataset was taken by the professional SPINENDOS, SP081375.030 machines, and we collected nearly 10000 consecutive frames of video images, each with a resolution of up to 1080*1080 (Fig. 2). The dataset aims to address the specific task of avoiding nerves during spinal endoscopic surgery. To this end, we selected typical scenes from the authors' surgical videos that not only have neural tissue as the target but also reflect the visual and motion characteristics of such scenes. Unlike other similar datasets with interval frame labeling [6,7], we labeled each image frame by frame, whether it contains spinal nerves or not. Each frame containing spinal nerves was outlined in detail as a semantic segmentation task. Additionally, we used images containing spinal nerves (approximately 4-5 k images) to split the dataset into training, validation, and test sets for model construction (65%: 17.5%: 17.5%)."
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,2.2,Network Architecture,"The overall network architecture is based on a classical Unet network [5] (Fig. 1). In the input part of the network, we integrate T frames (T > 1) in a batch to fully utilize and cooperate with the inter-frame attention module (IFA). The input features dimensions are (B, T , C, H , W ), where B is batchsize, C denotes number of channels, H and W are the height and width of the image. Since we use a convolutional neural network for feature extraction, the 5-dimensional feature map is first merged into the convolutional network with B and T channels. Afterwards, the convolutional features are fed into the IFA for inter-frame information integration.Unet's skip-connection method is a good complement to the information lost in the down-sampling reduction process, but it is difficult to capture the global contextual information due to the local dependency of the convolutional network. However, this global information is also crucial to the spinal nerve segmentation problem in the endoscopic scenario, so we inserted a channel self-attention module (CSA) in each up-sampling section with the global capability of the self-attention mechanism.  "
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,2.3,IFA Module,"To exploit the rich semantic information (such as blur, blisters, and other background semantics) between endoscopic frames, we designed the IFA to correlate the weight assignment between T frames. (Fig. 3). If the features extracted by convolutional feature extraction are (B × T , C, H , W ), we first need to split the features to obtain (B, C × T , H , W ), which expands the channel dimension and allows subsequent convolutional operations to share the information between frames. After that, the feature matrix is down-sampled by four Attention Down Block's (ADB) channel pyramid architecture to obtain the (B, T , 1, 1) vector, and each (1, 1) weight value of this vector in T dimension will be assigned to the attention weight of the segmentation result of T frames.Channel Pyramid. Although the multiscale operation in traditional convolutional neural networks can improve the generalization ability of the model to targets of different sizes [8,9], it is difficult to capture information across frames, and down-sampling losses spatial precision at each cross-frame scale. Meanwhile, in many cases of endoscopic video, only the keyframes are clear enough to localize and segment the target, which can guide other blurred frames.Hence, we propose a channel pyramid architecture to compress the channel dimension for capturing the cross-frame multiscale information, as well as to keep the feature map size unchanged in dimensions of height and width for preserving spatial precision. Such channel down-sampling obtains multi-scale information and semantic information in different cross-frame ranges. The result can adjust the frame weight on keyframe segmenting guidance. For detail, the feature matrix obtained by each ADB is compressed in the channel dimension, which avoids the loss of image size information. Like the perceptual field in the multi-scale approach, the number of inter-frame channels in the channel pyramid at different scales represents the magnitude of the scale across frames, and this inter-frame information at different scales is contacted for further fusion calculations, which is used to generate attention weights.Attention Down Block (ADB). This module (Fig. 3) is responsible for downsampling the channel dimension and generating the weight information between frames at one scale. We use the Light-RFB [10,11] module for channel down-sampling without changing the size in the height and width dimensions. In terms of the generation of attention weights, the feature vector after the adaptive global average pooling operation will be scaled to the T dimension by two 1 × 1 convolutions. "
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,2.4,CSA Module,"Inspired by the related work of vision transformer [12,13], we propose CSA mechanism (Fig. 4) to address the problem of the lack of global information of traditional convolutional networks under spinal nerve endoscopy videos. Different from the classical vision transformer work, firstly, in the image patching stage, we use the convolution operation to obtain a feature matrix with a smaller dimension (such as B × C × 32 × 32), and the length corresponding to each patch is the length of each pixel channel (32 × 32), which reduces the amount of computation while sharing of information between different patches. This is because, in the process of convolution down-sampling, the convolution kernels will naturally cause overlapping calculations on the feature map, which will lead to the increase of the receptive field and the overlap of information between different patches. We use three fully connected layers Lq, Lk, Lv to generate Query, Key and Value feature vectors, this can be expressed as follows:X ∈ R B×(H ×W )×C is the original feature matrix expanded by the flatten operation. At the same time, to supplement the loss of spatial position information, we supplement the pos embedding vector by addition operation.The multi-headed attention mechanism is implemented by a split operation. For calculation of self-attention, we use the common method [14], the Query matrix and the transpose matrix of Key are multiplied and then divided by the length d of the embedding vector, and this part of the result will be multiplied with Value after soft-max operation. Finally, this part of the features operated by self-attention will enter the LN layer, and after the sigmoid operation, the dimensions of the final attention weight matrix are consistent with the input vector. The formula for this part is as follows:3 Experiments"
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,3.1,Implementation Details,"Dataset. The self-built dataset is our first contribution. We set up the training set, test set, and validation set. For extended experiments on the polyp dataset, we used the same dataset configuration as PNS-Net [11]. The only difference is that in the testing phase, we used Valid and Test parts of the CVC-612 dataset [6] for testing (CVC-612-VT).Training. On the self-built dataset and the CVC-612 dataset, we both use learning rate and weight decay of 1e-4 with Adam optimizer. On the self-built dataset, our model converges after about 30 epochs, while on the CVC-616 dataset, we use the same pre-training and fine-tuning approach as PNS-Net. Methods involved in the data augmentation phase include flipping and cropping. A single TITAN RTX 24 GB graphics card is used for training.Testing. Five images (T = 5) are input to the IFA and use the same input resolution of 256*448 as PNSNet to ensure consistency in subsequent tests. Our FUnet is capable of inference at 75 fps on a single TITAN RTX 24 GB, which means that real-time endoscopic spinal nerve segmentation is possible in surgery."
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,3.2,Experimental Results,"On our spinal nerve dataset, we tested the classical and leading medical segmentation networks Unet [5], Unet++ [15], TransUnet (ViT-Base) [16], SwinUnet (ViT-Base) [17] and PNSNet [11] respectively. For comparison, we used the default hyperparameter settings of the networks and employed a CNN feature extractor consistent with that of PNSNet. Four metrics that are widely used in the field of medical image segmentation are chosen, maxIOU, maxDice, meanSpe/maxSpe and MAE. The quantitative result is in Table 1. Our FUnet achieves state of the art performance on different medical segmentation metrics. The Our-VT dataset means that we use the validation and test datasets for the testing phase (neither of which is involved in training).The qualitative comparison is in Fig. 5, which shows our FUnet can more accurately segment the contour of the model and the texture of the edges.  "
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,3.3,Ablation Experiments,"The baseline model uses Res2Net [18] as the basic feature extractor with the Unet model, and in the first output feature layer, we adopt a feature fusion strategy consistent with PNSNet. We have gradually verified the performance of the IFA and CSA modules on the baseline model (Table 2), and experiments have proved that our two modules can stably improve the segmentation performance of spinal nerves in endoscopic scene. A comparison of qualitative results is available in Fig. 6.  In addition, more extended experiments are carried out under a similar endoscopic polyp dataset CVC-612 [6] (Table 3, CVC-612-TV means that we used both test and valid parts during the testing phase, the validation part was not visible during the training phase.), and the experiments show that our FUnet has good generalization performance and can adapt to endoscopic segmentation in different scenarios. A comparison of qualitative results is available in Fig. 7, our FUnet still performs well in its ability to segment the edges of polyps.  "
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,4,Conclusion,"In this paper, we propose the industry's first semantic segmentation dataset of spinal nerves from endoscopic surgery to date and design the FUnet segmentation network based on inter-frame information and self-attention mechanism. FUnet has achieved state of the art performance on our dataset and shows strong generalization performance on polyp dataset with similar scenes. The IFA and CSA modules of FUnet can be easily incorporated into other networks. We plan to expand the dataset in the future with the help of self-supervised methods, to improve the performance of the model to provide better computer-assisted surgery capabilities for spinal endoscopy."
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Fig. 1 .,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Fig. 2 .,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Fig. 3 .,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Fig. 4 .,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Fig. 5 .,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Fig. 6 .,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Fig. 7 .,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Table 1 .,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Table 2 .,
Spinal Nerve Segmentation Method and Dataset Construction in Endoscopic Surgical Scenarios,,Table 3 .,
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,1,Introduction,"Intervention options for the treatment of abdominal aortic aneurysms (AAAs) include open surgery or endovascular repair (EVAR). A minimally invasive procedure, EVAR involves the peripheral delivery of one or more covered endografts to the aneurysmal segment via a catheter-based system. EVAR has been demonstrated to be superior to open surgery with regards to early mortality [1,2]. A particularly challenging group of patients to treat are those in which the aneurysmal sac extends proximally to include the origin of the renal arteries. These are known as juxtarenal or ""complex"" AAAs and account for roughly 15% of all AAAs [3].With complex AAA repair, it is of paramount importance to maintain the patency of major aortic side branches to allow for end-organ perfusion. Traditionally, open surgery was preferred for these complex cases; however, with the advent of endografts that have pre-made openings (fenestrations) in the graft material, these patients can be treated using fenestrated endovascular repair (FEVAR). The concept of in situ fenestration (ISF) for endografts during FEVAR has been proposed as an alternative to the use of pre-fenestrated endografts. With ISF, the fenestrations are generated within the aorta following deployment of the endograft. Potential benefits of this approach include both greater anatomical conformity and reduced device cost. Once ISF has been performed, a wire is passed through the fenestration into the side-branch. The fenestration is then dilated with incrementally sized non-compliant or cutting balloons and secured with a stent that maintains communication between the aorta and side-branch [4]. Several fenestration methods have been proposed, of which mechanical puncture and thermal ablation of stent material are the most prominent. Thermal ablation methods for ISF are attractive as there is no requirement to exert penetrative force on the endograft material. Instead, the material is vaporized through the use of radiofrequency (RF) [5] or laser energy, in which 94% success was achieved in 16 patients with no major complications [6].A key challenge with ISF that we address here is to visualize aortic side branches in order to precisely identify the locations for fenestration. Pre-procedural imaging in concert with X-ray fluoroscopy can be used for guidance [7]; however, distortion of the aorta due to deployment of the endograft limits the accuracy of this technique [8]. All-optical ultrasound (OpUS) is an emerging imaging modality that involves the optical generation and reception of ultrasound [9], which can be performed with fiber optics that are readily integrated into medical devices [10]. OpUS is a potentially attractive option for guiding ISF for several reasons: the depth penetration achieved for OpUS (>1 cm) is relevant to aneurysmal vessels, high bandwidths enable high resolution imaging (ca. 50-100 µm), and a blood-free environment is not required for image acquisition. The feasibility of visualizing aortic side branches with OpUS has previously been demonstrated [9,11] and intracardiac imaging in vivo has been performed [10]. In this study, we explore the application of OpUS to EVAR and ISF, by addressing two open questions: does this technique permit visualization of side branches through endograft material, and can the optical transducers be integrated into a steerable sheath with forward imaging that also permits optical fenestration?"
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,2,Methods,
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,2.1,Device Design,"The custom device comprised two primary fiber optic components which were integrated within a catheter: 1) an OpUS transmitter/receiver pair for visualizing side branches; 2) an ISF fiber for optical fenestration. These optical fibers were secured together using heat shrink and housed within a 6 Fr [inner diameter (ID): 2 mm] catheter (Multipurpose-1, Cordis, Santa Clara, CA, USA), leaving a 0.7 mm diameter channel to allow a 0.014 or 0.018 guidewire to be delivered through a generated fenestration. The catheter was delivered to the endovascular stent graft using a 7 Fr ID steerable sheath (Tourguide, Medtronic, USA), a current standard that allowed for bi-directional 180°/90°deflection with manual control and thereby allowed the catheter to be appropriately positioned to detect side-branches behind the endovascular stent (Fig. 1).Fig. 1. Custom device for performing all-optical ultrasound imaging and fenestration during endovascular repair (EVAR), shown schematically. The device comprised an optical ultrasound transmitter and receiver for imaging, and a third optical fiber to deliver light for in situ fenestration (ISF) of endograft stent material. A custom fiber optic contact sensor was not used in this study. This device was enclosed by heat shrink and positioned within two off-the-shelf, commerciallyavailable devices: a diagnostic catheter and a steerable sheath.The OpUS transmitter comprised a custom nanocomposite material for photoacoustic generation of ultrasound, with optical absorption of excitation light by a near-infrared absorbing dye embedded within a medical-grade elastomeric host [12] that was applied to a 400 µm circular surface following the design of Colchester et al. [13]. The OpUS receiver comprised a plano-concave microresonator positioned at the distal end of a fiber. These fiber optic OpUS components were interrogated by a custom console that delivered pulsed excitation multi-mode light to the transmitter (wavelength: 1064 nm; pulse duration: 2 ns; pulse energy: 20.1 µJ; repetition rate: 100 Hz) and wavelengthtunable continuous wave single-mode light to the receiver (1500-1600 nm; 4 mW). This custom console, previously described in detail [9], comprised software for real-time processing and display of concatenated A-scans as M-mode OpUS images. The ISF fiber for fenestration had an inner diameter of 400 µm and transmitted light from a laser unit that permitted control of the pulse duration and power (Diomed D15 Diode, Diomed Holdings, USA; wavelength: 808 nm; maximum power: 4W). This wavelength was chosen to minimize damage to the underlying vascular tissues [14]."
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,2.2,Benchtop Imaging and Fenestration,"To assess whether OpUS permits visualization of side branches through endograft material, a benchtop model with ex-vivo swine aorta (Medmeat Ltd, United Kingdom) was used. A section of the aorta was splayed open longitudinally to create a flat surface and secured to a cork board. Endograft stent material (Zenith Flex® AAA, Cook Medical, USA) made from a synthetic polyester Dacron Material (DM) (polyethylene terephthalate) was positioned above the aortic sample. An OpUS imaging scan was performed with 600 lateral steps of length 50 µm using a motorised stage [9].To test the effects of different laser parameters on optical fenestration, the following were varied: the pulse energy, the pulse duration, and the distance between the distal end of the fenestration optical fiber and the endograft material. Fenestrations with different combinations of these parameters (2.5W, 0.5s; 1.8W, 0.5s; 4.2W, 0.5s; 4.2W, 1s; 4.2W, 5s) were attempted in different sections of aortic tissue. A piece of endograft material was secured above each tissue section; the distance from the endograft material to the tissue was approximately 2 mm. The tissue and endograft material were immersed in anticoagulated sheep blood (TCS Biosciences, Buckingham, UK). A fenestration was deemed to have been successfully achieved if adjacent tissue was visible when the surrounding blood was drained."
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,2.3,In Vivo Imaging and Fenestration,"To obtain a preliminary validation of the system's potential for guiding EVAR and ISF, OpUS imaging and endograft fenestration were performed in a clinically realistic environment using an in vivo porcine model. The primary objectives of this experiment were a) to visualize an aortic side branch behind a deployed endograft; b) to perform optical fenestration of the endograft. All procedures on animals were conducted in accordance with U.K. Home Office regulations and the Guidance for the Operation of Animals (Scientific Procedures) Act (1986). The protocol for this study was reviewed and ratified by the local animal welfare ethical review board.Following successful anesthesia, transcutaneous ultrasound (US) was used to gain percutaneous access to both common femoral arteries; 8 Fr vascular sheaths were inserted bilaterally. Both fluoroscopic acquisition and digital subtraction angiography with an iodine-based contrast agent were performed to create a road-map for further stages of the experiment. The ISF device was inserted via the Right Femoral Artery (RFA) and directed with the steerable catheter into the aorta. The inner catheter of the ISF device was advanced into the aorta.For the EVAR procedure, a non-fenestrated iliac endograft was used (Zenith Spiral-Z, Cook Medical, USA; proximal/distal diameter: 13 mm/11 mm; length: 129 mm). The choice of an iliac endograft originally for human patients was for the comparatively smaller dimensions of the swine aorta. The sheath in the RFA was up-sized to a 14 Fr in order to accommodate the delivery system. The stent was deployed across the renal artery bifurcation. The ISF device was inserted into the lumen of the endograft and OpUS imaging was performed with a longitudinal pullback across the covered left renal artery. Once positioned over the desired location, a single pulse from the 808 nm fenestration laser (power: 4.2 W; pulse duration: 5 s) was delivered to the endograft to create a fenestration. A coronary guidewire (0.018 ; Balanced Middleweight Universal, Abbott, USA) was then inserted through the ISF device and through the fenestration into the side-branch. Expansion of the fenestration in the endograft was then performed with a series of three balloons (2 mm non-compliant, 3 mm non-compliant and 3 mm cutting) to create a fenestration large enough to accommodate a stent. Following successful ISF, the procedure was ended and the ISF device was removed."
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,3,Results,
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,3.1,Benchtop Imaging and Fenestration,"With OpUS imaging of the paired endograft and underlying aortic tissue, the stent graft manifested as an echo-bright linear structure (Fig. 2). Aortic tissue beneath the graft could be imaged to a depth >6 mm from the transducer surface (>3 mm from the tissue surface). The pullback revealed the side-branch within the aorta, which was apparent as a well demarcated area with signal drop-out, consistent with an absence of tissue.During benchtop attempts, a fenestration was successfully created with a pulse duration of 0.5 s and an optical power of 1.8 W when the distal end of the fiber was in contact with the endograft material. This combination of pulse duration and optical power corresponded to the smallest energy that yielded fenestration with this device. There was no macroscopic tissue damage with these parameters. The size of the fenestration increased with both higher power outputs and longer laser pulse durations. When the distal end of the optical fiber was recessed a distance of 3 mm from the endograft material, both a higher power output and longer pulse duration were required to generate a fenestration. No macroscopic tissue damage was identified during any of the fenestration experiments. As a control, the optical fiber was placed directly on the tissue with no stent material. In this configuration, a power of 4.2 W and a pulse duration of 5 s resulted in a macroscopic thermal ablation of the tissue."
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,3.2,In Vivo Imaging and Fenestration,"The renal bifurcation was readily observed with fluoroscopic guidance and X-ray contrast injections. Due to the small diameter of the swine aorta relative to that of an adult human, it proved challenging to achieve sufficient bending of the steerable sheath whilst maintaining a gap between the imaging device and the aortic tissue. In this configuration, bend-induced losses in the fiber optic receiver significantly lowered the signal-to-noise (SNR) ratio relative to that obtained during benchtop imaging. Nonetheless, a manual pullback with imaging performed at a slightly non-perpendicular angle relative to the aortic surface proved feasible. With OpUS imaging during this pullback, the endograft material presented as a strongly hyperechoic region and the underlying aorta could be imaged to a depth >3 mm. A side branch presented as a hypoechoic region consistent with an absence of tissue within the renal vessel lumen (Fig. 3a).For ISF, the laser output parameters that were chosen (4.2 W, 5 s) were intentionally higher than the minimum values observed in benchtop experiments. Following the ISF, the guidewire was passed into the renal artery. A post-mortem dissection confirmed a successful optical fenestration (Fig. 3b). Passage of the guidewire through the fenestration in the endograft material was confirmed with fluoroscopic imaging performed in vivo (Fig. 3c). No tearing in the endograft material that may have arisen from balloon-induced expansion was apparent (Fig. 3b). "
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,4,Discussion,"A key observation of this study is that OpUS offers direct visualization of aortic tissue beneath an AAA endograft, and detection of vascular side-branches. This capability is important for appropriate ISF positioning, thereby maintaining side-branch patency. The ability to detect a side-branch immediately before performing fenestration with the same device could reduce the risk of inappropriate positioning and the subsequent need for conversion to open surgery. In current clinical practice, there is no device that allows for direct imaging of aortic side branches with the added capability of achieving endograft fenestration.Current methods for generating ISF, although promising, are not guided by intravascular imaging. With these methods, there is a risk of creating a fenestration that is not aligned with the side-branch, leading to a potential endoleak, and a risk of causing injury to underlying tissue structures. The potential cost implication of ISF is significant. The average costs of pre-fenestrated and patient-specific grafts are ca. £15,400 and £24,000 (GBP), respectively; the cost of a non-fenestrated aortic endograft that could be used for ISF is ca. £6,000.Several improvements to the device can be envisaged. An increase in the lateral OpUS imaging resolution could be effected with greater US beam collimation, for instance with the use of a larger-diameter transmitting surface; however, this change would likely result in increased device size and reduced flexibility. Improvements in the SNR of OpUS images could potentially be achieved with depth-dependent frequency filtering and with the application of deep-learning methods [15]. In future iterations, imaging and ISF could be performed with a single optical fiber [16,17], thereby reducing device size, increasing device flexibility/deliverability and potentially reducing device cost. Using the same ultrasonic receiver used for reception during imaging, it could be possible to use ultrasonic tracking to localize the device relative to an external imaging device [15,18,19], thereby reducing the dependency on fluoroscopic image guidance and corresponding contrast injections.During the in vivo procedure in this study, the power of the fenestration laser was chosen to be higher than the minimum value observed in the benchtop component of this study to increases the chances of success. It remains to be seen whether a lower output setting may be viable in this context, with the presence of device motion and flowing blood and with pathological changes in the arterial wall. In future studies, real-time feedback from OpUS imaging could be incorporated to determine the power and pulse duration of the fenestrating laser; a similar probe could be used for cardiac ablation, for instance in the context of treatment of atrial fibrillation [20].This study presented a novel imaging probe for performing interventional image guidance for endovascular procedures. We conclude that OpUS imaging is a promising modality for guiding EVAR; in concert with and optical fenestration, it could find particularly utility with identifying aortic side branches and performing ISF during treatment of complex AAAs."
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,,Fig. 2 .,
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,,Fig. 3 .,
Optical Ultrasound Imaging for Endovascular Repair of Abdominal Aortic Aneurysms: A Pilot Study,,,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,1,Introduction,"Cataract surgeries are amongst the most frequently performed treatments, with 4,000 to 10,000 annual operations per million people [27]. The high demand naturally asks for automation and advanced assistance systems and has seen increasing attention within the CAI community in recent years [1,6,22].Nevertheless, the publicly available data for training such systems is limited: given the nature of the surgeries, certain surgical phases take more time than others. Further, there are variances in their length based on the surgeon's skill and the patient's particular needs. Since surgical tool usage is strongly coupled with the surgical phase, certain phases and tools are shown more frequently than others, constituting an inherent imbalance in the data. As displayed in Fig. 1 for the CATARACTS dataset [1], such imbalances impact the performance on downstream tasks, e.g. surgical phase prediction or tool classification.One cannot simply gather new data showing unusual tools to perform the required actions during a surgical step. Therefore, we must find different ways to represent them in the data and counteract the imbalance. The usual countermeasures in the form of oversampling can increase prediction accuracy [1,6,22]. Still, they only alter the number of times an underrepresented sample is seen during training, resulting in a fragile representation of the tools and phases and hindering generalisation. Generative models [9,18,24,26] can potentially solve this by synthesising unseen examples for underrepresented tool and phase combinations. Regarding image quality, generative models based on diffusion models reached superior performance over alternative methods in the recent past [4,7,16,21,25]. Despite the successes, these models have not yet found application in Surgical Data Science. In the broader medical domain, they have been utilised to generate thorax CT scans [10], brain MRI [5,10] and breast and knee MRI scans [10].Although these applications have shown promising results, there is a demand for conditional image generation for Surgical Data Science. Since most downstream applications consist of supervised methods, they require training targets, and the likelihood of unconditionally generating diverse samples for unusual cases is very low. For conditional generation with denoising diffusion models, classifier guidance has been introduced recently [4,15] but requires computationally extensive parallel training of a separate classifier model. Instead, Classifier-Free Guidance (CFG) [8] yields a simple trick to achieve class-constrained generative Fig. 2. Ground truth toolset occurrence for the worst performing CATARACTS phases. Some toolsets, e.g. (Bonn Forceps, Capsulorhexis Forceps) during Implant Ejection (left), are rarely present and poorly detected, deteriorating the overall performance. We focus on such rare toolsets to generate new samples for a phase. E.g. for Manual Aspiration (middle), we mainly want additional samples showing the Hydrodissection Canulla. The chord diagram for the Suturing phase (right) shows the complexity of such occurrences.results with diffusion models. Conditional diffusion models have been applied to generate medical images from a few binary label inputs [14,19]. Peng et al. [17] synthesise 3D volumes from 2D reference slides. Sagers et al. [23] have built on DALL•E2 for the targeted generation of images of skin diseases, and Moghadam et al. [13] have generated histopathology images with genotype guidance.Precise conditioning beyond a few binary labels is crucial for synthesising valuable surgical data. Instead, we need to train a model that can generate diverse examples based on multi-class or multi-label conditions, e.g. certain surgical phases, combinations of surgical tools, or both. We show that using an adapted denoising diffusion model together with CFG can yield high-quality samples of cataract surgery data, even for rare cases such as the CATARACTS phase and tool combinations shown in Fig. 2.To the best of our knowledge, ours is the first work combining CFG with diffusion models to efficiently generate realistic cataract surgery data with a complex underlying label structure. Additionally, we examine the cataract video data for the worst-performing phases of a pre-trained tool usage classifier. We then leverage the conditional denoising diffusion model to generate unseen samples for these phases. Our conditioned tools are recognisable by the tool classifier and are hard to differentiate from real images, even for clinicians with more than five years of experience. Further, we demonstrate how our synthetically extended data can alleviate the data sparsity problem for the downstream task. Overall, our evaluations show that the model can generate valuable examples to build the bridge to clinical application."
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,2,Method,"The following section describes how we build our generative diffusion model and integrate CFG to generate cataract surgery frames conditioned on surgical phases and tools. Furthermore, we provide an analysis of the worst-performing surgical steps for a pre-trained tool classifier model. Finally, we demonstrate the sampling procedure using the generative model to improve the classifier."
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,2.1,Denoising Diffusion Probabilistic Models,"The fundamental underlying idea of Denoising Diffusion Probabilistic Models (DDPMs) [7] is a forward diffusion process that gradually adds Gaussian noise to an image x. This process is defined by q(x t |x t-1 ) = N (x t ; √ 1β t x t-1 , β t I), which uses a pre-defined variance schedule {β t ∈ (0, 1)} T t=1 . Eventually, when T → ∞, x T becomes equivalent to an isotropic Gaussian distribution. When we can learn the reverse process q(x t-1 |x t ), we can generate samples starting from a simple Gaussian noise x T ∼ N (0, I). To achieve this, one can approximate the conditional probabilities by p θ (x t-1 |x t ) = N (x t-1 ; μ θ (x t , t), Σ θ (x t , t)). In practice and after some mathematical simplifications, this reduces the reverse process to estimating the noise t between x t and x t+1 , as shown by Ho et al. [7]. Usually, the noise is parameterised by a UNet-type architecture θ , optimised by minimising the simplified objectivewhereDenoising Diffusion Implicit Models (DDIMs) [25] are a generalisation of these formulations for non-Markovian, more efficient sampling. Instead of the complete Markov chain, they are defined on a reduced set of intermediate latents {x τ1 , ..., x τS }, where [τ 1 , ..., τ S ] ⊆ [1, ..., T ]. This reduction results in significantly fewer inference steps required to generate samples."
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,2.2,Classifier-Free Guidance,"By utilising Classifier-Free Guidance (CFG) [8], we can learn the unconditional model p θ (x) and the model p θ (x|y (p) , y (s) ) conditioned on phase y (p) and toolset y (s) using a single neural network. The corresponding gradient is given byThis gradient yields where w is a weighting hyperparameter. The weighted noise ¯ θ (x t , t, y (p) , y (s) ) can simply replace in Eq. 1. We add an embedding module emb (p) to the UNet architecture, which converts categorical phase labels y (p) into one-hot encoded vectors to include them into the input. To simultaneously include tool labels in the form of (non-exclusive) binary vectors, we compute projections emb (s) of the same size as the time-step and phase label embeddings using stacked dense layers. All embeddings are concatenated as {emb t (t), emb (p) (y (p ), emb (s) (y (s) )} and fed to the conditional denoising UNet model together with the noisy image x t . Figure 3 visualises the forward process, reverse process, and sampling procedure."
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,2.3,Tool Usage Analysis and Sample Generation,"Following Roychowdhury et al. [1,22], we deploy a ResNet50 architecture to predict tools present in a given frame. An inspection of the phase-wise performance reveals that the model underperforms for underrepresented phases, as shown in Fig. 2. Certain tool combinations are sparsely used in these phases, causing a significant drop in prediction performance. Appendix Fig. 6 displays the distribution of toolset labels for all surgical steps. We synthesise new examples for every phase to smooth out the distribution. We then re-train the classifier model on the original and extended data combined. Adding samples based on toolsets y s and phase labels y p requires a throughout pre-selection of query inputs due to the complex underlying latent structure. To automatise this process, we compute the joint probabilities p φ (y s , y p ) from the available CATARACTS annotations. We can then generate rare cases for a given phase by sampling tool labels from the inverse of p φ (y s , y p ) = p(y p )p(y s |y p ). This ensures we synthesise examples of underrepresented cases."
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,3,Experiments and Results,"In this section we explain our experimental setup, demonstrate the synthesis of high-quality samples and show how these can improve the downstream model's performance on challenging phases. "
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,3.1,Experimental Setup and Dataset,"To evaluate the quality of synthesised images, we generate 30,000 samples with phase and toolset conditions sampled from p -1 φ (y s , y p ) = (1p φ (y s , y p ))/ (1p φ (y s , y p )), as explained in Sect. 2.3. The resulting number of examples is close to the test split size of CATARACTS sampled at 3 FPS. We compare the proposed approach to state-of-the-art baselines for conditional generative modelling: A conditional LS-GAN (cLS-GAN) [11,12] and VQ-VAE2 [20]. For the latter, we deploy a PixelSNAIL prior [3] for bottom-and top-level features. Every model is trained on two NVIDIA A40 GPUs for 500 epochs with about 45,000 training examples each. Other hyperparameters vary for each model and can be accessed next to the code to reproduce our results and the generated data at https:// github.com/MECLabTUDA/CataSynth. We take τ S = 200 denoising steps using the DDIM formulation [25] to generate our images, yielding a reasonable inference speed and sample quality trade-off. The displayed and evaluated images are generated with a CFG weight of ω = 2.0. We use a random chance of p = 0.1 for the unconditional model during training. All models are trained on images of 128 × 128 pixels and up-sampled with bilinear interpolation to 270 × 480 for displaying purposes."
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,3.2,Quantitative Image Quality,"We deploy a variety of quantitative metrics to assess the quality of generated images, for which Table 1 lists the results. Firstly, FID [2] and KID [2] compare the spatial distribution of the synthesised images with the training set distribution of CATARACTS. Further, we use the classifier from Sect. 2.3 to obtain the Inception Score (IS) of the generated images. By using the scores of a classifier trained for tool recognition, this metric yields a measurement of tool realism. Additionally, we evaluate the F1 score for the pre-trained classifier identifying tools in the conditionally generated images. We denote this metric as CF1. Lastly, we also compute the perceptual LPIPS diversity [28] to catch mode collapses, a common problem with generated models, leading to reduced image variability. In summary, our model generates images of superior quality regarding spatial properties, tool label preservation and diversity. "
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,3.4,Downstream Tool Classification,"Finally, we re-train the tool-set classifier on a combined dataset of the original and the synthesised samples. As shown in Table 3, re-training on the combined data (Extended ) improves the tool-set classifier's prediction performance on the original test data compared to training solely on the original training data (Original ). For completeness, we also report the performance from fitting the classifier exclusively on the synthetic data and evaluating it on the test split of CATARACTS, denoted CAS [20]. Figure 5 displays the individual differences in the classifier's F1 scores for the originally worst-performing phases. Extending the data with synthetic samples yields performance gains for five of the seven most critical phases.  "
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,4,Conclusion,"We present a generative model based on denoising diffusion models and classifierfree guidance, powerful enough to synthesise cataract surgery images that are hard to distinguish for a pre-trained tool classifier and clinical experts. For underrepresented phases, state-of-the-art baselines tend to produce frames that show eyes without correct anatomy or barely recognisable tools, resulting in a significant performance gap. Distortions in the dataset further deteriorate their learning. On the contrary, we demonstrate that the proposed approach outperforms these baselines in terms of image quality and tool preservation. As a limitation of our approach, we found that the generalisation capabilities must be strengthened to generate unreasonable samples, e.g. completely wrong tools during a phase. Such samples can happen if they are present in the data. Though, a targeted generation would require a more substantial representation. Additionally, while tool realism is significantly better for the proposed method, the CF1 and CAS scores indicate that it can be further improved. Besides, the underlying class imbalances and lack of available data are even more severe for the downstream task of anatomy and tool segmentation. In future work, we will extend the proposed method to generate segmentation targets, temporally connected data and deploy a tighter structure for conditioning. Overall, we are the first to have shown how conditional diffusion models can successfully be applied to mend data sparsity and generate high-quality cataract surgery images suitable for clinical application. These improvements can bring computer-assisted cataract surgery one step closer to the next level of automation."
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,,Fig. 1 .,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,,Fig. 3 .,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,,Fig. 4 .,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,,Fig. 5 .,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,,Table 1 .,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,,experts favouring the gener- ated images 61% of the time,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,,Table 2 .,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,,Table 3 .,
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 34.
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,1,Introduction,"The human brain undergoes rapid and dynamic development during the early postnatal period. Research on infant brain development [1][2][3] has received significant attention over the last decade. While many infant neuroimaging studies have revealed the brain growth patterns during infancy [4][5][6] at the population level, knowledge on individualized brain development during infancy is still lacking, which is essential for mapping individual brain characteristics to individual phenotypes [7,8]. Various pioneering methods have been developed using longitudinal scans collected at some pre-defined discrete age time points, such as 3, 6, 9, and 12 months [9][10][11], which strictly requires each infant has all longitudinal scans at the given time points. However, in practice, longitudinal infant images are usually collected at diverse and irregular scan ages due to the inevitable missing data and imaging protocol design, leading to less usable data. As a result, predicting infant development at specific ages from irregular longitudinal data is extremely challenging, yet critical to understanding normal brain development and early diagnosis of neurodevelopmental abnormalities [12][13][14]. Besides, to the best of our knowledge, joint prediction of cognitive scores and cortical morphology is barely investigated, despite some studies revealing the tight relationship [15][16][17] between cognitive ability and cortical morphology, suggesting their potential mutual benefits.To address this issue, we propose a flexible multi-task framework to jointly predict cognitive score and cortical morphological development of infant brains at arbitrary time points with longitudinal data scanned irregularly within 24 months of age. Specifically, the cognitive ability of each infant was estimated using the Mullen Scales of Early Learning (MSEL) [18], including the visual reception scale (VRS), fine motor scale (FMS), receptive language scale (RLS), and expressive language scale (ELS). Our aim is to predict the four cognitive scores and cortical morphological feature maps (e.g., cortical thickness map) flexibly at arbitrary time points given cortical feature maps at a known age. The main contributions of this paper can be summarized as follows: 1) we propose an attention-based feature disentanglement module to separate the identity-and age-related features from the mixed latent features, which not only effectively extracts the discriminative information at individual-level, but also forms the basis for dealing with irregular and incomplete longitudinal data; 2) we introduce a novel identity conditional block to fuse identity-related information with designated age-related information, which can model the regression/progression process of brain development flexibly; 3) we propose a unified, multi-task framework to jointly predict the cognitive ability and cortical morphological development and enable flexible prediction at any time points during infancy by concatenating the subject-specific identity information and identity conditional block. We validated our proposed method on the Baby Connectome Project (BCP) [19] dataset, including 416 longitudinal scans from 188 subjects, and achieved superior performance on both cognitive score prediction and cortical morphological development prediction than state-of-the-art methods."
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,2,Methods,
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,2.1,Network Architecture,"The framework of our disentangled intensive triplet spherical adversarial autoencoder (DITSAA) is shown in Fig. 1. Since primary and secondary cortical folds in human brains are well established at term birth and keep relatively stable over time, the identityrelated cortical features should be approximately time-invariant and age-related features change over time due to brain development. Based on this prior knowledge, we first disentangle the mixed cortical surface feature maps into age-related and identity-related components. Specifically, in the training stage, we formulate the training samples in triplet units (S t a i , S t p i , S t n j ), where the first two are surface property maps from the same individual i but at different ages t a and t p , and the last one is a surface property map from another individual j at any age t n . Then we employ the intensive triplet loss [20] to encourage the disentanglement of identity-related features, and age prediction loss to encourage the disentanglement of age-related features. Encoder with Attention-Based Disentanglement. We employ a spherical ResNet, modified based on ResNet [21] and Spherical U-Net [22,23], denoted as E, as the basic encoder (Fig. 2(a)). The output of the encoder includes multi-level features with different resolutions, and the latent vector z t i captures the mixed features of the input cortical surface S t i . Then we employ the attention-based feature disentanglement (AFD) module, consisting of a channel attention module and a spatial attention module in parallel, to disentangle the age-related variance A t i and the identity-related invariance I t i , which is defined as:The operation indicates the element-wise multiplication and φ denotes an attention module in Fig. 2, which is computed as the average of channel attention [24] and spatial attention [25].Identity Conditional Block. To preserve the identity-level regression/progression pattern, we extended the identity conditional block (ICB) [26] to cortical surface data based on spherical convolution. Specifically, the ICB takes the identity-related feature from AFD as input to learn an identity-level regression/progression pattern. Then, a weightsharing strategy is adopted to improve the age smoothness by sharing same spherical convolutional filters across adjacent age groups as shown in Fig. 2(b). The idea of the weight-sharing strategy is that age-related features change gradually over time, and the shared filters can learn common evolving patterns between adjacent age groups. Then, we can select the features of the target age, which we call the identity-level age condition as shown in Fig. 2(b). Note that the percentage of shared features is empirically set to 0.  Cognition Prediction Module. To accurately predict individual-level cognitive scores, we use the identity-age condition features containing subject-specific information and conditional age features to predict the cognitive scores through a spherical convolutional network COG (Fig. 2(c)). The predicted cognitive scores are computed as: yCortical Morphology Prediction Module. To ensure the quality of the predicted cortical morphological maps, the generator G adopts the architecture of the UNet-based least-square GANs [27]. For the discriminator D, we extend a VGG style classification CNN to spherical surfaces. The generator G takes the selected identity-age conditional features ICB I t a i , t p as inputs and gradually upsamples them and concatenates with the corresponding-level features produced by the encoder as shown in Fig. 2, and aims to predict the target cortical surface map S t p i . This process can be summarized as:•] denotes the concatenation along the channel dimension."
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,2.2,Loss Design,"Intensive Triplet Loss. As illustrated in Fig. 1, we suppose identity-related invariance I from a pair of same subjects (S t a i , S t p i ) should be identical, while that from a pair of different subjects (S t a i , S t n j ) should be dissimilar. However, noticing that (S) is also a pair of different subjects, to enhance the relative distance between same subject pair and all the different subject pairs, we apply the intensive triplet loss [20] to help AFD extract identity-related features thoroughly from latent features. A simple linear network denoted as ID is designed to extract feature vector of identity-related feature I and the feature similarity is measured by Pearson's correlation coefficient (Corr): The regressor AgeP has three fully connected layers with 512, 720, and 24 neurons, respectively. It aims to predict the age by computing a softmax expected value [28]:Cognitive Prediction Loss. Given the identity-related age condition derived from ICB, we regress the cognitive scores directly by the cognitive prediction module at a selected age t. The loss function to train the cognitive prediction module is defined as:where y is the ground truth of cognitive scores.Adversarial Loss. To improve the quality of the predicted cortical property maps, we used the adversarial loss to train the generator G and discriminator D with respect to least-square GANs [27]. The aim of the discriminator D is to distinguish the generated cortical property maps as fake data, while the original cortical property maps as real data. The generator G aims to generate cortical property maps as close to the real data as possible. Thus, the objective function is expressed as:where C t denotes the one-hot age encoding at different age t.Reconstruction Loss. To guarantee the high-quality reconstruction and constrain the vertex-wise similarity of the input cortical morphological features and generated cortical morphological features, we adopted the L1 loss to evaluate the reconstruction:Full Objective. The objective functions of our model are written as:where λ IT , λ AE , λ Cog , λ G , λ D , and λ rec control the relative importance of the loss terms."
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,3,Experiments,
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,3.1,Dataset and Experimental Settings,"In our experiments, we used the public BCP dataset [19] with 416 scans from 188 subjects (104 females/84 males) within 24 months of age, where 71 subjects have the single-time-point scans, and 117 subjects have multi-time-point scans. Cortical surface maps were generated using an infant-dedicated pipeline (http://www.ibeat.cloud/) [5].All cortical surface maps were mapped onto the sphere [29] and registered onto the age-specific surface atlases (https://www.nitrc.org/projects/infantsurfatlas/) [6,30] and further resampled to have 40,962 vertices. We chose 70% of subjects with multi-timepoint scans as the train-validation set and conducted a 5-fold cross-validation for tuning the best parameter setting and left 30% for the test set. Of note, subjects with singletime-point scans were unable to evaluate the performance of prediction, hence, they were solely included in the training set to provide additional developmental information.We trained E, ID, and AgeP using an SGD optimizer under the supervision of L AE and L IT with an initial learning rate of 0.1, momentum of 0.9, and a self-adaption strategy for updating the learning rate, which was reduced by a factor of 0.2 once the training loss stopped decreasing for 5 epochs, and the hyper-parameters in the training loss functions were empirically set as follows: λ AE was 0.1 and λ IT was 0.1. We then fixed E and used an Adam optimizer with an initial learning rate of 0.01, β 1 of 0.5, and β 2 of 0.999 to train ICB, COG, G, and D using Eq. ( 8) with a self-adaption strategy for updating the learning rate. The hyper-parameters in the loss functions are empirically set as follows:We used the cortical thickness map at one time point to jointly estimate the four Mullen cognitive scores, i.e., VRS, FMS, RLS, and ELS, and the cortical thickness map at any other time points. To quantitatively evaluate the cognitive prediction performance, the Pearson's correlation coefficient (PCC) between the ground truth and predicted values was calculated. For the cortical morphology prediction, the PCC and mean absolute error (MAE) between the ground truth and predicted values were calculated. In the testing phase, the mean and standard deviation of the 5-fold results were reported."
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,3.2,Evaluation and Comparison,"To comprehensively evaluate the mutual benefits of the proposed modules in cognitive and cortical morphological development prediction, we conducted an ablation study on two modules in Table 1, where w/Cognitive and w/Cortical respectively denotes the variant for training cognitive prediction module and cortical prediction module only, and w/o AFD denotes the variant for training whole framework without AFD module. It can be observed that, the performance on cognitive and cortical prediction tasks has been improved by performing two tasks jointly, which highlights the associations and mutual benefits between cortical morphology and cognitive ability during infant brain development. We also compared our DITSAA with various existing methods given different tasks in our multi-task framework. For the cortical morphology and cognitive development prediction, the state-of-the-art spherical networks SUNet [22], UGSCNN [31], and Spherical Transformer [32] were utilized as baselines. However, these networks are unable to flexibly predict cortical morphology at any time point. Therefore, we apply one-hot encoding to specify the age condition and concatenate it with the cortical morphological features along the channel attention as input to predict cognitive scores and cortical thickness at specific time point. Note that these methods with one-hot age condition still lack the generality and exhibit unsatisfactory performance in predicting both cognition and cortical morphological properties with only a few training samples at each month due to the irregularity of the longitudinal dataset [19]. As illustrated in Table 2 and Fig. 3, we can see our proposed DITSAA achieves better performance in both cortical morphological and cognitive development prediction.  "
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,4,Conclusions,"In this study, we proposed a novel flexible multi-task joint prediction framework for infant cortical morphological and cognitive development, named disentangled intensive triplet spherical adversarial autoencoder. We effectively and sufficiently leveraged all existing longitudinal infant scans with highly irregular and nonuniform age distribution. Moreover, we leverage the mutual benefits between cortical morphological and cognitive development to improve the performance of both tasks. Our framework enables jointly predicting cortical morphological and cognitive development flexibly at arbitrary ages during infancy, both regression and progression. The promising results on the BCP dataset demonstrate the potential power for individual-level development prediction and modeling."
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,,Fig. 1 .,
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,,Fig. 2 .,
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,,Fig. 3 .,
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,,,
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,,,"i , ID I j (1)Age Estimation Loss. To ensure AFD extracts the age-relation information, a simple network is designed as the regressor AgeP to predict age from age-related variance A."
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,,Table 1 .,
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,,850 ± 0.004 * 0.820 ± 0.014 * 0.312 ± 0.032 *,
Multi-task Joint Prediction of Infant Cortical Morphological and Cognitive Development,,Table 2 .,
Self-distillation for Surgical Action Recognition,1,Introduction,"Surgical scene understanding is an important prerequisite for artificial intelligence (AI)-empowered surgery [12], underlying a range of application areas such as context-aware decision support, autonomous robotics, and workflow optimization. One of its key components is the fully-automatic recognition of the surgical action performed at a given point in time -a task not yet solved by state-of-theart-methods [4,20]. To advance the field, the CholecTriplet challenge was organized in the scope of the Medical Image Computing and Computer Assisted Interventions (MICCAI) conferences 2021 and 2022. However, according to the organizers analysis [15,18,20], the task still remains unsolved. The guiding hypothesis of our work was that self-distillation could address some of the challenges in surgical action recognition, namely the high number of classes (100 in the case of CholecTriplet); high class imbalance, and label ambiguity. Self-distillation builds upon the widespread concept of knowledge distillation (KD) [6], in which the knowledge is transferred from one deep model (i.e., a teacher) to another shallow model (i.e., a student). Self-distillation diverges from traditional KD by distilling knowledge within the network itself. While KD is already used in various communities, the purpose of this work was to pioneer the concept of selfdistillation in the context of surgical data science. Based on the CholecTriplet training data set, we developed a method for surgical action recognition (Fig. 2) that leverages self-distillation, Swin Transformers [11], multi-task learning, and ensembling. The following Sect. 2 presents our methodological contribution in detail. Sect. 3 presents ablation studies on the challenge data set that reveal the most important design choices as well as an external validation of our solution on an independent surgical video data set. We conclude with a brief discussion of the most relevant aspects of our work in Sect. 4.  "
Self-distillation for Surgical Action Recognition,2,Methods,
Self-distillation for Surgical Action Recognition,2.1,Task Description and Dataset,"Our study is based on the CholecTriplet Challenge 2022 [18], which was conducted under the umbrella of the Endoscopic Vision Challenges (EndoVis) in conjunction with MICCAI. The Surgical Action Recognition task required participants to submit solutions that recognize surgical action triplets in laparoscopic videos, as illustrated in Fig. 1. The challenge granted access to the CholecT45 [19] dataset which consists of 45 video recordings of laparoscopic cholecystectomy with a total of 90,489 frames. CholecT45 is annotated with 100 action triplet classes, with one instrument, verb, and target forming a triplet. The annotations include six different instrument classes, ten verbs (denoting the action performed), and 15 targets such as organs, tissues, or foreign bodies (clip, specimen bag, etc.). The theoretical maximum of 6 • 10 • 15 classes was reduced to the above-mentioned 100 based on medical relevance and prevalence. An example image from the CholecT45 dataset containing two triplet annotations can be seen in Fig. 1 (a). A chart depicting the highly imbalanced class distribution is shown in Fig. 1 (b)."
Self-distillation for Surgical Action Recognition,2.2,Concept Overview,"As illustrated in Fig. 2, our approach is based on the following key components: (1) Swin Transformer: The recently proposed Swin Transformer [11] architecture was chosen as backbone. (2) Multi-task learning: Based on the success of previous work that leveraged multi-task learning as its training paradigm, we incorporated multiple auxiliary tasks in our architecture, namely the classification of the individual components of the triplet (instrument, verb, and target) as well as the surgical phase. (3) Self-distillation: The core idea of our approach is the usage of soft labels to reduce overconfidence and address label ambiguity. (4) Ensemble: Following common successful training strategies, we implement ensembling to combine the predictions of three trained Swin Transformers of different scales."
Self-distillation for Surgical Action Recognition,2.3,Implementation Details,"Swin Transformer. We base our method on Swin Transformer (SwinT) models of the timm [24] library and adopted the final classification layer to output the 100 triplet predictions, as well as the individual instruments, verbs, targets and surgical phase as auxiliary tasks to leverage the interconnection between them in a multi-task fashion (+Multi)."
Self-distillation for Surgical Action Recognition,,Self-distillation. The concept of self-distillation was achieved by training a,"teacher Swin transformer on one-hot encoded hard labels for 20 epochs, with a batch size of 64, an Adam [10] optimizer, a learning rate of 2 ×10 -4 , a cosine annealing scheduler decreasing to a minimum learning rate of 2 ×10 -6 , and a binary cross-entropy loss function. The model was trained with light augmentations that comprise resizing the images to 224 × 224 pixels, horizontal and vertical flips, rotation, brightness and saturation perturbations with a probability of 0.5. We trained five teacher and five student models; one for each fold of the official five-fold cross validation splitting introduced by the challenge. The teacher was trained on four of the five splits of its fold. After convergence, the soft labels (i.e., the sigmoid probabilites) for the same four splits were computed and the student was trained using these soft labels. The validation was performed on the fifth split, using hard (i.e., the original) labels for both the teacher and the student. During inference, the sigmoid probabilities of the five student models were averaged to yield the final result.The five teacher models shared a common weight intialization seed. The five student models shared a separate weight initialization seed. The student models were trained for 40 epochs with the same augmentations as the teacher models. We saved the weights on the epoch with the best mean Average Precision (mAP) score based on the validation split for the current fold.Ensemble. We combined three trained Swin Transformers (SwinT) of different scales (SwinT base/SwinT large) and configurations for our final ensemble (Ens) model: First, we employed a SwinT base model with multi-task learning of instrument, verb, and target and trained it using self-distillation. Second, we used a SwinT large model using the same approach, and added label smoothing to the soft labels. Third, we included phase annotations as an additional task for the multi-task training of a SwinT base model still employing self-distillation. Please note that every single model mentioned here corresponds to the five aggregated student models of the previous paragraph. All the models were trained using Nvidia GPUs Geforce RTX 3090 and Tesla V100 32GB."
Self-distillation for Surgical Action Recognition,3,Experiments and Results,"The purpose of the experiments was to validate the performance of our method and to quantify the (potential) benefit of each individual component. To this end, we conducted (1) comprehensive ablation studies using the CholecT45 official 5-Fold cross-validation split [17], (2) an analysis of the specific benefit of soft labels, and (3) an external validation based on a Docker container submitted to the CholecTriplet 2022 challenge organizers. The Rendezvous Net [18], provided by the challenge organizers, served as the benchmark. In line with the challenge design [13], we validated the performance using mAP (following the aggregation scheme in [15]) and the top K=5 Accuracy as metrics. All scores were computed using the ivtmetrics library [17].Ablation Studies. We designed the ablation studies as follows: We first calculated the performance of our Swin Transformer backbone as a stand-alone triplet classifier (SwinT). Next, we added multiple auxiliary targets (instruments, verbs, targets, and phases) for multi-task classification (+MultiT). As a third component, we implemented self-distillation by training a student model on soft labels, acquired by training the teacher model (+SelfD). The fourth step was the ensembling of three student model SwinT (+Ens). The results are shown in Tab. 1. A single SwinT as model backbone yields a higher mAP for triplet classification (mAP=32.3%) than the benchmark (mAP=28.8%), which corresponds to a relative improvement by 10.3%. The biggest boost was achieved by including selfdistillation, which improved the Triplet mAP and top-5 accuracy by 3.8% points (pp) and 2.4pp, respectively, compared to our baseline. The final model yielded a mAP of 38.5% and a top-5 accuracy of 86.5%, which corresponds to a boost of 6.2pp in mAP and 2.7pp in top-5 accuracy compared to our own baseline, and a relative improvement by 33.7% for mAP compared to the state-of-the-art method. For transparency, we also provide per-video results, depicted in Fig. 3. With a few exceptions, our final model consistently provides the best results.Analysis of Soft Labels. The addition of self-distillation resulted in the highest boost in performance. This holds true despite the fact that the mAP of the teacher model, trained on hard labels, was about 88% on the training set, which is sub-optimal. The question is thus why the poorer soft labels still yielded a performance improvement. While part of the answer is provided in the literature on soft/noisy labels [9,14,23,26], we also speculated that the soft labels may address the issue of ambiguous/erroneous labels in our particular use case. More specifically, we assumed that if the teacher model increases the probability of semantically close triplets in the soft labels, it could lead to an enhanced level of confidence in the student model's prediction of the ground truth, potentially accounting for the observed performance improvement. To investigate whether this is the case, we first defined a pragmatic proxy metric for semantic similarity: the number of identical triplet items (max: two for different triplets). We then selected all frames with only one unique triplet label and retrieved the top five triplets (excluding the reference) with the highest soft label score. Figure 4, depicts an example of such a comparison. The reference triplet ""bipolar, dissect, cystic plate"" is shown with five soft label triplets ranked by probability. In the example, the top five triplets share an average of 1.6 components with the reference, indicating that they contain similar semantic information. We found that over all samples, the average number of component matches between reference and top five triplets is 1.0±0.002. In contrast, when comparing the reference with five triplets randomly drawn (while respecting prevalence), the agreement is 0.5±0.002."
Self-distillation for Surgical Action Recognition,,Table 1. Main quantitative results,"Starting from our backbone model -a Swin Transformer (Swin T) -we gradually added individual components, namely multi-task learning (MultiT), self-distillation (SelfD), and ensembling (Ens). Each component addition leads to an increase in mAP and top-5 accuracy, in both cross-validation (left) and independent external validation (right). This shows that self-distillation specifically leads to increased scores for semantically related classes. Independent External Validation. External validation was conducted on the CholecTriplet challenge test set. The results, shown in Table 1, confirm the results from cross-validation experiments, with the final ensemble scoring 37.4% in mAP. This equals an absolute improvement of 4.7pp and a relative improvement of 14.4% compared to the Rendezvous benchmark (mAP= 32.7%). With a previous version of the method presented in this paper (scoring slightly lower) we won the challenge in 2022 as the only team that explored the concept of self-distillation."
Self-distillation for Surgical Action Recognition,4,Discussion,"This paper pioneers the concept of self-distillation in the medical image analysis domain. Specifically, we are the first to tackle key challenges in surgical action recognition, namely the high number of classes and class imbalance, with selfdistillation. Comprehensive ablation studies combined with external validation yielded the following findings:1. Swin Transformers, as recently introduced by the computer vision community, can serve as a strong backbone in endoscopic vision tasks. This is suggested by the fact that even our most ablated model, consisting of a single SwinT, surpasses the state-of-the-art surgical action recognition method Rendezvouz. 2. Multi-task learning, here using the classification of instrument, verb, and target as well as of the surgical phase as auxiliary tasks, yielded a notable increase in performance. 3. Self-distillation yielded the biggest boost in performance, suggesting that soft labels are better suited for surgical action recognition. 4. Ensembling increased performance further, as also suggested by various publications in a wide range of fields. Overall, the addition of self-distillation (in combination with the SwinT as a backbone) resulted in the highest performance boost. While label noise has been shown to be beneficial in various work [9,14,23,26], the concept of selfdistillation may not necessarily be intuitive; although the mAP achieved by the teacher model, trained on hard labels, is sub-optimal (32%), the teacher's noisy labels lead to an overall improvement in performance when compared to the (presumably better-quality) hard labels. In the general machine learning literature, the knowledge encoded in noisy labels is referred to as ""dark knowledge"" because it is not yet well-understood. Aiming to shed light on this topic, our experiments on semantic similarity suggest that soft labels may actually address the issue of ambiguous/erroneous labels. Further analyses with more sophisticated metrics for semantic similarity are, however, needed to support this finding.Related work has so far tackled the challenge of surgical action recognition with various strategies including multi-task learning [16,21], and different attention mechanisms [3,19] incorporated into diverse architectures based on temporal convolutional networks [2,21], transformers [3,5,19], or combinations of convolutional neural networks (CNN) with recurrent neural networks (RNN) [7,8,16,25] or hidden Markov models (HMM) [22]. While our approach was particularly successful according to the challenge analysis, the overall performance is still not optimal. Advancing the methods will require more data that features a sufficient number of samples for each triplet and captures the full variability of scenes that might be encountered in practice. From a methodological perspective, future work should be directed to efficiently taking temporal context into account and addressing potential domain shifts [1].In conclusion, our study is the first to demonstrate the benefit of selfdistillation for surgical vision tasks. Based on the substantial performance boost obtained, the usage of soft labels could become a valuable tool in the endoscopic vision community."
Self-distillation for Surgical Action Recognition,,Fig. 1 .,
Self-distillation for Surgical Action Recognition,,Fig. 2 .,
Self-distillation for Surgical Action Recognition,,Fig. 3 .,
Self-distillation for Surgical Action Recognition,,Fig. 4 .,
Self-distillation for Surgical Action Recognition,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 61.
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,1,Introduction,"Orbital fractures represent a frequent occurrence of orbital trauma, with their incidence on the rise primarily attributed to assault, falls, and vehicle collisions [1]. Orbital blow-out fracture (OBF) is a frequent type of fracture where one of the orbital walls fractures due to external force, while the orbital margin remains intact [2]. It is a complex disease that can result in the destruction or collapse of the orbital wall, orbital herniation, invagination of the eyeball, and even visual dysfunction or changes in appearance in severe cases [3]. OBF repair surgery is the ultimate treatment for this disease and involves implanting artificial implants to repair and fill the fractured area. Automatic reconstruction of the orbital wall is a crucial step in this procedure to achieve precise preformed implants and assisted intraoperative navigation.Orbital wall reconstruction is challenging due to the complex and diverse OBF types, as shown in Fig. 1, including (a) medial wall fracture, (b) floor wall fracture, (c) fractures in both the medial and floor walls, (d) roof wall fracture, (e) and other types. The orbital walls in these cases are often collapsed, damaged, fractured, deviated, or exhibit a large number of defects in severe cases, which makes reconstruction more difficult. Furthermore, the orbital medial and floor walls are thin bones with low CT gradient values and blurred boundaries, which further increases the complexity of reconstruction. Currently, commercial software is typically used for semi-automatic reconstruction in clinical practice. However, this method is inefficient and inaccurate, requiring tedious manual adjustment and correction. As a result, doctors urgently need fast and accurate automated surgical reconstruction methods.Several automatic segmentation methods for the orbital wall have been explored in previous studies [4], such as Kim et al.'s [5] orbital wall modeling based on paranasal sinus segmentation and Lee et al.'s [6] segmentation algorithm of the orbital cortical bone and thin wall with a double U-Net network structure. However, they did not explore segmentation for OBF. Taghizadeh et al. [7] proposed an orbital wall segmentation method based on a statistical shape model and local template matching, but individualized differences and factors such as orbital wall deviation and collapse greatly affect its performance in fractured orbits. Deep learning-based algorithms for skull defect reconstruction have also been proposed, such as Li et al.'s [8] automatic repair network for skull defects, Xiao et al.'s [9] network model that estimates bone shape using normal facial photos and CT of craniomaxillofacial trauma, and Han et al.'s [10] craniomaxillofacial defect reconstruction algorithm with a statistical shape model and individual features. However, these methods require the removal of the lesion area followed by reconstruction of the defect, which is different from OBF repair that directly performs orbital wall reconstruction without removal of the destroyed bone. The above methods may be less effective in cases of OBF where factors such as deviation, collapse, and fracture greatly impact the reconstruction network, reducing the reconstruction effect. As of now, there are no automated reconstruction methods reported for OBF surgery. To address the above challenges, this paper proposes a symmetric prior anatomical knowledge-guided adversarial generative network (GAN) for reconstructing orbital walls in OBF surgery. Firstly, the paper proposes an automatic generation method of symmetric prior anatomical knowledge (SPAK) based on spatial transformation, which considers that the symmetrical normal orbital anatomy can guide the reconstruction of the fractured orbital wall. Secondly, the obtained SPAK is used as a prior anatomical guidance for GAN to achieve more accurate automatic reconstruction of the orbital wall. To further improve the network's reconstruction performance, the paper adopts a supervision strategy of multi-loss function combination. Finally, experimental results demonstrate that the proposed network outperforms some state-of-the-art networks in fracture orbital wall reconstruction.The main contributions of this paper are summarized as follows: (1) A GAN model guided by SPAK is developed for automatic reconstruction of the orbital wall in OBF surgery, which outperforms the existing methods. (2) The proposed method is the first AI-based automatic reconstruction method of the orbital wall in OBF surgery, which can enhance the repair effectiveness and shorten the surgical planning time."
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,2,Methods,"The structure of the proposed SPAK-guided GAN is illustrated in Fig. 2, and it mainly comprises three components: automatic generation of SPAK, GAN network structure, and multiple loss function supervision. Each component is elaborated below."
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,2.1,Automatic Generation of SPAK Based on Spatial Transformation,"The reconstruction of the orbital wall in OBF is a complex task, made more difficult when severe displacement or defects are present. However, since the left and right orbits are theoretically symmetrical structures, utilizing the normal orbital wall on the symmetrical side as prior anatomical guidance in the reconstruction network can aid in accuracy. Prior knowledge has been demonstrated to be effective in medical image computing [11][12][13]. Nonetheless, left and right orbits are not perfectly symmetrical, and the mirror plane can be challenging to locate, leading to substantial errors and unwieldy operations. To address these issues, we propose an automatic generation method for SPAK based on spatial transformations, as depicted in Fig. 2. The main steps of this method include: in 3D dimensions and obtaining their respective three-dimensional models; (7) using the ICP algorithm to register the normal and fractured orbital walls and acquire their deformation field; (8) based on the previous step, transforming the symmetrical normal orbital wall to the side of the fractured orbital wall using the deformation field; (9) separating the transformed normal orbital wall to obtain a single orbital wall that can serve as a SPAK to guide GAN."
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,2.2,SPAK-Guided GAN for Orbital Wall Reconstruction,"Reconstructing the fractured orbital wall requires generative prediction of the damaged area, which is why we adopted the GAN. Our proposed SPAK-guided GAN consists of a generative network (GN) and a discriminative network (DN), as illustrated in Fig. 2. The GN uses a network structure based on 3D V-Net and consists of five encoded layers and four decoded layers to achieve automatic reconstruction of the fractured orbital wall. The input of the GN is the merged block of SPAK and the original image, which guides the GAN network to make more accurate predictions of the orbital wall. To expand the receptive field, each convolution layer uses two convolution stacks and includes residual connections to reduce gradient dissipation. The activation function is ReLU, and group normalization [14] is added after each convolution to prevent network overfitting. To avoid the loss of shallow features as the number of network layers increases, we added skip connections between the corresponding convolutional layers of the encoded and decoded sections. The DN identifies the authenticity of the reconstructed orbital wall and the ground truth to produce a more realistic orbital wall. It includes five encoded feature layers and one fully connected layer. Each encoded feature layer comprises stacked convolutions and a max-pooling layer with a filter kernel of 2 × 2 × 2 and a step size of 2 to compress the feature map. The input of the DN is either the merged image block of the original image and the reconstructed orbital wall area or the original image and the ground truth. The DN distinguishes between the authenticity of these inputs. After restoring the output result of the network to the original position and resampling, we perform 3D reconstruction to obtain the reconstructed orbital wall model."
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,2.3,Multiple Loss Function Supervision for GAN,"The GAN network's loss function comprises two components: the loss function of the discriminative network, denoted as Loss D , and the loss function of the generative network, denoted as Loss G . To enhance the reconstruction performance, we adopt a supervision strategy that combines multiple loss functions. To ensure that the GAN network can identify the authenticity of samples, we incorporate the commonly used discriminative loss function in GAN, as presented in Eq. (1).where D represents the network discriminator, G represents the network generator, G(x) represents reconstruction result, x represents input image, y represents ground truth.To improve the accuracy of the reconstructed orbital wall, a combination of multiple loss functions is utilized in Loss G . First, to better evaluate the area loss, the dice coefficient loss function Loss dice is adopted, which can calculate the loss between the reconstructed orbital wall region and the ground truth using Eq. ( 2). Secondly, due to the potential occurrence of boundary fractures and orbital wall holes during GAN reconstruction, the cross-entropy loss function Loss ce is added to Loss G . Its equation is shown in (3) to evaluate the reconstruction of the boundary and holes.where N represents the total number of voxels, G(x i ) represents the voxels of reconstruction results, y i represents the voxels of ground truth. Furthermore, an adversarial loss function L adv , is also incorporated into Loss G . This function evaluates the loss of the generator's output by the discriminator, thus enabling the network to improve its performance in reconstructing the fractured orbital wall. The equation for L adv is shown in (4).Finally, we combine the loss function Loss dice , Loss ce and L adv to obtain the final Loss G , whose equation is shown in (5) and λ is the weight parameter.3 Experiments and Results"
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,3.1,Data Set and Settings,"The dataset for this study was obtained from Shanghai Ninth People's Hospital Affiliated to Shanghai Jiao Tong University School of Medicine. It included 150 cases of OBF CT data: 100 for training and 50 for testing. Each case had a blowout orbital fracture on one side and a normal orbit on the other. For normal orbit segmentation training, 70 additional cases of normal orbit CT data were used. The images were 512 × 512 in size, with resolutions ranging from 0.299 mm × 0.299 mm to 0.717 mm × 0.717 mm. The number of slices varied from 91 to 419, with thicknesses ranging from 0.330 mm to 1.0 mm. The ground truth was obtained through semi-automatic segmentation and manual repair by experienced clinicians. To focus on the orbital area, CT scans were resampled to 160 × 160 with a multiple of 32 slices after cutting out both orbits. This resulted in 240 single-orbital CT data for normal orbital segmentation training and 100 for OBF reconstruction training. The proposed networks used patches of size 32 × 160 × 160, with training data augmented using sagittal symmetry.The proposed and comparison networks all adopted the same input patch size of 32 × 160 × 160, a batch size of 1, a learning rate of 0.001, and were trained for 30,000 iterations using TensorFlow 1.14 on an NVIDIA RTX 8000 GPU. Evaluation of the reconstruction results was based on the dice similarity coefficient (DSC), intersection over union (IOU), precision, sensitivity, average surface distance (ASD), and 95% Hausdorff distance (95HD). "
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,3.2,Ablation Experiment Results,"The proposed network is based on the GN, and an ablation experiment was conducted to verify the effectiveness of the adopted strategy. The experimental results are presented in Table 1. Comparing SPAK with other networks, it is evident that the accuracy of the reconstructed orbital wall is relatively poor, and it cannot be directly employed as a reconstruction network. However, it can be used as a prior guide for GAN. By comparing GN with GN+DN (GAN), it is apparent that the accuracy of GAN, except for precision, is better than GN. This finding shows that DN can promote GN to better reconstruct the orbital wall, indicating the correctness of adopting GAN as a reconstruction network for OBF. Furthermore, the addition of SPAK to GN and GAN significantly improved their reconstruction accuracy. Notably, compared with GAN, the proposed reconstruction network improved the DSC of orbital wall reconstruction accuracy by more than 3.5%, increased IOU by more than 5%, and decreased 95HD by 0.35 mm. These results indicate that SPAK guidance plays a crucial role in orbital wall reconstruction. It further demonstrates that the improved strategy can effectively achieve the precise reconstruction of the orbital wall in OBF."
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,3.3,Comparative Experiment Results,"To demonstrate the superior performance of the proposed reconstruction algorithm, we compared it with several state-of-the-art networks used in medical image processing, including U-Net [15], V-Net [16], Attention U-Net [17], and Attention V-Net. The accuracy of the reconstructed results from these networks is compared in Table 2. The comparison with U-Net, V-Net, and Attention U-Net reveals that the proposed reconstruction network outperforms them significantly in terms of reconstruction accuracy evaluation. Specifically, the DSC is improved  by more than 2.5%, the IOU is improved by more than 4.5%, the sensitivity is improved by more than 6.5%, and the 95HD distance error is reduced by more than 0.35 mm. These results indicate that the proposed network has better reconstruction performance for the orbital wall. Comparing with Attention V-Net, it is shown that the proposed reconstruction network has better reconstruction accuracy, except for precision. Although Attention V-Net has higher precision, its standard deviation is relatively large. Figure 3 is a comparison chart of the reconstruction results of each method, showing that other methods are difficult to accurately predict the orbital wall boundary, while the proposed method can generate the orbital wall more accurately. Figure 4 presents a comparison chart of surface distance errors from the reconstruction results, which shows that other methods have relatively large distance errors in the medial and floor walls of the orbit, and even lead to cracks and holes. In contrast, the proposed network significantly addresses these issues, indicating its superior performance."
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,4,Conclusion,"In summary, this paper proposes a SPAK-guided GAN for accurately automating OBF wall reconstruction. Firstly, we propose an automatic generation method of SPAK based on spatial transformation, which maps the segmented symmetrical normal orbital wall to a fractured orbit to form an effective SPAK. On this basis, a SPAK-guided GAN network is developed for the automatic reconstruction of the fractured orbital wall through adversarial learning. Furthermore, we use the strategy of multi-loss function supervision to improve the accuracy of network reconstruction. The final experimental results demonstrate that the proposed reconstruction network achieves accurate automatic reconstruction of the fractured orbital wall, with a DSC of 92.35 ± 2.13% and a 95% Hausdorff distance of 0.59 ± 0.23 mm, which is significantly better than other networks. This network achieves the automatic reconstruction of the orbital wall in OBF, which effectively improves the accuracy and efficiency of OBF surgical planning. In the future, it will have excellent application prospects in the repair surgery of OBF."
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,,Fig. 1 .,
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,,( 1 )Fig. 2 .,
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,,Fig. 3 .Fig. 4 .,
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,,Table 1 .,
Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network,,Table 2 .,NetworksDSC (%) ↑ IOU (%) ↑ Precision (%) ↑ Sensitivity (%) ↑ ASD (mm) ↓ 95HD (mm) ↓ U-
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,1,Introduction,"Vertebra localization and identification from CT scans is an essential step in medical applications, such as pathology diagnosis, surgical planning, and outcome assessment [3,6]. This is however a tedious manual task in a time-sensitive setting that can benefit a lot from automation. However, automatically identifying and determining the location and orientation of each vertebra from a CT Fig. 1. Summary of the proposed method. A CNN generates a heatmap from which local maxima are connected using k-nearest neighbours to form a graph. Then a single GNN associates the keypoints, performs classification and filters out false postives. The full pipeline is trained and does not require hand-tuned post-processing.scan can be very challenging: (i) scans vary greatly in intensity and constrast, (ii) metal implants and other materials can affect the scan quality, (iii) vertebrae might be deformed, crushed or merged together due to medical conditions, (iv) vertebrae might be missing due to accidents or previous surgical operations.Recently, public challenges like the VerSe challenge [24] have offered a common benchmarking platform to evaluate algorithms to automate this task, resulting in a boost in research on this topic. However, these challenges focus on finding the position of vertebrae, ignoring the orientation or direction. Additionally, practically all methods employ manual heuristic methods to identify landmarks and filter out false positives.In this paper, we introduce a trainable method that performs vertebrae localization, orientation estimation and classification with a single architecture. We replace all hand-crafted rules and post-processing steps with a single trainable Graph Neural Network (GNN) that learns to filter out, associate and classify landmarks. We apply a generalized Message Passing layer that can perform edge and node classification simultaneously. This alleviates the need for sensitive hand-tuned parameter tuning, and increases robustness of the overall pipeline.The main contributions of our work are: (1) introducing a pipeline that uses a single Graph Neural Network to perform simultaneous vertebra identification, landmark association, and false positive pruning, without the need for any heuristic methods and (2) building and releasing a new spine detection dataset that adds pedicles of vertebrae to create a more complex task that includes orientation estimation of vertebrae, which is relevant for clinical applications."
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,2,Related Work,"Spine Landmark Prediction and Classification. The introduction of standardised spine localization and classification challenges [10,24] resulted in a boost in research on this problem. Convolutional Neural Networks (CNN) became a dominant step in most approaches shortly after their introduction [7]. Most modern methods process the results of a CNN using a heuristic method to create a 1-dimensional sequence of vertebra detections, before applying classification: [27] generate heatmaps for body positions, and refine it into a single sequence graph that uses message passing for classification. [14] generate heatmaps, extract a 1-dimensional sequence and use a recurrent neural network for classification. [18] produce a heatmap using a U-Net [21], but use a simpler approach by taking the local maxima as landmarks, and forming a sequence by accepting the closest vertebra that is within a defined distance range. [16] uses a directional graph and Dynamic Programming to find an optimal classification. Graph Neural Networks. In recent years, Graph Neural Networks (GNN) have surged in popularity [11], with a wide and growing range of applications [26]. A prominent task in the literature is node-level representation learning and classification. A less prominent task is edge classification, for which early work used a dual representation to turn edge-into node representations [1]. Other approaches model edge embeddings explicitly, such as [4,12]. The most general formulation of GNNs is the message-passing formulation [5] which can be adapted to perform both edge and node classification at the same time. We use this formulation in our method.Various methods have applied GNNs to keypoint detection, however they all apply to 2-dimensional input data. In [20] GNNs are used to detect cars in images. An edge classification task is used to predict occluded parts of the car. However, the GNN step is ran individually for every car detection and the relation between cars is not taken into account, unlike our task. Also there is no node classification applied. In [15] a GNN is used to group detected keypoints for human-pose estimation on images. Keypoints are grouped using edge prediction where edge-embeddings are used as input the GNN. A separate GNN processes node embeddings to facilitate the final grouping. However, the node and edge embeddings are processed separately from each other."
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,3,Method,"In this paper we tackle vertebra localization and classification, but unlike other methods that only focus on detecting the body of the vertebrae, we also detect the pedicles and associate them with their corresponding body. This allows us to also define the orientation of the vertebra defined by the plane passing through the body and pedicles1 . To this end we introduce a new dataset that includes pedicles for each vertebra, described in Sect. 4.1, creating a challenging keypoint detection and association task.Our method consists of a two-stage pipeline shown in Fig. 1: we detect keypoints from image data using a CNN stage, and form a connected graph from these keypoints that is processed by a GNN stage to perform simultaneous node and edge classification, tackling classification, body to pedicle association as well as false positive detection with a single trainable pipeline without heuristics.CNN Stage. The CNN stage detects candidate body, left pedicle and right pedicle keypoints and provides segment classifications for the body keypoints as either cervical, thoracic, lumbar or sacral. We use a UNet 2 CNN [19] and select all local maxima with an intensity above a certain threshold τ , in this paper we use τ = 0.5. These keypoints are connected to their k nearest neighbours, forming a graph. In rare cases this can result in unconnected cliques, in which case the nearest keypoint of each clique is connected to k 3 nearest points in the other cliques, ensuring a fully connected graph. All nodes and edges are associated with information through embeddings, described below."
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,,GNN Stage.,"The second stage employs a generalized message-passing GNN following [5] to perform several prediction tasks on this graph simultaneously:1. keypoint association prediction: we model association between body keypoints and their corresponding pedicle keypoints as binary edge classification on the over-connected k-NN graph. 2. body keypoint level prediction: for body keypoints, we model the spine level prediction as multi-class node classification. 3. keypoint legitimacy prediction: to filter out false-positive keypoints, we additionally compute an binary legitimacy prediction for each node.To perform these task, our message-passing GNN maintains edge and node embeddings which are updated in each layer. A message-passing layer performs a node update and edge update operation. Denoting the feature vector of a node v by x v , and the feature vector of a directed edge (u, v) by x uv , the node and edge features are updated as follows:Here denotes a symmetric pooling operation (in our case max pooling) over the neighborhood N u . ψ node and edge are trainable parametric functions: in our case, two distinct two-layer MLPs with ReLU nonlinearities. After N such message-passing layers we obtain an embedding vector for each node and edge. Each node/edge embedding is passed through a linear layer (distinct for nodes and edges) to obtain a vector of node class logits or a single edge prediction logit, respectively. The last entry in the node prediction vector is interpreted as a node legitimacy prediction score: nodes predicted as illegitimate are discarded for the output.The node input features x u ∈ R 7 consist of the one-hot encoded keypoint type (body, left or right pedicle) and the segment input information (a pseudoprobability in [0, 1] for each of the four spine segments of belonging to that segment, computed by applying a sigmoid to the heatmap network's output channels which represent the different spine segments). The edge input features x uv ∈ R 4 consist of the normalized direction vector of the edge and the distance between the two endpoints.The output of the GNN contains finer spine-level classification (i.e. C1-C7, T1-T13, L1-L6, S1-S2), keypoint-level legitimacy (legitimate vs. false-positive detection) and body-pedicle association via edge prediction, implicitly defining the orientation of each vertebra. Prediction scores of corresponding directed edges (u, v) and (v, u) are symmetrized by taking the mean.In our experiments we consider variations to our architecture: weight sharing between consecutive GNN layers, multiple heads with a shared backbone (jointly trained) and dedicated networks (separately trained) for edge/node prediction. 4 Experiments"
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,4.1,Dataset,"Our main dataset consists of 2118 scans, of which 1949 form the training dataset, and 169 the validation dataset. This includes 360 scans from the VerSe datasets [24], 1676 scans from the CT Colonography dataset [25] and 82 scans from the CT Pancreas dataset [22]. Of these datasets, only VerSe has labels for spine levels, and none of the datasets have labels for the pedicles of vertebrae. The labeling was done in a two-step process with initial pedicle locations determined through post-processing of ground truth segmentations, then transfered to other datasets via bootstrapping. For each vertebra, the vertebra level is labeled, including the position of the body and the right and left pedicles. This keypoint dataset is publicly available at https://github.com/ImFusionGmbH/VIDvertebra-identification-dataset. Additionally, we also perform the VerSe body classification task on the original VerSe dataset [24] which contains 160 scans in an 80/40/40 split."
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,4.2,Training,"Heatmap Network. The heatmap is generated by a UNet 2 network [19], with 4 layers and 32 channels. The network is trained on crops of 128 3 voxels with 1.5 mm spacing. The target data consists of Gaussian blobs (σ = 6 mm) at the landmark positions. We sample 70% of the crops around landmark locations, and 30% randomly from the volume. Additionally we apply 50% mirror augmentations, and 20 • rotations around all axes. We use the MADGRAD optimizer [8] with a learning rate of 10 -3 . We use a binary cross-entropy loss, with an 80% weighting towards positive outputs to counter the data's balancing.Graph Neural Network. The GNN is implemented in PyTorch Geometric 2.0.4 [9]. The three predictions of the graph neural network -edge classification, node classification and node legitimacy prediction -are trained via cross-entropy losses which are weighted to obtain the overall loss:We only make edge predictions on edges that run between a body and a pedicle keypoint -the other edges are only used as propagation edges for the GNN. Similarly, we only make spine level predictions on body keypoints. Solely these subsets of nodes/edges go into the respective losses.As input data, we use the predicted keypoints of the heatmap network on the training/validation set. The ground-truth keypoints are associated to this graph to create the target of the GNN. We include three synthetic model spines during training (keypoints in a line spaced 30 mm apart) to show the network typical configurations and all potential levels (with all levels/without T13, L6, S2/without T12, T13, L6, S2).We tune various hyperparameters of our method, such as network depth and weight sharing, the k of the k-NN graph, the loss weighting parameters α, β, γ and the number of hidden channels in the message-passing MLP. We use a short notation for our architectures, such as (5 × 1, 4, 1) for 5 independent messagepassing layers followed by 4 message-passing layers with shared weights and another independent message-passing layer.Various data augmentations are used to make our network more robust to overfitting: (i) rotation of the spine by small random angles, (ii) mirroring along the saggital axis (relabeling left/right pedicles to keep consistency), (iii) perturbation of keypoints by small random distances, (iv) keypoint duplication and displacement by a small distance (to emulate false-positive duplicate detections of the same keypoint), (v) keypoint duplication and displacement by a large distance (to emulate false-positive detections in unrelated parts of the scan) and (vi) random falsification of spine segment input features. We define four levels of augmentation strength (no/light/default/heavy augmentations) and refer to the supplementary material for precise definitions of these levels."
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,4.3,Evaluation,"We evaluate our method on two tasks. The first one is the full task consisting of node and edge classification for vertebra level and keypoint association detection. We evaluate this on the 2118 scan dataset which comes with pedicle annotations. The second task consists only of vertebra localization, which we evaluate on the VerSe 2019 dataset [24]. Unless otherwise specified, we use k = 14, the (13 × 1) architecture, batch size 25 and reaugment every 25 epochs for the full task, and k = 4, the (9 × 1) architecture, batch size 1 and reaugment every epoch for the VerSe task. In both cases we use the default augmentation level and α = β = 1. As evaluation metrics we use the VerSe metrics identification rate (ratio of ground-truth body keypoints for which the closest predicted point is correctly classified and within 20mm) and d mean (mean distance of correctly identified body keypoints to their 1-NN predictions) [24]. Furthermore we evaluate the edge and illegitimacy binary predictions by their F 1 scores. Since the identification rate is largely unaffected by false-positive predicted keypoints, we disable legitimacy predictions unless for specific legitimacy prediction experiments to help comparability. We compare our methods to two baselines: For node prediction, we use a Hidden Markov Model (HMM) [2] that is fitted to the training data using the Baum-Welch algorithm using the pomegranate library [23]. The HMM gets the predicted segment labels in sequence. Dealing with false-positive detection outliers is very difficult for this baseline, therefore we filter out non-legitimate detections for the HMM inputs to get a fairer comparison, making the task slightly easier for the HMM. For the VerSe challenge, we also compare our results to the top papers reported in the VerSe challenge [24]. For edge prediction, we compare our method to Hungarian matching on the keypoints from the CNN pipeline."
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,4.4,Results,"The results on our introduced dataset are shown in Table 1. Our method outperforms the baseline methods, both in identification rate and edge accuracy. We also show results on a 'hard' subset, selected as the data samples where either of the methods disagree with the ground truth. This subset contains 47 scans and represents harder cases where the association is not trivial. We give p-values of the Wilcoxon signed-ranked test against the respective baseline for numbers that are better than the baseline. Figure 2 shows a qualitative example where the baseline method fails due to false-positive and false-negative misdetections in the input (these errors in the input were generated by augmentations and are more extreme than usual, to demonstrate several typical baseline failures in one example). The GNN correctly learns to find the correct association and is not derailed by the misdetections. The examples where our architecture fails typically have off-by-one errors in the output from the CNN: for example, the last thoracic vertebra is detected as a lumbar vertebra (usually in edge cases where the two types are hard to distinguish). Hence the GNN classifications of the lumbar segment, and possibly the thoracic segment, will be off by one (see Figure S2 in the supplementary).Table 2 shows the performance differences of various GNN architectures. The single-head architecture with 13 individual layers performs the best on identification rate, although the 13-layer architecture with 11 shared layers performs very similarly. The architectures with fewer layers perform slightly better in edge accuracy since this task is less context dependent. Multi-head architectures perform slightly worse on identification, but retain a good edge accuracy. Training a model solely directed to edge classification does yield the highest edge accuracy, as expected. Enabling legitimacy predictions slightly degrades the performance, likely due to two facts: for one, an extra loss term is added, which makes the model harder to train. Also, the identification rate metric is not majorly affected by having additional false-positive detections, hence there is little to be gained in terms of this metric by filtering out false positives. An optimal legitimacy loss weighting seems to be λ = 1.0. Finally, augmentations help generalization of the model, but the amount of augmentations seems to have little effect.Table 3 shows the result on the traditional VerSe body-identification task. Our method yields a competitive performance despite not being optimized on this task (identification rate slightly lower than the leaders but a better average distance to the landmarks)."
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,5,Conclusion,"We introduced a simple pipeline consisting of a CNN followed by a single GNN to perform complex vertebra localization, identification and keypoint association. We introduced a new more complex vertebra detection dataset that includes associated pedicles defining the full orientation of each vertebra, to test our method. We show that our method can learn to associate and classify correctly with a single GNN that performs simultaneous edge and node classification.The method is fully trainable and avoids most heuristics of other methods. We also show competitive performance on the VerSe body-identification dataset, a dataset the method was not optimized for. We believe this method is general enough to be usable for many other detection and association tasks, which we will explore in the future."
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,,Fig. 2 .,
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,,Table 1 .,
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,,Table 2 .,
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,,Table 3 .,
Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_46.
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,1,Introduction,"Pelvic fracture is a severe type of high-energy injury, with a fatality rate greater than 50%, ranking the first among all complex fractures [8,16]. Surgical planning and reduction tasks are challenged by the complex pelvic structure, as well as the surrounding muscle groups, ligaments, neurovascular and other tissues. Robotic fracture reduction surgery has been studied and put into clinical use in recent years, and has successfully increased reduction precision and reduced radiation exposure [2]. Accurate segmentation of pelvic fracture is required in both manual and automatic reduction planning, which aim to find the optimal target location to restore the healthy morphology of pelvic bones. Segmenting pelvic fragments from CT is challenging due to the uncertain shape and irregular position of the bone fragments and the complex collision fracture surface. Therefore, surgeons typically annotate the anatomy of pelvic fractures in a semi-automatic way. First, by tuning thresholds and selecting seed points, adaptive thresholding and region-growing methods are used to extract bone regions [1,11,15]. Then, the fracture surfaces are manually delineated by outlining the fragments in 3D view or even modifying the masks in a slice-by-slice fashion. Usually, this tedious process can take more than 30 min, especially when the fractured fragments are collided or not completely separated.Several studies have been proposed to provide more efficient tools for operators. A semi-automatic graph-cut method based on continuous max-flow has been proposed for pelvic fracture segmentation, but it still requires the manual selection of seed points and trail-and-error [4,22]. Fully automatic max-flow segmentation based on graph cut and boundary enhancement filter is useful when fragments are separated, but it often fails on fragments that are collided or compressed [9,19]. Learning-based bone segmentation has been successfully applied to various anatomy, including the pelvis, rib, skull, etc. [12,13]. Some deep learning methods have been proposed to detect fractures [17,18,21], but the output from these methods cannot provide a fully automated solution for subsequent operations. In FracNet, rib fracture detection was formulated as a segmentation problem, but with a resultant Dice of 71.5%, it merely outlined the fracture site coarsely without delineating the fracture surface [7]. Learning-based methods that directly deal with fracture segmentation have rarely been studied.Fracture segmentation is still a challenging task for the learning-based method because (1) compared to the more common organ/tumor segmentation tasks where the model can implicitly learn the shape prior of an object, it is difficult to learn the shape information of a bone fragment due to the large variations in fracture types and shapes [10]; (2) the fracture surface itself can take various forms including large space (fragments isolated and moved), small gap (fragments isolated but not moved), crease (fragments not completely isolated), compression (fragments collided), and their combinations, resulting in quite different image intensity profiles around the fracture site; and (3) the variable number of bone fragments in pelvic fracture makes it difficult to prescribe a consistent labeling strategy that applies to every type and case.This paper proposes a deep learning-based method to segment pelvic fracture fragments from preoperative CT images automatically. Our major contribution includes three aspects. (1) We proposed a complete automatic pipeline for pelvic fractures segmentation, which is the first learning-based pelvic fracture segmentation method to the best of our knowledge. (2) We designed a novel multi-scale distance-weighted loss and integrated it into the deeply supervised training of the fracture segmentation network to boost accuracy near the fracture site. (3) We established a comprehensive pelvic fracture CT dataset and provided ground-truth annotations. Our dataset and source code are publicly available at https://github.com/YzzLiu/FracSegNet. We expect them to facilitate further pelvis-related research, including but not limited to fracture identification, segmentation, and subsequent reduction planning."
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,2,Methods,
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,2.1,The Overall Segmentation Framework,"Our study aims to automatically segment the major and minor fragments of target bones (left and right ilia and sacrum) from CT scans. As illustrated in Fig. 1, our method consists of three steps. In the first step, an anatomical segmentation network is used to extract the pelvic bones from the CT scan. With a cascaded 3D nn-Unet architecture, the network is pre-trained on a set of healthy pelvic CT images [5,13] and further refined on our fractured dataset. In the second step, a fracture segmentation network is used to separate the bone fragments from each iliac and sacral region extracted from the first step. To define a consistent labeling rule that is applicable to all fracture types, we prescribe three labels for each bone, namely the background, the main fragment, and other fragments. The main fragment is the largest fragment at the center. In the third step, isolated components are further separated and labeled, and small isolated bone fragments are removed to form the final output."
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,2.2,Fracture Segmentation Network,"The contact fracture surface (CFS) is the part where the bones collide and overlap due to compression and impact, and is the most challenging part for human operators to draw. We are particularly concerned about the segmentation performance in this region. Therefore, we introduce guidance into the network training using fracture distance map (FDM). A 3D UNet is selected as the base model [6]. The model learns a non-linear mapping relationship M : X → Y , where X and Y are the input and ground truth of a training sample, respectively.Fracture Distance Map. The FDM is computed on the ground-truth segmentation of each data sample before training. This representation provides information about the boundary, shape, and position of the object to be segmented. First, CFS regions are identified by comparing the labels within each voxel's neighbourhood. Then, we calculate the distance of each foreground voxel to the nearest CFS as its distance value D v , and divide it by the maximum.is the indicator function for foreground, and D is the normalized distance. The distance is then used to calculate the FDM weight Ŵ using the following formula:To ensure the equivalence of the loss among different samples, the weights are normalized so that the average is always 1.FDM-Weighted Loss. The FDM weight Ŵ is then used to calculate the weighted Dice and cross-entropy losses to emphasize the performance near CFS by assigning larger weights to those pixels.where L is the number of labels, P l v , Y l v are the network output prediction and the one-hot encoding form of the ground truth for the l th label of the v th voxel. The overall loss is their weighted sum:where λ ce is a balancing weight.Multi-scale Deep Supervision. We use a multi-scale deep supervision strategy in model training to learn different features more effectively [20]. The deep layers mainly capture the global features with shape/structural information, whereas the shallow layers focus more on local features that help delineate fracture surfaces. We add auxiliary losses to the decoder at different resolution levels (except the lowest resolution level). The losses are calculated using the corresponding down-sampled FDM Ŵ n v , and down-sampled ground truth Y n v . We calculate the output of the n th level L n by changing λ F DM in Eq. ( 3). The λ F DM of each layer decreases by a factor of 2 as the depth increases, i.e., λ n+1 = λ n /2. In this way, the local CFS information are assigned more attention in the shallow layers, while the weights become more uniform in the deep layers.Smooth Transition. To stabilize network training, we use a smooth transition strategy to maintain the model's attention on global features at the early stage of training and gradually shift the attention towards the fracture site as the model evolves [14]. The smooth transition dynamically adjusts the proportion of the FDM in the overall weight matrix based on the number of training iterations. The dynamic weight is calculated using the following formula:where δ = -ln(1 -t τ ), J is an all-ones matrix with the same size as the input volume, t is the current iteration number, and τ is a hyper-parameter. The dynamic weight W st is adjusted by controlling the relative proportion of J and Ŵ . The transition terminates when the epoch reaches τ ."
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,2.3,Post-processing,"Connected component analysis (CCA) has been widely used in segmentation [3], but is usually unsuitable for fracture segmentation because of the collision between fragments. However, after identifying and removing the main central fragment from the previous step, other fragments are naturally separated. Therefore, in the post-processing step, we further isolate the remaining other fragments by CCA. The isolated components are then assigned different labels. In addition, we remove fragments smaller than a certain threshold, which has no significant impact on planning and robotic surgery."
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,2.4,Data and Annotation,"Although large-scale datasets on pelvic segmentation have been studied in some research [13], to the best of our knowledge, currently there is no well-annotated fractured pelvic dataset publicly available. Therefore, we curated a dataset of 100 preoperative CT scans covering all common types of pelvic fractures. These data is collected from 100 patients (aged 18-74 years, 41 females) who were to undergo pelvic reduction surgery at Beijing Jishuitan Hospital between 2018 and 2022, under IRB approval (202009-04). The CT scans were acquired on a Toshiba Aquilion scanner. The average voxel spacing is 0.82 × 0.82 × 0.94 mm 3 . The average image shape is 480 × 397 × 310.To generate ground-truth labels for bone fragments, a pre-trained segmentation network was used to create initial segmentations for the ilium and sacrum [13]. Then, these labels were further modified and annotated by two annotators and checked by a senior expert."
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,3,Experiments and Results,
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,3.1,Implementation,"We compared the proposed method (FDMSS-UNet) against the network without smooth transition and deep supervision (FDM-UNet) and the network without distance weighting (UNet) in an ablation study. In addition, we also compared the traditional max-flow segmentation method. In the five-fold cross-validation, each model was trained for 2000 epochs per fold. The network input was augmented eight times by mirror flip. The learning rate in ADAM optimizer was set to 0.0001. λ back was set to 0.2. λ ce was set to 1. The initial λ F DM was set to 16. The termination number for smooth transition τ was set to 500 epochs.The models were implemented in PyTorch 1.12. The experiments were performed on an Intel Xeon CPU with 40 cores, a 256 GB memory, and a Quadro RTX 5000 GPU. The comprehensive code and pertinent details are provided at https://github.com/YzzLiu/FracSegNet."
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,3.2,Evaluation,"We calculate the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff Distance (HD) to evaluation the performance. In addition, we evaluated the local Dice (LDSC) within the 10 mm range near the CFS to assess the performance in the critical areas. We reported the performance on iliac main fragment (I-main), iliac other fragment(s) (I-other), sacral main fragment (Smain), sacral other fragment(s) (S-other), and all together."
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,3.3,Results,"Figure 2 shows a qualitative comparison among different methods. Max-flow was able to generate reasonable segmentation only when the CFS is clear and mostly non-contact. UNet correctly identified the fracture fragments, but was often confused by the complicated CFS regions, resulting in imprecise fracture lines. The introduction of FDM weighting and deep supervision with smooth transition successfully improved the performance near the CFS, and achieved the overall best result.Table 1 shows the quantitative results. The results on the main fragments were better than other fragments due to the larger proportion. The deep learning methods had much higher success rates in identifying the fragments, resulting in significantly better results in minor fragments than max-flow, with p < 0.05 in paired t-test. Introducing the FDM significantly improved the prediction accuracy in the CFS region (p < 0.05). Although FDM-UNet achieved the best LDSC results in several parts, it compromised the global performance of DSC and HD significantly, compared to FDMSS-UNet. The deep supervision and smooth transition strategies stabilized the training, and achieved the overall best results, with balanced local and global performance.The average inference time for the fracture segmentation network was 12 s. The overall running time for processing a pelvic CT was 0.5 to 2 min, depending on the image size and the number of fractured bones. "
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,4,Discussion and Conclusion,"We have introduced a pelvic fracture CT segmentation method based on deep convolutional networks. A fracture segmentation network was trained with a distance-weighted loss and multi-scale deep supervision to improve fracture surface delineation. We evaluated our method on 100 pelvic fracture CT scans and made our dataset and ground truth publicly available. The experiments demonstrated the method's effectiveness on various types of pelvic fractures. The FDM weighted loss, along with multi-scale deep supervision and smooth transition, improved the segmentation performance significantly, especially in the areas near fracture lines. Our method provides a convenient tool for pelvis-related research and clinical applications, and has the potential to support subsequent automatic fracture reduction planning. One obstacle for deep learning-based fracture segmentation is the variable number of bone fragments in different cases. The ultimate goal of this study is to perform automatic fracture reduction planning for robotic surgery, where the main bone fragment is held and moved to the planned location by a robotic arm, whereas minor fragments are either moved by the surgeons' hands or simply ignored. In such a scenario, we found isolating the minor fragments usually unnecessary. Therefore, to define a consistent labeling strategy in annotation, we restrict the number of fragments of each bone to three. This rule of labelling applies to all 100 cases we encountered. Minor fragments within each label can be further isolated by CCA or handcrafting when needed by other tasks.We utilize a multi-scale distance-weighted loss to guide the network to learn features near the fracture site more effectively, boosting the local accuracy without compromising the overall performance. In semi-automatic pipelines where human operators are allowed to modify and refine the network predictions, an accurate initial segmentation near the fracture site is highly desirable because the fracture surface itself is much more complicated, often intertwined and hard to draw by manual operations. Therefore, with the emphasis on fracture surface, even when the prediction from the network is inaccurate, manual operations on 3D view can suffice for most modifications, eliminating the need for the inefficient slice-by-slice handcrafting. In future studies, we plan to integrate the proposed method into an interactive segmentation and reduction planning software and evaluate the overall performance."
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,,Fig. 1 .,
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,,Fig. 2 .,
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,,Table 1 .,
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,1,Introduction,"Skin flap is a widely used technique to close the wound after the resection of a lesion. In a skin flap procedure, a healthy piece of tissue is harvested from a nearby site to cover the defect [1]. Careful planning of such procedure is often needed, especially in the facial region where aesthetic of the final outcome is crucial. The ideal wound closure is dependent on many factors including incision path, anisotropic skin tension, location with respect to aesthetic sub-units, and adjacent tissue types [1]. For each patient, there often exists many valid reconstructive plans and a surgeon needs to take the above factors into account to strive for the most optimal aesthetic result while making sure facial sub-units functionalities are not affected. While there are basic guidelines in medical textbooks, there is often no single recipe that a surgeon can follow to determine the most optimal surgical plan for all patients. Surgeons require a lot of training and experience before they can start to make such decisions. This poses a challenge as each surgeon has their own opinion on the best flap design for each case (so it is hard to reach a consensus) and training a new surgeon to perform this task remains difficult.In this paper, we build a system to help surgeons pick the most optimal flap orientation. We focus on the rhomboid flap as it is a very versatile flap. According to medical textbooks [1,2], the course of Relaxed Skin Tension Lines (RSTLs) is important information for surgeons when deciding on the orientation of rhomboid flaps. We validate this claim with our simulation and provide new insights into designing rhomboid flaps. The main contributions of the paper are:-We created a skin flap FEM simulation and validated the simulation outputs of rhomboid flaps against other simulators; -We performed quantitative evaluation of the suture forces for rhomboid flaps and compared the result against textbook knowledge; -We provided an objective and methodical way to make recommendations for aligning rhomboid flaps relative to RSTLs; -We generated a database of rhomboid flap simulations under various material properties and relative angles to RSTLs."
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,2,Related Works,"There exists various virtual surgical simulators for facial skin flap procedures. In a recent work, Wang and Sifakis et al. built an advanced skin flap simulator that allows users to construct free-form skin flaps on a 3D face model interactively [3] (We further reference this work as ""UWG simulator""). Mitchell and Sifakis et al. also worked on earlier versions of interactive simulators where a novel computational pipeline was presented in GRIDiron [4] and an embedding framework was introduced [5]. Additionally, there are also many other interactive tutorials created for educational purposes [6][7][8]. While the above simulators are all very valuable for surgical education, they do not provide recommendations for flap type or alignment.To gain insights into deformations in a skin flap procedure, there are various FEM simulations with stress/strain analysis. Most of those simulations are built based on a commercial FEM software such as Abaqus [9][10][11][12][13][14][15][16], ANSYS [17][18][19][20][21][22], or MSC. Marc/Mentat [23,24]. In those simulations, one or more skin flap types are analyzed and visualizations of different stress/strain measures are provided. Stowers et al. [9] constructed a surrogate model for creating simulated result with different material properties efficiently. With this model, they ran simulations with various material properties of three different flaps (including rhomboid flap) and provided visualizations of strain distribution and optimal flap alignment with various material properties. However, they did not come up with a consistent conclusion for the most optimal alignment for rhomboid flap. Spagnoli et al. [10] and Rajabi et al. [20] both evaluated different internal angles of the rhomboid flap and provided recommendations for the most optimal design. Rajabi et al. [20] also provided a modified design for the rhomboid flap based on stress optimization. Rhomboid flaps were also analyzed for comparison between different age groups [13], for comparison between different skin models [17] and for analysis on 3D face models [19]. Although the above works all offer valuable insights, they do not provide recommendations for the most optimal orientation for rhomboid flap nor evaluation/comparison for their results."
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,3,Methods,We built our skin flap FEM simulation based on the open-source library Bartels [25] and different components of the simulation are described in this section.
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,3.1,Constitutive Model of Skin,"The skin patch is modelled as a thin membrane that resides in two dimensional space (planar stress formulation), as it is a common setup in the existing literature [9][10][11][12]. We model the skin using the adapted artery strain energy model proposed by Gasser-Ogden-Holzapfel (GOH) [26]. The strain energy density function is the sum of a volumetric component (Ψ vol ), an isochoric component ( Ψ iso ) and an anisotropic component ( Ψ f ):The isochoric component is proportional to μ, the shear modulus and the anisotropic component is parameterized by k 1 and k 2 , which are stiffness parameters. More details of the skin constitutive model can be found in Stowers et al. [9]."
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,3.2,Cutting and Suturing,Cutting is accomplished by duplicating vertices along the cutting line and remeshing the local neighborhood to setup the correct connections with the duplicated vertices. This mechanism can run very efficiently and is sufficient for our setup since we always triangulate the mesh area based on the specific flap type so cutting can only happen on edges. An illustration of such mechanism is shown in the Supplementary Material. The suture process is implemented by adding zero rest-length springs between vertices. Different stages of a rhomboid flap procedure is shown in Fig. 1b to d.
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,3.3,Triangulation,"In our setup, we assume the lesion is within a circular area at the center of the patch. Similar to [9], for each flap type, a predefined template is used to build the geometric shape of the flap and the corresponding suture points are manually specified. To ensure the symmetry of the patch boundary, the vertices on the outer bounds of the circular patch are generated so that there is one vertex every 90 • (i.e. at 0 • , 90 • , 180 • , and 270 • ). Then the vertices in each quadrant are evenly spaced, and the number of vertices in each quadrant is the same (the exact number depends on the triangulation density and size of the patch). The triangulation is generated with the publicly available library Triangle [27,28]. An example triangulation for rhomboid flap can be seen in Fig. 1. "
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,3.4,Validation,"To validate our FEM simulation, we compared it with UWG simulator [3,29]. We first performed a skin flap procedure in UWG simulator and took a screenshot of the head model with the flap design (""pre-operative"" image) and another one with the completed flap procedure (""post-operative"" image). Then we aligned the flap design in our simulation with the ""pre-operative"" image taken from UWG simulator. Next, we ran our simulation and obtained the final suture line.Lastly, we compared the suture line of our simulation with the ""post-operative"" image. We used normalized Hausdorff distance [30] (nHD) to quantify similarities between two suture lines, where Hausdorff distance is divided by the larger of the height and width of the bounding box of the suture line generated by UWG simulator (an illustration of the pipeline is shown in Fig. 2)."
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,3.5,Suture Force Analysis,"Multi-sutures. As excess force along the suture line can cause wound dehiscence and complications [9], the flap rotation angle that requires the lowest suture force should be the optimal orientation. For this experiment, we set the direction of RSTLs to be the horizontal direction. The initial pose of the flap was oriented with its first segment of the tail (the one that is connected to the rhomboid) perpendicular to RSTLs, as seen in Fig. 1a. To reduce the effect of fixed boundary, we set the radius of the patch to be 1 unit and the radius of the lesion to be 0.1 unit. To find the optimal alignment, we rotated the flap design counter-clockwisely in increment of 10 • . After we found the rotation angle with minimum suture force, we fine-tuned the result by searching between -10 • and +10 • of that minimum rotation angle at increments of 1 • . For each rotation, after reaching steady state, we took the maximum suture force among all the sutures shown as colored dots in Fig. 1d (see Algorithm 1 in Fig. 3). "
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,,Fig. 3. Pseudocode of two different suture force analyses,"To investigate the effect of material properties, we repeated the same procedure using the material properties listed in Table 1 (same range as in [9]). We changed one material property at a time while keeping other properties at the average (default) value. The material properties used in each trial can be seen in Table 1.Table 1. Material properties of different trials (rows represent material parameters and columns represent trial setups). The ranges of valid material properties of the skin were taken from [9], where µ is the shear modulus, k1 and k2 are stiffness parameters.def ault small µ large µ small k1 large k1 small k2 large k2 Single-suture. Instead of taking the maximum suture force after completing all sutures, we also experimented with making one suture at a time (see Algorithm 2 in Fig. 3) and picking the maximum suture force for any single suture (example of a single-suture step is shown in Fig. 1e)."
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,4,Results,The simulation results were generated using a Macbook Pro and an iMac. It required around 30 s -2 min to compute suture force for each rotation.
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,4.1,Validation with UWG Simulator,"Further comparisons of skin flap placed at various locations of the face model are shown in Fig. 4. The normalized Hausdorff distances shown in Fig. 2 and 4 are all very small (below 0.1), indicating a good match between the two simulators. "
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,4.2,Suture Force Analysis,"Multi-sutures. The suture forces of various flap rotation angles under different material properties are shown in Table 2 and Fig. 5. Further analysis shows that the minimum suture force occurred at 99 • and 281 • (median value). As seen in Fig. 5b, the maximum force occurred at vertex 6 for all material properties (yellow dot in Fig. 1d), which is consistent with what is reported in literature [31].  Single-suture. We repeated the same experiments with Algorithm 2 in Fig. 3 and the results are shown in Table 2 and Fig. 6. For our single-suture analysis, the minimum suture force occurred at 105 • and 284 • (median value)."
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,4.3,Database Creation,"While performing different experiments, we also created a dataset of rhomboid flap simulation videos under different material properties and the corresponding mesh at the end of each simulation. There are in total over 500 video clips of around 30 s. Overall, there are more than 10,000 frames available. A sample video can be found in the Supplementary Material. The dataset can be accessed through the project website: https://medcvr.utm.utoronto.ca/ miccai2023-rhomboidflap.html. "
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,5,Discussion and Conclusion,"We built a FEM simulation to find the most optimal flap orientation for rhomboid flap based on suture force analysis. Through minimizing suture forces, we found that the optimal orientation occurred at 99 • /281 • and 105 • /284 • for multisutures and single-suture settings, respectively. The range of forces we obtained in our simulation is similar to what was reported in literature [22,24,32]. For Limberg flap (rhomboid flap with equal edge lengths and a top angle of 120 • ), there is a textbook description that aligns it based on lines of maximum skin extensibility (LMEs), which are perpendicular to RSTLs (see Supplementary Material). For our setting, this textbook alignment occurs at 120 • and 300 • . A comparison between our simulation and textbook knowledge is shown in Fig. 7.Our experiment suggests that the minimal suture force occurs at a configuration that is close to textbook knowledge recommendation, but we found that rhomboid flaps have to be rotated further 15 • to 20 • away from the LMEs as seen in Fig. 7. This could be because that the textbook knowledge is trying to optimize a different goal or has taken other factors into account. Additionally, as seen in Table 2, the optimal angle is consistent among all trials, except for trial small k 1 for both settings, which suggests that a good estimation of material properties of the patient's skin is needed before more specific recommendation of rhomboid flap orientation can be made.Our current simulation does not take the complex physical structure of the human face into account. In this study, we aim to compare the prediction made by this setup with textbook knowledge based on RSTLs, which also does not take the more complex facial feature differences into account. This means our model will likely not work well on non-planar regions of the face. Further investigation and comparison with a physical setup (with either synthetic model or clinical setting) is also needed to show clinical significance of our result (for both suture forces comparison and flap orientation design). In the future, it would be interesting to continue investigating this topic by comparing behaviors of different flap types."
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,,Fig. 1 .,
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,,Fig. 2 .,
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,,Fig. 4 .,
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,,Fig. 5 .,
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,,Fig. 6 .Fig. 7 .,
Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps,,Table 2 .,
A Transfer Learning Approach to Localising a Deep Brain Stimulation Target,1,Introduction,"Functional neurosurgical techniques, such as deep brain stimulation (DBS), and MR-guided stereotactic ablation, have been used as effective treatments of neurological and psychiatric disorders for decades. By intervening on a target brain structure, a neurosurgical treatment typically modulates brain activity in disease-related circuits and can often mitigate the disease symptoms and restore brain function when drug treatments are ineffective. An example is the ventral intermediate nucleus of thalamus (Vim), a well-established surgical target in DBS and stereotactic ablation for the treatment of tremor in Parkinson's Disease, essential tremor, and multiple sclerosis [1]. The Vim plays a central role in tremor circuitry [2][3][4][5][6][7][8][9]. It receives efferent fibers from the dentate nucleus of the contralateral cerebellum and projects primarily to the primary motor cortex (M1) [5], as part of the dentato-thalamo-cortical pathway (DTCp).As the structure is not readily visible on conventional MR images, targeting the Vim has relied primarily on standardised coordinates provided by stereotactic atlases, instead of directly targeting the structure based on subject-specific image-derived features. Such standardised stereotactic coordinates/atlases provide a reproducible way to identify the nucleus. However, the atlas-based Vim targeting falls short of accounting for the inter-individual and often interhemispheric anatomical variability, which are often substantial for thalamic nuclei [14], whilst efficacy of stereotactic operations heavily depends on accurate identification of the target nucleus.To overcome this limitation, several recent studies proposed to localise the Vim using its anatomical connectivity features (e.g., with M1 and the dentate nucleus) in vivo on an individual basis [11,15,16], aided by cutting-edge diffusion-weighted imaging (DWI) and tractography techniques. DWI allows estimation of local fiber orientations, based upon which tractography generates streamline samples representative of the underlying white matter pathway, starting from a given seed. Typically, these studies identify Vim by finding the region of maximum connectivity with both M1 and contralateral dentate, and thus better capture individual variations of the nucleus, leading to improved clinical outcome [11,12].However, the connectivity-driven Vim requires high angular resolution diffusion imaging (HARDI) techniques to allow reconstruction of Vim's connectivity features, which are often not readily available in advanced-care clinical contexts. Furthermore, even with cutting-edge HARDI and higher order diffusion modelling techniques, the connectivity-derived Vim has exhibited considerable variations across different acquisition protocol and processing pipelines, suggesting that they have to be used with caution to ensure that they adhere to the true underlying anatomical variability instead of simply reflecting the methodological confounds erroneously interpreted as variability.Given the limitations of the standardised approaches and connectivity-based methods, we propose a novel approach, HQ-augmentation, to reliably target the Vim, particularly for clinical (or generally lower-quality) data. We utilised the publicly-available high-quality (HQ) Human Connectome Project (HCP) dataset [18,19] to augment surgical targeting of the Vim on low-quality (LQ) data. More specifically, the approach transfers the anatomical information derived from high-quality data, i.e., the approximate position of the Vim, to a wide range of low-quality white matter connectivity features in order to infer the likelihood that a given voxel is classified as part of Vim on the corresponding low-quality data. We demonstrate that the proposed approach not only yields Fig. 1. Schematic illustration of the HQ-augmentation model. The process begins with the use of HQ diffusion data to establish a ""ground truth"" Vim within the thalamic masks, utilising white matter connectivity with the M1 and contralateral cerebellum. Following this, surrogate low-quality diffusion datasets are generated through intentional degradation of the HQ datasets. The HQ-augmentation model is then trained using the ""ground truth"" Vim derived from the HCP HQ data as target labels, and a broad set of HCP's low-quality connectivity profiles (in the form of voxel-by-ROI matrices) as input features. After training, the model is applied to unseen low-quality datasets, which include surrogate low-quality HCP and UK Biobank diffusion data, to predict Vim location. The performance of the model is subsequently evaluated against the corresponding HQ ""ground truth"" data.consistent Vim targets despite compromised data quality, but also preserves inter-individual anatomical variability of the structure. Furthermore, the approach generalises to unseen datasets with different acquisition protocols, showing potential of translating into a reliable clinical routine for MR-guided surgical targeting."
A Transfer Learning Approach to Localising a Deep Brain Stimulation Target,2,Materials and Methods,"HQ Dataset I: Human Connectome Project (HCP) 3T MRI Datasets. We leveraged the 3T diffusion MRI data from HCP [18] as the HQ dataset. The minimally pre-processed T1-, T2-, and diffusion-weighted MRI scans from a total of 1,062 healthy young adults were included, among which 43 subjects were scanned twice. Diffusion-weighted imaging was acquired at isotropic spatial resolution 1.25 mm, with three shells (b-values = 1000, 2000 and 3000 s/mm 2 ) and approximately 90 unique diffusion directions per shell, acquired twice (total scan time 60 min per subject) [17].HQ Dataset II: UK Biobank (UKB) 3T MRI Datasets. The UKB 3T MRI datasets [24] were also used as HQ data. The T1, T2 and diffusion-weighted scans from 2,560 subjects with retest (second scan) sessions were included. The diffusion MRI was carried out at isotropic spatial resolution 2 mm with two shells (b-values = 1000 and 2000 s/mm 2 ), 50 diffusion directions per shell (total scan time 6min per subject).Surrogate Low-Quality (LQ) Datasets. We considered a range of lowquality datasets representing the typical data quality in clinical contexts. This included 1) HCP surrogate low angular resolution diffusion dataset, obtained by extracting the b = 0 s/mm 2 and 32 b = 1000 s/mm 2 volumes from HQ HCP 3T diffusion MRI (""LQ-LowAngular""); 2) HCP surrogate low spatial resolution dataset, obtained by downsampling the HCP 3T diffusion MRI to isotropic 2 mm sptial resolution (""LQ-LowSpatial""); 3) HCP surrogate low angular and spatial resolution dataset, created by downsampling the surrogate low-angularresolution dataset to isotropic 2 mm spatial resolution (""LQ-LowAngular-LowSpatial""); 4) UKB surrogate low angular resolution dataset, created by extracting the b = 0 s/mm 2 and 32 b = 1000 s/mm 2 volumes from the original UKB diffusion dataset (""LQ-UKB"").The Connectivity-Driven Approach. We followed the practice in [11] to generate connectivity-driven Vim. This approach identifies the Vim by finding the maximum probability of connection to M1 and contralateral dentate nucleus within the thalamic mask, generated via probablistic tractography. When this is applied to HQ original data, it generates the ground truth segmentations. When it is applied to LQ surrogate data, it provides results against which other methods (atlas-based and HQ-augmentation) can be compared.The Atlas-Defined Approach. We used a previously published and validated Vim atlas [11] to find atlas-defined Vim on individual subjects. The atlas was registered into the individual T1 space (also referred to native space) via the warp fields between the corresponding individual T1 scans and the MNI152 standard brain. The warped group-average Vim atlas was thresholded at 50% percentile in the native space as the atlas-defined Vim.The HQ-Augmentation Approach. The goal of this approach is to leverage anatomical information in HQ data to infer the likelihood of a voxel belonging to the Vim, given a wide range of tract-density features (multiple distinct tract bundles) derived from low-quality data. The HQ-augmentation model was trained on the HCP dataset for each type of low-quality dataset. Using the HCP HQ data, we first generated the connectivity-driven Vim (referred to as HQ-Vim) as the ""ground truth"" location of the nucleus, serving as training labels in the model. Next, for each low-quality counterpart, we generated an extended set of tract-density features, targeting a wide range of region-of-interests (ROIs), as the input features of the model. The richer set of connectivity features serves to compensate for the primary tract-density features (with M1 and dentate), when those are compromised by less sufficient spatial or angular resolution in low-quality diffusion MRI, thus making Vim identification less reliant on the primary tract-density features used in the connectivity-driven approach and more robust to variations in data quality. During training, the model learns to use the extended set of low-quality connectivity features to identify the Vim that is closest to the one that can be otherwise obtained from its HQ counterpart.Specifically, assume X = [x 1 , x 2 , ...x V ] T is a V ×d connectivity feature matrix for a given subject, where x i is a d × 1 vector representing the connectivity features in voxel i, V is the total number of voxels of the thalamus (per hemisphere) for this subject; y = [y 1 , y 2 , ...y V ] T is a V × 1 vector containing the HQ-Vim labels for the V voxels, in which y i is the label of voxel i. Given the low-quality features X, we seek to maximise the probability of reproducing the exact same HQ-Vim label assignment y on its low-quality counterparts, across the training subjectsHere E(y|X) is the cost of the label assignment y given the features X, whilst Z(X) serves as an image-dependent normalising term. Maximising the posterior P (y|X) across subjects is equivalent to minimising the cost of the label assignment y given the features X. Suppose N i is the set of voxels neighbouring voxel i, the cost E(y|X) is modelled asThe first component ψ u (y i ) measures the cost (or inverse likelihood) of voxel i taking label y i . Here ψ u (y i ) takes the form ψ u (y i ) = w T yi φ(x i ), where φ(•) maps a feature vector x i = [x 1 , x 2 , ...x d ] to a further expanded feature space in order to provide more flexibility for the parameterisation. W = [w 1 , w 2 ] is the coefficient matrix, each column containing the coefficients for the given class (i.e., belonging to the HQ-Vim or not). Here we chose a series of polynomials along with the group-average Vim probability (registered into native space) to expand the feature space, i.e.,"
A Transfer Learning Approach to Localising a Deep Brain Stimulation Target,,φ(x,"where p 1 = 2, p 2 = 0.5, p 3 = 0.2 are the power of the polynomials, and g i is the group-average probability of voxel i classified as Vim. The second pairwise cost encourages assigning similar labels to neighbouring voxels, particularly for those sharing similar connectivity features. We modelled this component as) is a kernel function modelling the similarity between voxel i and j in the extended feature space, with length scale γ m , chosen via cross-validation. μ(•) is a label compatibility function where μ(y i , y j ) = 0 ifTherefore, in a local neighbourhood, the kernel function penalises inconsistent label assignment of voxels that have similar features, thus allowing modelling local smoothness. ρ m controls the relative strength of this pairwise cost weighted by k m (•). Lastly, the L1 and L2 penalty terms serve to prevent overfitting of the model. We used a mean-field algorithm to iteratively approximate the maximum posterior P (y|X) [29] summed across the subjects. The approximated posterior is maximised via gradient descent in a mini-batch form, where the connectivity feature matrix of each subject serves as a mini-batch, demeaned and normalised, and sequentially fed into the optimisation problem."
A Transfer Learning Approach to Localising a Deep Brain Stimulation Target,3,Experiments and Discussions,"Accuracy of the HQ-Augmentation Model on the HCP Surrogate Low-Quality Datasets. The HQ-augmentation model was trained using HQ-Vim as labels and the LQ connectivity profiles as features, and then tested on left-out subjects from the same LQ dataset. Its accuracy was evaluated against the HQ-Vim counterpart obtained from the HQ HCP data of these left-out subjects, which served as ground truth. The accuracies of the HQ-augmentation Vim were also compared with those of the atlas-defined Vim and the connectivity-driven Vim, the latter obtained using the low-quality M1 and dentate tract-density features. We considered two accuracy metrics: Dice coefficient, which measures the overlap with the ground truth, and centroid displacement, which calculates the Euclidean distance between the predicted and ground truth centers-of-mass. As the connectivity-driven approach may even fail on HQ data, resulting in unreliable HQ-Vim, the training and evaluation of HQ-augmentation model were conducted only on a subset of ""good subjects"", whose HQ-Vim's center-of-mass was within 4 mm from the atlas's center-of-mass in native space. For the subjects which HQ-Vim regarded as unreliable (i.e., deviating too much from the atlas), the accuracy of HQ-augmentation model was evaluated against the atlas-defined Vim. The HQ-augmentation model produced Vim predictions that are closest to the HQ-Vim (i.e., higher Dice coefficient and smaller centroid displacement) than the LQ connectivity-driven approach and the atlas-defined Vim, evaluated on the reliable subset (see Fig. 2A for the results on LQ-LowSpatial-LowAngular data; see supplementary materials for results on the other low-quality datasets). When the HQ-vim failed to serve as approximate ground truth, the HQ-augmentation model on low-quality data yielded Vim predictions that were closer to the atlasdefined Vim than the HQ-Vim derived from HQ data (Fig. 2B)."
A Transfer Learning Approach to Localising a Deep Brain Stimulation Target,,Generalisability of the HQ-Augmentation,"Model to UK Biobank. We also tested whether the HQ-augmentation models trained on HCP were generalisable to other datasets collected under different protocols. It is crucial for a model to be generalisable to unseen protocols, as collecting large datasets for training purposes in clinical contexts can often be impractical. We therefore applied the HQ-augmentation models trained on different HCP LQ datasets to UK Biobank surrogate low-quality data (LQ-UKB) and averaged the outputs to give a single Fig. 2. Accuracy of HQ-augmentation on LQ-HCP-LowSpatial-LowAngular data. (A) When using the HQ-Vim as ground truth, the HQ-augmentation approach using low-quality features (here LQ-LowAngular-LowSpatial features) gives higher Dice coefficient and smaller centroid displacement with the HQ-Vim, than the atlas-defined Vim (green) and the low-quality connectivity-driven Vim (orange). (B) When using the atlas-defined Vim as ground truth (because, in these ""unreliable"" subjects, the HQconnectivity method was considered to have failed), the HQ-augmentation model using low-quality features even gives more reliable results than the HQ-Vim (red). ensembled Vim prediction per UKB subject in the LQ-UKB dataset. (Note that this step did not involve retraining or fine-tuning for LQ-UKB.) To evaluate its performance, similarly, we derived the HQ-Vim from the original (HQ) UK Biobank data via the connectivity-driven approach, and split it into a reliable subset, in which the UKB HQ-Vim served as the ground truth, and an unreliable subset, in which the atlas-defined Vim (warped into the UKB individual native space) served as the ground truth. On the reliable subset, the evaluations were conducted against the UKB HQ-Vim (Fig. 3A), whilst on the unreliable subset, the evaluations were conducted against the atlas-defined Vim (Fig. 3B). Despite being trained on HCP, the HQ-augmentation model produced Vim predictions that were closer to the corresponding HQ-Vim on the UKB subjects, outperforming the connectivity-driven approach on the UKB surrogate low-quality data and the atlas-defined approach. Even when using the atlas-defined Vim as the ground truth, the HQ-augmentation Vim on the UKB low-quality data showed higher accuracy than the HQ-Vim derived from the HQ data.Reliability Analysis of the HQ-Augmentation Model. We also conducted a reliability analysis for the HQ-augmentation model and the connectivity-driven approach to assess their consistency in providing results despite variations in data quality (across-quality reliability) and across scanning sessions (test-retest reliability). To evaluate the HQ-augmentation model's across-quality reliability, we trained it using HQ tract-density maps to produce an ""HQ version"" of HQaugmented Vim. The similarity between HQ-augmentation outputs using high-or low-quality features was assessed using the Dice coefficient and centroid displacement measures. Test-retest reliability was determined by comparing the outputs of the HQ-augmentation model on first-visit and repeat scanning sessions. Similarly, we assessed the across-quality reliability and test-retest reliability for the connectivity-driven approach, applied to high-and low-quality data accordingly. The HQ-augmentation model consistently provided more reliable results than the connectivity-driven approach, not only across datasets of different quality but also across different scanning sessions (Fig. 4).Discussion. Our study presents the HQ-augmentation technique as a robust method to improve the accuracy of Vim targeting, particularly in scenarios where data quality is less than optimal. Compared to existing alternatives, our approach exhibits superior performance, indicating its potential to evolve into a reliable tool for clinical applications. The enhanced accuracy of this technique has significant clinical implications. During typical DBS procedures, one or two electrodes are strategically placed near predetermined targets, with multiple contact points to maximise the likelihood of beneficial outcomes and minimise severe side effects. Greater accuracy translates into improved overlap with the target area, which consequently increases the chances of successful surgical outcomes. Importantly, the utility of our method extends beyond Vim targeting. It can be adapted to target any area in the brain that might benefit from DBS, thus expanding its clinical relevance. As part of our ongoing efforts, we are developing a preoperative tool based on the HQ-augmentation technique. This tool aims to optimise DBS targeting tailored to individual patients' conditions, thereby enhancing therapeutic outcomes and reducing patient discomfort associated with suboptimal electrode placement."
A Transfer Learning Approach to Localising a Deep Brain Stimulation Target,,Fig. 3 .Fig. 4 .,
A Transfer Learning Approach to Localising a Deep Brain Stimulation Target,,,
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,1,Introduction,"Gliomas are the most common central nervous system (CNS) tumors in adults, accounting for 80% of primary malignant brain tumors [1]. Early surgical treatment to remove the maximum amount of cancerous tissues while preserving the eloquent brain regions can improve the patient's survival rate and functional outcomes of the procedure [2]. Although the latest multi-modal medical imaging (e.g., PET, diffusion/functional MRI) allows more precise pre-surigcal planning, during surgery, brain tissues can deform under multiple factors, such as gravity, intracranial pressure change, and drug administration. The phenomenon is referred to as brain shift, and often invalidates the pre-surgical plan by displacing surgical targets and other vital anatomies. With high flexibility, portability, and cost-effectiveness, intra-operative ultrasound (US) is a popular choice to track and monitor brain shift. In conjunction with effective MRI-US registration algorithms, the tool can help update the pre-surgical plan during surgery to ensure the accuracy and safety of the intervention.As the true underlying deformation from brain shift is impossible to obtain and the differences of image features between MRI and US are large, quantitative validation of automatic MRI-US registration algorithms often rely on homologous anatomical landmarks that are manually labeled between corresponding MRI and intra-operative US scans [3]. However, manual landmark identification requires strong expertise in anatomy and is costly in labor and time. Moreover, inter-and intra-rater variability still exists. These factors make quality assessment of brain shift correction for US-guided brain tumor resection challenging. In addition, due to the time constraints, similar evaluation of inter-modal registration quality during surgery is nearly impossible, but still highly desirable. To address these needs, deep learning (DL) holds the promise to perform efficient and automatic inter-modal anatomical landmark detection.Previously, many groups have proposed algorithms to label landmarks in anatomical scans [4][5][6][7][8][9]. However, almost all earlier techniques were designed for mono-modal applications, and inter-modal landmark detection, such as for USguided brain tumor resection, has rarely been attempted. In addition, unlike other applications, where the full anatomy is visible in the scan and all landmarks have consistent spatial arrangements across subjects, intra-operative US of brain tumor resection only contains local regions of the pathology with noncanonical orientations. This results in anatomical landmarks with different spatial distributions across cases. To address these unique challenges, we proposed a new contrastive learning (CL) framework to detect matching landmarks in intra-operative US with those from MRI as references. Specifically, the technique leverages two convolutional neural networks (CNNs) to learn features between MRI and US that distinguish the inter-modal image patches which are centered at the matching landmarks from those that are not. Our approach has two major novel contributions to the field. First, we proposed a multi-modal landmark detection algorithm for US-guided brain tumor resection for the first time. Second, CL is employed for the first time in inter-modal anatomical landmark detection. We developed and validated the proposed technique with the public RESECT database [10] and compared its landmark detection accuracy against the popular scale-invariant feature transformation (SIFT) algorithm in 3D [11]."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,2,Related Work,"Contrastive learning has recently shown great results in a wide range of medical image analysis tasks [12][13][14][15][16][17][18]. In short, it seeks to boost the similarity of feature representations between counterpart samples and decrease those between mismatched pairs. Often, these similarities are calculated based on deep feature representations obtained from DL models in the feature embedding space. This self-supervised learning set-up allows robust feature learning and embedding without explicit guidance from fine-grained image annotations, and the encoded features can be adopted in various downstream tasks, such as segmentation. A few recent works [19][20][21] explored the potential of CL in anatomical landmark annotation in head X-ray images for 2D skull landmarks. Quan et al. [19,20] attempted to leverage CL for more efficient and robust learning. Yao et al. [21] used multiscale pixel-wise contrastive proxy tasks for skull landmark detection in X-ray images. With a consistent protocol for landmark identification, they trained the network to learn signature features within local patches centered at the landmarks. These prior works with CL focus on single-modal 2D landmark identification with systematic landmark localization protocols and sharp image contrast (i.e., skull in X-ray). In contrast, our described application is more challenging due to the 3D nature, difficulty in inter-modal feature learning, weaker anatomical contrast (i.e., MRI vs US), and variable landmark locations. In CL, many works have employed the InfoNCE loss function [22,23] in attaining good outcomes. Inspired by Yao et al. [21], we aimed to use InfoNCE as our loss function with a patch-based approach. To date, CL has not been explored in multi-modal landmark detection, a unique problem in clinical applications. In this paper, to bridge this knowledge gap, we proposed a novel CL-based framework for MRI-US anatomical landmark detection."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,3,Methods and Materials,
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,3.1,Data and Landmark Annotation,"We employed the publicly available EASY-RESECT (REtroSpective Evaluation of Cerebral Tumors) dataset [10] (https://archive.sigma2.no/pages/ public/dataset Detail.jsf?id=10.11582/2020.00025) to train and evaluate our proposed method. This dataset is a deep-learning-ready version of the original RESECT database, and was released as part of the 2020 Learn2Reg Challenge [24]. Specifically, EASY-RESECT contains MRI and intra-operative US scans (before resection) of 22 subjects who have undergone low-grade glioma resection surgeries. All images were resampled to a unified dimension of 256 × 256 × 288 voxels, with an isotropic resolution of ∼0.5mm. Between MRI and the corresponding US images, matching anatomical landmarks were manually labeled by experts and 15∼16 landmarks were available per case. A sample illustration of corresponding inter-modal scans and landmarks is shown in Fig. 1. For the target application, we employed the T2FLAIR MRI to pair with intra-operative US since low-grade gliomas are usually more discernible in T2FLAIR than in T1-weighted MRI [10]. "
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,3.2,Contrastive Learning Framework,"We used two CNNs with identical architectures in parallel to extract robust image features from MRI and US scans. Specifically, these CNNs are designed to acquire relevant features from MRI and US patches, and maximize the similarity between features of corresponding patches while minimizing those between mismatched patches. Each CNN network contains six successive blocks, and each block consists of one convolution layer and one group norm, with Leaky ReLU as the activation function. Also, the convolution layer of the first and last three blocks of the network has 64 and 32 convolutional filters, respectively, and a kernel size of 3 is used across all blocks. After the convolution layers, the proposed network has two multi-layer perceptron (MLP) layers with 64 and 32 neurons and Leaky ReLU as the activation function. These MLP layers compress the extracted features from convolutional layers and produce the final feature vectors. The resulting CNN network is depicted in Fig. 2. "
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,3.3,Landmark Matching with a 2.5D Approach,"Working with 3D images is computationally expensive and can make the model training unstable and prone to overfitting, especially when the size of the database is limited. Therefore, instead of a full 3D processing, we decided to implement a 2.5D approach [25] to leverage the efficiency of 2D CNN in the CL framework for the task. In this case, we extracted a series of three adjacent 2D image patches in one canonical direction (x-, y-, or z-direction), with the middle slice centred at the true or candidate landmarks in a 3D scan to provide slight spatial context for the middle slice of interest. To construct the full 2.5D formulation, we performed the same image patch series extraction in all x-, y-, and z-directions for a landmark, and this 2.5D patch forms the basis to compute the similarity between the queried US and reference MRI patches. Note that the setup of CL requires three types of samples, anchor, positive sample pairs, and negative sample pairs. Specifically, the anchor is defined as the 2.5D MRI patch centred at a predefined landmark, a positive pair is represented by an anchor and a 2.5D US patch at the corresponding landmark, and finally, a negative pair means an anchor and a mismatched 2.5D US patch. Note that during network training, instead of 2.5D patches, we compared the 2D image patch series in one canonical direction between MRI and US, and 2D patch series in all three directions were used. During the inference stage, the similarity between MRI and US 2.5D patches was obtained by summing the similarities of corresponding 2D image patch series in each direction, and a match was determined with the highest similarity from all queried US patches. With the assumption that the brain shift moves the anatomy within a limited range, during the inference time, we searched within a range of [-5,5] mm in each direction in the US around the reference MRI landmark location to find the best match. Note that this search range is an adjustable parameter by the user (e.g., surgeons/clinicians), and when no match is found in the search range, an extended search range can be used. The general overview of the utilized framework for 2D image patch extraction is shown in Fig. 3."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,3.4,Landmark Matching with 3D SIFT,"The SIFT algorithm [11] is a well-known tool for keypoint detection and image registration. It has been widely used in multi-modal medical registration, such as landmark matching for brain shift correction in image-guided neurosurgery [8,26]. To further validate the proposed CL-based method for multi-modal anatomical landmark detection in US scans, we replicated the procedure using the 3D SIFT algorithm as follows. First, we calculated the SIFT features at the reference landmark's location in MRI. Then, we acquired a set of candidate SIFT points in the corresponding US scan. Finally, we identified the matching US landmark by selecting the top ranking candidate based on SIFT feature similarity measured with cosine similarity. Note that, for SIFT-based landmark matching, we have attempted to impose a similar spatial constraint like in the CL-based approach. However, as the SIFT algorithm pre-selects keypoint candidates based on their feature strengths, with this in consideration, we saw no major benefits by imposing the spatial constraint."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,4,Experimental Setup,
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,4.1,Data Preprocessing,"For CL training, both positive and negative sample pairs need to be created. All 2D patch series were extracted according to Sect. 3.3 with a size of 42 × 42 × 3 voxels. These sample pairs were used to train two CNNs to extract relevant image features across MRI and US leveraging the InfoNCE loss."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,4.2,Loss Function,"We used the InfoNCE loss [23] for our CL framework. The loss function has been widely used and demonstrated great performance in many vision tasks. Like other contrastive loss functions, InfoNCE requires a similarity function, and we chose commonly used cosine similarity. The formulas for InfoNCE (L Inf oNCE ) and cosine similarity (CosSim) are as follows:where F θ and G β are the CNN feature extractors for MR and US patches. X A i and X P i are the cropped image patches around the corresponding landmarks in MR and US scans, respectively, and X N i is a mismatched patch in the US image to that cropped around the MRI reference landmark. Here, F θ • X A i , G β • X P i , and G β • X N j give the extracted feature vectors for MR and US patches."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,4.3,Implementation Details and Evaluation,"To train our DL model, we made subject-wise division of the entire dataset into 70%:15%:15% as the training, validation, and testing sets, respectively. Also, to improve the robustness of the network, we used data augmentation for the training data by random rotation, random horizontal flip, and random vertical flip. Furthermore, an AdamW optimizer with a learning rate of 0.00001 was used, and we trained our model for 50 epochs with a batch size of 256.In order to evaluate the performance of our technique, we used the provided ground truth landmarks from the database and calculated the Euclidean distance between the ground truths and predictions. The utilized metric is as follows: where x i and x i , and N are the ground truth landmark location, model prediction, and the total number of landmarks per subject, respectively."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,5,Results,"Table 1 lists the mean and standard deviation of landmark identification errors (in mm) between the predicted position and the ground truth in intra-operative US for each patient of the RESECT dataset. In the table, we also provide the severity of brain shift for each patient. Here, tissue deformation measured as mean target registration errors (mTREs) with the ground truth anatomical landmarks is classified as small (mTRE below 3 mm), median (3-6 mm), or large (above 6 mm). The results show that our CL-based landmark selection technique can locate the corresponding US landmarks with a mean landmark identification error of 5.88±4.79 mm across all cases while the SIFT algorithm has an error 18.78±4.77 mm. With a two-sided paired-samples t-test, our method outperformed the SIFT approach with statistical significance (p <1e-4). When reviewing the mean landmark identification error using our proposed technique, we also found that the magnitude is associated with the level of brain shift. However, no such trend is observed when using SIFT features for landmark identification. When inspecting landmark identification errors across all subjects between the CL and SIFT techniques, we also noticed that our CL framework has significantly lower standard deviations (p <1e-4), implying that our technique has a better performance consistency."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,6,Discussion,"Inter-modal anatomical landmark localization is still a difficult task, especially for the described application, where landmarks have no consistent spatial arrangement across different cases and image features in US are rough. We tackled the challenge with the CL framework for the first time. As the first step towards more accurate inter-modal landmark localization, there are still aspects to be improved. First, while the 2.5D approach is memory efficient and quick, 3D approaches may better capture the full corresponding image features. This is partially reflected by the observation that the quality of landmark localization is associated with the level of tissue shift. However, due to limited clinical data, 3D approaches caused overfitting in our network training. Second, in the current setup, we employed landmarks in pre-operative MRIs as references since its contrast is easier to understand and it allows sufficient time for clinicians to annotate the landmarks before surgery. Future exploration will also seek techniques to automatically tag MRI reference landmarks. Finally, we only employed US scans before resection since tissue removal can further complicate feature matching between MRI and US, and requires more elaborate strategies, such as those involving segmentation of resected regions [27]. We will explore suitable solutions to extend the application scenarios of our proposed framework as part of the future investigation. As a baseline comparison, we employed the SIFT algorithm, which has demonstrated excellent performance in a large variety of computer vision problems for keypoint matching. However, in the described inter-modal landmark identification for US-guided brain tumor resection, the SIFT algorithm didn't offer satisfactory results. This could be due to the coarse image features and textures of intra-operative US and the differences in the physical resolution between MRI and US. One major critique for using the SIFT algorithm is that it intends to find geometrically interesting keypoints, which may not have good anatomical significance. In the RESECT dataset, eligible anatomical landmarks were defined as deep grooves and corners of sulci, convex points of gyri, and vanishing points of sulci. The relevant local features may be hard to capture with the SIFT algorithm. In this sense, DL-based approaches may be a better choice for the task. With the CL framework, our method learns the common features between two different modalities via the training process.Besides better landmark identification accuracy, the tighter standard deviations also imply that our DL approach serves a better role in grasping the local image features within the image patches."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,7,Conclusions,"In this project, we proposed a CL framework for MRI-US landmark detection for neurosurgery for the first time by leveraging real clinical data, and achieved state-of-the-art results. The algorithm represents the first step towards efficient and accurate inter-modal landmark identification that has the potential to allow intra-operative assessment of registration quality. Future extension of the method in other inter-modal applications can further confirm its robustness and accuracy."
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,,Fig. 1 :,
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,,Fig. 2 :,
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,,Fig. 3 :,
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,,Table 1 :,
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,1,Introduction,"Automating systems to interpret complex behaviors in surgical operating rooms (OR) has seen a surge of interest in recent years [13,20]. Robot-assisted surgeries Lennart Bastian and Daniel Derkacz-Bogner contributed equally to this work.have improved patient outcomes by reducing blood loss, recovery periods, and hospitalization times [26,27]. For robotic systems to autonomously interact with hospital staff, surgical tools, or patients, they must attain a sophisticated and detailed understanding of a highly complex environment. To achieve this, robotic detection systems must comprehend high-level surgical phases and granular 3D object semantics and interactions [16,25].Recent works have established the necessity of combining data from multiple cameras to obtain better coverage of surgical procedures [25,26], as frequent occlusions and obstructions due to personnel and medical equipment obscure important events for individual cameras. Furthermore, a metric 3D semantic understanding is crucial for robotic systems operating and interacting with objects in an environment [31]. In surgical operating rooms, 3D segmentation has previously been approached by fusing semantic RGB predictions from multiple views in 3D under the full supervision of dense 2D labels [16].However, to adequately represent the distribution of possible events in the surgical domain, large volumes of data must be acquired [27]. Training deeplearning models for automated recognition tasks thus induces an enormous annotation burden, particularly as the privacy-sensitive nature of such materials can prevent annotation outsourcing. Therefore, the surgical data science community actively seeks methods to alleviate this burden, particularly through means of domain-adaptation [24], as well as unsupervised and self-supervised learning [12]. While progress has been made in surgical workflow recognition, methods for 3D surgical scene understanding still require fine-grained labels [25], particularly for semantic segmentation [16].To this end, we propose SegmentOR, a weakly-supervised indoor semantic segmentation method for 4D multi-view OR datasets. By leveraging the innate temporal consistency of 4D point cloud sequences, we reduce the annotation burden to only a single click per class (about 0.005% of points), decreasing average annotation time per surgical phase from 3 h to 9.6 min while achieving a higher segmentation mIoU than existing methods that use four times the amount of labels. Furthermore, we establish the soundness of semantic predictions from our model for surgical scene understanding by showing that surgical phase recognition performance can be improved using our segmentation predictions as input. Our main contributions can thus be summarized as follows:-We propose the first 3D weakly-supervised semantic segmentation method for operating room environments and validate it on a manually annotated dataset of 3D point clouds from real surgical acquisitions. -We demonstrate that various temporal priors can be used to exploit consistency in weakly-supervised semantic segmentation, improving performance to 10% mIoU above baseline methods. -We show that the semantic outputs from our model can improve the performance of downstream surgical phase recognition methods, formally establishing the link between these two previously disjoint tasks.-Finally, we release all code and tools, as well as 2577 anonymized and annotated point clouds from the dataset, to advance progress in surgical scene understanding. https://bastianlb.github.io/segmentOR/"
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,2,Related Work,"Surgical Scene Understanding. Workflow recognition is pivotal for contextual awareness in operating room (OR) intelligent systems. Activity recognition has been achieved for single-frame [27,29] and multi-view acquisitions [26], including laparoscopic views and ceiling-mounted cameras [6,7].  Modeling OR activities also requires semantic understanding [16]. Semantic scene graphs offer a detailed approach to surgical procedure modeling [25]. While future intelligent OR systems will employ semantics, manual labeling is time-consuming. However, recent progress in weakly-supervised semantic segmentation, such as one-thing-one-click (OTOC) [19], can decrease this burden, requiring only a single annotation per semantic class [18,19,30].The effectiveness of such weakly-supervised segmentation methods in dynamic OR environments remains unclear. Unlike static indoor datasets like ScanNet [8], dynamic ORs blur the geometric class separation due to humanobject interaction. Moreover, 3D surgical acquisitions, unlike static indoor reconstructions, are fragmented due to static sensor positions and severe occlusions. Imprecise 3D registration or temporal synchronization creates artifacts that further complicate 3D modeling, worsened by dynamic non-rigid movements (see suppl. for examples).Temporal Modeling. Optical flow effectively extracts movement from image sequences [1,9]. Self-supervised methods have recently offered scene flow extraction from LiDAR point clouds [15,23], but few ground-truth flow annotated datasets exist, leading to potential generalization issues [21,22].Few have combined temporal consistency with 3D semantic segmentation for dynamic point cloud sequences outside autonomous driving settings [4,10,14,28]. These methods typically rely on dense ground truth labels or require multi-stage label propagation methods and pre-training. Our approach guides temporal label propagation with unsupervised priors, resolving this cold-start problem costeffectively."
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,3,Method,"Problem Setting. Given a point cloudIn contrast to the supervised setting where dense ground truth labels y i ∈ Y t are available for every point p i , we infer dense semantic labels in an unseen test sequence by training on sparsely annotated point clouds. We refer to this setting as ""weakly-supervised"", meaning ground truth train annotations consist of a randomly chosen point per class (see Fig. 1b). This results in an average of 0.005% of ground truth labels compared to full supervision.Inspired by recent works, we assume semantic instances in a point cloud X t adhere to geometric boundaries and partition the point cloud into a set of supervoxels S t [18,19]. For S j , S j ∈ S t we have S j ∩ S j = ∅, and ∪ N j=1 S j = X t . This allows us to represent a group of points with a single feature, increasing the level of supervision obtained from a single ""click"", and enabling efficient label propagation. Due to the sparse nature of the click annotations, most supervoxels in each point cloud are unlabeled (see Fig. 1c). By propagating learned information into unlabeled supervoxels, the level of supervision can be drastically increased through pseudo-labeling. For clarity, we use indices i for points and j for supervoxels [19]. Indices t are used to indicate timesteps. Proposed Method. Following the approach of OTOC [19], we train a sparse 3D-UNet [5] F Θ to model the semantic class of each point p i , Y t = F Θ (X t ) with a cross-entropy loss L CE , and an identically structured relation network R Ψ to predict a category specific embedding R t = R Ψ (X t ) with a contrastive loss L Cont . The point-wise embeddings obtained from both networks are accumulated using mean-pooling over each supervoxel S j . The pooled feature embeddings f j , r j can then be used to construct a fully-connected graph G i to propagate labels to unlabeled supervoxels. This is achieved by maximizing the expectation E of unlabeled supervoxels given the complete supervoxel set S:where ψ u represents the pooled class predictions, and ψ p is a pairwise similarity between supervoxels S j , S j [19]. By measuring the similarity between supervoxels in this manner, semantic information in the two networks F Θ and R Ψ can be propagated to unlabeled supervoxels iteratively through pseudo-labeling, using a likelihood threshold of, e.g., E(Y |S j ) ≥ 0.90. Setting a high confidence threshold reduces incorrect pseudo-labels, which would negatively impact subsequent training iterations. OTOC [19] considers all supervoxel pairs in a single static acquisition as candidates during this expansion. An intuitive way to extend the graph propagation in the temporal dimension would be to pool all supervoxels over a pair of frames X t and X t+1 , creating a fully connected graph over both supervoxel sets. We refer to this method as OTOC+T, as the original method uses only spatial context. Aside from being computationally expensive, this naive approach does not consider that the nearest supervoxel in an adjacent timestep is highly likely to describe a similar region in the point cloud.To further improve upon this idea, we propose to enforce temporal consistency through the use of a supervoxel matching matrix M [t,t+1] ∈ R m×n where |S t | = m and |S t+1 | = n are the dimensions of the respective supervoxel sets, reducing computational complexity to an additional comparison per supervoxel instead of n as with the OTOC+T. An entry m j,j ∈ M [t,t+1] indicates the probability that supervoxels S j and S j describe a similar region across time steps. Intuitively, this can establish consistency between the pair of point clouds by considering matched supervoxels from a different timestep X t as pseudo-label candidates. To initialize the matching matrix, we explore how nearest neighbor, unsupervised optical [9] and scene flow [23] priors can improve temporal pseudo-label propagation (see suppl. sec. 1 for mathematical details).After initialization through any of these priors, we propose to update the matching iteratively during training, establishing a link between temporal consistency and semantic understanding. To strengthen this dynamic and account for potentially incorrect matches, we additionally incorporate relation net R Ψ features to refine the supervoxel matching matrix M . Formally, we can define the matching update for a single entry m j,j ∈ M [t,t+1] as follows:The updated matching is the supervoxel with the highest matching probability, i.e., mj,j = arg max Sj ∈Yt+1 p(S j |S j , M [t,t+1] ). We can then additionally regularize the graph propagation (Eq. 1) using the updated matching matrix for the merged supervoxel sets Ŝ = S t ∪ S t+1 :where ψ u describes the probability of S j being assigned label y j based on the prediction f j = F Θ (S j ). ψ p describes the pairwise similarity between two supervoxels additionally based on r j = R Ψ (S j ), mean supervoxel color c j , and mean coordinate p j ."
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,4,Experiments,"Dataset Description. All experiments are carried out on an existing dataset of 18 laparoscopic surgeries [2,3]. The full dataset consists of RGB-D video acquisitions from four co-registered ceiling-mounted Azure Kinect cameras, containing 582,000 images per camera, with video annotations for 8 surgical workflow phases. We uniformly split a subset of the fused point cloud data into training and validation sets, ensuring similar distributions of non-overlapping surgeries, camera calibrations, and class distributions. We use 1500 point clouds for training and 1077 for validation. We additionally manually annotate these point clouds with segmentation labels covering 12 medically relevant classes.Each training split contains around 500 sparsely annotated point clouds, with 5436 click annotations on average per split. The densely labeled validation annotations comprise approximately 93% of the on average one million points per point cloud. To reduce the bias from a subjective annotation, we employed four different data annotators for a total annotation time of ∼115 h. To measure the robustness of our method, we split the train and validation sets into 3 nonoverlapping splits, referring to this as ""cross-validation"" despite the train and validation splits having different types of labels (sparse click and dense, respectively). For more details on data annotation and splits, as well as qualitative examples, please refer to the supplementary materials.Experimental Setup. In contrast to OTOC [19], which relies on additional ground truth instance segmentation as input [8], we use a heuristic oversegmentation approach to generate supervoxels [17]. All experiments use the same hyperparameters unless otherwise noted. The relation and feature networks were pre-trained on ScanNet [8]. We then fine-tune on our dataset for four iterations, with pseudo-label propagation occurring after each iteration. We report standardized segmentation metrics. The average training time on an NVIDIA A40 GPU was 7.45 h, using PyTorch 1.13.1 with CUDA 11.7."
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,5,Results and Discussion,"Experiment 1: Baseline Comparisons and Color Ablation. To quantify the overall impact of temporal guidance in the weakly-supervised setting, we compare SegmentOR with the baseline OTOC [19] over three-fold crossvalidation. As color information may be unavailable in OR datasets due to privacy concerns, we perform additional experiments without RGB features to contextualize the performance in such a setting. Furthermore, we explore how different unsupervised priors can improve semantic predictions, namely (i) nearest neighbor matching, (ii) optical flow matching, and (iii) scene flow matching. For each prior, the matching matrix M [t,t+1] is then initialized based on obtained  flow. Interestingly, initializing the matching matrix with the nearest neighbor prior outperformed optical and scene flow initialization. Using any single prior resulted in improvements over OTOC+T. We refer to the supplementary materials for a detailed description and ablation of the flow priors.Results: Table 1 shows that SegmentOR consistently outperforms the baseline both with and without RGB features when initialized with a nearest neighbor temporal prior and a learned update according to Eq. 2. The addition of RGB features marginally impacts the baseline, with a difference of only 0.9% mIoU. Without color, SegmentOR outperforms OTOC by ∼6% mIoU, making it more suitable for privacy-constrained setups. This improves to ∼10% for colored point clouds, indicating that the proposed learned temporal matching can leverage color similarities across time steps more effectively. The most significantly moving entities in ORs are surgical staff, who tend to wear gowns with specific and consistent colors. The presence of color features could influence the ability to distinguish humans from other objects more consistently (Fig. 2). This is supported by a segmentation mIoU increase of over 15% concerning the human class for SegmentOR (see supplementary for all class distributions).Experiment 2: Number of Clicks. A model's performance should theoretically increase with the level of supervision. We thus quantify the impact of adding up to three additional clicks on OTOC's performance. This experiment is performed on the first training and validation split, using a varying number of click annotations per class. Results: Increasing annotations by three or four times lead to an improvement of approximately 8% mIoU (see Table 2). The performance saturates with three clicks. Notably, the baseline does not achieve the 73% mIoU of the proposed method, even with increased supervision. This could suggest that temporal consistency not only increases the supervision signal but enables a more robust overall feature representation.Experiment 3: Application to Surgical Phase Recognition. To further assess the quality of our semantic predictions, we evaluate their impact on surgical workflow analysis (see Table 3). We use a ResNet50 [11] backbone and perform four-fold cross-validation using random splits over different surgeries of the complete, larger RGB-D video dataset. We use our best-performing segmentation model to infer the semantic predictions for each of the 582k fused point clouds (inference @12.5 fps), projecting them back into the two cameras. We then compare the performance of raw RGB, depth, and semantic labels inputs against fusing the latter two via late fusion [29]. Results: Consistent with previous works [2,29], RGB achieves the best performance across both cameras. Semantic predictions alone yield a performance well below RGB or depth. However, the fact that noisy semantic predictions from our network (achieving 73% mIoU) can be used for this challenging task demonstrates the benefits of our segmentation outputs for surgical scene understanding. Furthermore, when combined with depth through late fusion, results are improved by nearly 6% and 0.6% accuracy over depth for the surgical and workflow cameras, respectively. This suggests that segmentation maps could substitute RGB features when unavailable due to privacy reasons [2,27]."
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,,Conclusion.,"This work presents a novel semantic segmentation method for surgical scene understanding that significantly reduces the annotation burden by leveraging the temporal consistency of point cloud sequences. We demonstrate the effectiveness of our approach on point clouds from a surgical phase recognition dataset, which we enrich with manual 3D annotations. By incorporating self-supervised temporal priors, our method achieves a high segmentation mIoU of 73.10% using only 0.005% of annotated points. Furthermore, we establish a formal link between semantic segmentation and workflow analysis by demonstrating that our semantic predictions benefit downstream surgical phase recognition methods. Finally, we release all anonymized point clouds, annotations, and code used to ease the deployment of context-aware systems in surgical environments."
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,,Fig. 1 .,
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,,Fig. 2 .,
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,,Table 1 .,
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,,99 ± 0.19 82.77 ± 0.13 83.25 ± 0.71 83.31 ± 1.08 83.25 ± 0.71,
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,,Table 2 .,
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,,Table 3 . Downstream Surgical Phase Recognition.,
SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 6.
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,1,Introduction,"Tracking of tissue and organs in surgical stereo endoscopy is essential to enable downstream tasks in image guidance [8], surgical perception [5,13], motion compensation [17], and colonoscopy coverage estimation [28]. Given the difficulty in creating labelled training data, we train an unsupervised model that can estimate motion for anything in the surgical field: tissue, gauze, clips, instruments. Recent models for estimating deformation either use classical features and an underlying model (splines, embedded deformation, etc. [11,21]), or neural networks (eg. CNNs [24] or 2D graph neural networks (GNNs) [19]). The issue with 2D methods is that downstream applications often require depth. For example a correct physical 3D is needed location to enable augmented reality image guidance, motion compensation, and robotic automation (eg. suturing). For our model, SENDD, we extend a sparse neural interpolation paradigm [19] to simultaneously perform depth estimation and 3D flow estimation. This allows us to estimate depth and 3D flow all with one network rather than having to separately estimate dense depth maps. With our approach, SENDD computes motion directly in 3D space, and parameterizes a 3D flow field that estimates the motion of any point in the field of view.We design SENDD to use few parameters (low memory cost) and to scale with the number of points to be tracked (adaptive to different applications). To avoid having to operate a 3D convolution over an entire volume, we use GNNs instead of CNNs. This allows applications to tune how much computation to use by using more/fewer points. SENDD is trained end-to-end. This includes the detection, description, refinement, depth estimation, and 3D flow steps. SENDD can perform frame-to-frame tracking and 3D scene flow estimation, but it could also be used as a more robust data association term for SLAM. As will be shown in Sect. 2, unlike in prior work, our proposed approach combines feature detection, depth estimation and deformation modeling for scene flow all in one. After providing relevant background for tissue tracking, we will describe SENDD, quantify it with a new IR-labelled tissue dataset, and finally demonstrate SENDD's efficiency. The main novelties are that it is both 3D and Efficient by: estimating scene flow anywhere in 3D space by using a GNN on salient points (3D), and reusing salient keypoints to calculate both sparse depth and flow at anywhere (Efficient)."
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,2,Background and Related Work,"Different components are necessary to enable tissue tracking in surgery: feature detection, depth estimation, deformation modelling, and deformable Simultaneous Localization and Mapping (SLAM). Our model acts as the feature detection, depth estimation, and deformation model for scene flow, all in one.Recently, SuperPoint [3] features have been applied to endoscopy [1]. These are trained using loss on image pairs that are warped with homographies. In SENDD we use similar detections, but use a photometric reconstruction loss instead. SuperGlue [18] is a GNN method that can be used on top of SuperPoint to filter outliers, but it does not enable estimation of flow at non-keypoint locations, in addition to taking ∼270 ms for 2048 keypoints. Its GNN differs in that we use k-NN connected graph rather than a fully connected one. In stereo depth estimation, BDIS [22] introduces efficient improvements on classical methods, running in ∼70 ms for images of size (1280, 720). For flow estimation, there are the CNN-based RAFT [24], and RAFT3D [25] (45M params ∼386 ms) which downsample by 8x and require computation over full images. KINFlow [19] estimates motion using a GNN, but only in 2D. For SLAM in endoscopy, MIS-SLAM [21] uses classical descriptors and models for deformation. More recent work still does this, mainly due to the high cost of using CNNs when only a few points are actually salient. Specifically, Endo-Depth-and-Motion [16] performs SLAM in rigid scenes. DefSLAM and SD-DefSLAM [6,9] both use meshes along with ORB features or Lucas-Kanade optical flow, respectively, for data association. Lamarca et al. [10] track surfels using photometric error, but they do not have an underlying interpolation model for estimating motion between surfels. SuPer [11], and its extensions [12,13] use classical embedded deformation for motion modelling (∼500 ms/frame). Finally, for full scene and deformation estimation, NERF [15] has recently been applied to endoscopy [26], but requires per-scene training and computationally expensive. With SENDD, we fill the gap between classical and learned models by providing a flexible and efficient deformation model that could be integrated into SuPer [11], SLAM, or used for short-term tracking."
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,3,Methods,"The SENDD model is described in Fig. 1. SENDD improves on prior sparse interpolation methods by designing a solution that: learns a detector as part of the network, uses detected points to evaluate depth in a sparse manner, and uses a sparse interpolation paradigm to evaluate motion in 3D instead of 2D. SENDD consists of two key parts, with the first being the 1D stereo interpolation (Depth), and the second being the 3D flow estimation (Deformation). Both use GNNs to estimate query disparity/motion using a coordinate-based Multi Layer Perceptron (MLP) (aka. implicit functions [23]) of keypoints neighboring it. Before either interpolation step, SENDD detects and matches keypoints. After detailing the detection step, we will explain the 3D flow and disparity models. For the figures in this paper, we densely sample query points on a grid to show dense visualizations, but in practice the query points can be user or application defined. Please see the supplementary video for examples of SENDD tracking tissue points.Learned Detector: Like SuperPoint [3], we detect points across an image, but we use fewer layers and estimate a single location for each (32, 32) region (instead of (8, 8)). Our images (I t l , I t r ) are of size (1280, 1024), so we have N = 1280 detected points per frame. Unlike SuperPoint or others in the surgical space [1], we do not rely on data augmentation for training, and instead allow the downstream loss metric to optimize detections to best reduce the final photometric reconstruction error. Thus the detections are directly trained on real-world deformations. Since we have a sparse regime, we use a softmax dot product score in training, and use max in inference. See Fig. 1 for examples of detections.3D Flow Network: SENDD estimates the 3D flow d 3D (q) ∈ R 3 for each query q ∈ R 2 using two images (I t l , I t+1 l ), and their depth maps (D t , D t+1 ). For a means to query depth at arbitrary points, see the section on sparse depth estimation. To obtain initial matches, we match the detected points in 2D from frame I t l to I t+1 l . We use ReTRo keypoints [20] as descriptors by training the ReTRo network paired with SENDD's learned detector. We do this to remain as lightweight as possible. Matches further than 256 pixels away are filtered out. This results in N pairs of points in 2D with positions p 2D i , p 2D i and feature descriptors f i , f i , {i ∈ 1, . . . , N}, f i ∈ R c . These pairs are nodes in our graph, G. Given the set of preliminary matches, we can then get their 3D positions by using our sparse depth interpolation and backprojecting using the camera parameters at each pointThe graph, G, is defined with edges connecting each node to its k-nearest neighbors (k-NN) in 3D, with an optional dilation to enable a wider field of view at low cost. The positions and features of each correspondence are combined to act as features in this graph. For each node (detected point in image I t l ), its feature is:denotes a positional encoding layer [23], and γ * are linear layers followed by a ReLU, where different subscripts denote different weights. b i are used as features in our graph attention network, with the bold version defined as the set of all node features b = {b i |i ∈ N }. By using a graphattention neural network (GNN), as described in [19], we can refine this graph in 3D to estimate refined offsets and higher level local-neighborhood features. b = Ga (Ga (Ga (Ga (b, G)))), is the final set of node features that are functions of their neighborhood, N , in the graph. Ga is the graph attention operation. In practice, for each layer we use dilations of [1,8,8,1], and k = 4. The prior steps only need to run once per image pair. The motion d 3D (q) of each query point q is a function of the nearby nodes in 3D, with G q denoting a k-clique graph of the query point and its k -1 nearest nodes; d 3D (q) ∈ R 3 = Lin 3D Ga {q} {b N }, G q . Lin 3D is a linear layer that converts from c channels to 3.Sparse Depth Interpolation: Instead of running a depth CNN in parallel, we estimate disparity sparsely as needed by using the same feature points with another lightweight GNN. We do this as we found CNNs (eg. GANet [27]) too expensive in terms of training and inference time. We adapt the 3D GNN interpolation methodology to estimate 1D flow along epipolar lines in the same way that we modified a 2D sparse flow model to work in 3D. First, matches are found along epipolar lines between left and right images (I t r , I t l ). Then matches are refined and query points are interpolated using a GNN. a, a are the 1D equivalents of b, b from the 3D flow network. For the refined node features, a ∈ R 1 = Ga (Ga (Ga (Ga (a, G)))), and the final disparity estimate, d disp (q) ∈ R 1 = Lin 1D (Ga ({q} {a N } , G q )). Lin 1D is a linear layer that converts from c channels to 1. This can be seen like a neural version of the classic libELAS [4], where libELAS uses sparse support points on a regular grid along Sobel edge features to match points within a search space defined by the Delauney triangulation of support point matches.Loss: We train SENDD using loss on the warped stereo and flow images: L p (A, B) is a photometric loss function of input images (A, B) that is used for both the warped depth pairs (L p (I l , I r→l )) and warped flow pairs (L p (I t , I t+1→t )), L s (V ) is a smoothness loss on a flow field V , and c denotes image color channels [7]. These loss functions are used for both the warped depth (I l , I r→l ) and the flow reconstruction pairs (I t , I t+1→t ). F and D are densely queried images of flow and depth. We add a depth matching loss, L d which encourages the warped depth map to be close to the estimated depth map.We set α = 0.85, β = 150, λ d = 0.001, λ F = 0.01, λ D = 1.0, using values similar to [7], and weighting depth matching lightly as a guiding term."
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,4,Experiments,"We train SENDD with a set of rectified stereo videos from porcine clinical labs collected with a da Vinci Xi surgical system. We randomly select images and skip between (1, 45) frames for each training pair. We train using PyTorch with a batch size of 2 for 100,000 steps using a one-cycle learning rate schedule with maxlr = 1e-4, minlr = 4e-6. We use 64 channels for all graph attention operations.Dataset: In order to quantify our results, we generate a new dataset. The primary motivation for this is to have metrics that are not dependent on visible markers or require human labelling (which can bias to salient points). Some other datasets have points that are hand labelled in software [2,11] or use visible markers [12]. Others rely on RMSE of depth maps as a proxy for error, but this does not account for tissue sliding. Our dataset uses tattooed points that flouresce  under infrared (IR) light by using ICG dye. For each ground truth clip, we capture an IR frame at the start, record a deformation action, and then capture another IR frame. The start and end IR segments are then used as ground truth for quantification. Points that are not visible at both the start and finish are then manually removed. See Fig. 2 for examples of data points at start and end frames. The dataset includes multiple different tissue types in ex vivo, including: stomach, kidney, liver, intestine, tongue, heart, chicken breast, pork chop, etc. Additionally the dataset includes recordings from four different in vivo porcine labs. This dataset will be publicly released separately before MICCAI. No clips from this labelled dataset are used in training.Quantification: Instead of using Intersection Over Union (IOU) which would fail on small segments (eg. IOU = 0 for a one pixel measurement that is one pixel off), we use endpoint error and chamfer distance between segments to quantify SENDD's performance. We compute endpoint error between the centers of each segmentation region. We use chamfer distance as well because it allows us to see error for regions that are larger or non-circular. Chamfer distance provides a metric that has an lower bound on error, in that if true error is zero then this will also be zero.Experiments: The primary motivation for creating a 3D model (SENDD) instead of 2D is that it enables applications that require understanding of motion in 3D (automation, image guidance models). That said, we still compare to the 2D version to compare performance in pixel space. To quantify performance on clips, since these models only estimate motion for frame pairs, we run SENDD for each frame pair over n frames in a clip, where n is the clip length. No relocalization or drift prevention is incorporated (the supplementary material shows endpoint error on strides other than one). For all experiments we select clips with length less than 10 s, as we are looking to quantify short term tracking methods.The 2D model has the exact same parameters and model structure as SENDD (3D), except it does not do any depth map calculation, and only runs in image space. First, we compare SENDD to the equivalent 2D model. We do this with endpoint error as seen in Fig. 3, and show that the 3D method outperforms 2D over the full dataset. Additionally, we compare to baselines of CSRT [14] which is a high performer in the SurgT challenge [2] and RAFT [24]. To track with CSRT and RAFT, we track in each left and right frame and backproject as done in SurgT [2]. SENDD outperforms these methods used off-the-shelf; see Fig. 3. Performance of the RAFT and CSRT methods could likely be improved with a drift correction, or synchronization between left and right to prevent depth from becoming more erroneous. Then we compare chamfer distance. The 3D method also outperforms the 2D method, on 64 randomly selected clips, with a total average error of 19.18 px vs 20.88 px. The reasons for the SENDD model being close in performance to the equivalent 2D model could be that the 3D method uses the same amount of channels to detect and describe the same features that will be used for both depth and flow. Additionally, lens smudges or specularities can corrupt the depth map, leading to errors in the 3D model that the purely photometric 2D model might not encounter. In 3D all it takes is for one camera artifact to in either image to obscure the depth map, and the resulting flow. The 2D method decreases the likelihood of this happening as it only uses one image. Finally, we compare performance in terms of endpoint error of our 3D model on in vivo vs ex vivo labelled data. As is shown in Fig. 4, the in vivo experiments have a more monotonic performance decrease relative to clip length. Actions in the ex vivo dataset were performed solely to evaluate tracking performance, while the in vivo data was collected alongside training while performing standard surgical procedures (eg. cholecystectomy). Thus the in vivo scenes can have more complicated occlusions or artifacts that SENDD does not account for, even though it is also trained on in vivo data.Benchmarking and Model Size: SENDD has only 366,195 parameters, compared to other models which estimate just flow (RAFT-s [24] with 1.0M params.) or depth (GANet [27] with 0.5M params.) using CNNs. We benchmark SENDD on a NVIDIA Quadro RTX 4000, with a batch size of one, 1280 query points, and 1280 control points (keypoints). As seen in Table 1, the stereo estimation for each frame takes 21.4 ms, and total time for the whole network to estimate stereo (at both t and t + 1) and flow is 145.4 ms. When estimating flow for an image pair, we need to calculate two sets of keypoint features, but for a video, we can reuse the features from the previous frame instead of recalculating the full pair each time. Results with reuse are exactly the same, only timing differs. Subtracting out the time for operations that do not need to be repeated leaves us with a streaming time of 97.3 ms, with a frame rate of 10fps for the entire flow and depth model. This can be further improved by using spatial data structures for nearest neighbor lookup or PyTorch optimizations that have not been enabled (eg. float16). The number of salient (or query) points can also be changed to adjust refinement (or neural interpolation) time."
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,5,Conclusion,"SENDD is a flexible model for estimating deformation in 3D. A limitation of SENDD is that it is unable to cope with occlusion or relocalization, and like all methods is vulnerable to drift over long periods. These could be amended by integrating a SLAM system. We demonstrate that SENDD performs better than the equivalent sparse 2D model while additionally enabling parameterization of deformation in 3D space, learned detection, and depth estimation. SENDD enables real-time applications that can rely on tissue tracking in surgery, such as long term tracking, or deformation estimation."
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,,Fig. 1 .,
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,,Fig. 2 .,
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,,Fig. 4 .,
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,,,
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,,Table 1 .,
SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_23.
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,1,Introduction,
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,1.1,Background,"Traditional optical imaging samples the visual spectrum in three diffuse spectral bands (RGB), while hyperspectral imaging (HSI) provides much more detailed spectral information. This information is potentially valuable for making intraoperative decisions, particularly in cases where tissue differentiation is critical but challenging to perform using traditional visualisation techniques. In the case of brain tumour excision, fluorescence-guided resection is commonly used to minimize damage to healthy tissue [2] but is limited to high-grade gliomas, and results in added cost and workflow disruptions. Thanks to a more detailed definition between tissue types [5], HSI is seen as a promising alternative with wider applicability and smoother integration into the workflow.While HSI has been integrated into surgical microscope systems [11], it is suggested that handheld systems are better suited to translational research [4]. Such handheld systems consist of an exoscope coupled to a draped optical stack, as shown in Fig. 1. The optics in the exoscope typically result in a short focal depth, making manual focusing tricky, particularly as the tuning must be performed through the drape. As such, these systems are commonly left at a fixed focal power and the surgeon must keep the working distance fixed to keep the subject in focus. Furthermore, the narrow spectral bands of HSI sensors reduce the amount of light collected [12]. To avoid increasing exposure time, a large aperture size is needed, at a cost of further reducing focal depth. This exacerbates the focusing issues, making current real-time handheld HSI imaging systems particularly challenging to focus, posing significant usability issues. Figure 1 highlights the limited focal depth of our system, and shows a typical target that the surgeon must manually bring into focus during surgery.The issue of reduced focal depth in real-time HSI systems could be mitigated by the introduction of a video autofocus system. Autofocus methods are divided into active methods, which use transmission to probe the scene, and passive methods, which rely only on incoming light. Passive methods are further split into phase-based, which require specialised hardware, and contrast-based, which compare images captured at different focal powers. Our investigation focuses on contrast-based methods, which require minimal hardware development. "
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,1.2,Related Autofocusing Works,"While autofocusing systems are prevalent in consumer device, the scientific literature is sparse, especially for dynamic video autofocusing. Many publications in the field are concerned with benchtop microscope autofocus systems [7,8,15]. This environment is conducive to autofocus as the scene is typically static with a single focal plane across the whole image. Additionally, the focus can be adjusted easily by moving the stage vertically. [8] take a traditional approach, making use of a Laplacian focal metric combined with a modified hill-climber optimisation scheme. [15] input a stack of sequential images to a 3d convolutional neural network (CNN) trained as a deep reinforcement agent trained to output changes in stage height. [7] train a CNN to regress the optimal focal power from just two images taken at different focal powers. Beyond benchtop microscopy, [6] also use a CNN to directly regress optimal focal powers, this time from varying number of samples from the full focal stacks. [1] take a novel approach by using pre-trained object detection models to generate latent vector representations of images and using these as inputs to a deep reinforcement agent. [14] train two CNNs, one to regress focal steps from a single image, the other to determine if the current image is in focus."
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,1.3,Contributions,"This work aims to improve intraoperative handheld HSI systems by alleviating one of their main usability drawbacks, that of shortened focal depth. We introduce an autofocus system to an existing handheld intraoperative real-time HSI system [4]. The focus adjustments are handled by a focus tunable liquid lens which is integrated into the setup. We propose autofocusing policies based on deep reinforcement learning and compare these to traditional heuristic approaches. Our final model is similar to that presented in [15] but differs in its use of a weight shared image encoder, software simulated defocusing for training data, and small input patch size. In addition, our method is designed and trained to handle dynamic environments, something entirely missing in the literature. We performed a robotic focal-time scan to create a reproducible testing benchmark and allow quantitative comparison of autofocus policies. Finally, we demonstrate the utility of our approach in a blinded user study involving two neurosurgeons."
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,2,Materials and Methods,
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,2.1,Optical System,"Our intraoperative HSI system, shown in Fig. 2, builds on our existing system [4] by integrating an Optotune EL-10-30-Ci focus-tunable liquid lens to allow electrical control of the focal length. The hyperspectral camera is based on an IMEC 2/3"" snapshot mosaic CMV2K-SSM4X4-VIS sensor, which acquires 16 spectral bands in a 4 × 4 mosaic between the spectral range of 460 nm and 600 nm. With a sensor resolution of 2048 × 1088 pixels, hyperspectral data is acquired with a spatial resolution of 512 × 272 pixels per spectral band. Video-rate imaging of snapshot data is achieved with a speed of up to 50 FPS depending on acquisition parameters."
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,2.2,Datasets,"Software Simulated Focal-Time Scans. We define a focal-time scan as a time series of focal stacks, with a focal stack being a single image captured at multiple focal lengths. In order to assemble a large and diverse focal-time scan dataset, we choose to simulate focal-time scans using existing in-focus video data. To ensure the resulting focal-time scan features diverse camera motion, we implement a smooth random walk to step a cropping rectangle across the video after each frame. This also allows for the construction of plausible focaltime scans from single images, although features such as dynamic subjects or imaging noise will be missing. In order to simulate defocus, we implement another random walk to simulate a dynamic optimal focal power. When an agent is interacting with the simulated scan, a Gaussian filter is used to approximate focal blurring with σ = σ 0 |f *f | where f and f * are the current and optimal focal powers and σ 0 is chosen randomly from the range 2-8 for each scan. We use this technique to create a training and testing dataset consisting of 1000 and 200 simulated focal-time scans based on 200 10-second video clips sampled from Cholec80 [13], a popular endoscopic dataset. In addition, we created simulated focal-time scans from 200 in focus images taken of a brain phantom with our HSI system. These act as a validation dataset to help prevent over fitting and aid generalisation. While Gaussian blur is a reasonable approximation, we note that more rigours methods exist to simulate defocus blur that may produce better simulated data [9].Robotic Focal-Time Scan. As a testing dataset similar to our intended use case, we chose to approximate a real focal-time scan by controlling conditions during capture of the individual focal stacks. Our optical system was fixed to a robotic arm, which was then used in a compliant control mode to record a natural hand-guided trajectory whilst imaging a brain phantom. The motion was performed to try to emulate typical usage during a surgery, whilst also trying to cover the range of plausible working distances. The focal range of the liquid lens is discretised into a set of focal powers, and the recorded trajectory is discretised into a sequence of 1184 poses. For each discrete pose, the robotic arm is fixed, and an image captured for each focal power. We randomise the order of the focal powers to reduce systematic bias caused by the response of the liquid lens. Auto-exposure was implemented in order to ensure good exposure across all working distances. To ensure consistency within a given focal stack, autoexposure was only stepped in-between discrete poses. The robotic arm holding our optical system and a sample of the resulting focal-time scan can be seen in Fig. 3. The optimal focus for all focal stacks was computed via global search of a traditional focal metric (mean gradient magnitude) as detailed below. This was then validated visually and corrected where appropriate.Integration and Usability Trial. To ensure the validity of our quantitative evaluation, and to get feedback on the system in general, a blinded trial was set up with two practising neurosurgeons. A set was made containing two repeats of three selected autofocus policies. This set was then shuffled, and the surgeons remained blinded to the autofocus policy until after the trial. Each surgeon used our optical system to inspect a brain phantom with each policy in the set. The surgeon was made aware when the policy was changed and prompted to make comments throughout the trial, which were recorded."
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,2.3,Autofocus Policies,"As seen in Fig. 1, the area of surgical interest can make up a rather small amount of the overall image, as such, we limit ourselves to a patch size of just 32 × 32 pixels. The positioning of the patch could be dictated by a second algorithm or user input, but this is outside the scope of this work. Here, we simply position the patch at the centre of the circular content area, which is detected using the method presented in [3]. All of our autofocus policies deal with the grayscale reconstruction of the HSI images. Throughout this work, we further deal with a normalised focal power range (0-1).Traditional Approach. We implement two traditional autofocus policy based on different focal metrics combined with a simple hill-climber optimisation policy. We choose mean gradient magnitude (MGM) and mean local ratio (MLR). Two focal metrics which are conceptually simple but competitive [6] and implemented in quite different ways. They are defined aswhere p is the set of all pixels in the image, I x and I y are defined as the x and y responses of a Sobel filter, and G σ is a Gaussian blur. The kernel size is chosen as σ = 4 for all our experiments. Our hill-climber optimisation policy O HC sets the focal power f at time t + 1 based on information at time t and is defined aswhere) is the direction of the previous step and h is a step size which we set to h = 0.05 for all our experiments. We note that our definition is different from standard hill-climber. A normal hill-climber will repeat a step while the focal metric is increasing, and either stop or change direction with a smaller step size when the focal metric decreases, but this does not translate to a continuous and dynamic environment.Learned Optimisation Policy. Due to our dynamic environment, it seems likely that considering a sequence of the N last focal metrics, rather than the last two, would help to build a strong optimisation policy. However, as N increases, it quickly becomes unclear how to incorporate this information effectively. It is likely that a learning based solution would uncover a better strategy than heuristic approaches. While regression based approcahes may work, reinforcement learning provides a natural framework for this problem by allowing the policy to model the trade-off between maximisation and exploration. By modelling the autofocus task as a Markov process, we can define a Q-function Q(s, a) which maps state-action pairs to expected future rewards. We define our state, actions, and reward function aswhere f * t is the optimal focal power at t which can only be known in controlled environment. As before, we take h = 0.05. Our learned optimisation policy O RL can then be defined asTo model Q(s, a), we use an MLP consisting of 2 hidden layers of 256 ReLUs each and a third layer with 3 outputs corresponding to the 3 possible actions. The MLP takes as input the state vector s containing the N most recent focal metrics and focal powers, we take N = 8 for all our experiments. To train the model, we use Deep Q Learning following the recommendations set out by the DQN method [10] to improve training stability. We use an experience memory with size 2.5 × 10 6 , and an -greedy exploration policy where exponentially decays from 1.0 to 0.1 over the first 2 × 10 6 experiences. Our target model is updated with exponential moving average (EMA) weight updates with a β = 0.005, and we use γ = 0.99 in our Bellman equation. Finally, we use a smoothed L1 loss function and optimise with RMSProp with learning rate 1×10 -5 and momentum 0.95. We trained on our software simulated focal-time scans created from real endoscopy videos and validated against our simulated focal-time scans created with HSI images taken with our optical system mounted on a robotic arm.End-to-End Model. In addition to learning the optimisation policy, we can also learn the focal metric. By learning the two together, we are no longer constrained to a scalar metric and can instead learn a latent vector encoding of the image patches. To do this, we construct a CNN consisting of 4 convolutions with 8 filters each and a stride of 2, outputting a vector of 8 logits for our patch size of 32 × 32. The CNN is run on each of the N most recent image patches as a batch during training, but only the most recent during inference, with the previous encodings stored between steps. The encodings are concatenated with the N most recent focal powers and fed into an MLP. The MLP and training procedure are the same as before."
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,3,Results,"We evaluated each autofocus policies on both our simulated focal-time scan test set, and the robotically recorded focal-time scan. The mean focal errors are shown in Table 1. The scores show an improvement in almost all cases by the introduction of a learned optimiser. The paths taken and the focal error over time for the robotic focal-time scan for a selection of policies are plotted in Fig. 4.During the usability trial, the surgeon participants were positive about all presented policies. In line with our quantitative results, the participants both showed preference for the CNN-based policy. It was thought by both to be smoother and more deliberate in its adjustments, and felt more robust to minor accidental motions inherent to hand-operated system. One commented that it felt slower to focus but more stable, going on to state that this was desirable behaviour. All algorithms handled the brain fissure well, this is likely due to the small patch size used, allowing for precise targeting. Overall, the surgeons were very positive about the integration of autofocus into optical imaging systems.  "
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,4,Conclusion,"We have successfully designed a handheld intraoperative HSI imaging system with autofocusing capability. We developed a novel CNN-based autofocus policy suitable for video data. In addition, we performed a robotic focal-time scan to evaluate our methods. Our novel method significantly outperforms a traditional baseline on our robotic focal-time scan, and performs preferably in a usability trial by two neurosurgeons. The comments from the usability trial also suggest that the dynamic video autofocusing systems will be well received among surgeons."
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,,Fig. 1 .,
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,,Fig. 2 .,
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,,Fig. 3 .,
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,,Fig. 4 .,
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,,Table 1 .,
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 63.
Surgical Video Captioning with Mutual-Modal Concept Alignment,1,Introduction,"Automatic surgical video captioning is critical to understanding the surgery with complicated operations, and can produce the natural language description with given surgical videos [24,26]. In this way, these techniques can reduce the workload of surgeons with multiple applications, such as providing the intraoperative surgical guidance [17], generating the post-operative surgical report [4], and even training junior surgeons [8].To generate text descriptions from input videos, existing captioning works [5,9,21,24,26] mostly consist of a visual encoder for visual representations and a text decoder for text generation. Some early works [9,11,21,23] adopted a fixed object detector as the visual encoder to capture object representations for text decoding. This paradigm in Fig. 1(a) requires auxiliary annotations (e.g., bounding box) to pre-train the visual encoder, and cannot adequately train the entire network for captioning. To improve performance with high efficiency in practice, recent works [13,24,25] followed the detector-free strategy, and opened up the joint optimization of visual encoder and text decoder towards captioning, as shown in Fig. 1(b). Despite great progress in this field, these works can be further improved with two limitations of surgical video captioning.First, existing surgical captioning works [23,24,26] did not fully consider the inherent patterns of surgery to facilitate captioning. Due to the variability of lesions and surgical operations, surgical videos contain complex visual contents, and thus it is difficult to directly learn the mapping from the visual input to the text output. In fact, the same type of surgery has relatively fixed semantic patterns, such as using specific surgical instruments for a certain surgical action. Therefore, we introduce the surgical concepts (e.g., surgical instruments, operated targets and surgical actions) from a semantic perspective, and guide the surgical captioning network to perceive these surgical concepts in the input video to generate more accurate surgical descriptions. Second, existing studies [9,24,26] simply processed visual and text modalities in sequential, while ignoring the semantic gap between these two modalities. This restricts the integration of visual and text modality knowledge, thereby damaging the captioning performance. Considering that both visual and text modalities revolve around the same set of surgical concepts, we aim to align the features in the visual and text modalities with each other through surgical concepts, and achieve more efficient multi-modal fusion for accurate text predictions.To address these two limitations in surgical video captioning, we propose the Surgical Concept Alignment Network (SCA-Net) to bridge the visual and text modalities through the surgical concepts, as illustrated in Fig. 1(c). Specifically, to enable the SCA-Net to accurately perceive surgical concepts, we first devise the Surgical Concept Learning (SCL) to predict the presence of surgical concepts with the representations of visual and text modalities, respectively. Moreover, to mitigate the semantic gap between visual and text modalities of captioning, we propose the Mutual-Modality Concept Alignment (MC-Align) to mutually coordinate the encoded features with surgical concept representations of the other modality. In this way, the proposed SCA-Net achieves the surgical concept alignment between visual and text modalities, thereby producing more accurate captions with aligned multi-modal knowledge. To the best of our knowledge, this work represents the first effort to introduce the surgical concepts for the surgical video captioning. Extensive experiments are performed on neurosurgery video and nephrectomy image datasets, and demonstrate the effectiveness of our SCA-Net by remarkably outperforming the state-of-the-art captioning works."
Surgical Video Captioning with Mutual-Modal Concept Alignment,2,Surgical Concept Alignment Network,
Surgical Video Captioning with Mutual-Modal Concept Alignment,2.1,Overview of SCA-Net,"As illustrated in Fig. 2, the Surgical Concept Alignment Network (SCA-Net) follows the advanced captioning architecture [25], and consists of visual and text encoders, and a multi-modal decoder. We implement the visual encoder with VideoSwin [15] to capture the discriminative spatial and temporal representations from input videos, and utilize the Vision Transformer (ViT) [7] with causal mask [6] as the text encoder to exploit text semantics with merely previous text tokens. The multi-modal decoder with ViT structure takes both visual and text tokens as input, and finally generates the caption of the input video. Moreover, to accurately perceive surgical concepts in SCL (Sect. 2.2), the SCA-Net learns from surgical concept labels using separate projection heads after the visual and text encoders. In the MC-Align (Sect. 2.3), the visual and text tokens from two encoders are mutually aligned with the concept representations of the other modality for better multi-modal decoding."
Surgical Video Captioning with Mutual-Modal Concept Alignment,2.2,Surgical Concept Learning,"Previous surgical captioning works [23,24,26] generated surgical descriptions directly from input surgical videos. Considering the variability of lesions and surgical operations, these methods may struggle to understand complex visual contents and generate erroneous surgical descriptions, thereby hindering performance to meet clinical requirements. In fact, both the surgical video and surgical caption represent the same surgical semantics in different modalities. Therefore, we decompose surgical operations into surgical concepts, and guide these two modalities to accurately perceive the presence of surgical concepts, so as to better complete this cross-modal task. Given a type of surgery, we regard the surgical instruments, surgical actions and the operated targets used in surgical videos as surgical concepts. Considering that both the visual input and the shifted text input contain the same set of surgical concepts, we find out which surgical concepts appear in the input surgical video by parsing the caption label. In this way, the presence of surgical concepts can be represented in a multi-hot surgical concept label y cpt ∈ {0, 1} C , where the surgical concepts that appear in the video are marked as 1 and the rest are marked as 0, and C is the number of possible surgical concepts. For example, the surgical video in Fig. 2 contains the instrument cutting forcep, the action removing and the target bone, and thus the surgical concept label y cpt represents these surgical concepts in corresponding dimensions.To guide the visual modality to perceive surgical concepts, we aggregate visual tokens generated by the visual encoder in average, and add a linear layer to predict the surgical concepts of input videos, where the normalized output p v ∈ [0, 1] C estimates the probability of each surgical concept. We perform the multi-label classification using binary sigmoid cross-entropy loss, as follows:In this way, the visual tokens are supervised to contain discriminative semantics related to valid surgical concepts, which can reduce prediction errors in surgical descriptions. For the text modality, we also perform SCL for surgical concept prediction p t c ∈ [0, 1] C and calculate the loss L t SCL in the same way. By optimizing L SCL = L v SCL + L t SCL , the SCL enables visual and text encoders to exploit multi-modal features with the perception of surgical concepts, thereby facilitating the SCA-Net towards the captioning task."
Surgical Video Captioning with Mutual-Modal Concept Alignment,2.3,Mutual-Modality Concept Alignment,"With the help of SCL in Sect. 2.2, our SCA-Net can perceive the shared set of surgical concepts in both visual and text modalities. However, given the differences between two modalities with separate encoders, it is inappropriate for the decoder to directly explore the cross-modal relationship between visual and text tokens [24,26]. To mitigate the semantic gap of two modalities, we devise the MC-Align to bridge these tokens in different modalities through surgical concept representations for better multi-modal decoding, as shown in Fig. 2.To align these two modalities, we first collect surgical concept representations for each modality. Note that text tokens are separable for surgical concepts, while visual tokens are part of the input video containing multiple surgical concepts. For text modality, we parse the label of each text token and average text tokens of each surgical concept as t c , and update the historical text concept representations { tc } C c=1 using Exponential Moving Average (EMA), as tc ← γ tc +(1-γ)t c , where the coefficient γ controls the updating for stable training and is empirically set as 0.9. For visual modality, we average visual tokens as the representation of each surgical concept present in the input video (i.e., v c if surgical concept label y cpt c = 1), and update the historical visual concept representations {v c } C c=1 with EMA, as vc ← γ vc + (1γ)v c . In this way, we obtain the text and visual concept representations with tailored strategies for the alignment.Then, we mutually align visual and text concept representations with corresponding historical ones in another modality. For visual-to-text alignment, visual concept representations are expected to be similar to corresponding text concept representations, while differing from other text concept representations as possible. Thus, we calculate the alignment objective L v→t MCA with regard to surgical concepts [10], and the visual encoder can be optimized with the gradients of visual concept representations in backward, thereby gradually aligning visual modality to text modality. Similarly, text concept representations are also aligned to the historical visual ones, as text-to-visual alignment L t→v MCA . The MC-Align is summarized as follows:where V and T denote all visual and text representations respectively, and • is the inner product of vectors. In this way, the MC-Align aligns visual and text representations with each other modality according to the surgical concept, thus benefiting multi-modal decoding for captioning. "
Surgical Video Captioning with Mutual-Modal Concept Alignment,2.4,Optimization,"For the surgical captioning task, we adopt standard captioning loss L Cap to optimize the cross-entropy of each predicted word based on previous words y <t and input video x, as follows:where T is the length of caption prediction. Overall, the final objective of SCA-Net is summarized as L = L Cap + λ 1 L SCL + λ 2 L MCA , where loss coefficients λ 1 and λ 2 control the trade-off of SCL and MC-Align. By optimizing this final objective L, the proposed SCA-Net can achieve multi-modal concept alignment, and generate superior descriptions for the surgical video captioning."
Surgical Video Captioning with Mutual-Modal Concept Alignment,3,Experiment,
Surgical Video Captioning with Mutual-Modal Concept Alignment,3.1,Dataset and Implementation Details,"Neurosurgery Video Captioning Dataset. To evaluate the effectiveness of surgical video captioning, we collect a large-scale dataset with 41 surgical videos of endonasal skull base neurosurgery. These surgical videos are recorded at the Prince of Wales Hospital, Chinese University of Hong Kong, where surgeons remove pituitary tumors through the endonasal corridor to the skull base. After necessary data cleaning, we divide these surgical videos with resolution of 1, 920× 1, 080 into 11, 004 thirty-second video clips with clear surgical purposes. These video clips are annotated under Tool-Tissue Interaction (TTI) principle [18], and include a total of 16 instruments, 8 targets, and 10 surgical actions. The annotation preprocessing follows [26] using NLTK [16] toolkit. The proportion of surgical concepts is illustrated in Fig. 3. We split these video clips at patientlevel, where the video clips of 31 patients are used for training and the rest of 10 patients are utilized for test.EndoVis Image Captioning Dataset. We further compare our method with state-of-the-arts on the public EndoVis-2018 Image Captioning Dataset [1,23]. This dataset reveals robotic nephrectomy procedures acquired by the da Vinci X or Xi system, and is annotated with surgical actions between 9 possible tools and surgical targets [23]. We follow the official split in [24] with 11 sequences for training and 3 sequences for test. In this way, these two datasets can comprehensively evaluate the captioning tasks under both surgical videos and images.Implementation Details. We implement our SCA-Net and state-of-the-art captioning methods [5,9,21,24,26] in PyTorch [20]. We optimize the SCA-Net and compared captioning methods using Adam with the batch size of 12 for both captioning datasets. All models are trained for 20 and 50 epochs in neurosurgery and EndoVis datasets, respectively. We adopt the step-wise learning rate decay strategy to facilitate training convergence, where the learning rate is initialized as 1 × 10 -2 and halved after every 5 epochs. The loss coefficients λ 1 of L SCL and λ 2 of of L MCA are empirically set to 0.1 and 0.01, respectively. All experiments are performed on a single NVIDIA A100 GPU.Evaluation Metrics. To evaluate the captioning performance, we adopt standard metrics, including BLEU@4 [19], METEOR [3], SPICE [2], ROUGE [12] and CIDEr [22]. Specifically, BLEU@4 [19] evaluates the 4-gram precision of the predicted caption, and CIDEr [22] is based on the n-gram similarity with TF-IDF weights. METEOR [3] considers both precision and recall. ROUGE [12] and SPICE [2] measure the matching between predictions and ground truth. The higher scores of these metrics indicate better performance in surgical captioning."
Surgical Video Captioning with Mutual-Modal Concept Alignment,3.2,Comparison on Neurosurgery Video Captioning,"To evaluate the performance of our SCA-Net, we perform a comprehensive comparison with the state-of-the-art captioning methods, including Self-Seq [21], AOANet [9], SIG-Former [26], M 2 Transformer [5], and SwinMLP-TranCAP [24].As illustrated in Table 1, our SCA-Net achieves the best performance, with the overwhelming BLEU@4 of 48.1%, METEOR of 35.1% and CIDEr of 368.4%. Noticeably, our SCA-Net outperforms the surgical captioning work, SwinMLP-TranCAP [24], by a large margin, e.g., 16.9% in SPICE and 13.0% in ROUGE. This advantage confirms that the proposed surgical concept alignment can alleviate the modalities gap in surgical captioning. Moreover, compared with the second-best M 2 Transformer [5] with meshed attention between the visual encoder and the text decoder, our SCA-Net obtains superior performance with a remarkable increase of 9.5% in SPICE and 7.1% in ROUGE. These experimental results demonstrate the performance advantage of our SCA-Net over state-of-the-arts in the neurosurgery video captioning.Ablation Study. To further validate the effectiveness of SCL and MC-Align, we perform the detailed ablation study in Table 1. Specifically, we implement three ablative baselines of the proposed SCA-Net, by removing the MC-Align (denoted as w/o MC-Align) and the SCL (denoted as w/o SCL) individually, as well as removing both (denoted as w/o SCL, MC-Align). As illustrated in Table 1, the proposed SCL and MC-Align can bring an individual improvement of 4.0% and 5.5% in BLEU@4, respectively, to the baseline of 40.3%. Furthermore, the SCL and MC-Align can work together to facilitate the captioning, with a BLEU@4 gain of 7.8%. These ablation experiments confirm that the proposed SCL and MC-Align play an important role in solving the modality gap in surgical video captioning, resulting in the performance advantage of our SCA-Net.Qualitative Analysis. We present qualitative results of our SCA-Net and state-of-the-arts [5,24] on neurosurgery video captioning. In Fig. 4(a), SwinMLP-TranCAP [24] and M 2 Transformer [5] incorrectly predict the operated targets and ignore important surgical instruments, respectively, and both methods [5,24] cannot recognize the rare instrument ultrasound probe as well as the corresponding surgical action in Fig. 4(b). With the help of surgical concept alignment, our SCA-Net can perceive the surgical concepts present in the surgical videos and thus generate correct descriptions in these two complex videos."
Surgical Video Captioning with Mutual-Modal Concept Alignment,3.3,Comparison on EndoVis Image Captioning,"To further confirm the effectiveness of surgical captioning, we perform the comparison on the public EndoVis image captioning dataset. As shown in Table 2, the end-to-end captioning methods [24,26] outperform the detector-based works using instrument bounding box as auxiliary annotations [9,21], by optimizing the visual encoder to meet the requirement of the captioning task. In particular, our SCA-Net with Swin Transformer [14] as visual encoder achieves the best performance of four metrics (e.g., 47.6% in BLEU@4 and 58.4% in SPICE), and outperforms the surgical state-of-the-art [24] with the advantage of 7.3% in BLEU@4 and 5.1% in METEOR. These comparisons confirm that our SCA-Net with surgical concept alignment can produce more accurate surgical captions. "
Surgical Video Captioning with Mutual-Modal Concept Alignment,4,Conclusion,"To achieve accurate surgical video captioning, we propose the SCA-Net to mitigate the semantic gap of visual and text modalities with surgical concepts. Specifically, we devise the SCL to enable the SCA-Net with the perception of surgical concepts in visual and text modalities, respectively. Moreover, we propose the MC-Align to mutually coordinate visual and text representations with surgical concept representations of the other modality for multi-modal decoding, thereby generating more accurate captions with aligned multi-modal knowledge.Extensive experiments on neurosurgery and nephrectomy datasets confirm the advantage of our SCA-Net over state-of-the-arts on the surgical captioning."
Surgical Video Captioning with Mutual-Modal Concept Alignment,,Fig. 1 .,
Surgical Video Captioning with Mutual-Modal Concept Alignment,,Fig. 2 .,
Surgical Video Captioning with Mutual-Modal Concept Alignment,,Fig. 3 .,
Surgical Video Captioning with Mutual-Modal Concept Alignment,,Fig. 4 .,
Surgical Video Captioning with Mutual-Modal Concept Alignment,,Table 1 .,
Surgical Video Captioning with Mutual-Modal Concept Alignment,,Table 2 .,
Surgical Video Captioning with Mutual-Modal Concept Alignment,,Acknowledgments,". This work is supported by National Key R&D Program of China under Grant No. 2021YFE0205700, National Natural Science Foundation of China (No. 62276260, 62076235, 62176254, 61976210, 62002356, 62006230), sponsored by Zhejiang Lab (No. 2021KH0AB07) and the InnoHK program."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,1,Introduction,"The recent evolution of large language models (LLMs) is revolutionizing natural language processing and their use across various sectors (e.g., academia, healthcare, business, and IT) and daily applications are being widely explored. In medical diagnosis, recent works [23] have also proposed employing the LLM models to generate condensed reports, interactive explanations, and recommendations based on input text descriptions (predicted disease and report). While the current single-modality (language) LLMs can robustly understand the questions, they still require prior text descriptions to generate responses and are unable to directly infer responses based on the medical image. Although language-only models can greatly benefit the medical domain in language processing, there is a need for robust multi-modality models to process both medical vision and language. In the surgical domain, in addition to the scarcity of surgical experts, their daily schedules are often overloaded with clinical and academic work, making it difficult for them to dedicate time to answer inquiries from students and patients on surgical procedures [3]. Although various computer-assisted solutions [1,10,11,16,17] have been proposed and recorded surgical videos have been made available for students to sharpen their skills and learn from observation, they still heavily rely on surgical experts to answer their surgery-specific questions. In such cases, a robust and reliable surgical visual question answering (VQA) model that can respond to questions by inferring from context-enriched surgical scenes could greatly assist medical students, and significantly reduce the medical expert's workload [19].In the medical domain, MedfuseNet [19], an attention-based model, was proposed for VQA in medical diagnosis. Utilizing the advancements in the transformer models, VisualBert RM [18], a modified version of the VisualBert [12] model was also proposed for VQA in robotic surgery. Compared to most VQA models that require a region proposal network to propose vision patches, the VisualBert RM [18] performed VQA based on features extracted from the whole image, eliminating the need for a region proposal network. However, they were extracted using a non-trainable fixed feature extractor. While VisualBert [12] models and LLMs are transformer models, there are fundamentally different. VisualBert [12] transformers are bidirectional encoder models and are often employed for multi-modality tasks. In contrast, ChatGPT1 (GPT3.5) and BARD (LaMDA [20]) are language-only uni-directional transformer decoder models employed for language generation. As they are proving to be robust in language generation, exploiting them to process the questions and enabling them to process vision could greatly improve performance in VQA tasks.In this work, we develop an end-to-end trainable SurgicalGPT model by exploiting a pre-trained LLM and employing a learnable feature extractor to generate vision tokens. In addition to word tokens, vision tokens (embedded with token type and pose embedding) are introduced into the GPT model, resulting in a Language-Vision GPT (LV-GPT) model. Furthermore, we carefully sequence the word and vision tokens to leverage the GPT model's robust language processing ability to process the question and better infer an answer based on the vision tokens. Through extensive experiments, we show that the SurgicalGPT(LV-GPT) outperforms other state-of-the-art (SOTA) models by ∼ 3-5% on publically available EndoVis18-VQA [18] and Cholec80-VQA surgical-VQA [18] datasets. Additionally, we introduce a novel PSI-AVA-VQA dataset by adding VQA annotations to the publically available holistic surgical scene dataset(PSI-AVA) and observe similar performance improvement. Furthermore, we study and present the effects of token sequencing, where model performance improved by ∼ 2-4% when word tokens are sequenced earlier. Finally, we also study the effects of token type and pose embedding for vision tokens in the LV-GPT model."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,2,Proposed Method,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,2.1,Preliminaries,"GPT2 [6], a predecessor to GPT3.5 (ChatGPT), is a transformer decoder model that performs next-word prediction. Auto-regressive in nature, its self-attention blocks attend to earlier word tokens to predict the next word token iteratively, allowing the model to generate complex paragraphs [15]. Although robust in language generation, due to its unidirectional attention [13], in a given iteration, the generated token knows all earlier tokens but does not know any subsequent token (Fig. 1(a)), restricting the model's ability to capture the entire context between all tokens. VisualBert [12], fundamentally different from GPT models, is a non-auto-regressive transformer encoder model. Its bidirectional self-attention blocks attend in both directions (earlier and subsequent tokens) [13], allowing the model to capture the entire context all at once (Fig. 1(b)). Due to this, bi-directional attention models are often preferred for multi-modality tasks.Vision-Language Processing: Employed mostly for language-only tasks, GPT models do not natively process vision tokens [8]. While it supports robust word embedding, it lacks vision tokenizer and vision embedding layers. This limits exploiting its language processing ability for multi-modality tasks. Alternate to GPT, as the VisualBert model is often preferred for multi-modality tasks, it encompasses dedicated embedding layers for both vision and word tokens."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,2.2,LV-GPT: Language-Vision GPT,Overall Network: We design an end-to-end trainable multi-modality (language and vision) LV-GPT model (Fig. 2) for surgical VQA. We integrate a vision tokenizer (feature extractor) module and vision embedding with the GPT model to exploit its language processing ability in performing VQA tasks. Current token iteration  
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Self-Attention,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Language-Vision Processing:,"The questions are tokenized using the inherent GPT2 tokenizer. The word tokens are further embedded based on token-id, token type (0) and token position by the inherent GPT2 word embedding layers. To tokenize the input surgical scene (image) into vision tokens, the LV-GPT includes a vision tokenizer (feature extractor): ResNet18 (RN18) [9]/Swin [14]/ViT [7]. Given an image, the tokenizer outputs vision tokens, each holding visual features from an image patch. Additionally, the vision tokens are further embedded based on token type (1) and token position (pos = 0) embeddings. The final embedded word and vision tokens (w e and v e ) can be formulated as:where, T t () is type embedding, P pos () is pose embedding, w x and v x are initial word and vision embedding, and v t are vision tokens. Initial word embeds (w x ) are obtained using word embedding based on word token id. Depending on the size (dim) of each vision token, they undergo additional linear layer embedding (f ()) to match the size of the word token.Token Sequencing: LLMs are observed to process long sentences robustly and hold long-term sentence knowledge while generating coherent paragraphs/reports. Considering GPT's superiority in sequentially processing large sentences and its uni-directional attention, the word tokens are sequenced before the vision tokens. This is also aimed at mimicking human behaviour, where the model understands the question before attending to the image to infer an answer.Classification: Finally, the propagated multi-modality features are then passed through a series of linear layers for answer classification."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,3,Experiment,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,3.1,Dataset,"EndoVis18-VQA: We employ publically available EndoVis18-VQA [18] dataset to benchmark the model performance. We use the classification subset that includes classification-based question-and-answer (Q&A) pairs for 14 robotic nephrectomy procedure video sequences of the MICCAI Endoscopic Vision Challenge 2018 [2] dataset. The Q&A pairs are based on the tissue, actions, and locations of 8 surgical tools. The dataset includes 11783 Q&A pairs based on 2007 surgical scenes. The answers consist of 18 classes (1 kidney, 13 tool-tissue interactions, and 4 tool locations). Additionally, we further annotated the validation set (video sequences 1, 5, and 16) on question types to assist in additional analysis. We followed the EndoVis18-VQA [18] dataset's original train/test split."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Cholec80-VQA:,"The classification subset of the Cholec80-VQA [18] is also employed for model evaluation. It contains Q&A pairs for 40 video sequences of the Cholec80 dataset [21]. The subset consists of 43182 Q&A pairs on the surgical phase and instrument presence for 21591 frames. The answers include 13 classes (2 instrument states, 4 on tool count, and 7 on surgical phase). We additionally annotated the validation set (video sequences: 5, 11, 12, 17, 19, 26, 27 and 31) on the Q&A pairs types for further model analysis. The VQA [18] dataset's original train/test split is followed in this work."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,PSI-AVA-VQA:,"We introduce a novel PSI-AVA-VQA dataset that consists of Q&A pairs for key surgical frames of 8 cases of the holistic surgical scene dataset (PSI-AVA dataset) [22]. The questions and answers are generated in sentence form and single-word (class) response form, respectively. They are generated based on the surgical phase, step, and location annotation provided in the PSI-AVA dataset [22]. The PSI-AVA-VQA consists of 10291 Q&A pairs and with 35 answer classes (4 locations, 11 surgical phases, and 211 surgical steps). The Q&A pairs are further annotated into 3 types (location, phase, and step). The fold-1 train/test split of parent PSI-AVA [22] dataset is followed in this work."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,3.2,Implementation Details,"All variants of our models2 are trained based on cross-entropy loss and optimized using the Adam optimizer. The models were trained for 80 epoch, with a batch size of 64, except for LV-GPT (ViT) ( batch size = 32 due to GPU limitation). learning rates lr = 1×10 -5 , 1×10 -5 and 5×10 -6 are used for EndoVis18-VQA, PSI-AVA-VQA and Cholec80-VQA dataset, respectively. The SOTA Visu-alBert [12] and VisualBert RM [18] models were implemented using their official code repositories. The Block [5], MUTAN [4], MFB [24] and MFH [25] were implemented using the official codes of Block [5]."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,4,Results,"All our proposed LV-GPT model variants are quantitatively benchmarked (Table 1) against other attention-based/bi-directional encoder-based SOTA models on EndoVis18-VQA, Cholec80-VQA and PSI-AVA-VQA datasets based VisualBert [12] 0.6143 0.4282 0.3745 0.9007 0.6294 0.6300 0.5853 0.3307 0.3161 VisualBert RM [18] 0.6190 0.4079 0.3583 0.9001 0.6573 0.6585 0.6016 0.3242 0.3165 Block [5] 0.6088 0.4884 0.4470 0.8948 0.6600 0.6413 0.5990 0.5136 0.4933 Mutan [4] 0.6303 0.4969 0.4565 0.8699 0.6332 0.6106 0.4971 0.3912 0.3322 MFB [24] 0.5238 0.4205 0.3622 0.8410 0.5303 0.4588 0.5712 0.4379 0.4066 MFH [25] 0 VisualBert RM [18], Block [5], and our LV-GPT (Swin) models against the ground truth based on input surgical scene and question.on the accuracy (Acc), recall, and Fscore. In most cases, all our variants, LV-GPT (Swin), LV-GPT (RN18) and LV-GPT (ViT), are observed to significantly outperform SOTA models on all three datasets in terms of Acc. Specifically, the LV-GPT (Swin) variant (balanced performance across all datasets) is observed to outperform all SOTA models on all datasets and significantly improve the performance (∼ 3-5% improvement) on EndoVis18-VQA and Cholec80-VQA dataset.Additionally, it should be noted our model variants can be trained end-to-end, whereas, most of the SOTA models requires a region proposal network to process input image into vision tokens. Figure 3 shows the qualitative performance of LV-GPT (Swin) against SOTA models on three datasets. A Comparison of our LV-GPT model performance on the EndoVis18-VQA dataset with default test queries vs rephrased test queries is presented in supplementary materials that highlight the model's robustness in language reasoning."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Early Vision vs Early Word:,"The performance of LV-GPT based on word and vision token sequencing (Table 2) is also studied. While all three variants of the LV-GPT models processing vision tokens earlier are observed to perform on par with SOTA models reported in Table 1, in most cases, their performances on both datasets further improved by ∼ 2-4% when word tokens are processed earlier. This improvement could be attributed to LLM's ability to hold sentence (question) context before processing the vision tokens to infer an answer. This behaviour, in our view, mimics the human thought process, where we first understand the question before searching for an answer from an image."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Pose Embedding for Vision Tokens:,"The influence of positional embedding of the vision tokens (representing a patch region) in all the LV-GPT variants is studied by either embedded with position information (pos = 1, 2, 3, .., n.) or zero-position (pos = 0). Table 3 shows the difference in the performance of the best-performing LV-GPT variant in each dataset, with its vision tokens   embedded with actual-position or zero-position. While we expected the positional embedding to improve the performance (dataset Q&A pairs related to tool location), from the results, we observe that embedding vision tokens with zero-position embedding results in better performance. In-depth analysis shows that our CNN-based LV-GPT (RN18) model improved with positional embedding (Table 3 and Table 4). In the transformer-based LV-GPT (Swin)/LV-GPT (ViT) models, positional embedding is already incorporated at the vision tokenizer (VIT/Swin) layer, and adding positional embedding at the GPT level results in double Position embedding. Thus, ""zero-position"" can be interpreted as ""LV-GPT only requires one layer of positional embedding"". A sub-type analysis (Fig. 4) is also performed on the model performance to analyze the effect of positional embedding of the vision tokens. The model in which the vision tokens were embedded with zero-position (at the GPT level), performed marginally better/similar on all sub-types in the Cholec80-VQA dataset. However, its performance improvement was significant in the PSI-AVA-VQA dataset sub-types, including the 'tool location' sub-types that contain questions on tool location."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Ablation Study on Vision Token,"Embedding: An ablation study on the vision token embedding in the LV-GPT model on the EndoVis18-VQA dataset is also shown in Table 4. VB-VE refers to vision token embedding using Visu-alBert vision embedding. The C-VE refers to custom embedding, where, in LV-GPT (RN18), the vision token undergoes additional linear layer embedding to match the word-token dimension, and in other variants, vision tokens from the Swin/VIT are directly used. The subsequent VT-TY + VT-PE and VT-TY + VT-ZPE refers to the additional vision token type (TY) and actual-position (PE)/zero-position (ZPE) embedding. We observe that employing C-VE with VT-TY + VT-ZPE results in better performance."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,5,Conclusion,"We design an end-to-end trainable SurgicalGPT, a multi-modality Language-Vision GPT model, for VQA tasks in robotic surgery. In addition to GPT's inherent word embeddings, it incorporates a vision tokenizer (trainable feature extractor) and vision token embedding (type and pose) to perform multimodality tasks. Furthermore, by carefully sequencing the word tokens earlier to vision tokens, we exploit GPT's robust language processing ability, allowing the LV-GPT to significantly perform better VQA. Through extensive quantitative analysis, we show that the LV-GPT outperforms other SOTA models on three surgical-VQA datasets and sequencing word tokens early to vision tokens significantly improves the model performance. Furthermore, we introduce a novel surgical-VQA dataset by adding VQA annotations to the publically available holistic surgical scene dataset. While multi-modality models that process vision and language are often referred to as ""vision-language"" models, we specifically name our model ""language-vision GPT"" to highlight the importance of the token sequencing order in GPT models. Integrating vision tokens into GPT also opens up future possibilities of generating reports directly from medical images/videos."
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Fig. 1 .,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Fig. 2 .,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Fig. 3 .,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Fig. 4 .,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Table 1 .,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Table 2 .,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Table 3 .,
SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery,,Table 4 .,
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,1,Introduction,"A cochlear implant (CI) has an electrode array (EA) that is surgically inserted into the cochlea to stimulate the auditory nerves and treat patients with severe-to-profound sensorineural hearing loss. Although CIs have achieved great success, hearing outcomes among recipients vary significantly [1,2]. Recent studies have shown that factors related to electrode positioning impact on audiological outcomes [3,4]. These studies require knowing the precise electrode locations, which can be obtained by postoperative CT imaging. Another clinical application that requires precise electrode locations is imageguided cochlear implant programming [5]. After surgical implantation, the CI needs to be programmed such that an optimized frequency mapping [24] can be determined for each patient. Knowledge about the spatial relationship between the electrodes and the intracochlear anatomy permits to generate programming solutions which have been shown to significantly improve the hearing outcomes for both adult and pediatric CI recipients [6,7].To facilitate the EA localization process, automated methods have been proposed by several groups. In [8] EAs are categorized as closely-spaced and distantly-spaced. EAs with an interelectrode spacing such that the intensity contrast between electrodes cannot be distinguished are considered closely-spaced, while the opposite applies to distantlyspaced EAs. Note that a given EA model could be categorized as closely-spaced or distantly-spaced, depending on the image resolution. For example, some closely-spaced EAs included in [8] could be considered distantly-spaced in some images acquired at very high resolution (0.08mm isotropic) as in [9]. For algorithms localizing closely-spaced EAs, extracting the centerline of an EA is often an important step [8,10,11]. It is achieved by either using intensity-based features only [11] or combining both intensity-based and EA shape-based features [8]. For localizing distantly-spaced EAs, hand-crafted feature extractors are utilized to detect individual blobs [9,[12][13][14]. To link the electrode candidates in the correct order and remove false positive candidates, graph-based pathfinding algorithms [12] or Markov random field models [14] have been proposed. While there has been an emergence of methods based on deep learning (DL) recently [15][16][17], they cannot be viewed as a complete solution to the automatic EA localization problem because the networks are only trained and validated on one type of EA model [17] or cannot order/link the detected electrodes to form a complete array [15,16].In this work, we present a novel DL-based framework that consists of a multi-task network and a set of postprocessing algorithms to localize the cochlear implant electrode array. Our contribution is three-fold: (1) To the best of our knowledge, it is the first unified DL-based framework designed for localizing both distantly-and closely-spaced EAs in CT and cone-beam CT (CBCT) images. (2) We propose four (three detection and one segmentation) tasks for the multi-task network such that this single network can be trained on various kinds of EAs. (3) We extensively evaluate this framework on datasets that significantly exceed the scale of all datasets reported in the literature to date: a heterogeneous clinical test set with CT or CBCT images of 561 implanted ears and a test set with gold standard ground truth for 27 implanted cadaveric ears. Results show that the proposed framework is significantly more robust (generates results that require manual adjustments less often) than the state-of-the-art (SOTA) techniques, while also being slightly more accurate. These findings indicate that the proposed framework could be reliably used to support large-scale quantitative studies and deployed in the clinical workflow to provide clinicians with critical information at the time and point of care. "
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,2,Method,
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,2.1,Data,"The data in this study consist of a large-scale clinical dataset from CI recipients (dataset #1) as well as 27 cadaveric samples (dataset #2). Dataset #1 includes 1324 implanted ears from CI recipients treated at two institutions (datasets #1A and #1B). 8 types of distantlyand closely-spaced EA from 3 manufacturers are included in these cases. Dataset #1A has 958 implanted ears of which 97% are scanned with CBCT scanners, and the remaining (3%) are scanned with conventional CT scanners. Dataset #1B includes 366 implanted ears of which most (98%) are scanned with conventional CT scanners and the remaining (2%) are scanned with CBCT scanners. Dataset #2 contains 4 types of distantly-spaced EAs, and these specimens are scanned with conventional CTs.The training and validation sets are all from dataset #1A and constitute 60% and 20% of that dataset, respectively. The remaining 20% of dataset #1A along with dataset #1B (561 implanted ears in total) are used to test the robustness of the proposed framework. Dataset #2 is used to test its accuracy because the gold standard ground truth can be obtained using their paired micro-CT [16,18]. Details on the datasets and EA specifications can be found in the supplementary material."
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,2.2,Multi-task U-Net,"The proposed framework is designed to localize various types of EAs in both CT and CBCT. This is challenging because the number of electrodes to be detected and the interelectrode spacing is different among EA models, and the postoperative images can have different intensity characteristics depending on the type of scanner, i.e., CT or CBCT, that is used for their acquisition. To normalize the input image in a way that enhances both the nearby anatomy, which contains contextual information, and the high intensity (usually the electrodes) component, inspired by [16], we separate the raw image into two channels. One is the original image clipped at the 99.9% of its intensity histogram. The other is the original image in the [99.9%-100%] interval of its intensity histogram. Each channel is linearly normalized to [0,1] based on its own min-max values. We refer to these as the low and high intensity bands and an example can be seen in Fig. 1A.To avoid having to train a network for each EA type, we define four tasks that a single multi-task network can learn simultaneously and whose output can be used to localize and order the electrodes in all arrays. Specifically, as shown in Fig. 1B, the four tasks include (1) detection of all the electrodes on the EA by heatmap regression, (2) detection of the most apical endpoint (tip) of the EA by heatmap regression, (3) detection of the most basal endpoint (the farthest electrode from the tip) of the EA by heatmap regression, and (4) segmentation of the EA centerline that starts and ends with the two endpoints. Although the network is a simple U-Net-like encoder-decoder architecture, the innovation is more focused on the multi-task strategy such that the network can serve as a robust feature extractor that leverages a large heterogeneous dataset.We train the multi-task network using the electrode positions obtained with the methods described in [8,12] corrected manually when a large localization error is visually observed. For tasks (#1,2, and 3) aiming at localizing electrodes, the ground truth is a one-channel heatmap for each task with a Gaussian kernel (variance of 2 voxels) at each electrode location. Following [19], we use a penalty-reduced voxel-wise logistic regression with the focal loss [20] as the training objective.Depending on the image quality and the interelectrode spacing, EAs can appear as a whole bright (i.e., with high Hounsfield Unit (HU) values) tubular structure or distinguishable bright blobs representing individual electrodes. We define the centerline of the EA as a line connecting each electrode in sequential order (from most apical to most basal or the opposite). The motivation for segmenting the EA centerline is twofold. First, after we extract all the electrodes from the predicted heatmap (Task #1), we need to order them. We will show later in our postprocessing algorithms that these detected electrodes can be linked in the correct order using the segmented centerline and the detected two endpoints (Algorithm 1 in Fig. 1C). The second reason is that it can serve as an alternative EA localization method by sampling the centerline using the known interelectrode spacing specific to a particular EA model (Algorithm 2 in Fig. 1C). It is essential when the electrodes are not discernible due to the low image resolution and/or the small interelectrode spacing. Different from [8], we resort to a DL approach rather than human-crafted feature extractors. To make the training of the centerline segmentation easier, we dilate the ground truth centerline to a tubular-structured mask with a radius of 3 voxels as the learning target. In addition to the Dice loss, we adopt a clDice loss which has shown its superiority in improving the accuracy and preserving the topology of the underlying one-voxel wide centerline [21]."
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,2.3,Postprocessing Algorithms for EA Localization,"As said above, although the output of the multi-task network contains essential information to identify the EA, it does not provide the desired final output, i.e., the position (coordinates) of the ordered electrodes in the image. To do so, we have designed a series of postprocessing algorithms that can effectively extract the ordered electrode locations from the four output maps. As shown in Fig. 1C, there are two main algorithms (Algorithm 1 and Algorithm 2) that are suitable for distantly-and closely-spaced EAs, respectively.Algorithm 1 takes the heatmap of all the electrodes (Task #1) as input and utilizes a non-maximum suppression (NMS) algorithm, which is a common postprocessing step for object detection [22], to obtain the desired number of electrodes. Then, the centerline of the EA is extracted by skeletonizing its segmented mask. Its two endpoints are further refined by merging the detection results of the most apical and basal points. Finally, all the detected electrodes are linked with the guidance of the centerline along the direction from the most apical to the most basal. Algorithm 1 works well if there is an apparent contrast between the electrodes in the heatmap, which is the case for most distantly-spaced EAs.However, for closely-spaced EAs, it is nearly impossible to differentiate the individual electrodes. Algorithm 2 is designed to localize EAs in such situations. After the extraction and refinement of the EA centerline, the centerline is smoothed with a cubic spline, and the final electrode positions are obtained by resampling along it using known interelectrode spacing for the EA [8]. Note that for distantly-spaced EAs, Algorithm 1 can occasionally lead to abnormal localization results, such as an incorrect number of detected electrodes or the spacing between the detected electrodes being inconsistent with the known interelectrode spacing for the EA. This is most often caused by poor image quality that affects the creation of the subsequent heatmap (Task #1). We have designed simple rules to detect these anomalies. When one is detected, the framework switches to Algorithm 2 for a more reliable sampling-based localization for distantly-spaced EAs."
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,3,Experiments and Results,
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,3.1,Implementation Details,"The multi-task U-Net is trained with PyTorch 1.12 on an NVIDIA RTX 2080 Ti GPU. We use MONAI [23] for data augmentation which contains an additive Gaussian noise and random affine transformations. The images are preprocessed by being rigidly registered to the left ear (mirroring is performed if one case is a right ear) of a template volume (atlas), resampled to a 0.1mm isotropic voxel size, and cropped into a region of interest (ROI) with a dimension of 320 × 320 × 320. Due to GPU memory size limit, we use a patch-based strategy (with a dimension of 256 × 256 × 192) for training. The batch size is set to 1 and we use AdamW optimizer with learning rate of 5e-4. At inference we use a sliding-window approach to merge the results. The network is trained for 250 epochs, and we select the epoch with the lowest validation loss as our final model. Note that a minority (3%) of the images in dataset #1 are reconstructed with a limited HU range, i.e., [-1024,3071]HU. In these images bone and electrodes have similar intensity values. The remaining (97%) images have maximum intensities far larger than 3071HU. In these images electrodes have larger intensity values than bone. To address this issue, we train another DL model with the same training strategy and dataset, but the intensity of all the training images is saturated at 3071HU, i.e., every pixel with an intensity above 3071HU is assigned a value of 3071HU. All the results obtained for limited HU range images presented herein are obtained with this dedicated model. The postprocessing algorithms are implemented in Python 3.9 and NumPy. We use skimage for 3D skeletonization and the csaps package for calculating the cubic spline. The inference time for the proposed framework (from loading the image to outputting the electrode locations) ranges between 5 to 20 s."
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,3.2,Evaluation and Results,"The techniques described in [8] (for closely-spaced EAs) and [12] (for distantly-spaced EAs) are designed and validated with EAs and imaging protocols that are similar to those used in this study. They are also in routine use with over 2000 ears processed at the two institutions in dataset #1. They are considered to be the SOTA methods for comparison. We first evaluate the accuracy of the proposed framework on dataset #2 (27 implanted cadaveric ears) for which the localization ground truth is known. Since the EAs in dataset #2 are all distantly-spaced, [12] is used as the previous SOTA method for comparison. We define the point-to-point error (P2PE) as the Euclidean distance between the predicted electrode location and the ground truth location, and we calculate five P2PE-based metrics (maximum, median, mean, standard deviation (std), and the minimum P2PE) for all the electrodes in each case. The quantitative results are shown in Fig. 2. We can see from the left box plot that the results of the previous method [12] contain an outlier with relatively large P2PE, which is attributed to the low quality of the image. The median values in the right box plot show that the proposed framework has slightly smaller P2PE across the five metrics, but the differences are not statistically significant.As introduced in Sect. 2.1, the 561 clinical cases in dataset #1 used for testing are highly heterogeneous. They contain CBCT and CT images from dataset #1A and #1B. Since manual annotations are not available for these images, we ask three experts to evaluate and compare the results obtained with the previous SOTA methods ( [8] and [12]) and those obtained with the proposed method. Specifically, as shown in Fig. 4, we generate MIP (Maximum Intensity Projection) images and mark the locations at which the electrodes have been localized. This permits a rapid visual assessment of the localization quality. Next, for each case, we present the visualization of the results generated by the previous SOTA methods and by the proposed method side-by-side but in a random order, such that the experts are blind to the methods used to localize the contacts. They are then asked to rate the localization results as acceptable, i.e., no need to adjust the localization result or ""Needs Adjustment (NA)"", i.e., at least one electrode location needs to be adjusted by more than half a contact size. When both results are acceptable, raters are also asked to decide which one is preferred (i.e., has a more accurate localization result) or there is no preference.Before performing the expert evaluation, we calculate the maximum P2PE between the results from the previous SOTA and the proposed methods. For cases with maximum P2PE larger than 0.3 mm (we refer to them as the large-error subset), there is a high probability that at least one of the methods generates NA results. For cases with maximum P2PE smaller than 0.3 mm (we refer to them as the small-error subset), we presume that the probability of NA results is relatively small. To limit the demand on the experts' time, we ask three experts (R1, R2, and R3) to rate the large error subset (164 cases) and only one expert (R1) to rate the small error subset (397 cases).The expert evaluation results are shown as radar plots in Fig. 3. The left figure shows the acceptance rate for the large error cases evaluated by raters R1, R2, and R3. Except for the cases rated by R1 that contain closely-spaced EAs in CBCT (""R1-CBCT-Closely""), the localization results generated by the proposed method have a substantially higher acceptance rate than the previous SOTA methods ( [8] for closely-spaced and [12] for distantly-spaced). For the cases in which both methods are acceptable, the average preference rates across the three raters are: 20.2% for ""SOTA preferred"", 30.3% for ""Proposed preferred"", and 49.5% for ""No preference"". As can be seen in the right plot of Fig. 3, the overall acceptance rate on the whole clinical test set evaluated by R1 is 80.7% for the previous SOTA methods and 89.7% for the proposed method. It is interesting to note that although the proposed method is trained mostly on CBCT images, it still generalizes well on CT images. Figure 4 shows two representative cases from the large-error subset."
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,4,Conclusions,"In this work, we have proposed a novel DL-based framework for cochlear implant EA localization. To the best of our knowledge, it is the first unified DL-based framework designed for localizing both distantly-and closely-spaced EAs in CT and CBCT images.Compared to the SOTA methods, the proposed framework is substantially more robust (9% less NA results) when evaluated on a large-scale clinical dataset and achieves slightly more accurate localization results on a dataset containing 27 cadaveric samples with gold standard ground truth. While it may be possible to improve our success rate further, a low percentage of NA results is unavoidable. We are thus developing quality assessment techniques to alert end users when images have poor quality and/or results are unreliable."
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,,Fig. 1 .,
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,,Fig. 2 .,
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,,Fig. 3 .,
A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization,,Fig. 4 .,
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,1,Introduction,"Surgical procedures are becoming increasingly complex, requiring intricate coordination between medical staff, patients, and equipment [11,12]. Effective oper-Fig. 1. Overview of our bimodal scene graph generation architecture. We use the visual information extracted from point clouds and images and temporal information represented as memory scene graphs resulting in more accurate and consistent predictions.ating room (OR) management is critical for improving patient outcomes, optimizing surgical team performance, and developing new surgical technologies [10]. Scene understanding, particularly in dynamic OR environments, is a challenging task that requires holistic and semantic modeling [11,16], where both the coarse and the fine-level activities and interactions are understood. While many recent works addressed different aspects of this understanding, such as surgical phase recognition [2,9,22], action detection [14,15], or tool detection [4,7], these approaches do not focus on holistic OR understanding. Most recently, Özsoy et al. [16] proposed a new dataset, 4D-OR, and an approach for holistic OR modeling. They model the OR using semantic scene graphs, which summarize the entire scene at each timepoint, connecting the different entities with their semantic relationships. However, they did not propose remedies for challenges caused by occlusions and visual similarities of scenes observed at different moments of the intervention. In fact, they rely only on single timepoints for OR understanding, while temporal history is a rich information source that should be utilized for improving holistic OR modeling.In endoscopic video analysis, using temporality has become standard practice [2,5,19,22]. For surgical workflow recognition in the OR, the use of temporality has been explored in previous studies showcasing their effectiveness [6,13,18]. All of these methods utilize what we refer to as latent temporality, which is a non-interpretable, hidden feature representation. While some works utilize twostage architectures, where the temporal stage uses precomputed features from a single timepoint neural network, others use 3D(2D + time) methods, directly considering multiple timepoints [3].Both of these approaches have some downsides. The two-stage approaches are not trained end-to-end, potentially limiting their performance. Additionally, certain design choices must be made regarding which feature from every timepoint should be used as a temporal summary. For scene graph generation, this can be challenging, as most SGG architectures work with representations per relation and not per scene. The end-to-end 3D methods, on the other hand, are computationally expensive both in training and inference and practical hardware limitations mean they can only effectively capture short-term context. Finally, these methods can only provide limited insight into which temporal information is the most useful.In the computer vision community, multiple studies on scene understanding using scene graphs [24,25] have been conducted. Ji et al. [8] proposes Action Genome, a temporal scene graph dataset containing 10K videos. They demonstrate how scene graphs can be utilized to improve the action recognition performance of their model. While there have been some works [1,21] on using temporal visual information to enhance scene graph predictions, none consider the previous scene graph outputs as a temporal representation to enhance the future scene graph predictions.In this paper, we propose LABRAD-OR(Lightweight Memory Scene Graphs for Accurate Bimodal ReAsoning in Dynamic Operating Rooms), a novel and lightweight approach for generating accurate and consistent scene graphs using the temporal information available in OR recordings. To this end, we introduce the concept of memory scene graphs, where, for the first time, the scene graphs serve as both the output and the input, integrating temporality into the scene graph generation. Our motivation behind using scene graphs to represent memory is twofold. First, by design, they summarize the most relevant information of a scene, and second, they are lightweight and interpretable, unlike a latent feature-based representation. We design an end-to-end architecture that fuses this temporal information with visual information. This bimodal approach not only leads to significantly higher scene graph generation accuracy than the state-of-the-art but also to better inter-timepoint consistency. Additionally, by choosing lightweight architectures to encode the memory scene graphs, we can integrate the entire temporal information, as human-interpretable scene graphs, with only 40% overhead. We show the effectiveness of our approach through experiments and ablation studies."
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,2,Methodology,"In this section, we introduce our memory scene graph-based temporal modeling approach (LABRAD-OR), a novel bimodal scene graph generation architecture for holistic OR understanding, where both the visual information, as well as the temporal information in the form of memory scene graphs, are utilized. Our architecture is visualized in Fig. 1."
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,2.1,Single Timepoint Scene Graph Generation,"We build up on the 4D-OR [16] method, which uses a single timepoint for generating semantic scene graphs. The 4D-OR method extracts human and object poses and uses them to compute point cloud features for all object pairs. Additionally, image features are incorporated into the embedding to improve the recognition of details. These representations are then further processed to generate object and relation classes and are fused to generate a comprehensive scene graph."
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,2.2,Scene Graphs as Memory Representations,"In this study, we investigate the potential of using scene graphs from previous timepoints, which we refer to as ""memory scene graphs"", to inform the current prediction. Unlike previous research that treated scene graphs only as the final output, we use them both as input and output. Scene graphs are particularly well-suited for encoding scene information, as they are low-dimensional and interpretable while capturing and summarizing complex semantics. To create a memory representation at a timepoint T, we use the predicted scene graphs from timepoints 0 to T-1 and employ a neural network to compute a feature representation. This generic approach allows us to easily fuse the scene graph memory with other modalities, such as images or point clouds.Memory Modes: While our efficient memory representation allows us to look at all the previous timesteps, this formulation has two downsides. Surgical duration differs between procedures, and despite the efficiency of scene graphs, prolonged interventions can still be costly. Second, empirically we find that seeing the entire memory leads to prolonged training time and can cause overfitting. To address this, we propose four different memory modes, ""All"", ""Short"", ""Long"", and ""LongShort"". The entire surgical history is visible only in the ""All"" mode. In the ""Short"" mode, only the previous S scene graphs are utilized, while in the ""Long"" mode, every S.th scene graph is selected using striding. In ""LongShort"" mode, both ""Long"" and ""Short"" modes are combined. The reasoning behind this approach is that short-term context should be observed in detail, while longterm context can be viewed more sparsely. This reduces computational overhead compared to ""All"" and leads to better results with less overfitting, as observed empirically. The value of S is highly dependent on the dataset and the surgical procedures under analysis (Fig. 2)."
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,2.3,Architecture Design,"We extract the visual information from point clouds and, optionally, images using a visual scene encoder, as described in Sect. 2.1. To integrate the temporal information, we convert each memory scene graph into a feature vector using a graph neural network. Then we use a Transformer block [23] to summarize the feature vectors from the #T memory scene graphs into a single feature vector, which we refer to as the memory representation. Finally, this representation is concatenated with the visual information, forming a bimodal representation. Intuitively, this allows our architecture to consider both the long-term history, such as previous key surgical steps and the short-term history, such as what was just happening. Fig. 2. Visualization of both the ""Short"" and ""Long"" memory attention while predicting for the timepoint t = 530. While ""Short"" attends naturally to the nearest scene graphs, ""Long"" seems to be concentrating on previous key moments of the surgery, such as ""drilling"", ""sawing"", ""and hammering"". The graphs are simplified for clarity. The shown ""current scene graph"" is the correct prediction of our model.Memory Augmentations: While we use the predicted scene graphs from previous timepoints for inference, as these are not available during training, we use the ground truth scene graphs for training. However, training with ground truth scene graphs and evaluating with predicted scene graphs can decrease test performance, as the predicted scene graphs are imperfect. To increase our robustness towards this, we utilize memory augmentations during training. Concretely, we randomly replace part of either the short-term memory (timepoints closest to the timepoint of interest) or the long-term memory (timepoints further away from the timepoint of interest) with a special ""UNKNOWN"" token. Intuitively, this forces our model to rely on the remaining information and better deal with wrong predictions in the memory during inference.Timepoint of Interest(ToI) Positional Ids: Transformers [23] employ positional ids to encode the absolute location of each feature in a sequence. However, as we are more interested in the relative distance of the features, we introduce Timepoint of Interest(ToI) positional ids to encode the distance of every feature to the current timepoint T . This allows our network to assign meaning to the relative distance to other timepoint features rather than their absolute locations.Multitask Learning: In the scene graph generation task, the visual and temporal information are used together. In practice, we found it valuable to introduce a secondary task, which can be solved only using temporal information. We propose the task of ""main action recognition"", where instead of the scene graph, only the interaction of the head surgeon to the patient is predicted, such as ""sawing"" or ""drilling"". During training, in addition to fusing the memory representation with the visual information, we use a fully connected layer to estimate the main action from the memory representation directly. Learning both scene graph generation and main action recognition tasks simultaneously gives a more direct signal to the memory encoder, resulting in faster training and improved performance.Table 1. We compare our results to the current SOTA, 4D-OR, on the test set. We experimented with different hyperparameters and found that longer training can improve the 4D-OR results. We report both the original 4D-OR, and the longer trained results, indicated by †, and a latent-based temporality(LBT) baseline, and compare LABRAD-OR to them. All methods use both point clouds and images as visual input."
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,,Method,4D-OR [16] 4D-OR † [16] LBT LABRAD-OR Macro F1 0.75 0.83 0.86 0.88
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,3,Experiments,"Dataset: We use the 4D-OR [16] dataset following the official train, validation, and test splits. It comprises ten simulated knee surgeries recorded using six Kinect cameras at 1 fps. Both the 3D point cloud, as well as multiview images are provided for all 6734 scenes. Each scene additionally includes a semantic scene graph label, as well as clinical role labels for staff.Model Training: Our architecture consists of two components implemented in PyTorch 1.10, a visual model and a memory model. For our visual model, we use the current SOTA model from 4D-OR [16], which uses Pointnet++ [17] as the point cloud encoder, and EfficientNet-B5 [20] as the image encoder. We could improve the original 4D-OR results through longer training than in the original code. As our memory model, we use a combination of Graphormer [26], to extract features from individual scene graphs and Transformers [23], to fuse the features into one memory representation. The visual scene encoder is initialized in all our experiments with the best-performing visual-only model weights.We use the provided human and object pose predictions from 4D-OR and stick to their training setup and evaluation metrics. We use memory augmentations, timepoint of interest positional ids, end-to-end training, and multitask learning. The memory encoders are purposefully designed to be lightweight and fast. Therefore, we use a hidden dimension of 80 and only two layers. We use S, to control both the stride of the ""Long"" mode and the window size of the ""Short"" mode and set it to 5. The choice of S ensures we do not miss any phase, as all phases last longer than 5 timepoints while reducing the computational cost.Unless otherwise specified, we use ""LongShort"" as memory mode and train all models until the validation performance converges.Evaluation Metrics: We use the official evaluation metrics from 4D-OR for semantic scene graph generation and the role prediction downstream tasks. In both cases, a macro F1 over all the classes is computed. Further, we introduce a consistency metric, where first, for each timepoint, a set of predicates P t , such as {""assisting"", ""drilling"", ""cleaning""} is extracted from the scene graphs. Then, for two timepoints T and T -1, the intersection of union(IoU) between P t and P t-1 is computed. This is repeated for all pairs of adjacent timepoints in a sequence, and the IoU score is averaged over them to calculate the consistency score.Table 2. We demonstrate the impact of temporal information on the consistency of our results. We compare only using the point cloud(PC), using images(Img) in addition to point clouds, and temporality(T). We also show the ground truth(GT) consistency score, which should be considered the ceiling for all methods.Method PC PC+Img PC+T PC+Img+T GT Consistency 0.83 0.84 0.86 0.87 0.9 Fig. 3. Qualitative example on the improvement of the scene graph consistency. For clarity, only the ""main action"" is shown, while only relying on the visual information (V) compared to also using our proposed SG memory (M)."
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,4,Results and Discussion,"Scene Graph Generation. In Table 1, we compare our best-performing model LABRAD-OR against the previous SOTA on 4D-OR. We build a latent-based temporal baseline (LBT), which uses a mean of the pairwise relation features as representation per timepoint, which then gets processed analogously with a Transformer architecture. Overall, LABRAD-OR increases the F1 results for all predicates, significantly increasing SOTA from 0.75 F1 to 0.88 F1. We also show that LABRAD-OR improves the F1 score compared to both the longer trained 4D-OR and LBT by 5% and 2%, respectively, demonstrating both the value of temporality for holistic OR modeling as well as the effectiveness of memory scene graphs. Additionally, in Table 2, we show the consistency improvements achieved by using temporal information, from 0.84 to 0.87. Notably, a perfect consistency score is not 1.0, as the scene graph naturally changes over the surgery. Considering the ground truth consistency is at 0.9, LABRADOR-OR (0.87) exhibits superior consistency compared to the baselines without being excessively smooth.A qualitative example of the improvement can be seen in Fig. 3. While the visualonly model confuses ""suturing"" for ""cleaning"" or ""touching"", our LABRAD-OR model with memory scene graphs identifies all correctly ""suturing"".Clinical Role Prediction. We also compare LABRAD-OR to 4D-OR on the downstream task of role prediction in Table 5, where the only difference between the two is the improved predicted scene graphs used as input for the downstream task. We improve the results by 4%, showing our improvements in scene graph generation also translate to downstream improvements.Ablation Studies. We conduct multiple ablation studies to motivate our design choices. In Table 3, we demonstrate the effectiveness of the different components of our method and see that both memory augmentations and ToI positional ids are crucial and significantly contribute to the performance, whereas E2E and multitask have less but still measurable impact. We note that multitask learning also helps in stabilizing the training. In Table 4, we ablate the different memory modes, ""Short"", ""Long"", ""LongShort"", and ""All"", and find that the strided ""Long"" mode is the most important. Where the ""Short"" mode can often lead to insufficient contextual information, ""All"" can lead to overfitting. Both the ""Long"" and ""LongShort"" perform similarly and are more efficient than ""All"". We use ""LongShort"" as our default architecture to guarantee no short-term history is overseen. Comparing the ""LongShort"" F1 result of 0.87 when only using point clouds as visual input to 0.88 when using both point cloud and images, it can be seen that images still provide valuable information, but significantly less than for Özsoy et al. [16]."
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,5,Conclusion,"We propose LABRAD-OR, a novel lightweight approach based on human interpretable memory scene graphs. Our approach utilizes both the visual information from the current timepoint and the temporal information from the previous timepoints for accurate bimodal reasoning in dynamic operating rooms. Through experiments, we show that this leads to significantly improved accuracy and consistency in the predicted scene graphs and an increased score in the downstream task of role prediction. We believe LABRAD-OR offers the community an effective and efficient way of using temporal information for a holistic understanding of surgeries."
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,,,
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,,Table 3 .,
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,,Table 4 .,
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,,Table 5 .,
LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_29.
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,1,Introduction,"During colonoscopy screenings, localizing the camera and reconstructing the colon directly from the video feed could improve the detection of polyps and help with navigation. Such tasks can be either treated individually using depth [3,6,15,16] and pose estimation approaches [1,14] or jointly, using structure from motion (SfM) and visual simultaneous localization and mapping (VSLAM) algorithms [5,9,10]. However, the limited data availability present in surgery often makes evaluation and supervision of learning-based approaches difficult.To address the lack of data in surgery, previous work has explored both synthetic pose and depth data generation [2,15], and real data acquisition [2,4,12]. Generating datasets from endoscopic sequences using game engines is scalable and noise-free but often cannot replicate the material properties and lighting conditions of the target environment. In contrast, capturing real data is a laborious process that often introduces sources of error.Neural Radiance Field (NeRF) [11] networks aim to learn an implicit 3D representation of a 3D scene from a set of images captured from known poses, enabling image synthesis from previously unseen viewpoints. NeRF models render 3D geometry and color, including view-dependent reflections, enabling the rendering of photo-realistic images and geometrically consistent depth maps. EndoNeRF [20] applied NeRF techniques for the first time on surgical video. The method fits a dynamic NeRF [13] on laparoscopic videos, showing tools manipulating tissue from a fixed viewpoint. After training, the video sequences were rendered again without the tools obstructing the tissue. However, directly applying similar techniques in colonoscopy, is challenging because NeRF assumes fixed illumination. As soon as the endoscope moves, changes in tissue illumination result in color ambiguities.This paper aims to mitigate the depth and pose data scarcity in colonoscopy. Inspired by work in data generation using NeRF [18], we present an extension of NeRF which makes it more suitable for use in endoscopic scenes. Our approach aims to expand colonoscopy VSLAM datasets [4] by rendering views from novel trajectories while allowing simulation of different camera models Fig. 1. Our approach addresses the scalability issues of real data generation techniques while reproducing realistic images. Our main contributions are: 1)The introduction of a depth-supervised NeRF variant conditioned on the location of the endoscope's light source. This extension is important for modeling variation in tissue illumination while the endoscope moves. 2) We evaluate our model design choices on the C3VD dataset and present renditions of the dataset from previously unseen viewpoints in addition to simulating different camera systems."
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,2,Method,"Our method requires a set of images with known intrinsic and extrinsic camera parameters and sparse depth maps of a colonoscopy sequence. This information is already available in high-quality VSLAM [2,4,12] datasets which we wish to expand or can be extracted by running an SfM pipeline such as COLMAP [17] on prerecorded endoscopic sequences. Our pipeline involves first optimizing a special version of NeRF modified to model the unique lighting conditions present in endoscopy. The resulting network is used to render views and their associated dense depth maps from user-defined camera trajectories while allowing to specify of the camera model used during rendering. Images rendered from our models closely resemble the characteristics of the training samples. Similarly, depth maps are geometrically consistent as they share a commonly learned 3D representation. Those properties make our method appealing as it makes realistic data generation easy, configurable, and scalable."
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,2.1,Neural Radiance Fields,"A NeRF [11] implicitly encodes the geometry and radiance of a scene as a continuous volumetric function F Θ : (x, d) → (c, σ)). The inputs of F Θ are the 3D location of a point in space x = (x, y, z) and the 2D direction from which the point is observed d = (φ, θ). The outputs are the red-green-blue (RGB) color c = (r, g, b) and opacity σ. F θ is learned and stored in the weights of two cascaded multi-layer perceptron (MLP) networks. The first, f σ , is responsible for encoding the opacity σ of a point, based only on x. The second MLP, f c , is responsible for encoding the point's corresponding c based on the output of the f σ and d. NeRFs are learned using differentiable rendering techniques given a set of images of scenes and their associated poses. Optimization is achieved by minimizing the L2 loss between the predicted and reference color of all pixels in the training images. To predict the color of a pixel, a ray r(t) = o + td is defined in space starting for the origin of the corresponding image o and heading towards d which is the direction from where light projects to the corresponding pixel. N points r(t i ), i ∈ [n, f ] are sampled along the ray between a near t n and a far t f range to query NeRF for both σ and c. The opacity values σ of all points along the r can be used to approximate the accumulated transmittance T i as defined in Eq. ( 1), which describes the probability of light traveling from o to r(t i ). T i together with the color output of f σ for every point along the ray, can be used to compute the pixel color C p using alpha composition as defined in Eq. ( 2)Similarly, the expected ray termination distance D p can be computed from Eq.(3), which is an estimate of how far a ray travels from the camera until it hits solid geometry. D p can be converted to z-depth by knowing the uv coordinates of the corresponding pixel and camera model.In practice, NeRF uses two pairs of MLPs. Initially, a coarse NeRF F Θc is evaluated on N c samples along a ray. The opacity output of the coarse network is used to re-sample rays with more dense samples where opacity is higher. The new N f ray samples are used to query a fine NeRF F Θf . During both training and inference, both networks are working in parallel. Lastly, to enable NeRF to encapsulate high-frequency geometry and color details, every input of F Θ is processed by a hand-crafted positional encoding module γ(•), using Fourier features [19]."
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,2.2,Extending NeRF for Endoscopy,"Light-Source Location Aware MLP. During a colonoscopy, the light source always moves together with the camera. Light source movement results in illumination changes on the tissue surface as a function of both viewing direction (specularities), camera location (exposure changes), and distance between the tissue and the light source (falloff) Fig. 2. NeRF [11], only models radiance as a function of viewing direction as this is enough when the scene is lit uniformly from a fixed light source and the camera exposure is fixed. To model changes in illumination as a function of light source location, we extend the original NeRF formulation by conditioning f c on both the 2D ray direction γ(d) and also the location of the light source o. For simplicity, throughout this work, we assume a single light source co-located with the camera. This parameterization allows the network to learn how light decays as it travels away from the camera and adjusts the scene brightness accordingly.Depth Supervision. NeRF achieves good 3D reconstruction of scenes using images captured from poses distributed in a hemisphere [11]. This imposes geometric constraints during the optimization because consistent geometry would result in a 3D point projecting in correct pixel locations across different views.Training a NeRF on colonoscopy sequences is hard because the camera moves along a narrow tube-like structure and the colon wall is often texture-less. Supervising depth together with color can guide NeRF to learn a good 3D representation even when pose distribution is sub-optimal [7]. In this work, we compute the distance between the camera and tissue D ref from the reference depth maps and we sample K out of M pixel Eq. ( 5) to optimize both color and depth as described in Eq. ( 4).• 1 is the L1 loss, • 2 2 is the L2 loss, U denotes uniform sampling."
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,3,Experiments and Results,
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,3.1,Dataset,"We train and evaluate our method on C3VD [4], which provides 22 small video sequences captured from a real wide-angle colonoscopy at 1350 × 1080 resolution, moving inside 4 different colon phantoms. The videos include sequences from the colon cecum, descending, sigmoid, and transcending. Videos range from 61 to 1142 frames adding to 10.015 in total. We use the per-frame camera poses and set K/M = 0.03 in Eq. ( 5). For each scene, we construct a training set using one out of every 5 frames. We further remove and allocate one out of every 5 poses from the training set for evaluation. Frames not present in either the train or evaluation set are used for testing. We choose to sample both poses and depth information to allow our networks to interpolate more easily between potentially noisy labels and also create a dataset that resembles the sparse output of SfM algorithms."
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,3.2,Implementation Details,"Before training, we spatially re-scale and shift the 3D scene from each video sequence such that every point is enclosed within a cube with a length of two, centered at (0,0,0). Prescaling is important for the positional encoding module γ(x) to work properly. We configure positional encoding modules to compute 10 frequencies for each component of x and 4 frequencies for each component of d and o. We train models on images of 270 × 216 resolution to ignore both depth and RGB information outside a circle with a radius of 130 pixels centered at a principal point to avoid noise due to inaccuracies of the calibration model. We used Adam optimizer [8] with a batch size of 1024 for about 140K iterations for all sequences, with an initial learning rate of 5e-4 which we later multiply by 0.5 at 50% and 75% of training. We set the number of samples along the ray to N c = 64 and N f = 64. We configure positional encoding modules to compute 10 frequencies for each component of x and 4 frequencies for each component of d and o. Each model from this work is trained for around 30 min on 4 graphics cards from an NVIDIA DGX-A100."
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,3.3,Model Ablation Study,"We ablate our model showing the effects of conditioning NeRF on the light source location and supervising depth. To assess RGB reconstruction, we measure the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) between reconstructed and reference images at 270 × 216 resolution. We evaluate depth using the mean squared error (MSE) in the original dataset scale. For each metric, we report the average across all sequences together with the average standard deviation in Table 1. Conditioning NeRF based on the light source location (this work) produces better or equally good results compared to vanilla NeRF for all metrics. Both depth-supervised models, learn practically the same 3D representation but our model achieves better image quality metrics. Figure 3 shows renditions of each model of the ablation study for the same frame. Both non-depth-supervised models failed to capture correct geometry but were able to reconstruct accurate RGB information. Since non-depth-supervised networks were optimized only on color with weak geometric constraints, they learn floating artifacts in space which when viewed from a specific viewpoint, closely approximate the training samples. In contrast, depth-supervised networks learned a good representation of (3D) geometry while being able to reconstruct RGB images accurately. The depth-supervised NeRF model produces flare artifacts in the RGB image. That is because, during optimization, points are viewed from the same direction but at different distances from the light source. Our Full model is able to cope with illumination changes resulting in artifact-free images and accurate depth. Notably, most of the errors in depth for the depth-supervised approaches are located around sharp depth transitions. Such errors in depth may be a result of inaccuracies in calibration or imperfect camera pose information. Nevertheless, we argue that using RGB images and depth maps produced from our approach can be considered error-free because during inference the learned 3D geometry is fixed and consistent across all rendered views."
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,3.4,Data Generation,"We directly use our proposed model of the d4v2 C3VD scene from the ablation study to render novel views and show results in Fig. 4. In the second column, we show an image rendered from a previously unseen viewpoint, radially offset from the original camera path. Geometry is consistent and the RGB image exhibits the same photo-realistic properties observed in the training set. In the third column, we render a view by rotating a pose from the training trajectory. In the rotated view, the tissue is illuminated in a realistic way even though the camera never pointed in this direction in the training set. In the fourth column, we show an image rendered using a pinhole camera model whilst only fisheye images were used during training. This is possible because NeRF has captured a good representation of the underlying scene and image formation is done by projecting rays in space based on user-defined camera parameters. All the above demonstrate the ability of our method to render images from new, user-defined, trajectories and camera systems similar to synthetic data generation while producing photo-realistic images."
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,4,Conclusion,"We presented an approach for expanding existing VSLAM datasets by rendering RGB images and their associated depth maps from user-defined camera poses and models. To achieve this task, we propose a novel variant of NeRF, conditioned on the location of the light source in 3D space and incorporating sparse depth supervision. We evaluate the effects of our contributions on phantom datasets and show that our work effectively adapts NeRF techniques to the lighting conditions present in endoscopy. We further demonstrate the efficacy of our method by showing RGB images and their associated depth maps rendered from novel views of the target endoscopic scene. 3D information and conditioning NeRF based on the light source location made NeRF suitable for use in Endoscopy. Currently, our method assumes a static environment and requires accurate camera intrinsic and extrinsic information. Subsequent work can incorporate mechanisms to represent deformable scenes [13] and refine camera parameters during training [21]. Further research can investigate adopting the proposed model for data generation in other endoscopic scenes or implementing approaches to perform label propagation for categorical data [22]. We hope this work will mitigate the data scarcity issue currently present in the surgical domain and inspire the community to leverage and improve neural rendering techniques for data generation."
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,,Fig. 1 .,
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,,Fig. 2 .,
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,,Fig. 3 .,
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,,Fig. 4 .,
Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation,,Table 1 .,
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",1,Introduction,"Cranial damage is a common outcome of traffic accidents, neurosurgery, and warfare. Each year, thousands of patients require personalized cranial implants [2]. Nevertheless, the design and production of personalized implants are expensive and time-consuming. Nowadays, it requires trained employees working with computer-aided design (CAD) software [11]. However, one part of the design pipeline, namely defect reconstruction, can be directly improved by the use of deep learning algorithms [7,8].The problem can be formulated as a shape completion task and solved by dedicated neural networks. Its importance motivated researchers to organize two editions of the AutoImplant challenge, during which researchers proposed several unique contributions [7,8]. The winning contributions from the first [3] and second editions [18] proposed heavily-augmented U-Net-based networks and treated the problem as segmentation of missing skull fragment. They have shown that data augmentation is crucial to obtain reasonable results [19]. Other researchers proposed similar encoder-decoder approaches, however, without significant augmentation and thus limited performance [10,14]. Another group of contributions attempted to address not only the raw performance but also the computational efficiency and hardware requirements. One contribution proposed an RNN-based approach using 2-D slices taking into account adjacent slices to enforce the continuity of the segmentation mask [21]. The contribution by Li et al. has taken into account the data sparsity and proposed a method for voxel rearrangement in coarse representation using the high-resolution templates [6]. The method was able to substantially reduce memory usage while maintaining reasonable results. Another contribution by Kroviakov et al. proposed an approach based on sparse convolutional neural networks [5] using Minkowski engine [1]. The method excluded the empty voxels from the input volume and decreased the number of the required convolutions. The work by Yu et al. proposed an approach based on principal component analysis with great generalizability, yet limited raw performance [23]. Interestingly, methods addressing the computational efficiency could not compete, in terms of the reconstruction quality, with the resource-inefficient methods using dense volumetric representation [7].The current state-of-the-art solutions, even though they reconstruct the defects accurately, share some common disadvantages. First, they operate in the volumetric domain and require significant computational resources. The GPU memory consumption scales cubically with the volume size. Second, the most successful solutions do not take into account data sparsity. The segmented skulls are binary and occupy only a limited part of the input volume. Thus, using methods dedicated to 3-D multi-channel volumes is resource-inefficient. Third, the final goal of the defect reconstruction is to propose models ready for printing/manufacturing. Working with volumetric representation requires further postprocessing to transfer the reconstructed defect into a manufacturable model.Another approach, yet still unexplored, to cranial defect reconstruction is the use of deep networks dedicated to point clouds (PCs) processing. Since the introduction of PointNet [15] and PointNet++ [16], the number of contributions in the area of deep learning for PC processing exploded. Several notable contributions, like PCNet [26], PoinTr [24], AdaPoinTr [25], 3DSGrasp [12], MaS [9], have been proposed directly to the PC completion task. The goal of the PC completion is to predict a missing part of an incomplete PC.The problem of cranial defect reconstruction can be reformulated into PC completion which has several advantages. First, the representation is sparse, and thus requires significantly less memory than the volumetric one. Second, PCs are unordered collections and can be easily splitted and combined, enabling further optimizations. Nevertheless, the current PCs completion methods focus mostly on data representing object surfaces and do not explore large-scale PCs representing solid objects.In this work, we reformulate the problem from volumetric segmentation into PC completion. We propose a dedicated method to complete large-scale PCs representing solid objects. We extend the geometric aware transformers [24] and propose an iterative pipeline to maintain low memory consumption. We compare the proposed approach to the state-of-the-art networks for volumetric segmentation and PC completion. Our approach provides high-quality reconstructions while maintaining computational efficiency and good generalizability into previously unseen cases."
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",2,Methods,
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",2.1,Overview,"The input is a 3-D binary volume representing the defective skull. The output is a PC representing the missing skull fragment and (optionally) its meshed and voxelized representation. The processing pipeline consists of: (i) creating the PC from the binary volume, (ii) splitting the PC into a group of coarse PCs, (iii) calculating the missing PC by the geometric aware transformer for each group, (iv) merging the reconstructed coarse PCs, (v) optional voxelization and postprocessing for evaluation. The pipeline is shown in Fig. 1. "
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",2.2,Preprocessing,"The preprocessing starts with converting the binary volume to the PC. The coordinates of the positive voxels are created only from the voxels representing the skull. The PC is normalized to [0-1] range, randomly permuted, and split into N equal groups, where N is calculated based on the number of points in the input PC in a manner that each group contains 32768 randomly sampled input points and outputs 16384 points. Thus, the N depends on the number of positive voxels. The higher the resolution, the more groups are being processed. The number of outputs points is lower than the input points because we assumed that the defect is smaller than the skull."
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",2.3,Network Architecture -Point Cloud Completion Transformer,"We adapt and modify the geometry-aware transformers (PoinTr) [24]. The PoinTr method was proposed and evaluated on coarse PCs representing object surfaces. The full description of the PoinTr architecture is available in [24].We modify the network by replacing the FoldingNet [22] decoder working on 2-D grids with a folding decoder operating on 3-D representation. The original formulation deforms the 2-D grid into the surface of a 3-D object, while the proposed method focuses on solid 3-D models. Moreover, we modify the original k-NN implementation (with quadratic growth of memory consumption with respect to the input size) to an iterative one, to further decrease and stabilize the GPU memory consumption."
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",2.4,Objective Function,"We train the network using a fully supervised approach where the ground-truth is represented by PCs created from the skull defects. In contrast to other PC completion methods, we employ the Density Aware Chamfer Distance (DACD) [20]. The objective function enforces the uniform density of the output and handles the unpredictable ratio between the input/output PCs size. We further extend the DACD by calculating the distance between the nearest neighbours for each point and enforcing the distance to be equal. The final objective function is:where P r , P gt are the reconstructed and ground-truth PC respectively, S is the number of points in P rec , k is the number of nearest neighbours of point i, α is the weighting parameter. We apply the objective function to all PC ablation studies unless explicitly stated otherwise. The volumetric ablation studies use the soft Dice score. The traditional objective functions like Chamfer Distance (CD) [20], Extended Chamfer Distance (ECD) [22], or Earth Mover's Distance (EMD) [9] are not well suited for the discussed application. The CD/ECD provide suboptimal performance for point clouds with uniform density or a substantially different number of samples, tends to collapse, and results in noisy training. The EMD is more stable, however, explicitly assumes bijective mapping (requiring knowledge about the desired number of points) and has high computational complexity."
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",2.5,Iterative Completion,"The coarse PCs are processed by the network separately. Afterwards, the reconstructed PCs are combined into the final reconstruction. To improve the results, the process may be repeated M times with a different initial PC split and a small Gaussian noise added. The procedure improves the method's performance and closes empty holes in the voxelized representation. The optional multi-step completion is performed only during the inference.The iterative completion allows one to significantly reduce the GPU memory usage and the number of network parameters. The PCs are unordered collections and can be easily split and merged. There is no need to process large PCs in one shot, resulting in the linear growth of inference time and almost constant GPU memory consumption."
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",2.6,Postprocessing,"The reconstructed PCs are converted to mesh and voxelized back to the volumetric representation, mainly for evaluation purposes. The mesh is created by a rolling ball pivoting algorithm using the Open3D library [27]. The voxelization is also performed using Open3D by the PC renormalization and assigning positive values to voxels containing points in their interior. The voxelized representation is further postprocessed by binary closing and connected component analysis to choose only the largest volume. Then, the overlap area between the reconstructed defect and the defective input is subtracted from the reconstructed defect by logical operations."
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",2.7,Dataset and Experimental Setup,"We use the SkullBreak and SkullFix datasets [4] for evaluation. The datasets were used during the AutoImplant I and II challenges and enable comparison to other reconstruction algorithms. The SkullBreak dataset contains 114 highresolution skulls for training and 20 skulls for testing, each with 5 accompanying defects from various classes, resulting in 570 training and 100 testing cases. All volumes in the SkullBreak dataset are 512 × 512 × 512. The SkullFix dataset is represented by 100 training cases mostly located in the back of the skull with a similar appearance, and additional 110 testing cases. The volumes in the SkullFix dataset are 512 × 512 × Z where Z is the number of axial slices. The SkullBreak provides more heterogeneity while the SkullFix is better explored and enables direct comparison to other methods.We perform several ablation studies. We check the influence of the input physical spacing on the reconstruction quality, training time, and GPU memory consumption. Moreover, we check the generalizability by measuring the gap between the results on the training and the external testing set for each method. We compare our method to the methods dedicated to PC completion: (i) PCNet [26], (ii) PoinTr [24], (iii) AdaPoinTr [25], as well as to methods dedicated to volumetric defect reconstruction: (i) 3-D VNet, and (ii) 3-D Residual U-Net. Moreover, we compare the reconstruction quality to results reported by other state-of-the-art methods.We trained our network separately on the SkullBreak and SkullFix datasets. The results are reported for the external test set containing 100 cases for Skull-Break and 110 cases for the SkullFix datasets, the same as in the methods used for comparison. The models are implemented in PyTorch [13], trained using a single RTX GeForce 3090. We augment the input PCs by random permutation, cropping, rotation, and translation. The volumetric ablation studies use random rotation and translation with the same parameters as for the PCs. All the methods were trained until convergence. The hyperparameters are reported in the associated repository [17]."
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",3,Results,"The comparison in terms of the Dice coefficient (DSC), boundary Dice coefficient (BDSC), 95th percentile of Hausdorff distance (HD95), and Chamfer distance (CD), are shown in Table 1. Exemplary visualization, presenting both the PC and volumetric outcomes, is shown in Fig. 2. The results of the ablation studies showing the influence of the input size, generalizability, objective function, and the effect of repeating the iterative refinement are presented in Table 2.The results present that the quantitative outcomes of the proposed method are comparable to the state-of-the-art methods, however, with significantly lower GPU memory consumption that makes it possible to perform the reconstruction at the highest available resolution. The results are slightly worse when compared to the volumetric methods, however, significantly better than other PC-based approaches.Table 1. Quantitative results on the SkullBreak and SkullFix datasets. The final results are reported for original resolution using the DACD + kNN objective function and 3 iterative refinements (-denotes that results were not reported). The methods used for comparison are reported for the most successful setup (see Table 2). "
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",4,Discussion,"The reconstruction quality of the method is comparable to the volumetric networks, as shown in Table 1. Meanwhile, the proposed method takes into account the data sparsity, does not require significant computational resources, and scales well with the input size. The proposed method has good generalizability. The gap between the training and testing set is negligible, unlike the volumetric methods that easily overfit and require strong augmentation for practical use. The DACD, as well as the proposed extension, improve the reconstruction quality compared to the CD or ECD by taking into account the uniformity of the expected PC. The original PC completion methods do not scale well with the increase of PC size. The possible reason for this is connected with the noisy kNN graph construction when dealing with large PCs and increasing the number of neighbours is unacceptable from the computational point of view. The proposed method has almost constant memory usage, independent of the input shape, in contrast to both the volumetric methods and PC completion methods without the iterative approach. Interestingly, the proposed method outperforms other methods taking into account the data sparsity. The inference speed is slightly lower than for the volumetric methods (several seconds), however, this application does not require real-time processing and anything in the range of seconds is acceptable. However, the required memory consumption is crucial to ensure that the method can be eventually applied in clinical practice. The disadvantages of the proposed algorithm are connected to long training time, noise at the object boundaries, and holes in the voxelized output. The FoldingNet-based decoder requires a significant number of iterations to converge, thus resulting in training time comparable or even longer than the volumetric methods. Moreover, the voxelization of PCs results in noisy edges and holes that require further morphological postprocessing. What is important, the method has to be extended by another algorithm responsible for converting the reconstruction into implantable implant before being ready to be used in clinical setup.In future work, we plan to further reformulate the problem and, similarly to Kroviakov et al. [5], use only the skull contours. Since the ultimate goal is to propose models ready for 3-D printing, the interior of the skull defect is not required to create the mesh and STL/OBJ file. Another research direction is connected to the PC augmentation to further increase the network generalizability since it was shown that heavy augmentation is crucial to obtain competitive results [3,18]. Moreover, it is challenging to perform the qualitative evaluation of the context of clinical need and we plan to perform evaluation including clinical experts in the future research.To conclude, we proposed a method for cranial defect reconstruction by formulating the problem as the PC completion task. The proposed algorithm achieves comparable results to the best-performing volumetric methods while requiring significantly less computational resources. We plan to further optimize the model by working directly at the skull contour and heavily augmenting the PCs."
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",,Fig. 1 .,
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",,Fig. 2 .,
"High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers",,Table 2 .,
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,1,Introduction,"Foundation models pre-trained on large-scale data have recently showed success in various downstream tasks on medical images including classification [9], detection [33], and segmentation [31]. However, medical data have various imaging modalities, and clinical data collection is expensive. It is arguable that a specific foundation model trained on some certain type of data is useful at the moment. In this paper, we focus on endoscopic video, which is a routine imaging modality and increasingly studied in gastrointestinal disease diagnosis, minimally invasive procedure and robotic surgery. Having an effective foundation model is promising to facilitate downstream tasks that necessitate endoscopic video analysis.Existing work on foundation models for medical tasks, such as X-ray diagnosis [4] and radiology report generation [20,21], involves pre-training on large-scale image-text pairs and relies on large language models to learn cross-modality features. However, since clinical routines for endoscopy videos typically do not involve text data, a pure image-based foundation model is currently more feasible. To this end, we develop a video transformer, based on ViT B/16 [8], containing 121M parameters, which serves as the foundation model backbone for our video data. We note that a similarly scaled foundation model in recent work [33] based on Swin UNETR [11] with 62M parameters has been successfully employed for CT scans. This would indicate that our video transformer could have sufficient capacity to model the rich spatial-temporal information of endoscopy videos.To learn rich spatial-temporal information from endoscopy video data [12], our Endo-FM is pre-trained via a self-supervised manner by narrowing the gap between feature representations from different spatial-temporal views of the same video. These views are generated to address the variety of context information and motions of endoscopy videos. Drawing inspiration from self-supervised vision transformers [6,29], we propose to pre-train the model via a teacherstudent scheme. Under this scheme, the student is trained to predict (match) the teacher's output in the latent feature space. In other words, given two spatialtemporal aware views from the same video, one view processed by the teacher is predicted by another one processed by the student to learn the spatial-temporal information. Therefore, designing effective and suitable matching strategies for different spatial-temporal views from the same endoscopy video is important.In this paper, we propose Endo-FM, a novel foundation model designed for endoscopic video analysis. First, we build a video transformer based on ViT [8] to capture long-range spatial and temporal dependencies, together with dynamic spatial-temporal positional encoding designed for tackling input data with diverse spatial sizes and temporal frame rates. Second, Endo-FM is pretrained under a teacher-student scheme via spatial-temporal matching on diverse video views. Specifically, we create various spatial-temporal aware views differing in spatial sizes and frame rates for an input video clip. Both teacher and student models process these views of a video and predict one view from another in the latent feature space. This enables Endo-FM to learn spatial-temporal invariant (to view, scale, and motion) features that are transferable across different endoscopic domains and disease types while retaining discriminative features that are specific to each context. We construct a large-scale endoscopic video dataset by combining 9 public and a new private collected dataset from Baoshan Branch of Renji Hospital in Shanghai, China, with over 33K video clips with up to 5 million frames. Our pre-trained Endo-FM can be easily applied to various downstream tasks by serving as the backbone. Experimental results on 3 different types of downstream tasks demonstrate the effectiveness of Endo-FM, surpassing the current state-of-the-art self-supervised pre-training and adapter-based transfer learning methods by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6% Dice, and 9.9% F1 for classification, segmentation, and detection)."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,2,Method,"To begin with, we build a video transformer as the architecture of our Endo-FM (Sect. 2.1). Then, we propose a novel self-supervised spatial-temporal matching scheme (Sect. 2.2). Finally, we describe the overall training objective and specifics in Sect. 2.3. An overview of our method is shown in Fig. 1."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,2.1,Video Transformer for Spatial-Temporal Encoding,"We build a video transformer to encode input endoscopic video. The spatial and temporal attention mechanisms in our model capture long-range dependencies across both spatial and temporal dimensions, with a larger receptive field than conventional convolutional kernels [22]. Our model is built using 12 encoder blocks, equipped with space-time attention [3]. Specifically, given an endoscopic video clip X ∈ R T×3×H×W as input, consisting of T frames with size H×W , each frame in X is divided into N = HW/P 2 patches of size P×P , and these patches are then mapped into N patch tokens. Thus, each encoder block processes N patch (spatial) and T temporal tokens. Given the intermediate token z m ∈ R D for a patch from block m, the token computation in the next block is as follows:where MHSA denotes multi-head self-attention, LN denotes LayerNorm [1], and MLP denotes multi-layer perceptron. Our model also includes a learnable class token, representing the global features learned by the model along the spatial and temporal dimensions. For pre-training, we use a MLP to project the class token from the last encoder block as the feature f of X.Different from static positional encoding in ViT [8], we design a dynamic spatial-temporal encoding strategy to help our model tackle various spatialtemporal views with different spatial sizes and frame rates (Sect. 2.2). Specifically, We fix the spatial and temporal positional encoding vectors to the highest resolution of the input view for each dimension, making it easy to interpolate for views with smaller spatial size or lower temporal frame rate. These spatial and temporal positional encoding vectors are added to the corresponding spatial and temporal tokens. Such dynamic strategy ensures that the learned positional encoding is suitable for downstream tasks with diverse input sizes."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,2.2,Self-supervised Pre-train via Spatial-Temporal Matching,"Considering the difficulties of tackling the context information related with lesions, tissues, and dynamic scenes in endoscopic data, we pre-train Endo-FM to be robust to such spatial-temporal characteristics. Inspired by self-supervised vision transformers [6], the pre-training is designed in a teacher-student scheme, where the student is trained to match the teacher's output. To achieve this, given an input video X, we create two types of spatial-temporal views serving as the model inputs: global and local views, as shown in Fig. 1. The global views {v i g ∈R T i g ×3×Hg×Wg } G i=1 are generated by uniformly sampling X with different frame rates, and the local ones {v j l ∈R T j l ×3×H l ×W l } L j=1 are generated by uniformly sampling video frames with different frame rates from a randomly cropped region of X (T l ≤ T g ). During pre-training, the global views are fed into both teacher and student, and the local ones are only fed into the student. The model output f is then normalized by a softmax function with a temperature τ to obtain the probability distribution p=softmax(f /τ ). In the following, we design two matching schemes with respect to the difficulties of tackling endoscopy videos.Cross-View Matching. Different from image-based pre-training [33], our video-oriented pre-training is designed to capture the relationships between different spatial-temporal variations. Specifically, the context information presented in different frames of the same endoscope video can vary under two key factors: 1) the proportion of tissue and lesions within the frame, and 2) the presence or absence of lesion areas. To address these, we employ a cross-view matching approach where the target global views processed by the teacher ({p) are predicted from the online local views processed by the student ({p s v j l } L j=1 ). By adopting this strategy, our model learns high-level context information from two perspectives: 1) spatial context in terms of the possible neighboring tissue and lesions within a local spatial crop, and 2) temporal context in terms of the possible presence of lesions in the previous or future frames of a local temporal crop. Thus, our method effectively addresses the proportion and existence issues that may be encountered. We minimize the following loss for cross-view matching:(2) Dynamic Motion Matching. In addition to the proportion and existence issues of lesions, a further challenge arises from the inherent dynamic nature of the scenes captured in endoscopy videos. The speeds and ranges of motion can vary greatly across different videos, making it difficult to train a model that is effective across a wide range of dynamic scenarios. The previous model [27] learned from clips with fixed frame rate can not tackle this issue, as clips sampled with various frame rates contain different motion context information (e.g., fast v.s. slow scene changing) and differ in nuanced tissue and lesions. To address this challenge, our approach involves motion modeling during pretraining under dynamic endoscope scenes by predicting a target global view (p t ) processed by the student. Moreover, by predicting the nuanced differences of tissue and lesions in a view with a high frame rate from another with a low frame rate, the model is encouraged to learn more comprehensive motion-related contextual information. The dynamic motion difference among global view pairs is minimized bywhere 1[•] is an indicator function."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,2.3,Overall Optimization Objective and Pre-training Specifics,"The overall training objective for Endo-FM is L pre-train = L cv + L dm . Centering and sharpening schemes [6] are incorporated to the teacher outputs. To prevent the problem of the teacher and student models constantly outputting the same value during pre-training, we update the student model θ through backpropagation, while the teacher model φ is updated through exponential moving average (EMA) using the student's weights. This is achieved by updating the teacher's weights as φ t ← αφ t-1 + (1α)θ t at each training iteration t. Here, α is a momentum hyper-parameter that determines the updating rate. Except for the challenges posed by the issues of size proportion, existence, and dynamic scenes in Sect. 2.2, we have also observed that the appearance of  endoscope videos is highly diverse. These videos are captured using different surgical systems and in a wide range of environmental conditions [10]. To address this variability, we apply temporally consistent spatial augmentations [27] to all frames within a single view. Our augmentation approach includes random horizontal flips, color jitter, Gaussian blur, solarization, and so on, which enhances the robustness and generalizability of Endo-FM.For Endo-FM, we set the patch size P as 16 and embedding dimension D as 768. We create G = 2 global views and L = 8 local views for every input endoscopy video, where T g ∈ [8,16] and T l ∈ [2,4,8,16]. The spatial sizes of global and local views are 224×224 and 96×96, respectively. The MLP head projects the dimension of class token to 65536. The temperature hyper-parameters are set as τ t = 0.04 and τ s = 0.07. The EMA update momentum α is 0.996. The training batch size is 12 with AdamW [17] optimizer (learning rate 2e-5, weight decay 4e-2). The pre-training is finished with 30 epochs with a cosine schedule [16]."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,3,Experiment,
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,3.1,Datasets and Downstream Setup,"We collect all possible public endoscope video datasets and a new one from Baoshan Branch of Renji Hospital for pre-training. As shown in Table 1, these public datasets are provided by world-wide research groups [5,13,18,19,30] and previous EndoVis challenge [23], covering 3 endoscopy protocols and 10+ types of diseases. We process the original videos into 30fps short clips with a duration of 5 s on average. We evaluate our pre-trained Endo-FM on three downstream tasks: disease diagnosis (PolypDiag [32]), polyp segmentation (CVC-12k [2]), and detection (KUMC [15]). The detailed information of three downstream datasets is shown in Table 1. The example frames of the 10 datasets are shown in Fig. 2.For downstream fine-tuning, we utilize the following setup: 1) PolypDiag: A randomly initialized linear layer is appended to our pre-trained Endo-FM. We sample 8 frames with spatial size 224 × 224 for every video as the input and train for 20 epochs. 2) CVC-12k: A TransUNet equipped with Endo-FM as the backbone is implemented. We resize the spatial size as 224×224 and train for 150 epochs. 3) KUMC: we implement a STFT [34] with our pre-trained model as backbone for generating feature pyramid. We resize the spatial size as 640×640 and train for 24k iterations. We report F1 score for PolypDiag, Dice for CVC-12k, and F1 score for KUMC."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,3.2,Comparison with State-of-the-Art Methods,"We compare our method with recent SOTA video-based pre-training methods, including the TimeSformer [3] introduces spatial-temporal attention for video processing, the CORP [12] presents a self-supervised contrast-and-order representation framework, the FAME [7] proposes a foreground-background merging scheme, the ProViCo [25] applies a self-supervised probabilistic video contrastive learning strategy, the VCL [26] learns the static and dynamic visual concepts, and the ST-Adapter [24] adapts the CLIP [28] by a depth-wise convolution. We also train our model from scratch to serve as a baseline. The same experimental setup is applied to all the experiments for fair comparisons.Quantitative comparison results are shown in Table 2. We can observe that the scratch model shows low performance on all 3 downstream tasks, especially for segmentation. Compared with training from scratch, our Endo-FM achieves +7.2% F1, +20.7% Dice, and +10.6% F1 improvements for classification, seg-mentation, and detection tasks, respectively, indicating the high effectiveness of our proposed pre-training approach. Moreover, our Endo-FM outperforms all SOTA methods, with +3.1% F1, +4.8% Dice, and +5.5% F1 boosts for the 3 downstream tasks over the second-best. Such significant improvements are benefited from our specific spatial-temporal pre-training designed for endoscopy videos to tackle the complex context information and dynamic scenes. Meanwhile, Endo-FM requires less pre-training time than SOTA pre-training methods, except the lighter but much worse ST-Adapter [24]."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,3.3,Analytical Studies,"Without loss of generality, we conduct ablation studies on polyp diagnosis task from 3 aspects: 1) components analysis of our pre-training method; 2) varying combinations of global and local views in spatial-temporal matching; 3) varying the construction of global and local views. Components Analysis. We first study each component in our approach, as shown in Fig. 3(a). Here, ""w/L cv (spat.)"" and ""w/L cv (temp.)"" indicate that only spatial and temporal sampling are used for generating the local views. We can learn that both spatial and temporal sampling for local views can help improve the performance and their combination produces a plus, yielding +4.3% F1 improvement. Furthermore, our proposed dynamic matching scheme boosts the performance to 89.7%, demonstrating the importance of capturing the motion related context information from dynamic scenes. Additionally, the performance is further improved with video augmentations from 89.7% to 90.7%. Spatial-Temporal Matching Combinations. We further investigate the effects of combinations of global and local views in spatial-temporal matching, as depicted in Fig. 3(b). Here, the notation v l → v g represents the prediction of v g from v l , and vice versa. It indicates that joint prediction scenarios, where we predict v g from both v l (cross-view matching) and v g (dynamic motion matching), result in optimal performance. This trend can be attributed to the fact that joint prediction scenarios allow for a more comprehensive understanding of the context in complex endoscopy videos, which is lacking in individual cases."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,,Z,
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,,Fig. 1 .,
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,,,
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,,Fig. 2 .,
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,,Fig. 3 .,
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,,Table 1 .,
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,,Table 2 .,
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,,Acknowledgements,". This work was supported in part by Shenzhen Portion of Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone under HZQB-KCZYB-20200089, in part by Science, Technology and Innovation Commission of Shenzhen Municipality Project No. SGDX20220530111201008, in part by Hong Kong Innovation and Technology Commission Project No. ITS/237/21FP, in part by Hong Kong Research Grants Council Project No. T45-401/22-N, and in part by the Action Plan of Shanghai Science and Technology Commission [21SQBS02300]."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,,Construction of Global and Local,"Views. We conduct a further analysis of the strategies for constructing global (G ∈ [1,2]) and local views (L ∈ [1,2,4,6,8]). We vary the number of global and local views, and the length of local views (T l ), as depicted in Fig. 3(c), and Fig. 3(d). We find that incorporating more views and increasing the length variations of local views yields better performance. For ""G = 1"", we still create 2 global views for L dm but only consider the longer one for L cv . These improvements stem from the spatial-temporal change invariant and cross-video discriminative features learned from the diverse endoscopy videos."
Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train,4,Conclusion and Discussion,"To the best of our knowledge, we develop the first foundation model, Endo-FM, Which is specifically designed for analyzing endoscopy videos. Endo-FM is built upon a video transformer to capture rich spatial-temporal information and pre-trained to be robust to diverse spatial-temporal variations. A largescale endoscope video dataset with over 33K video clips is constructed. Extensive experimental results on 3 downstream tasks demonstrate the effectiveness of Endo-FM, significantly outperforming other state-of-the-art video-based pretraining methods, and showcasing its potential for clinical application.Regarding the recent SAM [14] model, which is developed for segmentation task, we try to apply SAM for our downstream task CVC-12k with the same fine-tuning scheme as Endo-FM. The experimental results show that SAM can achieve comparable performance with our Endo-FM for the downstream segmentation task. Considering that SAM is trained with 10x samples, our domain Endo-FM is considered to be powerful for endoscopy scenarios. Moreover, besides segmentation, Endo-FM can also be easily applied to other types of tasks including classification and detection. Therefore, we envision that, despite existence of general-purpose foundation models, Endo-FM or similar domain-specific foundation models will be helpful for medical applications."
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,1,Introduction,"Despite that deep learning models have shown success in surgical data science to improve the quality of surgical intervention [20][21][22], such as intelligent workflow analysis [7,13] and scene understanding [1,28], research on higher-level cognitive assistance for surgery still remains underexplored. One essential task is supporting decision-making on dissection trajectories [9,24,29], which is challenging yet crucial for ensuring surgical safety. Endoscopic Submucosal Dissection (ESD), a surgical procedure for treating early gastrointestinal cancers [2,30], involves multiple dissection actions that require considerable experience to determine the optimal dissection trajectory. Informative suggestions for dissection trajectories can provide helpful cognitive assistance to endoscopists, for mitigation of intraoperative errors, reducing risks of complications [15], and facilitating surgical skill training [17]. However, predicting the desired trajectory for future time frames based on the current endoscopic view is challenging. First, the decision of dissection trajectories is complicated and depends on numerous factors such as safety margins surrounding the tumor. Second, dynamic scenes and poor visual conditions may further hamper scene recognition [27]. To date, there is still no work on data-driven solutions to predict such dissection trajectories, but we argue that it is possible to reasonably learn this skill from expert demonstrations based on video data.Imitation learning has been widely studied in various domains [11,16,18] with its good ability to learn complex skills, but it still needs adaptation and improvement when being applied to learn dissection trajectory from surgical data. One challenge arises from the inherent uncertainty of future trajectories. Supervised learning such as Behavior Cloning (BC) [3] tends to average all possible prediction paths, which leads to inaccurate predictions. While advanced probabilistic models are employed to capture the complexity and variability of dissection trajectories [14,19,25], how to ensure reliable predictions across various surgical scenes still remains a great challenge. To overcome these issues, implicit models are emerging for policy learning, inspiring us to rely on implicit Behavior Cloning (iBC) [5], which can learn robust representations by capturing the shared features of both visual inputs and trajectory predictions with a unified implicit function, yielding superior expressivity and visual generalizability. However, these methods still bear their limitations. For instance, approaches leveraging energy-based models (EBMs) [4][5][6]12] suffer from intensive computations due to reliance on the Langevin dynamics, which leads to a slow training process. In addition, the model performance can be sensitive to data distribution and the noise in training data would result in unstable trajectory predictions.In this paper, we explore an interesting task of predicting dissection trajectories in ESD surgery via imitation learning on expert video data. We propose Implicit Diffusion Policy Imitation Learning (iDiff-IL), a novel imitation learning approach for dissection trajectory prediction. To effectively model the surgeon's behaviors and handle the large variation of surgical scenes, we leverage implicit modeling to express expert dissection skills. To address the limitations of inefficient training and unstable performance associated with EBM-based implicit policies, we formulate the implicit policy using an unconditional diffusion model, which demonstrates remarkable ability in representing complex high-dimensional data distribution for videos. Subsequently, to obtain predictions from the implicit policy, we devise a conditional action inference strategy with the guidance of forward-diffusion, which further improves the prediction accuracy. For experimental evaluation, we collected a surgical video dataset of ESD procedures, and preprocessed 1032 short clips with dissection trajectories labelled. Results show that our method achieves superior performances in different contexts of surgical scenarios compared with representative popular imitation learning methods."
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,2,Method,"In this section, we describe our approach iDiff-IL, which learns to predict the dissection trajectory from expert video data using the implicit diffusion policy. An overview of our method is shown in Fig. 1. We first present the formulation of the task and the solution with implicit policy for dissection trajectory learning. Next, we present how to train the implicit policy as an unconditional generative diffusion model. Finally, we show the action inference strategy with forward-diffusion guidance which produces accurate trajectory predictions with our implicit diffusion policy."
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,2.1,Implicit Modeling for Surgical Dissection Decision-Making,"In our approach, we formulate the dissection trajectory prediction to an imitation learning from expert demonstrations problem, which defines a Markov Decision Process (MDP) M = (S, A, T , D), comprising of state space S, action set A, state transition distribution T , and expert demonstrations D. The goal is to learn a prediction policy π * (a|s) from a set of expert demonstrations D. The input state of the policy is a clip of video frames s = {I t-L+1 , I t-L+2 , . . . , I t }, I t ∈ R H×W ×3 and the output is an action distribution of a sequence of 2D coordinates a = {y t+1 , y t+2 , ..., y t+N }, y t ∈ R 2 indicating the future dissection trajectory projected to the image space.In order to obtain the demonstrated dissection trajectories from the expert video data, we first manually annotate the dissection trajectories on the video frame according to the moving trend of the instruments observed from future frames, then create a dataset D = {(s, a) i } M i=0 containing M pairs of video clip (state) and dissection trajectory (action).To precisely predict the expert dissection behaviors and effectively learn generalizable features from the expert demonstrations, we use the implicit model as our imitation policy. Extending the formulation in [5], we model the dissection trajectory prediction policy to a maximization of the joint state-action probability density function arg max a∈A p θ (s, a) instead of an explicit mapping F θ (s). The optimal action is derived from the policy distribution conditioned on the state s, and p θ (s, a) represents the joint state-action distribution.To learn the implicit policy from the demonstrations, we adopt the Behavior Cloning objective which is to essentially minimize the Kullback-Leibler (KL) divergence between the learning policy π θ (a|s) and the demonstration distribution D, also equivalent to maximize the expected log-likelihood of the joint state-action distribution, as shown:(In this regard, the imitation of surgical dissection decision-making is converted to a distribution approximation problem."
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,2.2,Training Implicit Policy as Diffusion Models,"Approximating the joint state-action distribution in Eq. 1 from the video demonstration data is challenging for previous EBM-based methods. To address the learning of implicit policy, we rely on recent advances in diffusion models. By representing the data using a continuous thermodynamics diffusion process, which can be discretized into a series of Gaussian transitions, the diffusion model is able to express complex high-dimensional distribution with simple parameterized functions. In addition, the diffusion process also serves as a form of data augmentation by adding a range of levels of noise to the data, which guarantees a better generalization in high-dimensional state space.As shown in Fig. 1 (a), the diffusion model comprises a predefined forward diffusion process and a learnable reverse denoising process. The forward process gradually diffuses the original data x 0 = (s, a), to a series of noised data {x 0 , x 1 , • • • , x T } with a Gaussian kernel q(x t |x t-1 ), where T denotes the diffusion step. In the reverse process, the data is recovered via a parameterized Gaussian p θ (x t-1 |x t ) iteratively. With the reverse process, the joint state-action distribution in the implicit policy can be expressed as:The probability of the noised data x t in forward diffusion process is a Gaussian distribution expressed as q(x t |x 0 ) = N (x t , √ α t x 0 , (1α t )I ), where α t is a scheduled variance parameter, which can be referred from [10], and I is an identity matrix. The trainable reverse transition is a Gaussian distribution as well, whose posterior isand Σ θ (x t , t) are the means and the variances parameterized by a neural network.To train the implicit diffusion policy, we maximize the log-likelihood of the state-action distribution in Eq. 1. Using the Evidence Lower Bound (ELBO) as the proxy, the likelihood maximization can be simplified to a noise prediction problem, more details can be referred to [10]. Noise prediction errors for the state and the action are combined using a weight γ ∈ [0, 1] as the following:where s and a are sampled from N (0, I s ), N (0, I a ) respectively. To better process features from video frames and trajectories of coordinates, we employ a variant of the UNet as the implicit diffusion policy network, where the trajectory information is fused into feature channels via MLP embedding layers. Then the trajectory noise is predicted by an MLP branch at the bottleneck layer."
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,2.3,Conditional Sampling with Forward-Diffusion Guidance,"Since the training process introduced in Sect. 2.2 is for unconditional generation, the conventional sampling strategy through the reverse process will predict random trajectories in expert data. An intuitive way to introduce the condition into the inference is to input the video clip as the condition state s * to the implicit diffusion policy directly, then only sample the action part. But there is a mismatch between the distribution of the state s * and the s t in the training process, which may lead to inaccurate predictions. Hence, we propose a sampling strategy to correct such distribution mismatch by introducing the forward-process guidance into the reverse sampling procedure.Considering the reverse process of the diffusion model, the transition probability conditioned by s * can be decomposed as:where x t = (s t , a t ), p θ (x t-1 |x t ) denotes the learned denoising function of the implicit diffusion model, and q(s t |s * ) represents a forward diffusion process from the condition state to the t-th diffused state. Therefore, we can attain conditional sampling via the incorporation of forward-process guidance into the reverse sampling process of the diffusion model. The schematic illustration of our sampling approach is shown in Fig. 1 (b). At the initial step t = T , action a T is sampled from a pure Gaussian noise, whereas the input state s T is diffused from the input video clip s * through a forward-diffusion process. At the t-th step of the denoising loop, the action input a t comes from the denoised action from the last time step, while the visual inputs s t are still obtained from s * via the forward diffusion process. The above forward diffusion process and the denoising step are repeated till t = 0. The final action â0 is the prediction from the implicit diffusion policy. The deterministic action can be obtained by taking the most probable samples during the reverse process."
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,3,Experiments,
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,3.1,Experimental Dataset and Evaluation Metrics,"Dataset. We evaluated the proposed approach on a dataset assembled from 22 videos of ESD surgery cases, which are collected from the Endoscopy Centre of the Prince of Wales Hospital in Hong Kong. All videos were recorded via Olympus microscopes operated by an expert surgeon with over 15 years of experience in ESD. Considering the inference speed, we downsampled the original videos to 2FPS frames which are resized to 128 × 128 in resolution. The input state is a 1.5-s length video clip containing 3 consecutive frames, and the expert dissection trajectory is represented by a 6-point polyline indicating the tool's movements in future 3 s. We totally annotated 1032 video clips, which contain 3 frames for each clip. We randomly selected 742 clips from 20 cases for training, consisting of 2226 frames, where 10% of these are for validation. The remaining 290 clips (consisting of 970 frames) were used for testing.Experiment Setup. First, to study how the model performs on data within the same surgical context as the training data, we define a subset, referred as to the ""in-the-context"" testing set, which consists of consecutive frames selected from the same cases as included in the training data. Second, to assess the model's ability to generalize to visually distinct scenes, we created an ""out-ofthe-context"" testing set that is composed of video clips sampled from 2 unseen surgical cases. The sizes of these two subsets are 224 and 66 clips, respectively. Evaluation Metrics. To evaluate the performance of the proposed approach, we adopt several metrics, including commonly used evaluation metrics for trajectory prediction as used in [23,26], including Average Displacement Error (ADE), which respectively reports the overall deviations between the predictions and the ground truths, and Final Displacement Error (FDE) describing the difference from the moving target by computing the L2 distance between the last trajectory points. Besides, we also use the Fréchet Distance (FD) metric, to indicate the geometrical similarity between two temporal sequences. Pixel errors are used as units for all metrics, while the input images are in 128 × 128 resolution."
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,3.2,Comparison with State-of-the-Art Methods,"To evaluate the proposed approach, we have selected popular baselines and stateof-the-art methods for comparison. We have chosen the fully supervised method, Behavior Cloning, as the baseline, which is implemented using a CNN-MLP network. In addition, we have included iBC [5], an EBM-based implicit policy learning method and MID [8], a diffusion-based trajectory prediction approach, as comparison state-of-the-art approaches.As shown in Table 1, our method outperforms the comparison approaches in both ""in-the-context"" and ""out-of-the-context"" scenarios on all metrics. Compared with the diffusion-based method MID [8], our iDiff-IL is more effective in predicting long-term goals, particularly in the ""out-of-the-context"" scenes, with the evidence of 2.18 error reduction on FDE. For iBC [5], the performance did not meet our expectations and was even surpassed by the baseline. This exhibits the limitations of EBM-based methods in learning visual representations from complex endoscopic scenes. The superior results achieved by our method demonstrate the effectiveness of the diffusion model in learning the implicit policy from the expert video data. In addition, our method can learn generalizable dissection skills by exhibiting a lower standard deviation of the prediction errors compared to the BC, which severely suffers from over-fitting to the training data. The qualitative results are presented in Fig. 2. We selected three typical scenes in ESD surgery (i.e., submucosa dissection, mucosa dissection and mucosa incision), and showed the predictions of iDiff-IL accompanying the ground truth trajectories. From the results, our method can generate reasonable visual guidance aligning with the expert demonstrations on both evaluation sets. "
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,3.3,Ablation Study,"Implicit Modeling. First, we examined the importance of using implicit modeling as the policy representation. We simulated the explicit form of the imitation policy by training a conditional diffusion model whose conditional input is a video clip. According to the bar charts in Fig. 3, the explicit diffusion policy shows a performance drop for both evaluation sets on ADE compared with the implicit form. The implicit modeling makes a more significant contribution in predicting within the ""in-the-context"" scenes, suggesting that the implicit model excels at capturing subtle changes in surgical scenes. While our method improves marginally compared with the explicit form on the ""out-of-the-context"" data, exhibiting a slighter over-fitting with a lower standard deviation.Forward-Diffusion Guidance. We also investigated the necessity of the forward-diffusion guidance in conditional sampling for prediction accuracy. We remove the forward-diffusion guidance during the action sampling procedure so that the condition state is directly fed into the policy while sampling actions through the reverse process. As shown in Fig. 3, the implicit diffusion policy benefits more from the forward-diffusion guidance in the ""in-the-context"" scenes, achieving an improvement of 0.33 on ADE. When encountered with the unseen scenarios in ""out-of-the-context"" data, the performance improvement of such inference strategy is marginal.Value of Synthetic Data. Since the learned implicit diffusion policy is capable of generating synthetic expert dissection trajectory data, which can potentially reduce the expensive annotation cost. To better explore the value of such synthetic expert data for downstream tasks, we train the baseline model with the generated expert demonstrations. We randomly generated 9K video-trajectory pairs by unconditional sampling from the implicit diffusion policy. Then, we train the BC model with different data, the pure expert data (real), synthetic data only (synt) and the mixed data with the real and the synthetic (mix).The table in Fig. 3 shows the synthetic data is useful as the augmented data for downstream task learning. "
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,4,Conclusion,"This paper presents a novel approach on imitation learning from expert video data, in order to achieve dissection trajectory prediction in endoscopic surgical procedure. Our iDiff-IL method utilizes a diffusion model to represent the implicit policy, which enhances the expressivity and visual generalizability of the model. Experimental results show that our method outperforms state-of-the-art approaches on the evaluation dataset, demonstrating the effectiveness of our approach for learning dissection skills in various surgical scenarios. We hope that our work can pave the way for introducing the concept of learning from expert demonstrations into surgical skill modelling, and motivate future exploration on higher-level cognitive assistance in computer-assisted intervention."
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,,Fig. 1 .,
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,,Fig. 2 .,
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,,Fig. 3 .,
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,,Table 1 .,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,1,Introduction,"Cochlear implantations (CIs) are considered to be a standard treatment in case of individuals with severe-to-profound hearing loss [1]. During CI surgical procedure an electrode array (EA) is implanted in the cochlea for stimulating the auditory nerve. Along the cochlear duct length, the neural pathways are arranged in a tonotopic manner by decreasing frequency [2]. Naturally, these pathways get activated according to their characteristic frequencies present in the incoming sound. After implantation, the electrode arrays are used to stimulate the nerve pathways and induce hearing sensation [3].During the insertion process, one complication surgeons aim to avoid is called ""tip fold-over,"" where the tip of the array curls in an irregular manner inside the cochlear volume resulting in array folding [4]. This can occur when the tip of the electrode array gets stuck within an intracochlear cavity as the surgeon threads the array into the cochlea, largely blind to the intra-cochlear path of the array and with little tactile feedback available to indicate the tip of the array is folding [5]. Therefore, further pushing the base of the EA into the cochlea results in folding of the array (shown in Fig. 1) [5]. Tip fold-over can result in many complications which include trauma, damage to residual hearing and poor positioning of the EA inside cochlea, which ultimately leads to poor hearing restoration for the patient. If intra-operative detection of a tip fold-over is possible, the surgeon can address it through re-implantation of the electrode array [6]. In addition, post-operative detection of minor fold-over can help audiologists to deactivate the affected electrodes to attempt to reach a more satisfactory hearing outcome [4].Although most CI centers do not currently attempt to detect tip foldovers, the current standard approach among sites that do is visual inspection of intraoperative fluoroscopy. However, these identification methods require experience to align the view optimally and limit the radiation exposure during the fluoroscopy [7].  [7][8][9]. In some studies, it is reported that the intraoperative electrophysiological measures, such as NRT or EcochG, sometimes fail to identify the tip fold-over cases [9,10]. Pile et al. [11] developed a robotic system for tip fold-over detection where the support vector machine classifier is used on the EA insertion force profile. This approach is associated with robot assisted insertion techniques. This broad body of work emphasizes the need for an accurate approach for intra and/or post operative fold-over detection. Therefore, the goal of this study is to develop an approach to detect tip fold-overs in cone beam or conventional CT images using state-of-the-art deep neural network-based image analysis.As tip fold-over cases are reported to be rare, it would be difficult to acquire a substantial number of cases for fully supervised training of any data-driven method. Zuniga et al. [4] studied CI surgeries in 303 ears and reported 6 tip fold-over cases (less than 2%). Dhanasingh et al. [5] investigated 3177 CI recipients' cases from 13 studies and reported 50 tip fold-over cases (1.57%). Only 0.87% (15 cases) tip fold-over cases were reported among 1722 CI recipients according to Gabrielpillai et al. [12]. Dhanasingh et al. [5] analyzed 38 peer reviewed publications and reported that the rate of tip fold-over with certain types of arrays might be as high as 4.7%. Data scarcity thus makes it difficult to curate a balanced training dataset of tip fold-over cases. Deep learning methods are the current state-of-the-art in medical image analysis, including image classification tasks. Numerous approaches have been proposed for using 3D networks to solve image classification tasks, e.g. [13][14][15]. Multi-tasking neural networks have also been used for simultaneous medical image segmentation and classification tasks. These networks have shared layers from the input side which branch into multiple paths for multiple outputs, e.g., [16,17]. Along with choosing the appropriate network for the CT image classification task one of the typical concerns is the class balance of the training dataset [17]. As tip foldovers are rare, augmentation approaches are needed to reduce the effect of data imbalance.Therefore, in this work we design a dataset of CT images with folded synthetic arrays with realistic metal artifact to assist with training. We propose a multi-task neural network based on the U-Net [18], train it using the synthetic CT dataset, and then test it on a small dataset of real tip fold-over cases. Our results indicate that the model performs well in detecting fold-over cases in real CT images, and therefore, the trained model could help the intra-operative detection of tip fold-over in CI surgery."
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,2,Methodology,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,2.1,Dataset,"In this study, we utilize CT images from 312 CI patients acquired under local IRB protocols. This included 192 post-implantation CTs (185 normal and 7 tip-fold), acquired either intra-operatively (cone beam) or post-operatively (conventional or cone beam), and 120 pre-implantation CTs used to create synthetic post-implantation CT. As image acquisition parameters (dimensionality, resolution and voxel intensity) varies among the images, we preprocessed all the CT images to homogenize these parameters. First, the intracochlear structures (e.g., Scala Tympani (ST) and Scala Vestibuli (SV)) were segmented from the CT image using previously developed automatic segmentation techniques [19][20][21]. Using the ST segmentation, a region-of-interest CT image was cropped from the full-sized CT image keeping the ST at the center of the cropped image. Then the cropped CT resolution was resampled to an isotropic voxel size of 0.3 mm with a 32 × 32 × 32 grid. As the final step of the preprocessing, the voxel intensity of the cropped image was normalized to ensure comparable intensity distribution among all the CT images."
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,2.2,Synthetic CT Generation,"Our synthetic post-operative CT generation approach is inspired by the process of synthetic preoperative CT generation by Khan et al. [22]. First, a random but realistic location for the electrodes is estimated in a real pre-implantation CT image. This was done by considering some constraints, such as the relative location of the ST, electrode spacing, active array length, relative smoothness of the electrode curve, whether a fold exists, and if so, the location of the fold. Randomized variability within plausible margins ensured generating a unique and realistic electrode array shape for each case.Once we estimated the probable locations of the electrodes, we placed high intensity (around 3-4 times the bone intensity) cubic blocks with a dimensionality of 4×4× 4 voxels in an empty 32 × 32 × 32 grid in locations corresponding to the electrode sites in the preoperative CT. We also added small high intensity cubic blocks (with a dimensionality of 2 × 2 × 2 voxels) between the electrodes to represent the wires that connect to the electrodes. This resulted in an ideal image with a synthetic electrode array (shown in Fig. 2) but lacking realistic reconstruction artifacts. To get a realistic metal artifact, we applied radon transformation on the high intensity blocks to project the data on the detector space. Then, with the resulting sinogram, we applied inverse radon transformation to backproject the sinogram into the world space. The backprojection process is done two times separately: first time with low frequency scaling (to achieve blurry metal edge) and second time with high frequency scaling (to achieve dominant metal artifact). A Hamming filter was used in the backprojection process. Finally, the preoperative CT and the images with back-projected synthetic EA with realistic metal artifact were merged together additively to generate a synthetic postoperative CT image. Wang et al. [23] also presented a method to generate synthetic CT images with metal artifact, however, the aim of the study was to remove metal artifact from post-implant CT images.  Using the described synthetic CT generation process, we produced 215 synthetic pseudo CTs (PCTs) with random electrode locations sampled from 100 preoperative real CT images with stratified sampling into the training, validation, and testing datasets. In these PCTs, 155 images have a synthetic EA with tip fold-over and the remaining 60 do not have any fold over. The combined dataset including 379 real CTs and 215 synthetic CTs was divided into training (335), validation (22) and testing (237) subsets. The overall distribution of the dataset is presented in Table 1. As the number of real fold-over cases is very low (about 1.85% in this study), we allotted all of them in the testing dataset. "
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,2.3,Multi-task Deep Learning Network,"The neural network model proposed in this study for EA fold-over detection was inspired by the 3D U-Net architectures proposed by Isensee et al. and Ronneberger et al. [18,24]. The model is a multitasking network where the outputs are the segmentation of the EA and fold-over classification. Our hypothesis is that this multi-task approach helps the network focus attention on the shape of the EA when learning to classify the CT, rather than overfitting to a spurious local minima driven by non-EA features in the training dataset. The architecture is comprised of a context pathway for encoding the increasingly abstract representation of the input as we advance deeper into the neural network. A localization pathway recombines the representation for localizing the EA [20]. In the model, the context modules compute the context pathways. Each of these modules is a pre-activation residual block [25] which has a dropout layer p dropout = 0.2 in between two convolutional layers of 3 × 3 × 3 dimensionality.The localization pathways collect features at lower spatial resolution where the contextual information is encoded and transfer it to the higher resolution. This is done by using an upsampling step followed by a convolutional layer. The upsampled features are then concatenated with the corresponding context pathway level. Segmentation layers from different levels of the architecture are convolutional layers that are combined by elementwise summation to build the segmentation in a multi-scale fashion and obtain the final segmentation output. This approach was inspired by Kayalibay et al. [26].A classification branch is added to the network at the point where the contextual information is encoded at the lowest resolution (shown in Fig. 3). The classification branch consists of 4 residual blocks [27] followed by an average pooling layer and a sigmoid function layer.We used binary cross entropy (BCE) loss between the EA ground truth, created by manually selected thresholding of the CT image, and the predicted segmentation. For fold-over classification, we also used BCE loss between the ground truth and the predicted class. However, to place emphasis on the classification performance of the model, the classification loss was weighted 5 times the loss for the segmentation. The learning rate and the batch size were considered 3e-5 and 20, respectively. While training the model, random horizontal flipping and 90°rotation were used as data augmentation techniques for generalization.To evaluate the performance of the proposed model compared to some other neural network models, we implemented 3D versions of ResNet18 [15], Variational Autoencoder (VAE) [28] and Generative Adversarial Network (GAN) [29]. The overall performance comparison among these network architectures is presented in the result section. Similar to the multitasking 3D U-Net, proposed in this study, the VAE and the GAN architectures were designed with the same multi-task objective. In the VAE network, the information of the input image was encoded in 128 latent variables in the encoder section of the model. With these latent variables the decoder section reconstructs a 3D image with the EA segmentation. A classification branch with the same architectures as proposed above was added to the encoder section for fold-over detection. Similarly, the GAN also had a classification branch in the generator model of the architecture. The ResNet18 performs only classification, and thus represents classification performance achievable without multi-task training."
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,3,Results,"The training and validation loss curves of multitasking 3D U-Net are presented in Fig. 4. The graph at the top presents the overall training and validation loss. A rapid drop is visible in the segmentation loss curves where the validation loss curve swiftly follows the training loss curve. On the other hand, the classification loss demonstrates a gradual drop.Next, we compare the performance of different models for fold-over classification. In addition to the performance analysis of these networks for the whole testing data, we separately reported the performance analysis for the synthetic as well as the real portions of the testing data. The separate analysis provides insight about the applicability of the trained model for the real CT images. As reported in Table 2, the proposed multitask network has a classification accuracy of 98% for all the testing data (237 CT images). Among those 207 (7 tip fold-over and 200 non fold-over cases) are real CT images where the proposed model is 99% accurate regarding the classification by misclassifying 3 non fold-over test cases. In addition, the model misclassified 1 synthetic CT with folded over condition which degraded its accuracy to 97% for synthetic data. Although segmentation is not our primary goal, the model was able to consistently capture the location of the EA (example shown in Fig. 5). Inference time for our network was 0.60 ± 0.10 s.  On the other hand, ResNet18 demonstrated lower classification accuracy (around 87%) while misclassifying several synthetic CTs and the real CTs with tip folded over.In our study, the 3D versions of VAE and the GAN networks rendered promising segmentation results comparable to those of the 3D U-Net. However, in case of tip fold-over detection both the networks classified all the fold-over cases as non fold-overs. This suboptimal performance made our implemented VAE and GAN impractical for intra and/or postoperative fold-over detection.Table 3 presents results of a hyperparameter settings evaluation study. As the optimizer, we considered the Adam and Stochastic Gradient Descent (SGD). The learning rate was varied between 1e-3 to 1e-5; however, the best classification output was obtained using a learning rate of 3e-4. As stated in the methodology, we assigned a higher weight with the classification loss for emphasizing on the classification branch during the training process. Better classification accuracy was obtained for real CTs when the weight was either 2 or 5. From the classification accuracy, sensitivity, and specificity analysis in Table 3 it is evident that the Adam optimizer with a learning rate of 3e-5 outperforms the other hyperparameter combinations for real CT fold-over detection. The classification loss weight and the batch size were considered 5 and 20, respectively. Similar hyperparameter analysis was done to select parameters for the other networks evaluated in this study, but not included here for the sake of brevity.Using Adam optimizer with learning rate 3e-4, batch size of 20, and classification loss weight of 5, we repeated the training process 8 times to investigate training stability. In one out of eight cases, the resulting model could again correctly classify all the real CTs (7 with folded EA and 13 without) in the testing dataset. For the remaining 7 of 8 models, the network could correctly classify 19 out of 20 real CTs (misclassifying one fold-over case), which results in a classification accuracy of 95% for real postoperative CT images. "
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,4,Discussion and Conclusion,"In a CI surgical procedure, the relative positioning of the EA influences the overall outcome of the surgery [30,31]. A tip fold-over case results in poor positioning of the apical electrodes and, hence, can lead to severe consequences including trauma, damage to the residual hearing region and poor hearing restoration. Upon intraoperative detection of such a case, the surgeon can extract and reimplant the array to avoid any folding. The conventional detection processes require experience yet are prone to failure. In addition, due to the incidences of fold-over cases being low, training a model to detect these cases with real data is difficult. Therefore, in this study, we generated a dataset of CT images with folded synthetic electrode arrays with realistic metal artifact. A multitask custom network was proposed and trained with the dataset for array fold detection. We tested the trained model on real post-implantation CTs (7 with folded arrays and 200 without). We were able to train a model that could correctly classify all the fold-over cases while misclassifying only 3 non fold-over cases. In future work, clinical deployment of the model will be investigated."
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,,Fig. 1 .,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,,Fig. 2 .,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,,Fig. 3 .,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,,Fig. 4 .,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,,Fig. 5 .,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,,,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,,Table 1 .,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,,Table 2 .,
Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning,,Table 3 .,
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,1,Introduction,"The significance of depth information is undeniable in computer-assisted surgical systems [20]. In robotic-assisted surgery, the depth value is used to accurately map the surgical field and track the movement of surgical instruments [7,22]. Additionally, the depth value is essential to virtual and augmented reality to create 3D models and realize surgical technique training [18].Learning-based approaches have significantly improved monocular depth estimation (MDE) in recent years. As the pioneering work, Eigen et al. [2] proposed the first end-to-end deep learning framework using multi-scale CNN under supervised learning for MDE. Following this work, ResNet-based and Hourglass-based models were introduced as the variants of CNN-based methods [9,21]. However, supervised learning requires large amounts of annotated data. Collecting depth value from hardware or synthesis scenes is time-consuming and expensive [14]. To address this issue, researchers have explored self-supervised methods for MDE. Zhou et al. [30] proposed a novel depth-pose self-supervised monocular depth estimation from a video sequence. They generate synthesis views through the estimated depth maps and relative poses. Gordon et al. [3] optimized this work by introducing a minimum reprojection error between adjacent images and made a notable baseline named Monodepth2. Subsequently, researchers have proposed a variety of self-supervised methods for MDE, including those based on semantic segmentation [4], adversarial learning [28] and uncertainty [17]. These days, MDE has been applied to laparoscopic images. Ye et al. [27] and Max et al. [1] have provided exceptional laparoscopic scene datasets. Huang et al. [6] used generative adversarial networks to derive depth maps on laparoscopic images. Li et al. [12] combined depth estimation with scene coordinate prediction to improve network performance.This study presents a novel approach to predict depth values in laparoscopic images using spatio-temporal correspondence. Current self-supervised models for monocular depth estimation face two significant challenges in laparoscopic settings. First, monocular models individually predicted depth maps, ignoring the temporal correlation between adjacent images. Second, accurate point matching is difficult to achieve due to the misleading of large textureless regions caused by the smooth surface of organs. And the homogenous color misleads that the local areas of the edge are regarded with the same depth value. To overcome these obstacles, We introduce multi-view depth estimation (MVDE) with the optimized cost volume to guide the self-supervised monocular depth estimation model. Moreover, we exploit more informative values in a spatio-temporal manner to address the limitation of existing multi-view and monocular models.Our main contributions are summarized as follows. (i) A novel self-supervised monocular depth estimation guided by a multi-view depth model to leverage adjacent images when estimating depth value. (ii) Cost volume construction for multi-view depth estimation under minimum reprojection error and an optimized point cloud consistency for the monocular depth estimation. (iii) An extended deformable patch matching based on the spatial coherence in local regions and a cycled prediction learning for view synthesis and relative poses to exploit the temporal correlation between adjacent images. "
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,2,Method,
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,2.1,Self-supervised Monocular Depth Estimation,"Following [3,12], we train a self-supervised monocular depth network using a short clip from a video sequence. The short clip consists of a current frame I t as reference image I r and adjacent images I s ∈ {I t-1 , I t+1 } regarded as source images. As shown in Fig. 1, a monocular depth network f m D (I r , I s ; θ m D ) respectively predicts pixel-level depth maps D r and D s corresponding to I r and I s . A pose network f T (I r , I s ; θ T ) estimates a transformation matrix T s r as a relative pose of the laparoscope from view I r to I s . We use D r and T s r to match the pixels between I r and I s bywhere p r and p s are the 2D pixel coordinates in I r and I s . K is the laparoscope's intrinsic parameter matrix. This allows for the generation of a synthetic image I s→r through I s→r (p r ) = I s (p s ). To implement the self-supervised learning strategy, the reprojection error is calculated based on I r and I s→r bywithandwhere structured similarity (SSIM) [24] and L1-norm operator both adopt α at 0.85 followed as [3,13]. Instead of adding the auxiliary task proposed in prior work [12], we back-project the 2D pixel coordinates to the 3D coordinates P (p) = K -1 D(p)p. All the 3D coordinates from I r and I s gather as point cloud S r and S s . Then synthesized point cloud is warped as S s→r (p r ) = S s (p s ). We construct the point cloud consistency bywhere S r and S s→r are based on depth maps from the monocular depth network."
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,2.2,Improving Depth with Multi-view Guidance,"Unlike where f p r is the pixel coordinates in F r and f p d s is the pixel coordinates in F s based on the depth value d. Then F s is warped to the synthesis feature map by F d s→r ( f p r ) = F s f p d s . Feature volumes V r and V s→r are aggregations of feature maps F r and F s→r . We construct a cost volume C byFig. 3. Illustration of deformable patch matching process with pixel coordinates offset and depth propagation.Previous approaches average the difference between V r and all V s→r from adjacent views to generate cost volumes without considering the occlusion problem and uniform differences between the reference and adjacent feature maps [25,26]. Motivated by these challenges, we introduce the minimum reprojection loss to C, as shown in Fig. 2. We construct the cost volume Ĉ via the minor difference value on the corresponding coordinates of the feature volumes as Ĉ = minWe construct a consistency between MVDE and MDE by "
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,2.3,Deformable Patch Matching and Cycled Prediction Learning,"Since large areas of textureless areas and reflective parts will cause brightnessbased reprojection errors to become unreliable. Furthermore, the homogeneous color on the edge of organs causes local regions to be regarded in the same depth plane. We introduced deformable patch-matching-based local spatial propagation to MDE. As shown in Fig. 3, an offset map O r (p r ) is adopted to obtain a local region for each pixel by transforming the pixel coordinates in the reference image I r . Inspired by [15,23], to avoid marginal areas affecting the spatial coherence of the local region, an offset network f O (I r ; θ O ) generates a pixel-level add additional offset map ΔO r (p r ). The deformable local region for each pixel can be obtained bywhere R r is the deformable local regions for pixel p r . After sharing the same depth value by depth propagation in the local region, we implement the deformable local regions on Eq. 1 to complete patch matching bywhere R s is the matched local regions in source views I s . Based on R r and R s , I s (R s ) is warped to the synthesised regions I s→r (R r ). The patch-matching-based reprojection error is calculated byTo better use the temporal correlation, we considered each image as a reference to construct a cycled prediction learning for depth and pose. The total loss on the final computation is averaged from the error of each combination aswhere L m r is the reprojection error term for MDE, and L mv r is for MVDE. i is the index number of views in the short clip. L s is the smoothness term [3,12]."
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,3,Experiments,
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,3.1,Datasets and Evaluation Metrics,"SCARED [1] datasets were adopted for all the experiments. This dataset contained 35 laparoscopic stereo videos with nine different scenes. And the corresponding depth values obtained through coded structured light images served as ground-truth. We divided the SCARED datasets into a 10:1:1 ratio for each scene based on the video sequence to conduct our experiments. For training, validation, and testing, there were 23,687, 2,405, and 2,405 frames, respectively. Because of limitations in computational resources, we resized the images to 320 × 256 pixels, a quarter of their original dimensions. Following the previous methods [3,25], we adopted seven classical 2D metrics to evaluate the predicted depth maps. Additionally, we only used the monocular depth model to predict depth values with a single RGB image as input during testing.  "
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,3.2,Implementation Details,"We utilized PyTorch [16] for model training, employing the Adam optimizer [8] across 25 training epochs. The learning rate started at 1 × 10 -4 and dropped by a scale factor of 10 for the final 10 epochs. A batch size is 6, and the total loss function's parameters γ, μ, λ, and δ were set to 0.5, 0.5, 0.2, and 1 × 10 -3 , respectively. Additionally, we capped the predicted depth values at 150mm. To construct the cost volume, we adopted the adaptive depth range method [25] and set the number of hypothesized depth values k to 96. Following [25], we used ResNet-18 [5] with pretrained weights on the Ima-geNet dataset [19] as encoder module. The feature extractor comprised the first five layers of ResNet-18 [5]. The offset network was two 2D convolution layers."
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,3.3,Comparison Experiments,"We conducted a comprehensive evaluation of our proposed method by comparing it with several classical and state-of-the-art techniques [3,[10][11][12][13]25,29] retrained on SCARED datasets [1]. Table 1 presents the quantitative results. We also assessed the baseline performance of our proposed method. We compared the depth maps and generated error maps on various laparoscopic scenes based on absolute relative error [25], as shown in Fig. 4. "
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,3.4,Ablation Study,"We conducted an ablation study to evaluate the influence of different components in our proposed approach. Table 2 shows the results of our method with four different components, namely, point cloud consistency (PCC), minimum reprojection error (MRE), deformable patch matching (DPM), and cycled prediction learning (CPL). We adopted GCDepthL [12] and Manydepth [25] as baselines for our method's monocular and multi-view depth models. We proposed PCC and MRE as two optimized modules for these baseline models and evaluated their impact on each baseline individually. Furthermore, we trained the monocular and multi-view depth models separately in our proposed method without consistency to demonstrate the contribution of combining these two models."
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,4,Discussion and Conclusions,"The laparoscopic scenes typically feature large, smooth regions with organ surfaces and homogeneous colors along the edges of organs. This can cause issues while matching pixels, as the points in the boundary area can be mistaken to be in the same depth plane. Our proposed method's depth maps, as shown in Fig 4, exhibit a smoother performance in the large regions of input images when compared to the existing methods [3,[10][11][12][13]25,29]. Additionally, the error maps reveal that our proposed method performs better even when the depth maps look similar qualitatively. At the marginal area of organs, our proposed method generates better depth predictions with smoother depth maps and lower errors, despite the color changes being barely perceptible with the depth value changes. Our proposed method outperforms current approaches on the seven metrics we used, as demonstrated in Table 1. The ablation study reveals that the proposed method improves significantly when combining each component, and each component contributes to the proposed method. Specifically, the optimized modules PCC and MRE designed for monocular and multi-view depth models enhance the performance of the baselines [12,25]. The combination of monocular and multi-view depth models yields better results than the single model trained independently, as seen in the last three rows of Table 2.In conclusion, we incorporate more temporal information in the monocular depth model by leveraging the guidance of the multi-view depth model when predicting depth values. We introduce the minimum reprojection error to construct the multi-view depth model's cost volume and optimize the monocular depth model's point cloud consistency module. Moreover, we propose a novel method that matches deformable patches in spatially coherent local regions instead of point matching. Finally, cycled prediction learning is designed to exploit temporal information. The outcomes of the experiments indicate an improved depth estimation performance using our approach."
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,,Fig. 1 .,
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,,Fig. 2 .,
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,,,
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,,Fig. 4 .,
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,,,"Then, a feature extractor f F (I r , I s ; θ F ) obtains the deep feature map F r and F s from input images I r and I s . Similar to the pixel-coordinate matching between I r and I s as Eq. 1, 2D pixel coordinates of F s is back-projected to the each plane Z d k of hypothesised depth value. Z d k shares the same depth value d for each pixels. Then the pixel coordinates are matched by"
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,,Table 1 .,
Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence,,Table 2 .,
From Tissue to Sound: Model-Based Sonification of Medical Imaging,1,Introduction,"Carrying out a surgical procedure requires not only spatial coordination but also the ability to maintain temporal continuity during time-critical situations, underscoring the crucial importance of auditory perception due to the temporal nature of sound. Perceptual studies have shown that stimuli in different sensory modalities can powerfully interact under certain circumstances, affecting perception or behavior [1]. In multisensory perception, sensory experiences are integrated when they occur simultaneously, i.e., when complementary or redundant signals are received from the same location at the same time. Previous studies have shown that combining independent but causally correlated sources of information facilitates information processing [2]. It has been demonstrated that multisensory integration, particularly auditory cues embedded in complex sensory scenes, improves performance on a wide range of tasks [3]. Biologically, humans have evolved to process spatial dimensions visually [4]. At the same time, their auditory system, which works in an omnidirectional manner, helps to maintain a steady pace in dynamic interaction, facilitating hand-eye coordination. It has been shown that bimanual coordination augmented with auditory feedback, compared to visual, leads to enhanced activation in brain regions involved in motor planning and execution [5].However, the integration of auditory systems with visual modalities in biomedical research and applications has not been fully achieved. This could be due to the challenges involved in providing perceptually unequivocal and precise sound while reflecting high-resolution and high-dimensional information, as available in medical data. This paper proposes a novel approach, providing an intelligent modeling as a design methodology for interactive medical applications. Considering the limitations of the state-of-the-art, we investigate potential design approaches and the possibility of establishing a new research direction in sonifying high-resolution and multidimensional medical imaging data."
From Tissue to Sound: Model-Based Sonification of Medical Imaging,2,State-of-the-Art of Sonification in the Medical Domain,"Sonification is the data-dependent generation of sound, if the transformation is systematic, objective and reproducible, so that it can be used as scientific method [6,7]. Researchers have investigated the impact of sound as an integral part of the user experience in interactive systems and interfaces [8]. In this form of interaction design, sound is used to create rich, dynamic, and immersive user experiences and to convey information, provide feedback, and shape user behavior. The use of sonification has expanded in recent years to a wide range of tasks, such as navigation, process monitoring, data exploration, among others [7]. A variety of sonification techniques for time-indexed data such as audification, parameter-mapping sonification (PMSon), and model-based sonification (MBS) translate meaningful data patterns of interaction into perceptual patterns, allowing listeners to gain insight into those patterns and be aware of their changes.In PMSon, data features are input parameters of a mapping function to determine synthesis parameters. PMSon allows explicit definition of mapping functions in a flexible and adaptive manner, which has made it the most popular approach for medical applications. Medical sonification, as demonstrated by pioneering works such as [9][10][11][12], has predominantly used PMSon for sonifying the position or state of surgical instruments with respect to predetermined structures or translating spatial characteristics of medical imaging data into acoustic features. Research in surgical sonification [13,14] has mainly focused on image-guided navigation, showing significant results in terms of accuracy and facilitating guidance. Fundamental research behind these studies has aimed to expand mapping dimensionality beyond 1D [15] and 2D [16,17] up to three orthogonal dimensions [18]. A sonification method combining two orthogonal mappings in two alignment phases has been proposed in [19] for the placement of pedicle screws with four degrees of freedom. In addition to expanding the data bandwidth, there have been studies [11,12,14,20,21] which address the issues of integrability and pleasantness of the resulting sound signal. This is arguably one of the most significant challenges in surgical sonification, which still requires further research.Medical sonification has shown great potential, despite being a relatively new field of research. However, there are limitations in terms of design and integrability. The tedious and case-specific process of defining a mapping function that can be perceptually resolved, even in low dimensional space, poses a significant challenge, rendering the achievement of a generalized method nearly impossible. Consequently, practitioners endeavor to minimize data complexity and embed data space into low dimensional space, giving rise to sonification models that offer only an abstract and restricted understanding of the data, which is inadequate in the case of complex medical data. Although these methods achieve adequate perceptual resolution and accuracy, they still are not optimized for integration into the surgical workflow and demonstrate low learning rates, which also demand increased cognitive load and task duration. Furthermore, fine-tuning these models involves a considerable amount of artistic creativity in order to avoid undesirable effects like abrasiveness and fatigue."
From Tissue to Sound: Model-Based Sonification of Medical Imaging,3,Towards Model-Based Sonic Interaction with Multimodal Medical Imaging Data,"The need for an advanced multisensory system becomes more critical in scenarios where the anatomy is accessed through limited means, as in minimally invasive surgery. In such cases, an enriched auditory feedback system that conveys precise information about the tissue or surrounding structures, capturing spatial characteristics of the data (such as density, solidity, softness, or sparsity), can enhance the surgeon's perception and, on occasion, compensate for the lack of haptic feedback in such procedures. Embedding such use cases into a low-dimensional space is not feasible. In the same way that tapping on wood or glass imparts information about the object's construction, an effective sonification design can convey information in a manner that is easily interpreted with minimal cognitive effort. Intuitive embodiment of information in auditory feedback, facilitates the learning process to the extent that subconscious association of auditory cues with events can be achieved. Despite the fact that anatomical structures do not produce sound in human acoustic ranges, and surgeons do not have a direct perceptual experience of them, these structures still adhere to the physical principles of dynamics. The hypothesis of this paper is that sounds based on the principles of physics are more straightforward to learn and utilize due to their association with real-world rules. MBS is a technique that utilizes mathematical models to represent data and then convert those models into audible sounds. This approach is often used in scientific or data analysis applications such as clustering [22] or data scanning [23], where researchers aim to represent complex data sets or phenomena through sound. MBS employs a mapping approach that associates data features with the features of a sound-capable system, facilitating generalization across different cases. MBS is often coupled with physical modeling synthesis that uses algorithms to simulate the physical properties of real-world instruments and generate sound in a way that mimics the behavior of physical objects [24][25][26]. This approach is often used to create realistic simulations of acoustic instruments or explore new sonic possibilities that go beyond the limitations of traditional instruments. Sound is a physical phenomenon that arises from the vibration of objects in a medium, such as air, resulting in a complex waveform composed of multiple frequencies at different amplitudes. This vibrational motion can be generated by mechanical impacts, such as striking, plucking, or blowing, applied to a resonant object capable of sustaining vibration. Physical modeling has been previously introduced to the biomedical research community as a potential solution for the issue of unintuitive sounds, as reported in [21]. However, as mentioned earlier, it is limited by the shortcomings of the current state-of-the-art, particularly with respect to reduced data dimensionality.Physical modeling can be approached using the mass interaction method, which is characterized by its modular design and the capacity to incorporate direct gestural interaction, as demonstrated in [27][28][29]. This approach offers the advantage of describing highly intricate virtual objects as a construction of elementary physical components. Additionally, such an approach is highly amenable to the iterative and exploratory design of ""physically plausible"" virtual objects, which are grounded in the principles of Newtonian physics and three laws of motion but are not necessarily limited to the mechanical constraints of the physical world."
From Tissue to Sound: Model-Based Sonification of Medical Imaging,,Contribution,"This paper presents a novel approach for transforming spatial features of multimodal medical imaging data into a physical model that is capable of generating distinctive sound. The physical model captures complex features of the spatial domain of data, including geometric shapes, textures, and complex anatomical structures, and translates them to sound. This approach aims to enhance experts' ability to interact with medical imaging data and improve their mental mapping regarding complex anatomical structures with an unsupervised approach. The unsupervised nature of the proposed approach facilitates generalization, which enables the use of varied input data for the development of versatile soundcapable models."
From Tissue to Sound: Model-Based Sonification of Medical Imaging,4,Methods,"The proposed method involves multimodal imaging data serving as input, a topology to capture spatial structures, and an interaction module to establish temporal progress. We consider medical imaging data as members of a R d+m space, where d is the data dimensionality in space domain, and m the dimension of measured features by the imaging systems. A sequence of physics-based sound signals, S, expressed ascan be achieved by the sum of n excitations of a physical model P with user interaction act at the position pos with applied force of F , where f transfers a region of interest (RoI) A ∈ R d+m to parameters of the physical model P using the topology matrix T . These components are described in detail in Sect. 4.1.Figure 1 provides an illustration of the method overview and data pipeline. "
From Tissue to Sound: Model-Based Sonification of Medical Imaging,4.1,Physical Model,"The mass-interaction physics methodology [28] allows the formulation of physical systems, such as the linear harmonic oscillator, which comprise two fundamental constituents: masses, representing material points within a 3D space, with corresponding inertial behaviors, and connecting springs, signifying specific types of physical couplings such as viscoelastic and collision between two mass elements."
From Tissue to Sound: Model-Based Sonification of Medical Imaging,,Implementation of a Mass-Interaction System in Discrete Time.,"To represent and compute discretized modular mass-interaction systems, a widely used method involves applying a second-order central difference scheme to Newton's second law, which states that force F is equal to mass m times acceleration a, or the second derivative of its position vector x with respect to time t. The total force exerted by the dampened spring, denoted aswhere F s represents the elastic force exerted by a linear spring (the interaction) with stiffness K, connecting two masses m 1 , m 2 located at positions x 1 , x 2 , can be expressed using the discrete-time equivalent of Hooke's law. Similarly, the friction force F d applied by a linear damper with damping parameter z can be derived using the Backward Euler difference scheme with the discrete-time inertial parameter Z = z/ΔT . F (t n ) is applied symmetrically to each mass in accordance with Newton's third law:The combination of forces applied to masses and the connecting spring yields a linear harmonic oscillator as described inwhich is a fundamental type of the mass-interaction system. This system is achieved by connecting a dampened spring between a mass and a fixed pointA mass-interaction system can be extended to a physical model network with an arbitrary topology by connecting the masses via dampened springs. The connections are formalized as a routing matrix T of dimensions r × c, where r denotes the number of mass points in the physical model network, and c represents the number of connecting springs, each having only two connections. A single mass can be connected to multiple springs in the network."
From Tissue to Sound: Model-Based Sonification of Medical Imaging,,Interaction,"Module. An interaction module, act, excites the model P by applying force to one or more input masses of the model I ∈ T r×c . f maps the intensities of the input data to M, K, Z, and F . Therefore, the input force is propagated through the network according to the Eq. 2 and observed by the output masses O ∈ T r×c .To summarize, the output masses are affected by the oscillation of all masses activated in the model with various frequencies and corresponding amplitudes, resulting in a specific sound profile, i.e., tone color. This tone color represents the spatial structure and physical properties of the RoI, which is transformed into the features of the output sound. The wave propagation is significantly influenced by the model's topology and the structure of the inter-mass connections, which have great impact on activating spatial relevant features and sound quality."
From Tissue to Sound: Model-Based Sonification of Medical Imaging,4.2,Experiment and Results,"The objective is to evaluate the feasibility of the proposed method in creating a model that is capable of generating discernible sound profiles in accordance with the underlying anatomical structures. In particular, we aim to differentiate between the sound of a set of tissue types. Through empirical experimentation on an abdominal CT volume [31], we determined a model configuration that achieves stability and reduces noise to the desired level. The shape of the topology is a 3D cube of size 7 mm 3 . The inter-mass connections are established at the grid spacing distance of 1 mm between each mass and its adjacent neighbor masses. All the masses located on the surface of the model are set as fixed points. To excite the model, equal forces are applied to the center of the model in a 3D direction and observed at the same position. The RoI is obtained by selecting 3D cubes with the same topology and size in the CT volume. The intensities in the RoI are transformed to define the model parameters. The spring parameters K and Z are derived by averaging the intensities of their adjacent CT voxels, by a linear mapping and M and F are set to constant values. We defined a sequence of RoI starting in the heart, passing through lung, liver, bone, muscle, and ending in the air in a 16-steps trajectory, as shown in Fig. 2. For visualizing the trajectory and processing image intensities, we used ImFusion Suite [32]. For generating physical models and generating sound, we used mass-interaction physical modelling library [29] for the Processing sketching environment: https://processing.org/. Visual demonstrations of the models, along with the corresponding sound samples, are provided in the supplementary material, along with additional explanations.A mel spectrogram is a visual representation of the frequency content of an audio signal, where the frequencies are mapped to a mel scale, which is a perceptual frequency scale based on how humans hear sounds. Therefore, we used this representation to show the frequency content of the resulting sound of the trajectory, presented in Fig. 2. "
From Tissue to Sound: Model-Based Sonification of Medical Imaging,5,Discussion and Conclusion,"This paper introduced a general framework for MBS of multimodal medical imaging data. The preliminary study demonstrates perceptually distinguishable sound profiles between various anatomical tissue types. These profiles are achieved through a minimal preprocessing based on the model topology and a basic mapping definition. This indicates that the model is effective in translating intricate imaging data into a discernible auditory representation, which can conveniently be generalized to a wide range of applications. In contrast to the traditional methods that directly convert low-dimensional data into global sound features, this approach maps features of a data model to the features of a sound model in an unsupervised manner and enables processing of high-dimensional data.The proposed method presents opportunities for several enhancements and future directions. In the case of CT imaging, it may be feasible to establish modality-specific generalized configurations and tissue-type-specific transfer functions to standardize the approach to medical imaging sonification. Such efforts could lead to the development of an auditory equivalent of 3D visualization for medical imaging, thereby providing medical professionals with a more immersive and intuitive experience. Another opportunity could be in the sonification of intraoperative medical imaging data, such as ultrasound or fluoroscopy, to augment the visual information that a physician would receive by displaying the tissue type or structure which their surgical instruments are approaching. To accommodate various application cases, alternative interaction modes can be designed that simulate different approaches to the anatomy with different tool materials. Additionally, supervised features can be integrated to improve both local and global perception of the model, such as amplifying regions with anomalies. Different topologies and configurations can be explored for magnifying specific structures in data, such as pathologies, bone fractures, and retina deformation. To achieve a realistic configuration of the model regarding the physical behavior of the underlying anatomy, one can use several modalities which correlate with physical parameters of the model. For instance, the masses can be derived by intensities of CT and spring stiffness from magnetic resonance elastography, both registered as a 3D volume.An evaluation of the model's potential in an interactive setting can be conducted to determine its impact on cognitive load and interaction intuitiveness. Such an assessment can shed light on the practicality of the model's application by considering the user experience, which is influenced by cognitive psychological factors. As with any emerging field, there are constraints associated with the methodology presented in this paper. One of the constraints is the requirement to manually configure the model parameters. Nevertheless, a potential future direction would be to incorporate machine learning techniques and dynamic modeling to automatically determine these parameters from underlying physical structure. For instance, systems such as [30] can provide a reliable reference for such investigations. Considering the surgical auditory scene understanding is vital when incorporating the sonification model into the surgical workflow. Future studies can investigate this by accounting for realistic surgical environments, including existing sound sources, and considering auditory masking effects."
From Tissue to Sound: Model-Based Sonification of Medical Imaging,,Fig. 1 .,
From Tissue to Sound: Model-Based Sonification of Medical Imaging,,Fig. 2 .,
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,1,Introduction,"Previous studies have shown that surgeon performance directly affects patient clinical outcomes [1,2,13,14]. In one instance, manually rated suturing technical skill scores were the strongest predictors of patient continence recovery following a robot-assisted radical prostatectomy compared to other objective measures of surgeon performance [3]. Ultimately, the value of skill assessment is not only in its ability to predict surgical outcomes, but also in its function as formative feedback for training surgeons. The need to automate skills assessment is readily apparent, especially since manual assessments by expert raters are subjective, time-consuming, and unscalable [4,5]. View Fig. 1 for problem setup.Preliminary work has shown favorable results for automated skill assessments on simulated VR environments, demonstrating the benefits of machine learning (ML) methods. ML approaches for automating suturing technical skills leveraged instrument kinematic (motion-tracking) data as the sole input to recurrent networks have been able to achieve effective area-under-ROC-curve (AUC), up to 0.77 for skill assessment in VR sponge suturing exercises [6]. Multi-modality approaches that fused information from both kinematics and video modalities have demonstrated increased performance over uni-modal approaches in both VR sponge and tube suturing exercises, reaching up to 0.95 AUC [7].Despite recent advances, automated skill assessment in live scenarios is still a difficult task due to two main challenges: 1) the lack of kinematic data from the da Vinci R system, and 2) the lack of training data due to the labor-intensive labeling task. Unlike simulated VR environments where kinematic data can be readily available, current live surgical systems do not output motion-tracking data, which is a key source of information for determining the movement and trajectory of robotic manipulators and suturing needles. Moreover, live surgical videos do not have a clear and painted target area for throwing stitches, unlike VR videos, which makes the task additionally difficult. On the other hand, due to the labor-intensive task of segmenting and scoring individual stitches from each surgical video, the quantity of available and labeled training data is quite low, rendering traditional supervised learning approaches ineffective. To address these challenges, we propose LiveMAE which learns sim-to-real generalizable representations without requiring any live kinematic annotations. Leveraging available video and sensor data from previous VR studies, LiveMAE can map from surgical images to instrument kinematics and derive surrogate ""kinematic"" automatically by learning to reconstruct images from both VR and live stitches while also predicting the corresponding VR kinematics. This creates a shared encoded representation space between the two visual domains while using available kinematic data from only one domain, the VR domain. Moreover, our pre-training strategy is not skill-specific which brings a bonus in improving data efficiency. LiveMAE enjoys up to six times more training data across the six suturing skills seen in Fig. 1c, especially when we further break down video clips and kinematic sequences into multiple (image, kinematic) pairs. Overall, our main contributions include:1. We propose LiveMAE which learns sim-to-real generalizable representations without requiring any live kinematic annotations. 2. We design a pre-training paradigm that increases the number of effective training samples significantly by combining data across suturing skills. 3. We conduct rigorous evaluations to verify the effectiveness of LiveMAE on surgical data collected and labeled across multiple institutions and surgeons. Finetuning on suturing skill assessment tasks yields better performance on 5/6 skills on live surgical videos compared to supervised learning baselines."
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,2,Methodology,"Masked autoencoding is a method for self-supervised pre-training of Vision Transformers (ViTs [12]) on images. It has demonstrated the capability to learn efficient and useful visual representations for downstream tasks such as image classification and segmentation. Our model builds on top of mask autoencoders (MAEs) and we provide a preliminary intro for MAE in Appendix 1.1. The input to our system contains both VR and live surgical data. VR data for a suturing skill s is defined as, and EASE technical skill score y i ∈ {0, 1} for non-ideal vs ideal performance. F denotes the number of frames in the video clip. Live data for s is similarly D L s = {(x i , y i )} Ms i=0 , except there are no aligned kinematics. Kinematic data has 70 features tracking 10 instruments of interest, each pose contains 3 elements for coordinates and 4 elements for quarternions. There are six technical skill labels, see Fig. 1c."
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,2.1,LiveMAE,"Since D L s lacks kinematic information that is crucial for suturing skill assessment, we propose LiveMAE to automatically derive ""kinematics"" from live videos that can be helpful for downstream prediction. Specifically, we aim to learn a mapping φ : R H×W ×3 → R 70 from images to instrument kinematics using available video and sensor data from D V R s , and subsequently utilizing that mapping φ on live videos. Although the visual style between VR and live surgical videos can differ, this mapping is possible since we know that both simulated VR and live instruments share the exact same dimensions and centered coordinate frames. Our method builds on top of MAE and has three main components: a kinematic decoder, a shared encoder, and an expanded training set. Kinematic Decoder. For mapping from a surgical image to the instrument kinematics, we propose an additional kinematic output head along with a corresponding self-supervised task of reconstructing kinematics from masked input. See Fig. 2a. The kinematic decoder is also a lightweight series of Transformer blocks that takes in a full sequence of both the (i) encoded visible patches, and (ii) learnable mask tokens. The last layer of the decoder is a linear projection whose number of output channels equals to 70, the dimension of the kinematic data. Similar to the image reconstruction task, which aims to learn visual concepts and semantics by encoding them into a compact representation for reconstruction, we additionally require these representations to contain information regarding possible poses of the surgical instruments. The kinematic decoder also has a reconstruction loss, which computes the mean squared error (MSE) between the reconstructed and original kinematic measurements. Shared Encoder. To learn sim-to-real generalizable representations that generalize across the different visual styles of VR and live videos, we augment live images with VR videos for pre-training. Since we do not have live kinematics, the reconstruction loss from the kinematic decoder will be set to zero for live samples within a training batch. This creates a shared encoded representation space between the two visual domains such that visual concepts and semantics about manipulators and suturing needles can be shared between them. Moreover, as we simultaneously train the kinematic reconstruction task, we are learning a mapping that can generalize to live videos, since two similar positioning in either VR or live should have similar corresponding kinematics.Expanded Training Set. Since we have limited surgical data, and the mapping from image to instrument kinematics is not specific to any one suturing skill, we can combine visual and kinematic data across different skills during pre-training. Specifically, we pre-train the model on all data combined across 6 suturing skills to help learn the mapping. In addition, we can further break down video clips and kinematic sequences into F * (N s +M s ) (image, kinematics) pairs to increase the effective training set size without needing heavy data augmentations. These two key facts provide is a unique advantage over traditional supervised learning, since training each skill assessment task required the full video clip to learn temporal signature along with skill-specific scorings."
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,,Finetuning of LiveMAE for Skill Assessment,"After pre-training, we discard the image decoder and only use the pathway from the encoder to the kinematic decoder as our mapping φ. See Fig. 2b. We applied φ to our live data D L s and extract a surrogate kinematic sequence for each video clip. The extracted kinematics are embedded by a linear projection with added positional embeddings and processed with a lightweight sequential DistilBERT model. We append a linear layer on top of the pooled output from DistilBERT for classification. We finetune the last layer of φ and the sequential model with the cross-entropy loss using a small learning rate, e.g. 1e-5."
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,3,Experiments and Results,"Datasets. We utilize a previously validated suturing assessment tool (EASE [8]) to evaluate the robotic suturing skill in both VR and live surgery. We collected 156 VR videos and 54 live surgical videos from 43 residents, fellows, and attending urologic surgeons in this 5-center multi-institutional study. VR suturing exercises were completed on the Surgical Science TM Flex VR simulator and live surgical videos of surgeons performing the vesico-urethra anastomosis (VUA) step of a RARP were recorded. Each video was split into stitches, (n = 3448) total, and each stitch was segmented into sub-phrases with 6 binary assessment labels (low vs. high skill). See data breakdown and processing in Appendix 1.2. "
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,,Metrics and Baselines.,"Across the five institutions, we use 5-fold cross-valid ation to evaluate our model, training and validating on data from 4 institutions while testing on the 5th held-out institution. This allows us to test for generalization on unseen cases across both surgeons and medical centers. We measure and report the mean ± std. dev. for the two metrics: (1) Area-under-the-ROC curve (AUC) and ( 2) Area-under-the-PR curve (AUPRC) for the 5 test folds.To understand the benefits of each data modality, we compare LiveMAE against 3 setups: (1) train/test using only kinematics, (2) train/test using only videos, and (3) train using kinematic and video data while testing only on video (no live kinematics). For kinematics-only baselines, we use two sequential models (1) LSTM recurrent model [9], and (2) DistillBERT transformer-based model [10]. For video-only baselines, we used two models based on pre-trained CNNs(3) ConvLSTM and (4) ConvTransformer. Both used pre-trained AlexNet to extract visual and flow features from the penultimate layer for each frame. The features are then flattened as used as input vectors to the sequential model ( 1) and ( 2). For a multi-modal baseline, we compare against recent work, AuxTransformer [7], which uses kinematics as privileged data in the form of an auxiliary loss during training. Unlike our method, they have additional kinematic supervision for the live video domain which we do not have."
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,3.1,Understanding the Benefits of Kinematics,"Table 1 presents automated suturing assessment results for each technical skill on VR data from unseen surgeons across the 5 held-out testing institutions. We make 3 key observations: (1) we successfully reproduced assessment performance seen in previous works and showed that sequential models trained on kinematiconly data often achieve the best results (outperforming video and multi-modal on 5/6 skills with high mean AUCs (0.652-0.878) and AUPRC (0.895-0.963). ( 2) Vision model trained on video-only data can help with skill assessment, especially in certain skills such as needle hold angle where the angle between the needle tip and the target tissue (largely responsible for high/low score) is better represented visually, opposed kinematic poses. ( 3) Lastly, we demonstrated the benefits of using kinematics data as supervisory signals during training, which yields improved performance on video-only baselines where kinematic data are not available during testing, seen with AuxTranformer's numbers. Overall, kinematics provide a wealth of clean motion signals that is essential for skill assessment, which helps to inspire LiveMAE for assessment in live videos. "
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,3.2,Evaluation of LiveMAE on Live Videos,"Quantitative Results. Table 2 presents automated suturing assessment results for each technical skill on Live data across the 5 held-out institutions. We make 3 key observations: (1) Skill assessment on live stitch using LiveMAE or LiveMAEfinetuned often achieves the best results (outperforming supervised baselines and AuxTransformer with mean AUCs (0.550-0.837) and AUPRC (0.733-0.912) with particular improvements of 35.78% in AUC for wrist rotation skills and 8.7% for needle driving skill. (2) LiveMAE can learn generalizable representations from VR to Live using its shared encoder and kinematic mapping, achieving reasonable performance even without fine-tuning in the needle repositioning, hold angle and wrist rotation skills. (3) Clinically, we observe that VR data can directly help with live skill assessment, especially in certain skills such as wrist rotation and wrist rotation withdrawal (+35.78% increase in AUC), where medical students confirmed that the rotation motions (largely responsible for high/low score) are more pronounced in VR suturing videos and less so in Live videos due to how manipulators are visualized in the simulation. Hence training with VR data can help to teach LiveMAE of the desired assessment procedure that is not as clear in Live data and supervised training paradigm. Overall, LiveMAE contributes positively to the task of automated skill assessment, especially in live scenarios where it is not possible to obtain kinematics from the da Vinci R surgical system. Qualitative results. We visualized both reconstructed images and kinematics from masked inputs in Fig. 3. Top row of Fig. 3 a shows the 75% masked image where only 1/4 of the visible patches are input into the model. The block patterns are input patches to LiveMAE that were not masked. The middle row shows the image's visual reconstruction vs. the original images (last row). We observe that LiveMAE can pick out and reconstruct the positioning of the manipulators quite well. It also does a good job at reconstructing the target tissue, especially in Tube1 and Sheet1. However, we also observe very small reconstruction artifacts in darker/black regions. This can be attributed to the training data, which sometimes contain all black borders that were not cropped out, yielding the confusion between black borders in live videos and black manipulators in the VR videos. In Fig. 3b, we plot in the top row the original and predicted kinematics of the VR samples in blue and orange, respectively. The bottom row plots their absolute difference. LiveMAE does well in predicting kinematics from unseen samples, especially in Sheet1 where it gets both positioning and orientations correctly for all instruments of interest, off by at most 0.2. In Sponge1 and Tube1, we notice it does a poor job at estimating poses for some of the instruments, namely the orientation of the needle and target positions (index 4-7, 60-70) in Sponge1 and the needle orientation (index 4-7) in Tube1. This can happen in cases where it is hard to see and recognize the needle in the scene, making it difficult to estimate the exact needle orientation, which may explain LiveMAE's poorer performance for the skill Needle hold ratio. and presents a promising direction for future work in diving deeper into CV models to segment out instruments of interest since they can be easily ignored."
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,4,Conclusion,"Self-supervised learning methods, as utilized in our work, showed that videobased evaluation of suturing technical skills in live surgical videos is achievable with robust performance across multiple institutions. Although current work is limited to using VR data from one setup, namely Surgical Science TM Flex VR, our approach is independent from that system and can be applied on top of other surgical simulation systems with synchronized kinematics and video recordings. Future work will expand on the applications we demonstrated to determine whether it is possible to have a fully autonomous process, or semiautonomously with a ""human-in-the-loop""."
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,,Fig. 1 .,
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,,Fig. 2 .,
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,,Fig. 3 .,
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,,Table 1 .,
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,,Table 2 .,
Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills,,.725 ±0.12 0.903 ±0.06 0.562 ±0.08 0.733 ±0.08 0.634 ±0.06 0.826 ±0.01,
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,1,Introduction,"Image-to-physical registration is a necessary process for computer-assisted surgery to align preoperative imaging to the intraoperative physical space of the patient to in-form surgical decision making. Most intraoperatively utilized image-to-physical regis-trations are rigid transformations calculated using fiducial landmarks [1]. However, with better computational resources and more advanced surgical field monitoring sensors, nonrigid registration techniques have been proposed [2,3]. This has made image-guided surgery more tractable for soft tissue organ systems like the liver, prostate, and breast [4][5][6]. This work focuses specifically on nonrigid breast registration, although these methods could be adapted for other soft tissue organs. Current guidance technologies for breast conserving surgery localize a single tumor-implanted seed without providing spatial information about the tumor boundary. As a result, resections can have several centimeters of tissue beyond the cancer margin. Despite seed information and large resections, reoperation rates are still high (~17%) emphasizing the need for additional guidance technologies such as computer-assisted surgery systems with nonrigid registration [7].Intraoperative data available for registration is often sparse and subject to data collection noise. Image-to-physical registration methods that accurately model an elastic soft-tissue environment while also complying with intraoperative data constraints is an active field of research. Determining correspondences between imaging space and geometric data is required for image-to-physical registration, but it is often an inexact and ill-posed problem. Establishing point cloud correspondences using machine learning has been demonstrated on liver and prostate datasets [8,9]. Deep learning image registration methods like VoxelMorph have also been used for this purpose [10]. However, these methods require extensive training data and may struggle with generalizability. Other non-learning image-to-physical registration strategies include [11] which utilized a corotational linear-elastic finite element method (FEM) combined with an iterative closest point algorithm. Similarly, the registration method introduced in [12] iteratively updated the image-to-physical correspondence between surface point clouds while solving for an optimal deformation state.In addition to a correspondence algorithm, a technique for modeling a deformation field is required. Both [11] and [12] leverage FEM, which uses a 3D mesh to solve for unique deformation solutions. However, large deformations can cause mesh distortions with the need for remeshing. Mesh-free methods have been introduced to circumvent this limitation. The element-free Galerkin method is a mesh-free method that requires only nodal point data and uses a moving least-squares approximation to solve for a solution [13]. Other mesh-free methods are reviewed in [14]. Although these methods do not require a 3D mesh, solving for a solution can be costly and boundary condition designation is often unintuitive. Having identified these same shortcomings, [15] proposed regularized Kelvinlet functions for volumetric digital sculpting in computer animation applications. This sculpting approach provided de-formations consistent with linear elasticity without large computational overhead.In this work, we propose an image-to-physical registration method that uses regularized Kelvinlet functions as a novel deformation basis for nonrigid registration. Regularized Kelvinlet functions are analytical solutions to the equations for linear elasticity that we superpose to compute a nonrigid deformation field nearly instantaneously [15].We utilize ""grab"" and ""twist"" regularized Kelvinlet functions with a linearized iterative reconstruction approach (adapted from [12]) that is well-suited for sparse data registration problems. Sensitivity to regularized Kelvinlet function hyperparameters is explored on a supine MR breast imaging dataset. Finally, our approach is validated on an exemplar breast cancer case with a segmented tumor by comparing performance to previously proposed registration methods."
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,2,Methods,"In this section, closed-form solutions to linear elastic deformation responses in an infinite medium are derived to obtain regularized Kelvinlet functions. Then, methods for constructing a superposed regularized Kelvinlet function deformation basis for achieving registration within an iterative reconstructive framework are discussed. Equation notation is written such that constants are italicized, vectors are bolded, and matrices are double-struck letters. Linear elasticity in a homogeneous, isotropic media is governed by the Navier-Cauchy equations in Eq. ( 1), where E is Young's modulus, ν is Poisson's ratio, u(x) is the displacement vector, and F(x) is the forcing function. Analytical displacement solutions to Eq. ( 1) that represent elastostatic states in an infinite solid can be found for specific forcing functions F(x). Equation (2) represents the forcing function for a point source F δ (x), where f is the point source forcing vector and x 0 is the load location. The closed-form displacement solution for Eq. ( 1) given the forcing function in Eq. ( 2) is classically known as the Kelvin state in Eq. ( 3), rewritten as a function of r where r = xx 0 and r = r . The coefficients are a"
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,2.1,Regularized Kelvinlet Functions,", and I is the identity matrix.We note that the deformation response is linear with respect to f , which implies that forcing functions can be linearly superposed. However, practical use of Eq. ( 3) becomes numerically problematic in discretized problems because the displacement and displacement gradient become indefinite as x approaches x 0 .To address numerical singularity, regularization is incorporated with a new forcing function Eq. ( 4), where r ε = √ r 2 + ε 2 is the regularized distance, and ε is the regularization radial scale. Solving Eq. (1) using Eq. ( 4) yields a formula for the first type of regularized Kelvinlet functions used in this work in Eq. ( 5), which is the closed-form, analytical solution for linear elastic translational (""grab"") deformations.The second type of regularized Kelvinlet functions represent ""twist"" deformations which are derived by expanding the previous formulation to accommodate locally affine loads instead of displacement point sources. This is accomplished by associating each component of the forcing function Eq. ( 4) with the directional derivative of each basis g i of the affine transformation, leading to the regularized forcing matrix in Eq. ( 6). An affine loading configuration consisting of pure rotational (""twist"") deformation constrains F ij ε (x) to a skew-symmetric matrix that simplifies the forcing function to a cross product about a twisting force vector f in Eq. (7). The pure twist displacement field response u ε,twist (r) to the forcing matrix in Eq. ( 7) can be represented as the second type of regularized Kelvinlet functions used in this work in Eq. (8).Superpositions of Eq. ( 5) and Eq. ( 8) are used in a registration workflow to model linear elastic deformations in the breast. These deformations are visualized on breast geometry embedded in an infinite medium with varying ε values in Fig. 1."
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,2.2,Registration Task,"For registration, x 0 control point positions for k number of total regularized Kelvinlets ""grab"" and ""twist"" functions are distributed in a predetermined configuration. Then, the f grab and f twist vectors are optimized to solve for a displacement field that minimizes distance error between geometric data inputs. For a predetermined configuration of regularized Kelvinlet ""grab"" and ""twist"" functions centered at different x 0 control point locations, an elastically deformed state can be represented as the summation of all regularized Kelvinlet displacement fields where ∼ u (x) is the superposed displacement vector and k = k grab + k twist in Eq. (9). Equation ( 9) can be rewritten in matrix form shown in Eq. (10), where α is a concatenated vector of length 3k such that αThis formulation decouples the forcing magnitudes from the Kelvinlet response matrix ∼ K (x), which is composed of column u ε,grab (x) and u ε,twist (x) vectors calculated with unit forcing vectors for each K grab (x) and K twist (x) function. This allows for linear scaling of ∼ K (x) using α. By setting x 0 locations, ε grab , and ε twist as hyperparameters, deformation states can be represented by various α vectors with the registration task being to solve for the optimal α vector.An objective function is formulated to minimize misalignment between the moving space x moving and fixed space x fixed through geometric data constraints. For the breast imaging datasets in this work, we used simulated intraoperative data features that realistically could be collected in a surgical environment visualized in Fig. 2. The first data feature is MR-visible skin fiducial points placed on the breast surface (Fig. 2,red). These fiducials have known point correspondence. The other two data features are an intra-fiducial point cloud of the skin surface (Fig. 2, light blue) and sparse contour samples of the chest wall surface (Fig. 2, yellow). These data features are surfaces that do not have known correspondence. These data feature designations are consistent with implementations in previous work [16,17].For a given deformation state, each data feature contributes to the total error measure. For the point data, the error e i point for each point i is simply the distance magnitude between corresponding points in x fixed and x moving space. For the surface data, the error e i surface is calculated as the distance from every point i in the x fixed point cloud surface to the closest point in the x moving surface, projected onto the surface unit normal which allows for sliding contact between surfaces. The optimization using the objective function in Eq. ( 11) includes two additions to improve the solution. The first is rigid parameters, translation τ and rotation θ, that are optimized simultaneously with the vector α. β represents the deformation state with β = [α, τ , θ ], and this compensates for rigid deformation between x fixed and x moving . The second is a strain energy regularization term e SE which penalizes deformations with large strain energy. e SE is the average strain energy density within the breast geometry, and it is computed for each β at every iteration. It is scaled by weight w SE . The optimal state β is iteratively solved using Levenberg-Marquardt optimization terminating at | (β)|<10 -12 ."
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3,Experiments and Results,"In this section, two experiments are conducted. The first explores sensitivity to regularized Kelvinlet function hyperparameters k grab , k twist , ε grab , and ε twist and establishes optimal hyperparameters in a training dataset of 11 breast deformations. The second validates the registration method in a breast cancer patient and compares registration accuracy and computation time to previously proposed methods."
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.1,Hyperparameters Sensitivity Analysis,"This dataset consists of supine breast MR images simulating surgical deformations of 11 breasts from 7 healthy volunteers. Volunteers (ages 23-57) were enrolled in a study approved by the Institutional Review Board at Vanderbilt University. Prior to imaging, 26 skin fiducials were distributed on the breast surface. MR images (0.391 × 0.391 × 1 mm 3 or 0.357 × 0.357 × 1 mm 3 ) were acquired with the volunteers' arms placed by their sides. This image was used as the x moving space. The volunteers were then instructed to raise one arm above their heads, causing deformation of the ipsilateral breast. A second MR image in the deformed state was acquired to create simulated intraoperative physical data and to use for validation. This second image was used as the x fixed space. The breast in x moving was segmented at the boundary between the chest wall and breast parenchyma to create a 3D model. The posterior surface was labeled to inform x 0 control point locations. The skin fiducials and intra-fiducial surface point clouds were labeled in both images as data features. Sparse tracked ultrasound data collection patterns were projected on the posterior surface for use as the third data feature. Subsurface anatomical targets were labeled in both images and used to compute target error after registration.Three configurations were explored to test different distributions of grab and/or twist regularized Kelvinlet functions: grab functions only, twist functions only, and a combination of grab and twist functions. Grab function control points were distributed evenly on the posterior surface of the breast to approximate forces from the chest wall. Twist function control points were distributed evenly within the breast to approximate internal body forces. Three hyperparameter sweeps were used:  "
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.2,Registration Methods Comparison,"This dataset consists of supine breast MR images simulating surgical deformations from one breast cancer patient. A 71-year-old patient with invasive mammary carcinoma in the left breast was enrolled in a study approved by the Institutional Review Board at Vanderbilt University. Skin fiducial placement, image acquisition, arm placement, and preprocessing steps followed the same protocol detailed in Sect. 3.1. The tumor was segmented in both images by a subject matter expert, and a 3D tumor model was created to evaluate tumor overlap metrics after registration.Regularized Kelvinlet function registration was compared to 3 other registration methods: rigid registration, an FEM-based image-to-physical registration method, and an image-to-image registration method. A point-based rigid registration using the skin fiducials provided a baseline comparator for accuracy without deformable correction. The FEM-based image-to-physical registration method, detailed in [12] and implemented in breast in [16], utilizes the same optimization scheme as this method but with an FEM-generated basis. k = 40 control points were used for the FEM-based registration. The image-to-image registration method was a symmetric diffeomorphic method with explicit B-spline regularization publicly available in the Advanced Normalization Toolkit (ANTs) repository [19,20]. Image-to-image registration would not be possible for intraoperative registration in most surgical settings. However, it was included to demonstrate accuracy when volumetric imaging data is available, as opposed to sparse geometric point data as in the surgical application case. The rigid and image-to-physical registrations were performed on a single thread of a 3.6 GHz AMD Ryzen 7 3700X CPU. Image-to-image registration was multithreaded on 2.3 GHz Intel Xeon (E5-4610 v2) CPUs.Registration results for the 4 methods are shown in Table 1. The regularized Kelvinlet method accuracy was comparable (if not slightly improved) to the FEM-based method for this example case. Runtime for the regularized Kelvinlet method was improved compared to the FEM-based method. As expected, registration without deformable correction was poor, and image-to-image registration had the best accuracy. Registered tumor geometry results are shown in Fig. 4.  "
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,4,Limitations and Conclusion,"Several limitations should be noted. Regularized Kelvinlet functions describe solutions that assume a physical embedding within an infinite elastic domain, which does not account for organ-specific geometry. This approach may not be well suited for problems where geometry has significant influence. This method is derived from a linear elastic model, and nonlinear models are known to better describe soft tissue mechanics. Additionally, this method assumes homogeneity and isotropy -it does not account for different tissue types and directional structures in the breast. With regards to clinical feasibility, supine MR imaging with skin fiducials is not the standard-of-care. However, using supine MR imaging for surgery is becoming increasingly investigated, and previous work demonstrated the potential of ink-based skin fiducial markings on the breast [21,22]. Despite these limitations, this method's accuracy and speed may be appropriate for surgical guidance applications.In this work, we demonstrated the use of regularized Kelvinlet functions for imageto-physical registration of the breast. We achieved near real-time registration with comparable accuracy to previously proposed methods. We believe that this approach is generalizable to other soft-tissue organ systems and is well-suited for improving navigation during image-guided surgeries."
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,,Fig. 1 .,
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,,Fig. 2 .,
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,,•,
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,,Fig. 3 .,
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,,Fig. 4 .,
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,,Table 1 .,Target Error (mm)6.1 ± 1.4 3.3 ± 1.1 3.0 ± 1.1 2.3 ± 1.5
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,1,Introduction,"One of the significant logistical challenges facing hospital administrations today is operating room (OR) efficiency. This parameter is determined by many fac-tors, one of which is surgical procedure duration that reflects intracorporeal time, which in itself poses a challenge as, even across the same procedure type, duration can vary greatly. This variability is influenced by numerous elements, including the surgeon's experience, the patient's comorbidities, unexpected events occurring during the procedure, the procedure's complexity and more. Accurate realtime estimation of procedure duration improves scheduling efficiency because it allows administrators to dynamically reschedule before a procedure has run overtime. Another important aspect is the ability to increase patient safety and decrease complications by dosing and timing anesthetics more accurately.Currently, methods aiming for OR workflow optimization through ETC are lacking. One study showed that surgeons underestimated surgery durations by an average of 31 min, while anesthesiologists underestimated the durations by 35 min [21]. These underestimations drive inefficiencies, causing procedures to be delayed or postponed, forcing longer waiting times for patients. For example, a large variation of waiting time (47 ± 17 min) was observed in a study assessing 157 Cholecystectomy patients [15].As AI capabilities have evolved greatly in recent years, the field of minimally invasive procedures, which is inherently video-based, has emerged as a potent platform for the harnessing of these capabilities to improve both patient care and workflow efficiency. Consequently, ETC has become a technologically achievable and clinically beneficial task."
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,2,Related Work,"Initially, ETC studies performed preoperative estimates based on surgeon data, patient data, or a combination of these [2,10]. Later on, intraoperative estimates were performed, with some studies requiring manual annotations or the addition of external information [7,11,12,16]. Recently, a study by Twinanda et al. [23] achieved robust ETC results, even without incorporating external information, showing that video-based ETC is better than statistical analysis of past surgeons' data. However, all these studies have evaluated ETC using limited size datasets with inherent biases, as they are usually curated from a small number of surgeons and medical centers or exclude complex cases with significant unexpected events.In this work, we study the key elements important to the development of ETC models and perform an in-depth methodical analysis of this task. First, we suggest an adequate metric for evaluation -SMAPE, and introduce two new architectures, one based on LSTM networks and one on the transformer architecture. Then, we examine how different ETC methods perform when trained with various loss functions and show that their errors are not necessarily correlated. We then test the hypothesis that an ensemble composed of several ETC model variations can significantly improve estimation compared to any single model."
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,3,Methods,
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,3.1,Evaluation Metrics,"Mean Absolute Error (MAE). The evaluation metric used in prior work was MAE.where T is a video duration, y is the actual time left until completion, and ŷ is the ETC predictions. A disadvantage of MAE is its reliance on the magnitude of values, consequently, short videos are likely to have small errors while long videos are likely to have large errors. In addition, MAE does not consider the actual video duration or the temporal location for which the predictions are made.Symmetric Mean Absolute Percentage Error (SMAPE). SMAPE is invariant to the magnitude and keeps an equivalent scale for videos of different duration, thus better represents ETC performance [3,4,20]."
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,3.2,Datasets,"We focus on three different surgical video datasets (a total of 3,993 videos) that were curated from several medical centers (MC) and include procedures performed by more than 100 surgeons. The first dataset is Laparoscopic Cholecystectomy that contains 2,400 videos (14 MC and 118 surgeons). This dataset was utilized for the development and ablation study. Additionally, we explore two other datasets: Laparoscopic Appendectomy which contains 1,364 videos (5 MC and 61 surgeons), and Robot-Assisted Radical Prostatectomy (RARP) which contains 229 videos (2 MC and 14 surgeons). The first two datasets are similar, both are relatively linear and straightforward procedures, have similar duration distribution, and are abdominal procedures with similarities in anatomical views. However, RARP is almost four times longer on average. Therefore, it is interesting to explore how methods developed on a relatively short and linear procedure will perform on a much longer procedure type such as RARP. Table 3 in the appendix provides a video duration analysis for all datasets. The duration is defined as the difference between surgery start and end times, which is the time interval between scope-in and scope-out. All datasets were randomly divided into training, validation, and test sets with a ratio of 60/15/25%."
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,3.3,Loss Functions,"Loss values are calculated by comparing ETC predictions ( ŷt ) for each timestamp to the actual time left until the procedure is complete (y t ). The final loss for each video is the result of averaging these values across all timestamps.MAE Loss. The MAE loss is defined by:Smooth L1 Loss. The smooth L1 loss is less sensitive to outliers [9].in whichSMAPE Loss. Based on the understanding that SMAPE (Sect. 3.1) is a good representation of the ETC problem, we also formulated it as a loss function:Importantly, SMAPE produces higher loss values for the same absolute error as the procedure progresses, when the denominator is getting smaller. This property is valuable as the models should be more accurate as the surgery nears its end.Corridor Loss. A key assumption overlooked in developing ETC methods is that significant and time-impacting events might occur during a procedure. For example, a prolonged procedure due to significant bleeding occurring after 30 min of surgery is information that is absent from the model when providing predictions at the 10 min timestamp. To tackle this problem, we apply the corridor loss [17] that considers both the actual progress of a procedure and the average duration in the dataset (see Fig. 2 in the appendix for a visual example). The corridor loss acts as a wrapper (π) for other loss functions:Interval L1 Loss. The losses described above focus on the error between predictions and labels for each timestamp independently. Influenced by the total variation loss, we suggest considering the video's sequential properties. The interval L1 loss focuses on jittering in predictions between timestamps in a pre-defined interval, aiming to force them to act more continuously. ŷt are the predictions per timestamp, and S is an interval time span (jump) between two timestamps.Total Variation Denoising Loss. This loss is inspired by a 1D total variation denoising loss and was modified to fit as part of the ETCouple model.L total_variation_denoising (y, ŷ) = L squared_error (y, ŷ) + λ • L S=120 IntervalL1 (ŷ) (10)"
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,3.4,ETC Models,"Feature Representation. All models and experiments described in this work are based on fixed visual features that were extracted from the surgical videos using a pre-trained model. This approach allows for shorter training cycles, less computing requirements, and benefits from a model that was trained on a different task [6,14]. Previous works showed that pre-training could be done with either progress labels or surgical steps labels and that similar performances are achieved, with a slight improvement when using the steps label pre-training [1,23]. In this work, we use a pre-trained Video Transformer Network (VTN) [13] model with a Vision Transformer (ViT) [8] backbone as a feature extraction module. It was originally trained using the same training set (Sect. 3.2) for the step recognition task with a similar protocol to the one described by [5].Inferring ETC. Our ETC architectures end with a single shared fully connected (FC) layer and a Sigmoid that outputs two values: ETC and progress.ETC is inferred by averaging the predicted ETC value and the one calculated from the progress.where T is the video duration and t el marks the elapsed time. Inspired by [12], we also incorporate t max which is defined as the expected maximum video length. t max is applied to scale the elapsed time and ensures values in a range [0, 1]."
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,,ETC-LSTM.,"A simple architecture that consists of an LSTM layer with a hidden size of 128. Following hyperparameters tuning on the validation set, the ETC-LSTM was trained using an SGD optimizer with a constant learning rate of 0.1 and a batch size of 32 videos.ETCouple. ETCouple is a different approach to applying LSTM networks. In contrast to ETC-LSTM and similar methods which predict ETC for a single timestamp, here we randomly select one timestamp from the input video and set it as an anchor sample. The anchor is then paired with a past timestamp using a fixed interval of S = 120 s. The model is given two inputs, the features from the beginning of the procedure up to the anchor and the features up to the pair location. Instead of processing the entire video in an end-to-end manner, we only process past information and are thus able to use a bi-directional LSTM (hidden dimension is 128). The rest of the architecture contains a dropout layer (P = 0.5), the shared FC layer, and a Sigmoid function. We explored various hyperparameters and the final model was trained with a batch size of 16, an AdamW optimizer with a learning rate of 5 • 10 -4 , and a weight decay of 5 • 10 -3 .ETCformer. LSTM networks have been shown to struggle with capturing longterm dependencies [22]. Intuitively, ETC requires attending to events that occur in different temporal locations throughout the procedure. Thus, we propose a transformer-based network that uses attention modules [24]. The transformer encoder architecture has four self-attention heads with a hidden dimension of size 512. To allow this model to train in a framework where all the video's features are the input to the model but still maintain a causal system, we used a forward direction self-attention [18,19]. This is done by masking out future samples for each timestamp, thus relying only on past information. Best results on the validation set were achieved when training with a batch size of two videos, an AdamW optimizer with a learning rate of 10 -4 , and a weight decay of 0.1. 4 Results"
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,4.1,Ablation Experiments,"This section provides ablation studies on the Cholecystectomy dataset.Loss Functions. Table 1 provides a comparison on the same validation set when using one or the sum of a few loss functions. The classic approach of using LSTM produces the best results when using only MAE loss. However, ETCouple and ETCformer benefit from the combination of several loss functions.Error Analysis. To test whether the errors of the various models are correlated, we compared the predictions made by the different models on a per-video basis. We use SMAPE and analyze the discrepancy by comparing the difference of every two model variations independently. Then, we divided the videos into a similar and a dissimilar group, by using a fixed threshold, i.e., if the SMAPE difference is smaller than the threshold the models are considered as providing similar results. The threshold was empirically set to 2, deduced from the ETC curves, which are almost identical when the SMAPE difference is smaller than 2 (Fig. 1(a) and appendix Fig. 4. We demonstrate these results visually in Fig. 1(b). Interestingly, there are significant differences in SMAPE between different models (disagreement in more than 50%). ETC-LSTM and ETCouple show the highest disagreement.  Baseline Comparison. We reproduce RSDNet [23] on top of our extracted features and use it as a baseline for comparison. We followed all methodical details described in the original paper, only changing the learning rate reduction policy to match the same epoch proportion in our dataset. Table 2 shows that ETC-LSTM and RSDNet have similar results, ETC-LSTM achieves better SMAPE scores while RSDNet is more accurate in MAE. These differences can be the product of scaling the elapsed time using t max vs. s norm and shared vs. independent FC layer. The ETCformer model reaches the best SMAPE results but is still short on MAE.Ensemble Analysis. There are many tasks in machine learning in which data can be divided into easy or hard samples. We argue that the ETC task is different in these regards. Based on the error analysis, we explored how an ensemble of models performs and if it produces better results (Table 2). In contrast to a classic use case of models ensemble, in which the same model is trained with bootstrapping on different folds of data, here we suggest an ensemble that uses different models, which essentially learn to perform differently on the same input video. Figure 3 in the appendix illustrates the MAE error graph for the ensemble, presenting the mean and SD of the MAE. All model variations' performance is also provided in the appendix in Table 4. When using more than one model, the ETC predictions for each model are averaged into a single end result. "
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,4.2,Appendectomy and RARP Results,"We examine the results on two additional datasets to showcase the key elements explored in this work (Table 2). In Appendectomy, the ensemble also achieves the best results on the test set with a significant drop in SMAPE and MAE scores. The ETCformer performs the worst compared to other model variations, this might be because transformers require more data for training, therefore additional data could show its potential as seen in Cholecystectomy. The RARP dataset contains fewer videos, but they are of longer duration. Here too, the ensemble achieves better SMAPE scores."
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,5,Discussion,"In this work, we examine different architectures trained with several loss functions and show how SMAPE can be utilized as a better metric to compare ETC models. In the error analysis, we conclude that each model learns to operate differently on the same videos. This led us to explore an ensemble of models which eventually achieves the best results. Yet, this conclusion can facilitate future work, focusing on understanding the differences and commonalities of the models' predictions and developing a unified or enhanced model, potentially reducing the complexity of training several ETC models and achieving better generalizability. Future work should also incorporate information regarding the surgeon's experience, which may improve the model's performance. This work has several limitations. First, the proposed models need to be evaluated across other procedures and specialties for their potential to be validated further. Second, the ensemble's main disadvantage is its requirement for more computing resources. In addition, there may be data biases due to variability in the time it takes surgeons to perform certain actions at different stages of their training. Finally, although our model relies on video footage only, and no annotations are required for ETC predictions, manual annotations of surgical steps are still needed for pre-training of the feature extraction model.Real-time ETC holds great potential for surgical management. First, in optimizing OR scheduling, and second as a proxy to complications that cause unusual deviations in anticipated surgery duration. However, optimizing OR efficiency with accurate procedural ETC, based on surgical videos, has yet to be realized. We hope the information in this study will assist researchers in developing new methods and achieve robust performance across multiple surgical specialties, ultimately leading to better OR management and improved patient care."
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,,Fig. 1 .,
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,,Table 1 .,
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,,Table 2 .,
Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_16.
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,1,Introduction,"In some ways, surgical data is like the expanding universe: 95% of it is dark and unobservable [2]. The vast majority of intra-operative X-ray images, for example, are ""dark"", in that they are not further analyzed to gain quantitative insights into routine practice, simply because the human-hours required would drastically outweigh the benefits. As a consequence, much of this data not only goes un-analyzed but is discarded directly from the imaging modality after inspection. Fortunately, machine learning algorithms for automated intra-operative image analysis are emerging as an opportunity to leverage these data streams. A popular application is surgical phase recognition (SPR), a way to obtain quantitative analysis of surgical workflows and equip automated systems with situational awareness in the operating room (OR). SPR can inform estimates of surgery duration to maximize OR throughput [7] and augment intelligent surgical systems, e.g. for suturing [20] or image acquisition [4,10,11], enabling smooth transitions from one specialized subsystem to the next. Finally, SPR provides the backbone for automated skill analysis to produce immediate, granular feedback based on a specific surgeon's performance [5,21]. The possibilities described above have motivated the development of algorithms for surgical phase recognition based on the various video sources in the OR [15,19,22,23]. However, surgical phase recognition based on interventional X-ray sequences remains largely unexplored. Although X-ray guidance informs more than 17 million procedures across the United States (as of 2006) [13], the unique challenges of processing X-ray sequences compared to visible or structured light imaging have so far hindered research in this area. Video cameras collect many images per second from relatively stationary viewpoints. By contrast, C-arm Xray imaging often features consecutive images from vastly different viewpoints, resulting in highly varied object appearance due to the transmissive nature of X-rays. X-ray images are also acquired irregularly, usually amounting to several hundred frames in a procedure of several hours, limiting the availability of training data for machine learning algorithms.Following recent work that enables sim-to-real transfer in the X-ray domain [6], we now have the capability to train generalizable deep neural networks (DNNs) using simulated images, where rich annotations are freely available. This paper represents the first step in breaking open SPR for the X-ray domain, establishing an approach to categorizing phases, simulating realistic image sequences, and analyzing real procedures. We focus our efforts on percutaneous pelvic fracture fixation, which involves the acquisition of standard views and the alignment of Kirschner wires (K-wires) and orthopedic screws with bony corridors [17]. We model the procedure at four levels, the current target corridor, activity (position-wire, insert-wire, and insert-screw), C-arm view (AP, lateral, etc.), and frame-level clinical value. Because of radiation exposure for both patients and clinicians, it is relevant to determine which X-ray images are acquired in the process of ""fluoro-hunting"" (hunting) versus those used for clinical assessment. Each of these levels is modeled as a Markov process in a stochastic simulation, which provides fully annotated training data for a transformer architecture."
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,2,Related Work,"SPR from video sources is a popular topic, and has benefited from the advent of transformer architectures for analyzing image sequences. The use of convolutional layers as an image encoder has proven effective for recognizing surgical phases in endoscopic video [22] and laparoscopic video [3,19]. These works especially demonstrate the effectiveness of transformers for dealing with long image sequences [3], while added spatial annotations improve both the precision and information provided by phase recognition [19]. Although some work explores activity recognition in orthopedic procedures [8,9] they rely on head-mounted cameras with no way to assess tool-to-tissue relationships in percutaneous procedures. The inclusion of X-ray image data in this space recenters phase recognition on patient-centric data and makes possible the recognition of surgical phases which are otherwise invisible."
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,3,Method,"The Pelphix pipeline consists of stochastic simulation of X-ray image sequences, based on a large database of annotated CT images, and a transformer architecture for phase recognition with additional task-aware supervision. A statistical shape model is used to propagate landmark and corridor annotations over 337 CTs, as shown in Fig. 2a. The simulation proceeds by randomly aligning virtual K-wires and screws with the annotated corridors (Sect. 3.1). In Sect. 3.2, we describe a transformer architecture with a U-Net style encoder-decoder structure enables sim-to-real transfer for SPR in X-ray.  "
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,3.1,Image Sequence Simulation for Percutaneous Fixation,"Unlike sequences collected from real surgery [15] or human-driven simulation [14], our workflow simulator must capture the procedural workflow while also maintaining enough variation to allow algorithms to generalize. We accomplish this by modeling the procedural state as a Markov process, in which the transitions depend on evaluations of the projected state, as well as an adjustment factor λ adj ∈ [0, 1] that affects the number of images required for a given task. A low adjustment factor decreases the probability of excess acquisitions for the simulated procedure. In our experiments, we sample λ adj ∈ U(0.6, 0.8) at the beginning of each sequence. Figure 3 provides an overview of this process. Given a CT image with annotated corridors, we first sample a target corridor with start and endpoints a, b ∈ R 3 . For the ramus corridors, we randomly swap the start and endpoints to simulate the retrograde and antegrade approaches. We then uniformly sample the initial wire tip position within 5 mm of a and the direction within 15 • of ba. Sample Desired View. The desired view is sampled from views appropriate for the current target corridor. For example, appropriate views for evaluating wire placement in the superior ramus corridor are typically the inlet and obturator oblique views, and other views are sampled with a smaller probability. We refer to the ""oblique left"" and ""oblique right"" view independent of the affected patient side, so that for the right pubic ramus, the obturator oblique is the ""oblique left"" view, and the iliac oblique is ""oblique right."" We define the ""ideal"" principle ray direction r * for each standard view in the anterior pelvic plane (APP) coordinate system (see supplement) and the ideal viewing point p * as the midpoint of the target corridor. At the beginning of each sequence, we sample the intrinsic camera matrix of the virtual C-arm with sensor width w s ∼ U(300, 400) mm, d sd ∼ U(900, 1200), and an image size of 384 × 384. Given a viewing point and direction (p, r), the camera projection matrix P is computed with the X-ray source (or camera center) at p -d sp r and principle ray r, where d sp U(0.65 d sd , 0.75 d sd ) is the source-to-viewpoint distance, and d sd is the source-to-detector distance (or focal length) of the virtual C-arm.Evaluate View. Given a current view (p, r) and desired view (p * , r * ), we first evaluate whether the current view is acceptable and, if it is not, make a random adjustment. View evaluation considers the principle ray alignment and whether the viewing point is reasonably centered in the image, computing,where the angular tolerance θ t ∈ [3 • , 10 • ] depends on the desired view, ranging from teardrop views (low) to lateral (high tolerance).Sample View. If Eq. 1 is not satisfied, then we sample a new view (p, r) uniformly within a uniform window that shrinks every iteration by the adjustment factor, according towhere U • (c, r) is the uniform distribution in the sphere with center c and radius r, and U (r, θ) is the uniform distribution on the solid angle centered on r with colatitude angle θ. This formula emulates observed fluoro-hunting by converging on the desired view until a point, when further adjustments are within the same random window [12]. We proceed by alternating view evaluation and sampling until evaluation is satisfied, at which point the simulation resumes with the current activity: wire positioning, wire insertion, or screw insertion.Evaluate Wire Placement. During wire positioning, we evaluate the current wire position and make adjustments from the current view, iterating until evaluation succeeds. Given the current wire tip x, direction v, and projection matrix P, the wire placement is considered ""aligned"" if it appears to be aligned with the projected target corridor in the image, modeled as a cylinder. In addition, we include a small likelihood of a false positive evaluation, which diminishes as the wire is inserted.Sample Wire Placement. If the wire evaluation determines the current placement is unsuitable, then a new wire placement is sampled. For the down-thebarrel views, this is done similarly to Eq. 2, by bringing the wire closer to the corridor in 3D. For orthogonal views, repositioning consists of a small random adjustment to x, a rotation about the principle ray (the in-plane component), and a minor perturbation orthogonal to the ray (out-of-plane). This strategy emulates real repositioning by only adjusting the degree of freedom visible in the image, i.e. the projection onto the image plane:and θ * is the angle between the wire and the target corridor in the image plane.If the algorithm returns ""Good,"" the sequence either selects a new view to acquire (and stays in the position-wire activity) or proceeds to insert-wire or insert-screw, according to random transitions.In our experiments, we used 337 CT images: 10 for validation, and 327 for generating the training set. (Training images were collected continuously during development, after setting aside a validation set.) A DRR was acquired at every decision point in the simulation, with a maximum of 1000 images per sequence, and stored along with segmentations and anatomical landmarks. We modeled a K-wire with 2 mm diameter and orthopedic screws with lengths from 30 to 130 mm and a 16 mm thread, with up to eight instances of each in a given sequence. Using a customized version of DeepDRR [18], we parallelized image generation across 4 RTX 3090 GPUs with an observed GPU memory footprint of ∼ 13 GB per worker, including segmentation projections. Over approximately five days, this resulted in a training set of 677 sequences totaling 279,709 images and 22 validation sequences with 8,515 images."
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,3.2,Transformer Architecture for X-ray-based SPR,"Figure 1 shows the transformer architecture used to predict surgical phases based on embedding tokens for each frame. To encourage local temporal features in each embedding token, we cross-pollinate adjacent frames in the channel dimension, so that each (3, H, W ) encoder input contains the previous, current, and next frame. The image encoder is a U-Net [16] encoder-decoder variant with 5 Down and Up blocks and 33 spatial output channels, consisting of (a) 7 segmentation masks of the left hip, right hip, left femur, right femur, sacrum, L5 vertebra, and pelvis; (b) 8 segmentation masks of bony corridors, including the ramus (2), teardrop (2) and sacrum corridors (4), as in Fig. 2a; (c) 2 segmentation masks for wires and screws; and (d) 16 heatmaps corresponding to the anatomical landmarks in Fig. 2a. These spatial annotations provide additional supervision, trained with DICE loss L DICE for segmentation channels and normalized cross correlation L NCC for heatmap channels as in [1,6]. To compute tokens for input to the transformer, we apply a 1 × 1 Conv + BatchNorm + ReLU block with kernel size 512 to the encoder output, followed by global average pooling. The transformer has 6 layers with 8 attention heads and a feedforward dimension of 2048. During training and inference, we apply forward masking so that only previous frames are considered. The output of the transformer are vectors in R 21  Training Details. Following [6,11], we use a pipeline of heavy domain randomization techniques to enable sim-to-real transfer. In our experiments, we trained the transformer for 200 epochs on 2 RTX 3090 GPUs with 24 GB of memory each, with a sequence length of 48 and a batch size of 4. The initial learning rate was 0.0001, reduced by a factor of 10 at epoch 150 and 180."
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,4,Evaluation,"Simulation. We report the results of our approach first on simulated image sequences, generated from the withheld set of CT images, which serves as an upper bound on real X-ray performance. In this context our approach achieves an accuracy of 99.7%, 98.2%, 99.8%, and 99.0% with respect to the corridor, activity, view, and frame level, respectively. Moreover, we achieve an average DICE score of 0.72 and landmark detection error of 1.01 ± 0.153 pixels in simulation, indicating that these features provide a meaningful signal. That the model generalizes so well to the validation set reflects the fact that these sequences are sampled using the same Markov-based simulation as the training data. Cadaver Study. We evaluate our approach on cadaveric image sequences with five screw insertions. An attending orthopedic surgeon performed percutaneous fixation on a lower torso specimen, taking the antegrade approach for the left and right pubic ramus corridors, followed by the left and right teardrop and S1 screws. An investigator acted as the radiological technician, positioning a mobile C-arm according to the surgeon's direction. A total of 257 images were acquired during these fixations, with phase labels based on the surgeon's narration.Our results, shown in Fig. 4 demonstrate the potential for Pelphix as a viable approach to SPR in X-ray. We achieve an overall accuracy of 84%, 60%, 65%, and 77% with respect to the corridor, activity, view, and frame levels, respectively. Figure 5 shows exemplary success and failure modes for our approach, which struggles with ambiguities that may arise due to the similarity of certain views. For instance, image 98 was acquired during fluoro-hunting for the teardrop left view, but our approach associates this sequence with verification of the left ramus screw, which was just finished. Similarly, after the right teardrop wire was inserted, our approach anticipated the insertion of an S2 wire. This was a valid transition in our simulation, so surgeon preferences may be needed to resolve the ambiguity. At the same time, we observe significantly higher accuracy for the pubic ramus corridors (97.7, 76.9, 98.3, and 84.4% respectively) than the teardrop (60.2, 56.9, 64.2, 73.7%) and S1 corridors (100%, 40.6%, 23%, 71%), which may reflect the challenges of interpreting associated images or simply the accumulation of orthopedic hardware."
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,5,Discussion and Conclusion,"As our results show, Pelphix is a potentially viable approach to robust SPR based on X-ray images. We showed that stochastic simulation of percutaneous fracture fixation, despite having no access to real image sequences, is a sufficiently realistic data source to enable sim-to-real transfer. While we expect adjustments to the simulation approach will close the gap even further, truly performative SPR algorithms for X-ray may rely on Pelphix-style simulation for pretraining, before fine-tuning on real image sequences to account for human-like behavior. Extending this approach to other procedures in orthopedic surgery, angiography, and interventional radiology will require task-specific simulation capable of modeling possibly more complex tool-tissue interactions and human-in-the-loop workflows. Nevertheless, Pelphix provides a viable first route toward X-ray-based surgical phase recognition, which we hope will motivate routine collection and interpretation of these data, in order to enable advances in surgical data science that ultimately improve the standard of care for patients."
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,,Fig. 1 .,
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,,,
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,,Fig. 2 .,
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,,Fig. 3 .,
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,,Fig. 4 .,
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,,,
Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation,,Fig. 5 .,
Learnable Query Initialization for Surgical Instrument Instance Segmentation,1,Introduction,"Background: Minimally invasive surgeries (MIS) such as laparoscopic and endoscopic surgeries have gained widespread popularity due to the significant reduction in the time of surgery and post-op recovery [16,21]. Surgical instrument instance segmentation (SIIS) in these surgeries opens up the doors to increased precision and automation [18]. However, the problem is challenging due to the lack of large-scale well-annotated datasets, occlusions of tool-tip (the distinguishing part of the surgical instrument), rapid changes in the appearance, reflections due to the light source of the endoscope, smoke, blood spatter etc. [5].The Challenge: Most modern techniques for SIIS [11,18,31] are multi-stage architectures, with the first stage generating region proposals (rectilinear boxes) and the second stage classifying each proposal independently. Unlike natural images, rectilinear bounding boxes are not an ideal choice for medical instruments, which are long, thin, and often visible diagonally in a bounding box. Thus, the ratio of the visible tool area to the area of the bounding box is highly skewed in medical scenarios. E.g., the ratio is 0.45 for the Endovis17 dataset and 0.47 for Endovis18, the two popular MIS datasets. In contrast, it is 0.60 for the MS-COCO dataset of natural images. The ratio is important because lower numbers imply more noise due to background and a more difficult classification problem.Current Solution Strategy: Recently, S3Net [4] adapted the MaskRCNN [14] backbone to propose a 3-stage architecture. Their third stage implements hard attention based on the predicted masks from the second stage and re-classifies the proposals. The hard attention avoids distraction due to the presence of large background regions in the proposal boxes, allowing them to outperform all the previous state of the art for medical instruments or natural images.Our Observation: In the last few years, attention-based transformer architectures have outperformed CNN architectures for many computer-vision-based tasks. Recent transformer-based object detection models implement deformable attention [6,19,33] which predicts sampling points to focus attention on the fine-grained features in an image. One expects that this would allow transformer architectures to concentrate only on the tool instead of the background, leading to high accuracy for medical instrument instance segmentation. However, in our experiments, as well as the ones reported by [4], this is not observed. We investigate the reasons and report our findings on the probable causes. We also propose a solution strategy to ameliorate the problem. Our implementation of the strategy sets up a new state of the art for the SIIS problem."
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Contributions: (1),"We investigate the reason for the failure of transformerbased object detectors for the medical instrument instance segmentation tasks. Our analysis reveals that incorrect query initialization is to blame. We observe that recall of an instrument based on the initialized queries is a lowly 7.48% at 0.9 IOU, indicating that many of the relevant regions of interest do not even appear in the initialized queries, thus leading to lower accuracy at the last stage. (2) We observe that CNN-based object detectors employ a non-maximal suppression (NMS) at the proposal stage, which helps spread the proposal over the whole image. In contrast in transformer-based detection models, this has been replaced by taking the highest confidence boxes. In this paper, we propose to switch back to NMS-based proposal selection in transformers. (3) The NMS uses only bounding boxes and does not allow content interaction for proposal selection. We propose a Query Proposal Decoder block containing multiple layers of selfattention and deformable cross-attention to perform region-aware refinement of the proposals. The refined proposals are used by a transformer-based decoder backbone for the prediction of the class label, bounding box, and segmentation mask. (4) We show an improvement of 1.84% over the best-performing SOTA technique on the Endovis17 and 2.09% on the Endovis18 dataset as measured by ISI-IOU."
Learnable Query Initialization for Surgical Instrument Instance Segmentation,2,Related Work,"Datasets: Surgical tool recognition and segmentation has been a well-explored research topic [11]. Given the data-driven nature of recent computer vision techniques, researchers have also proposed multiple datasets for the problem. Twinanda et al. [28] have proposed an 80-video dataset of cholecystectomy surgeries, with semantic segmentation annotation for 7 tools. Al Hajj et al. [1] proposed a 50-video dataset of phacoemulsification cataract surgeries with 21 tools annotated for bounding boxes. Cadis [12] complements this dataset with segmentation masks. Ross et al. [25] in 2019 proposed a 30-video dataset corresponding to multiple surgeries with one instrument, annotated at image level for its presence. Endovis datasets, Endovis 2017 (EV17) [3] and Endovis 2018 (EV18) [2] have gained popularity in the recent past. Both of them have instance-level annotations for 7 tools. EV17 is a dataset of 10 videos of the Da Vinci robot, and EV18 is a dataset of 15 videos of abdominal porcine procedures.Instance Segmentation Techniques for Medical Instruments: Multiple attempts have been made to perform instance segmentation using these datasets. [11] and [18] use a MaskRCNN-based [14] backbone pre-trained on natural images and perform cross-domain fine-tuning. Wang et al. [31] assign categories to each pixel within an instance and convert the problem into a pixel classification. They then use the ResNet backbone to solve the problem. [29] modified the MaskRCNN architecture and proposed a Sample Consistency Network to bring closer the distribution of the samples at training and test time. Ganea et al. [10] use the concept of few-shot learning on top of MaskRCNN to improve the performance. Wentao et al. [8] add a mask prediction head to YoLo V3 [23] All these algorithms use CNN-based architectures, with ROI-Align, to crop the region of interest. Since the bounding boxes are not very tight in surgical cases due to the orientation of the tools, a lot of background information is passed along with the tool, and thereby the classification performance is compromised. Baby et al. [4] use a third-stage classifier on top of MaskRCNN to correct the misclassified masks and improve the performance."
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Transformer-based Instance Segmentation for Natural Images:,"On the other hand, transformer-based instance segmentation architectures [6,13,17,19] generate sampling points to extract the features and thereby learn more localised information. This gives extra leverage to transformer architectures to perform better classification. [17] propose the first transformer-based end-to-end instance segmentation architecture. They predict low-level mask embeddings and combine them to generate the actual masks. [13] learn the location-specific features by providing the information on position embeddings. [6] uses a deformable-multihead-attention-based mechanism to enrich the segmentation task. Mask DINO [19] utilizes better positional priors as originally proposed in [33]. They also perform box refinement at multiple levels to obtain the tight instance mask. In these architectures, the query initialization is done using the top-k region proposals based on their corresponding classification score. Thus ambiguity in the classification results in poor query initialization, and thereby the entire mask corresponding to that instance is missed. This leads to a significant reduction in the recall rate of these models."
Learnable Query Initialization for Surgical Instrument Instance Segmentation,3,Proposed Methodology,"Backbone Architecture: We utilize Mask DINO [19] as the backbone architecture for our model. As illustrated in Fig. 1, it uses ResNet [15] as the feature extractor to generate a multi-scale feature map at varying resolutions. These feature maps are then run through an encoder to generate an enhanced feature map with the same resolution as the original feature map. The enhanced feature maps are used to generate a set of region proposals. Then, the region proposals are sorted based on their classification logit values. Mask DINO uses a d dimensional query vector to represent an object's information. The top n q generated region proposals are used to initialize a set of n q queries. These queries are then passed through a series of decoder layers to obtain a set of refined queries. These refined queries are used for the purpose of detection, classification, and segmentation. Problems with Mask DINO: The model predicts various outputs using the queries initialized with the top n q region proposals. Our analysis reveals that most false negative outputs are due to the queries initialized with significantly fewer to no region proposals corresponding to the missed objects. During the initialization procedure, Mask DINO sorts region proposals based on their classification logit values. The surgical instruments resemble one another and have few distinguishing features. In case of label confusion, which happens often in medical instruments, the encoder outputs a proposal with low confidence. When sorted, these low-confidence proposals get deleted. Hence, in the proposed architecture we argue for class-independent proposal selection, which does not rely on the classification label or its confidence at this stage.Query Proposal Network: Spatial Diversification of the Proposals: The proposed Query Proposal Network (QPN) is shown in Fig. 1. QPN takes the enhanced feature maps as input and performs a pixel-wise classification and regression to obtain the initial region proposals. These initial region proposals undergo Non-Maximal Suppression (NMS) in order to remove the duplicates, but more importantly, output the proposal boxes which are spread all through the image. This is important because, given the complexity of the classification in medical instruments, we do not wish to overly rely on the classification label and would rather explore more regions of interest in the decoder. Hence, we choose top k proposals based on the NMS instead of the label confidence. "
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Query Proposal Network: Content-based Inter-proposal Interaction:,"The top k region proposals from NMS are used to initialize the queries for the proposed Query Proposal Decoder (QPD). Note that the NMS works only on the basis of box coordinates and does not take care of content embeddings into account. We try to make up for the gap through the QPD module. The QPD module consists of self-attention, cross-attention and feed-forward layers. The self-attention layer of QPD allows the queries to interact with each other and the duplicates are avoided. We use the standard deformable cross-attention module as proposed in [35]. Unlike traditional cross-attention-based mechanisms, this method attends only to a fixed number of learnable key points around the middlemost pixel of every region proposal (query) irrespective of the size of the feature maps. This allows us to achieve better convergence in larger feature maps. Thus, the cross-attention layer allows the interaction of queries with the enhanced feature maps from the encoder and feature representation for each query is obtained. The feed-forward layer refines the queries based on the feature representation obtained in the previous layer. We train the QPD layers using the mask and bounding box loss as is common in transformer architectures [19]. Note that no classification loss is back-propagated. This allows the network to perform query refinement irrespective of the classification label and retains queries corresponding to the object instances, which were omitted due to ambiguity in classification. The queries outputted by the QPD module are used to initialize the standard decoder network of Mask DINO. Implementation Details: We train the proposed architecture using three kinds of losses, classification loss L cls , box regression loss L box , and the mask prediction loss L mask . We use focal loss [20] as L cls . We use 1 and GIOU [24] loss for L box .For L mask , we use cross entropy and IOU (or dice) loss. The total loss is:Through hyper-parameter tuning, we set λ = [0.19, 0.24, 0.1, 0.24, 0.24]. We use a batch size of 8. The initial learning rate is set to 0.0001, which drops by 0.1 after every 20 epochs. We set 0.9 as the Nesterov momentum coefficient. We train the network for 50 epochs on a server with 8 NVidia A100, 40 GB GPUs.Besides QPN we use the exact same architecture as proposed in MaskDINO [19]. However, we perform transfer learning using the MaskDINO pre-trained weights, and therefore, we do not use ""GT+noise"" as an input to the decoder."
Learnable Query Initialization for Surgical Instrument Instance Segmentation,4,Results and Discussion,"Evaluation Methodology: We demonstrate the performance of the proposed methodology on two benchmark Robot-assisted endoscopic surgery datasets Endovis 2017 [3] (denoted as EV17), and Endovis 2018 [2] (denoted as EV18) as performed by [4]. EV17 is a dataset of 10 videos (1800 images) obtained from the Da Vinci robotic system with 7 instruments. We adopt the same four-fold cross-validation strategy as shown by [26] for the EV17 dataset. EV18 is a real surgery dataset of 15 videos containing 1639 training and 596 validation images, with 7 instruments. [4] corrected the misclassified ground truth labels and added instance-level annotations to this dataset. We evaluate the performance of the proposed algorithm using challenge IOU as proposed by [3], as well as ISI IOU and Mean class IOU as reported in [11]. We also report per instrument class IOU as suggested by [4].Quantitative Results on EV17, EV18, and Cadis: The results of our technique for the EV17 dataset are shown in Table 1 and for the EV18 dataset are shown in Table 2. It can be observed that the proposed technique outperforms the best-performing SOTA methods by 1.84% in terms of challenge IOU for EV17 and 1.96% for EV18 datasets. Due to the paucity of space, we show the performance of the proposed methodology on the Cadis [12] dataset in the supplementary material. The qualitative analysis of instance segmentation by the proposed methodology against the other SOTA algorithms is shown in Fig. 2. We demonstrate improved performance in the occluded and overlapping cases. We observe a testing speed of 40 FPS on a standard 40GB Nvidia A100 GPU cluster.  Evidence of Query Improvement: The improvement in the query initialization due to the proposed architecture is demonstrated in Fig. 3. Here, we mark the centre of initialized query boxes and generate the scatter plot. The average recall rate for the EV18 dataset at 0.9 IOU for the top 300 queries in vanilla Mask DINO is 7.48%. After performing NMS, the recall rate decreases to 0.00%, but the queries get diversified. After passing the same through the proposed Query Proposal Decoder (QPD), the recall rate is observed to be 52.38%. The increase in recall rate to 52.38% indicates successful learning of QPD. It indicates that the proposed architecture is able to cover more objects in the initialized queries thereby improving the performance at the end. While the diversity of the queries is important, it is also important to ensure that the queries with higher confidence are near the ground truth to ensure better learning. Random initialization for NMS is observed with sub-optimal performance and is shown in Table 3."
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Ablation Study:,We perform an ablation on the EV18 dataset to show the importance of each block in the proposed methodology. The results of the same are summarised in Table 3. We also experiment with the number of layers in QPD. We have used the best-performing 2-layer QPD architecture for all other experiments. The reduction in performance with more QPD layers can be attributed to the under-learning of negative samples due to stricter query proposals
Learnable Query Initialization for Surgical Instrument Instance Segmentation,5,Conclusion,"In this paper, we proposed a novel class-agnostic Query Proposal Network (QPN) to better initialize the queries for a transformer-based surgical instrument instance segmentation model. Towards this, we first diversified the queries using the non-maximal suppression and proposed a deformable-cross-attentionbased learnable Query Proposal Decoder (QPD). On average, the proposed QPN improved the recall rate of the query initialization by 52.38% at 0.9 IOU. The improvement translates to an improved ISI-IOU of 1.84% and 2.09% in the publicly available Endovis 2017 and Endovis 2018 datasets, respectively."
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Fig. 2 .,
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Fig. 3 .,
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,,
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,NMS Enhanced Feature Maps Query Proposal Network Decoder Input Image Queries Initialisation Enhanced Feature Maps Query Proposal Decoder x k Queries Initialisation Pixel-wise Classification and Regression Self Attention Deformable Cross Attention Feed Forward Network Decoder Output Boxes Output Masks Self Attention Feed Forward Network Encoder Self Attention Deformable Cross Attention Feed Forward Network Backbone Feature Maps Fig,. 1. Proposed Query Proposal Network
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Table 1 .,
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Table 2 .,
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Table 3 .,
Learnable Query Initialization for Surgical Instrument Instance Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 70.
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,1,Introduction,"We address the important problem of intraoperative patient-to-image registration in a new way by relying on preoperative data to synthesize plausible transformations and appearances that are expected to be found intraoperatively. In particular, we tackle intraoperative 3D/2D registration during neurosurgery, where preoperative MRI scans need to be registered with intraoperative surgical views of the brain surface to guide neurosurgeons towards achieving a maximal safe tumor resection [22]. Indeed, the extent of tumor removal is highly correlated with patients' chances of survival and complete resection must be balanced against the risk of causing new neurological deficits [5] making accurate intraoperative registration a critical component of neuronavigation.Most existing techniques perform patient-to-image registration using intraoperative MRI [11], CBCT [19] or ultrasound [9,17,20]. For 3D-3D registration, 3D shape recovery of brain surfaces can be achieved using near-infrared cameras [15], phase-shift 3D shape measurement [10], pattern projections [17] or stereovision [8]. The 3D shape can subsequently be registered with the preoperative MRI using conventional point-to-point methods such as iterative closest point (ICP) or coherent point drift (CPD). Most of these methods rely on cortical vessels that bring salient information for such tasks. For instance, in [6], cortical vessels are first segmented using a deep neural network (DNN) and then used to constrain a 3D/2D non-rigid registration. The method uses physics-based modeling to resolve depth ambiguities. A manual rigid alignment is however required to initialize the optimization. Alternatively, cortical vessels have been used in [13] where sparse 3D points, manually traced along the vessels, are matched with vessels extracted from the preoperative scans. A model-based inverse minimization problem is solved by estimating the model's parameters from a set of pre-computed transformations. The idea of pre-computing data for registration was introduced by [26], who used an atlas of pre-computed 3D shapes of the brain surface for registration. In [7], a DNN is trained on a set of pre-generated preoperative to intraoperative transformations. The registration uses cortical vessels, segmented using another neural network, to find the best transformation from the pre-generated set.The main limitation of existing intraoperative registration methods is that they rely heavily on processing intraoperative images to extract image features (eg., 3D surfaces, vessels centerlines, contours, or other landmarks) to drive registration, making them subject to noise and low-resolution images that can occur in the operating room [2,25]. Outside of neurosurgery, the concept of pregenerating data for optimizing DNNs for intraoperative registration has been investigated for CT to x-ray registration in radiotherapy where x-ray images can be efficiently simulated from CTs as digital radiographic reconstructions [12,27]. In more general applications, case-centered training of DNNs is gaining in popularity and demonstrates remarkable results [16]."
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Contribution:,"We propose a novel approach for patient-to-image registration that registers the intraoperative 2D view through the surgical microscope to preoperative MRI 3D images by learning Expected Appearances. As shown in Fig. 1, we formulate the problem as a camera pose estimation problem that finds the optimal 3D pose minimizing the dissimilarity between the intraoperative 2D image and its pre-generated Expected Appearance. A set of Expected Appearances are synthesized from the preoperative scan and for a set of poses covering the range of plausible 6 Degrees-of-Freedom (DoF) transformations. This set is used to train a patient-specific pose regressor network to obtain a model that is texture-invariant and is cross-modality to bridge the MRI and RGB camera modalities. Similar to other methods, our approach follows a monocular singleshot registration, eliminating cumbersome and tedious calibration of stereo cameras, the laser range finder, or optical trackers. In contrast to previous methods, our approach does not involve processing intraoperative images which have several advantages: it is less prone to intraoperative image acquisition noise; it does not require pose initialization; and is computationally fast thus supporting real-time use. We present results on both synthetic and clinical data and show that our approach outperformed state-of-the-art methods."
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Expected Appearances MRI Scan,
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Intraoperative Registration Image Synthesis Pose Regression,Pose Sampling Fig. 1. Our approach estimates the 6-DoF camera pose that aligns a preoperative 3D mesh derived from MRI scans onto an intraoperative RGB image acquired from a surgical camera. We optimize a regressor network PΩ over a set of Expected Appearances that are generated by first sampling multiple poses and appearances from the 3D mesh using neural image analogy through SΘ.
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2,Method,
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2.1,Problem Formulation,"As illustrated in Fig. 1, given a 3D surface mesh of the cortical vessels M, derived from a 3D preoperative scan, and a 2D monocular single-shot image of the brain surface I, acquired intraoperatively by a surgical camera, we seek to estimate the 6-DoF transformation that aligns the mesh M to the image I. Assuming a set of 3D points u = {u j ∈ R 3 } ⊂ M and a set of 2D points in the image v = {v i ∈ R 2 } ⊂ I, solving for this registration problem can be formalized as finding the 6-DoF camera pose that minimizes the reprojection error:where R ∈ SO(3) and t ∈ R 3 represent a 3D rotation and 3D translation, respectively, and A is the camera intrinsic matrix composed of the focal length and the principal points (center of the image) while {c i } i is a correspondence map and is built so that if a 2D point v i corresponds to a 3D point u j where c i = j for each point of the two sets. Note that the set of 3D points u is expressed in homogenous coordinates in the minimization of the reprojection error.In practice, finding the correspondences set {c i } i between u and v is nontrivial, in particular when dealing with heterogeneous preoperative and intraoperative modality pairs (MRI, RGB Cameras, ultrasound, etc.) which is often the case in surgical guidance. Existing methods often rely on feature descriptors [14], anatomical landmarks [13], or organ's contours and segmentation [6,18] involving tedious processing of the intraoperative image that is sensitive to the computational image noise. We alleviate these issues by directly minimizing the dissimilarity between the image I and its Expected Appearance synthesized from M.By defining a synthesize function S Θ that synthesizes a new image I given a projection of a 3D surface mesh for different camera poses, i.e. I = S Θ (A[R|t], M), the optimization problem above can be rewritten as:argminThis new formulation is correspondence-free, meaning that it alleviates the requirement of the explicit matching between u and v. This is one of the major strengths of our approach. It avoids the processing of I at run-time, which is the main source of registration error. In addition, our method is patient-specific, centered around M, since each model is trained specifically for a given patient. These two aspects allow us to transfer the computational cost from the intraoperative to the preoperative stage thereby optimizing intraoperative performance.The following describes how we build the function S Θ and how to solve Eq. 1. "
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2.2,Expected Appearances Synthesis,"We define a synthesis network S Θ : (A[R|t], M, T) → I, that will generate a new image resembling a view of the brain surface from the 2D projection of the input mesh M following [R|t], and a texture T. Several methods can be used to optimize Θ. However, they require a large set of annotated data [3,24] or perform only on modalities with similar sensors [12,27]. Generating RGB images from MRI scans is a challenging task because it requires bridging a significant difference in image modalities. We choose to use a neural image analogy method that combines the texture of a source image with a high-level content representation of a target image without the need for a large dataset [1]. This approach transfers the texture from T to I constrained by the projection of M using A[R|t] by minimizing the following loss function:where G l ij (T ) is the Gram matrix of texture T at the l-th convolutional layer (pre-trained VGG-19 model), and w l,c T class are the normalization factors for each Gram matrix, normalized by the number of pixels in a label class c of T class . This allows for the quantification of the differences between the texture image T and the generated image I as it is being generated. Importantly, computing the inner-most sum over each label class c allows for texture comparison within each class, for instance: the background, the parenchyma, and the cortical vessels.In practice, we assume constant camera parameters A and first sample a set of binary images by randomly varying the location and orientation of a virtual camera [R|t] w.r.t. to the 3D mesh M before populating the binary images with the textures using S Θ (see Fig. 2). We restrict this sampling to the upper hemisphere of the 3D mesh to remain consistent with the plausible camera positions w.r.t. patient's head during neurosurgery.We use the L-BFGS optimizer and 5 convolutional layers of VGG-19 to generate each image following [1] to find the resulting parameters Θ. The training to synthesize for a single image typically takes around 50 iterations to converge."
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2.3,Pose Regression Network,"In order to solve Eq. 1, we assume a known focal length that can be obtained through pre-calibration. To obtain a compact representation of the rotation and since poses are restricted to the upper hemisphere of the 3D mesh (No Gimbal lock), the Euler-Rodrigues representation is used. Therefore, there are six parameters to be estimated: rotations r x , r y , r z and translations t x , t y , t z . We estimate our 6-DoF pose with a regression network P Ω : I → p and optimize its weights Ω to map each synthetic image I to its corresponding camera pose p = [r x , r y , r z , t x , t y , t z ] T .The network architecture of P Ω consists of 3 blocks each composed of two convolutional layers and one ReLU activation. To decrease the spatial dimension, an average pooling layer with a stride of 2 follows each block except the last one. At the end of the last hierarchy, we add three fully-connected layers with 128, 64, and 32 neurons and ReLU activation followed by one fully-connected with 6 neurons with a linear activation. We use the set of generated Expected Appearances T P = {(I i ; p i )} i ; and optimize the following loss function over the parameters Ω of the network P Ω :where t and R vec are the translation and rotation vector, respectively. We experimentally noticed that optimizing these entities separately leads to better results. The model is trained for each case (patient) for 200 epochs using mini-batches of size 8 with Adam optimizer and a learning rate of 0.001 and decays exponentially to 0.0001 over the course of the optimization. Finally, at run-time, given an image I we directly predict the corresponding 3D pose p so that: p ← P(I; Ω), where Ω is the resulting parameters from the training."
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,3,Results,"Dataset. We tested our method retrospectively on 6 clinical datasets from 6 patients (cases) (see Fig. 5). These consisted of preoperative T1 contrast MRI scans and intraoperative images of the brain surface after dura opening. Cortical vessels around the tumors were segmented and triangulated to generate 3D meshes using 3D Slicer. We generated 100 poses for each 3D mesh (i.e.: each case) and used a total of 15 unique textures from human brain surfaces (different from our 6 clinical datasets) for synthesis using S Θ . In order to account for potential intraoperative brain deformations [4] we augment the textured projection with elastic deformation [21] resulting in approximately 1500 images per case. The surgical images of the brain (left image of the stereoscopic camera) were acquired with a Carl Zeiss surgical microscope. The ground-truth poses were obtained by manually aligning the 3D meshes on their corresponding images. We evaluated the pose regressor network on both synthetic and real data. The model training and validation were performed on the synthesized images while the model testing was performed on the real images. Because a conventional train/validation/test split would lead to texture contamination, we created our validation dataset so that at least one texture is excluded from the training set. On the other hand, the test set consisted of the real images of the brain surface acquired using the surgical camera and are never used in the training. Accuracy-threshold curves on the validation set.Metrics. We chose the average distance metric (ADD) as proposed in [23] for evaluation. Given a set of mesh's 3D vertices, the ADD computes the mean of the pairwise distance between the 3D model points transformed using the ground truth and estimated transformation. We also adjusted the default 5 cm-5 deg translation and rotation error to our neurosurgical application and set the new threshold to 3 mm-3 deg.Accuracy-Threshold Curves. We calculated the number of 'correct' poses estimated by our model. We varied the distance threshold on the validation sets (excluding 2 textures) in order to reveal how the model performs w.r.t. that threshold. We plotted accuracy-threshold curves showing the percentage of pose accuracy variation with a threshold in a range of 0 mm to 20 mm. We can see in Fig. 3 that a 80.23% pose accuracy was reached within the 3 mm-3 deg threshold for all cases. This accuracy increases to 95.45% with a 5 mm-5 deg threshold."
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Validation and Evaluation of Texture Invariance.,"We chose to follow a Leave-one-Texture-out cross-validation strategy to validate our model. This strategy seemed the most adequate to prevent over-fitting on the textures. We measured the ADD errors of our model for each case and report the results in  We observed a variance in the ADD error that depends on which texture is left out. This supports the need for varying textures to improve the pose estimation. However, the errors remain low, with a 2.01 ± 0.58 mm average ADD error, over all cases. The average ADD error per case (over all left-out textures) is reported in Table 1. We measured the impact of the number of textures on the pose accuracy by progressively adding new textures to the training set, starting from 3 to 12 textures, while leaving 3 textures out for validation. We kept the size of the training set constant to not introduce size biases.  shows that increasing the number and variation of textures improved model performances. Test and Comparison on Clinical Images. We compared our method (Ours) with segmentation-based methods (ProbSEG) and (BinSEG) [7]. These methods use learning-based models to extract binary images and probability maps of cortical vessels to drive the registration. We report in Table 1 the distances between the ground truth and estimated poses. Our method outperformed ProbSEG and BinSEG with an average ADM error of 3.26 ± 1.04 mm compared to 4.13 ± 0.70 mm and 8.67 ± 2.84 mm, respectively. Our errors remain below clinically measured neuronavigation errors reported in [4], in which a 5.26 ± 0.75 mm average initial registration error was measured in 15 craniotomy cases using intraoperative ultrasound. Our method outperformed ProbSEG in 5 cases out of 6 and BinSEG in all cases and remained within the clinically measured errors without the need to segment cortical vessels or select landmarks from the intraoperative image. Our method also showed fast intraoperative computation times. It required an average of only 45 ms to predict the pose (tested on research code on a laptop with NVidia GeForce GTX 1070 8 GB without any specific optimization), suggesting a potential use for real-time temporal tracking.Figure 5 shows our results as Augmented Reality views with bounding boxes and overlaid meshes. Our method produced visually consistent alignments for all 6 clinical cases without the need for initial registration. Because our current method does not account for brain-shift deformation, our method produced some misalignment errors. However, in all cases, our predictions are similar to the ground truth. "
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,4,Discussion and Conclusion,"Clinical Feasibility. We have shown that our method is clinically viable. Our experiments using clinical data showed that our method provides accurate registration without manual intervention, that it is computationally efficient, and it is invariant to the visual appearance of the cortex. Our method does not require intraoperative 3D imaging such as intraoperative MRI or ultrasound, which require expensive equipment and are disruptive during surgery. Training patient-specific models from preoperative imaging transfers computational tasks to the preoperative stage so that patient-to-image registration can be performed in near real-time from live images acquired from a surgical microscope.Limitations. The method presented in this paper is limited to 6-DoF pose estimation and does not account for deformation of the brain due to changes in head position, fluid loss, or tumor resection and assumes a known focal length. In the future, we will expand our method to model non-rigid deformations of the 3D mesh and to accommodate expected changes in zoom and focal depth during surgery. We will also explore how texture variability can be controlled and adapted to the observed image to improve model accuracy."
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Conclusion.,"We introduced Expected Appearances, a novel learning-based method for intraoperative patient-to-image registration that uses synthesized expected images of the operative field to register preoperative scans with intraoperative views through the surgical microscope. We demonstrated state-ofthe-art, real-time performance on challenging neurosurgical images using our method. Our method could be used to improve accuracy in neuronavigation and in image-guided surgery in general."
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Fig. 2 .,
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Fig. 3 .,
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Fig. 4 .,
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Fig. 4 -,
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Fig. 5 .,
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,,Table 1 .,
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,1,Introduction,"Radiotherapy (RT) has proven effective and efficient in treating cancer patients. However, its application depends on treatment planning involving target lesion and radiosensitive organs-at-risk (OAR) segmentation. This is performed to guide radiation to the target and to spare OAR from inappropriate irradiation. Hence, this manual segmentation step is very time-consuming and must be performed accurately and, more importantly, must be patient-safe. Studies have shown that the manual segmentation task accounts for over 40% of the treatment planning duration [7] and, in addition, it is also error-prone due to expert-dependent variations [2,24]. Hence, deep learning-based (DL) segmentation is essential for reducing time-to-treatment, yielding more consistent results, and ensuring resource-efficient clinical workflows.Nowadays, training of DL segmentation models is predominantly based on loss functions defined by geometry-based (e.g., SoftDice loss [15]), distributionbased objectives (e.g., cross-entropy), or a combination thereof [13]. The general strategy has been to design loss functions that match their evaluation counterpart. Nonetheless, recent studies have reported general pitfalls of these metrics [4,19] as well as a low correlation with end-clinical objectives [11,18,22,23]. Furthermore, from a robustness point of view, models trained with these loss functions have been shown to be more prone to generalization issues. Specifically, the Dice loss, allegedly the most popular segmentation loss function, has been shown to have a tendency to yield overconfident trained models and lack robustness in out-of-distribution scenarios [5,14]. These studies have also reported results favoring distribution-matching losses, such as the cross-entropy being a strictly proper scoring rule [6], providing better-calibrated predictions and uncertainty estimates. In the field of RT planning for brain tumor patients, the recent study of [17] shows that current DL-based segmentation algorithms for target structures carry a significant chance of producing false positive outliers, which can have a considerable negative effect on applied radiation dose, and ultimately, they may impact treatment effectiveness. In RT planning, the final objective is to produce the best possible radiation plan that jointly targets the lesion and spares healthy tissues and OARs. Therefore, we postulate that training DL-based segmentation models for RT planning should consider this clinical objective.In this paper, we propose an end-to-end training loss function for DL-based segmentation models that considers dosimetric effects as a clinically-driven learning objective. Our contributions are: (i) a dosimetry-aware training loss function for DL segmentation models, which (ii) yields improved model robustness, and (iii) leads to improved and safer dosimetry maps. We present results on a clinical dataset comprising fifty post-operative glioblastoma (GBM) patients. In addition, we report results comparing the proposed loss function, called Dose-Segmentation Loss (DOSELO), with models trained with a combination of binary cross-entropy (BCE) and SoftDice loss functions."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,2,Methodology,"Figure 1 describes the general idea of the proposed DOSELO. A segmentation model (U-Net [20]) is trained to output target segmentation predictions for the Gross Tumor Volume (GTV) based on patient MRI sequences. Predicted segmentations and their corresponding ground-truth (GT) are fed into a dose predictor model, which outputs corresponding dose predictions (denoted as D P and D P in Fig. 1). A pixel-wise mean squared error between both dose predictions is then A segmentation model (U-Net [20]) is trained to output target segmentation predictions ( ST ) for the Gross Tumor Volume (GTV) based on patient MRI sequences IMR. Predicted ( ST ) and ground-truth segmentations (ST ) are fed into the dose predictor model along with the CT-image (ICT ), and OAR segmentation (SOR). The dose predictor outputs corresponding dose predictions DP and DP . A pixel-wise mean squared error between both dose predictions is calculated, and combined with the binary crossentropy (BCE) loss to form the final loss, L total = LBCE + λLDSL. calculated and combined with the BCE loss to form the final loss. In the next sections we describe the adopted dose prediction model [9,12], and the proposed DOSELO."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,2.1,Deep Learning-Based Dose Prediction,"Recent DL methods based on cascaded U-Nets have demonstrated the feasibility of generating accurate dose distribution predictions from segmentation masks, approximating analytical dose maps generated by RT treatment planning systems [12]. Originally proposed for head and neck cancer [12], this approach has been recently extended for brain tumor patients [9] with levels of prediction error below 2.5 Gy, which is less than 5% of the prescribed dose. This good level of performance, along with its ability to yield near-instant dose predictions, enables us to create a training pipeline that guides learned features to be dose-aware.Following [12], the dose predictor model consists of a cascaded U-Net (i.e., the input to the second U-Net is the output of the first concatenated with the input to the first U-Net) trained on segmentation masks, CT images, and reference dose maps. The model's input is a normalized CT volume and segmentation masks for target volume and OARs. As output, it predicts a continuous-valued dose map of the same dimension as the input. The model is trained via deep supervision as a linear combination of L2-losses from the outputs of each U-Net in the cascade. We refer the reader to [9,12] for further implementation details. We remark that the dose predictor model was also trained with data augmentation, so imperfect segmentation masks and corresponding dose plans are included. This allows us in this study to use the dose predictor to model the interplay between segmentation variability and dosimetric changes.Formally, the dose prediction model M D receives as inputs: segmentations masks for the GTV S T ∈ Z W ×H and the OARs S OR ∈ Z W ×H , the CT image (used for tissue attenuation calculation purposes in RT) I CT ∈ R W ×H , and outputs M D (S T , S OR , I CT ) → D P ∈ R W ×H , a predicted dose map where each pixel value in D corresponds to the local predicted dose in Gy. Due to the limited data availability, we present results using 2D-based models but remark that their extension to 3D is straightforward. Working in 2D is also feasible from an RT point of view because the dose predictor is based on co-planar volumetric modulated arc therapy (VMAT) planning, commonly used in this clinical scenario."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,2.2,Dose Segmentation Loss (DOSELO),"During the training of the segmentation model, we used the dose predictor model to generate pairs of dose predictions for the model-generated segmentations and the GT segmentations. The difference between these two predicted dose maps is used to guide the segmentation model. The intuition behind this is to guide the segmentation model to yield segmentation results being dosimetrically consistent with the dose maps generated via the corresponding GT segmentations. To guide the training process with dosimetry information stemming from segmentation variations, we propose to use the mean squared error (MSE) between dose predictions for the GT segmentation (S T ) and the predicted segmentation ( S T ), and construct the following dose-segmentation loss,where D i P and D i P denote pixel-wise dose predictions. The final loss is then,where λ is a hyperparameter to weigh the contributions of each loss term. We remark that during training we use standard data augmentations including spatial transformations, which are also subjected to dose predictions, so the model is informed about relevant segmentation variations producing dosimetry changes."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,3,Experiments and Results,
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,3.1,Data and Model Training,We divide the descriptions of the two separate datasets used for the dose prediction and segmentation models.
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,,Dose Prediction:,"The dose prediction model was trained on an in-house dataset comprising a total of 50 subjects diagnosed with post-operative GBM. This includes CT imaging data, segmentation masks of 13 OARs, and the GTV. GTVs were defined according to the ESTRO-ACROP guidelines [16]. The OARs were contoured by one radiotherapist according to [21] and verified by mutual consensus of three experienced radiation oncology experts. Each subject had a reference dose map, calculated using a standardized clinical protocol with Eclipse (Varian Medical Systems Inc., Palo Alto, USA). This reference was generated on basis of a double arc co-planar VMAT plan to deliver 30 times 2 Gy while maximally sparing OARs. We divided the dataset into training (35 cases), validation (5 cases), and testing (10 cases). We refer the reader to [9] for further details.Segmentation Models: To develop and test the proposed approach, we employed a separate in-house dataset (i.e., different cases than those used to train the dose predictor model) of 50 cases from post-operative GMB patients receiving standard RT treatment. We divided the dataset into training (35 cases), validation (5 cases), and testing (10 cases). All cases comprise a planning CT registered to the standard MRI images (T1-post-contrast (Gd), T1-weighted, T2-weighted, FLAIR), and GT segmentations containing OARs as well as the GTV. We note that for this first study, we decided to keep the dose prediction model fixed during the training of the segmentation model for a simpler presentation of the concept and modular pipeline. Hence, only the parameters of the segmentation model are updated."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,,Baselines and Implementation Details:,"We employed the same U-Net [20] architecture for all trained segmentation models, with the same training parameters but two different loss functions, to allow for a fair comparison. As a strong comparison baseline, we used a combo-loss formed by BCE plus SoftDice, which is also used by nnUNet and recommended by its authors [8]. This combo-loss has also been reported as an effective one [13]. For each loss function, we computed a five-fold cross-validation. Our method1 was implemented in PyTorch 1.13 using Adam optimizer [10] with β 1 = 0.9, β 2 = 0.999, batch normalization, dropout set at 0.2, learning rate set at 10 -4 , 2 • 10 4 update iterations, and a batch size of 16. The architecture and trained parameters were kept constant across compared models. Training and testing were performed on an NVIDIA Titan X GPU with 12 GB RAM. The input image size is 256 × 256 pixels with an isotropic spacing of 1 mm."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,3.2,Evaluation,"To evaluate the proposed DOSELO, we computed dose maps for each test case using a standardized clinical protocol with Eclipse (Varian Medical Systems Inc., Palo Alto, USA). We calculated dose maps for segmentations using the stateof-the-art BCE+SoftDice and the proposed DOSELO. For each obtained dose map, we computed the dose score [12], which is the mean absolute error between the reference dose map (D ST ) and the dose map derived from the corresponding segmentation result (D ST , where S T ∈ {BCE+SoftDice, DOSELO}), and set it relative to the reference dose map (D ST ) (see Eq. 5)."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,,RM AE,"Although it has been shown that geometric-based segmentation metrics poorly correlate with the clinical end-goal in RT [4,11,18,23], we report in supplementary material Dice and Hausdorff summary statistics as well (supplementary Table 3). We nonetheless reemphasize our objective to move away from such proxy metrics for RT purposes and promote the use of more clinically-relevant ones."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,3.3,Results,"Figure 2 shows results on the test set, sorted by their dosimetric impact. We found an overall reduction of the relative mean absolute error (RMAE) with respect to the reference dose maps, from 0.449 ± 0.545, obtained via the BCE+SoftDice combo-loss, to 0.258 ± 0.201 for the proposed DOSELO (i.e., an effective 42.5% reduction with λ = 1). This significant dose error reduction shows the ability of the proposed approach to yield segmentation results in better agreement with dose maps obtained using GT segmentations than those obtained using the state-of-the-art BCE+SoftDice combo-loss.Table 1 shows results for the first and most significant four cases from a RT point of view (due to space limitations, all other cases are shown in supplementary material). We observe the ability of the proposed approach to significantly reduce outliers, generating a negative dosimetry impact on the dose Fig. 2. Relative mean absolute dose errors/differences (RMAE) between the reference dose map and dose maps obtained using the predicted segmentations. Lower is better. Across all tested cases and folds we observe a large RMAE reduction for dose maps using the proposed DOSELO (average RMAE reduction of 42.5%).maps. We analyzed case number 3, 4, and 5 from Fig. 2 for which the standard BCE+SoftDice was slightly better than the proposed DOSELO. For case no. 3 the tumor presents a non-convex shape alongside the skull's parietal lobe, which was not adequately modeled by the training dataset used to train the segmentation models. Indeed, we remark that both models failed to yield acceptable segmentation quality in this area. In case no. 4, both models failed to segment the diffuse tumor area alongside the skull; however, as shown in Fig. 2-case no. 4, the standard BCE+SoftDice model would yield a centrally located radiation dose, with strong negative clinical impact to the patient. Case no. 5 (shown in supplementary material) is an interesting case called butterfly GBM, which is a rare type of GBM (around 2% of all GBM cases [3]), characterized by bihemispheric involvement and invasion of the corpus callosum. In this case, the training data also lacked characterization for such cases. Despite this limitation, we observed favorable dose distributions with the proposed method.Although we are aware that classical segmentation metrics poorly correlate with dosimetric effects [18], we report that the proposed method is more robust than the baseline BCE+SoftDice loss function, which yields outliers with Hausdorff distances: 64.06 ± 29.84 mm vs 28.68 ± 22.25 mm (-55.2% reduction) for the proposed approach. As pointed out by [17], segmentation outliers can have a detrimental effect on RT planning. We also remark that the range of HD values is in range with values reported by models trained using much more training data (see [1]), alluding to the possibility that the problem of robustness might not be directly solvable with more data. Dice coefficients did not deviate significantly between the baseline and the DOSELO models (DSC: 0.713 ± 0.203 (baseline) vs. 0.697 ± 0.216 (DOSELO)).Table 1. Comparison of dose maps and their absolute differences to the reference dose maps (BCE+SoftDice (BCE+SD), and the proposed DOSELO). It can be seen that DOSELO yields improved dose maps, which are in better agreement with the reference dose maps (dose map color scale: 0 (blue) -70Gy (red)). "
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,,Case Input Image,
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,4,Discussion and Conclusion,"The ultimate goal of DL-based segmentation for RT planning is to provide reliable and patient-safe segmentations for dosimetric planning and optimally targeting tumor lesions and sparing of healthy tissues. However, current loss functions used to train models for RT purposes rely solely on geometric considerations that have been shown to correlate poorly with dosimetric objectives [11,18,22,23]. In this paper, we propose a novel dosimetry-aware training loss function, called DOSELO, to effectively guide the training of segmentation models toward dosimetric-compliant segmentation results for RT purposes. The proposed DOSELO uses a fast-dose map prediction model, enabling model guidance on how dosimetry is affected by segmentation variations. We merge this information into a simple yet effective loss function that can be combined with existing ones. These first results on a dataset of post-operative GBM patients show the ability of the proposed DOSELO to deliver improved dosimetric-compliant segmentation results. Future work includes extending our database of GBM cases and to other anatomies, as well as verifying potential improvements when cotraining the segmentation and dose predictor models, and jointly segmenting GTVs and OARs. With this study, we hope to promote more research toward clinically-relevant DL training loss functions."
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,,Fig. 1 .,
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,,,
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,,,
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,1,Introduction,"Residual tumor in the cavity after head and neck cancer (HNC) surgery is a significant concern as it increases the risk of cancer recurrence and can negatively impact the patient's prognosis [1]. HNC comprises the third highest positive surgical margins (PSM) rate across all oncology fields [2]. Achieving clear margins can be challenging in some cases, particularly in tumors with involved deep margins [3,4].During transoral robotic surgery (TORS), surgeons may assess the surgical margin via visual inspection, palpation of the excised specimen and intraoperative frozen sections analysis (IFSA) [5]. In the surgical cavity, surgeons visually inspect for residual tumors and use specimen driven or defect-driven frozen section analysis to identify any residual tumor [6,7]. The latter involves slicing a small portion of the tissue at the edge of the cavity and performing a frozen section analysis. These approaches are error-prone and can result in PSMs and a higher risk of cancer recurrence [7]. In an effort to improve these results, recent studies reported the use of exogenous fluorescent markers [8] and wide-field optical coherence tomography [9] to inspect PSMs in the excised specimen. While promising, each modality presents certain limitations (e.g., time-consuming analysis, administration of a contrast agent, controlled lighting environment), which has limited their clinical adoption [10,11].Label-free mesoscopic fluorescence lifetime imaging (FLIm) has been demonstrated as an intraoperative imaging guidance technique with high classification performance (AUC = 0.94) in identifying in vivo tumor margins at the epithelial surface prior to tumor excision [12]. FLIm can generate optical contrast using autofluorescence derived from tissue fluorophores such as collagen, NADH, and FAD. Due to the sensitivity of these fluorophores to their microenvironment, the presence of tumor changes their emission properties (i.e., intensity and lifetime characteristics) relative to healthy tissue, thereby enabling the optical detection of cancer [13].However, ability of label-free FLIm to identify residual tumors in vivo in the surgical cavity (deep margins) has not been reported. One significant challenge in developing a FLIm-based classifier to detect tumor in the surgical cavity is the presence of highly imbalanced labels.Surgeons aim to perform an en bloc resection, removing the entire tumor and a margin of healthy tissue around it to ensure complete excision. Therefore, in most cases, only healthy tissue in left in the cavity. To address the technical challenge of highly imbalanced label distribution and the need for intraoperative real-time cavity imaging, we developed an intraoperative FLIm guidance model to identify residual tumors by classifying residual cancer as anomalies. Our proposed approach identified all patients with PSM. In contrast, the IFSA reporting a sensitivity of 0.5 [6,7]. "
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2,Method,"As illustrated in Fig. 1, the proposed method uses a clinically-compatible FLIm system coupled to the da Vinci SP transoral robotic surgical platform to scan the surgical cavity in vivo and acquire FLIm data. We used the cumulative distribution transform (CDT) of the fluorescence decay curves extracted from the FLIm data as the input feature. The novelty detection model classified FLIm points closer to the healthy distribution as healthy and further from the healthy distribution as a residual tumor. We implemented the image guidance by augmenting the classification map to the surgical view using the predictor output and point locations of the scan."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.1,FLIm Hardware and Data Acquisition,"This study used a multispectral fluorescence lifetime imaging (FLIm) device to acquire data [14]. The FLIm device features a 355 nm UV laser for fluorescence excitation, which is pulsed at a 480 Hz repetition rate. A 365 µm multimode optical fiber (0.22 NA) delivers excitation light to tissue and relays the corresponding fluorescence signal to a set of dichroic mirrors and bandpass filters to spectrally resolve the autofluorescence. Three variable gain UV enhanced Si APD modules with integrated trans-impedance amplifiers receive the autofluorescence, which is spectrally resolved as follows: (1) 390/40 nm attributed to collagen autofluorescence, (2) 470/28 nm to NADH, and (3) 542/50 nm to FAD. The resulting autofluorescence waveform measurements for each channel are averaged four times, thus with a 480 Hz excitation rate, resulting in 120 averaged measurements per second [15].The FLIm device includes a 440 nm continuous wave laser that serves as an aiming beam; this aiming beam enables real-time visualization of the locations where fluorescence (point measurements) is collected by generating visible blue illumination at the location where data is acquired. Segmentation of the 'aiming beam' allows for FLIm data points to be localized as pixel coordinates within a surgical white light image (see Fig. 1). Localization of these coordinates is essential to link the regions where data is obtained to histopathology, which is used as the ground truth to link FLIm optical data to pathology status [16]. FLIm data was acquired using the da Vinci SP robotic surgical platform. As part of the approved protocol for this study, the surgeon performed in vivo FLIm scan on the tumor epithelial surface and the surrounding uninvolved benign tissue. Upon completing the scan, the surgeon proceeded with en bloc excision of the tissue suspected of cancer. An ex vivo FLIm scan was then performed on the surgically excised specimen. Finally, the patient's surgical cavity was scanned to check for residual tumor."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.2,Patient Cohort and FLIm Data Labeling,"The research was performed under the approval of the UC Davis Institutional Review Board (IRB) and with the patient's informed consent. All Patients were anesthetized, intubated, and prepared for surgery as part of the standard of care. N = 22 patients are represented in this study, comprising HNC in the palatine tonsil (N = 15) and the base of the tongue (N = 7). For each patient, the operating surgeon conducted an en bloc surgical tumor resection procedure (achieved by TORS-electrocautery instruments), and the resulting excised specimen was sent to a surgical pathology room for grossing. The tissue specimen was serially sectioned to generate tissue slices, which were then formalin-fixed, paraffin-embedded, sectioned, and stained to create Hematoxylin & Eosin (H&E) slides for pathologist interpretation (see Fig. 1).After the surgical excision of the tumor, an in vivo FLIm scan of approximately 90 s was conducted within the patient's surgical cavity, where the tumor was excised. To validate optical measurements to pathology labels (e.g., benign tissue vs. residual tumor), pathology labels from the excision margins were digitally annotated by a pathologist on each H&E section. The aggregate of H&E sections was correspondingly labeled on the ex vivo specimen at the cut lines where the tissue specimen was serially sectioned.Thereafter, the labels were spatially registered in vivo within the surgical cavity. This process enables the direct validation of FLIm measurements to the pathology status of the electrocauterized surgical margins (see Table 1)."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.3,FLIm Preprocessing,"The raw FLIm waveform contains background noise, instrument artifacts, and other types of interference, which need to be carefully processed and analyzed to extract meaningful information (i.e., the fluorescence signal decay characteristics). To account for background noise, the background signal acquired at the beginning of each clinical case was subtracted from the measured raw FLIm waveform. To retrieve the fluorescence function, we used a non-parametric model based on a Laguerre expansion polynomials and a constrained least-square deconvolution with the instrument impulse response function as previously described [17]. In addition, an SNR threshold of ≥50 dB was applied as a filtering criterion to select FLIm points with good signal quality."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.4,Novelty Detection Model,"The state-of-the-art novelty detection models were comprehensively reviewed in the literature [18,19]. Due to its robust performance, we chose the Generalized One-class Discriminative Subspaces (GODS) classification model [20] to classify healthy FLIm points from the residual tumor. The model trained only on the healthy FLIm points and use a semi-supervised technique to classify residual tumor from healthy. The GODS is a pairwise complimentary classifier defined by two separating hyperplanes to minimize the distance between the two classifiers, limiting the healthy FLIm data within the smallest volume and maximizing the margin between the hyperplanes and the data, thereby avoiding overfitting while improving classification robustness. The first hyperplane (w 1 , b 1 ) projects most of the healthy FLIm points to the positive half of the space, whereas the second hyperplane (w 2 , b 2 ) projects most of the FLIm points in the negative half.minwhere W 1 , W 2 are the orthonormal frames, minis the Stiefel manifold, η is the sensitivity margin, and was set η = 0.4 for our experiments. ν denote a penalty factor on these soft constraints, and b is the biases. x i denotes the training set containing CDT of the concatenated FLIm decay curve across channels 1-3 along the time axis. The CDT of the concatenated decay curves is computed as follows: Normalize the decay curves to 0-1. Compute and normalize the cumulative distribution function (CDF). Transforming the normalized CDF into the cumulative distribution transform by taking the inverse cumulative distribution function of the normalized CDF [21]."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.5,Classifier Training and Evaluation,"The novelty detection model used for detecting residual cancer is evaluated at the pointmeasurement level to assess the diagnostic capability of the method over an entire tissue surface. The evaluation followed a leave-one-patient-out cross-validation approach. The study further compared GODS with two other novelty detection models: robust covariance and, one-class support vector machine (OC-SVM) [22]. Novelty detection model solely used healthy labels from the in vivo cavity scan for training. The testing data contained both healthy and residual cancer labels. We used grid search to optimize the hyper-parameters and features used in each model and are tabulated in the supplementary section Table S1. The sensitivity, specificity, and accuracy were used as evaluation metrics to assess the performance of classification models in the context of the study.Results of a binary classification model using SVM are also shown in the supplementary section Table S2."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.6,Classifier Augmented Display,"The classifier augmentation depends on three independent processing steps: aiming beam localization, motion correction, and interpolation of the point measurements. A detailed description of implementing the augmentation process is discussed in [23]. The interpolation consists of fitting a disk to the segmented aiming beam pixel location for each point measurement and applying a color map (e.g., green: healthy and red: cancer) for each point prediction. Individual pixels from overlapping disks are averaged to produce the overall classification map and augmented to the surgical field as a transparent overlay."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,3,Results,"Table 2 tabulates the classification performance comparison of novelty detection models for classifying residual cancer vs. healthy on in vivo FLIm scans in the cavity. Three novelty detection models were evaluated, and all three models could identify the presence of residual tumors in the cavity for the three patients. However, the extent of the tumor classification over the entire tissue surface varied among the models. The GODS reported the best classification performance with an average sensitivity of 0.75 ± 0.02 (see Fig. 2). The lower standard deviation indicates that the model generalizes well. The OC-SVM and robust covariance reported a high standard deviation, indicating that the performance of the classification model is inconsistent across different patients. The model's ability to correctly identify negative instances is essential to its reliability. The GODS model reported the highest mean specificity of 0.78 ± 0.14 and the lowest standard deviation. The Robust Covariance model reported the lowest specificity, classifying larger portions of healthy tissue in the cavity as a residual tumor; indicating that the model did not generalize well to the healthy labels. We also observed that changing the hyper-parameter, such as the anomaly factor, biased the model toward a single class indicating overfitting (see supplementary section Fig. S1).The GODS uses two separating hyperplanes to minimize the distance between the two classifiers by learning a low-dimensional subspace containing FLIm data properties of healthy labels. Residual tumor labels are detected by calculating the distance between the projected data points and the learned subspace. Points that are far from the subspace are classified as residual tumors. We observed that the GODS with the FLIm decay curves in the CDT space achieve the best classification performance compared to other novelty detection models with a mean accuracy of 0.76 ± 0.02. This is mainly due to the robustness of the model, the ability to handle high-dimensional data, and the contrast in the FLIm decay curves. The contrast in the FLIm decay curves was further improved in the CDT space by transforming the FLIm decay curves to a normalized scale and improving linear separability."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,4,Discussion,"Curent study demonstrates that label-free FLIm parameters-based classification model, using a novelty detection aproach, enables identification of residual tumors in the surgical cavity. The proposed model can resolve residual tumor at the point-measurement level over a tissue surface. The model reported low point-level false negatives and positives. Moreover, the current approach correctly identified all patients with PSMs (see Fig. 2). This enhances surgical precision for TORS procedures otherwise limited to visual inspection of the cavity, palpation of the excised specimen, and IFSA. The FLImbased classification model could help guide the surgical team in real-time, providing information on the location and extent of cancerous tissue.In context to the standard of care, the proposed residual tumor detection model exhibits high patient-level sensitivity (sensitivity = 1) in detecting patients with PSMs. In contrast, defect-driven IFSA reports a patient-level sensitivity of 0.5 [6,7]. Our approach exhibits a low patient-level specificity compared to IFSA. Surgeons aim to achieve negative margins, meaning the absence of cancer cells at the edges of the tissue removed during surgery. The finding of positive margins from final histology would result in additional surgical resection, potentially impacting the quality of life. Combining the proposed approach and IFSA could lead to an image-guided frozen section analysis to help surgeons achieve negative margins in a more precise manner. Therefore, completely resecting cancerous tissue and improving patient outcomes.The false positive predictions from the classification model presented two trends: false positives in an isolated region and false positives spreading across a larger region. Isolated false positives are often caused by the noise of the FLIm system and are accounted for by the interpolation approach used for the classifier augmentation (refer to supplementary section Fig. S2). On the other hand, false positives spreading across a larger region are much more complex to interpret. One insight is that the electrocautery effects on the tissues in the cavity may have influenced them [24]. According to Jackson's burn wound model, the thermal effects caused by electrocautery vary with the different burnt zones. We observed a correlation between a larger spread of false positive predictions associated with a zone of coagulation to a zone of hyperemia.The novelty detection model generalizes to the healthy labels and considers data falling off the healthy distribution as residual cancer. The FLIm properties associated with the healthy labels in the cavity are heterogeneous due to the electrocautery effects. Electrocautery effects are mainly thermal and can be observed by the levels of charring in the tissue. Refining the training labels based on the levels of charring could lead to a more homogeneous representation of the training set and result in an improved classification model with better generalization."
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,5,Conclusion,This study demonstrates a novel FLIm-based classification method to identify residual cancer in the surgical cavity of the oropharynx. The preliminary results underscore the significance of the proposed method in detecting PSMs. The model will be validated on a larger patient cohort in future work and address the limitations of the point-level false positive and negative predictions. This work may enhance surgical precision for TORS procedures as an adjunctive technique in combination with IFSA.
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,,Fig. 1 .,
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,,Fig. 2 .,
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,,Table 1 .,
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,,Table 2 .,
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,1,Introduction,"Transoesophageal echocardiography (TEE) is a valuable diagnostic and monitoring imaging modality with widespread use in cardiovascular surgery for anaesthesia management and outcome assessment, as well as in emergency and intensive care medicine. The quality of TEE views is important for diagnosis and professional organisations publish guidelines for performing TEE exams [5,22]. These guidelines standardise TEE view acquisition and set benchmarks for the education of new echocardiographers. Computational methods for automated quality assessment (AQA) will have great impact, guaranteeing quality of examinations and facilitating training of new TEE operators. Deep models for AQA have shown promise in transthoracic echocardiography (TTE) and other ultrasound (US) applications [1,9,13,14]. Investigation of such methods in the real TEE domain remains underexplored and has been restricted to simulated datasets from Virtual Reality (VR) systems [15]. Although VR technology is useful for developing and retaining US scanning skills [7,10,16,17,20], AQA methods developed on simulation settings cannot meet real-world usability without addressing the significant domain gap. As shown in Fig. 1, there are significant content differences between simulated and real TOE images. Simulated images are free of speckle noise and contain only the heart muscle, ignoring tissue in the periphery of the heart. In this work, we take the first step in exploring AQA in the real TEE domain. We propose to leverage readily accessible simulated data, and transfer knowledge learned in the simulated domain, to boost performance in real TEE space.To alleviate the domain mismatch, a feasible solution is unsupervised domain adaptation (UDA). UDA aims to increase the performance on the target domain by using labelled source data with unlabelled target data to reduce the domain shift. For example, Mixstyle [24], performs style regularization by mixing instance-level feature statistics of training samples. The most relevant UDA work for our AQA regression task is representation subspace distance (RSD) [4], which aligns features from simulated and real domains via representation sub-spaces. Despite its effectiveness in several tasks, performing UDA on the simulation-to-real AQA task of TEE has two key challenges that need to be addressed. From Fig. 1, it is evident that: 1) there are many unknown intradomain shifts in real TEE images due to different scanning views and complex heart anatomy, which requires uncertainty estimation; 2) the inter-domain gap (heart appearance, style, and resolution) between simulated and real data is considerable, necessitating robust, domain-invariant features.In this paper, we propose a novel UDA regression network named SR-AQA that performs style alignment between TEE simulated and real domains while retaining domain-invariant and task-specific information to achieve promising AQA performance. To estimate the uncertainty of intra-domain style offsets in real data, we employ uncertainty-aware feature stylization (UFS) utilizing multivariate Gaussians to regenerate feature statistics (i.e. mean and standard deviation) of real data. To reduce the inter-domain gap, UFS augments simulated features to resemble diverse real styles and obtain real-stylized variants. We then design a style consistency learning (SCL) strategy to learn domaininvariant representations by minimizing the negative cosine similarity between simulated features and real-stylized variants in an extra feature space. Enforcing task-specific learning (TL) in real-stylized variants allows SR-AQA to keep task-specific information useful for AQA. Our work represents the original effort to address the TEE domain shift in AQA tasks. For method evaluation, we present the first simulation-to-real TEE dataset with two AQA tasks (see Fig. 1), and benchmark the proposed SR-AQA model against four state-of-the-art UDA methods. Our proposed SR-AQA outperforms other UDA methods, achieving 2.13%-5.08% and 4.37%-16.28% mean squared error (MSE) reduction for two AQA tasks, respectively."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,2,Methodology,
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,2.1,Dataset Collection,"We collected a dataset of 16,192 simulated and 4,427 real TEE images from 9 standard views. From Fig. 1, it is clear that significant style differences (e.g. brightness, contrast, acoustic shadowing, and refraction artifact) exist between simulated and real data, posing a considerable challenge to UDA. Simulated images were collected with the HeartWorks TEE simulation platform from 38 participants of varied experience asked to image the 9 views. Fully anonymized real TEE data were collected from 10 cardiovascular procedures in 2 hospitals, with ethics for research use and collection approved by the respective Research Ethics Committees. Each image is annotated by 3 expert anaesthetists with two independent scores w.r.t. two AQA tasks for TEE. The criteria percentage (CP) score ranging from ""0-100"", measuring the number of essential criteria, from the checklists (provided in supplementary material) of the ASE/SCA/BSE imaging guidelines [5], met during image acquisition and a general impression (GI) score ranging from ""0-4"", representing overall US image quality.As the number of criteria thus the maximum score varies for different views, we normalise CP as a percentage to provide a consistent measure across all views. Scores from the 3 raters were averaged to obtain the final score for each view. The Pearson product-moment correlation coefficients between CP and GI are 0.81 for simulated data and 0.70 for real data, indicating that these two metrics are correlated but focus on different clinical quality aspects. Inter-rater variability is assessed using the two-way mixed-effects interclass correlation coefficient (ICC) with the definition of absolute agreement. Both CP and GI, show very good agreement between the 3 annotators with ICCs of 0.959, 0.939 and 0.813, 0.758 for simulated and real data respectively."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,2.2,Simulation-to-Real AQA Network (SR-AQA),"Overview of SR-AQA. Illustrated in Fig. 2, the proposed SR-AQA is composed of ResNet-50 [6] encoders, regressors and projectors, with shared weights. Given the simulated x s and real TEE image x r as input, SR-AQA first estimates the uncertainty in the real styles of x s , from the batch of real images and transfers real styles to simulated features by normalizing their feature statistics (i.e. mean and standard deviation) via UFS. Then, we perform style consistency learning with L SCL and task-specific learning with L T L for the final real-stylized features f s→r f inal and the final simulated features f s f inal to learn domain-invariant and task-specific information. Ultimately, the total loss function of SR-AQA is2 is the MSE loss calculated from the simulated data result R s and its label y s , while λ 1 and λ 2 are parameters empirically set to ""10"" and ""1"" to get a uniform order of magnitude at the early training stage. The input is fed into one encoder and regressor to predict the score during inference."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,,Uncertainty-Aware Feature Stylization (UFS).,"The UFS pipeline is shown in the right part of Fig. 2. Different domains generally have inconsistent feature statistics [12,21]. Since style is related to the features' means and standard deviations [2,8,11,24], simulated features can be augmented to resemble real styles by adjusting their mean and standard deviation with the help of unlabelled real data. Let f s l and f r l be the simulated features and real features extracted from the l th layer of the encoder, respectively. We thus can transfer the style of f r l to f s l to obtain real-stylized features f s→r l as:where: μ(f ) and σ(f ) denote channel-wise mean and standard deviation of feature f , respectively. However, due to the complexity of real-world TEE, there are significant intra-domain differences, leading to uncertainties in the feature statistics of real data. To explore the potential space of unknown intra-domain shifts, instead of using fixed feature statistics, we generate multivariate Gaussian distributions to represent the uncertainty of the mean and standard deviation in the real data. Considering this, the new feature statistics of real features f r l , i.e. mean β(f r l ) and standard deviation α(f r l ), are sampled from, respectively and computed as:where:2 are estimated from the mean and standard deviation of the batch B of real images, I ρ>0.5 is an indicator function and ρ ∼ U(0, 1). Finally, our UFS for l th layer is defined as:To this end, the proposed UFS approach can close the reality gap by generating real-stylized features with sufficient variations, so that the network interprets real data as just another variation."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,,Style Consistency Learning (SCL).,"Through the proposed UFS, we obtain the final real-stylized features f s→r f inal that contain a diverse range of real styles. The f s→r f inal can be seen as style perturbations of the final simulated features f s f inal . We thus incorporate a SCL step, that maximizes the similarity between f s f inal and f s→r f inal to enforce their consistency in the feature level, allowing the encoder to learn robust representations. Specifically, the SCL adds a projector independently of the regressor to transform the f s f inal (f s→r f inal ) in an extra feature embedding, and then matches it to the other one. To prevent the Siamese encoder and Siamese projector (i.e. the top two encoders and projectors in Fig. 2) from collapsing to a constant solution, similar to [3], we adopt the stop-gradient (stopgrad) operation for the projected features z s and z s→r . The SCL process is summarized as:is the negative cosine similarity between the input features f 1 and f 2 , and • 2 is L2-normalization.The SCL guides the network to learn domain-invariant features, via various style perturbations, so that it can generalize well to the different visual appearances of the real domain."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,,Task-Specific Learning (TL).,"While alleviating the style differences between the simulated and real domain, UFS filters out some task-specific information (e.g. semantic content) encoded in the simulated features, as content and style are not orthogonal [11,19], resulting in performance deterioration. Therefore, we embed TL to retain useful representations for AQA. Specifically, f s→r final should retain task-specific information to allow the regressor to predict results R s→r that correspond to the quality scores (CP, GI) in the simulated data. In TL, simulated labels y s are used as the supervising signal:The TL performs AQA tasks for style variants to complement the loss of information due to feature stylization."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,3,Experiments,
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,3.1,Experimental Settings,"Experiments are implemented in PyTorch on a Tesla V100 GPU. The maximum training iteration is 40,000 with a batch size of 32. We adopted the SGD optimizer with a weight decay of 5e-4, a momentum of 0.9 and a learning rate of 1e-4. Input images are resized to 224 × 224, the CP and GI scores are normalized to [0, 1]. The MSE is adopted as the evaluation metric for both the CP and GI regression tasks. Following the standard approach for UDA [4,18,23], we use all 16,192 labelled simulated data and all 4,427 unlabelled real data for domain adaptation, and test on all 4,427 real data. To further explore the data efficiency of UDA methods on our simulation-to-real AQA tasks, we also conduct experiments with fractions (10%, 30%, and 50%) of unlabeled real data for domain adaptation, randomly selected from the 10 TEE real cases."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,3.2,Comparison with State-of-the-Arts,"We compare the proposed SR-AQA with MixStyle [24], MDD [23], RSD [4], and SDAT [18]. All methods are implemented based on their released code and original literature, and fine-tuned to fit our tasks to provide a basis for a fair comparison.As shown in Table 1, all UDA methods show better AQA overall performance (lower MSE) compared to the model trained only with simulated data (""Simulated Only""), demonstrating the effectiveness of UDA methods in TEE simulation-to-real AQA. The proposed SR-AQA achieves superior performance with the lowest MSE in all CP experiments and all but one GI experiment (50%), in which is very close (0.7648 to 0.76) to the best-performing SDAT. We calculate the MSE reduction percentage between our proposed SR-AQA and the second-best method, to obtain the degree of performance improvement. Specifically, on the CP task among the five real data ratio settings, the MSE of our method dropped by 2.13%-5.08% against the suboptimal method SDAT. It is evident that even with a small amount (10%) of unlabelled real data used for UDA, our SR-AQA still achieves a significant MSE reduction, of at least 3.02% and 4.37% compared to other UDA methods, on the CP and GI tasks respectively, showcasing high data efficiency. We also conduct paired t-tests on MSE results, from multiple runs with different random seeds. The obtained p-values (p < 0.05 in all but one case, see supplementary material Table S3), validate that the improvements yielded by the proposed SR-AQA are statistically significant. In Table 2, we report the performance over different (low, medium, and high) score ranges with SR-AQA, obtaining promising results among all ranges 2 ."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,3.3,Ablation Study,"We first explore the impact of the amount of UFS on generalization performance. As shown in the left part of Table 3, the performance continues to improve as UFS is applied to more shallow layers, but decreases when UFS is added to deeper layers. This is because semantic information is more important than style in the deeper layers. Using a moderate number of UFS to enrich simulated features with real-world styles, without corrupting semantic information, improves model generalization. Secondly, we study the effect of uncertainty-aware, SCL, and TL, as shown in the right part of Table 3, removing each component leads to performance degradation."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,4,Conclusion,"This paper presents the first annotated TEE dataset for simulation-to-real AQA with 16,192 simulated images and 4,427 real images. Based on this, we propose a novel UDA network named SR-AQA for boosting the generalization of AQA performance. The network transfers diverse real styles to the simulated domain based on uncertainty-award feature stylization. Style consistency learning enables the encoder to learn style-independent representations while taskspecific learning allows our model to naturally adapt to real styles by preserving task-specific information. Experimental results on two AQA tasks for CP and GI scores show that the proposed method outperforms state-of-the-art methods with at least 5.08% and 16.28% MSE reduction, respectively, resulting in superior TEE AQA performance. We believe that our work provides an opportunity to leverage large amounts of simulated data to improve the generalisation performance of AQA for real TEE. Future work will focus on reducing negative transfer to extend UDA methods towards simulated-to-real TEE quality assessment."
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,,Fig. 1 .,
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,,Fig. 2 .,
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,,Table 1 .,
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,,Table 2 .,
Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography,,Table 3 .,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,1,Introduction,"Trustworthy and reliable visual question-answering (VQA) models have proved their potential in the medical domain [17,22]. A deep learning (DL)-based surgical VQA system [22] has been developed as a surgical training and popularization tool for junior surgeons, medical students, and patients. However, one pivotal problem with surgical VQA is the lack of localized answers. VQA can provide the answer to the question, but cannot relate the answers to its localization at an instance level. Surgical scenarios with various similar instruments and actions may further confuse the learners. Answers with localization can further assist learners in dealing with confusion. In this case, a surgical visual-question localized-answering (VQLA) system can thereby be established for effective surgical training and scene understanding [3].Meanwhile, catastrophic forgetting has become a largely discussed topic in deep neural networks. Deep neural networks (DNNs) shall abruptly and drastically forget old knowledge when learning new [16]. Various continual learning (CL) methods have been proposed to mitigate catastrophic forgetting and study the balance of rigidity and plasticity in deep models [16,20]. Rigidity refers to the ability of the model not to diverge and remember old knowledge, while plasticity represents the acquisition of new knowledge by DNNs [4]. Some pioneering works have attempted to tackle the CL problem in the medical domain [6]. Catastrophic forgetting may occur in various real-world medical scenarios, e.g., data collected (i) over time, and (ii) across devices/institutions. More seriously, due to issues of data privacy, storage, and licensing, old data may not be accessible anymore [13]. Therefore, it is necessary to develop a non-exemplar CL method for surgical VQLA tasks to resist catastrophic forgetting in clinical applications.Furthermore, most medical decision-making tasks shall include classes overlapping with the old tasks and newly appeared classes, as shown in Fig. 1. We should not distillate the entire previous model when we deal with CL with overlapping classes. Firstly, the model will not emphasize new classes and have a high bias toward overlapping classes rather than new classes. Overlapping classes will dominate the model prediction if we naively follow the distillation from existing CL models. Secondly, catastrophic forgetting will be severe in old non-overlapping classes and the overlapping classes will dominate in the model prediction, and forget the old classes. For this purpose, we revisit distillation methods in CL and design a Continual Surgical VQLA (CS-VQLA) framework for learning incremental classes by balancing the performance of the old overlapping and non-overlapping classes. CS-VQLA has the following attributes: (i) it is a multi-task model including answering and localization, (ii) domain shift and class increment problems both exist, (iii) there may be overlapping classes between old and new tasks. These points shall further complicate the CL tasks.In this work, (1) We establish a non-exemplar CS-VQLA framework. While being applied to surgical education and scene understanding, the framework can learn data in a streaming manner and effectively resist catastrophic forgetting. (2) We revisit the distillation method for CL, and propose rigidityplasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distilla- tion (SH-Dist) for the output logits and intermediate feature maps, respectively. The weight aligning (WA) technique is further integrated to adjust model bias between old and new data. (3) Extensive comparison and ablation studies prove the outstanding performance of our method in mitigating catastrophic forgetting, demonstrating its potential in real-world applications."
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,2,Methodology,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,2.1,Preliminaries,"Problem Definition. We define the continual learning sequence with T P time periods, and t ∈ {1, ..., T P} means the current time period. D t denotes the training dataset at time period t, with x representing a sample of the input question and image pair in D t . C old denotes the classes appearing in previous time period {1, ..., t -1}, and C new represents the classes appearing in current time period t. Furthermore, we define the classes existing in both C old and C new as overlapping classes C op , and define unique classes in C old as old non-overlapping classes C no . F stands for the output feature map from the network backbone.Knowledge Distillation (KD) [9,26] on output logits [16] or intermediate feature map [19] is a widely used approach to retain knowledge on old tasks. With z o and z cl denote the output logits from the old and CL model, respectively, we can formulate the logits distillation loss [16] as:in which p o T (x) = SM (z o /T ) and p cl T (x) = SM (z cl /T ) represent the probabilities. SM means Softmax. T is temperature normalization for all old classes.Weight Aligning (WA) [27] is a simple technique to align the weight bias in the classifier layer. We use W new to represent the weights for newly appeared classes in the classifier, and W old to denote those of old classes, then we have:where norm means normalizing all the elements in the vector. In classincremental learning, WA can effectively avoid the model bias towards new classes. "
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,Old Model ConƟnual Learning Model,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,2.2,Continual Surgical VQLA (CS-VQLA),"Visual-Question Localized Answering. We define our VQLA framework following [3], by building a parallel detector on top of the VQA-based classification model. Therefore, the VLQA model includes the following components: a ResNet18 [8] pre-trained on ImageNet [5] as a prior image feature extractor, a BERT tokenizer [7], the VisualBERT [15] as the backbone (it can also be called as the deep feature extractor), a fully-connected layer as the classifier, and a 3-layer MLP as the detector. The classification task is optimized via the cross-entropy loss L CE , and the bounding box regression is optimized by the sum of L 1 and GIoU loss [21]. Thus, the VQLA loss can be formulated as:, where μ is set as 100 to balance the optimization progress of the two tasks.Rigidity-Plasticity-Aware Distillation (RP-Dist). The current rigidityplasticity trade-off is towards the entire model. However, we shall make the rigidity-plasticity aware in overlapping and non-overlapping classes.There is no overlap between C old and C new in an ideal class-incremental learning setup, so the temperature T in Equ. 1 is set to 2 by [16]. However, in a real-world application setup, T should not smooth the logits equally for old nonoverlapping C on and overlapping classes C op . Specifically, through adjusting for T , we shall endow the model greater plasticity on C op , and keep the rigidity on C on . We first establish a regularized distillation loss. Originally, the old model shall serve as the 'teacher' model in CL-based distillation. Instead of directly distilling the old model output logits, we construct a perfect pseudo teacher for distillation. To begin with, a pseudo answering label set a can be built from the old model classification probabilities p o (x) via a = Max[p(x)]. Based on the idea of label smoothing, we can manually setup a pseudo old model to have a high probability of predicting a correct class, and its probability distribution shall be:When λ is set to a very high number (e.g., λ ≥ 0.9), we will have a high probability of getting a correct class, allowing the teacher model to have perfect performance. The probability output of the CL model can be optimized with this pseudo-teacher based on the Kullback-Leibler divergence D KL :T is the KD temperature used to generate soft probabilities for the pseudo old model. As discussed above, this naive setting of T is not suitable for general CL scenarios. Therefore, we treat T op and T on differently to strengthen the plasticity on C op and the rigidity on C on respectively. L RKD can thereby be rewritten as:We keep T op > T on to balance the rigidity and plasticity trade-off in the CL model, and set T op = 25, T on = 20 empirically in our implementation."
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,Self-Calibrated Heterogeneous Distillation (SH-Dist).,"Works have discussed the use of self-calibration to improve model performance [18,32]. However, assuming we obtain an old model and we would like to conduct CL training on it, we can hardly modify the old model itself directly. Therefore, we perform a self-calibration operation on the heterogeneous output features F t from the Visu-alBERT backbone to get self-calibrated feature F t . The details can be referred to at the bottom of Fig. 2. Without engaging more learnable parameters, we endow the heterogeneous features with adaptively modeled long-range context information. Therefore, we can construct our feature distillation using the selfcalibrated feature map F t and the old model feature map F t-1 . L 2 loss is used to minimize the distance between F t and F t-1 empirically by following [19]:Subsequently, the self-calibrated feature map F t shall be propagated through the parallel classifier and detector for the multi-task prediction.Overall Framework. Figure 2 shows the overview of our CS-VQLA framework. The given image and question input are respectively processed as feature embedding by pre-trained ResNet18 and BERT tokenier, and fed to the VisualBERT backbone after embedding fusion. Then the output heterogeneous feature map is used to train the parallel predictors. The loss functions establish the essential components of our CS-VQLA framework. In the initial time period t = 0, the model is only trained on the VQLA loss. When t > 0, we combine the VQLA loss for training on the current dataset D t , with the RP-Dist & SH-Dist loss to retain the old knowledge. We can summarize our final loss function as follows:We set α = β = 1 and γ = 5 in our implementation. Furthermore, WA is deployed after training on each time period, to balance the weight bias of new classes on the classification layer. Through the combination of multiple distillation paradigms and model weight adjustment, we successfully realize the general continual learning framework in the VQLA scenario."
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,3,Experiments,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,3.1,Dataset and Setup,"Dataset. We construct our continual procedure as follows: when t = 0, we train on EndoVis18 Dataset, t = 1 on EndoVis17 Dataset, and t = 2 on M2CAI Dataset. Therefore, we can establish our CS-VQLA framework with a large initial step, and several smaller sequential steps. When splitting the dataset, we isolate the training and test sets in different sequences to avoid information leakage.EndoVis18 Dataset is a public dataset with 14 videos on robotic surgery [1]. The question-answer (QA) pairs are accessible in [22], and the bounding box annotations are from [10]. The answers are in single-word form with three categories (organ, interaction, and locations). We further extend the QA pairs and include cases when the answer is a surgical tool. Besides, if the answer is regarding the organ-tool interaction, the bounding box shall contain both the organ and the tool. Statistically, the training set contains 1560 frames with 12741 QA pairs, and the test set contains 447 frames with 3930 QA pairs.EndoVis17 Dataset is a public dataset with 10 videos on robotic surgery [2]. We randomly select frames and manually annotate the QA pairs and bounding boxes. The training set contains 167 frames with 1034 QA pairs, and the test set contains 40 frames with 201 QA pairs.M2CAI Dataset is also a public robotic surgery dataset [24,25], and the location bounding box is publicly accessible in [11]. Similarly, we randomly select 167 Implementation Details. We compare our solution against the fine-tuning (FT) baseline and state-of-the-art (SOTA) methods, including LwF [16], WA [27], iCaRL [20], IL2A [29], PASS [30], SSRE [31], CLVQA [14], and CLiMB [23]. All the methods are implemented using [28] 1 , with PyTorch and on NVIDIA RTX 3090 GPU. We removed the exemplars in all methods for a non-exemplar comparison. All methods are firstly trained on EndoVis18 (t = 0) for 60 epochs with a learning rate of 1 × 10 -5 , and then trained on EndoVis17 (t = 2) and M2CAI (t = 2) for 30 epochs with a learning rate of 5 × 10 -5 . We use Adam optimizer [12] and a batch size of 64. Answering and localization performance are evaluated by Accuracy (Acc) and mean intersection over union (mIoU), respectively. "
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,3.2,Results,"Except for testing on three datasets separately, we set three specific categories in our continual learning setup: old non-overlapping (old N/O) classes, overlapping classes, and new non-overlapping (new N/O) classes. By measuring the performance in these three categories, we can easily observe the catastrophic forgetting phenomenon and the performance of mitigating catastrophic forgetting.As shown in Table 1 & 2, firstly, catastrophic forgetting can be apparently observed in the performance of FT. Then, among all baselines, iCaRL achieves the best performance when the model learns from t = 0 to t = 1, and gets to forget when there are more time periods. On the contrary, LwF exhibits a strong retention of old knowledge, but a lack of ability to learn new. Our proposed methods demonstrate superior performance in almost all metrics and classes. In classification tasks, the overall average of our methods outperforms the second best with 2.60% accuracy improvement at t = 1 and 2.68% at t = 2. In localization tasks, our method is 0.44 mIoU higher than the second best at t = 1 and 0.94 mIoU higher at t = 2. The results prove the remarkable ability of our method to balance the rigidity-plasticity trade-off. Furthermore, an ablation study is conducted to demonstrate the effectiveness of each component in our proposed method. We (i) degenerate the RP-Dist to original logits distillation [16], (ii) degenerate the SH-Dist to normal feature distillation [19], and (iii) remove the WA module, as shown in Table 3. Experimental results show that each component we propose or integrate plays an essential role in the final rigidity-plasticity trade-off. Therefore, we demonstrate that each of our components is indispensable. More evaluation and ablation studies can be found in the supplementary materials."
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,4,Conclusion,"This paper introduces CS-VQLA, a general continual learning framework on surgical VQLA tasks. This is a significant attempt to continue learning under complicated clinical tasks. Specifically, we propose the RP-Dist on output logits, and the SH-Dist on the intermediate feature space, respectively. The WA technique is further integrated for model weight bias adjustment. Superior performance on VQLA tasks demonstrates that our method has an excellent ability to deal with CL-based surgical scenarios. Except for giving localized answers for better surgical scene understanding, our solution can conduct continual learning in any questions in surgical applications to solve the problem of class increment, domain shift, and overlapping/non-overlapping classes. Our framework can also be applied when adapting a vision-language foundation model in the surgical domain. Therefore, our solution holds promise for deploying auxiliary surgical education tools across time/institutions. Potential future works also include combining various surgical training systems (e.g., mixed reality-based training, surgical skill assessment) to develop an effective and comprehensive virtual teaching system."
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,Fig. 1 .,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,Fig. 2 .,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,Table 1 .,"experiments from the time period t = 0 to t = 1. Bold and underlined represent best and second best, respectively. 'W/O' denotes 'without', and 'W/I' denotes 'within'. 'N/O' means non-overlapping. 'Old N/O' represents the classes that exist in t = 0 but do not exist in t = 1, and 'New N/O' represents the opposite. 'Overlapping' denotes the classes that exist in both t = 0, 1."
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,.98 74.57 78.26 78.59 54.33 74.02 75.12 77.02 64.73 75.52Table 2 .,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,Table 3 .,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,53 61.08 56.98 74.57 78,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,73 75.52 28.20 68.14 41.04 65.74 44.44 74.41 42.46 66.14,
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,Acknowledgements,". This work was funded by Hong Kong RGC CRF C4063-18G, CRF C4026-21GF, RIF R4020-22, GRF 14203323, GRF 14216022, GRF 14211420, NSFC/RGC JRS N CUHK420/22; Shenzhen-Hong Kong-Macau Technology Research Programme (Type C 202108233000303); Guangdong GBABF #2021B1515120035. M. Islam was funded by EPSRC grant [EP/W00805X/1]."
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 7.
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,1,Introduction,"Surgical video analysis is a rapidly growing field that aims to improve and gain insights into surgical practice by leveraging increasingly available surgical video footage [12,25]. Several key applications have been well explored, ranging from surgical skill assessment to workflow analysis to intraoperative safety enhancement [4,5,11,27]. Yet, effectively learning and reasoning based on surgical anatomy remains a challenging problem, as evidenced by lagging performance in fine-grained tasks such as surgical action triplet detection and critical view of safety prediction [15,16]. .. .Fig. 1. Overview of our proposed approach. We begin by computing graphs for each frame using [15], then add temporal edges (shown with solid lines) between graphs at different horizons to obtain the video-level graph GV . We process GV with a GNN to yield spatiotemporally-aware node features, which we use for downstream prediction. Each node color corresponds to an object class, and each edge color to a relation class.We retain spatial edges (shown with dotted lines) from the graph encoder in GV . (Color figure online)Such anatomy-based reasoning can be accomplished through object-centric modeling, which is gaining popularity in general computer vision [6,14,19,26,28]. Object-centric models represent images or clips according to their constituent objects by running an object detector and then using the detections to factorize the visual feature space into per-object features. By retaining implicit visual features, these approaches maintain differentiability, allowing them to be fine-tuned for various downstream tasks. Meanwhile, they can also be extended to include object attributes such as class, location, and temporal order for tasks that rely heavily on scene semantics. Recent works have explored object-centric representations in the surgical domain, but they are characterized by one of several limitations: they often include only surgical tools [9,22], preventing anatomydriven reasoning; they are limited to single-frames or short clips [9,15,18,22], preventing video-level understanding; or they formulate the object-centric representation as a final output (e.g. scene graph prediction) and only include scene semantics, which limits their effectiveness for downstream tasks [8,17,23].In this work, we tackle these challenges by proposing to build latent spatiotemporal graph representations of entire surgical videos, with each node representing a surgical tool or anatomical structure and edges representing rela-tionships between nodes across space and time. To build our graphs, rather than use an off-the-shelf object detector, we employ the latent graph encoder of [15] to generate per-frame graphs that additionally encode object semantics, segmentation details, and inter-object relations, all of which are important for downstream anatomy-driven reasoning. We then add edges between nodes in different graphs, resulting in a spatiotemporal graph representation of the entire video. We encounter two main challenges when building these graphs for surgical videos: (1) surgical scenes evolve slowly over time, calling for long-term modeling, and (2) object detection is often error-prone due to annotated data scarcity. To address the first challenge, we introduce a framework to add temporal edges at multiple horizons, enabling reasoning about the short-term and long-term evolution of the underlying video. Then, to address the error-prone object detection, we propose a Graph Editing Module that leverages the spatiotemporal graph structure and predicted object semantics to efficiently correct errors in object detection.We evaluate our method on two downstream tasks: critical view of safety (CVS) clip classification and surgical phase recognition. CVS clip classification is a fine-grained task that requires accurate identification and reasoning about anatomy, and is thus an ideal target application for our object-centric approach. On the other hand, phase recognition is a coarse-grained task that requires holistic understanding of longer video segments, which can demonstrate the effectiveness of our temporal edge building framework. We achieve competitive performance in both of these tasks and show that our graph representations can be used with or without task-specific finetuning, thereby demonstrating their value as general-purpose video representations.In summary, we contribute the following:1. A method to encode surgical videos as latent spatiotemporal graphs that can then be used without modification for two diverse downstream tasks. 2. A framework for effectively modeling long-range relationships in surgical videos via multiple-horizon temporal edges. 3. A Graph Editing Module that can correct errors in the predicted graph based on temporal coherence cues and prior knowledge."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,2,Methods,"In this section, we describe our approach to encode a T -frame video V = {I t | 1 ≤ t ≤ T } as a latent spatiotemporal graph G V (illustrated in Fig. 1). Our method consists of a frame-wise object detection step followed by a temporal graph building step and a graph editing module to correct errors in the predicted graph representation. We also describe our graph neural network decoder to process the resulting representation G V for downstream tasks."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,2.1,Graph Construction,"Object Detection. To construct a latent spatiotemporal graph representation, we must first detect the objects in each frame, along with any additional prop-erties. We do so by employing the graph encoder φ SG proposed in [15], yielding a graph G t for each frame I t ∈ V . The resulting G t is composed of nodes N t and edges E t ; N t and E t are in turn composed of features h i , h i,j , bounding boxes b i , b i,j , and object/relation class r i , r i,j .Spatiotemporal Graph Building. Once we have computed the graphs G t , we add temporal edges to construct a single graph G V that describes the entire video. G V retains the spatial edges from the various G t to describe geometric relations between objects in the same frame (i.e. to the left of, above), while also including temporal edges between spatially and visually similar nodes. It can then be processed with a graph neural network during downstream evaluation to efficiently propagate object-level information across space and time. We add temporal edges to G V based on object bounding box overlap and visual feature similarity, inspired by [26]; however, we extend their approach to construct edges at multiple temporal horizons rather than between adjacent frames alone. Specifically, we design an operator φ TE that takes a pair of graphs G t , G t+w and outputs a list of edges, which are defined by their connectivity C t,t+w containing pairs of adjacent nodes, and their relation class R t,t+w containing relation class ids. To compute the edges, φ TE calculates pairwise similarities between nodes in G t and nodes in G t+w using two separate kernels: K B , which computes the generalized IoU between node bounding boxes, and K F , which computes the cosine similarity between node features. This yields similarity matrices M B and M F , each of size N t × N t+w . Using each matrix, we select the most similar node n j,t+w ∈ N t+w for each n i,t ∈ N t and vice-versa. Altogether, this yields 4 * (|N t | + |N t+w |) edges consisting of connectivity tuples c m,n = ((i, t), (j, t + w)) and relation classes r m,n , which we store in C t,t+w and R t,t+w respectively. We apply φ TE to all pairs of graphs G t , G t+w for various temporal horizons w ∈ W, then combine the resulting C t,t+w and R t,t+w to obtain temporal edge connectivities C ST and relation classes R ST . Finally, we augment each temporal edge with features h m,n and bounding boxes b m,n , yielding the video-level graph G V :(1) Edge Horizon Selection. While φ TE is designed to construct edges between arbitrarily distant graphs, effective selection of temporal horizons W is nontrivial. We could naively include every possible temporal horizon, setting W = {1, 2, ..., T -1} to maximize temporal information flow; however, making W too dense results in redundancies in the resulting graph, which can have an oversmoothing effect during downstream processing with a graph neural network (GNN) [29]. To avoid this issue, we take inspiration from temporal convolutional networks (TCN) [10], which propagate information over long input sequences using a series of convolutions with exponentially increasing dilation. We similarly use exponentially increasing temporal horizons, setting W = {1, 2, 4, ..., 2 l } to enable efficient information flow at each GNN layer and long-horizon message passing via a stack of GNN layers."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,2.2,Graph Editing Module,"One limitation of object-centric approaches is a reliance on high quality object detection, which is particularly steep in surgical videos. These difficulties in object detection could be tackled by incorporating prior knowledge such as anatomical scene geometry, but incorporating these constraints into the learning process often requires complex constraint formulations and methodologies. We posit that our spatiotemporal graph structure represents a simpler framework to incorporate such constraints; to demonstrate this, we introduce a module to filter detections of anatomical structures, which are particularly difficult to detect, incorporating the constraint that there is only one of each structure in each frame. Specifically, after building the spatiotemporal graph, we compute a dropout probability p i,t = 1 deg(ni,t) for each node, where deg is the degree operator. Then, for each frame t, for each object class r j , we select the highest scoring node n t from {n i,t |r i,t }. During training, we apply graph editing with probability p edit , providing robustness to a wide range of input graphs."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,2.3,Downstream Task Decoder,"For downstream prediction from G V , we first apply a GNN using the architecture proposed in [3], yielding spatiotemporally-aware node features. Then, we pool the node features within each frame and apply a linear layer to yield frame-wise predictions (see Fig. 1). We process these predictions differently depending on the task: for clip classification, we output only the prediction for the last frame, while for temporal video segmentation, we output the frame-wise predictions."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,2.4,Training,"We adopt a two-stage training approach, starting by training φ SG as proposed in [15] and then extracting graphs for all images. Then, in the second stage, we process a sequence of graphs with our model to predict frame-wise outputs. We supervise each prediction with the corresponding frame label, propagating the clip label to each frame when per-frame labels are unavailable."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,3,Experiments and Results,"In this section, we describe our evaluation tasks and datasets, describe baseline methods and our model, then present results for each task. We conclude with an ablation study that illustrates the impact of our various model components."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,3.1,Evaluation Tasks and Datasets,"Critical View of Safety (CVS) Prediction. The CVS consists of three independent criteria, and can be viewed as a multi-label classification problem [13].For our experiments, we use the Endoscapes+ dataset introduced in [15], which contains 11090 images annotated with CVS evenly sampled from the dissection phase of 201 cholecystectomies at 0.2 fps; it also includes a subset of 1933 images with segmentation masks and bounding box annotations. We model CVS prediction as a clip classification problem, constructing clips of length 10 at 1 fps, and use the label of the last frame as the clip label. As in [15], we investigate CVS prediction performance in two experimental settings to study the label-efficiency of various methods: (1) using only the bounding box labels and CVS labels and (2) additionally using the segmentation labels. We report mean average precision (mAP) across the three criteria for all methods. Surgical Phase Recognition. For surgical phase recognition, we use the publically available Cholec80 dataset [24], which includes 80 videos with frame-wise phase annotations ({1, 2, ..., 7}). We use the first 40 videos for training, the next 8 for validation, and the remaining 32 for testing, as in [2,20]. In addition, to enable object-centric approaches, we use the publically available CholecSeg8k dataset [7] as it represents multiple surgical phases unlike Endoscapes+. As CholecSeg8k is a subset of Cholec80, we split the images into training, validation, and testing following the aforementioned video splits. We model phase recognition as a temporal video segmentation problem, and process the entire video at once. Again, we explore two experimental settings: (1) temporal phase recognition without single-frame finetuning to evaluate the surgical video representations learned by each method and (2) temporal phase recognition with single-frame finetuning, the classical setting [2,5,20]. We report mean F1 score across videos for all methods, as suggested in [20]."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,3.2,Baseline Methods,"Single-Frame Methods. As CVS clip classification is unexplored, we compare to two recent single-frame methods for reference: LG-CVS [15], a graph-based approach, and DeepCVS [13], a non-object-centric approach. We quote results from [15], which improves DeepCVS and enables training with bounding boxes.DeepCVS-Temporal. We also extend DeepCVS for clip classification by replacing the last linear layer of the dilated ResNet-18 with a Transformer decoder, referring to this model as DeepCVS-Temporal.STRG. Space-Time Region Graphs (STRG) [26] is a spatiotemporal graphbased approach for action recognition that builds a latent graph by predicting region proposals and extracting per-region visual features using an I3D backbone; we repurpose STRG for CVS clip classification and phase recognition. Because STRG is trained end-to-end, it can only process clips of limited length; consequently, we train STRG on clips of 15 frames for phase recognition rather the entire video as in other methods. We also only consider phase recognition with finetuning. For CVS clip classification, we additionally pre-train the I3D feature extractor in STRG on bounding box/segmentation labels using a Faster-RCNN box head [21] or DeepLabV3+ decoder [1].TeCNO. TeCNO [2] is a temporal model for surgical phase recognition consisting of frame-wise feature extraction followed by temporal decoding with a causal TCN [10] to classify phases. For phase recognition without single-frame finetuning, we use a ResNet-50 pre-trained on CholecSeg8k using a DeepLabV3+ head to extract features, enabling fair comparisons with our method. For the other setting, we report performance from [20]. Ours. We train our model in two stages, starting by training the graph encoder φ SG as described in [15] on the subset of Endoscapes+ annotated with segmentation masks or bounding boxes for CVS clip classification, or on CholecSeg8k for phase recognition. We then extract frame-wise graphs for the entire dataset and apply our spatiotemporal graph approach to predict CVS or phase. In the second experimental setting for phase recognition, we additionally finetune φ SG on all training images with the frame-wise phase labels before extracting the graphs. Finally, we evaluate a version of our method that additionally applies a TCN to the un-factorized image features and adds the TCN-processed features to the pooled temporally-aware node features prior to linear classification. We set l = 3, p edit = 0.5, and use a 5-layer GNN for CVS prediction and an 8-layer GNN for phase recognition."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,3.3,Main Experiments,"CVS. Our first takeaway from Table 1 is that temporal models provide a methodagnostic boost of 3% mAP for CVS prediction. Furthermore, our approach outperforms both non-object-centric and object-centric temporal baselines, achieving a substantial performance boost in the label-efficient bounding box setting while remaining competitive when also trained with segmentation masks. In the box setting, we observe that the non-object-centric DeepCVS approaches perform rather poorly due to an over-reliance on predicted semantics rather than effective visual encodings [15]. Object-centric modeling addresses some of these limitations, as evidenced by STRG outperforming DeepCVS-Temporal. Nevertheless, our method achieves a much stronger performance boost, owing to multiple factors: (1) our model is based on the underlying LG-CVS, which constructs its frame-wise object-centric representation by using the final bounding box predictions rather than just region proposals like STRG, and also encodes semantic information, and (2) our proposed improvements (multiple-horizon edges, graph editing) are critical to improving model performance. Meanwhile, in the segmentation setting, the object-centric STRG is ineffective, performing worse than DeepCVS-Temporal; this discrepancy arises because, as previously mentioned, STRG relies on region proposals rather than object-specific bounding boxes in its graph representation, and as a result, cannot fully take advantage of the additional information provided by the segmentation masks. Our approach translates the ideas of STRG but importantly builds on top of the already effective representations learnt by LG-CVS to achieve universal effectiveness for spatiotemporal modeling of CVS.Phase. Table 2 shows the phase recognition results for various methods with (bottom) and without (top) finetuning the underlying single-frame model. Our model is already highly effective for phase recognition without any finetuning, outperforming the corresponding TeCNO model by 6.1% F1 in its original form and by nearly 10% F1 when also using a TCN. This shows that the graph representations contain general-purpose information about the surgical scenes and their evolution. Finally, by finetuning the underlying single-frame graph encoder, we match the existing state-of-the-art, highlighting our method's flexibility."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,3.4,Ablation Studies,"Table 3 illustrates the impact of each model component on CVS clip classification and phase recognition performance. The first two rows illustrate the importance of using exponential edge horizons. Without any long-term edges (as in STRG), we observe a staggering 7.20% drop in Phase F1; naively building edges between all the graphs improves performance but is still 4.14% worse than our proposed method. We observe similar trends for the CVS mAP but with lower magnitude, as CVS prediction is not as reliant on long-term video understanding. Meanwhile, we observe the opposite effect for the graph editing module, which is quite effective for CVS clip classification but does not considerably impact phase F1. This is again consistent with the nature of the tasks, as CVS requires fine-grained understanding of the surgical anatomy, and performance can suffer greatly from errors in object detection, while phase recognition is more coarse-grained and is thus less impacted by errors at this fine-grained level."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,4,Conclusion,"We introduce a method to encode surgical videos in their entirety as latent spatiotemporal graph representations. Our graph representations enable finegrained anatomy-driven reasoning as well as coarse long-range video understanding due to the inclusion of edges at multiple-temporal horizons, robustness against errors in object detection provided by a graph editing module, and memory-and computational-efficiency afforded by a two-stage training pipeline. We believe that the resulting graphs are powerful general-purpose representations of surgical videos that can fuel numerous future downstream applications."
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,,Table 1 .,
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,,Table 2 .,
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,,Table 3 .,
Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_62.
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,1,Introduction,"Reduction planning is a crucial phase in pelvic fracture surgery, which aims to restore the anatomical structure and stability of the pelvic ring [21]. Traditional reduction planning relies on surgeons to form a ""mental"" plan through preoperative CT images, and the results are dependent on the surgeon's skill and experience. Computer-aided diagnosis (CAD) tools have been used for reduction planning, by virtually manipulating 3D bone models [2,23]. For complex fractures, 3D manual planning is not only time-consuming but also error-prone [19]. An automatic reduction planning method that offers improvements in both accuracy and efficiency is in demand.The most intuitive method for reduction planning is by matching the fracture surface characteristics [16,20]. However, these methods rely on the precise identification and segmentation of fracture lines, which is typically challenging, especially in comminuted fractures where small fragments are ""missing"". Template-based approaches avoid the fracture line identification process. These methods include the average template statically modeled from pelvic shapes [3] and the template mirrored from the healthy contralateral side [6,22]. The former is not adaptive to individual morphology, usually resulting in poor robustness. Whereas the latter can only handle iliac fractures and is limited by the fact that the two ilia are usually not absolutely symmetric [7]. In addition, the contralateral mirroring (CM) method faces a severe challenge in bilateral injuries, where the mirrored part cannot provide accurate references.Han et al. [11] used a statistical shape model (SSM) for pelvic fracture reduction, which adaptively matches the target morphology of pelvic bone. However, differences in the shape and integrity of the model bone and target fragments challenge the accurate adaptation of the SSM. Although the SSM method applies to all situations, without fully utilizing the whole pelvic structure, including the symmetricity, it often presents lower robustness. A recent study have pointed out that the CM method achieved better results than the adaptable iliac SSM [14].While the SSM method addresses the reduction within single bones, it does not apply well to inter-bone matching for joint dislocations [8]. Statistical pose model (SPM) was proposed to jointly model the similarity transformations between bones, and to decouple the morphology of the entire pelvis into single bone shape and inter-bone poses [10]. The SPM models the statistics of bone positions but ignores the details in the joint connections which has rich information useful for aligning the joint surfaces. An under-fitted SPM may result in poor joint alignment and overall pelvic symmetry, which are very important considerations in reduction surgery in clinical practice.In this study, we present a two-stage method for automatic pelvic reduction planning, addressing bone fractures and joint dislocations sequentially. A novel SSM-based symmetrical complementary (SSC) registration is designed to register multiple bone fragments to adaptive templates to restore the symmetric structure. Then, an articular surface (AS) detection and matching algorithm is designed to search the optimal target pose of dislocated bones with respect to joint alignment and symmetry constraints. The proposed method was evaluated in simulated experiments on a public dataset, and further validated in typical clinical cases. We have made our clinical dataset publicly available at https:// github.com/Sutuk/Clinical-data-on-pelvic-fractures. "
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,2,Method,"As shown in Fig. 1, 3D models of the bone fragments in ilia and sacrum are obtained by semi-automatic segmentation [15]. Our automatic planning algorithm consists of two stages. The first stage computes the transformation of each bone fragment to restore single-bone morphology. The second stage estimates the target pose of each dislocated bone with respect to the whole pelvic anatomy."
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,2.1,Gaussian Process Morphable Models for Pelvic Bone,"We use Gaussian process morphable models (GPMMs) [17] to model the shape statistics of pelvic bones. A healthy bone template is created for fractured fragments using empirical kernel, which enables the model to generate physiologically feasible bone morphology. In addition, localized kernel is used to create models with higher flexibility in order to approximate the morphology of the reduced/intact bone and then identify the pelvic AS areas.Parameterized Model. The GPMMs of the left ilium and sacrum are modeled separately by first aligning bone segmentations to a selected reference Γ R . A bone shape Γ B can be obtained by warping the reference Γ R with a deformation field u(x). The GPMMs models deformation as a Gaussian process GP (μ, k) with a mean function μ and covariance function k, and is invariant to the choice of reference Γ R . Parameterized with principal component analysis (PCA) in a finite dimension, the GPMMs can be written as:where P and b are the principal components and the weight vector, respectively. GPMMS modeled with the empirical kernel in a finite domain is equivalent to a statistical shape model (SSM), and the parameter is denoted as b SM . The parameters of the localized model are denoted as b LC .Empirical Kernel for Fracture Reduction. A Gaussian process GP (μ SM , k SM ) that models the distinctive deformations is derived from theand covariance function:where n represents the number of training surfaces, and u i (x) denotes single point deformation on the i-th sample surfaces.Localized Kernel for as Detection. In order to increase the flexibility of the statistical model, we limit the correlation distance between point clouds. The localized kernel is obtained by multiplying the empirical kernel and a Gaussian kernel k g (x, y) = exp(xy 2 /σ 2 ):where denotes element-wise multiplication, the identity matrix I 3×3 is a 3D vector field, and σ determines the range of correlated deformations. The kernel is then used in AS detection."
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,2.2,Symmetrical Complementary Registration,"A novel SSC registration is designed to register bone fragments to adaptive templates in joint optimization, which alternatingly estimates the desired reduction of fragments and solves the corresponding transformations. A bilateral supplementation strategy is used in model adaptation: mirrored counterparts are aligned to the target point cloud to provide additional guidance. In more detail, both the fragment and its mirrored counterpart are first rigidly aligned to the GPMM with empirical kernel k SM , using the iterative closest point (ICP) algorithm. Then, the GPMM is non-rigidly deformed towards the merged point clouds of the main fragment and its mirrored counterparts. During this step, the model's parameters b SM are optimized using bi-directional vertex correspondences (target to source v → (x) and backward v ← (x)) based on the nearest neighbour. For each source point x, there is a forward displacement δ → (x) = v → (x)x, and one or more backward displacement δ ← (x) = v ← (x)-x. The farthest point within the given threshold ε is selected as the corresponding point displacement δ(x) of the template:where I is the indicator function and O is a zeros vector. The optimal parameter bSM of the statistical model can be calculated from Eq. ( 1), and is constrained within three standard deviations of the eigenvalue λ i [1]:The adaptive shape of the GPMM is regarded as the target morphology of a bone, and the fracture fragments are successively registered to the adaptive template to form the output of the first stage, reduction transformation T l of each bone fragment S l , l = 1, ..., L. As shown in Fig. 1, in the SSC registration stage, contralateral complementation and self-mirroring complementation are used for ilium and sacrum, respectively."
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,2.3,Articular Surface Detection and Matching,"Articular Surface Detection. The AS of the sacroiliac (SI) joint and pubic symphysis (PS) are detected using the GPMMs for the ilia and sacrum, with the localized kernel. As indicated by the red and blue regions in Fig. 1 -AS detection, surface points in the joint regions are first manually annotated in the mean model template (b LC = 0) and then propagated to each instance using non-rigid adaptation of GPMM model. The optimal parameter bLC is obtained via the alternative optimization method in Sect. 2.2, and the marked points on the adaptive templates are mapped to the target bones as the AS regions. Due to the symmetric structure of the pelvis, we use a unilateral model to identify bilateral AS through mirror-flipped registration. Each AS landmark (vertex in mesh data) is associated with a normal vector. The average normal direction is computed from these vectors, and any surface landmark pairs deviating from this average direction are removed. The identified surface point sets are denoted as SI and P S.Articular Surface Matching. For each joint, a ray-tracing collision detection algorithm is performed between opposing surfaces using surface normal [13].Then, a cost function L local is constructed to measure the degree of alignment between two surfaces, including a distance term and a collision term. The former measures the Chamfer distance between complementary surfaces d cd [4]. The latter uses an exponential function to map the IoU of the joint surface, so that slight collisions can be admitted to compensate for the potential inaccurate segmentation from the preprocessing stage.where S 1 and S 2 are two sets of 3D points, and γ is a balancing weight for the collision term.In addition to the local cost for each joint, a global cost measuring the degree of symmetry is used to regularize the overall anatomy of the pelvis:where the first term measures the paired difference between the point-wise distance within SI lef t and SI right . D lef t and D Right represent the distance between points in SI lef t and SI right , respectively. The second term measures the angle between the mean vector of the PS, denoted as -→ V P S , and the mean vector of the bilateral SI joint -→ V SI . The AS matching problem is formulated as optimizing the transformations with respect to the misalignment of PS and SI joints, plus the pelvic asymmetry:where L local (SI lef t ) or L local (SI right ) can be a constant when the corresponding joint is intact, and can be omitted accordingly. The cost function in Eq. ( 8) is optimized with respect to T D , which determines the pose of the moving bone and its joint surfaces. The covariance matrix adaptation evolution strategy (CMA-ES) is used as the optimizer to avoid local minimum problem [12]. The final output point clouds of the pelvis is obtained by combining the transformation for the dislocated bone and the transformation for each fragment in the first stage T D T l S l ."
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,3,Experiments and Results,
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,3.1,Experiments on Simulated Data,"We evaluated the proposed fracture reduction planning method in a simulation experiment with leave-one-out cross-validation on the open-source pelvic atlas [9]. Extending from the simulation category in previous research [10], the current study simulated six types of fractures (the first column in Fig. 2), including single-bone fractures, joint dislocation, and combined pelvic trauma. The single-bone fractures include bilateral iliac wing fractures and sacral fractures, and the combined simulation of pelvic trauma includes iliac fracture with dislocation (type A), sacral fracture with dislocation (type B), as well as iliac and sacral fractures with dislocation (type C). In total, 240 instances were simulated (6 fracture categories for each of the 40 samples). On the single-bone fracture data, the proposed SSC registration was tested against the mean shape reference and the SSM reference [11]. On the dislocation-only data, the proposed AS matching method was tested against the pelvic mean shape reference and contralateral mirroring (CM) [22]. On the combined-trauma data, the proposed two-stage method in Fig. 1 was tested against the pelvic mean shape reference and combined SSM and CM (S&C).The Gaussian kernel parameter σ was set to 30, the collision regularization factor γ was set to 2, and the bidirectional correspondence threshold ε was set to 5. The accuracy of reduction was evaluated by the root-mean-square error (RMSE) on all fragments. Specifically, the average distance between the ground truth and the planned target location of all points was calculated.As shown in Fig. 2, due to the strong individual variations, the mean reference was insufficient to cope with most fracture reduction problems, resulting in uneven fracture fragments and distorted bone posture. Without the symmetric complement, the SSM planning was more prone to large fragment gaps or overlaps than our method. Solely dependent on sacrum symmetry, the CM method often resulted in bone overlap or tilted reduction poses. The flaws of the cascaded comparison method were further magnified in combined pelvic fracture reduction. Meanwhile, under the structural constraints, our method payed more attention to the overall symmetry, collision and articular surface anastomosis, and the results were closer to the ground truth.As shown in Fig. 3, our method achieved the best median RMSE and interquartile range (IQR). Paired t-tests against other methods indicated statistical significance, with p < 0.001. For additional results, please refer to the Supplementary Material. The overall running time for the planning algorithm was 5 to 7 min on an Intel i9-12900KS CPU and an NVIDIA RTX 3070Ti GPU."
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,3.2,Experiments on Clinical Data,"To further evaluate the proposed method on real clinical data, we collected CT data from four clinical cases (aged 34-40 years, one female), each representing a distinct type of pelvic fractures corresponding to the simulated categories. The data usage is approved by the Ethics Committee of the Beijing Jishuitan Hospital (202009-04). The proposed method was applied to estimate transformations of bone fragments to obtain a proper reduction in each case. Due to the lack of ground-truth target positions for fractured pelvis in clinical data, we employed  geometric measurements to evaluate the planning result. This evaluation method was inspired by Matta's trauma surgery criteria [18]. Surgeons manually measured the distances between all fragments and joints on a 3D slicer platform [5] by identifying a dislocated landmark pair across each fracture line or dislocated joint. The pelvic morphology in the planning result can be appreciated in Fig. 4. The measured distances between fragments or across joints are reduced to 1.8 to 4.4 mm. These planning results were examined by two senior experts and were deemed acceptable."
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,4,Discussion and Conclusion,"We have proposed a two-stage method for pelvic fracture reduction planning based on morphable models and structural constraints. The SSM-based symmetrical complementary registration was successfully applied to both bilateral iliac and sacral fractures, which further extended the unilateral iliac CM reduction. Combining the SSM approach with symmetrical complementation provided more complete morphological guidance for model adaptation, and improved the registration performance significantly. The AS detection and matching method which combines local joint matching and global symmetry constraints achieved significantly higher accuracy than the pelvic mirroring reference. The proposed planning method also achieved satisfactory results on simulated data with combined pelvic trauma and real clinical cases.We have demonstrated the synergy of the combined statistical model and anatomical constraints. In future work, we plan to further investigate this direction by incorporating feasibility constraints into SPM-based method to benefit from both cohort-based statistics and individual-specific matching criteria.In the experiments, we simulated the most common fracture types and obtained the ground truth for evaluation. Due to the absence of ground truth in clinical data, we resort to an independent distance metric. We plan to test our method on more clinical data, and combine geometric measurements and manual planning for a comprehensive evaluation and comparison. We also intend to further automate the planning pipeline using a pelvic fracture segmentation network in future research to avoid the tedious manual annotation process."
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,,Fig. 1 .,
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,,Fig. 2 .,
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,,Fig. 3 .,
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,,Fig. 4 .,
Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_31.
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,1,Introduction,"Resection of early-stage brain tumors can greatly reduce the mortality rate of patients. During the surgery, brain tissue deformation (called brain shift) can occur due to various causes, such as gravity, drug administration, and pressure change after craniotomy. While modern magnetic resonance imaging (MRI) techniques can provide rich anatomical and physiological information with various contrasts (e.g., fMRI) for more elaborate pre-surgical planning, intra-operative MRI that can track brain shift requires a complex setup and is costly. In contrast, intra-operative ultrasound (iUS) has gained popularity for real-time imaging during surgery to monitor tissue deformation and surgical tools because of its lower cost, portability, and flexibility [1]. Accurate and robust MRI-iUS registration techniques [2] can greatly enhance the value of iUS for updating pre-surgical plans and guiding the interpretation of iUS, which has an unintuitive contrast and non-standard orientations. This can greatly enhance the safety and outcomes of the surgical procedure by allowing maximum brain tumor removal while avoiding eloquent regions [3]. However, as the true underlying tissue deformation is unknown due to the 3D nature of the surgical data and the time constraint, real-time manual inspection of MRI-iUS registration results is challenging and error-prone, especially for precision-sensitive neurosurgery. Therefore, algorithms that can detect and quantify unreliable inter-modal medical image registration results are highly beneficial.Recently, automatic quality assessment for medical image registration has attracted increasing attention [4] from the domains of big medical data analysis and surgical interventions. With high efficiency, machine, and deep learning techniques have been proposed to allow automatic grading and dense estimation of medical image registration errors. Early endeavors on this topic primarily relied on hand-crafted features, including information theory-based metrics [5][6][7][8][9][10]. More recently, deep learning (DL) techniques that learn task-specific features have also been adopted in automatic evaluation of medical image registration, with a primary focus on intra-contrast/modal applications, including CT [9,10] and MRI [11]. Unfortunately, so far, error grading and estimation in inter-contrast/modal registration have rarely been explored, despite the particular demand in surgical applications. In this direction, Bierbrier et al. [12] made the first attempt using simulated iUS from MRI to train 3D convolutional neural networks (CNNs) to perform dense error regression for MRI-iUS registration in brain tumor resection. Although their algorithm performed well in simulated cases, the results on real clinical scans still required improvements. In this paper, we propose a novel 3D CNN to perform patch-wise error estimation for MRI-iUS registration in neurosurgery, by using focal modulation [13], a recent alternative DL technique to self-attention [14] for encoding contextual information, and uncertainty estimation. We call our method FocalErrorNet, which has three main novelties. First, we adapted the focal modulation network [13] from 2D to 3D and employed the technique in registration error assessment for the first time. Second, we incorporated uncertainty estimation using Monte Carlo (MC) dropouts [15] to offer assurance for error regression. Lastly, we developed and thoroughly evaluated our technique against a recent baseline model [12] using real clinical data and showed excellent results."
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2,Methods and Materials,
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.1,Dataset and Preprocessing,"For methodological development and assessment, we used the RESECT (REtro-Spective Evaluation of Cerebral Tumors) dataset [16], which has pre-operative MRI, and iUS scans at different surgical stages from 23 subjects who underwent low-grade glioma resection surgeries. As it is still challenging to model iUS scans with tissue resection, we took 22 cases with T2FLAIR MRI that better depicts tumor boundaries and iUS acquired before resection. An example of an MRI-iUS pair from a patient is shown in Fig. 1. We hypothesized that directly leveraging clinical iUS could help learn more realistic image features with potentially better outcomes in clinical applications than with simulated contrasts [9,12]. However, since the true brain shift model is impossible to obtain, we followed the strategy of creating silver ground truths for image alignment [9,12], upon which simulated misalignment is augmented in the iUS to build and test our DL model. To create the silver registration ground truths, we used the homologous landmarks between MRI and iUS in the RESECT dataset to perform landmark-based 3D B-Spline nonlinear registration to register iUS to the corresponding MRI for all 22 cases. To tackle the limited field of view (FOV) in iUS, we cropped the T2FLAIR MRI to the same FOV of the iUS, which was resampled to a 0.5 × 0.5 × 0.5 mm 3 resolution. To perform spatial misalignment augmentation, we continued to leverage 3D B-Spline transformation, similar to earlier reports on the same topic [10,12,17]. In short, B-Spline transformation can be modeled by a grid of regularly spaced control points and the associated parameters to allow various levels of nonlinear deformation. While the spacing of the control points determines the levels of details in local deformation fields, the displacement parameters control the magnitude of the deformation. To ensure that simulated registration errors are of different varieties and sizes, we randomly selected the number of control points and the associated displacements (in each 3D axis) with a maximum of 20 points and 30 mm, respectively. Note that the control point grid is isotropic, and the density is arbitrarily determined per deformation in our case. Each coregistered iUS scan was deformed ten times. After misalignment augmentation on the previously co-registered iUS, matching pairs of 3D image patches of size 33 × 33 × 33 voxels were taken from both the iUS volume and the corresponding MRI. As iUS has limited FOV and may contain no anatomical features, to ensure that the patches we extracted contain useful information (e.g. to avoid the dark background) in iUS, we focused on acquiring patches centered around the anatomical landmark locations available through the RESECT database. Since B-spline transformation offers a displacement vector at each voxel of the iUS volume, we directly considered the norm of the vector as the simulated registration error at the associated voxel. In our design, we determined the registration error of the image patch pair as the mean of all voxel-wise errors within the iUS patch. Finally, the image patch pairs, along with corresponding registration errors were then fed to the proposed DL algorithm for training and validation."
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.2,Network Architecture,"We proposed a novel 3D neural network, named FocalErrorNet, based on the recent focal modulation networks [13] that was originally proposed for 2D vision tasks to estimate the registration error between MRI and iUS patches. With a similar goal as the Vision Transformer (ViT), the focal modulation network was designed to model contextual information in images. It incorporates three main elements to achieve the goal: 1) focal contextualization that comprises a stack of depth-wise convolutional layers to account for long-to short-range dependencies, 2) gated aggregation to collect contexts into a modulator for individual query tokens, and 3) element-wise affine transformation to inject the modulator into the query. In the architecture of FocalErrorNet (see Fig. 2), all layers contain two focal modulator blocks, where two depth-wise convolutional layers focally extract contexts around each voxel, selectively aggregate and inject them into the query, and pass the information to the next block. We designed the FocalErrorNet as a ResNet-like variant of the focal modulation network to better encode relevant features across the input image and ensure a better gradient flow. Finally, the information from the backbone was propagated to a multi-layer perceptron (MLP) to regress registration errors, and two MC dropout layers were added to the MLP to allow uncertainty quantification for the results.  "
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.3,Uncertainty Quantification,"For registration error regression in surgical applications, knowledge regarding the reliability of the automated results is instrumental for the safety and wellbeing of the patients. Uncertainty estimation has gained popularity in probing the trustworthiness and credence of DL algorithms. Although the concept has been widely applied in image segmentation and classification, it has not been employed for registration error estimation, especially in the case of multi-modal situations, such as MRI-iUS alignment. Therefore, we incorporated uncertainty estimation in our proposed FocalErrorNet. For each MRI-iUS patch pair, 200 regression samples were collected by random sampling from MC dropouts [15] at test time. While the final patch registration error was obtained as the mean of all the samples, the sample standard deviation was used as the uncertainty metric. "
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.4,Experimental Setup and Implementation,"From the transformation augmentation, we acquired 3380 samples of MRI-iUS pairs. For our experiments, we arbitrarily split the subjects into training, validation, and test sets with the proportion of 60%, 20%, and 20%, respectively. To prevent information leakage, we ensured that each patient was included in only one of the split sets. For model training, we adopted the Adam optimization with a learning rate of 5 × 10 -5 and a batch size of 64. For the loss function, we used mean squared error (MSE) to minimize the difference between the predicted MRI-iUS registration error and the ground truths. Furthermore, in addition to the transformation augmentation, we also included additional data augmentation, including random noise addition and random image flipping on training sets to mitigate overfitting and increase the model's generalizability. To assess our proposed FocalErrorNet, we compared it against a 3D CNN [9,12] (see Fig. 3) that was employed for medical image registration error regression. The two DL models were trained with the same dataset and procedure, and their prediction accuracies, measured as the absolute error between the predicted and ground truths mis-registration on the test set were compared with two-sided pairedsamples t-tests to confirm the superiority of the proposed method, in addition to correlations between their estimated and ground truth errors. To validate the proposed uncertainty estimation method, we calculated the correlation between the uncertainty measure and absolute error of FocalErrorNet, and the correlation between the uncertainty and mutual information between MRI and iUS, which is often used to measure the information overlap in multi-modal registration. Finally, to test the robustness of the FocalErrorNet, we acquired additional MRI-iUS patch pairs from the test subjects, by introducing random linear shifts (the max displacement from landmark locations is 10 voxels) from the selected locations in the original set, and evaluated the DL model performance. "
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,3,Results,
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,3.1,Error Regression Accuracy,"The accuracy comparison between the proposed FocalErrorNet and the baseline 3D CNN [9,12] is shown in Table 1. Across all samples in the testing data, we achieved an accuracy of 0.59 ± 0.57 mm, while the counterpart obtained a prediction error of 1.69 ± 1.37 mm. With the t-test, our FocalErrorNet outperformed the 3D CNN [9,12] (p < 1e-4). In addition, the correlations between the predicted and ground truths errors are 0.82 (p < 1e-4) and 0.61 (p < 1e-3) for FocalErrorNet and 3D CNN, respectively, further confirming the advantage of the proposed technique. To allow a qualitative comparison, scatter plots for predicted vs. ground truth errors of the two models are depicted in Fig. 4a and5a. At larger error levels, it is evident that the point clouds exhibit a wider shape. "
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,3.2,Validation of the Uncertainty Evaluation,"We obtained correlations of 0.70 (p < 1e-4) and 0.34 (p < 1e-4) between estimated uncertainty and prediction error for FocalErrorNet and the baseline 3D CNN, respectively. Additionally, the uncertainty vs. mutual information uncertainties was assessed at -0.67 (p < 1e-4) for our proposed method and -0.18 (p < 1e-3) for the baseline. To allow better visual comparisons, the associated scatter plots are illustrated in Fig. 4 and5. These metrics proved the validity of our uncertainty measure and further confirmed the performance of FocalErrorNet. Note the scatter plots for uncertainty measure validation were performed using value binning (with 20 values per bin) for each axis to better reveal the trends of the metrics."
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,3.3,Robustness of the Proposed Model,"To examine the performance of our proposed method for image regions that contain fewer potent anatomical features, we acquired additional image pairs from test subjects, according to Sect. 2.4. With the new test set, the prediction errors for our method and the baseline model were 1.28 ± 0.99 mm and 2.49 ± 1.87 mm, respectively. Furthermore, the correlations between estimated and true error were calculated at 0.41 for FocalErrorNet and 0.20 for the baseline. These results supported the benefits of focal modulation in registration error estimation. In this test, patches can contain large areas of zeros (image content out of the scanning FOV of the iUS). The main reason for the observed performance decline is due to the reduction in sufficient image features in iUS. However, despite these challenges, we saw an acceptable outcome from FocalEr-rorNet (absolute error = 1.28 mm or ∼1 voxel in clinical MRIs)."
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,4,Discussion,"In image-guided interventions, there is an urgent need for automatic assessment of image registration quality. Multi-modal registration quality evaluation poses major challenges due to three main factors. First, dissimilar contrasts between images require more elaborate strategies to derive relevant features for error assessment. Second, unlike segmentation or classification, the ground truths of registration errors are difficult to obtain. Finally, compared with classification, regression tasks tend to be more error-prone for deep learning algorithms. To tackle these challenges, we employed 3D focal modulation with depth-wise convolution to encode contextual information for the image pair. Compared with the ViT and its variants, focal modulation allows a more lightweight setup, which could be desirable for 3D data. Although we admit that residual errors still remain after landmark-based B-Spline nonlinear alignment, this approach has been adopted in different prior studies, considering the residual landmark registration error is fairly low (mTRE of 0.0008 ± 0.0010mm). Although simulated ultrasound has been used to provide a perfect alignment with MRIs, the fidelity of the simulated results is still suboptimal, and this may explain the underperformance of the previous technique in real clinical data [12]. To ensure the performance of our FocalErrorNet, we opted to regress the mean registration error of image patches than simplistic error grades or voxel-wise error maps. We believe that this design choice offers a more stable performance, which is supported by our validation. We adopted uncertainty estimation in inter-modal registration error assessment for the first time. While other techniques exist to provide model uncertainty [18], MC dropout is more flexible for various DL models. Furthermore, the use of standard deviation as an uncertainty measurement maintains the same unit as the regressed errors, thus making the interpretation more intuitive. From quantitative and qualitative evaluations using correlation coefficients and scatter plots to assess the association of uncertainty measures with the prediction errors and image entropy, we confirmed the validity of the proposed uncertainty estimation approach. For our FocalErrorNet, we achieved a prediction error of 0.59 ± 0.57 mm, which is on par with the image resolution (0.5 mm). Additionally, the standard deviation of our results is lower than the baseline model [12]. These signify a robust performance of the FocalErrorNet. One limitation of our work lies in the limited patient data, as public iUS datasets are scarce, while the settings and properties of US scanners can vary, potentially affecting the DL model designs. Therefore, we created random deformations for patch-wise error estimation, and will further explore data-efficient approaches for registration error assessment."
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,5,Conclusion,"We proposed FocalErrorNet, a novel DL model for uncertainty-aware inter-modal registration error estimation in iUS-guided neurosurgery, leveraging the latest focal modulation technique and MC dropout. With thorough assessments of the accuracy and uncertainty measures, we have confirmed the performance of the proposed method against a baseline model previously adopted for the same task. As the first to introduce uncertainty measures and 3D focal modulation in registration error evaluation, our work provides the first step for fast and reliable feedback in inter-modal medical image registration to guide clinical decisions in surgery. We plan to adapt the presented framework for other inter-modal/contrast image registration applications in the future."
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,,Fig. 1 .,
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,,Fig. 2 .,
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,,Fig. 3 .,
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,,Fig. 4 .,
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,,Fig. 5 .,
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,,Table 1 .,
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",1,Introduction,"Surgical step recognition is necessary to enable downstream applications such as surgical workflow analysis [4,26], context-aware decision support [21], anomaly detection [14], and record-keeping purposes [5,28]. Some factors that make recognition of steps in surgical videos a challenging problem [20,21] include variability in patient anatomy and surgeon style [11], similarities across steps in a procedure [5,16], online recognition [25] and scene blur [12,21].Early statistical methods to recognize surgical workflow, such as Conditional Random Fields [19,23], Hidden Markov Models (HMMs) [6,24] and Dynamic Time Warping [3,18], have limited representation capacity due to pre-defined dependencies. Multiple deep learning based methods have been proposed for surgical step recognition. SV-RCNet [15] jointly trains ResNet [13] and a long shortterm memory (LSTM) model and uses a prior knowledge scheme during inference. TMRNet [16] utilizes a memory bank to store long range information on the relationship between the current frame and its previous frames. SV-RCNet and TMR-Net use LSTMs, which are constrained in capturing long-term dependencies in surgical videos due to their limited temporal memory. Furthermore, LSTMs process information in a sequential manner that results in longer inference times. Methods that don't use LSTMs include a 3D-covolutional neural network (3D-CNN) to learn spatio-temporal features [10] and temporal convolution networks (TCNs) [27]. In recent work, a two-stage network called TeCNO included a ResNet to learn spatial features, which are then modeled with a multi-scale TCN to capture longterm dependencies [5]. The previous networks use multi-stage training to exploit spatial and temporal information separately. This approach limits model capacity to learn spatial features using the temporal information. Furthermore, the temporal modeling does not sufficiently benefit from low-dimensional spatial features resulting in low sensitivity to identify transitions between activities [12,15]. [12] attempts to address this issue by adding one more stage to the network using a feature fusion technique that employs a transformer to refine features extracted from the first and second stages of TeCNO [5].Transformers improve feature representation by effectively modeling longterm dependencies, which is important for recognizing surgical steps in complex videos. In addition, they offer fast processing due to their parallel computing architecture. To exploit these benefits and address the issue of inductive bias (e.g. local connectivity and translation equivariance) in CNNs, recent methods use only transformers as their building blocks, i.e., Vision Transformer. For example, TimesFormer [2], applies self-attention mechanisms to learn spatial and temporal relations in videos, and ViViT [1] utilizes a vision transformer architecture [8] for video recognition. However, these models were proposed for temporal clips and do not specifically focus on capturing long-range dependencies, which is important for surgical step recognition in long-duration untrimmed videos.In this work, we propose a transformer-based model with the following contributions: (1) Spatio-temporal attention is used as the building blocks to address issues with inductive bias and end-to-end learning of surgical steps; (2) A twostream model, called Gated -Long, Short sequence Transformer GLSFormer , is proposed to capture long-range dependencies and a gating module to leverage cross-stream information in its latent space; and (3) The proposed GLSFormer is extensively evaluated on two cataract surgery video datasets to show that it outperforms all compared methods."
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",2,The GLSFormer Model,"Given an untrimmed video with X[1 : T ] frames, where T represents the total number of frames in the video, the objective is to predict the step Y [t] of a given frame at time t (Fig. 1). Fig. 1. Overview of the proposed GLSFormer . Specifically, GLSFormer takes two streams (long-stream and short-stream) image sequences as input, sampled with sampling period of s and 1 respectively. Later, each frame is decomposed into nonoverlapping patches and each of these patches are linearly mapped to an embedding vector. These embedded features are spatio-temporally encoded into a feature representation using a sequential temporal-spatial attention block as shown in (b). Architecture of gated-temporal attention is showed in detail in (c). The final feature representation is then examined by a multilayer perceptron head and a linear layer to produce a step prediction at every time-point. Our method provides a single-stage, end-to-end trainable model for surgical step recognition.Long-short Sequence. We propose GLSFormer model that can capture both short-term and long-term dependencies in surgical videos. The input to our GLSFormer are two video sequences, a short-term sequence consisting of the last n st frames from time t, and a long-term sequence composed of n lt frames selected from a sub-sampled set of frames with a sampling period of s. The longterm sequence provides a coarse overview of information distant in time and can aid in accurate predictions of the current frame, overcoming false prediction based on common artifacts in short-term sequences. In addition, the overview of information can address the high variability in surgical scenes [5,9,12,16]. By leveraging both short-term and long-term sequences, our model can accurately capture the complex context present in long surgical videos."
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",,Patch Embedding. We decompose each frame of dimension H,"Each patch is flattened into a vector x p,t ∈ R 3Q 2 for each frame t and spatial location, p ∈ (1, N). We linearly map the patches of short and long term videos frames into embedding vector of dimension R K using a shared learnable matrix E ∈ R K×3Q 2 . We concatenate the patch embeddings of the short and long-term streams along the frame dimension to form feature representations x st p,t and x lt p,t of size N × n st × K and N×n lt ×K, respectively. along with a learnable positional embedding e st-pos p,t .(Note that a special learnable vector z st 0,0 ∈ R K representing the step classification token is added in the first position. Our approach is similar to word embeddings in NLP transformer models [1,2,7].Gated Temporal, Shared Spatial Transformer Encoder. Our Transformer Encoder, consisting of Gated Temporal Attention module and Shared Spatial Attention module takes the sequence of embedding vectors z st p,t and z lt p,t as input. In a self-attention module for spatio-termporal models, computational complexity increases non-linearly O(T 2 S 2 ) with increase in spatial resolution(S) or temporal frames(T). Thus, to reduce the complexity, we sequentially process our gated temporal cross attention module and spatial attention module [1,2]. Transformer Encoder consists of L Gated-Temporal, Spatial Attention blocks. At each block l, feature representation is computed for both streams from the representation z lt l-1 and z st l-1 encoded by the preceding block l -1. We explain our Gated Temporal attention and shared spatial attention in more detail in the rest of the section."
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",,Gated Temporal Attention. The temporal cross-attention module aligns the long-term(z lt,"l-1 ) and short-term features(z st l-1 ) in the temporal domain, allowing the model to better capture the relationship between the long and short term streams. We concatenate both of these streams to form a strong joint stream that has both fine-grained information from the short-term stream and global context information from the long-term stream. Firstly, a query/key/value vector for each patch in the representations z lt l-1 (p, t), z st l-1 (p, t) and z lt,st l-1 (p, t) using linear transformations with weight matrices U lt qkv , U st qkv and U lt,st qkv respectively and normalization using LayerNorm is computed as follows:where a ranges from 1 to A representing attention heads. The total number of attention heads is denoted by A, and each has a latent dimensionality of K h = K A . The computation of these QKV vectors is essential for multi-head attention in transformer.Now for refining the streams, with most relevant cross-stream information, we gate the individual stream's temporal features(I) (QKV ) a;lt/st with the joint stream temporal features(J) (QKV ) a;lt,st . Gating parameters are calculated by concatenating I and J and passing them through linear and softmax layers which predict Gt st and Gt lt for (QKV ) a;st and (QKV ) a;lt , respectively. By gating the individual stream's temporal features with the joint stream temporal features, the model is able to selectively attend to the most relevant features from both streams, resulting in a more informative representation. This helps in capturing complex relationships between the streams and improves the overall performance of the model. This computation can be described as followsLater, temporal attention is computed by comparing each patch (p, t) with all patches at the same spatial location in other frames of both streams, as followsHere, αis separately calculated for the long-stream and shortstream, where (•) = lt or st. Similar to the vision transformer, encoding blocks for each layer (z lt and z st ) are computed by taking the weighted sum of value vectors (SA a (z)) using self-attention coefficients from each attention head as followsNext, the self-attention block (SA a (z)) for each attention head is projected along with a residual connection from the previous layer. This multi-head self-attention (MSA) operation can be described as followsHere, (z ) l is the concatenation of z lt and z st ."
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",,Shared Spatial Attention.,"Next, we apply self-attention to the patches of the same frame to capture spatial relationships and dependencies within the frame.To accomplish this, we calculate new key/query/value using Eq. ( 2) and use it to perform spatial attention in Eq. (6).The encoding blocks are also calculated using Eq. ( 4) and ( 5), and the resulting vector is passed through a multilayer perceptron (MLP) of Eq. ( 7) to obtain the final encoding z (p, t) for the patch at block l as followsThe embedding for the entire clip is obtained by taking the output from the final block and passing it through a MLP with one hidden layer. The corresponding computation can be described as y = LN(z L (0,0) ) ∈ R D . The classification token is used as the final input to the MLP for predicting the step class at time t. Our GLSFormer is trained using the cross-entropy loss."
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",3,Experiments and Results,"Datasets. We evaluate our GLS-Former on two video datasets of cataract surgery, namely Cataract-101 [22] and D99 [26]. The Cataract-101 dataset comprises of 101 cataract surgery video recordings, each captured at 25 frames per second and annotated into 10 steps by surgeons. The spatial resolution of these videos is 720 × 540, and temporal resolution of 25 fps. In accordance with previous studies [12,24], we use 50 videos for training, 10 for validation and 40 videos for testing. The D99 dataset, which consists of 99 videos with temporal segment annotations of 12 steps by expert physicians, has a frame resolution of 640 × 480 at 59 fps. We randomly shuffled videos and select 60, 20 and 19 videos for training, validation and testing respectively. All videos are subsampled to 1 frame per second, as done in previous studies [12,24], and the frames are resized to 250 × 250 resolution.Evaluation Metrics. To accurately evaluate the results of surgical step prediction models, we use four different metrics, namely Accuracy, Precision, Recall, and Jaccard index [5,12,24]. Implementation Details. We utilized an NVIDIA RTX A5000 GPU to train our GLSFormer model on PyTorch. The batch size was set equal to 64. Data augmentations were applied including 224 × 224 cropping, random mirroring, and color jittering. We employed the Adam optimizer with an initial learning rate of 5e-5 for 50 epochs. Additionally, we initialized the shared parameters of the model from a pre-trained model on Kinetics-400 [2,17]. The model's depth and the number of attention heads were set equal to 12 each. We used 8 frames for both short-stream and long-stream, and sampling rate of 8, unless stated otherwise.Table 1. Quantitative results of step recognition from different methods on the Cataract-101 and D99 datasets. The average metrics over five repetitions of the experiment with different data partitions for cross-validation are reported (%) along with their respective standard deviation (±). Comparison with State-of-the-art Methods. Table 1 presents a comparison between our proposed approach, GLSFormer , and current state-of-the-art methods for surgical step prediction. The comparison includes six models (1-6) that utilize ResNet [13] as a spatial feature extractor and two models (7-8) that use vision transformer backbones specifically designed for surgical step prediction. While SV-RCNet, OHFM, and TMRNet use ResNet-LSTM to capture shortrange dependencies, OHFM uses a multi-step framework and TMRNet uses a multi-stage network to refine predictions using non-trainable long-range memory banks. Our approach achieves a significant improvement of 7%-10% in Jaccard index using a simpler, single-stage training procedure with higher temporal resolution. Although other multi-stage approaches like TeCNO and Trans-SVNet use temporal convolutions to capture long-range dependencies, we achieve a boost of 6%-9% with joint spatiotemporal modeling. In contrast, transformer-based models capture spatial information (ViT) and short-term temporal (TimesFormer) efficiently, but they lack the long-term coarse step information required for complex videos. Our approach combines short-term and long-term spatiotemporal information in a single stage using gated-temporal and shared-spatial attention. This approach outperforms ViT and TimesFormer by a relative improvement of 6% to 11% in Jaccard index across both datasets. ) and noisy patterns (such as at P10) due to their high reliance on extracted spatial features and error propagation across stages in the model. Specifically, errors in the early stages of spatial modeling ResNet can propagate and accumulate across later stages, and lead to incorrect predictions. The ribbon plot of TimesFormer (i) demonstrates significant improvement compared to previous methods due to joint learning of spatio-temporal features. However, due to the lack of pivotal coarse long-term information, misclassifications are observed in local step transition areas, as seen in the incorrect prediction of P2 at locations of P1 and P11. On the other hand, GLSFormer elegantly aggregates spatio-temporal information from both streams, making the features more reliable. Additionally, our approach uses a single stage to limit the amount of error propagated across stages, contributing to improved accuracy in surgical step recognition.Ablation Studies. The top part of Table 2 shows the effect of different sampling rates in the long-term stream for step prediction in the Cataract-101 dataset.The results demonstrate that incorporating a coarse long-term stream is crucial for achieving significant performance gains compared to not using any long-term sequence (as in ViT and TimesFormer). Additionally, we observe that gradually increasing the sampling rate from 2 to 8 improves performance across all metrics, except for a slight decline at a sampling rate of 16. This decline may be due to a loss of information and noisy predictions resulting from the high number of frames skipped between each selected frame. Therefore, we chose a sampling rate of 8, as it provided the optimal balance between capturing valuable information and avoiding noise in our long-stream sequence. To evaluate our gating mechanism's effectiveness, we conducted ablation experiments with three different settings, as summarized in the bottom part of Table 2. Initially, we passed both short-term and long-term stream features directly in the shared multi-head attention layer, but the model's performance was worse compared to the model trained only with short-term information (75.97 vs 74.82 Jaccard). The reason for this could be the lack of filtering to extract coarse temporal information. For instance, the long-term stream may contain noisy spatial information that is irrelevant to the temporal attention mechanism, which can affect the model's ability to attend to relevant information. Additionally, incorporating a learnable gating parameter to regulate the flow of information between the short-term and long-term streams enhanced our model's performance by 4%, enabling individual stream refinement through cross-stream information. However, we observe that this approach has a limitation as the amount of crossstream information sharing remains fixed during inference regardless of the quality of the feature representation in both streams at a particular time-frame. To address this limitation, we propose predicting gating parameters directly based on the spatio-temporal representation in both streams for that time frame. This approach allows for dynamic gating parameters, which means that at a particular time-point, short-term temporal feature representation can variably leverage the long-term coarse information as well can prioritize its own representation if it is more reliable. Improvement of 3% Jaccard score is realized by using featurebased gating parameter estimation in GLSFormer compared to a fixed parameter gating mechanism. Our ablation study clearly highlights the significance of our gated temporal mechanism for feature refinement."
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",4,Conclusion,"We propose GLSFormer , a vision transformer-based method for recognizing surgical steps in complex videos. Our approach uses a gated temporal attention mechanism to integrate short and long-term cues, resulting in superior performance compared to recent LSTM and vision transformer-based approaches that only use short-term information. Our end-to-end joint learning captures spatial representations and sequential dynamics more effectively than multi-stage networks. We extensively evaluated GLSFormer and found that it consistently outperformed state-of-the-art models for surgical step recognition."
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",,Fig. 2 .,
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",,Table 2 .,
"$$\textsf{GLSFormer}$$: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos",,.67 90.04 ± 0.71 89.45 ± 0.79 81.89 ± 0.92,
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,1,Introduction,"Craniomaxillofacial (CMF) deformities can affect the skull, jaws, and midface. When the primary cause of disfigurement lies in the skeleton, surgeons cut the bones into pieces and reposition them to restore normal alignment [12]. In this context, the focus is on correcting bone deformities, as it is anticipated that the restoration of normal facial appearance will follow automatically. Consequently, it is customary to initiate the surgical planning process by estimating the positions of normal bones. The latter has given rise to bone-driven approaches [1,9,14]. For example, methods based on sparse representation [13] and deep learning [15] have been proposed to estimate the bony shape that may lead to an acceptable facial appearance.However, the current bone-driven methods have a major limitation, subjecting to the complex and nonlinear relationship between the bones and the draping soft-tissues. Surgeons estimate the required bony movement through trial and error, while computer-aided surgical simulation (CASS) software [14] simulates the effect on facial tissues resulting from the proposed movements. Correcting the bone deformity may not completely address the soft-tissue disfigurement. The problem can be mitigated by iteratively revising the bony movement plan and simulating the corresponding soft-tissue changes. However, this iterative planning revision is inherently time-consuming, especially when the facial change simulation is performed using computationally expensive techniques such as the finite-element method (FEM) [4]. Efforts have been made to accelerate facial change prediction using deep learning algorithms [2,5,8], which, however, do not change the iterative nature of the bone-driven approaches.To address the above challenge, this paper proposes a novel soft-tissue driven surgical planning framework. Instead of simulating facial tissue under the guessed movement, our approach directly aims at a desired facial appearance and then determines the optimal bony movements required to achieve such an appearance without the need for iterative revisions. Unlike the bone-driven methods, this soft-tissue driven framework eliminates the need for surgeons to make educated guesses about bony movement, significantly improving the efficiency of the surgical planning process. Our proposed framework consists of two main components, the Bony Planner (BP) and the Facial Simulator (FS). The BP estimates the possible bony movement plans (bony plans) required to achieve the desired facial appearance change, while the FS verifies the effectiveness of the estimated plans by simulating the facial appearance based on the bony plans. Without the intervention of clinicians, the BP automatically creates the most clinically feasible surgical plan that achieves the desired facial appearance.The main contributions of our work are as follows. 1) This is the first softtissue driven approach for CMF surgical planning, which can substantially reduce the planning time by removing the need for repetitive guessing bony movement.2) We develop a deep learning model as the bony planner, which can estimate the underlying bony movement needed for changing a facial appearance into a targeted one. 3) The developed FS module can qualitatively assess the effect of surgical plans on facial appearance, for virtual validation."
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,2,Method,"Figure 1 shows an overview of the proposed framework, which consists of two primary modules. The first module is a BP network that plans bony movement based on the desired facial outcome and the given preoperative facial and bony surfaces. The second module is the FS network that simulates corresponding postoperative facial appearances by applying the estimated bony plans. Instead of providing one single bony plan, the BP will estimate multiple plans because not all the generated plans may result in the desired clinical effect. The FS then simulates facial appearances by using those plans and chooses the plan leading to the facial appearance closest to the desired target. The two models are deployed together to determine and confirm the final bony plan. Below we first present the details of BP and FS modules, then we introduce how they are deployed together for inference. Fig. 1. Overview of the proposed soft-tissue driven framework. The framework is composed of two main components: the creation of candidate bony plans using BP, and the simulation of facial outcomes following the plans using FS. Finally, the facial outcomes are compared with the desired face to select the final bony plan."
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,2.1,Bony Planner (BP),"Data Preprocessing: The bony plan is created by the BP network using preoperative facial F pre , bony surface B pre , and desired facial surface F des . The goal of the BP network is to convert the desired facial change from F pre to F des into rigid bony movements, denoted as T S for each bony segment S, as required for surgical planning. However, it is very challenging to directly estimate the rigid bone transformation from the facial difference. Therefore, we first estimate the non-rigid bony movement vector field and then convert that into the rigid transformations for each bone segment. Figure 2 illustrates the BP module and the details are provided as follows. For computational efficiency, pre-facial point set P Fpre , pre-bony point set P Bpre , desired point set P F des are subsampled from the pre-facial/bony and desired facial surfaces."
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,,Non-rigid Bony Movement Vector Estimation:,"We adopt the Attentive Correspondence assisted Movement Transformation network (ACMT-Net) [2], which was originally proposed for facial tissue simulation, to estimate the pointwise bony displacements to acquire F des . This method is capable of effectively computing the movement of individual bony points by capturing the relationship between facial points through learned affinity. In the network, point-wise facial features (F Fpre ) and bony features (F Bpre ) are extracted from P Fpre and P Bpre . We used a pair of modified PointNet++ modules where the classification layers were removed and a 1D convolution layer was added at the end to project F Fpre and F Bpre into the same lower dimensions [10]. A correlation matrix R is established by computing the normalized dot product between F Fpre and F Bpre to evaluate the relationship between the facial and bony surfaces:where N Fpre denotes the number of facial points P Fpre . On the other hand, desired facial movement V F is computed by subtracting P F des and P Fpre . V F is then concatenated with P Fpre and fed into a 1D convolution layer to encode the movement information. Then the movement feature of each bony point is estimtated by the normalized summary of facial features using R. Finally, the transformed bony movement features are decoded into movement vectors after being fed into one 1D convolution layer and normalization:Rigid Bony Movement Regression: Upon obtaining the estimated pointwise bony movements, they are added to the corresponding bony points, resulting in a non-rigidly transformed bony point set denoted as P B pdt . The resulting points are grouped based on their respective bony segments, with point sets P Spre and P S pdt representing each bony segment S before and after movement, respectively. To fit the movement between P Spre and P S pdt , we estimate the rigid transformations [R S , T S ] by minimizing the mean square error as follows:where i and N represent the i-th point and the total number of points in P Spre , respectively. First, we define the centroids of P Spre and P S pdt to be P Spre and P S pdt . The cross-covariance matrix H can be computed asThen we can use singular value decomposition (SVD) to decomposethen the alignment minimizing E(R S , T S ) can be solved byFinally, the rigid transformation matrices are applied to their corresponding bone segments for virtual planning."
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,2.2,Facial Simulator (FS),"While the bony planner can estimate bony plans and we can compare them with ground truth. However, the complex relationship between bone and face makes it unknown whether adopting the bony plan will result in the desired facial outcome or not. To evaluate the effectiveness of the BP network in simulating facial soft tissue, an FS is developed to simulate the facial outcome following the estimated bony plans. For facial simulation, we employ the ACMT-Net, which takes P Fpre , P Bpre , and P B pdt as input and predicts the point-wise facial movement vector V F . The movement vector of all vertices in the facial surface is estimated by interpolating V F . The simulated facial surface F pdt is then reconstructed by adding the predicted movement vectors to the vertices of F pre ."
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,2.3,Self-verified Virtual Planning,"To generate a set of potential bony plans, we randomly perturbed the surfaces by flipping and translating them up to 10mm in three directions during inference. We repeated this process 10 times in our work. After estimation, the bony surfaces were re-localized to their original position prior to perturbation. Sequentially, the FS module generated a simulated facial appearance for each bony plan estimated from the BP module, serving two purposes. Firstly, it verified the feasibility of the bony plan through facial appearance. Secondly, it allowed us to evaluate the facial outcomes of different bony plans. The simulated facial surfaces were compared with the desired facial surface, and the final plan was selected based on the similarity of the resulting facial outcome. This process verified the efficacy of the selected plan for achieving the desired facial outcome. 3 Experiments and Results"
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,3.1,Dataset,"We employed a five-fold cross-validation technique to evaluate the performance of the proposed network using 34 sets of patient CT data. We partitioned the data into five groups with {7, 7, 7, 7, 6} sets of data, respectively. During each round of validation, four of these groups (folds) were used for training and the remaining group was used for testing. The CT scans were randomly selected from our digital archive of patients who had undergone double-jaw orthognathic surgery. To obtain the necessary data, we employed a semi-automatic method to segment the facial and bony surface from the CT images [6]. Then we transformed the segmentation into surface meshes using the Marching Cube approach [7]. To retrospectively recreate the surgical plan that could ""achieve"" the actual postoperative outcomes, we registered the postoperative facial and bony surfaces to their respective preoperative surfaces based on surgically unaltered bony volumes, i.e., cranium, and utilized them as a roadmap [8]. To establish the surgical plan to achieve actual postoperative outcomes, virtual osteotomies were first reperformed on the preoperative bones to create bony segments, including LeFort 1 (LF), distal (DI), right proximal (RP), and left proximal (LP). The movement of the bony segments, which represents the surgical plan to achieve the actual postoperative outcomes, was retrospectively established by manually registering each bony segment to the postoperative roadmap, which served as the ground truth in the evaluation process. We rigidly registered the segmented bony and facial surfaces to their respective bony and facial templates to align different subjects. Additionally, we cropped the facial surfaces to retain only the regions of interest for CMF surgery. In this study, we assume the postoperative face is the desired face. For efficient training, we subsampled 4096 points from the facial and bony surfaces, respectively, with 1024 points for each bony segment. To augment the data, we randomly flipped the point sets symmetrically and translated them along three directions within a range of 10 mm."
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,3.2,Implementation and Evaluation Methods,"To compare our approach, we implemented the state-of-the-art bone-driven approach, i.e., deformation network (DefNet) [15], which takes the point coordinates and their normal vectors as input and generates the displacement vectors to deform the preoperative bones. Our method has two variations: BP and BP+FS. BP estimates only one plan, while BP+FS selects the final plan based on the simulated facial outcomes of our facial simulator. The PointNet++ networks utilized in both BP and FS are comprised of four feature-encoding blocks and four feature-decoding blocks. The output dimensions for each block are 128, 256, 512, 1024, 512, 256, 128, and 128, respectively, and the output point numbers of the modules are 1024, 512, 256, 64, 256, 512, 1024, and 4096, sequentially. We used the Adam optimizer with a learning rate of 0.001, Beta1 of 0.9, and Beta2 of 0.999 to train both the BP and FS networks. For data augmentation, both facial and bone data are subjected to the same random flipping and translation, ensuring that the relative position and scale of facial changes and bony movements remain unchanged. The models were trained for 500 epochs with a batch size of 4 and MSE loss was used, after which the models were used for evaluation. The models were trained on an NVIDIA DGX-1 deep learning server with eight V100 GPUs. The prediction accuracy of the soft-tissue driven approach was evaluated quantitatively and qualitatively by comparing it with DefNet. Then quantitative evaluation was to assess the accuracy using the mean absolute error (MAE) between the predicted bony surfaces and the ground truth. For detailed evaluation, MAE was also separately calculated for each bony segment, including LF, DI, RP, and LP. Statistical significance was determined using the Wilcoxon signed-rank test to compare the results obtained by different methods [11]. Qualitative evaluation was carried out by directly comparing the bony surfaces and simulated facial outcomes generated by our approach and DefNet with the ground truth postoperative bony and facial surfaces. "
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,3.3,Results,"The results of the quantitative evaluation are shown in Table 1. The results of the Wilcoxon signed-rank test showed that BP outperforms DefNet on LF (p < 0.05), DI (p < 0.001), LP segments (p < 0.01), and the entire bone (p < 0.001) by statistically significant margins. In addition, BP+FS significantly outperforms BP on DI segment (p < 0.05). Two randomly selected patients are shown in Fig. 3. To make a clear visual comparison, we superimposed the estimated bony surfaces (Blue) with their corresponding ground truth (Red) and set the surfaces to be transparent as shown in Fig. 3(a). Figure 3(b) displays the surface distance error between the estimated bony surfaces and ground truth.The evaluation of the bony surface showed that the proposed method successfully predicted bony surfaces that were similar to the real postoperative bones. To further assess the performance of the method, a facial simulator was used to qualitatively verify the simulated faces from the surgical plans. Figure 4 shows the comparison of the simulated facial outcomes of different methods using FS network and the desired facial appearance. The facial outcomes derived from bony plans of DefNet and our method are visualized, and the preoperative face is also superimposed with GT for reference. The results of the facial appearance simulation further validate the feasibility of the proposed learning-based framework for bony movement estimation and indicate that our method can achieve comparable simulation accuracy with the real bony plan."
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,4,Discussions and Conclusions,"As a result of our approach that considers both the bony and soft-tissue components of the deformity, the accuracy of the estimated bony plan, especially on the DI segment, has significantly improved. Nonetheless, the non-linear relationship between the facial and bony surfaces cannot be adequately learned using only facial and bony surface data, and additional information such as tissue properties can also affect the facial outcome. To account for uncertainty, we introduce random perturbations to generate different plans. In the future, we plan to incorporate additional uncertainty into the bony planner by using stronger perturbations or other strategies such as dropout [3,16] and adversarial attacks [17,18], which could help create more diverse bony plans. Also, relying solely on a deep learning-based facial simulation to evaluate our method might not fully validate its effectiveness. We plan to utilize biomechanical models such as FEM to validate the efficacy of our approach in the future. Moreover, for our ultimate goal of translating the approach to clinical settings, we will validate the proposed method using a larger patient dataset and compare the predicted bony plans with the actual surgical plans.In conclusion, we have developed a soft-tissue driven framework to directly predict the bony plans that achieve a desired facial outcome. Specifically, a bony planner and a facial simulator have been proposed for generating bony plans and verifying their effects on facial appearance. Evaluation results on a clinical dataset have shown that our method significantly outperforms the traditional bone-driven approach. By adopting this approach, we can create a virtual surgical plan that can be assessed and adjusted before the actual surgery, reducing the likelihood of complications and enhancing surgical outcomes. The proposed soft-tissue driven framework can potentially improve the accuracy and efficiency of CMF surgery planning by automating the process and incorporating a facial simulator to account for the complex non-linear relationship between bony structure and facial soft-tissue."
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,,Fig. 2 .,
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,,Fig. 3 .,
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,,Fig. 4 .,
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,,Table 1 .,
Soft-Tissue Driven Craniomaxillofacial Surgical Planning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 18.
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,1,Introduction,"Hip osteoarthritis, with the top 10% occurrence in all diseases, brought a high demand for total hip arthroplasty (THA) in the past few decades [2]. According to the American Academy of Orthopaedic Surgeons, in the United States, approximately 450,000 THA surgeries are performed each year. One of the main challenges in THA is achieving accurate restoring leg length and femoral offset [3]. Failure in doing so can lead to instability, leg length discrepancies, impingement, persistent pain, and early implant failure [22], thus significantly affecting the clinical outcome and hip durability [19]. Therefore, a reliable intraoperative limb length measurement and restoration method is crucial to optimise patient outcomes and implant survival.Intraoperatively, leg length and femoral offset can be determined manually using a calliper between two reference points [1,3,11,17], but using a calliper is prone to measurement error [13]. Many computer-assisted methods [6] rely on numerous landmarks, such as condyles, or tibial spines, to determine the limb length [10], which could be inconvenient during surgeries. The optical tracking system is often used in THA for measurement as it has shown higher accuracy and reliability during interventions involving dynamic motion [18]. Sarin et al. [16] fixed optical tracking devices on the pelvis and femur as two references to measure the leg length and offset. Intellijoint HIP [13] is another 3D optical navigation tool, with the camera attached to the pelvis rather than placed next to the patient. Mako combines the preoperative CT 3D reconstruction and intraoperative optical tracking feedback for registration to determine the leg length and offset [4,20]. The main limitation of optical tracking is the requirement for a direct and free line-of-sight between markers and cameras [18].In contrast, the Electromagnetic (EM) sensor based navigation system can provide fast and accurate tracking without line-of-sight constraints [5,18]. Zhao et al. [23] proposed a real-time robust simultaneous catheter and environment modelling for endovascular navigation, which is based on intravascular ultrasound and EM sensing. Mohammadbagherpoor et al. [12] developed an EMbased inductive proximity sensor system for detecting hip joint implant loosening in the micron range. Intracs R em is an intelligent navigation system based on EM tracking for endoscopic minimally invasive spine surgery [8].In the commonly used intraoperative leg length equalisation and offset recovery techniques, both traditional and computer-assisted methods require the femur to be held and stored at the preoperative neutral reference position (0 • flexion, 0 • rotation, and 0 • abduction) prior to hip dislocation, and measure the changes in leg length and femoral offset as the femur is returned to the neutral reference position [1,13,15,16]. Inaccurate repositioning of the femur w.r.t. the pelvis can result in additional measurement errors since only 4 • of abduction/adduction could cause 5-7 mm error in leg length and 2-4 mm error in offset [9]. In our method, we aim to eliminate the femoral repositioning prior to measurement to avoid the additional errors.In this work, we propose a robust and accurate intraoperative limb length measurement method for THA based on EM sensing. Using the idea that the femoral movement can be mathematically modelled as a vector rotating around a fixed rotation centre, we develop a closed-form optimisation solution that uses a set of sampled poses from EM readings to calculate the intraoperative limb length change. The experiment results demonstrate that the proposed method can be more accurate. In summary, the key advantages of our method include: (i) the optimisation with a closed-form solution is an active compensation [5,18], which can effectively reduce static errors in the EM tracking itself and significantly improve the accuracy; (ii) different from pivot calibration, only slight movement of the femur is required to obtain accurate limb length measurements;(iii) no need to return the leg back to the neutral reference position again after replacing the damaged hip joint with artificial implants, which effectively avoids measurement errors due to inaccurate abduction/adduction repositioning; (iv) the proposed method does not require the direct line-of-sight, and can be easily integrated clinically, without interrupting the workflow of THA. "
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,2,Methodology,
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,2.1,Problem Formulation,"Standard THA Routine. During a standard THA, the surgery is often performed in the lateral position. Two reference points are marked on the pelvis and femur, respectively. The reference can be iliac fixation pins, sensors, or optical trackers. The surgeon then finds the preoperative neutral reference position by experience and records the relative position between the two reference points prior to the femoral head resection. The following routines include damaged femoral head removal, femoral canal broaching, acetabular preparation, and component selection and alignment. The component alignment requires the surgeon to return the femur to the neutral reference position and measure the change in leg length and offset.Our Setup. In our proposed method (Fig. 1), the EM tracking board is placed under the patient's hip during the THA surgery, and one pin with EM sensor is installed on the pelvis. The supercapsular percutaneously assisted (SuperPath) approach [14] is used to insert a metal stem (or implant) into the hollow centre of the femur, without the need for femoral head resection and removal prior to the femoral canal broaching. A T-shaped adaptor mounted with another EM sensor is rigidly attached to the stem. When measuring changes in leg length and femoral offset, we sample poses of the postoperative femur during a slight femoral movement. The problem considered in this work is to use the sampling postoperative femoral poses to estimate the leg length and offset change instead of repositioning the femur to the neutral reference position. Denote the frames of EM sensors on the pelvis and femur as {P } and {F }, respectively. The real-time readings of two EM sensors are represented aswhich are respectively the rotation matrices and translation vectors of frames {P } and {F } w.r.t. the frame of EM tracking board denoted by {B} (Fig. 2). Then, to eliminate the effect of patient motion, the relative pose of frame {F } w.r.t. {P } is used and denoted byProblem Statement. Suppose P X pre F 0 = { P R pre F 0 , P t pre F 0 } is the recorded pose of {F } w.r.t. {P } at the preoperative neutral reference position before femoral head resection, and N sampling postoperative femoral poses in frame {P }, denoted byare collected through small motion around the neutral position after femoral head replacement. Since the relative rotations of frame {F } in {P } should be the same at the neutral reference position before and after the femoral head replacement, mathematically, the problem considered in this paper is, given sampling postoperative femoral poses P X F i (i ∈ {1, • • • , N}), accurately estimate the current translation vector P t post F 0 when the relative rotation is P R pre F 0 , and then use it to calculate the change in leg length and offset."
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,2.2,Limb Length Measurement Without Femoral Repositioning,"Since the joint between the acetabular component and the metal femoral head is a perfect sphere, the femoral movement in the frame {P } can be mathematically modelled as a vector b rotating around a centre c (Fig. 2Therefore, the relation among the vectors t, b, and c (refer to Fig. 2: Right) can be described aswhere P b and F b are the rotating vector b in {P } and {F }, respectively, and P c is the rotation centre c in {P }.The relation ( 1) is valid for all P X F , so F b and P c can be obtained from the N sampled poses P X F i (i ∈ {1, • • • , N}) by solving an optimisation problem.Then, the postoperative translation vector P t post F 0 can be calculated by the relative rotation P R pre F 0 . In contrast to pivot calibration [21] which is commonly used to estimate the tip location of a pointer tool, our method focuses on estimating limb change after femoral head replacement and therefore requires only minor leg movements for sampling, and inaccurate rotation centre estimate due to singularity has almost no effect on limb length estimation. See below for details.Full Least Squares Solution. Through the iterative Gauss-Newton (GN) method, the solution for F b and P c can be obtained by solving the following full nonlinear least squares (Full LS) optimisation problem, arg minwhere r( P R F i ) and r( P RFi ) are the Euler angles of rotation measurement P R F i and the corresponding rotation variable P RFi , respectively. Ω ti and Ω Ri are the covariance matrices of EM measurement noises w.r.t. translation and rotation.Closed-Form Solution. Since the EM measurements of rotations are accurate enough [7], i.e. P RFi ≈ P R F i , the contribution of the second term in (2) is limited. As a result, the optimisation problem can be simplified as a linear least squares problem by letting P RFi = P R F i : arg minwhich has an easier and more efficient closed-form solutionThe comparison in Sect. 3 will show that the closed-form solution ( 4) is almost the same as the solution to Full LS (2), but only requires sub-millisecond computational cost which is thousands of times less. Closed-form solution also benefits from the fact that solutions can be obtained in one step, avoiding potential local minima, providing greater robustness, and being easier to implement. So our proposed measurement approach is based on the closed-form solution.Limb Length Change Measurement. After F b * and P c * are obtained, the postoperative translation vector P t post F 0 at neutral reference position (where the relative rotation is P R pre F 0 ) can be calculated by (1)The change of relative translation vector at the neutral reference position is Δ P t F 0 = P t post F 0 -P t pre F 0 . Further, we denote P L as the projection of Δ P t F 0 onto the sagittal plane. Then, the changes in leg length and offset are computed by the norms P L and Δ P t F 0 -P L , respectively. The covariance of P t post F 0 in ( 5) is calculated bywhere I is the identity matrix, Ω ti is the covariance matrices of EM measurement noises w.r.t. translation. It can be proved that (6) tends to be zero as increasing samples if data are all sampled around the neutral position ( P R F i ≈ P R pre F 0 ), although the uncertainty of F b * and P c * in ( 4) is large due to the near singularity in this case. Therefore, a slight movement of the leg (rotating around the neutral position within a few degrees) can guarantee the accuracy of limb length estimate while preventing injury to the patient and the workload of surgeons."
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,3,Experiments,
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,3.1,Simulation and Robustness Assessment,"To compare the closed-form solution (4) and the solution to Full LS (2), five different levels of zero-mean Gaussian noises are added to the rotation angles and translations of sampling femoral poses P X F i (i ∈ {1, • • • , 100}) from EM readings (first row in Table 1). Twenty independent runs are executed for each noise level to test the robustness and accuracy of both two methods. The mean absolute errors compared with the ground truth and standard deviation (STD) of the twenty runs for estimating the neutral femur position are shown in Table 1. It shows that the proposed closed-form solution can achieve similar accuracy compared with the solution to the Full LS problem and the robustness to additionally added sensor noises is high. "
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,3.2,Phantom Experiments,"The phantom experiments (Fig. 3) were performed using two different sawbones models. One experienced surgeon executed a normal surgical routine using standard hip arthroplasty components. Three commonly used standard femoral heads ({-4, 0, +4} mm) were used for the alignment (Fig. 3: Right). After placing a metal acetabular shell into the pelvic cavity and inserting a stem into the femur, the surgeon selected one femoral head component and placed it on top of the stem, and the femoral head was placed into the liner. Then the surgeon found and recorded the neutral reference position of the femur model. After that, another femoral head was replaced to change the limb length. Finally, the limb length change before and after the femoral component replacement was calculated by different methods. The ground truth was available from the size changes between femoral head components. "
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Number of Samples and Comparison with Full LS.,"The first setup of phantom experiments (Fig. 3: Left) was designed to analyse the effect of the number of sampled poses on the performance of our method and to further compare the closed-form solution (4) and the solution to Full LS (2). Six different alignments of the femoral head components were performed (-4 to 0, -4 to +4, 0 to -4, 0 to +4, +4 to -4, and +4 to 0). After installing the replaced femoral head components, the surgeon slightly rotated the femur to collect the sampling pose data from EM sensors ({50, 100, 200, 500, 1000} samples for each alignment).As shown in Fig. 4: Left, the closed-form solution performs as good as Full LS, with computational costs ranging from 0.3 ms to 3.3 ms corresponding to data numbers of 50 to 1000, which is thousands of times less than Full LS. Estimation error decreases as the number of samples increases. Three examples of results are shown in Fig. 4: Right. The acquisition frequency of EM tracking is 20 Hz and the improvement in accuracy is limited when the number of data is more than 100. To balance the efficiency and accuracy, the closed-form solution with 100 data is our choice for the experiments in the rest of the paper.Comparison with Other Methods. We compared our proposed method (Closed-form) with three other different measurement approaches in the second phantom experiment setup (Fig. 3: Middle). The standard optical tracking based approach [13,16], the conventional mechanical method [1,3,11,17] by manual gauge, and our method, were performed at the same time. A straightforward idea of using EM sensor (one EM reading only) was also carried out to demonstrate the advantage of our closed-form solution further. The groin pins, optical trackers, and EM sensors were fixed on the sawbones models. All three methods other than ours require repositioning the femur to the neutral reference position before measurements. Table 2 summarises the mean absolute errors (MAE) of leg length change (LC) and offset change (OC) using all methods for six different alignments. Three independent runs are executed for each alignment and overall the proposed method achieves the highest accuracy (the p-values for the other three methods compared to ours are shown in the last column of Table 2). "
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,3.3,Cadaver Experiments,"The proposed method was also tested in cadaver experiments (Fig. 5: Left). The cadaveric body was operated on lateral decubitus. The whole THA routines including SuperPath broaching, hip dislocation, femoral head removal as well as acetabular and femoral preparation were executed in the cadaver experiment. To conduct multiple sets of experiments, a standard adjustable trial neck (Fig. 5: Right) was inserted which allowed limb length changes without dislocating and altering the femoral head. Finally, the cadaver experiment yielded eight sets of alignments by changing the trial neck length. As shown in Table 3, the proposed method can reach a mean absolute error (MAE) of about 0.4 mm, which is much smaller than directly using one EM reading."
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,4,Conclusion,"This paper presents an efficient closed-form solution based on EM sensing to robustly and accurately calculate the intraoperative change in leg length and offset. Simulations, phantom experiments, and cadaver tests demonstrate the efficiency and accuracy of our proposed algorithm compared to the conventional manual gauge method and standard optical tracking based method, showing the potential value in clinical practice. The reasons why the proposed solution can significantly improve the accuracy are: (i) it uses a set of sampled poses from EM readings to optimise the intraoperative limb length change instead of only using one sensor reading; (ii) there is no requirement of repositioning the femur to the neutral position before the measurements. The computational time of our method is only around 0.3 ms mainly due to the closed-form solution. Some studies assessed that metals in the surgical environment might affect the accuracy of EM tracking [18]. However, the design of EM is not affected by titanium and 300 series stainless steel, which are the main materials of surgical instruments used in THA, and our cadaver experiments in a surgical environment have shown that our method still guarantees measurement accuracy (the mean absolute error is around 0.4 mm). In the future, we aim to further validate our approach through clinical trials and have plans to extend the EM-based intraoperative limb length measurement to total knee arthroplasty surgery."
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Fig. 1 .,
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Fig. 2 .,
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Fig. 3 .,
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Fig. 4 .,
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Fig. 5 .,
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Table 1 .,"Noises: {Rot,"
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Table 2 .,
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Table 3 .,
A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 35.
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,1,Introduction,"Cancer remains a significant public health challenge worldwide, with a new diagnosis occurring every two minutes in the UK (Cancer Research UK 1 ). Surgery is one of the main curative treatment options for cancer. However, despite substantial advances in pre-operative imaging such as CT, MRI, or PET/SPECT to aid diagnosis, surgeons still rely on the sense of touch and naked eye to detect cancerous tissues and disease metastases intra-operatively due to the lack of reliable intraoperative visualization tools. In practice, imprecise intraoperative cancer tissue detection and visualization results in missed cancer or the unnecessary removal of healthy tissues, which leads to increased costs and potential harm to the patient. There is a pressing need for more reliable and accurate intraoperative visualization tools for minimally invasive surgery (MIS) to improve surgical outcomes and enhance patient care. A recent miniaturized cancer detection probe (i.e., 'SENSEI R ' developed by Lightpoint Medical Ltd.) leverages the cancer-targeting ability of nuclear agents typically used in nuclear imaging to more accurately identify cancer intraoperatively from the emitted gamma signal (see Fig. 1b) [6]. However, the use of this probe presents a visualization challenge as the probe is non-imaging and is air-gapped from the tissue, making it challenging for the surgeon to locate the probe-sensing area on the tissue surface.It is crucial to accurately determine the sensing area, with positive signal potentially indicating cancer or affected lymph nodes. Geometrically, the sensing area is defined as the intersection point between the gamma probe axis and the tissue surface in 3D space, but projected onto the 2D laparoscopic image. However, it is not trivial to determine this using traditional methods due to poor textural definition of tissues and lack of per-pixel ground truth depth data. Similarly, it is also challenging to acquire the probe pose during the surgery.Problem Redefinition. In this study, in order to provide sensing area visualization ground truth, we modified a non-functional 'SENSEI' probe by adding a miniaturized laser module to clearly optically indicate the sensing area on the laparoscopic images -i.e. the 'probe axis-surface intersection'. Our system consists of four main components: a customized stereo laparoscope system for capturing stereo images, a rotation stage for automatic phantom movement, a shutter for illumination control, and a DAQ-controlled switchable laser module (see Fig. 1a). With this setup, we aim to transform the sensing area localization problem from a geometrical issue to a high-level content inference problem in 2D. It is noteworthy that this remains a challenging task, as ultimately we need to infer the probe axis-surface intersection without the aid of the laser module to realistically simulate the use of the 'SENSEI' probe."
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,2,Related Work,"Laparoscopic images play an important role in computer-assisted surgery and have been used in several problems such as object detection [9], image segmentation [23], depth estimation [20] or 3D reconstruction [13]. Recently, supervised or unsupervised depth estimation methods have been introduced [14]. Ye et al. [22] proposed a deep learning framework for surgical scene depth estimation in self-supervised mode and achieved scalable data acquisition by incorporating a differentiable spatial transformer and an autoencoder into their framework. A 3D displacement module was explored in [21] and 3D geometric consistency was utilized in [8] for self-supervised monocular depth estimation. Tao et al. [19] presented a spatiotemporal vision transformer-based method and a selfsupervised generative adversarial network was introduced in [7] for depth estimation of stereo laparoscopic images. Recently, fully supervised methods were summarized in [1] for depth estimation. However, acquiring per-pixel ground truth depth data is challenging, especially for laparoscopic images, which makes it difficult for large-scale supervised training [8].Laparoscopic segmentation is another important task in computer-assisted surgery as it allows for accurate and efficient identification of instrument position, anatomical structures, and pathological tissue. For instance, a unified framework for depth estimation and surgical tool segmentation in laparoscopic images was proposed in [5], with simultaneous depth estimation and segmentation map generation. In [12], self-supervised depth estimation was utilized to regularize the semantic segmentation in knee arthroscopy. Marullo et al. [16] introduced a multi-task convolutional neural network for event detection and semantic segmentation in laparoscopic surgery. The dual swin transformer U-Net was proposed in [11] to enhance the medical image segmentation performance, which leveraged the hierarchical swin transformer into both the encoder and the decoder of the standard U-shaped architecture, benefiting from the self-attention computation in swin transformer as well as the dual-scale encoding design.Although the intermediate depth information was not our final aim and can be bypassed, the 3D surface information was necessary in the intersection point inference. ResNet [3] has been commonly used as the encoder to extract the image features and geometric information of the scene. In particular, in [21], concatenated stereo image pairs were used as inputs to achieve better results, and such stereo image types are also typical in robot-assisted minimally invasive surgery with stereo laparoscopes. Hence, stereo image data was also adopted in this paper.If the problem of inferring the intersection point is treated as a geometric problem, both data collection and intra-operative registration would be difficult, which inspired us to approach this problem differently. In practice, we utilize the laser module to collect the ground truth of the intersection points when the laser is on. We note that the standard illumination image from the laparoscopic probe is also captured with the same setup when the laser module is on. Therefore, we can establish a dataset with an image pair (RGB image and laser image) that shares the same intersection point ground truth with the laser image (see Fig. 2a and Fig. 2b). The assumptions made are that the probe's 3D pose when projected into the two 2D images is the observed 2D pose, and that the intersection point is located on its axis. Hence, we input these axes to the network as another branch and randomly sampled points along them to represent the probe."
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,3,Dataset,"To validate our proposed solution for the newly formulated problem, we acquired and publicly released two new datasets. In this section, we introduce the hardware and software design that was used to achieve our final goal, while Fig. 2 shows a sample from our dataset. Data Collection. Two miniaturized, high-resolution cameras were coupled onto a stereo laparoscope using a custom-designed connector. The accompanying API allowed for automatic image acquisition, exposure time adjustment, and white balancing. An electrically controllable shutter was incorporated into the standard laparoscopic illumination path. To indicate the probe axis-surface intersection, we incorporated a DAQ controlled cylindrical miniature laser module into a 'SENSEI' probe shell so that the adapted tool was visually identical to the real probe. The laser module emitted a red laser beam (wavelength 650 nm) that was visible as a red spot on the tissue surface. We acquired the dataset on a silicone tissue phantom which was 30 × 21 × 8 cm and was rendered with tissue color manually by hand to be visually realistic. The phantom was placed on a rotation stage that stepped 10 times per revolution to provide views separated by a 36-degree angle. At each position, stereo RGB images were captured i) under normal laparoscopic illumination with the laser off; ii) with the laparoscopic light blocked and the laser on; and iii) with the laparoscopic light blocked and the laser off. Subtraction of the images with laser on and off readily allowed segmentation of the laser area and calculation of its central point, i.e. the ground truth probe axis-surface intersection.All data acquisition and devices were controlled by Python and LABVIEW programs, and complete data sets of the above images were collected on visually realistic phantoms for multiple probe and laparoscope positions. This provided 10 tissue surface profiles for a specific camera-probe pose, repeated for 120 different camera-probe poses, mimicking how the probe may be used in practice. Therefore, our first newly acquired dataset, named Jerry, contains 1200 sets of images. Since it is important to report errors in 3D and in millimeters, we recorded another dataset similar to Jerry but also including ground truth depth map for all frames by using structured-lighting system [8]-namely the Coffbee dataset.These datasets have multiple uses such as:-Intersection point detection: detecting intersection points is an important problem that can bring accurate surgical cancer visualization. We believe this is an under-investigated problem in surgical vision. -Depth estimation: corresponding ground truth will be released.-Tool segmentation: corresponding ground truth will be released."
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,4,Probe Axis-Surface Intersection Detection,
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,4.1,Overview,"The problem of detecting the intersection point is trivial when the laser is on and can be solved by training a deep segmentation network. However, segmentation requires images with a laser spot as input, while the real gamma probe produces no visible mark and therefore this approach produces inferior results.An alternative approach to detect the intersection point is to reconstruct the 3D tissue surface and estimate the pose of the probe in real time. A tracking and pose estimation method for the gamma probe [6] involved attaching a dualpattern marker to the probe to improve detection accuracy. This enabled the derivation of a 6D pose, comprising a rotation matrix and translation matrix with respect to the laparoscope camera coordinate. To obtain the intersection point, the authors used the Structure From Motion (SFM) method to compute the 3D tissue surface, combining it with the estimated pose of the probe, all within the laparoscope coordinate system. However, marker-based tracking and pose estimation methods have sterilization implications for the instrument, and the SFM method requires the surgeon to constantly move the laparoscope, reducing the practicality of these methods for surgery.In this work, we propose a simple, yet effective regression approach to address this problem. Our approach relies solely on the 2D information and works well without the need for the laser module after training. Furthermore, this simple methodology facilitated an average inference time of 50 frames per second, enabling real-time sensing area map generation for intraoperative surgery. "
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,4.2,Intersection Detection as Segmentation,"We utilized different deep segmentation networks as a first attempt to address our problem [10,18]. Please refer to the Supplementary Material for the implementation details of the networks. We observed that when we do not use images with the laser, the network was not able to make any good predictions. This is understandable as the red laser spot provides the key information for the segmentation. Therefore the network does not have any visual information to make predictions from images of the gamma probe. We note that to enable real-world applications, we need to estimate the intersection point using the images when the laser module is turned off."
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,4.3,Intersection Detection as Regression,"Problem Formulation. Formally, given a pair of stereo images I l , I r , n points {P l 1 , P l 2 , ..., P l n } were sampled along the principal axis of the probe, P l i ∈ R 2 from the left image. The same process was repeated for the right image. The goal was to predict the intersection point P intersect on the surface of the tissue. During the training, the ground truth intersection point position was provided by the laser source, while during testing the intersection was estimated solely based on visual information without laser guidance (see Fig. 3).Network Architecture. Unlike the segmentation approach, the intersection point was directly predicted using a regression network. The images fed to the network were 'laser off' stereo RGB, but crucially, the intersection point for these images was known a priori from the paired 'laser on' images. The raw image resolution was 4896×3680 but these were binned to 896×896. Principal Component Analysis (PCA) [15] was used to extract the central axis of the probe and 50 points were sampled along this axis as an extra input dimension. A network was designed with two branches, one branch for extracting visual features from the image and one branch for learning the features from the sequence of principal points using ResNet [3] and Vision Transformer (ViT) [2] as two backbones. The principal points were learned through a multi-layer perception (MLP) or a long short-term memory (LSTM) network [4]. The features from both branches were concatenated and used for regressing the intersection point (see Fig. 4). Finally, the whole network is trained end-to-end using the mean square error loss."
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,4.4,Implementation,"Evaluation Metrics. To evaluate sensing area location errors, Euclidean distance was adopted to measure the error between the predicted intersection points and the ground truth laser points. We reported the mean absolute error, the standard derivation, and the median in pixel units.Implementation Details. The networks were implemented in PyTorch [17], with an input resolution of 896 × 896 and a batch size of 12. We partitioned the Jerry dataset into three subsets, the training, validation, and test set, consisting of 800, 200, and 200 images, respectively, and the same for the Coffbee dataset. The learning rate was set to 10 -5 for the first 300 epochs, then halved until epoch 400, and quartered until the end of the training. The model was trained for 700 epochs using the Adam optimizer on two NVIDIA 2080 Ti GPUs, taking approximately 4 h to complete. "
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,5,Results,"Quantitative results on the released datasets are shown in Table 1 and Table 2 with different backbones for extracting image features, ResNet and ViT. For the 2D error on two datasets, among the different settings, the combination of ResNet and MLP gave the best performance with a mean error of 70.5 pixels and a standard deviation of 56.8. The median error of this setting was 59.8 pixels while the R2 score was 0.82 (higher is better for R2 score). Comparing the Table 1 and Table 2, we found that the ResNet backbone was better than the ViT backbone in the image processing task, while MLP was better than LSTM in probe pose representation. ResNet processed the input images as a whole, which was better suited for utilizing the global context of a unified scene composed of the tissue and the probe, compared to the ViT scheme, which treated the whole scene as several patches. Similarly, the sampled 50 principal points on the probe axis were better processed using the simple MLP rather than using a recurrent procedure LSTM. It is worth noting that the results from stereo inputs exceeded those from mono inputs, which can be attributed to the essential 3D information included in the stereo image pairs.For the 3D error, the ResNet backbone still gave generally better performance than the ViT backbone while under the ResNet backbone, LSTM and MLP gave competitive results and they are all in sub-milimeter level. We note that the 3D error subjected to the quality of the acquired ground truth depth maps, which had limited resolution and non-uniformly distributed valid data due to hardware constraints. Hence, we used the median depth value of a square area of 5 pixels around the points where depth value was not available.Figure 5 shows visualization results of our method using ResNet and MLP. This figure illustrates that our proposed method successfully detected the intersection point using solely standard RGB laparoscopic images as the input. Furthermore, based on the simple design, our method achieved the inference time of 50 frames per second, making it well-suitable for intraoperative surgery."
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,6,Conclusion,"In this work, a new framework for using a laparoscopic drop-in gamma detector in manual or robotic-assisted minimally invasive cancer surgery was presented, where a laser module mock probe was utilized to provide training guidance and the problem of detecting the probe axis-tissue intersection point was transformed to laser point position inference. Both the hardware and software design of the proposed solution were illustrated and two newly acquired datasets were publicly released. Extensive experiments were conducted on various backbones and the best results were achieved using a simple network design, enabling real time inference of the sensing area. We believe that our problem reformulation and dataset release, together with the initial experimental results, will establish a new benchmark for the surgical vision community."
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,,Fig. 1 .,
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,,Fig. 2 .,
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,,Fig. 3 .Fig. 4 .,
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,,Fig. 5 .,
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,,Table 1 .,
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,,Table 2 .,
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 25.
From Mesh Completion to AI Designed Crown,1,Introduction,"If a tooth is missing, decayed, or fractured, its treatment may require a dental crown. Each crown must be customized to the individual patient in a process, as depicted in Fig. 1. The manual design of these crowns is a time-consuming and labor-intensive task, even with the aid of computer-assisted design software.Designing natural grooves and ensuring proper contact points with the opposing jaw present significant challenges, often requiring technicians to rely on trial and error. As such, an automated approach capable of accelerating this process and generating crowns with comparable morphology and quality to that of a human expert would be a groundbreaking advancement for the dental industry. A limited number of studies have focused on how to automate dental crown designs. In [2,3], a conditional generative adversarial based network (GAN) is applied to a 2D depth image obtained from a 3D scan to generate a crown for a prepared tooth. Depth images created from a 3D scan can be used directly with 2D convolutional neural networks (CNNs) such as pix2pix [4], which are wellestablished in computer vision. However, depth images are limited in their ability to capture fine-grained details and can suffer from noise and occlusion issues. By contrast, point clouds have the advantage of being able to represent arbitrary 3D shapes and can capture fine-grained details such as surface textures and curvatures. [5,8] use point cloud-based networks to generate crowns in the form of 3D point clouds. Input point clouds used by [5] are generated by randomly removing a tooth from a given jaw; then the network estimates the missing tooth by utilizing a feature points-based multi-scale generating network. [8] propose a more realistic setting by generating a crown for a prepared tooth instead of a missing tooth. They also incorporate margin line information extracted from the prepared tooth in their network to have a more accurate prediction in the margin line area. The crowns generated by both approaches are represented as point clouds, so another procedure must convert these point clouds into meshes. Creating a high-quality mesh that accurately represents the underlying point cloud data is a challenging task which is not addressed by these two works. [6] proposed a transformer-based network to generate a surface mesh of the crown for a missing tooth. They use two separate networks, one responsible for generating a point cloud and the other for reconstructing a mesh given the crown generated by the first network. Similar to [5,8], the point completion network used by [6] only uses the Chamfer Distance (CD) loss to learn crown features. This metric's ability to capture shape details in point clouds is limited by the complexity and density of the data.Although all aforementioned methods are potentially applicable to the task of dental crown design, most of them fail to generate noise-free point clouds, which is critical for surface reconstruction. One way to alleviate this problem is to directly generate a crown mesh. In [28], the authors develop a deep learningbased network that directly generates personalized cardiac meshes from sparse contours by iteratively deforming a template mesh, mimicking the traditional 3D shape reconstruction method. To our knowledge, however, the approach in [28] has not been applied to 3D dental scans.In this paper, we introduce Dental Mesh Completion (DMC), a novel endto-end network for directly generating dental crowns without using generic templates. The network employs a transformer-based architecture with self-attention to predict features from a 3D scan of dental preparation and surrounding teeth. These features deform a 2D fixed grid into a 3D point cloud, and normals are computed using a simple MLP. A differentiable point-to-mesh module reconstructs the 3D surface. The process is supervised using an indicator grid function and Chamfer loss from the target crown mesh and point cloud. Extensive experiments validate the effectiveness of our approach, showing superior performance compared to existing methods as measured by the CD metric. In summary, our main contributions include proposing the first end-to-end network capable of generating crown meshes for all tooth positions, employing a non-templatebased method for mesh deformation (unlike previous works), and showcasing the advantages of using a differentiable point-to-mesh component to achieve highquality surface meshes."
From Mesh Completion to AI Designed Crown,2,Related Work,"In the field of 3D computer vision, completing missing regions of point clouds or meshes is a crucial task for many applications. Various methods have been proposed to tackle this problem. Since the introduction of PointNet [13,14], numerous methods have been developed for point cloud completion [12]. The recent works PoinTr [11] and SnowflakeNet [16] leverage a transformer-based architecture with geometry-aware blocks to generate point clouds. It is hypothesized that using transformers preserves detailed information for point cloud completion. Nonetheless, the predicted point clouds lack connections between points, which complicates the creation of a smooth surface for mesh reconstruction.Mesh completion methods are usually useful when there are small missing regions or large occlusions in the original mesh data. Common approaches based on geometric priors, self-similarity, or patch encoding can be used to fill small missing regions, as demonstrated in previous studies [26,27], but are not suitable for large occlusions. [18] propose a model-based approach that can capture the variability of a particular shape category and enable the completion of large missing regions. However, the resulting meshes cannot achieve the necessary precision required by applications such as dental crown generation. Having a mesh prior template can also be a solution to generate a complete mesh given a sparse point cloud or a mesh with missing parts. In [28], cardiac meshes are reconstructed from sparse point clouds using several mesh deformation blocks. Their network can directly generate 3D meshes by deforming a template mesh under the guidance of learned features.We combine the advantages of point cloud completion techniques with a differentiable surface reconstruction method to generate a dental mesh. Moreover, we used the approach in [28] to directly produce meshes from 3D dental points and compared those results with our proposed method."
From Mesh Completion to AI Designed Crown,3,Method,
From Mesh Completion to AI Designed Crown,3.1,Network Architecture,"Our method is an end-to-end supervised framework to generate a crown mesh conditioned on a point cloud context. The overview of our network is illustrated in Fig. 2. The network is characterized by two main components: a transformer encoder-decoder architecture and a mesh completion layer. The following sections explain each part of the network. Transformer Encoder-Decoder. We adapt the transformer encoder-decoder architecture from [11] to extract global and local 3D features from our input (context) using the encoder and generate crown points via the decoder. A dynamic graph convolution network (DGCNN) [29] is used to group the input points into a smaller set of feature vectors that represent local regions in the context. The generated feature vectors are then fed into the encoder with a geometry-aware block. This block is used to model the local geometric relationships explicitly. The self-attention layer in the encoder updates the feature vectors using both long-range and short-range information. The feature vectors are further updated by a multi-layer perceptron. The decoder's role is to reason about the crown based on the learnable pairwise interactions between features of the input context and the encoder output. The decoder incorporates a series of transformer layers with a self-attention and cross-attention mechanisms to learn structural knowledge. The output of the transformer decoder is fed into a folding-based decoder [15] to deform a canonical 2D grid onto the underlying 3D surface of the crown points.Mesh Completion Layer. In this stage, to directly reconstruct the mesh from the crown points, we use a differentiable Poisson surface reconstruction (DPSR) method introduced by [17]. We reconstruct a 3D surface as the zero level set of an indicator function. The latter consists in a regular 3D point grid associated with values indicating whether a point is inside the underlying shape or not. To compute this function, We first densify the input unoriented crown points. This is done by predicting additional points and normals for each input point by means of an MLP network. After upsampling the point cloud and predicting normals, the network solves a Poisson partial differential equation (PDE) to recover the indicator function from the densified oriented point cloud. We represent the indicator function as a discrete Fourier basis on a dense grid (of resolution 128 3 ) and solve the Poisson equation (PDE) with the spectral solver method in [17].During training, we obtain the estimated indicator grid from the predicted point cloud by using the differentiable Poisson solver. We similarly acquire the ground truth indicator grid on a dense point cloud sampled from the ground truth mesh, together with the corresponding normals. The entire pipeline is differentiable, which enables the updating of various elements such as point offsets, oriented normals, and network parameters during the training process. At inference time, we leverage our trained model to predict normals and offsets using Differentiable Poisson Surface Reconstruction (DPSR) [17], solve for the indicator grid, and finally apply the Marching Cubes algorithm [22] to extract the final mesh.Loss Function. We use the mean Chamfer Distance (CD) [20] to constrain point locations. The CD measures the mean squared distance between two point clouds S 1 and S 2 . Individual distances are measured between each point and its closest point in the other point set, as described in Eq. (1). In addition, we minimize the L 2 distance between the predicted indicator function x and a ground truth indicator function x gt , each obtained by solving a Poisson PDE [17] on a dense set of points and normals. We can express the Mean Square Error (MSE) loss as Eq. ( 2), where f θ (X) represents a neural network (MLP) with parameters θ conditioned on the input point cloud X, D is the training data distribution, along with indicator functions x i and point samples X i on the surface of shapes. The sum of the CD and MSE losses is used to train the overall network.(1)4 Experimental Results"
From Mesh Completion to AI Designed Crown,4.1,Dataset and Preprocessing,"Our dataset consisted of 388 training, 97 validation, and 71 test cases, which included teeth in various positions in the jaw such as molars, canines, and incisors. The first step in the preprocessing was to generate a context from a given 3D scan. To determine the context for a specific prepared tooth, we employed a pre-trained semantic segmentation model [21] to separate the 3D scan into 14 classes representing the tooth positions in each jaw. From the segmentations, we extracted the two adjacent and three opposing teeth of a given prepared tooth, as well as the surrounding gum tissue. To enhance the training data, we conducted data augmentation on the entire dental context, which included the master arch, opposing arch, and shell, treated as a single entity. Data augmentation involved applying 3D translation, scaling, and rotation, thereby increasing the training set by a factor of 10. To enrich our training set, we randomly sampled 10,240 cells from each context to form the input during training. We provide two types of ground truth: mesh and point cloud crowns. To supervise network training using the ground truth meshes, we calculate the gradient from a loss on an intermediate indicator grid. We use the spectral method from [17] to compute the indicator grid for our ground truth mesh."
From Mesh Completion to AI Designed Crown,4.2,Implementation Details,"We adapted the architecture of [11] for our transformer encoder-decoder module. For mesh reconstruction, we used Differentiable Poisson Surface Reconstruction (DPSR) from [17]. All models were implemented in PyTorch with the AdamW optimizer [23], using a learning rate of 5e-4 and a batch size of 16. Training the model took 400 epochs and 22 h on an NVIDIA A100 GPU."
From Mesh Completion to AI Designed Crown,4.3,Evaluation and Metrics,"To evaluate the performance of our network and compare it with point cloudbased approaches, we used the Chamfer distance to measure the dissimilarity between the predicted and ground truth points. We employed two versions of CD: CD L1 uses the L 1 -norm, while CD L2 uses the L 2 -norm to calculate the distance between two sets of points. Additionally, we used the F-score [24] with a distance threshold of 0.3, chosen based on the distance between the predicted and ground truth point clouds. We also used the Mean Square Error (MSE) loss [17] to calculate the similarity between the predicted and ground truth indicator grids or meshes.We conducted experiments to compare our approach with two distinct approaches from the literature, as shown in Table 1. The first such approach, PoinTr+margin line [8], uses the PoinTr [11] point completion method as a baseline and introduces margin line information to their network. To compare our work to [8], we used its official implementation provided by the author. In the second experiment, PoinTr+graph, we modified the work of [28] to generate a dental crown mesh. To this end, we use deformation blocks in [28] to deform a generic template mesh to output a crown mesh under the guidance of the learned features from PoinTr. The deformation module included three Graph Convolutional Networks (GCNs) as in [9]. All experiments used the same dataset, which included all tooth positions, and were trained using the same methodology. To compare the results of the different experiments, we extracted points from the predicted meshes of our proposed network (DMC), as illustrated in Fig. 3. Table 1 shows that DMC outperforms the two other networks in terms of both CD and F-score. PoinTr+graph achieves poor CD and F-score results compared to the other methods. While the idea of using graph convolutions seems interesting, features extracted from the point cloud completion network don't carry enough information to deform the template into an adequate final crown. Therefore, these methods are highly biased toward the template shape and need extensive pre-processing steps to scale and localize the template. In the initial two experiments, the MSE metric was not applicable as it was calculated on the output meshes. Figure 4 showcases the visual results obtained from our proposed network (DMC). Furthermore, Fig. 5 presents a visual comparison of mesh surfaces generated by various methods for a sample molar tooth.    [11] for point cloud and Shape as points [17] for mesh; c) Proposed method (DMC); d) Ground truth shape. "
From Mesh Completion to AI Designed Crown,4.4,Ablation Study,"We conducted an ablation study to evaluate the components of our architecture. We started with the baseline PoinTr [8], a point completion method. To enhance it, we integrated Shape as Points (SAP) [17] as a separate network for mesh reconstruction from the PoinTr-generated point cloud. Next, we tested our proposed method (DMC) by excluding the Mean Square Error (MSE) loss function. Finally, we assessed DMC's performance, including the MSE loss function. The results, shown in Table 2, demonstrate the consistent improvements achieved by our full model across all evaluation metrics."
From Mesh Completion to AI Designed Crown,5,Conclusion,"Existing deep learning-based dental crown design solutions require additional steps to reconstruct a surface mesh from the generated point cloud. In this study, we propose a new end-to-end approach that directly generates high-quality crown meshes for all tooth positions. By utilizing transformers and a differentiable Poisson surface reconstruction solver, we effectively reason about the crown points and convert them into mesh surfaces using Marching Cubes. Our experimental results demonstrate that our approach produces accurately fitting crown meshes with superior performance. In the future, incorporating statistical features into our deep learning method for chewing functionality, such as surface contacts with adjacent and opposing teeth, could be an interesting avenue to explore."
From Mesh Completion to AI Designed Crown,,Fig. 1 .,
From Mesh Completion to AI Designed Crown,,Fig. 2 .,
From Mesh Completion to AI Designed Crown,,Fig. 3 .,
From Mesh Completion to AI Designed Crown,,Fig. 4 .,
From Mesh Completion to AI Designed Crown,,Fig. 5 .,
From Mesh Completion to AI Designed Crown,,Table 1 .,
From Mesh Completion to AI Designed Crown,,Table 2 .,MethodCD-L1 (↓) CD-L2 (↓) MSE (↓) F 1 0.3 (↑)
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,1,Introduction,"Thoracoscopy-assisted mitral valve replacement (MVR) has become routine for the treatment of mitral valve regurgitation [4]. Compared to other surgeries such as laparoscopic operations, thoracoscopy-assisted MVR requires higher surgical skills due to the intricate structure of the heart, the mitral valve's proximity to other vital cardiac structures, and the geometric limitations of the surgical field [11]. Improving surgical skills can prevent avoidable complications [9], leading to better patient outcomes, such as improved long-term survival and reduced postoperative complications [2,3]. Therefore, surgical skill assessment (SKA), i.e., evaluating the skill level of surgeons, is essential in the training and certification of novice surgeons [19,20,26].Traditionally, SKA has been reliant on manual observation by experienced surgeons either in the operating room or via recorded videos, as described by Reznick in his work on teaching [19]. However, this method is subjective, timeconsuming, and not very efficient for use in surgical education. To address these limitations, researchers have increasingly focused on developing automatic SKA tools. While current automatic SKA approaches [10,12,16,17,22] have demonstrated success on simulated and laparoscopic datasets, their application to thoracoscopy-assisted MVR poses several challenges. First, to the best of our knowledge, there are no publicly available clinical datasets for thoracoscopyassisted surgery. Second, most existing methods [5][6][7]10,12,16,17] focus solely on the global information within a single video to perform SKA, such as regressing a singular skill score from the video. However, these methods disregard the inter-video information, such as subtle differences between various videos, that could be critical in predicting surgical skill scores [13]. For instance, differences in haemorrhage loss, suture repairing times, and thread twining times among videos can have a significant impact on the final scores. Generally, more thread winding, haemorrhage, and suture repairing can indicate a lower skill level; see Fig. 1(a).To address the above challenges, we collect a new dataset for SKA, which is the first-ever long-form thoracoscopy-assisted MVR video dataset. Our dataset offers longer video duration and more surgical events with corresponding labels in comparison to the currently available public datasets such as JIGSAWS [8] or HeiChole [24]; see Fig. 1(b). Then, we present a novel Surgical Events Driven Skill assessment (SEDSkill) method to address the limitations of current automatic methods for MVR assessment. Unlike prior work [10,12,17], our key idea is to develop a long-form surgical events-driven method for skill assessment, which is based on the crucial insight that the skill level of a surgeon is closely tied to the occurrence of inappropriate operations such as excessively long suture repairing times. To achieve it, we propose a novel local-global difference method that can learn inter-video relations between both the global long-form and local surgical events correlated semantics. The method includes an event-aware module and a difference regression module. The event-aware module can automatically localize skill-related surgical events and extract their corresponding features to represent the local event semantics. As surgical skill is highly correlated with the occurrence of inappropriate events, this module is crucial for precise SKA. To enable the accurate detection of slight differences between videos, our difference regression module captures the relationships among videos and enhances the model's ability to detect subtle variations. By incorporating video-wise and event-wise difference learning, our framework can capture both local and global inter-video relations, thereby enabling precise SKA.In summary, our contributions are three-fold: (1) We introduce a novel SED-Skill method that aims to design a long-form, surgical events-driven approach for SKA, and it is the first method designed specifically for SKA in thoracoscopic surgical videos. (2) We propose a local-global difference framework that can learn inter-video relations between both the global long-form and local surgical events correlated semantics, thereby enabling enhanced SKA performance. (3) Experimental results demonstrate that our method outperforms existing SKA methods, as well as methods designed for video quality assessment in computer vision. This indicates the great potential of our method for use in clinical practice. Our code will be publicly released upon paper acceptance."
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,2,Method,"Figure 2 illustrates our SEDSkill framework for SKA, which takes a surgical video as input and regresses a surgical skill score. Our proposed framework consists of two main modules: (a) a basic regression module to output the skill score for each input, (b) a local-global difference module to learn both video-level and event-level inter-video differences for precise assessment. "
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,2.1,Basic Regression Module,"The basic regression module aims to regress a surgical skill score, i.e., Y i , for a raw surgical video input V i ∈ R Ti×H×W , where T i is the duration, H and W are the height and width of each frame. As shown in Fig. 2, the basic regression module consists of three components: a spatial feature extractor, a temporal feature extractor, and a regression block. Specifically, the video V i is first fed into the spatial feature extractor to obtain the spatial feature, denoted as F S i ∈ R Ti×Ds , where D s is the dimension of features, followed by a temporal feature extractor to model the intra-video relations to generate the global video featureFinally, a regression block consisting of several convolutional, max-pooling layers and a fully connected layer is applied to map F G i to the skill score Ŷi . Then, the loss function is to minimize the differences between predicted Ŷi and the ground-truth Y i as follows:where N is the number of videos."
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,2.2,Local-Global Difference Module,"Surgical Event-Aware Module for Local Information. Unlike prior datasets used for skill assessment, our MVR video dataset is much longer, ranging from 30 min to 1 h, and consists of multiple skill-related events such as thread twining, haemorrhage, and suture repairing; see Fig. 1. As surgical skill is highly correlated to the qualities of surgical events, directly using the basic regression module to predict a score from the long video would consider too many irrelevant parts, thus degrading the regression performance. To encourage the model to focus on the skill-related parts and remove the less informative ones, we devise an event-aware module to localize the skill-related events, i.e., haemorrhage, surgical thread twining and surgical suture repair, from the long videos and extract the local event-level features, as shown in Fig. 2(b). Specifically, given the spatial feature, e.g., F S i , we introduce an event detector, which is a transformer-like network that maps the video features to the classified logits and regressed start/end time. The detailed architecture can refer to [28]. Formally, the prediction of the detector is a set for each time t which can be formulated as:where p t ∈ R 4 consists of 4 values (including the background), which indicates the probability of event category, d s t > 0 and d e t > 0 denote the distance between time t to start and end time of events. Note that if p t equals to zero, d s t > 0 and d e t > 0 are not defined. Then, following [28] the loss function for the detector is defined as:where N + is the number of positive frames, L cls is a focal loss [15] and L loc is a DIoU loss [29]. 1 ct is the indicator function to identify where the time t is within a event. λ loc is set to 1 following [28]. Note that the detector is pre-trained and fixed during the training of the basic regression module and the local-global difference module. After obtaining the pre-trained detector, we generate the event confidence map for each video denoted by, where A i ∈ R Ti×1 , a t = max p t is the confidence for each time t and T i is the duration for the V i . Then, the local event-level feature is obtained by the multiplication of the confidence values and the global video feature, i.e., F L i = A i • F S i , where F L i ∈ R Ti×Ds and • is the element-wise multiplication. Local-Global Fusion. We introduce the local-global fusion module to aggregate the local (i.e.event-level) and global (long-form video) semantics. Formally, we can define the local-global fusion as, where F i ∈ R Ti×(Dt+Ds) . This module can be implemented by different types and we will conduct an ablation study to analyze the effect of this module in Table 3."
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,,Difference Regression Block.,"Most surgeries of the same type are performed in similar scenes, leading to subtle differences among surgical videos. For example, in MVR, the surgeon first stitches two lines on one side using a needle, and then passes one of the lines through to the other side, connecting it to the extracorporeal circulation tube. Although these procedures are performed in a similar way, the imperceptible discrepancies are very important for accurately assessing surgical skills. Hence, we first leverage the relation block to capture the inter-video semantics. We use the features of the pairwise videos, i.e., F i and F j , for clarity. Since attention [5,23] is widely used for capturing relations, we formulate the detailed relation block in the attention manner as follows:wherelinear layers, √ D controls the effect of growing magnitude of dot-product with larger D [23]. Since F j→i only learn the attentive relation from F j to F i . We then learn the bi-direction attentive relation byAfter that, we use the difference regression block to map F i-j to the difference scores Δ Ŷ . Then, we minimize the error as follows:where ΔY is the ground-truth of the difference scores between the pair videos, which can be computed by |Y i -Y j |. By optimizing L dif f , the model would be able to distinguish differences between videos for precise SKA. Finally, the overall loss function of our proposed method is as follows:where λ dif f is the hyper-parameter to control the weight between two loss functions (set to 1 empirically)."
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,3,Experiments,"Datasets. We collect the data from our collaborating hospitals. The data collection process follows the same protocol in a well-established study [2]. The whole procedure of the surgery is recorded by a surgeon's view camera. Each surgeon will submit videotapes when performing thoracoscopy-assisted MVR in the operating rooms. We have collected 50 high-resolution videos of thoracoscopy-assisted MVR from surgeons and patients, with a resolution of 1920×1080 and 25 frames per second. Each collected video lasts 30 min -1 h. 50 videos are randomly divided into training and testing subsets containing 38, and 12 videos, respectively. To evaluate skill level, each video will be rated along various dimensions of technical skill on a scale of 1 to 9 (with higher scores indicating more advanced skill) by at least ten authoritative surgeons who are unaware of the identity of the operating surgeon. Furthermore, we also provide the annotations (including the category and corresponding start and end time) for three skill-related events, i.e., haemorrhage, surgical thread twining and surgical suture repair times. The detailed annotation examples are illustrated in Fig. 1(a).Implementation Details. Our model is implemented on an NVIDIA GeForce RTX 3090 GPU. We use a pre-trained inception-v3 [21] and MS-TCN [5] as the spatial and temporal feature extractors, respectively. For each video, we sample one frame per second. As the durations of different videos vary, we resample all videos to 1000 frames. We trained our model using an Adam optimizer with learning rates initialized at 1e -3. The total number of epochs is 200, and the batch size is 4. "
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,3.1,Comparison with the State-of-the-Art Methods,"We compare our method with existing state-of-the-art methods in action quality assessment (AQA) [1,27] and surgical skill assessment (SKA) [14,18]. Note that the spatial and temporal feature extractors for ViSA, CoRe and TPT as the same as our method. As shown in Table 1, our method achieved the best performance with an MAE score of 1.83 and a Corr score of 0.54. The comparison demonstrates that our method not only outperformed existing SKA methods but also outshined existing AQA methods by a clear margin in surgical skill assessment."
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,3.2,Ablation Study,"Effectiveness of Proposed Modules.  Bidirectional (Fi-j) 1.83 0.54 Fig. 3. Qualitative results of sampled pairwise videos. For each video, we visualize its confidence values along video frames for skill-related events. Specifically, the green, orange, and blue lines indicate the confidences scores of thread twining (A0), haemorrhage (A1) and suture repairing (A2) along video frames. It is worth noting that the occurrence of inappropriate surgical events such as haemorrhage and more thread twining times is highly correlated with the surgical skill level. Therefore, a lower confidence value indicates a lower probability of an event occurring, leading to a higher skill score.methods can achieve comparable performance, indicating that different fusion methods can effectively aggregate local and global information. In this paper, we select concatenation as our default fusion method.Effect of the Attention in the Difference Block. In Sect. 2.2, we implement the difference block by the attention, shown in Eq. 4. Here, we conduct the ablation study on the effect of different attentions in Table 4. The results indicate that using bidirectional attention, i.e., F i-j , can achieve better performance, compared with the uidirectional one.Qualitative Results. Figure 3 shows the qualitative results of sampled videos to analyze the effectiveness of our method. The upper video presents a low surgical skill score, i.e., 5.0, while the score for the lower video is higher, i.e., 9.0. By comparing the two videos, the confidence lines generated by our model can find several factors that lower the skill score for the upper video, such as haemorrhage, multiple rewinds, and needle threading. Hence the upper video only obtains a skill score of 5.0, while the lower one achieves the better score, i.e., 8.0."
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,4,Conclusion,"This paper introduces a new surgical video dataset for evaluating thoracoscopyassisted surgical skills. This dataset constitutes the first-ever collection of long surgical videos used for skill assessment from real operating rooms. To address the challenges posed by long-range videos and multiple complex surgical actions in videos, we propose a novel SEDSkill method that incorporates a local-global difference framework. In contrast to current methods that solely rely on intravideo information, our proposed framework leverages local and global difference learning to enhance the model's ability to use inter-video relations for accurate SKA in the MVR scenario."
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,,Fig. 1 .,
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,,Fig. 2 .,
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,,Table 1 .,
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,,Table 2 .,
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,,,"Analysis of Local-Global Fusion.We explore the effect of different types of local-global fusion in Table3. ""Concatenation"" indicates concatenating the two features in the feature dimsension. ""Multiplication"" indicates the element-wise multiplication of the two features. The results show that the different fusion"
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,,Table 3 .,
SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos,,Table 4 .,
Point Cloud Diffusion Models for Automatic Implant Generation,1,Introduction,"The design of 3D-printed patient-specific implants, commonly used in cranioplasty and maxillofacial surgery, is a challenging and time-consuming task that is usually performed manually. To speed up the design process and enable point-ofcare implant generation, approaches for automatically deriving suitable implant designs from medical images are needed. This paper presents a novel approach based on a Denoising Diffusion Probabilistic Model for 3D point clouds that reconstructs complete anatomical structures S c from segmented CT images of subjects showing bone defects S d . An overview of the proposed method is shown in Fig. 1. Since performing the anatomy reconstruction task in a high resolution voxel space would be memory inefficient and computationally expensive, we propose a method that builds upon a sparse surface point cloud representation of the input anatomy. This point cloud c 0 , which can be obtained from the defective segmentation mask S d , serves as input to a Denoising Diffusion Probabilistic Model [37] that, conditioned on this input c 0 , reconstructs the complete anatomy x 0 by generating missing points x0 . The second network transforms the point cloud x 0 back into voxel space using a Differentiable Poisson Solver [23]. The final implant I is generated by the Boolean subtraction of the completed and defective anatomical structure. We thereby ensure a good fit at the junction between implant and skull. Our main contributions are:-We employ 3D point cloud diffusion models for an automatic patient-specific implant generation task. The stochastic sampling process of diffusion models allows for the generation of multiple anatomically reasonable implant designs per subject, from which physicians can choose the most suitable one. -We evaluate our method on the SkullBreak and SkullFix datasets, generating high-quality implants and achieving competitive evaluation scores.Related Work. Previous work on automatic implant generation methods mainly derived from the AutoImplant challenges [11][12][13] at the 2020/21 MICCAI conferences. Most of the proposed methods were based on 2D slice-wise U-Nets [27] and 3D U-Nets on downsampled [8,20,31] or patch-wise data [4,14,22]. Other approaches were based on Statistical Shape Models [34], Generative Adversarial Networks [24] or Variational Autoencoders [30]. This work is based on Diffusion Models [3,28], which achieved good results in 2D reconstruction tasks like image inpainting [17,26] and were also already applied to 3D generative tasks like point cloud generation [18,21,36] and point cloud completion [19,37].For retrieving a dense voxel representation of a point cloud, many approaches rely on a combination of surface meshing [1,2,7] and ray casting algorithms. Since surface meshing of unoriented point clouds with a non-uniform distribution is challenging and ray casting implies additional computational effort, we look at point cloud voxelization based on a Differentiable Poisson Solver [23]."
Point Cloud Diffusion Models for Automatic Implant Generation,2,Methods,"As presented in Fig. 1, we propose a multi-step approach for generating an implant I from a binary voxel representation S d of a defective anatomical structure.Point Cloud Generation. Since the proposed method for shape reconstruction works in the point cloud space, we first need to derive a point cloud c 0 ∈ R N ×3 from S d . We therefore create a surface mesh of S d using Marching Cubes [16].Then we sample N points from this surface mesh using Poisson Disk Sampling [35]. During training, we generate the ground truth point cloud x0 ∈ R M ×3 by sampling M points from the ground truth implant using the same approach.Diffusion Model for Shape Reconstruction. Reconstructing the shape of an anatomical structure can be seen as a conditional generation process. We train a diffusion model θ to reconstruct the point cloud x 0 = (x 0 , c 0 ) that describes the complete anatomical structure S c . The generation process is conditioned on the points c 0 belonging to the known defective anatomical structure S d . An overview is given in Fig. 2. For describing the diffusion model, we follow the formulations in [37]. Starting from x 0 , we first define the forward diffusion process, that gradually adds small amounts of noise to x0 , while keeping c 0 unchanged and thus produces a series of point clouds {x 0 = (x 0 , c 0 ),This conditional forward diffusion process can be modeled as a Markov chain with a defined number of timesteps T and transfers x0 into a noise distribution:Each transition is modeled as a parameterized Gaussian and follows a predefined variance schedule β 1 , ..., β T that controls the diffusion rate of the process:The goal of the diffusion model is to learn the reverse diffusion process that is able to gradually remove noise from xT ∼ N (0, I). This reverse process is also modeled as a Markov chainwith each transition being defined as a Gaussian, with the estimated mean μ θ :As derived in [37], the network θ can be adapted to predict the noise θ (x t , c 0 , t) to be removed from a noisy point cloud xt . During training, we compute xt at a random timestep t ∈ {1, ..., T } and optimize a Mean Squared Error (MSE) losswith ∼ N (0, I), α t = 1β t , and αt = t s=1 α s . To perform shape reconstruction with the trained network, we start with a point cloud x T = (x T , c 0 ) with xT ∼ N (0, I). This point cloud is then passed through the reverse diffusion processwith z ∼ N (0, I), for t = T, ..., 1. While this reverse diffusion process gradually removes noise from xT , the points belonging to the defective anatomical structure c 0 remain unchanged. As proposed in [37], our network θ is based on a PointNet++ [25] architecture with Point-Voxel Convolutions [15]. Details on the network architecture can be found in the supplementary material.Voxelization. To create an implant, the point cloud of the restored anatomy must be converted back to voxel space. We follow a learning-based pipeline proposed in [23]. The pipeline shown in Fig. 3 takes an unoriented point cloud x 0 as input and learns to predict an upsampled, oriented point cloud x with normal vectors n. From this upsampled point cloud, an indicator grid χ can be produced using a Differentiable Poisson Solver (DPSR). The complete voxel representation S c can then be obtained by evaluating the following equation for every voxel position i:During training, the ground truth indicator grid χ can be obtained directly from the ground truth voxel representation S c and, as described in [6], is defined as:Due to the differentiability of the used Poisson solver, the networks can be trained with an MSE loss between the estimated and ground truth indicator grid:For further information on the used network architectures, we refer to [23].Implant Generation. With the predicted complete anatomical structure S c and the defective input structure S d , an implant geometry I can be derived by the Boolean subtraction between S c and S d :To further improve the implant quality and remove noise, we apply a median filter as well as binary opening to the generated implant.Ensembling. As the proposed point cloud diffusion model features a stochastic generation process, we can sample multiple anatomically reasonable implant designs for each defect. This offers physicians the opportunity of selecting from various possible implants and allows the determination of a mean implant from a previously generated ensemble of n different implants. As presented in [32], the ensembling strategy can also be used to create voxel-wise variance over an ensemble. These variance maps highlight areas with high differences between multiple anatomically reasonable implants."
Point Cloud Diffusion Models for Automatic Implant Generation,3,Experiments,"We evaluated our method on the publicly available parts of the SkullBreak and SkullFix datasets. For the point cloud diffusion model we chose a total number of 30 720 points (N = 27 648, M = 3072), set T = 1000, followed a linear variance schedule between β 0 = 10 -4 and β T = 0.02, used the Adam optimizer with a learning rate of 2 × 10 -4 , a batch size of 8, and trained the network for 15 000 epochs. This took about 20 d/4 d for the SkullBreak/SkullFix dataset. For training the voxelization network, we used the Adam optimizer with a learning rate of 5 × 10 -4 , a batch size of 2 and trained the networks for 1300/500 epochs on the SkullBreak/SkullFix dataset. This took about 72 h/5 h. All experiments were performed on an NVIDIA A100 GPU using PyTorch as the framework.SkullBreak/SkullFix. Both datasets, SkullBreak and SkullFix [9], contain binary segmentation masks of head CT images with artificially created skull defects. While the SkullFix dataset mainly features rectangular defect patterns with additional craniotomy drill holes, SkullBreak offers more diverse defect patterns. The SkullFix dataset was resampled to an isotropic voxel size of 0.45 mm, zero padded to a volume size of 512 × 512 × 512, and split into a training set with 75 and a test set with 25 volumes. The SkullBreak dataset already has an isotropic voxel size of 0.4 mm and a volume size of 512 × 512 × 512. We split the SkullBreak dataset into a training set with 430 and a test set with 140 volumes. All point clouds sampled from these datasets were normalized to a range between [-3, 3] in all spatial dimensions. The SkullBreak and SkullFix datasets were both adapted from the publicly available head CT dataset CQ500, which is licensed under a CC-BY-NC-SA 4.0 and End User License Agreement (EULA). The SkullBreak and SkullFix datasets were adapted and published under the same licenses."
Point Cloud Diffusion Models for Automatic Implant Generation,4,Results and Discussion,"For evaluating our approach, we compared it to three methods from AutoImplant 2021 challenge: the winning 3D U-Net based approach [31], a 3D U-Net based approach with sparse convolutions [10] and a slice-wise 2D U-Net approach [33]. We also evaluated the mean implant produced by the proposed ensembling  strategy (n = 5). The implant generation time ranges from ∼1000 s for Skull-Break (n = 1) to ∼1200 s for SkullFix (n = 5), with the diffusion model requiring most of this time. In Table 1, the Dice score (DSC), the 10 mm boundary DSC (bDSC), as well as the 95 percentile Hausdorff Distance (HD95) are presented as mean values over the respective test sets.Qualitative results of the different implant generation methods are shown in Fig. 4 and Fig. 5, as well as in the supplementary material. Implementation detail for the comparing methods, more detailed runtime information, as well as the used code can be found at https://github.com/pfriedri/pcdiff-implant. We outperformed the sparse 3D U-Net, while achieving comparable results to the challenge winning 3D U-Net and the 2D U-Net based approach. By visually comparing the results in Fig. 4, our method produces significantly smoother surfaces that are more similar to the ground truth implant. In Fig. 5, we show that our method reliably reconstructs defects of various sizes, as well as complicated geometric structures. For large defects, however, we achieve lower evaluation scores, which can be explained with multiple anatomically reasonable implant solutions. This was also reported in [31] and [34]. In Fig. 6, we present the proposed ensembling strategy for an exemplary defect. Apart from generating multiple implants, we can compute the mean implant and the variance map. To the best of our knowledge, we are the first to combine such an approach with an automatic implant generation method. Not only do we offer a choice of implants, but we can also provide physicians with information about implant areas with more anatomical variation. "
Point Cloud Diffusion Models for Automatic Implant Generation,5,Conclusion,"We present a novel approach for automatic implant generation based on a combination of a point cloud diffusion model and a voxelization network. Due to the sparse point cloud representation of the anatomical structure, the proposed approach can directly handle high resolution input images without losing context information. We achieve competitive evaluation scores, while producing smoother, more realistic surfaces. Furthermore, our method is capable of producing different implants per defect, accounting for the anatomical variation seen in the training population. Thereby, we can propose several solutions to the physicians, from which they can choose the most suitable one. For future work, we plan to speed up the sampling process by using different sampling schemes, as proposed in [5,29]."
Point Cloud Diffusion Models for Automatic Implant Generation,,Fig. 1 .,
Point Cloud Diffusion Models for Automatic Implant Generation,,Fig. 2 .,
Point Cloud Diffusion Models for Automatic Implant Generation,,Fig. 3 .,
Point Cloud Diffusion Models for Automatic Implant Generation,,Fig. 4 .,
Point Cloud Diffusion Models for Automatic Implant Generation,,Fig. 5 .,
Point Cloud Diffusion Models for Automatic Implant Generation,,Fig. 6 .,
Point Cloud Diffusion Models for Automatic Implant Generation,,Table 1 .,
Point Cloud Diffusion Models for Automatic Implant Generation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 11.
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,1,Introduction,"Surgical workflow analysis in endoscopic procedures aims to process large streams of data [8] from the operating room (OR) to build context awareness systems [19]. These systems aim to provide assistance to the surgeon in decision making [9] and planning [6]. Most of these systems focus on coarse-grained recognition tasks such as phase recognition [15], instrument spatial localization and skill assessment [5]. Surgical action triplets [11], defined as 〈instrument, verb, target〉, introduce the fine-grained modeling of elements present in an endoscopic scene. In cataract surgery, [7] adopts similar triplet formulation and also provides bounding box details for both instrument and targets. Another related work [1], in prostatectomy, uses bounding box annotations for surgical activities defined as 〈verb, anatomy〉. On laparoscopic cholecystectomy surgical data, existing approaches [10,13,16] focus on the challenging triplet recognition task, where the objective is to predict the presence of triplets but ignores their spatial locations in a video frame.A recently conducted endoscopic vision challenge [14] introduced the triplet detection task that requires instrument localization and its association with the triplet. While most of the contributed methods employ weak supervision to learn the instrument locations by exploiting the model's class activation map (CAM), a few other methods exploit external surgical datasets offering complementary instrument spatial annotations. The significant number of triplet classes left the weakly supervised approaches at subpar performance compared to fully supervised methods. The results of the CholecTriplet2022 challenge [14] have led to two major observations. First, imprecise localization from the CAMbased methods impairs the final triplet detection performance, where the best weakly-supervised method reaches only 1.47% detection mean average precision. Second, the correct association of triplet predictions and their spatial location is difficult to achieve with instrument position information alone. This is mainly due to the possible occurrence of multiple instances of the same instrument and many options of targets/verbs that can be associated with one instrument instance. We set two research questions following these observations: (1) since manual annotation of instrument spatial locations is expensive and tedious, how can we use learned target/verb features to supplement a minimal amount of instrument spatial annotations? (2) since instrument cues are insufficient for better triplet association, how can we generate valid representative features of the targets/verbs that do not require additional spatial labels?To tackle these research questions, we propose a fully differentiable twostage pipeline, MCIT-IG, that stands for M ulti-Class Instrument-aware Transformer -Interaction Graph. The MCIT-IG relies on instrument spatial information that we generate with Deformable DETR [22], trained on a subset of Cholec80 [17] annotated with instrument bounding boxes. In the first stage, MCIT, a lightweight transformer, learns class wise embeddings of the target influenced by instrument spatial semantics and high level image features. This allows the embeddings to capture global instrument association features useful in IG. We train MCIT with target binary presence label, providing weak supervision. In the second stage, IG creates an interaction graph that performs dynamic association between the detected instrument instances and the target embeddings, and learns the verb on the interacting edge features, thereby detecting triplets. To train IG, triplet labels for the detected instrument instances are needed, which is unavailable. To circumvent this situation, we generate pseudo triplet labels for the detected instrument instances using the available binary triplet presence labels. In this manner, we provide mixed supervision to train MCIT-IG.We hypothesize that a precise instrument detector can reveal additional instrument-target associations as more instrument instances are detected. To test this hypothesis, we conduct a study to investigate how the accuracy of the instrument detector affects triplet detection. We train an instrument detector with limited spatial data and evaluate the impact on triplet detection. We find that enhancing instrument localization is strongly linked to improved triplet detection performance. Finally, we evaluate our model on the challenge split of CholecT50 [13,14]. We report both improved instrument localization and triplet detection performance, thanks to the graph-based dynamic association of instrument instances with targets/verbs, that captures the triplet label."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,2,Methodology,"We design a novel deep-learning model that performs triplet detection in twostages. In the first stage, we use a transformer to learn the instrument-aware target class embeddings. In the second stage, we construct an interaction graph from instrument instances to the embeddings, learn verb on the interacting edges and finally associate a triplet label with instrument instances. Using a trained Deformable DETR [22] based on the MMDetection [2] framework, we obtain bounding boxes for the instruments."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Backbone:,"To extract visual features, we utilize ResNet50 [4] as the backbone, and apply a 1×1 convolution layer to reduce the feature dimension from R h×w×c to R h×w×d , where c and d are 2048 and 512 respectively. We flatten the features to R hw×d and input to the Base Encoder, a lightweight transformer with b l layers. The base encoder modulates the local scene features from ResNet50 and incorporates context from other regions to generate global features F b ∈ R hw×d . We then apply ROIAlign on F b to generate instrument instance features F r ∈ R O×d , where O denotes the number of detected instruments. We also apply a linear layer Φ b on the instrument box coordinates and concatenate with the embeddings of predicted instrument class category to get d-dimensional features, and finally fuse with F r using linear layer Φ f to produce final d-dimensional instrument features F i ."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Learning Instrument-Aware Target Class Embeddings:,"To learn target features, we introduce a M ulti-C lass I nstrument-aware T ransformer ( MCIT ) that generates embeddings for each target class. In the standard transformer [3], a single class-agnostic token models the class distribution, but dilutes crucial class specific details. Inspired from [21], MCIT utilizes N class tokens, N t ∈ R N ×d , to learn class-specific embeddings of the target. However, to make the class embeddings aware of the instruments in the scene, we use the instrument features F i along with class tokens and region features. Specifically, MCIT takes input (F b , F i ) and creates learnable queries of dimension R (hw+N )×d and keys, values of dimension R (hw+N +O)×d to compute attention. Then, MCIT applies t l layers of attention to generate the output sequence, P t ∈ R (hw+N )×d . The learned class embeddings are averaged across N and input to a linear layer Φ t to generate logits y t ∈ R N following Eq. 1:MCIT learns meaningful class embeddings of the target enriched with visual and position semantics of the instruments. This instrument-awareness is useful to identify the interacting instrument-target pairs."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Learning Instrument-Target Interactions:,"To learn the interaction of the instrument and the target, we introduce a graph based framework I nteraction-Graph ( IG) that relies on the discriminative features of instrument instances and target class embeddings. We create an unidirectional complete bipartite graph, G = (U, V, E), where |U| = O and |V| = N denotes the source and destination nodes respectively, and edges E = {e u v , u ∈ U ∧ v ∈ V}. The node features of U and V correspond to the detected instrument instance features F i and target class embeddings N t respectively. We further project the nodes features to a lower dimensional space d using a linear layer Φ p . This setup provides an intuitive way to model instrument-tissue interactions as a set of active edges. Next, we apply message passing using GAT [18], that aggregates instrument features in U and updates target class embeddings at V."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Learning Verbs:,"We concatenate the source and destination node features of all the edges E in G to construct the edge feature E f = {e f , e f ∈ R 2d }. Then, we compute the edge confidence score E s = {e s , e s ∈ R} for all edges E in G by applying a linear layer Φ e on E f . As a result, shown in Fig. 1 Stage 2, only active edges (in blue) remain and inactive edges (in red) are dropped based on a threshold. The active edge indicates the presence of interaction between an instrument instance and the target class. To identify the verb, we apply a linear layer Φ v on E f and generate verb logits y v ∈ R V +1 , where V is the number of verb classes with an additional 1 to denote background class.Triplet Detection: To perform target and verb association for each instrument instance i, first we select the active edge e i j that corresponds to the target class j = argmax(α(E i s )), where α denotes softmax function and E i s = {e u s , ∀e u s ∈ E s ∧ u = i}. For the selected edge e = e i j , we apply softmax on the verb logits to obtain the verb class id, k = argmax(α(y e v )). The final score for the triplet 〈i, k, j 〉 is given by p(e i j ) × p(y e v k ), where p denotes the probability score. Mixed Supervision: We train our model in two stages. In the first stage, we train MCIT to learn target classwise embeddings with target binary presence label with weighted binary cross entropy on target logits y t for multi-label classification task following Eq. 2:where C refers to total number of target classes, y c and ŷc denotes correct and predicted labels respectively, σ is the sigmoid function and W c is the class balancing weight from [13]. For the second stage IG, we generate pseudo triplet labels for each detected instrument instances, where we assign the triplet from the binary triplet presence label if the corresponding instrument class matches.To train IG, we apply categorical cross entropy loss on edge set E i s and verb logits y e v for all instrument instances i to obtain losses L e G and L v G respectively following Eq. 3:where M denotes the number of classes which is N for L e G and V + 1 for L v G . The final loss for training follows Eq. 4:where α and β denote the weights to balance the loss contribution."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,3,Experimental Results and Discussion,
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,3.1,Dataset and Evaluation Metrics,"Our experiments are conducted on the publicly available CholecT50 [13] dataset, which includes binary presence labels for 6 instruments, 10 verbs, 15 targets, and 100 triplet classes. We train and validate our models on the official challenge split of the dataset [12]. The test set consists of 5 videos annotated with instrument bounding boxes and matching triplet labels. Since the test set is kept private to date, all our results are obtained by submitting our models to the challenge server for evaluation. The model performance is accessed using video-specific average precision and recall metrics at a threshold (θ = 0.5) using the ivtmetrics library [12]. We also provide box association results in the supplementary material for comparison with other methods on the challenge leaderboard."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,3.2,Implementation Details,"We first train our instrument detector for 50 epochs using a spatially annotated 12 video subset of Cholec80 and generate instrument bounding boxes and pseudo triplet instance labels for CholecT50 training videos. In stage 1, we set b l = 2, t l = 4, and d to 512. We initialize target class embeddings with zero values. We use 2-layer MLP for Φ b , Φ f , and 1-layer MLP for Φ t . We resize the input frame to 256 × 448 resolution and apply flipping as data augmentation. For training, we set learning rate 1e -3 for (backbone, base encoder), and 1e -2 for MCIT. We use SGD optimizer with weight decay 1e -6 and train for 30 epochs. To learn the IG, we fine-tune stage 1 and train stage 2. We use learning rate 1e -4 for (MCIT, base encoder), and 1e -5 for the backbone. In IG, Φ p , Φ e , and Φ v are 1-layer MLP with learning rate set to 1e -3 , and d set to 128 in Φ p to project node features to lower dimensional space. We use Adam optimizer and train both stage 1 and stage 2 for 30 epochs, exponentially decaying the learning rate by 0.99. The loss weights α and β is set to 1 and 0.5 respectively. We set batch size to 32 for both stages. We implement our model in PyTorch and IG graph layers in DGL [20] library. We train the model on Nvidia V100 and A40 GPUs and tune model hyperparameters using random search on 5 validation videos."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,3.3,Results,
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Comparison with the Baseline:,"We obtain the code and weights of Rendezvous (RDV) [13] model from the public github and generate the triplet predictions on the CholecT50-challenge test set. We then associate these predictions with the bounding box predictions from the Deformable DETR [22] to generate baseline triplet detections as shown in Table 1. With a stable instrument localization performance (60.1 mAP), our MCIT model leverage the instrument-aware target features to captures better semantics of instrument-target interactions than the baseline. Adding IG further enforces the correct associations, thus improving the triplet detection performance by +0.89 mAP, which is 13.8% increase from the baseline performance. Moreover, the inference time in frame per seconds (FPS) for our MCIT-IG model on Nvidia V100 is ∼29.Ablation Study on the Spatial Annotation Need: Here, we study the impact of an instrument localization quality on triplet detection and how the target features can supplement fewer spatial annotations of the instruments for better triplet detection. We compare with ResNet-CAM-YOLOv5 [14] and Distilled-Swin-YOLO [14] models which were also trained with bounding box labels. We observed that the triplet detection mAP increases with increasing instrument localization mAP for all the models as shown in Table 2. However, the scale study shows that with lesser bounding box instances, our MCIT-IG model stands tall: outperforming Distilled-Swin-YOLO by +0.86 mAP with ∼9K fewer frames and surpassing ResNet-CAM-YOLOv5 by +1.42 mAP with ∼7K frames to spare. Note that a frame can be annotated with one or more bounding boxes."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Ablation Studies on the Components of MCIT-IG:,"We analyze the modules used in MCIT-IG and report our results in Table 3. Using both ROI and box features provides a complete representation of the instruments that benefits IG, whereas using just ROI or box features misses out on details about instruments hurting the triplet detection performance. We further test the quality of target class embeddings without instrument awareness in MCIT. Results in Table 3 indicates that the lack of instrument context hampers the ability of the target class embeddings to capture full range of associations with the triplets. Also, message passing is key in the IG as it allows instrument semantics to propagate to target class embeddings, which helps distinguish interacting pairs from other non-interacting pairs."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Comparison with the State-of-the-Art (SOTA) Methods:,"Results in Table 4 show that our proposed model outperforms all the existing methods in the CholecTriplet 2022 challenge [14], obtained the highest score that would have placed our model 1 st on the challenge leaderboard in all the accessed metrics. Leveraging our transformer modulated target embeddings and graph-based associations, our method shows superior performance in both instrument localization and triplet detection over methods weakly-supervised on binary presence labels and those fully supervised on external bounding box datasets like ours. More details on the challenge methods are provided in [14]."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,4,Conclusion,"In this work, we propose a fully differentiable two-stage pipeline for triplet detection in laparoscopic cholecystectomy procedures. We introduce a transformerbased method for learning per class embeddings of target anatomical structures in the absence of target instance labels, and an interaction graph that dynamically associates the instrument and target embeddings to detect triplets. We also incorporate a mixed supervision strategy to help train MCIT and IG modules. We show that improving instrument localization has a direct correlation with triplet detection performance. We evaluate our method on the challenge split of the CholecT50 dataset and demonstrate improved performance over the leaderboard."
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Fig. 1 .,
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Table 1 .,
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Table 2 .,
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Table 3 .,
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Table 4 .,
Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 48.
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,1,Introduction,"Surgical videos can provide objective records in addition to medical records. Such videos are used in various applications, including education, research, and information sharing [2,5]. In endoscopic surgery and robotic surgery, the surgical field can be easily captured because the system is designed to place a camera close to it to monitor operations directly within the camera field of view. Conversely, in open surgery, surgeons need to observe the surgical field; therefore, the room for additional cameras can be limited and disturbed [6].To overcome this issue, Kumar and Pal [7] installed a stationary camera arm to record surgery. However, their camera system had difficulty recording details (i.e., close-up views) since the camera had to be placed far from the surgical field so as not to disturb the surgeons. Instead, Nair et al. [9] used a camera mounted on a surgeon's head, which moved frequently and flexibly.For solid and stable recordings, previous studies have installed cameras on surgical lights. Byrd et al. [1] mounted a camera on a surgical light, which could easily be blocked by the surgeon's head and body. To address this issue, Shimizu et al. [12] developed a surgical light with multiple cameras to ensure that at least one camera would observe the surgical field (Fig. 1). In such a multicamera system, automatically switching cameras can ensure that the surgical field is visible in the generated video [4,11,12]. However, the cameras move every time surgeons move the lighting system, and thus, the image alignment becomes challenging. Obayashi et al. [10] relied on a video player to manually seek and segment a video clip with no camera movement. This unique camera setup and view-switching approaches create a new task to be fulfilled, which we address in this paper: automatic occlusionfree video generation by automated change detection in camera configuration and multi-camera alignment to smoothly switch to the camera with the least occlusion. 1 In summary, our contributions are as follows:-We are the first to fulfill the task of automatic generation of stable virtual single-view video with reduced occlusion for a multi-camera system installed in a surgical light. -We propose an algorithm that detects camera movement timing by measuring the degree of misalignment between the cameras. -We propose an algorithm that finds frames with less occluded surgical fields.-We present experiments showing greater effectiveness of our algorithm than conventional methods."
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,2,Method,"Given a sequence of image frames captured by five cameras installed in a surgical light x i = [x 1 , x 2 , ..., x T ] (Fig. 1), our goal is to generate a stable single-view video sequence with less occlusion z = [z 1 , z 2 , ..., z T ](Fig. 2). Here, x i represents a sequence captured with i-th camera c i , and T indicates the number of frames.We perform an initial alignment using the method by Obayashi et al. [10] (Fig. 2a), which cumulatively collects point correspondences over none-feature rich frames and calculates homography matrices, M i , via a common planar scene proxy. Then, we iteratively find a frame ID, t c , where the cameras started moving (Sect. 2.1, Fig. 2b) and a subsequent frame ID, t h , to update homography matrices under no moving cameras and the least occlusion (Sect. 2.2, Fig. 2c). Updated homography warping can provide a newly aligned camera view sequence y i = [y 1 , y 2 , ..., y T ] after the cameras moved. Finally, using the learning-based object detection method by Shimizu et al. [12], we select camera views with the least occlusion from y i (Fig. 2d). Collecting such frames results in a stable single-view video sequence with the least occlusion z = [z 1 , z 2 , ..., z T ]. "
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,2.1,Camera Movement Detection,"To find the t c , we use the ""degree of misalignment"" obtained from the sequence y i of the five aligned cameras. Since the misalignment should be zero if the geometric calibration between the cameras works well and each view overlaps perfectly, it can be used as an indication of camera movement.First, the proposed method performs feature point detection in each frame of y i . Specifically, we use the SIFT algorithm [8]. Then, feature point matching is performed for each of the 10 combinations of the five frames. The degree of misalignment D t at frame t is represented aswhere p and k denote a keypoint position and its index in the i-th camera's coordinates respectively, l represents the corresponding index of k in the j-th camera's coordinates, and n represents the total number of corresponding points.If D t exceeds a certain threshold, our method detects camera movement. However, the calculated misalignment is too noisy to be used as is. To eliminate the noise, the outliers are removed, and smoothing is performed by calculating the movement average. Moreover, to determine the threshold, sample clustering is performed according to the degree of misalignment. Assuming that the multicamera surgical light never moves more than twice in 10 min, the detected degree of misalignment is divided into two classes, one for every 10 min.The camera movement detection threshold is expressed by Eq. ( 2), where t represents the frames classified as the frames before the camera movement. The frame at the moment when the degree of misalignment exceeds the threshold is considered as the frame when the camera moved.To make the estimation of t c more robust, this process is performed multiple times on the same group of frames, and the median value is used as t c . This is expected to minimize false detections."
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,2.2,Detecting the Timing for Obtaining a Homography Matrix,"t h represents the timing when to obtain homography matrix. Although it would be ideal to generate always-aligned camera sequences by performing homography transformation on every frame, this would incur high computational costs if the homography is constantly calculated. Therefore, the proposed method calculates the homography matrix only after the cameras have stopped moving. Unlike a previous work that determined the timing for performing homography transformation manually [10], our method automatically detects t h by using the area of surgical field appearing in surgical videos as an indication. This region indicates the extent of occlusion. Since the five cameras capture the same surgical field, if there is no occluded camera, the area of surgical field will have the same extent in all five camera frames. Eq. ( 3) is used to calculate the degree to which the area of surgical field is the same in all five camera frames, where s i is the area of surgical field detected in each camera.where S is calculated every 30 frames, and if it is continuously below a given threshold (0.5 in this method), the corresponding timing is selected as the t h ."
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,3,Experiments and Results,"We conducted two experiments to quantitatively and qualitatively investigate our method's efficacy. Dataset. We used a multi-camera system attached to a surgical light to capture videos of surgical procedures. We captured three types of actual surgical procedures: polysyndactyly, anterior thoracic keloid skin graft, and posttraumatic facial trauma rib cartilage graft. From these videos, we prepared five videos which were trimmed to one minute each. Videos 1 and 2 show the surgery of polysyndactyly, videos 3 and 4 show the anterior thoracic keloid skin graft scene, and video 5 shows the surgery of posttraumatic facial trauma rib cartilage graft.Implementation Details. We used Ubuntu 20.04 LTS OS, an Intel Core i9-12900 for the CPU, and 62GiB of RAM. We defined the area of surgical field as hue ranging from 0 to 30 or from 150 to 179 in HSV color space.Virtual Single-View Video Generation. Figure 3 shows a representative frame from the automatic positioning of the surgical video of the polysyndactyly operation using the proposed method. The figure also includes frames with detected camera movement. Once all five viewpoints were aligned, they were fed into the camera-switching algorithm to generate a virtual single-viewpoint video. The method requires about 40 min every time the cameras move. Please note that it is fully automated and needs no human labor, unlike the existing method, i.e., manual-alignment.Comparison with Conventional Methods. We compared our auto alignment method (auto-alignment) with two conventional methods. In one of these methods, which is used in a hospital camera switching is performed after manual alignment (manual-alignment). The other method switches between camera views with no alignment (no-alignment)."
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,3.1,Qualitative Evaluation,"To qualitatively compare our method against baseline methods, we conducted a subjective evaluation. 11 physicians involved in surgical procedures regularly who were expected to actually use the surgical videos were selected as subjects. Figure 4 shows single-view video frames generated by our method and the two conventional methods. The red lines indicate the position and orientation of the instep of patient's foot. In video generated with no-alignment, the position and orientation of the insteps changed every time camera switching was performed, making it difficult to observe the surgical region with comfort. Manualalignment showed better results than no-alignment. However, it was characterized by greater misalignment than the proposed method. It should also be noted that manual alignment requires time and effort. In contrast, our method effectively reduced misalignment between viewpoints even when camera switching was performed.To perform a qualitative comparison between the three methods, following a previous work [13], we recruited eleven experienced surgeons who were expected to actually use surgical videos and asked them to conduct a subjective evaluation of the captured videos. The subjects were asked to score five statements, 1 (""disagree"") to 5 (""agree"").  The results are shown in Fig. 5. For almost all videos and statements, the Wilcoxon signed-rank test showed that the proposed method (auto-alignment) scored significantly higher than the two conventional methods. Significant differences are indicated by asterisks in the graphs in Fig. 5. The specific p-values are provided in the supplemental material. The results showing that the proposed method outperformed the conventional methods in statements 1 and 2 suggest that our method generates a stable video with little misalignment between camera viewpoints. Additionally, the results indicating that the proposed method outperformed the conventional method in statements 3 and 4 suggest that the video generated by our method makes it easier to confirm the surgical area. Furthermore, as shown in statement 5, the proposed method received the highest score in terms of the subjects' willingness to use the system in actual medical practice. We believe that the proposed method can contribute to improving the quality of medical care by facilitating stable observation of the surgical field.Although we observed statistically significant differences between the proposed method and the baselines for almost all the test videos, significant differences were not observed only in Video 3, Statement 3 and Video 4, Statements 3 and 4. There may be two reasons for this. One is the small number of participants. Since we limited the participants to experienced surgeons, it was quite difficult to obtain a larger sample size. The second reason is differences in the geometry of the surgical fields. Our method is more effective for scenes with a three-dimensional geometry. If the surgical field is flat with fewer feature points, as in the case of the anterior thoracic keloid skin graft procedure, differences between our method and the manual alignment mehod, which does not take into account three-dimensional structures, are less likely to be observed."
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,3.2,Quantitative Evaluation,"Our method aims to reduce the misalignment between viewpoints that occurs when swıtching between multiple cameras and generate single-view surgical videos with less occlusion. To investigate the method's effectiveness, we conducted a quantitative evaluation to assess the degree of misalignment between video frames. Following a previous work that calculated degree of misalignment between consecutive time-series frames [3], we used two metrics, the interframe transformation fidelity (ITF) and the average speed (AvSpeed). ITF represents the average peak signal-to-noise ratio (PSNR) between frames aswhere N f is the total number of frames. ITF is higher for videos with less motion blur. AvSpeed expresses the average speed of feature points. With the total number of frames N f and the number of all feature points in a frame N p , AvSpeed is calculated aswhere z i (t) denotes the image coordinates of the feature point and is calculated as żi (t) = z i (t + 1)z i (t).The results are shown in Table 1. The ITF of the videos generated using the proposed method was 20%-50% higher than that of the videos with manual alignment. The AvSpeed of the videos generated using the proposed method was 40%-70% lower than that of the videos with manual alignment, indicating that the shake was substantially corrected."
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,4,Conclusion and Discussion,"In this work, we propose a method for generating high-quality virtual singleviewpoint surgical videos captured by multiple cameras attached to a surgical light without occlusion or misalignment through automatic geometric calibration. In evaluation experiments, we compared our auto-alignment method with manual-alignment and no-alignment. The results verified the superiority of the proposed method both qualitatively and quantitatively. The ability to easily confirm the surgical field with the automatically generated virtual single-viewpoint surgical video will contribute to medical treatment.Limitations. Our method relies on visual information to detect the timing of homography calculations (i.e., t h ). However, we may use prior knowledge of a geometric constraint such that cameras are at the pentagon corners (Fig. 1).We assume that the multi-camera surgical light does not move more than twice in 10 min for a robust calculation of D t . Although surgeons rarely moved the light more often, fine-tuning the parameter may result in further performance improvement. The current implementation shows misaligned images if the cameras move more frequently.In the user-involved study, several participants reported noticeable black regions where no camera views were projected. (e.g., Fig. 4). One possible complement is to project pixels from other views."
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,,Fig. 1 .,
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,,Fig. 2 .,
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,,Fig. 3 .,
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,,Fig. 4 .,
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,,1 .,
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,,Fig. 5 .,
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,,Table 1 .,
High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 26.
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,1,Introduction,"Specific knowledge in the medical domain needs to be acquired through extensive study and training. When faced with a surgical scenario, patients, medical students, and junior doctors usually come up with various questions that need to be answered by surgical experts, and therefore, to better understand complex surgical scenarios. However, the number of expert surgeons is always insufficient, and they are often overwhelmed by academic and clinical workloads. Therefore, it is difficult for experts to find the time to help students individually [22,24]. Automated solutions have been proposed to help students learn surgical knowledge, skills, and procedures, such as pre-recorded videos, surgical simulation and training systems [13,18], etc. Although students may learn knowledge and skills from these materials and practices, their questions still need to be answered by experts. Recently, several approaches [22,24] have demonstrated the feasibility of developing safe and reliable VQA models in the medical field. Specifically, Surgical-VQA [22] made effective answers regarding tools and organs in robotic surgery, but they were still unable to help students make sense of complex surgical scenarios. For example, suppose a student asks a question about the tooltissue interaction for a specific surgical tool, the VQA model can only simply answer the question, but cannot directly indicate the location of the tool and tissue in the surgical scene. Students will still need help understanding this complex surgical scene. Another problem with Surgical-VQA is that their sentence-based VQA model requires datasets with annotation in the medical domain, and manual annotation is time-consuming and laborious.Currently, extensive research and progress have been made on VQA tasks in the computer vision domain [17]. Models using long-short term memory modules [28], attention modules [24], and Transformer [17] significantly boost the performance in VQA tasks. Furthermore, FindIt [16] proposed a unified Transformer model for joint object detection and ViL tasks. However, firstly, most of these models acquire the visual features of key targets through object detection models. In this case, the VQA performance strongly depends on the object detection results, which hinders the global understanding of the surgical scene [23], and makes the overall solution not fully end-to-end. Second, many VQA models employ simple additive, averaging, scalar product, or attention mechanisms when fusing heterogeneous visual and textual features. Nevertheless, in heterogeneous feature fusion, each feature represents different meanings, and simple techniques cannot achieve the best intermediate representation from heterogeneous features. Finally, the VQA model cannot highlight specific regions in the image relevant to the question and answer. Supposing the location of the object in the surgical scene can be known along with the answer by VQLA models, students can compare it with the surrounding tissues, different surgical scenes, preoperative scan data, etc., to better understand the surgical scene [4].In this case, we propose CAT-ViL DeiT for VQLA tasks in surgical scene understanding. Specifically, our contributions are three-fold: (1) We carefully design a Transformer-based VQLA model that can relate the surgical VQA and localization tasks at an instance level, demonstrating the potential of AI-based VQLA system in surgical training and surgical scene understanding. (2) In our proposed CAT-ViL embedding, the co-attention module allows the text embeddings to have instructive interaction with visual embeddings, and the gated module works to explore the best intermediate representation for heterogeneous embeddings. (3) With extensive experiments, we demonstrate the extraordinary performance and robustness of our CAT-ViL DeiT in localizing and answering questions in surgical scenarios. We compare the performance of detection-based and detection-free feature extractors. We remove the computationally costly and error-prone detection proposals to achieve superior representation learning and end-to-end real-time applications."
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,2,Methodology,
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,2.1,Preliminaries,"VisualBERT [17] generates text embeddings (including token embedding e t , segment embedding e s , and position embedding e p ) based on the strategy of natural language model BERT [9], and uses object detection model to extract visual embeddings (consisting of visual features representation f v , segment embedding f s and position embedding f p ). Then, it concatenates visual and text embeddings before feeding the subsequent multilayer Transformer module.Multi-Head Attention [26] can focus limited attention on key and highvalue information. In each head h i , give the certain query q ∈ R dq , key matrix K ∈ R d k , value matrix V ∈ R dv , the attention for each head is calculated asare learnable parameters, and A represents the function of single-head attention aggregation. A linear conversion is then applied for the attention aggregation from multiple heads: h = MA(W o [h 1 . . . h h ]). W o ∈ R po×hpv is the learnable parameters in multiple heads. Each head may focus on a different part of the input to achieve the optimal output."
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,2.2,CAT-ViL DeiT,"We present CAT-ViL DeiT to process the information from different modalities and implement the VQLA task in the surgical scene. DeiT [25] serves as the backbone of our network. As shown in Fig. 1, the network consists of a vision feature extractor, a customized trained tokenizer, a co-attention gated embedding module, a standard DeiT module, and task-specific heads.Feature Extraction: Taking a given image and the associated question, conventional VQA models usually extract visual features via object proposals [17,28]. Instead, we employ ResNet18 [11] pre-trained on ImageNet [8] as our visual feature extractor. This design enables faster inference speed and global understanding of given surgical scenes. The text embeddings are acquired via a customized pre-trained tokenizer [22]. The CAT-ViL embedding module then processes and fuses the input embeddings from different modalities."
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,,CAT-ViL Embedding:,"In the following, the extracted features are processed into visual and text embeddings following VisualBERT [17] as described in Sect. 2.1. However, VisualBERT [17] and VisualBERT ResMLP [22] naively concatenate the embeddings from different modalities without optimizing the intermediate representation between heterologous embeddings. In this case, information and statistical representations from different modalities cannot interact perfectly and serve subsequent tasks. "
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,,Self-Attn,"Self-Attn Self-Attn Self-Attn Self-Attn Self-AttnSelf-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Guided-Attn Guided-Attn Guided-Attn Guided-Attn Guided-Attn Guided-Attn Inspired by [3,28], we replace the naive concatenation operation with a coattention gated ViL module. The gated module can explore the best combination of the two modalities. Co-attention learning enables active information interaction between visual and text embeddings. Specifically, the guided-attention module is applied to infer the correlation between the visual and text embeddings. The normal self-attention module contains the multi-head attention layer, a feed-forward layer, and ReLU activation. The guide-attention module also contains the above components, but its input is from both two modalities, in which the q is from visual embeddings and K,V are from text embeddings:Therefore, the visual embeddings shall be reconstructed with the original query, and the key and value of the text embeddings, which can realize the text embeddings to have instructive information interaction with the visual embeddings, and help the model to focus on the targeted image context related to the question. Six guided-attention layers are applied in our network. Thus, the correlation between questions and image regions can be gradually constructed. Besides, we also build six self-attention blocks for both visual and text embeddings to boost the internal relationship within each modality. This step can also avoid 'over' guidance and seek a trade-off. Then, the attended text embeddings and textguided attended visual embedding shall be output from the co-attention module and propagated through the gated module.Compared to the naive concatenation [17], summation, or the multilayer perceptron (MLP) layer [28], this learnable gated neuron-based model can control the contribution of multimodal input to output through selective activation (set as tanh here). The gate node α is employed to control the weight for selective visual and text embedding aggregation. The equations of the gated module are as follows: Subsequently, the fused embeddings E o shall feed the pre-trained DeiT-Base [25] module before the task-specific heads. The pre-trained DeiT-Base module can learn the joint representation, resolve ambiguous groundings from multimodel information, and maximize performance.Prediction Heads: The classification head, following the normal classification strategy, is a linear layer with Softmax activation. Regarding the localization head, we follow the setup in Detection with Transformers (DETR) [7]. A simple feed-forward network (FFN) with a 3-layer perceptron, ReLU activation, and a linear projection layer is employed to fit the coordinates of the bounding boxes. The entire network is therefore built end-to-end without multi-stage training.Loss Function: Normally, the cross-entropy loss L CE serves as our classification loss. The combination of L 1 -norm and Generalized Intersection over Union (GIoU) loss [21] is adopted to conduct bounding box regression. GIoU loss [21] further emphasizes both overlapping and non-overlapping regions of bounding boxes. Then, the final loss function is L = L CE + (L GIoU + L 1 )."
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,3,Experiments,
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,3.1,Dataset,"EndoVis 2018 Dataset is a public dataset with 14 robotic surgery videos from MICCAI Endoscopic Vision Challenge [1]. The VQLA annotations are publicly accessible by [4], in which the QA pairs are from [23] and the bounding box annotations are from [14]. Specifically, the QA pairs include 18 different singleword answers regarding organs, surgical tools, and tool-organ interactions. When the question is about organ-tool interactions, the bounding box will contain both the organ and the tool. We follow [22] to use video [1,5,16]  against VisualBERT (light blue) [17], VisualBERT ResMLP (green) [22], MCAN (orange) [28], VQA-DeiT (purple) [25], MUTAN (gray) [5], MFH (dark blue) [29], and BlockTucker (pink) [6]. The Ground Truth bounding box is red. (Color figure online)EndoVis 2017 Dataset is also a publicly available dataset from the MIC-CAI Endoscopic Vision Challenge 2017 [2], and the annotations are also available by [4]. We employ this dataset as an external validation dataset to demonstrate the generalization capability of our model in various surgical domains. Specifically, we manually select and annotate frames with common organs, tools, and interactions in EndoVis 2017 Dataset, generating 97 frames with 472 QA pairs. We conduct no training but only testing on this external validation dataset."
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,3.2,Implementation Details,"We conduct our comparison experiments against VisualBERT [17], VisualBERT ResMLP [22], MCAN [28], VQA-DeiT [25], MUTAN [5], MFH [29], and Block-Tucker [6]. In VQA-DeiT, we use pre-trained DeiT-Base block [25] to replace the multilayer Transformer module in VisualBERT [17]. To keep a fair comparison of VQLA tasks, we use the same prediction heads in and loss function in Sect. 2.2. The evaluation metrics are accuracy, f-score, and mean intersection over union (mIoU) [21]. All models are trained on NVIDIA RTX 3090 GPUs using Adam optimizer [15] with PyTorch. The epoch, batch size, and learning rate are set to 80, 64, and 1 × 10 -5 , respectively. The experimental results are the average results with five different random seeds."
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,3.3,Results,"Figure 2 presents the visualization and qualitative comparison of the surgical VQLA system. Quantitative evaluation in Table 1 presents that our proposed model using ResNet18 [11] feature extractor suppresses all SOTA models significantly. Additionally, we compare the performance between using object proposals (Faster RCNN [20]) and using features from the entire image (ResNet18 [11]). The experimental results in EndoVis-18 show that removing the object proposal model improves the performance appreciably on both question-answering and localization tasks, which demonstrates the impact of this approach in correcting potential false detections. Meanwhile, in the external validation set -EndoVis-17, our CAT-ViL DeiT with RCNN feature extractor suffers from domain shift  [22] 0.6064 0.3226 0.7305 0.4267 0.3506 0.6947 MCAN [28] 0.6084 0.3428 0.7257 0.4258 0.3035 0.6832 VQA-DeiT [25] 0.6089 0.3217 0.7338 0.4492 0.3213 0.7134 MUTAN [5] 0.6049 0.3238 0.7217 0.4364 0.3206 0.6870 MFH [29] 0.6179 0.3158 0.7227 0.3729 0.2048 0.7183 BlockTucker [6] 0.6067 0.3414 0.7313 0.4364 0.3210 0.6825 CAT-ViL DeiT (Ours) 0.6192 0.3521 0.7482 0.4555 0.3676 0.7049VisualBERT [17] 6.64 ms 0.6268 0.3329 0.7391 0.4005 0.3381 0.7073 VisualBERT R [22] 0.6301 0.3390 0.7352 0.4190 0.3370 0.7137 MCAN [28] 0.6285 0.3338 0.7526 0.4137 0.2932 0.7029 VQA-DeiT [25] 0.6104 0.3156 0.7341 0.3797 0.2858 0.6909 MUTAN [5] 0.6283 0.3395 0.7639 0.4242 0.3482 0.7218 MFH [29] 0.6283 0.3254 0.7592 0.4103 0.3500 0.7216 BlockTucker [6] 0.6201 0.3286 0.7653 0.4221 0.3515 0.7288 CAT-ViL DeiT (Ours) 0.6452 0.3321 0.7705 0.4491 0.3622 0.7322 Fig. 3. Robustness experiments on the EndoVis-18 dataset. We process the data with 18 corruption methods at each severity level and average the prediction results. and class imbalance problems, thus achieving poor performance. However, our final model, CAT-ViL DeiT with ResNet18 feature extractor, endows the network with global awareness and outperforms all baselines in terms of accuracy and mIoU, proving the superiority of our method. The inference speed is also enormously accelerated, demonstrating its potential in real-time applications. Furthermore, a robustness experiment is conducted to observe the model stability when test data is corrupted. We set 18 types of corruption on the test data based on the severity level from 1 to 5 by following [12]. Then, the performance of our model and all comparison methods on each corruption severity level is presented in Fig. 3. As the severity increases, the performance of all models degrades. However, our model shows good stability against corruption, and presents the best prediction results at each severity level. The excellent robustness of our model brings great potential for real-world applications.Finally, we conduct an ablation study on different ViL embedding techniques with the same feature extractors and DeiT backbone in Table 2. We compare with Concatenation [17], Joint Cross-Attention (JCA) [19], Multimodal Multi-Head Convolutional Attention (MMHCA) [10], Multimodal Attention Transformers (MAT) [27], Gated Fusion [3], Self-Attention Fusion [26], Guided-Attention Fusion [28], Co-Attention Fusion (T2V: Text-Guide-Vision) [28]. Besides, we explore the Co-Attention module with different directions (V2T: Vision-Guide-Text, and Bidirectional). Furthermore, we also incorporate the Gated Fusion with different attention mechanisms (Self-Attention, Guided-Attention, Bidirectional Co-Attention, Co-Attention (V2T), Co-Attention (T2V)) for detailed comparison. They are shown as 'Self-Attn Gated', 'Guided-Attn Gated', 'CAT-ViL (Bi)', 'CAT-ViL (V2T)' and 'CAT-ViL (T2V)' in Table 2. The study proves the superior performance of our ViL embedding strategy against other advanced methods. We also demonstrate that integrating attention feature fusion techniques and the gated module will bring performance improvement."
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,4,Conclusions,"This paper presents a Transformer model with CAT-ViL embedding for the surgical VQLA tasks, which can give the localized answer based on a specific surgical scene and associated question. It brings up a primary step in the study of VQLA systems for surgical training and scene understanding. The proposed CAT-ViL embedding module is proven capable of optimally facilitating the interaction and fusion of multimodal features. Numerous comparative, robustness, and ablation experiments display the leading performance and stability of our proposed model against all SOTA methods in both question-answering and localization tasks, as well as the potential of real-time and real-world applications. Furthermore, our study opens up more potential VQA-related problems in the medical community. Future work can be focused on quantifying and improving the reliability and uncertainty of these safety-critical tasks in the medical domain."
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,,Fig. 1 .,
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,,Fig. 2 .,
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,,Table 1 .,
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,,Table 2 .,
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,,Acknowledgements,". This work was funded by Hong Kong RGC CRF C4063-18G, CRF C4026-21GF, RIF R4020-22, GRF 14203323, GRF 14216022, GRF 14211420, NSFC/RGC JRS N CUHK420/22; Shenzhen-Hong Kong-Macau Technology Research Programme (Type C 202108233000303); Guangdong GBABF #2021B1515120035. M. Islam was funded by EPSRC grant [EP/W00805X/1]."
CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 38.
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,1,Introduction,"Video segmentation, which refers to assigning pixel-wise annotation to each frame in a video, is one of the most vital tasks in medical image analysis. Thanks to the advance in deep learning algorithms based on Convolutional Neural Networks, medical video segmentation has achieved great progress over recent years [9]. But a major problem of deep learning methods is that they are largely dependent on both the quantity and quality of training data [13]. Datasets annotated by non-expert humans or automated systems with little supervision typically suffer from very high label noise and are extremely time-consuming. Even expert annotators could generate different labels based on their cognitive bias [6]. Based on the above, noisy labels are inevitably in existence within medical video datasets causing misguidance to the network and resulting in severe performance degradation. Hence, it is of great importance to design medical video segmentation methods that are robust to noisy labels within training data [4,18].Most of the previous noisy label methods mainly focus on classification tasks. Only in recent years, the problem of noise labels in segmentation tasks has been more explored, but still less involved in medical image analysis. Previous techniques for solving noisy label problems in medical segmentation tasks can be categorized in three directions. The first type of method aims at deriving and modeling the general distribution of noisy labels in the form of Noise Transition Matrix (NTM) [3,8]. Secondly, some researchers develop special training strategies to re-weight or re-sample the data such that the model could focus on more dependable samples. Zhang et al. [19] concurrently train three networks and each network is trained with pixels filtered by the other two networks. Shi et al. [14] use stable characteristics of clean labels to estimate samples' uncertainty map which is used to further guide the network. Thirdly, label refinement is implemented to renovate noisy labels. Li et al. [7] represent the image with superpixels to exploit more advanced information in an image and refine the labels accordingly. Liu et al. [10] use two different networks to jointly determine the error sample, and use each other to refine the labels to prevent error accumulation. Xu et al. [15] utilize the mean-teacher model and Confident learning to refine the low-quality annotated samples.Despite the amazing performance in tackling noisy label issues for medical image segmentation, almost all existing techniques only make use of the information within a single image. To this end, we make the effort in exploring the feature affinity relation between pixels from consecutive frames. The motivation is that the embedding features of pixels from adjacent frames should be close if they belong to the same class, and should be far if they belong to different classes. Hence, if a pixel's feature is far from the pixels of the same class in the adjacent frame and close to the ones of different classes, its label is more likely to be incorrect. Meanwhile, the distribution of noisy labels may vary among different videos and frames, which also motivates us to supervise the network from multiple perspectives. We acquire the embedding feature maps of adjacent frames in the Backbone Section. Then, the temporal affinity is calculated for each pixel in current frame to obtain the positive and negative affinity map indicating possible noisy labels. The affinity maps are then utilized to supervise the network in a multi-scale manner.Inspired by the motivation above and to better resolve noisy label problems with temporal consistency, we propose Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework. Our contributions can be summarized as the following points:1. In this work, we first propose a novel Temporal Feature Affinity Learning (TFAL) method to evaluate the temporal feature affinity map of an image by calculating the similarity between the same and different classes' features of adjacent frames, therefore indicating possible noisy labels. 2. We further develop a Multi-Scale Supervision (MSS) framework based on TFAL by supervising the network through video, image, and pixel levels. Such a coarse-to-fine learning process enables the network to focus more on correct samples at each stage and rectify the noisy labels, thus improving the generalization ability. 3. Our method is validated on a publicly available dataset with synthetic noisy labels and a real-world label noise dataset and obtained superior performance over other state-of-the-art noisy label techniques. 4. To the best of our knowledge, we are the first to tackle noisy label problems using inter-frame information and discover the superior ability of sequential prior information to resolve noisy label issues."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,2,Method,"The proposed Multi-Scale Temporal Feature Affinity Learning Framework is illustrated in Fig. 1. We aim to exploit the information from adjacent frames to identify the possible noisy labels, thereby learning a segmentation network robust to label noises by re-weighting and refining the samples. Formally, given an input training image x t ∈ R H×W ×3 , and its adjacent frame x t-1 , two feature maps f t , f t-1 ∈ R h×w×C f are first generated by a CNN backbone, where h,w and C f represent the height, width and channel number. Intuitively, for each pair of features from f t and f t-1 , their distance should be close if they belong to the same class and far otherwise. Therefore for each pixel in f t , we calculate two affinity relations with f t-1 . The first one is called positive affinity, computed by averaging the cosine similarity between one pixel f t (i) in the current frame and all the same class' pixels as f t (i) in previous frame. The second one is called negative affinity, computed by averaging the cosine similarity between one pixel f t (i) in current frame and all the different class' pixels as f t (i) in previous frame.Then through up-sampling, the Positive Affinity Map a p and Negative Affinity Map a n can be obtained, where a p , a n ∈ R H×W , denote the affinity relation between x t and x t-1 . The positive affinity of clean labels should be high while the negative affinity of clean labels should be low. Therefore, the black areas in a p and the white areas in a n are more likely to be noisy labels.Then we use two affinity maps a p , a n to conduct Multi-Scale Supervision training. Multi-scale refers to video, image, and pixel levels. Specifically, for pixel-level supervision, we first obtain thresholds t p and t n by calculating the average positive and negative affinity over the entire dataset. The thresholds are used to determine the possible noisy label sets based on positive and negative affinity separately. The intersection of two sets is selected as the final noisy set and relabeled with the model prediction p t . The affinity maps are also used to estimate the image-level weights λ I and video-level weights λ V . The weights enable the network to concentrate on videos and images with higher affinity confidence. Our method is a plug-in module that is not dependent on backbone type and can be applied to both image-based backbones and video-based backbones by modifying the shape of inputs and feature maps."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,2.1,Temporal Feature Affinity Learning,"The purpose of this section is to estimate the affinity between pixels in the current frame and previous frame, thus indicating possible noisy labels. Specifically, in addition to the aforementioned feature map f t , f t-1 ∈ R h×w×C f , we obtain the down-sampled labels with the same size of feature map ỹ t , ỹ t-1 ∈ R h×w×C , where C means the total class number. We derive the positive and negative label maps with binary variables: M p , M n ⊆ {0, 1}hw×hw . The value corresponds to pixel (i, j) is determined by the label as:where 1 (•) is the indicator function. M p (i, j) = 1 when ith label in ỹ t and jth label in ỹ t-1 are the same class, while M p (i, j) = 0 otherwise; and M n vise versa. The value of cosine similarity map S ∈ R hw×hw corresponds to pixel (i, j) is determined by: S (i, j) =We then use the average cosine similarity of a pixel with all pixels in the previous frame belonging to the same or different class to represent its positive or negative affinity:where a p,f , a n,f ∈ R h×w means the positive and negative map with the same size as the feature map. With simple up-sampling, we could obtain the final affinity maps a p , a n ∈ R H×W , indicating the positive and negative affinity of pixels in the current frame."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,2.2,Multi-scale Supervision,"The feature map is first connected with a segmentation head generating the prediction p. Besides the standard cross entropy loss L CE (p, ỹ) = -HW i ỹ (i) logp (i), we applied a label corrected cross entropy loss L CE LC (p, ŷ) = -HW i ŷ (i) logp (i) to train the network with pixel-level corrected labels. We further use two weight factors λ I and λ V to supervise the network in image and video levels. The specific descriptions are explained in the following sections.Pixel-Level Supervision. Inspired by the principle in Confident Learning [12], we use affinity maps to denote the confidence of labels. if a pixel x (i) in an image has both small enough positive affinity a p (i) t p and large enough negative affinity a n (i) t n , then its label ỹ (i) can be suspected as noisy. The threshold t p , t n are obtained empirically by calculating the average positive and negative affinity, formulated as t p = 1 |Ap| ap∈Ap a p , t n = 1 |An| an∈An a n , where a p , a n means the average value of positive and negative affinity over an image. The noisy pixels set can therefore be defined by:Then we update the pixel-level label map ŷ as:where p(i) is the prediction of network. Through this process, we only replace those pixels with both low positive affinity and large negative affinity.Image-Level Supervision. Even in the same video, different frames may contain different amounts of noisy labels. Hence, we first define the affinity confidence value as: q = a p + 1 -a n . The average affinity confidence value is therefore denoted as: q = t p + 1 -t n . Finally, we define the image-level weight as:λ I > 1 if the sample has large affinity confidence and λ I < 1 otherwise, therefore enabling the network to concentrate more on the clean samples.Video-Level Supervision. We assign different weights to different videos such that the network can learn from more correct videos in the early stage. We first define the video affinity confidence as the average affinity confidence of all the frames:x∈V q x . Supposing there are N videos in total, we use k ∈ {1, 2, • • • , N} to represent the ranking of video affinity confidence from small to large, which means k = 1 and k = N denote the video with lowest and highest affinity confidence separately. Video-level weight is thus formulated as:where θ l and θ u are the preseted lower-bound and upper-bound of weight.Combining the above-defined losses and weights, we obtain the final loss as:LC , which supervise the network in a multi-scale manner. These losses and weights are enrolled in training after initialization in an order of video, image, and pixel enabling the network to enhance the robustness and generalization ability by concentrating on clean samples from rough to subtle."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,3,Experiments,
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,3.1,Dataset Description and Experiment Settings,"EndoVis 2018 Dataset and Noise Patterns. EndoVis 2018 Dataset is from the MICCAI robotic instrument segmentation dataset1 of endoscopic vision challenge 2018 [1]. It is officially divided into 15 videos with 2235 frames for training and 4 videos with 997 frames for testing separately. The dataset contains 12 classes including different anatomy and robotic instruments. Each image is resized into 256 × 320 in pre-processing. To better simulate manual noisy annotations within a video, we first randomly select a ratio of α of videos and in each selected video, we divide all frames into several groups in a group of 3-6 consecutive frames. Then for each group of frames, we randomly apply dilation, erosion, affine transformation, or polygon noise to each class [7,16,18,19]. We investigated our algorithms in several noisy settings with α being {0.3, 0.5, 0.8}. Some examples of data and noisy labels are shown in supplementary."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,,Rat Colon Dataset.,"For real-world noisy dataset, we have collected rat colon OCT images using 800nm ultra-high resolution endoscopic spectral domain OCT. We refer readers to [17] for more details. Each centimeter of rat colon imaged corresponds to 500 images with 6 class layers of interest. We select 8 sections with 2525 images for training and 3 sections with 1352 images for testing. The labels of test set were annotated by professional endoscopists as ground truth while the training set was annotated by non-experts. Each image is resized into 256 × 256 in pre-processing. Some dataset examples are shown in supplementary.Implementation Details. We adopt Deeplabv3+ [2] as our backbone network for fair comparison. The framework is implemented with PyTorch on two Nvidia 3090 GPUs. We adopt the Adam optimizer with an initial learning rate of 1e -4 and weight decay of 1e -4. Batch size is set to 4 with a maximum of 100 epochs for both Datasets. θ l and θ u are set to 0.4 and 1 separately. The video, image, and pixel level supervision are involved from the 16th, 24th, and 40th epoch respectively. The segmentation performance is assessed by mIOU and Dice scores."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,3.2,Experiment Results on EndoVis 2018 Dataset,"Table 1 presents the comparison results under different ratios of label noises. We evaluate the performance of backbone trained with clean labels, two state-ofthe-art instrument segmentation network [5,11], two noisy label learning techniques [4,8], backbone [2] and the proposed MS-TFAL. We re-implement [4,8] with the same backbone [2] for a fair comparison. Compared with all other methods, MS-TFAL shows the minimum performance gap with the upper bound (Clean) for both mIOU and Dice scores under all ratios of noises demonstrating the robustness of our method. As noise increases, the performance of all baselines decreases significantly indicating the huge negative effect of noisy labels. It is noteworthy that when the noise ratio rises from 0.3 to 0.5 and from 0.5 to 0.8, our method only drops 2.57% mIOU with 2.41% Dice and 8.98% mIOU with 9.49% Dice, both are the minimal performance degradation, which further demonstrates the robustness of our method against label noise. In the extreme noise setting (α = 0.8), our method achieves 41.36% mIOU and 51.01% Dice and outperforms second best method 5.37% mIOU and 6.26% Dice. As shown in Fig. 2, we provide partial qualitative results indicating the superiority of MS-TFAL over other methods in the qualitative aspect. More qualitative results are shown in supplementary.  Ablation Studies. We further conduct two ablation studies on our multi-scale components and choice of frame for feature affinity under noisy dataset with α = 0.5. With only video-level supervision (w/V), mIOU and Dice are increased by 4.93% and 4.43% compared with backbone only. Then we apply both video and image level supervision (w/V & I) and gain an increase of 0.92% mIOU and 1.09% Dice. Pixel-level supervision is added at last forming the complete Multi-Scale Supervision results in another improvement of 1.62% mIOU and 1.96% Dice verifying the effectiveness in attenuating noisy label issues of individual components. For the ablation study of the choice of frame, we compared two different attempts with ours: conduct TFAL with the same frame and any frame in the dataset (Ours is adjacent frame). Results show that using adjacent frame has the best performance compared to the other two choices.Visualization of Temporal Affinity. To prove the effectiveness of using affinity relation we defined to represent the confidence of label, we display comparisons between noise variance and selected noise map in Fig. 3. Noise variance (Fourth column) represents the incorrect label map and the Selected noise map (Fifth column) denotes the noise map we select with Eq. ( 3). We can observe that the noisy labels we affirm have a high overlap degree with the true noise labels, which demonstrates the validity of our TFAL module."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,,Method,Deeplabv3+ [2] STswin [5] RAUNet [11] JCAS [4] VolMin [8]  
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,3.3,Experiment Results on Rat Colon Dataset,"The comparison results on real-world noisy Rat Colon Dataset are presented in Table 2. Our method outperforms other methods consistently on both mIOU and Dice scores, which verifies the superior robustness of our method on real-world label noise issues. Qualitative results are shown in supplementary."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,4,Discussion and Conclusion,"In this paper, we propose a robust MS-TFAL framework to resolve noisy label issues in medical video segmentation. Different from previous methods, we first introduce the novel TFAL module to use affinity between pixels from adjacent frames to represent the confidence of label. We further design MSS framework to supervise the network from multiple perspectives. Our method can not only identify noise in labels, but also correct them in pixel-wise with rich temporal consistency. Extensive experiments under both synthetic and real-world label noise data demonstrate the excellent noise resilience of MS-TFAL."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,,Fig. 1 .,
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,,Fig. 2 .,
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,,Fig. 3 .,
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,,Table 2 .,
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,,Acknowledgements,". This work was supported by Hong Kong Research Grants Council (RGC) Collaborative Research Fund (C4026-21G), General Research Fund (GRF 14211420 & 14203323), Shenzhen-Hong Kong-Macau Technology Research Programme (Type C) STIC Grant SGDX20210823103535014 (202108233000303)."
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,,Data,Method mIOU (%) Sequence mIOU (%) Dice (%)
Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 9.
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,1,Introduction,"Reconstructing deformable tissues in surgical scenes accurately and efficiently from endoscope stereo videos is a challenging and active research topic. Such techniques can facilitate constructing virtual surgery environments for surgery robot learning and AR/VR surgery training and provide vivid and specific training for medics on human tissues. Moreover, real-time reconstruction further expands its applications to intraoperative use, allowing surgeons to navigate and precisely control surgical instruments while having a complete view of the surgical scene. This capability could reduce the need for invasive follow-up procedures and address the challenge of operating within a confined field of view.Neural Radiance Fields (NeRFs) [16], a promising approach for 3D reconstruction, have demonstrated strong potential in accurately reconstructing deformable tissues in dynamic surgical scenes from endoscope stereo videos. EndoNeRF [26], a recent representative approach, represents deformable surgical scenes using a canonical neural radiance field and a time-dependent neural displacement field, achieving impressive reconstruction of deformable tissues. However, the optimization for dynamic NeRFs is computationally intensive, often taking dozens of hours, as each generated pixel requires hundreds of neural network calls. This computational bottleneck significantly constrains the widespread application of these methods in surgical procedures.Recently, explicit and hybrid methods have been developed for modeling static scenes, achieving significant speedups over NeRF by employing explicit spatial data structures [8,9,27] or features decoded by small MLPs [5,17,25]. Nevertheless, these methods have only been applied to static scenes thus far. Adopting these methods to surgical scenes presents significant challenges for two primary reasons. Firstly, encoding temporal information is essential for modeling surgical scenes while naively adding a temporal dimension to the explicit data structure can significantly increase memory and computational requirements. Secondly, dynamic surgical scene reconstruction suffers from limited viewpoints, often providing only one view per timestep, as opposed to static scenes, which can fully use multi-view consistency for further regularization. This condition requires sharing information across disjoint timesteps for better reconstruction.To address the aforementioned challenges, we propose a novel method for fast and accurate reconstruction of deformable tissues in surgical procedures, Neural LerPlane (Linear Interpolation Plane), by leveraging explicitly represented multi-plane fields. Specifically, we treat surgical procedures as 4D volumes, where the time axis is orthogonal to 3D spatial coordinates. LerPlane factorizes 4D volumes into 2D planes and uses space planes to form static fields and space-time planes to form dynamic fields. This factorization results in a compact memory footprint and significantly accelerates optimization compared to previous methods [6,26], which rely on pure MLPs, as shown in Fig. 1. LerPlane enables information sharing across timesteps within the static field, thereby reducing the negative impact of limited viewpoints. Moreover, considering the surgical instrument occlusion, we develop a novel sample approach based on tool masks and contents, which assigns higher sampling probability to tissue pixels that have been occluded by tools or have a more extensive motion range. By targeting these regions, our approach allows for more efficient sampling during the training, leading to higher-quality results and faster optimization.We summarize our contributions:1. A fast deformable tissue reconstruction method, with rendering quality comparable to or better than the previous method in just 3 min, which is over 100× faster. 2. An efficient representation of surgical scenes, which includes static and dynamic fields, enabling fast optimization and high reconstruction quality. 3. A novel sampling method that boosts optimization and improves the rendering quality. Compared to previous methods, our LerPlane, achieves much faster optimization with superior quantitative and qualitative performance on 3D reconstruction and deformation tracking of surgical scenes, providing significant promise for further applications."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,2,Method,
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,2.1,Overview,"LerPlane represents surgical procedures using static and dynamic fields, each of which is made up of three orthogonal planes (Sect. 2.3). It starts by using spatiotemporal importance sampling to identify high-priority tissue pixels and build corresponding rays (Sect. 2.4). Then we sample points along each ray and query features using linear interpolation to construct fused features. The fused features and encoded coordinate-time information are input to a lightweight MLP, which predicts color and density for each point (Sect. 2.5). To better optimize LerPlane, we introduce some training schemes, including sample-net, various regularizers, and a warm-up training strategy (Sect. 2.6). Finally, we apply volume rendering to produce predicted color and depth values for each chosen ray. The overall framework is illustrated in Fig. 2."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,2.2,Preliminaries,"Neural Radiance Field (NeRF) [16] is a coordinate-based neural scene representation optimized through a differentiable rendering loss. NeRF maps the 3D coordinate and view direction of each point in the space into its color values c and volume density σ via neural networks Φ r .c, σ = Φ r (x, y, z, θ, φ).(1)It calculates the expected color Ĉ(r) and the expected depth D(r) of a pixel in an image captured by a camera by tracing a ray r(t) = o + td from the camera center to the pixel. Here, o is the ray origin, d is the ray direction, and t is the distance from a point to the o, ranging from a pre-defined near bound t n to a far bound t f . w(t) represents a weight function that accounts for absorption and scattering during the propagation of light rays. The pixel color is obtained by classical volume rendering techniques [10], which involve sampling a series of points along the ray.  "
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,2.3,Neural LerPlane Representations for Deformable Tissues,"A surgical procedure can be represented as a 4D volume, and we factorize the volume into 2D planes. Specifically, We represent a surgical scene using six orthogonal feature planes, consisting of three space planes (i.e., XY, YZ, and XZ ) for the static field and three space-time planes (i.e., XT, YT, and ZT ) for the dynamic field. Each space plane has a shape of N × N × D, and each space-time plane owns a shape of N × M × D, where N and M represent spatial and temporal resolution, respectively, and D is the size of the feature.To extract features from an image pixel p ij with color C at a specific timestep τ , we first cast a ray r(t) from o to the pixel. We then sample spatial-temporal points along the ray, obtaining their 4D coordinates. We acquire a feature vector for a point P(x, y, z, τ ) by projecting it onto each plane and using bilinear interpolation B to query features from the six feature planes. v(x, y, z, τ ) = B(F XY , x, y) B(F YZ , y, z) . . . B(F YT , y, τ) B(F ZT , z, τ), (3) where the represents element-wise multiplication, inspired by [4,22]. The fused feature vector v is then passed to a tiny MLP Θ, which predicts the color c and density σ of the point. Finally, we leverage the Eq. 2 to get the predicted color Ĉ(r). Inspired by [14,17], we build the feature planes with multi-resolution planes, e.g. F XY is represented by planes with N = 128 and 256.Existing methods [6,26] for reconstructing surgical procedures using pure implicit representations, requires traversing all possible positions in space-time, which is highly computationally and time-intensive. In contrast, LerPlane decomposes the surgical scene into six explicitly posed planes, resulting in a significant reduction in complexity and a much more efficient representation. This reduces the computational cost from O(N 4 ) to O(N 2 ) and enables the use of smaller neural networks, leading to a considerable acceleration in the training period. Besides, methods [7,12,19,20,23,26] using a single displacement field to supplement the static field struggle with handling variations in scene topology, such as non-linear deformations. In contrast, LerPlane can naturally model these situations using a dynamic field."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,2.4,Spatiotemporal Importance Sampling,"Tool occlusion in robotic surgery poses a challenge for reconstructing occluded tissues due to their infrequent occurrence in the training set, resulting in varied learning difficulties for different pixels. Besides, we observe that many tissues remain stationary over time, and therefore repeated training on these pixels contributes minor to the convergence, reducing efficiency. We design a novel spatiotemporal importance sampling strategy to address the issues above. In particular, we utilize binary masks {M i } T i=1 and temporal differences among frames to generate sampling weight maps {W} T i=1 . These weight maps represent the sampling probabilities for each pixel/ray, drawing inspiration from [12,26]. One sampling weight map W i can be determined by:where α is a lower-bound to avoid zero weight among unchanged pixels, Ω i specifies higher importance scaling for those tissue areas with higher occlusion frequencies, and β is a hyper-parameter for balancing augmentation among frequently occluded areas and time-variant areas. By unitizing spatiotemporal importance sampling, LerPlane concentrates on tissue areas and speeds up training, improving the rendering quality of occluded areas and prioritizing tissue areas with higher occlusion frequencies and temporal variability."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,2.5,Coordinate-Time Encoding,"Previous methods [6,26] apply positional encoded view direction γ(d) to model view-dependent appearance. However, during endoscopic operations, camera movements are restricted. The view direction changes are typically minimal. Instead of view encoding, we propose using γ(x, y, z, τ ) to enhance the spatiotemporal information. Specifically, the encoding along with the fused features v from feature planes is input to the MLP Θ, which predicts σ and c of each point. Then we utilize Eq. 2 to render the expected color Ĉ and depth D of one specific ray."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,2.6,Optimization,"We adopt a joint supervision approach to optimize the tiny MLP Θ and feature planes using rendered color and depth. To further improve the optimization process, we propose several optimization schemes, including a sample-net for better-sampled points, a warm-up strategy to address outliers, and several regularizers.Sample-Net. The sampling of spatiotemporal points is crucial for volume rendering, with a particular focus on sampling around tissue regions for optimal performance. We replaced the conventional two-stage time-consuming sampling strategy with a single sample-net and train it using histogram loss [3]. The sample-net is a lightweight single-resolution LerPlane model that provides more accurate sampling points for the full model."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,,Regularizers.,"We apply some regularization to address the limited information available in surgical scene reconstruction. We adopt 2D total variation (TV) loss for space planes in [5,8,24] and 1D TV loss on the space axis for space-time planes and a similar smooth loss on the time axis. Additionally, we introduce a minor time-invariant loss to separate the static and dynamic fields as much as possible, encouraging the features in space-time planes to remain unchanged."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,,Warm-Up Training Strategy.,"Since single-view captures cannot provide valid scale information, we leverage pseudo ground truth depth maps D(r) generated by STTR-light [13] from stereo images to guide the optimization. Specifically, we apply a Huber loss for depth regularization:where ΔD(r) = | D(r) -D(r)| represents the absolute depth difference among valid depth values, δ is a threshold at which to change loss type. Considering that the predicted depth maps encounter a lot of unreliable depth values and missing areas [26], we design a simple by effective warm-up training strategy. Specifically, we apply the L D to depths from both the sample-net and the full model during the first half of the training. In the remaining iterations, we disable the L D and use other regularization to refine unreliable depths.3 Experiments"
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,3.1,Dataset and Evaluation Metrics,"We evaluate our proposed method on the EndoNeRF dataset [26], a collection of typical robotic surgery stereo videos captured from stereo cameras at a single viewpoint during in-house DaVinci robotic prostatectomy procedures, which is designed to capture challenging surgical scenes with non-rigid deformation and tool occlusion. We evaluate our proposed method by comparing it to existing methods [15,26] using standard image quality metrics following [26], including PSNR, SSIM, and LPIPS. Additionally, to measure the consistency of the underlying 3D scene, we supplement these metrics using the FLIP metric [1,2]. For qualitative evaluation, we follow the exhibition method from [26].Ours-10min Reference Fig. 3. Qualitative results on scene ""traction"" from different timesteps τ ."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,3.2,Implementation Details,"We normalize the scene into device coordinates (NDC) to handle single-view endoscopy videos and then project rays within the NDC space. The video duration is normalized to [-1, 1]. We use a two-stage sampling network with 128 and 256 dimensional plane features for the sample-net. Oneblob encoding [18] is applied to encode the spatiotemporal information. The full model consists of four resolutions, 64, 128, 256, and 512 dimensions among space. Hyperparameters include D = 32 for feature planes, j = 25 for spatiotemporal importance sampling, ξ = 16 for Oneblob encoding dimensionality, and δ = 0.2 for depth loss across all experiments. An Adam [11] optimizer is used with default values for optimization. In each iteration, 2048 rays are randomly sampled from the whole dataset to form a batch. The initial learning rate is set to 0.01. We apply a cosine schedule with a 512 iterations warming-up stage. We train all scenes with 9k and 32k iterations, which take around 3 and 10 minutes, respectively, on a single RTX 3090 GPU running the Ubuntu 20.04. Our LerPlane is implemented with pure Pytorch [21]. The code is available at https://github.com/ Loping151/LerPlane."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,3.3,Evaluation,"We compare our proposed method, LerPlane, against two existing SOTA methods: the surfel warping-based method, E-DSSR [15] and NeRF-based method EndoNeRF [26]. We find that E-DSSR struggles to completely reconstruct surgical scenes, resulting in many holes and noisy points (see Fig. 3), which leads to poor numerical performance. In contrast, EndoNeRF achieves high-fidelity reconstruction of deformable tissues but requires around 14 h of optimization, which is computationally expensive and constrains intraoperative use. LerPlane, on the other hand, achieves comparable results to EndoNeRF with only 3 min of optimization, providing nearly 280-fold acceleration. Moreover, with a longer optimization time of 10 min, LerPlane outperforms both E-DSSR and EndoNeRF in terms of all metrics, as shown in Table 1. Our novel importance sampling and encoding strategies further enhance the ability of LerPlane to preserve details and produce accurate visualizations of deformable tissues, as demonstrated in Fig. 3. Our results demonstrate that LerPlane achieves significantly faster optimization without compromising reconstruction quality, showing great potential for future clinical applications in robotic surgery. "
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,3.4,Ablation Study,"We conduct ablation studies on the EndoNeRF dataset to understand the key components and demonstrate their effectiveness. Table 1 shows the performance of all experiments.1. Sampling Strategy. We compare with two different methods: naively avoiding tool masks, assigning equal weights to other pixels (Ours-NS), and assigning higher probabilities to highly occluded areas (Ours-TS), as in [26]. Our method effectively prioritizes time-variant and highly occluded areas, significantly improving convergence speed. 2. Encoding Strategy. Experiments showed that coordinate-time encoding achieves better performance in all metrics compared to no encoding (Ours-NE) or direction encoding (Ours-VE), showing the effectiveness of the proposed encoding.Further analysis of the optimization schemes is available in the Supplementary Materials."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,4,Conclusion and Future Work,"In this paper, we introduced LerPlane, a fast and accurate method for reconstructing deformable tissues from endoscopic videos. By utilizing multi-plane fields and spatiotemporal importance sampling, we can handle tool occlusion and large motion while significantly accelerating optimization. Our experiments show that LerPlane achieves rendering quality comparable to or better than EndoNeRF in just three minutes, which is over 100× faster. We believe that LerPlane could improve robotic surgery scene understanding, benefiting various clinical-oriented tasks and intraoperative surgery applications. Currently, the inference speed of our Lerplane is slow, In our future research endeavors, our main emphasis will revolve around improving the inference time of our approach, with the primary goal of efficiently supporting intraoperative operations. Moreover, we will dedicate our efforts to reducing the input data requirements, thereby aiming to broaden the applicability of LerPlane to a wider range of surgical scenarios."
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,,Fig. 1 .,
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,,,
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,,Fig. 2 .,
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,,,
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,,Table 1 .,± 3.076 0.935 ± 0.026 0.083 ± 0.022 0.075 ± 0.031 10 min
Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_5.
ACT-Net: Anchor-Context Action Detection in Surgery Videos,1,Introduction,"Surgery is often an effective therapy that can alleviate disabilities and reduce the risk of death from common conditions [17]. While surgical procedures are intended to save lives, errors within the surgery may bring great risks to the patient and even cause sequelae [18], which emphasizes the development of a computer-assisted system. A context-aware assistant system for surgery can not only decrease intraoperative adverse events, and enhance the quality of interventional healthcare [28], but also contribute to surgeon training, and assist procedure planning and retrospective analysis [9].Designing intelligent assistance systems for operating rooms requires an understanding of surgical scenes and procedures [20]. Most current works pay attention to phase and step recognition [3,27], which is to get the major types of events that occurred during the surgery. They merely provided very coarse descriptions of scenes. As the granularity of action increases, the clinical utility becomes more valuable in providing an accurate depiction of detailed motion [13,19]. Recent studies focus on fine-grained action recognition by modelling action as a group of the instrument, its role, and its target anatomy and capturing their associations [7,26]. Recognizing targets in different methods is dependent on different surgical scenarios and it also significantly increases the complexity and time consumption for anatomy annotation [30]. In addition, although most existing methods can provide accurate action positions, the predicted action class is often inaccurate. Moreover, they do not provide any information about the reliability of their output, which is a key requirement for integrating into assistance systems of surgery [11]. Thus, we propose a reliable surgical action detection method in this paper, with high-accuracy action predictions and their confidence.Mistrust is a major barrier to deep-learning-based predictions applied to clinical implementation [14]. Existing works measuring the model uncertainty [1,8] often need several-time re-evaluations, and store multiple sets of weights. It is hard for them to apply to surgery assistance applications to get confidence for each prediction directly [10], and they are limited to improving prediction performance. Conditional diffusion-based generative models have received significant attention due to their ability to accurately recover the full distribution of data guided by conditions from the perspective of diffusion probabilistic models [24]. However, they focus on generating high-resolution photo-realistic images. Instead, after observing our surgical video dataset, our conditional diffusion model aims to reconstruct accurately class distribution. We also access the estimation of confidence with the stochastic nature of the diffusion model.Here, to predict accurately micro-action (fine-grained action) categories happening every moment, we achieve it with two modules. Specifically, a novel anchor-context module for action detection is proposed to highlight the spatiotemporal regions that are interacted with the anchors (we extract instrument features as anchors), which includes surrounding tissues and movement information. Then, with the constraints of class distributions and the surgical videos, we propose a conditional diffusion model to cover the whole distribution of our data and to accurately reconstruct new predictions based on full learning. Furthermore, our class conditional diffusion model also accesses uncertainty for each prediction, through the stochasticity of outputs. We summarize our main contributions as follows: 1) We develop an anchorcontext action detection network (ACTNet), including an anchor-context detection (ACD) module and a class conditional diffusion (CCD) module, which combines three tasks: i) where actions locate; ii) what actions are; iii) how confident our model is about predictions. 2) For ACD module, we develop a spatiotemporal anchor interaction block (STAB) to spatially and temporally highlight the context related to the extracted anchor, which provides micro-action location and initial class. 3) By conditioning on the full distribution of action classes in the surgical videos, our proposed class conditional diffusion (CCD) model reconstructs better class prototypes in a stochastic fashion, to provide a more accurate estimations and push the assessment of the model confidence in its predictions. 4) We carry out comparison and ablation study experiments to demonstrate the effectiveness of our proposed algorithm based on cataract surgery."
ACT-Net: Anchor-Context Action Detection in Surgery Videos,2,Methodology,"The overall framework of our proposed ACTNet for reliable action detection is illustrated in Fig. 1. Based on a video frame sequence, the ACD module extracts anchor features and aggregates the spatio-temporal interactions with anchor features by proposed STAB, which generates action locations and initial action class distributions. Then considering the full distribution of action classes in surgical videos, we use the CCD module to refine the action class predictions and access confidence estimations."
ACT-Net: Anchor-Context Action Detection in Surgery Videos,2.1,Our ACD Module,"Anchor Extraction: Assuming a video X with T frames, denoted as X = {x t } T t=1 , where x t is the t-th frame of the video. The task of this work is to estimate all potential locations and classes P = {box n , c n } N n=1 for action instances contained in video X, where box n is the position of the n-th action happened, c n is the action class of n-th action, and N is the number of action instances. For video representation, this work tries to encode the original videos into features based on the backbone ResNet50 [5] network to get each frame's feature F = {f t } T t=1 . In surgical videos, the instruments, as action subjects, are significant to recognize the action. For instrument detection, it is very important but not very complicated. Existing excellent object detection method like Faster R-CNN [22] is enough to obtain results with high accuracy. After getting the detected instrument anchors, RoIAlign is applied to extract the instrument features from frame features. The instrument features are denoted as I = {i t } T t=1 . Since multiple instruments exist in surgeries, our action detection needs to solve the problem that related or disparate concurrent actions often lead to wrong predictions. Thus, in this paper, we propose to provide action location and class considering the spatio-temporal anchor interactions in the surgical videos, based on STAB.Spatio-Temporal Action interaction Block (STAB): For several actions like pushing, pulling, and cutting, there is no difference just inferred from the local region in one frame. Thus we propose STAB to utilize spatial and temporal interactions with an anchor to improve the prediction accuracy of the action class, which finds actions with strong logical links to provide an accurate class. The structure of STAB is shown in Fig. 1. We introduce spatial and temporal interactions respectively in the following.For spatial interaction: The instrument feature i t acts as the anchor. In order to improve the anchor features, the module has the ability to select value features that are highly active with the anchor features and merge them. The formulation is defined as:), where j is the index that enumerates all possible positions of f t . A pairwise function h(•) computes the relationship such as affinity between i t and all f tj . In this work, dot-product is employed to compute the similarity. The unary function g(f tj ) computes a representation of the input signal at the position j. The response is normalized by a factor. S j represents the set of all positions j. Through the formulation, the output a t obtains more information from the positions related to the instrument and catches interactions in space for the actions.For temporal interaction: We build memory features consisting of features in consecutive frames:To effectively model temporal interactions of the anchor, the network offers a powerful tool for capturing the complex and dynamic dependencies that exist between elements in sequential data and anchors. Same with the spatial interaction, we take i t as an anchor and calculate the interactions between the memory features and the anchor. The formulation  "
ACT-Net: Anchor-Context Action Detection in Surgery Videos,2.2,CCD Module for Reliable Action Detection,"Since the surgical procedures follow regularity, we propose a CCD module to reconstruct the action class predictions considering the full distribution of action classes in videos. The diffusion conditioned on the action classes and surgical videos is adopted in our paper. Let y 0 ∈ R n be a sample from our data distribution. As shown in Fig. 2, a diffusion model specified in continuous time is a generative model with latent y t , obeying a forward process q t (y t |y t-1 ) starting at data y 0 [6]. y 0 indicates a one-hot encoded label vector. We treat each one-hot label as a class prototype, i.e., we assume a continuous data and state space, which enables us to keep the Gaussian diffusion model framework [2,6].The forward process and reverse process of unconditional diffusion are provided in the supplementary material.Here, for the diffusion model optimization can be better guided by meaningful information, we integrate the ACD and our surgical video data as priors or constraints in the diffusion training process. We design a conditional diffusion model pθ (y t-1 |y t , x) that is conditioned on an additional latent variable x. Specifically, the model pθ (y t-1 |y t , x) is built to approximate the corresponding tractable ground-truth denoising transition step pt (y t-1 |y t , y 0 , x). We specify the reverse process with conditional distributions as [21]: pt (y t-1 |y t , y 0 , x) = pt (y t-1 |y t , y 0 , f ϕ (x)) = N y t-1 ; μ (y t , y 0 , f ϕ (x)) , βt I where μ (y t , y 0 , f ϕ (x)) and βt are described in supplementary material. f ϕ (x) is the prior knowledge of the relation between x and y 0 , i.e., the ACD module pre-trained with our surgical video dataset. The x indicates the input surgical video frames. Since ground-truth step pt (y t-1 |y t , y 0 , x) cannot get directly, the model pθ (y t-1 |y t , x) are trained by following loss function for estimating θ to approximate the ground truth:where α t := 1β t , ᾱt := t s=1 (1β s ), ∼ N (0, 1) and θ (•) estimates using a time-conditional network parameterized by θ. β t is a constant hyperparameter.To produce model confidence for each action instance, we mainly calculate the prediction interval width (IW). Specifically, we first sample N class prototype reconstruction with the trained diffusion model. Then calculate the IW between the 2.5 th and 97.5 th percentiles of the N reconstructed values for all test classes. Compared with traditional classifiers to get deterministic outputs, the denoising diffusion model is a preferable modelling choice due to its ability to produce stochastic outputs, which enables confidence generation."
ACT-Net: Anchor-Context Action Detection in Surgery Videos,3,Experimental Results,"Cataract Surgical Video Dataset: To perform reliable action detection, we build a cataract surgical video dataset. Cataract surgery is a procedure to remove the lens of the eyes and, in most cases, replace it with an artificial lens. The dataset consists of 20 videos with a frame rate of 1 fps (a total of 17511 frames and 28426 action instances). Under the direction of ophthalmologists, each video is labelled frame by frame with the categories and locations of the actions. 49 types of action bounding boxes as well as class labels are included in our dataset. The surgical video dataset is randomly split into a training set with 15 videos (13583 frames) and a testing set with 5 videos (3928 frames).Implementation Details: The proposed architecture is implemented using the publicly available Pytorch Library. A model with ResNet50 backbone from Faster R-CNN-benchmark [23] is adopted for our instrument anchor detection. In STAB, we use ten adjacent frames. During inference, detected anchor boxes with a confidence score larger than 0.8 are used. More implementation details are listed in the supplementary material. The performances are evaluated with official metric frame level mean average precision (mAP) at IoU = 0.1, 0.3, and 0.5, respectively, obtaining figures in the following named mAP 10 , mAP 30 and mAP 50 with their mean mAP mean .Method Comparison: In order to demonstrate the superiority of the proposed method for surgical action detection, we carry out a comprehensive comparison between the proposed method and the following state-of-the-art methods: 1) single-stage algorithms, including the Single Shot Detector (SSD) [16], SSDLite [25] and RetinaNet [12]. 2) two-stage algorithms, including Faster R-CNN [23], Mask R-CNN [4], Dynamic R-CNN [29] and OA-MIL [15]. The data presented in Table 1 clearly demonstrate that our method outperforms other approaches, irrespective of the IoU threshold being set to 0.1, 0.3, 0.5, or the average values. Notably, the results obtained after incorporating diffusion even surpass Faster R-CNN by 2.5% and baseline by 4.0% in terms of average mAP. This finding provides compelling evidence for the efficacy of our method in integrating spatiotemporal interactive information under the guidance of anchors and leveraging diffusion to optimize the category distribution. The quantitative results further corroborate the effectiveness of our approach in Fig. 3, which shows that our model does not only improve the performance of the baseline models but also localizes accurately the regions of interest of the actions. More results are listed in the material.Ablation Study: To validate the effectiveness of our ACTNet, we have done some ablation studies. We train and test the model with spatial interaction, temporal interaction, spatio-temporal interaction (STAB), and finally together with our CCD model. The testing results are shown in Fig. 3 and Table 1. For our backbone, it is achieved by concatenating the anchor features through RoIAlign and the corresponding frame features to get the detected action classes.The results reveal that the spatial and temporal interactions for instruments can provide useful information to detect the actions. What's more, spatial interaction has slightly better performance than temporal interaction. It may be led by the number of spatially related action categories being slightly more than that of temporally related action categories. It is worth noting that spatial interaction and temporal interaction can be enhanced by each other and achieve optimal performance. After being enhanced by the diffusion model conditioned on our obtained class distributions and video frames, we get optimal performance."
ACT-Net: Anchor-Context Action Detection in Surgery Videos,,Confidence Analysis:,"To analyze the model confidence, we take the best prediction for each instance to calculate the instance accuracy. We can observe   from Table 2 across the test set, the mean_IW of the class label among correctly classified instances by ACTNet is significantly narrower compared to that of incorrectly classified instances. This observation indicates that the model is more confident in its accurate predictions and is more likely to make errors when its predictions are vague. Furthermore, upon comparing the mean_IW at the true class level, we find that a more precise class tends to exhibit a larger disparity between the correct and incorrect predictions.  confidence estimations for some samples. We can see the correct prediction gets smaller IW values compared with the incorrect one (The rightmost figure in column (c)), which means it has more uncertainty for the incorrect prediction."
ACT-Net: Anchor-Context Action Detection in Surgery Videos,4,Conclusions,"In this paper, we propose a conditional diffusion-based anchor-context spatiotemporal action detection network (ACTNet) to achieve recognition and localization of every occurring action in the surgical scenes. ACTNet improves the accuracy of the predicted action class from two considerations, including spatiotemporal interactions with anchors by the proposed STAB and full distribution of action classes by class conditional diffusion (CCD) module, which also provides uncertainty in surgical scenes. Experiments based on cataract surgery demonstrate the effectiveness of our method. Overall, the proposed ACTNet presents a promising avenue for improving the accuracy and reliability of action detection in surgical scenes."
ACT-Net: Anchor-Context Action Detection in Surgery Videos,,Fig. 1 .,
ACT-Net: Anchor-Context Action Detection in Surgery Videos,,Fig. 2 .,
ACT-Net: Anchor-Context Action Detection in Surgery Videos,,,
ACT-Net: Anchor-Context Action Detection in Surgery Videos,,Fig. 3 .,
ACT-Net: Anchor-Context Action Detection in Surgery Videos,,,
ACT-Net: Anchor-Context Action Detection in Surgery Videos,,Table 1 .,
ACT-Net: Anchor-Context Action Detection in Surgery Videos,,Table 2 .,
Intelligent Virtual B-Scan Mirror (IVBM),1,Introduction,"Vitreoretinal surgeries are complex procedures that require extreme manual dexterity. Typically, surgeons operate through a stereoscopic microscope viewing the surgical area exclusively from an overhead perspective while manipulating delicate anatomical structures with sub-millimeter precision. In an effort to achieve improved surgical visualization, Optical Coherence Tomography (OCT) has been integrated into surgical microscopes, providing high-resolution depth-resolved imaging. Advances in spiral scanning and swept-source OCT [4,5,12] even paved the way for real-time volumetric imaging, enabling 4D visualizations of anatomical structures and surgical instruments. With the validation of intraoperative OCT through clinical studies [7,8], the emergence of 4D OCT systems prompts the anticipation of more precise and efficient microsurgical treatments. A common task in vitreoretinal surgery is to grasp and peel an Epiretinal Membrane (ERM), a 60 μm [24] thin layer that forms on top of the retina, while avoiding to damage the on average 250 μm thick retina [11].GPU-accelerated direct volume rendering (DVR) was shown to be an effective way of visualizing surgical maneuvers, enabling real-time rendering of 4D OCT data on stereo displays [20]. Instead of viewing the surgical site from the top, as with stereoscopic microscopes, the depth-resolved properties of 4D OCT can be directly and fully utilized when oblique or even more extreme lateral views are provided [6]. This could even lead to more precise tool-tissue interactions since visualization of the surgical area from alternative viewpoints enables improved distance perception. This can be leveraged for instance when approaching small structures located at or above the retina during ERM or Internal Limiting Membrane (ILM) peeling [7,9]. On the other hand, such setups also introduce new complexities since surgical interaction from a lateral perspective is not common in current ophthalmic procedures. In particular, hand-eye coordination is naturally challenged when navigating instruments to a target from an uncommon view, imposing a higher mental demand on surgeons. To exploit the full potential of the 4D depth-resolved imaging modality, advanced visualization techniques could provide additional guidance and support complex maneuvers.For this reason, we propose an Intelligent Virtual B-scan Mirror (IVBM), a novel visualization concept to improve targeted instrument interactions in 4D OCT-guided surgery. As illustrated in Fig. 1, the concept of an IVBM is inspired by the reflective but at the same time transparent nature of glass. Conceptually, grasping an arbitrarily positioned object in space is challenging in the absence of depth cues. However, the reflections can provide an additional view that aids navigation to the target. Semi-transparency, on the other hand, preserves the background. We transfer this concept to volume raymarching in 4D OCT and integrate a mirror into a selected volume cross-section, which represents a virtual B-scan. While the IVBM highlights volume intensities within this virtual Bscan, it integrates an intelligent mirror, which is only sensitive to voxels that are close to the IVBM and associated with a surgical instrument. Volume structures behind the IVBM are preserved through an adaptive opacity transfer function. In addition, we leverage a perceptually linear color map to encode distance information and enhance the mirrored instrument in the IVBM. Compared to previous works, the proposed method particularly focuses on targeted instrument interactions from alternative viewpoints. The results of our user study provide detailed insights into user perception and interaction with the IVBM and emphasize the benefits for targeted maneuvers in 4D OCT-guided vitreoretinal surgery."
Intelligent Virtual B-Scan Mirror (IVBM),2,Related Work,"Virtual mirrors have been initially proposed to support spatial perception and provide secondary views of virtual objects in augmented reality [1,13,19]. Applications have been introduced in angiography or laparoscopic surgery [15,21] and effectiveness of a mirror view in medical scenarios has been shown in user studies with clinical experts [2]. Previous works on OCT volume visualization have so far solely focused on depth perception in axial OCT direction. These include applying a color transfer function based on the distance to a central reference depth [3] or to an identified retinal reference layer [22]. Other ways of providing spatial cues include the augmentation of 2D OCT B-scans [17] or sound feedback to convey distance information [14].As opposed to previous works on perceptual OCT visualization, the IVBM provides depth cues that aid targeted instrument navigation when viewing the surgical scene from a lateral perspective and targeting a specific cross-section, where axial depth perception is not a primary issue. Augmentation methods proposed in previous works are not designed for such scenarios. Previous works on virtual mirrors have mainly been implemented by rendering the scene from an alternative viewpoint and showing the mirror view next to the virtual objects. In contrast, we augment a selective mirror in-situ into a OCT cross-section. "
Intelligent Virtual B-Scan Mirror (IVBM),3,Methodology,
Intelligent Virtual B-Scan Mirror (IVBM),3.1,Definition of an Intelligent Virtual B-Scan Mirror,"We define an Intelligent Virtual B-scan Mirror (IVBM) as an augmentation of a selected virtual B-scan, which fulfills the following two requirements: (i) voxels integrated into the IVBM are highlighted in the volume rendering, while still visualized semi-transparent to preserve volume structures behind the IVBM. (ii) The selected B-scan cross-section acts as an intelligent mirror by only being sensitive to voxels associated with the surgical instruments in proximity to the IVBM. As illustrated in Fig. 2, the IVBM has the advantage of providing depth cues by visualizing the intersection of the instrument with the cross-section, when the instrument tip meets the tip of the mirrored instrument. It is also capable of visualizing instrument structures that are occluded in the direct view or located behind the IVBM cross-section and amplifies target structures that are aligned with the IVBM. The following sections describe the required components, as well as the composition and direct integration of the IVBM in the volume raymarching algorithm."
Intelligent Virtual B-Scan Mirror (IVBM),3.2,Method Components,"Since the IVBM integrates a mirror that is sensitive only to surgical instruments, mirror candidate voxels need to be identified in the volume. To achieve the realtime processing rates necessary for 4D OCT visualizations, we first identify the instrument in a 2D projection image of the volume. Inspired by [23], we create a 2D projection image that encodes the average intensity along each OCT A-scan. This image is forwarded to a Unet-like [16] convolutional neural network with ResNet34 [10] backbone to generate a binary instrument map M tool . Since the OCT signal is fully blocked at the instrument surface, the anatomical structures below the instrument are obscured, and the corresponding A-scans contain only instrument-related voxels. Thus, the 3D position of the voxels associated with the instrument can be obtained by the detected A-scans in M tool , as further described in Sect. 3.3. Before forwarding the OCT volume and the instrument map M tool to the DVR algorithm, we additionally process the volume by applying a 3D median filter to reduce the OCT-typical speckle noise. An overview of the method is illustrated in Fig. 3. With this set of components in place, the composition of the IVBM is fully realized within the DVR algorithm, as described in the following section."
Intelligent Virtual B-Scan Mirror (IVBM),3.3,Composition of an Intelligent Virtual B-Scan Mirror,"During volume raymarching, if a camera ray #» r c intersects with the IVBM, as shown in Fig. 3, three components contribute to the resulting ray color: (i) any voxel p m along #» r c that is integrated into the IVBM, (ii) any voxel p v along #» r c behind the IVBM and (iii) the specific voxel p t , at which the mirror ray # » r m intersects with the surgical instrument. We define the IVBM plane M IV BM as any arbitrary, either manually or automatically selected plane and can thus be defined by the general equation:where #» n = (n x , n y , n z ) specifies the normal of the plane. We define a distance threshold d IV BM specifying the thickness of the IVBM. During volume raymarching, a voxel p m with position (p x , p y , p z ) is integrated in the IVBM, if the following condition is fulfilled:To visualize the mirror reflections of surgical instruments, a mirror ray # » r m is cast from each point p m obtained by (2). The mirror ray direction is determined by:During sampling along # » r m , we leverage the binary map M tool and an intensity threshold t tool = 0.25 (empirically obtained to discard OCT speckle noise), and select the mirrored instrument voxel p t if the following condition is fulfilled:To mirror the instrument only when close to the IVBM, we limit the number of sampled steps n steps along # » r m . In case of intersection with an instrument as determined by ( 4), the raymarching is terminated and the instrument reflection is augmented in the IVBM plane at p m with color component C pt . In particular, we encode the distance along # » r m between p m at the IVBM and p t at the instrument by employing a perceptually linear color map. We employ the L * a * b * color space similar to [22] with the following modifications:where δ * = (i m • 0.7)/n steps is a distance predicate that considers the step index i m ∈ [0, n steps ] along # » r m when reaching the instrument at p t . Further, γ(I) is a scaling factor as introduced in [22]. Choosing C 0 = (I(p t ), -1.5, 1) and C 1 = (I(p t ), -1.0, -1.0) achieves a color interpolation between blue hue when the instrument is further from the mirror and green hue when close to the mirror. The RGB component of the instrument reflection is then obtained with:where RGB(C) is an L * a * b * to RGB color space conversion. We additionally decrease the opacity of the reflections with increasing distance of the instrument to the mirror using σ(p t ) = 1.0 -(step m /n steps ) 2 .The overall appearance of a voxel at p m , integrating instrument reflections while enhancing the natural intensities in the IVBM plane is finally defined by:In practice, μ is a dynamically modifiable parameter to amplify the volume intensities of the virtual B-scan and α(I) is a conventional piece-wise linear opacity function. During volume raymarching, we use alpha blending to integrate the IVBM with the remaining volume structures as determined by (2). In general, voxels p v that are not in the IVBM can be rendered using any convention for direct volume rendering. During our experiments and for the visualizations in Fig. 2 and Fig. 3 we use classic Phong shading as previously employed for OCT volume rendering [20]. In our visualizations, it provides structural surface details while visually highlighting the IVBM in the final rendering."
Intelligent Virtual B-Scan Mirror (IVBM),4,Experimental Setup and User Study,"System Integration and Time Profiling. To evaluate if IVBMs can be deployed for the live display of 4D OCT data, we implemented the proposed concept in a C++ visualization framework. The 2D projection image generation and IVBMintegrated volume raymarching were implemented using OpenGL 4.6 and tested on a Windows 10 system with Intel Core i7-8700K @3.7 GHz and NVidia RTX 3090Ti GPU. We train our instrument segmentation network on a custom data set consisting of 3356 2D projection images generated from OCT volumes with a resolution of 391×391×644 voxels that contain a synthetic eye model and a surgical forceps and include random rotations and flipping for data augmentation. We use PyTorch 1.13 and TensorRT 8.4 for model training and optimization. The data was labeled by two biomedical engineers. The average overall processing and rendering time, based on 20 test volumes with the same resolution, was 44.3 (±3.1) ms (filter: 8.1 (± 1.2) ms, projection image generation and instrument segmentation: 5.7 (±2.6) ms, rendering: 30.5 (±0.6) ms). These benchmarks were achieved with n steps = 120 mirror sample steps. To demonstrate the live 4D interactions, our method was integrated into the 4D SS-OCT system presented in [4], acquiring OCT volumes with resolution 391 × 391 × 644 at 10 Hz volume update rates. In the supplementary materials, we include video recordings of instrument maneuvers in 4D OCT with our IVBM visualizations.User Study. To determine if an IVBM could aid users in performing targeted instrument maneuvers under 4D OCT guidance, we conducted a user study in which we asked participants to move the tip of a surgical instrument to defined target locations. To achieve continuous, accurate, and efficient data collection during the study, we employ a virtual environment (Unity 2021.3) with simulated 4D OCT based on the method proposed in [18]. Additionally, a haptic 3D input device (3D Systems Geomagic Touch) was integrated to navigate the virtual surgical instruments, where motion scaling of 4 : 1 was applied to reduce the influence of the individual manual tremor of the users. Small targets were generated on top of the retina, and the IVBM was automatically positioned at the target locations. To measure the effectiveness of the IVBM when interacting from uncommon perspectives, the virtual scene was rendered from a fixed view approximately orthogonal to the A-scan direction. As stereo vision through a microscope is an inherent part of common ophthalmic procedures, the simulation environment was displayed on an HTC Vive Pro headset leveraging stereo rendering. Users were asked to navigate the instrument tip to the target location and to press a button once satisfied with the positioning. The participants included in total 15 biomedical experts (12 male, 3 female) familiar with ophthalmology and OCT. The study was conducted in accordance with the declaration of Helsinki, the study data were anonymized, and vision tests were performed before the study to ensure healthy vision of all participants. After familiarizing themselves with the interaction in the virtual environment, participants performed 8 trials, with IVBM enabled. For ablation, the same number of trials were performed without IVBM, employing the method proposed in [20] for 4D OCT DVR with Phong shading as a baseline. The accuracy of the positioning and distance to the target was measured over the progression of the trials. Our results show that users reached the target with an average error of 70 µm (±40 µm) when the IVBM was activated, while in baseline rendering an average error of 135 µm (±128 µm) was measured, suggesting statistically significant differences between the distributions (p < 0.002 based on a Kruskal-Wallis test after detecting unequal variances). Furthermore, we analyzed the distance between the instrument tip and the target with respect to the progress of the trial. Figure 4a shows that when the IVBM was enabled, the deviation of the distance error continuously decreased, especially in the last quarter of the trial progressions, resulting in both more accurate and precise targeting. The outcomes of the NASA-TLX survey (Fig. 4b) conducted after the study showed improvements in all categories when the IVBM was activated. However, statistical significance could only be found in the categories performance (p < 0.001) and physical demand (p < 0.02) based on an ANOVA test for equal variances."
Intelligent Virtual B-Scan Mirror (IVBM),5,Discussion and Conclusion,"Discussion. When users were provided an IVBM, statistically significant improvements regarding the targeting error were found. In a clinical setting, such as during retinal membrane peeling, similar improvements could make a substantial difference and may lead to a safer and more efficient treatment. The results of the subjective task load assessment were also overall improved when the IVBM was enabled, however, statistical significance could not be obtained in categories such as mental demand or frustration. Potential depth conflicts could be investigated in further studies and evaluated with clinical experts. Interestingly, a higher targeting accuracy was achieved with IVBM, even in cases when users reported a higher effort or judged their performance inferior compared to baseline. An advantage of the IVBM is direct in-situ visualization that also highlights target structures, as users do not need to move their gaze from the surgical area to perceive the mirrored view. We envision an automatic identification of anatomical targets to find optimal IVBM positioning in the future."
Intelligent Virtual B-Scan Mirror (IVBM),,Conclusion.,"We presented a novel visualization concept that augments a selected volume cross-section with an intelligent virtual mirror for targeted instrument navigation in 4D OCT. We have provided a definition and implementation of an IVBM and demonstrated its potential to effectively support surgical tasks and to be integrated into a 4D OCT system. We demonstrated the IVBM in simulated vitreoretinal surgery, however, we intend to further apply this concept also to other 4D medical imaging modalities."
Intelligent Virtual B-Scan Mirror (IVBM),,Fig. 1 .,
Intelligent Virtual B-Scan Mirror (IVBM),,Fig. 2 .,
Intelligent Virtual B-Scan Mirror (IVBM),,Fig. 3 .,
Intelligent Virtual B-Scan Mirror (IVBM),,Fig. 4 .,
Intelligent Virtual B-Scan Mirror (IVBM),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_40.
Surgical Activity Triplet Recognition via Triplet Disentanglement,1,Introduction,"Surgical video activity recognition has become increasingly crucial in surgical data science with the rapid advancement of technology [1][2][3]. This important task provides comprehensive information for surgical workflow analysis [4][5][6][7] and surgical scene understanding [8][9][10], which supports the implementation of safety warning and computer-assisted systems in the operating room [11]. One of the most popular surgical procedures worldwide is laparoscopic cholecystectomy [12,13], which is in high demand for the creation of an effective computer-assisted system. Therefore, automated surgical activity triplet recognition is increasingly essential, and learning-based methods are promising solutions to address this need.Most current works in the field of surgical video analysis primarily focus on surgical phase recognition [6,[14][15][16][17][18]. However, only a small portion of the literature is dedicated to surgical activity recognition. The first relevant study [19] dates back to 2020, in which the authors built a relevant dataset and proposed a weakly supervised detection method that uses a 3D interacting space to identify surgical triplets in an end-to-end manner. An updated version Rendezvous (RDV) [20] employs a Transformer [21] inspired semantic attention module in their end-to-end network. Later this method is extended to include the temporal domain, named Rendezvous in Time (RiT) [22] with a Temporal Attention Module (TAM) to better integrates the current and past features of the verb at the frame level. Significantly, benchmark competitions such as the Cholectriplet2021 Challenge [28] have garnered interest in surgical action triplet recognition, with its evaluation centering on 94 valid triplet classes while excluding 6 null classes.Although significant progress has been made in surgical activity triplet recognition, they suffer from the same limitation of ambiguous supervision from the triplet components. Learning to predict the triplet in one shot is a highly imbalanced process, as only samples with correct predictions across all components are considered positive. This objective is particularly challenging in the following scenarios. Firstly, multiple surgical activities may occur in a single frame (see Fig. 1), with instruments appearing at the edge of the video or being obscured or overlapped, making it difficult to focus on them. Secondly, similar instruments, verbs, and targets that do not need to be recognized can have a detrimental impact on the task. As illustrated in the second row of Fig. 1, an obvious movement of the clipper may be labeled as null in the dataset because it is irrelevant to the recognition task. However, this occurrence is frequent in real situations, and labeling all surgical activities is time-consuming. Furthermore, not all surgical activities are indispensable, and some only appear in rare cases or among surgeons' habits. Moreover, the labels of instruments and triplets for this task are binary, and when multiple duplicate instruments or triplets appear, recognizing them directly only determines whether their categories have appeared or not. To improve recognition results, these factors should also be considered. Lastly, most of the previous methods are end-to-end multi-task learning methods, which means that training may be distracted by other auxiliary tasks and not solely focused on the key task.To solve the above problems, we propose a triplet disentanglement framework for surgical activity triplet recognition. This approach decomposes the learning objectives, thereby reducing the complexity of the learning process. As stated earlier, surgical activity relies on the presence of tools, making them crucial to our mission. Therefore, our approach concentrates on a simplified numerical representation as a means of mitigating these challenges. Initially, we face challenges such as multiple tools appearing at the same time or irrelevant surgical activities. Therefore, we adopt an intuitive approach to first identify the number/category of tools and whether those activities occur or not, instead of directly recognizing the tool itself. This numerical recognition task helps our network roughly localize the tool's location and differentiate irrelevant surgical activities. Subsequently, we employ a weakly supervised method to detect the tools' locations. However, unlike [20], we extend our architecture to 3D networks to better capture temporal information. Our approach separates different types of instruments using the class activation map (CAM) [23] based on the maximum number of identified instrument categories, allowing our model to slightly minimize the probability of mismatching and reduce the learning difficulty after separation when multiple surgical activities occur simultaneously. Additionally, we propose a hierarchical training schedule that decomposes our tasks into several sub-tasks, starting from easy to hard. This approach improves the efficiency of each individual task and makes training easier.In summary, this work makes the following contributions: 1) We propose a triplet disentanglement framework for surgical action triplet recognition, which decomposes the learning objectives in endoscopic videos. 2) By further exploit- ing the knowledge of decomposition, our network is extended to a 3D network to better capture temporal information and make use of temporal class activation maps to alleviate the challenge of multiple surgical activities occurring simultaneously. 3) Our experimental results on the endoscopic video dataset demonstrate that our approaches surpass current state-of-the-art methods."
Surgical Activity Triplet Recognition via Triplet Disentanglement,2,Methods,"Our objective is to recognize every triplet at each frame in each video. Let X = {X 1 , ..., X n } be the frames of endoscopic videos and Y = {Y 1 , ..., Y n } be the set of labels of triplet classes where n is the number of frames and the sets of triplet classes. Moreover, each set of labels of triplet classes can be denoted as Y = {Y I , Y V , Y T } where I, V, and T are indicated as Instrument, Verb, and Target. Figure 2 shows the overview of our method."
Surgical Activity Triplet Recognition via Triplet Disentanglement,2.1,Sampling Video Clips,"Unlike RDV [20] method which identifies surgical activities in individual frames, the video is cut into different segments to identify them. At the beginning of our process, short video clips are obtained from lengthy and differently-sized videos. Each sample clip drawn from the source videos is represented as X s ∈ H×W ×3×M , while X t represents a sample clip from the target. It should be noted that all sample clips have identical specifications, namely a fixed height H, a fixed width W, and a fixed number of frames M."
Surgical Activity Triplet Recognition via Triplet Disentanglement,2.2,Generating Soft Labels,"We generate four different soft labels for the number of instruments, the number of instrument categories, the presence of an unrelated surgical activity, and the presence of a critical surgical activity, which are labeled as <N I , N C , U A , S A > respectively. As for N I , our goal is to know the number of instruments occurrences because the appearance of an instrument also means the appearance of a surgical activity. For N C , it is employed to differentiate situations where multiple instruments of the same type are present. In terms of the other two soft labels, they are binary labels, which refer to presence or absence, respectively. In addition, labels with the format <instrument, null, null > are marked as irrelevant surgical activity. In contrast, those with the format <instrument, verb, target> are marked as critical surgical activity. For example, in the case shown in the second line of Fig. 1, our soft label will be marked as <2, 2, presence, presence>."
Surgical Activity Triplet Recognition via Triplet Disentanglement,2.3,Disentanglement Framework,"Our encoder backbone is built on the RGB-I3D [26] network (preserving the temporal dimension), which is pre-trained on both ImageNet [25] and Kinetics dataset [26]. Prior to training, each video is segmented into T non-overlapping segments consisting of precisely 5 frames, which are then fed into the soft label network to recognize their corresponding soft labels. This allows our network to more easily identify the location of the tool and to differentiate between crucial and irrelevant actions, which ultimately aids the network's comprehension of triplet associations. Once the network is trained, its parameters are stored and transferred for initialization to the backbones of other networks. In the second stage, we divide our triplet recognition task into three sub-networks: the tool network, verb network, and target branch network. Notably, our backbone operates at the clip level, whereas RDV is frame-based. The features extracted by the backbone from video clips are fed into the three sub-networks. The tool classifier recognizes the tool class and generates its corresponding Class Activation Map (CAM). The features of the verb and target networks, along with the CAMs of the corresponding tool network, are then passed into the Class Activation Guided Attention Mechanism (CAGAM) module [20]. Our CAGAM, a dualpath position attention mechanism, leverages the tool's saliency map to guide the location of the corresponding verb and target. Subsequently, the individual predictions of the three components are generated, and the last objective is to learn their association. Therefore, the CAMs and logits of the three components are aligned into the triplet network to learn their association and generate the final triplet prediction. It is important to note that the tool network, verb-target networks, and triplet network are all initialized by the soft label network and contain branches to predict soft labels."
Surgical Activity Triplet Recognition via Triplet Disentanglement,2.4,Hierarchical Training Schedule,"Our framework is a multi-task recognition approach, but training so many tasks simultaneously poses a huge challenge to balancing hyper-parameters. To address this, we propose a hierarchical training schedule method that divides training into different stages. Initially, we train only our soft label network to recognize soft labels, storing its parameters once training is complete. In the next stage, video clips are fed into the tool network to recognize tool categories while simultaneously identifying soft labels. After successful training, the parameters of the tool network are frozen. In the subsequent stage, the verb and target networks identify their respective components and soft labels. At this point, the tool network passes its class activation map to the verb-target networks without updating its parameters. Besides, following previous Tripnet [20], which masks out impossible results, we also mask out improbable outcomes for different components. For instance, predefined masks for the tool are based on possible combinations, while the verb and target masks follow the tool's predictions, excluding scissors and clippers; subsequently, the masking results undergo further refinement for the instrument. Finally, we train the final triplet network using the CAMs and output logits of the three components. Similarly, the parameters of the three components' networks are not updated at this stage. This approach allows us to break down the complexity of the task and improve the accuracy of each individual component at each stage, ultimately leading to higher overall accuracy."
Surgical Activity Triplet Recognition via Triplet Disentanglement,2.5,Separation Processing,"Our framework provides a unique approach to address the impact of multiple tool categories that may be present simultaneously. It enables the handling of different surgical instrument categories individually, which reduces the level of complexity for learning. In contrast to RDV [20] and RiT [22], we extend the Grad-CAM [27] approach to 3D-CNN by utilizing the input of 3D tensor data instead of a 2D matrix, resulting in a more precise class activation map. Based on the maximum number of instrument categories (K) predicted by the soft label and the scores obtained by summing each channel along the CAM, we isolate the top-K CAMs. Each isolated CAM is then used to guide the corresponding features of verbs and targets in our CAGAM module to generate individual predictions of the verb and target. Finally, the different predictions of the verb and target are combined. However, differentiating between various instruments, as we do, can help match them with the correct verbs and targets, especially when multiple tool categories appear at the same time, which may be challenging to do without this approach, such as in the RDV method [20]."
Surgical Activity Triplet Recognition via Triplet Disentanglement,2.6,Loss Function,"For the number of tools and the categories of tools, softmax cross-entropy loss is adopted, while for other soft labels, three components, and triplet, we employ sigmoid cross-entropy losses. Taking sigmoid cross-entropy loss as an example:where σ is the sigmoid function, while y c and ŷc are the ground truth label and the prediction for specific class c. Besides, the balanced weights are also adopted based on previous works [20]."
Surgical Activity Triplet Recognition via Triplet Disentanglement,3,Experiments,"Dataset. Following previous works [20], an endoscopic video dataset of the laparoscopic cholecystectomy, called CholecT45 [20], is used for all the experiments. The database consists of a total of 45 videos and is stripped of triplet classes of little clinical relevance. There are 6 instruments, 10 verbs, 15 targets, and 100 triplet classes in the CholecT45 dataset. For all the videos, each triplet class is always presented in the format of <instrument, verb, target>. In addition, following the data splits in [24], the K-fold cross-validation method was used to divide the 45 videos in the dataset into 5 folds, each containing 9 videos.Metric. The performance of the method is evaluated based on the mean average precision (mAP) metric to predict the triplet classes. In testing, each triplet class is computed its own average precision (AP) score, and then the AP score of a video will be calculated by averaging the AP scores of all the triplets in this video. Finally, the mean average precision (mAP) of the dataset is measured by averaging the AP scores of all tested videos. Besides, Top-N recognition performance is also adopted in our evaluation, which means that given a test sample X i , a model made a correctness if the correct label y i appears in its top N confident predictions Ŷi . We follow previous works [20] and measure top-5, top-10, and top-20 accuracy in our experiment. Implementation Details. I3D (Resnet50) [26] is adopted as a backbone network in our framework, which is pre-trained on the ImageNet [25] and the Kinetics dataset [26]. As for the branches of the different networks, they will be slightly modified to fit their corresponding subtasks. We use SGD as an optimizer and apply a step-wise learning rate of 1e-3 for all sub-tasks, but for the soft-labeled task branches that need to be finetuned, their learning rates start from 1e-6. Our batch size is 32, and there is no additional database for surgical detection."
Surgical Activity Triplet Recognition via Triplet Disentanglement,3.1,Results and Discussion,"Quantitative Evaluation. We demonstrate our experimental results with other state-of-the-art approaches on the CholecT45 [20] dataset.Table 1 presents the benchmark results on the CholecT45 cross-validation split, and compares our model with current state-of-the-art methods [19,20,22] using the mean Average Precision (mAP) metric. The key mission of surgical activity recognition is to calculate the average precision (AP) score of the triplet, denoted as AP IV T . We present tool recognition, verb recognition, and target recognition as AP I , AP V , and AP T , respectively. Compared to previous methods [19,20], our hierarchical training schedule method enables us to achieve  better results on individual sub-tasks because we can tune each task individually to achieve the best results. For AP IV T , our framework improves the current SOTA method RiT [22] by 4.1%, demonstrating that decomposing our tasks helps to improve the final result. In addition, we include Fig. 3 in our paper to illustrate the qualitative and representative results obtained from our triplet activity recognition model. Although we are unable to compare our results with other methods as their trained models have not been released, our method can successfully predict many difficult situations, such as the simultaneous appearance of multiple surgical activities, the influence of brightness, and the tool located at the edge. Following the previous experiments [20], we compare the Top N accuracy of the triplet predictions with different methods. However, the previous method [20] performed this metric only on the CholecT50 [20] dataset, and this dataset has not been published yet. Besides, the source codes of RiT [22] have not been released yet. However, the performance between RDV and RiT is very close according to Table 1. Hence, we try to reproduce the RDV method on this metric. As shown in Table 2, our framework outperforms the RDV method [20] by 8.1%, 5.9% and 2.0% in top-5, top-10, and top-20 respectively.Ablation Study. In this section, we conduct ablation studies to showcase the effectiveness of each module in our model. As shown in Table 3, the final triplet prediction outcomes experienced a slight decrease of 0.4% and 1.1% in the absence of the separation process or the soft label module, respectively. Additionally, a marginal decrease was observed in the individual sub-tasks. In conclusion, these results demonstrate that the inclusion of these modules can contribute to the overall performance of our model. "
Surgical Activity Triplet Recognition via Triplet Disentanglement,4,Conclusion,"In this paper, we introduce a novel triplet disentanglement framework for surgical activity recognition. By decomposing the task into smaller steps, our method demonstrates improved accuracy compared to existing approaches. We anticipate that our work will inspire further research in this area and promote the development of more efficient and accurate techniques."
Surgical Activity Triplet Recognition via Triplet Disentanglement,,Fig. 1 .,
Surgical Activity Triplet Recognition via Triplet Disentanglement,,Fig. 2 .,
Surgical Activity Triplet Recognition via Triplet Disentanglement,,Fig. 3 .,
Surgical Activity Triplet Recognition via Triplet Disentanglement,,Table 1 .,TTripnet[19] 89.9 ± 1.0 59.9 ± 0.9 37.4 ± 1.5 24.4 ± 4.7 Attention Tripnet [20] 89.1 ± 2.1 61.2 ± 0.6 40.3 ± 1.2 27.2 ± 2.7 RDV [20] 89.3 ± 2.1 62.0 ± 1.3 40.0 ± 1.4 29.4 ± 2.8 RiT [22] 88.6 ± 2.6 64.0 ± 2.5 43.4 ± 1.4 29.7 ± 2.6 Our method 91.2 ± 1.9 65.3 ± 2.8 43.7 ± 1.6 33.8 ± 2.5
Surgical Activity Triplet Recognition via Triplet Disentanglement,,Table 2 .,
Surgical Activity Triplet Recognition via Triplet Disentanglement,,Table 3 .,
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,1,Introduction,"Flexible ureteroscopy (FURS) is a routinely performed surgical procedure for renal lithotripsy. This procedure inserts a flexible ureteroscope through the blad-der and ureters to get inside the kidneys for diagnosis and treatment of stones and tumors. Unfortunately, such an examination and treatment depends on skills and experiences of surgeons. On the other hand, surgeons may miss stones and tumors and unsuccessfully orientate the ureteroscope inside the kidneys due to limited field of views, just 2D images without depth information, and the complex anatomical structure of the kidneys. To this end, ureteroscope tracking and navigation is increasingly developed as a promising tool to solve these issues.Many researchers have developed various methods to boost endoscopic navigation. These methods generally consist of vision-and sensor-based tracking. Han et al. [3] utilized the porous structures in renal video images to develop a vision-based navigation method for ureteroscopic holmium laser lithotripsy. Zhao et al. [15] designed a master-slave robotic system to navigate the flexible ureteroscope. Luo et al. [7] reported a discriminative structural similarity measure driven 2D-3D registration for vision-based bronchoscope tracking. More recently, Huang et al. [4] developed an image-matching navigation system using shape context for robotic ureteroscopy. Additionally, sensor-based methods are widely sued in surgical navigation [1,6]. Zhang et al. [14] employed electromagnetic sensors to estimate the ureteroscope shape for navigation.Although these methods mentioned above work well, ureteroscopic navigation is still a challenging problem. Compared to other endoscopes such as colonoscope and bronchoscope, the diameter of the ureteroscope is smaller, resulting in more limited lighting source and field of view. Particularly, ureteroscopy involves much solids (impurities) and fluids (liquids), making ureteroscopic video images low-quality, as well as these solids and fluids inside the kidneys cannot be regularly observed in computed tomography (CT) images. On the other hand, the complex internal structures such as calyx, papilla, and pyramids of the kidneys are difficult to be observed in CT images. These issues introduce a difficulty in directly aligning ureteroscopic video sequences to CT images, leading to a challenge of image-based continuous ureteroscopic navigation.This work aims to explore an accurate and robust vision-based navigation method for FURS procedures without using any external positional sensors. Based on ureteroscopic video images and preoperative computed tomography urogram (CTU) images, we propose a novel video-CTU registration method to precisely locate the flexible ureteroscope in the CTU space. Several highlights of this work are clarified as follows. To the best of our knowledge, this work shows the first study to continuously track the flexible ureteroscope in preoperative data using a vision-based method. Technically, we propose a novel 2D-3D (video-CTU) registration method that introduces a structural point similarity measure without using image pixel intensity information to characterize the difference between the structural regions in real video images and CTU-driven virtual image depth maps. Additionally, our proposed method can successfully deal with solid and fluid ureteroscopic video images and attains higher navigation accuracy than intensity-based 2D-3D registration methods."
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,2,Video-CTU Registration,"Our proposed video-CTU registration method consists of several steps: (1) ureteroscopic structure extraction, (2) virtual depth map generation, and (3) structural point similarity and optimization. Figure 1 illustrates the flowchart of our method. "
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,2.1,Ureteroscopic Image Characteristics,"The internal kidneys consist of complex anatomical structures such as pelvis and calyx and also contain solid particles (e.g., stones and impurities) floating in fluids (e.g., water, urine and small blood), resulting in poor image quality during ureteroscopy. Therefore, it is a challenging task to extract meaningful features from these low-quality images for achieving accurate 2D-3D registration.Our idea is to introduce specific structures inside the kidneys to boost the video-CTU registration since these structural regions are meaningful features that can facilitate the similarity computation. During ureteroscopy, various anatomical structures observed in ureteroscopic video images indicate different poses of the ureteroscope inside the kidneys. While some structural features such as capillary texture and striations at the tip of the renal pyramids are observed ureteroscopic images, they are not discernible in CT or other preoperative data. Typical structural or texture regions (Columns 1∼3 in Fig. 2 (b)) observed both in ureteroscopic video and CTU images are the renal papilla when the ureteroscope gets into the kidneys through the ureter and renal pelvis to reach the major and minor calyxes. Additionally, we also find that the renal pelvis (dark or ultra-low light) regions (Columns 4∼6 in Fig. 2 (b)) are also useful to enhance the registration. Hence, this work employs these interior renal structural characteristics to calculate the similarity between ureteroscopic images and CTU."
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,2.2,Ureteroscopic Structure Extraction,"Deep learning is widely used for medical image segmentation. Lazo et al. [5] used spatial-temporal ensembles to segment lumen structures in ureteroscopic images. More recently, vision transformers show the potential to precisely segment various medical images [8,12]. This work employs the dense prediction transformer (DPT) [11] to extract these structural regions from ureteroscopic images. DPT is a general deep learning framework for dense prediction tasks such as semantic segmentation and has three versions of DPT-Base, DPT-Large, and DPT-Hybrid. This work use DPT-Base since it only requires a small number of parameters but provides a high inference speed. DPT-Base consists of a transformer encoder and a convolutional decoder. Its backbone is vision transformers [2], where input images are transformed into tokens by non-overlapping patches extraction, followed by a linear projection of their flattened representation. The conventional decoder employs a reassemble operation [11] to assemble a set of tokens into image-like feature representations at various resolutions:where s is the output size ratio of the feature representation and D is the output feature dimension. Tokens from layers l = {6, 12, 18, 24} are reassembled in DPT-Base. These feature representations are subsequently fused into the final dense prediction. In the structure extraction, we define three classes: Non-structural regions (background), structural regions, and stones. We manually select and annotate ureteroscopic video images for training and testing. Vision transformers require large datasets for training, so we initialize the encoder with weights pretrained on ImageNet and further train it on our in-house database. Figure 3 displays some segmentation results of ureteroscopic video images."
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,2.3,Virtual CTU Depth Map Generation and Thresholding,"This step is to compute depth maps of 2D virtual images generated from CTU images by volume rendering [13]. Depth maps can represent structural information of virtual images. Note that this work uses CTU to create virtual images since non-contrast CT images cannot capture certain internal structures of the kidneys, although these structures can be observed in ureteroscopic video images. We introduce a ray casting algorithm in volume rendering to generate depth maps of virtual rending images [13]. The ray casting algorithm is to trace a ray that starts from the viewpoint and passes through a pixel on the screen. When the tracing ray intersects with a voxel, the properties of that voxel will affect the value of corresponding pixel in the final image. For a 3D point (x 0 , y 0 , z 0 ) and a normalized direction vector (r x , r y , r z ) of its casting ray, a corresponding point (x, y, z) at any distance d on the tracing ray is: (x, y, z) = (x 0 + dr x , y 0 + dr y , z 0 + dr z ).(The tracing ray R(x, y, z, r x , r y , r z ) will stop when it encounters opaque voxels. For 3D point (x,y,z), its depth value V can be calculated by projecting the ray R(x, y, z, r x , r y , r z ) onto the normal vector N(x, y, z) of the image plane:where symbol • denotes the dot product.To obtain the depth map with structural regions, we define two thresholds t u and t v . Only CT intensity values within [t u , t v ] are opaque voxels that the casting rays cannot pass through in the ray-casting algorithm. According to CTU characteristics [10], this work uses our excretory-phase data to generate virtual images and set [t u , t v ] to [-1000, 120], where -1000 represents air and 120 was determined by the physician's experience and characteristics of contrast agents.Unfortunately, the accuracy of thresholded structural regions suffers from inaccurate depth maps caused by renal stones and contrast agents. Stones and agents are usually high intensity in CTU images, which result in incorrect depth information of structural regions (e.g., renal papilla). To deal with these issues, we use the segmented stones as a mask to remove these regions with wrong depth. On the other hand, the structural regions usually have larger depth values than the agent-contrasted regions. Therefore, we sort the depth values outside the mask and only use the thresholded structural regions with the largest depth values for the structural point similarity computation."
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,2.4,Structural Point Similarity and Optimization,"We define a point similarity measure between DPT-base segmented structural regions in ureteroscopic images and thresholded structural regions in virtual depth maps generated by the volume rendering ray casting algorithm.The structural point similarity function (cost function) is defined as an intersection of point sets from the extracted real and virtual structural regions: where I i is the ureteroscopic video image at frame i, point sets P i and P v are from the ureteroscopic image extracted structural region E i (a, b) and the thresholded structural region E v (a, b) ((a, b) denotes a point)from the depth map D v of the 2D virtual rendering image I v (p i , q i ), respectively:where (p i , q i ) is the endoscope position and orientation in the CTU space. Eventually, the optimal pose (p i , qi ) of the ureteroscope in the CTU space can be estimated by maximizing the structural point similarity:where Powell method [9] is used as an optimizer to run this procedure. "
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,3,Results and Discussion,"We validate our method on clinical ureteroscopic lithotripsy data with video sequences and CTU volumes. Ureteroscopic video images were a size of 400 × 400 pixels, while the space parameters of CTU volumes were 512 × 512 pixels, 361∼665 slices, 0.625∼1.25 mm slice thickness. Three ureteroscopic videos more than 30000 frames were acquired from three ureteroscopic procedures for experiments. While we manually annotated ureteroscopic video images for DPT-base segmentation, three experts also manually generated ureteroscope pose groundtruth data by our developed software, which can manually adjust position and direction parameters of the virtual camera to visually align endoscopic real images to virtual images, evaluating the navigation accuracy of the different methods.  Figure 4 illustrates the navigation results of segmentation, depth maps, extracted structural regions for similarity calculation, and generated 2D virtual images corresponding to estimated ureteroscope poses. Structural regions can be extracted from ureteroscopic images and virtual depth maps. Particularly, we can see that our method generated virtual images (Row 7 in Fig. 4) resemble real video images (Row 1 in Fig. 4) much better than Luo et al. [7] generated ones (Row 6 in Fig. 4). This implies that our method can estimate the ureteroscope pose much more accurate than Luo et al. [7]. Table 1 summarizes quantitative segmentation results and position and orientation errors. DPT-Base can achieve average segmentation IoU 88.34%, accuracy 92.26%, and DSC 93.71%. The average position and orientation errors of our method were 5.39 mm and 8.14 • , which much outperform the compared method. Table 2 shows the results of sensitivity analysis for the threshold values. It can be seen that inappropriate threshold selection can lead to an increase in errors. Figure 5 boxplots estimated position and orientation errors for a statistical analysis of our navigation accuracy."
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,,Luo et al. Ours,"The effectiveness of our proposed method lies in several aspects. First, renal interior structures are insensitive to solids and fluids inside the kidneys and can precisely characterize ureteroscopic images. Next, we define a structural point similarity measure as intersection of point sets between real and virtual structural regions. Such a measure does not use any point intensity information for the similarity calculation, leading to an accurate and robust similarity characterization under renal floating solids and fluids. Additionally, CTU images can capture more renal anatomical structures inside the kidneys compared to CT slices, still facilitating an accurate similarity computation.Our method still suffers from certain limitations. Figure 6 displays some ureteroscopic video images our method fails to track. This is because that the segmentation method cannot successfully extract structural regions, while the ray casting algorithm cannot correctly generate virtual depth maps with structural regions. Both unsuccessfully extracted real and virtual structural regions collapse the similarity characterization. We will improve the segmentation of ureteroscopic video images, while generating more ground-truth data for training and testing."
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,4,Conclusion,"This paper proposes a new 2D-3D registration approach for vision-based FURS navigation. Specifically, such an approach can align 2D ureteroscopic video sequences to 3D CTU volumes and successfully locate an ureteroscope into CTU space. Different from intensity-based cost function, a novel structural point similarity measure is proposed to effectively and robustly characterize ureteroscopic video images. The experimental results demonstrate that our proposed method can reduce the navigation errors from (11.28 mm, 10.8 • ) to (5.39 mm, 8.13 • )."
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,,Fig. 1 .,
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,,Fig. 2 .Fig. 3 .,
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,,Fig. 4 .,
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,,Fig. 5 .,
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,,Fig. 6 .,
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,,Table 1 .,
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,,Table 2 .,
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,1,Introduction,"Adolescent idiopathic scoliosis (AIS), the most prevalent form of spinal deformity among children, and some patients tend to worsen over time, ultimately leading to surgical treatment if not being treated timely [1]. Determining the observation interval is crucial for monitoring the likelihood of curve progression. Some scoliosis patients may progress within a short period. The gold standard for clinical diagnosis of scoliosis is the Cobb angle measured with X-ray imaging. However, the follow-up observation using X-ray requires an interval of 3-12 months because of the potential oncogenic effect of radiation exposure [2,3]. To reduce X-ray exposure and assess scoliosis frequently, coronal ultrasound imaging of spine formed by 3D ultrasound scanning has recently been developed and commercialized for scoliosis assessment [4]. This technique used a volume projection imaging (VPI) method to form the coronal ultrasound image, which contains the information of lateral curvature of spine (Fig. 1(a)) [5]. The ultrasound curve angle (UCA) can be obtained using the coronal ultrasound image of spine and has been demonstrated to be comparable with the radiographic Cobb angle [6]. However, clinicians hesitate to adopt this image modality since spinal images formed by VPI method are new to users, and the bone features look different from those in X-ray images (Fig. 1). If these bony features can be presented similarly to X-ray images, this radiation-free spine image will be more accepted. Moreover, such a conversion from ultrasound coronal images to X-ray-like images can not only help clinicians understand bone structure without any barrier, but also indirectly minimizes the patient's exposure to cumulative radiation.In previous studies, generative adversarial networks (GANs) have been widely used in medical image translation applications. Long et al. proposed an enhanced Cycle-GAN for integrated translation in ultrasound, which introduced a perceptual constraint to increasing the quality of synthetic ultrasound texture [7,8]. For scoliosis assessment, UXGAN has been added with an attention mechanism into the Cycle-GAN model to focus on the spine feature during modal transformation for translating coronal ultrasound image to X-ray image [9]. However, it worked well on mapping local texture but was not so good for the translation with more extensive geometric changes. Besides, it was only tested on patients with less than 20 • of scoliosis, thus limiting its application for scoliosis assessment.Diffusion and score-matching models, which have emerged as high-fidelity image generation models, have achieved impressive performance in the medical field [10,11]. Pinaya et al. adapted denoising diffusion probabilistic models(DDPM) for high-resolution 3D brain image generation [12]. Qing et al. investigated the performance of the DPM-based model with different sampling strategies for the conversion between CT and MRI [13]. Despite the achievements of all these previous works, there is no research on the diffusion model for converting coronal ultrasound images to X-ray-like images. So far, it is still a challenging topic because the difference in texture and shape between the two modalities are substantial.In this study, based on the conditional diffusion model, we design an ultrasound-to-X-ray synthesis network, which incorporates an angle-embedding transformer module into the noise prediction model. We have found that the guidance on angle information can rectify for offsets in generated images with different amounts of the inclination of the vertebrae. To learn the posterior probability of X-ray in actual Cobb angle conditions, we present a conditional consistency function to utilize UCA as the prior knowledge to approximate the objective distribution. Our contributions are summarized as follows:• We propose a novel method for the Us-to-X-ray translation using probabilistic denoising diffusion probabilistic model and a new angle embedding attention module, which takes UCA and Cobb angle as the prerequisite for image generation. • Our attention module facilitates the model to close the objective X-ray distribution by incorporating the angle and source domain information. The conditional consistency function ensures that the angle guidance flexibly controls the curvature of the spine during the reverse process. • correlation with authentic Cobb angle indicates its high potential in scoliosis evaluation."
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,2,Method,
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,2.1,Conditional Diffusion Models for U2X Translation,"Diffusion models are the latent variable model that attempts to learn the data distribution, followed by a Markovian process. The objective is to optimize the usual variational bound on negative log likelihood denoted as:p θ (x 0:T ) is the joint distribution called the reverse process. q(x 1:T |x 0 ) is the diffusion process, progressively adding Gaussian noise to the previous state of the system. Given x ∼ p(x) ∈ R W ×H be an X-ray image, conditional diffusion models tend to learn the data distribution in condition y. In this work, we partition y into the set of Cobb angle y and paired ultrasound imageThen the training objective can be reparameterized as the prediction of mean of noising data distribution with y at all timestep t:"
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,2.2,Attention Module with Angle Embedding,"Theoretically, if the denoising ˘ θ is correct, then as T → ∞, we can obtain X-ray images that the sample paths are distributed as p θ (x t-1 |x t , c = [y, x src ]). However, acquiring Cobb angles from real X-ray images is not feasible during the sampling stage. Accordingly, we propose an auxiliary model ˜ θ , taking estimated UCA as the prior knowledge, to approximate the objective distribution. Specifically, let ỹ be the UCA of ultrasound images (Fig. 2). This transformer-based module establishes the relationship between ỹ and y. We run a linear projection of dimension R dim over the two scalars of angle, and a learnable 1D position embedding is employed to them, representing their location information. Then we reshape them to the same dimension as image tokens. A shared weight attention module compute the self-attention after taking all image tokens, and the projection of ỹ or y as input. where A is the operation of a standard transformer encoder [14]. N is the patch number of the source image. Since the transformer can record the indexes of each token, we set the value of the patch of y to -1 when predicting the ˜ (c = [x src , ỹ, y = φ]), and vice versa. The dimension of output z matches the noise prediction model's input for the self-attention mechanism. Thus, the sequence of the patch of the angle and image can be entered point-wise into the denoising model."
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,2.3,Consistent Conditional Loss,"Training UXDiffusion. As described in Algorithm 1, the input to the denoising model ˜ θ and ˘ θ are the corrupt image, source image and the list of angle. Rather than learning in the direction of the gradient of the Cobb angle, the auxiliary denoising model instead predicts the score estimates of the posterior probability in UCA conditions for approximating the X-ray distribution under the actual Cobb angle condition. The bound can be denoted as KL(p θ (x t-1 |x t , ỹ, x src )||p θ (x t-1 |x t , y, x src )), where KL denotes Kullback-Leibler divergence. The reverse process mean comes from an estimate x θ (ỹ) ≈ x θ (y) plugged into q(x t-1 |x t , y). Note that the parameterization of the mean of distribution can be simplified to the noise prediction [15]. The posterior probability of X-ray in the condition of Cobb angle can be calculated by minimizing the conditional consistency loss defined as:(5)Sampling with UCA. We follow the sampling scheme in [16] to speed up the sampling process. The embedded ultrasound image and corresponding UCA are fed into the trained noise estimation network ˜ to sample the X-ray-like image iteratively."
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,3,Experiments,
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,3.1,Dataset,"The dataset consists of 150 paired coronal ultrasound and X-ray images. Each patient took X-ray radiography and ultrasound scanning at the same day. Patients with BMI indices greater than 25.0 kg/m 2 and patients with scoliosis angles exceeding 60 • were excluded from this study, as the 7.5 MHz ultrasound transducer could not penetrate well for those fatty body and the spine would deform and rotate too much with large Cobb angle, thus affecting ultrasound image quality. The Cobb angle was acquired from an expert with 15 years of experience on scoliosis radiographs, while the UCA was acquired by two raters with at least 5 years of evaluating scoliosis using ultrasound. We manually aligned and cropped the paired images to the same reference space. The criterion for registration was to align the spatial positions of the transverse and spinous processes and the ribs. We resized them into 256 × 512, and grouped the data by 90, 30 and 30 for training, validation and test, respectively."
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,3.2,Implementation Details,"We followed the same architecture of DDPM, using a U-Net network with attention blocks to predict . For the angle embedding attention module, we transformed the token in ViT [14] for classification into the tokens for the angle list.The transformer encoder blocks were used to compute the self-attention on the embedding angle and image tokens. We set T = 2000 and forward variances from 10 -4 to 0.02 linearly. We apply the exponential moving average strategy to update the model weights, the decay rate is 0.999. All the experiments were conducted on a 48GB NVIDIA RTX A6000 GPU."
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,3.3,Synthesis Performance,"In this section, three different types of generated models were chosen for comparison: 1) GAN-based model for paired image-to-image translation [17], because of the paired data we use; 2) GAN-based model for unpaired image-to-image translation [9], the first model to synthesize X-ray image from coronal ultrasound image, is based on CycleGAN; 3) Classifier-free conditional diffusion-based model [18], which proposed a conditional diffusion model toward medical image segmentation (Table 1). We could transform the model into our U2X task by replacing the segmentation image with the ultrasound image. The visualization comparison is presented in Fig. 4, and the quantitative comparison is demonstrated. Our proposed model has a higher quality in the generation of vertebral  contour edges compared to the baselines. Also, the synthesis images have the same spine curvature orientation as ground-true images, with high fidelity of structural information. Then, we measured the structural similarity (SSIM) and peak signal-to-noise ratio (PSNR) to evaluate the performance of the synthesized images. The results show that our model outperforms paired and unpaired GAN-based methods by 2.5-6.7 points on SSIM and is also better than the reference diffusion-based method. Our model achieves the highest value of PSNR along with the lowest standard deviation, showing the stability of the model. We believe that the diffusion-based baseline only considers the source images and disregards the scoliosis offset of the generated X-ray images, which can be addressed using angle-based guidance. Experimental results demonstrate that the performance of predicting the conditional distribution with angle guidance is superior to simply using the source image. "
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,3.4,Comparison with Cobb Angle,"Since the Cobb angle is widely used as the gold standard for scoliosis assessment, our objective is to synthesize an X-ray-like image that can be applied to the measurement of the Cobb angle. As depicted in Fig. 3, we measure the Cobb angle difference between the synthesized image and the original X-ray image. Then we use linear regression to study the correlation between the Cobb angles measured using the generated and original images and the Bland-Altman plot to analyze the agreement between the two Cobb angles. The result demonstrates that our model can generate images maintaining a high consistency in restoring the overall curvature of the bones. The coefficient of determination is R 2 = 0.8531(p < 0.001). The slope of the regression line is 45.17 • for all parameters, which is close to the ideal value of 45 • . Bland-Altman plots demonstrate the measured angle value difference between the generated and GT X-rays. The mean difference is 1.4 • for all parameters indicating that it is comparable with the ground-truth Cobb angle. The experiments demonstrate that the proposed model for calculating the Cobb angle is practical and can be used to evaluate scoliosis."
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,4,Conclusion,"This paper developed a synthesized model for translating coronal ultrasound images to X-ray-like images using a probabilistic diffusion network. Our purpose is to use a single network to parameterize the X-ray distribution, and the generated images can be applied to the Cobb angle measurement. We achieved this by introducing the angular information corresponding to ultrasound and X-ray image to the model for noise prediction. An attention module was proposed to guide the model for generating high-quality images based on embedded image and angle information. Furthermore, to overcome the unavailability of the Cobb angle in the sampling process, we presented a conditional consistency function to train the model to learn the gradient according to the UCA for approximating the X-ray distribution in the condition of Cobb angle. Experiments on paired ultrasound and X-ray coronal images demonstrated that our diffusionbased method advanced the state-of-the-art significantly. In summary, this new model has great potential to facilitate 3D ultrasound imaging to be used for scoliosis assessment with accurate Cobb angle measurement and X-ray-like images obtained without any radiation."
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,,Fig. 1 .,
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,,Fig. 2 .,
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,,Algorithm 1 . 2 / 9 :,
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,,Fig. 3 .,
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,,Fig. 4 .,
UXDiff: Synthesis of X-Ray Image from Ultrasound Coronal Image of Spine with Diffusion Probabilistic Network,,Table 1 .,
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,1,Introduction,"Automated surgical scene segmentation is an important prerequisite for contextaware assistance and autonomous robotic surgery. Recent work showed that deep learning-based surgical scene segmentation can be achieved with high accuracy [7,14] and even reach human performance levels if using hyperspectral imaging (HSI) instead of RGB data, with the additional benefit of providing functional tissue information [15]. However, to our knowledge, the important topic of geometric domain shifts commonly present in real-world surgical scenes (e.g., situs occlusions, cf. Fig. 1) so far remains unaddressed in literature. It is questionable whether the state-of-the-art (SOA) image-based segmentation networks in [15] are able to generalize towards an out-of-distribution (OOD) context. The only related work by Kitaguchi et al. [10] showed that surgical instrument segmentation algorithms fail to generalize towards unseen surgery types that involve known instruments in an unknown context. We are not aware of any investigation or methodological contribution on geometric domain shifts in the context of surgical scene segmentation. Generalizability in the presence of domain shifts is being intensively studied by the general machine learning community. Here, data augmentation evolved as a simple, yet powerful technique [1,16]. In deep learning-based semantic image segmentation, geometric transformations are most common [8]. This holds particularly true for surgical applications. Our analysis of the SOA (35 publications on tissue or instrument segmentation) exclusively found geometric (e.g., rotating), photometric (e.g., color jittering) and kernel (e.g., Gaussian blur) transformations and only in a single case elastic transformations and Random Erasing (within an image, a rectangular area is blacked out) [22] being applied. Similarly, augmentations in HSI-based tissue classification are so far limited to geometric transformations. To our knowledge, the potential benefit of complementary transformations proposed for image classification and object detection, such as Hide-and-Seek (an image is divided into a grid of patches that are randomly blacked out) [17], Jigsaw (images are divided into a grid of patches and patches are randomly exchanged between images) [2], CutMix (a rectangular area is copied from one image onto another image) [21] and CutPas (an object is placed onto a random background scene) [4] (cf. Fig. 2), remains unexplored.Given these gaps in the literature, the contribution of this paper is twofold:1. We show that geometric domain shifts have disastrous effects on SOA surgical scene segmentation networks for both conventional RGB and HSI data. 2. We demonstrate that topology-altering augmentation techniques adapted from the general computer vision community are capable of addressing these domain shifts."
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,2,Materials and Methods,"The following sections describe the network architecture, training setup and augmentation methods (Sect. 2.1), and our experimental design, including an overview of our acquired datasets and validation pipeline (Sect. 2.2)."
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,2.1,Deep Learning-Based Surgical Scene Segmentation,"Our contribution is based on the assumption that application-specific data augmentation can potentially address geometric domain shifts. Rather than changing the network architecture of previously successful segmentation methods, we adapt the data augmentation.Surgery-Inspired Augmentation: Our Organ Transplantation augmentation illustrated in Fig. 2 has been inspired by the image-mixing augmentation CutPas that was originally proposed for object detection [4] and recently adapted for instance segmentation [5] and low-cost dataset generation via image synthesis from few real-world images in surgical instrument segmentation [19]. It is based on placing an organ into an unusual context while keeping shape and texture consistent. This is achieved by transplanting all pixels belonging to one object class (e.g., an organ class or background) into a different surgical scene. Our selection of further computer vision augmentation methods that could potentially improve geometric OOD performance (cf. Fig. 2) was motivated by the specific conditions encountered in surgical procedures (cf. Sect. 2.2 for an overview). The noise augmentations Hide-and-Seek and Random Erasing black out all pixels inside rectangular regions within an image, thereby generating artificial situs occlusions. Instead of blacking out, the image-mixing techniques Jigsaw and CutMix copy all pixels inside rectangular regions within an image into a different surgical scene. We adapted the image-mixing augmentations to our segmentation task by also copying and pasting the corresponding segmentations. Hence, apart from occluding the underlying situs, image parts/organs occur in an unusual neighborhood.Network Architecture and Training: We used a U-Net architecture [13] with an efficientnet-b5 encoder [18] pre-trained on ImageNet data and using stochastic weight averaging [6] for both RGB and HSI data as it achieved human performance level in recent work [15]. As a pre-processing step, the HSI data was calibrated with white and dark reference images and 1 -normalized to remove the influence of multiplicative illumination changes. Dice and cross-entropy loss were equally weighted to compute the loss function. The Adam optimization algorithm [9] was used with an exponential learning rate scheduler. Training was performed for 100 epochs with a batch size of five images."
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,2.2,Experiments,"To study the performance of SOA surgical scene segmentation networks under geometric domain shifts and investigate the generalizability improvements offered by augmentation techniques, we covered the following OOD scenarios:(I) Organs in isolation: Abdominal linens are commonly used to protect soft tissue and organs, counteract excessive bleeding, and absorb blood and secretion. Some surgeries (e.g., enteroenterostomy), even require covering all but a single organ. In such cases, an organ needs to be robustly identified without any information on neighboring organs. (II) Organ resections: In resection procedures, parts or even the entirety of an organ are removed and surrounding organs thus need to be identified despite the absence of a common neighbor. (III) Occlusions: Large parts of the situs can be occluded by the surgical procedure itself, introducing OOD neighbors (e.g., gloved hands). The nonoccluded parts of the situs need to be correctly identified.Real-World Datasets: In total, we acquired 600 intraoperative HSI cubes from 33 pigs using the HSI system Tivita R Tissue (Diaspective Vision GmbH, Am Salzhaff, Germany). These were semantically annotated with background and 18 tissue classes, namely heart, lung, stomach, small intestine, colon, liver, gallbladder, pancreas, kidney with and without Gerota's fascia, spleen, bladder, subcutaneous fat, skin, muscle, omentum, peritoneum, and major veins. Each HSI cube captures 100 spectral channels in the range between 500nm and 1000nm at an image resolution of 640 × 480 pixels. RGB images were reconstructed by aggregating spectral channels in the blue, green, and red ranges. To study organs in isolation, we acquired 94 images from 25 pigs in which all but a specific organ were covered by abdominal linen for all 18 different organ classes (dataset isolation real ). To study the effect of occlusions, we acquired 142 images of 20 pigs with real-world situs occlusions (dataset occlusion), and 364 occlusion-free images (dataset no-occlusion). Example images are shown in Fig. 2.Manipulated Data: We complemented our real-world datasets with four manipulated datasets. To simulate organs in isolation, we replaced every pixel in an image I that does not belong to the target label l either with zeros or spectra copied from a background image. We applied this transformation to all images in the dataset original and all target labels l, yielding the datasets isolation zero and isolation bgr. Similarly, we simulated organ resections by replacing all pixels belonging to the target label l either with zeros or background spectra, yielding the datasets removal zero and removal bgr. Example images are shown in Fig. 2."
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,,Train-Test Split and Hyperparameter Tuning:,"The SOA surgical scene segmentation algorithms are based on a union of the datasets occlusion and no-occlusion, termed dataset original, which was split into a hold-out test set (166 images from 5 pigs) and a training set (340 images from 15 pigs). To enable a fair comparison, the same train-test split on pig level was used across all networks and scenarios. This also holds for the occlusion scenario, in which the dataset no-occlusion was used instead of original for training. All networks used the geometric transformations shift, scale, rotate, and flip from the SOA prior to applying the augmentation under examination. All hyperparameters were set according to the SOA. Only hyperparameters related to the augmentation under examination, namely the probability p of applying the augmentation, were optimized through a grid search with p ∈ {0.2, 0.4, 0.6, 0.8, 1}. We used five-fold-cross-validation on the datasets original, isolation zero, and isolation bgr to tune p such that good segmentation performance was achieved on both in-distribution and OOD data.Validation Strategy: Following the recommendations of the Metrics Reloaded framework [11], we combined the Dice similarity coefficient (DSC) [3] as an overlap-based metric with the boundary-based metric ormalized surface distance (NSD) [12] for validation for each class l. To respect the hierarchical test set structure, metric aggregation was performed by first macro-averaging the classlevel metric value M l (M ∈ {DSC, NSD}) across all images of one pig and subsequently across pigs. The organ removal experiment required special attention in this context, as multiple M l values per image could be generated corresponding to all the possible neighbour organs that could be removed. In this case, we selected for each l the minimum of all M l values, which corresponds to the segmentation performance obtained after removing the most important neighbour of l. The same class-specific NSD thresholds as in the SOA were used."
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,3,Results,"Effects of Geometric Domain Shifts: When applying a SOA segmentation network to geometric OOD data, the performance drops radically (cf. Fig. 3). Starting from a high DSC for in-distribution data (RBG: 0.83 (standard deviation (SD) 0.10); HSI: 0.86 (SD 0.10)), the performance drops by 10%-46% for RGB and by 5 %-45% for HSI, depending on the experiment. In the organ resection scenario, the largest drop in performance of 63% occurs for the gallbladder upon liver removal (cf. Suppl. Fig. 1). Similar trends can be observed for the boundary-based metric NSD, as shown in Suppl. Fig. 2.Performance of Our Method: Figure 3 and Suppl. Fig. 2 show that the Organ Transplantation augmentation (gold) can address geometric domain shifts for both the RGB and HSI modality. The latter yields consistently better results, indicating that the spectral information is crucial in situations with limited context. The performance improvement compared to the baseline ranges from 9 %-67% (DSC) and 15 %-79% (NSD) for RGB, and from 9%-90% (DSC) and 16 %-96% (NSD) for HSI, with the benefit on OOD data being largest for organs in isolation and smallest for situs occlusions. The Organ Transplantation augmentation even slightly improves performance on in-distribution data (original and no-occlusion). Upon encountering situs occlusions, the largest DSC improvement is obtained for the organ classes pancreas (283 %) and stomach (69 %). For organs in isolation, the performance improvement on manipulated data (DSC increased by 57% (HSI) and 61% (RGB) on average) is comparable to that on real data (DSC increased by 50% (HSI) and 46% (RGB))."
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,,Comparison to SOA Augmentations:,"There is no consistent ranking across all six OOD datasets except for Organ Transplantation always ranking first and baseline usually ranking last (cf. Fig. 4 for DSC-and Suppl. Fig. 3 for NSD-based ranking). Overall, image-mixing augmentations outperform noise augmentations. Augmentations that randomly sample rectangles usually rank better than comparable augmentations using a grid structure (e.g., CutMix vs. Jigsaw). "
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,4,Discussion,"To our knowledge, we are the first to show that SOA surgical scene segmentation networks fail under geometric domain shifts. We were particularly surprised by the large performance drop for HSI data, rich in spectral information. Our results clearly indicate that SOA segmentation models rely on context information. Aiming to address the lack of robustness to geometric variations, we adapted so far unexplored topology-altering data augmentation schemes to our target application and analyzed their generalizability on a range of six geometric OOD datasets specifically designed for this study. The Organ Transplantation augmentation outperformed all other augmentations and resulted in similar performance to in-distribution performance on real OOD data. Besides its effectiveness and computational efficiency, we see a key advantage in its potential to reduce the amount of real OOD data required in network training. Our augmentation networks were optimized on simulated OOD data, indicating that image manipulations are a powerful tool for judging geometric OOD performance if real data is unavailable, such as in our resection scenario, which would have required an unfeasible number of animals. With laparoscopic HSI systems only recently becoming available, the investigation and compensation of geometric domain shifts in minimally-invasive surgery could become a key direction for future research. Our proposed augmentation is model-independent, computationally efficient and effective, and thus a valuable tool for addressing geometric domain shifts in semantic scene segmentation of intraoperative HSI and RGB data. Our implementation and models will be made publicly available."
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,,Fig. 1 .,
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,,Fig. 2 .,
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,,Fig. 3 .,
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,,Fig. 4 .,
Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 59.
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,1,Introduction,"The technical advances in endoscopes have extended the diagnostic and therapeutic value of endoluminal interventions in a wide range of clinical applications. Due to the restricted field of view, it is challenging to control the flexible endoscopes inside the lumen. Therefore, the development of navigation systems, which locates the position of the end-tip of endoscopes and enables the depth-wise visualization, is essential to assisting the endoluminal interventions. A typical task is to visualize the depth-wise information and estimate the six-degree-of-freedom (6DoF) pose of endoscopic camera based on monocular imaging. Due to the clinical limitations, the ground truth of depth and pose trajectory of endoscope imaging is difficult to acquire. Previous works jointly estimated the depth and the pose via the unsupervised frameworks [2,8,10,12,17]. The main idea is modeling the differences of video frames with the Structure-from-Motion (SfM) mechanisms. In this framework [17], the image triplet, including a specific frame (i.e. target frame) and its temporal neighborhood (i.e. reference frames), is fed into the model to estimate the pose and depth. The model is optimized by minimizing the warping loss between the target frame and reference frames. Following the basic SfM method [17], scale-consistency [2], auto-masking [5], cost-volume [14] and optical flows [15] are also introduced to further improve the performance. These methods have also been applied to endoscopic scenarios with specific designs of attention modules [10] and priors from sparse depth [8]. Although the performance of SfM methods is promising, the intrinsic challenges of endoscopic data still require further consideration.The first issue is the insufficient visual diversity of endoscopic datasets. Compared with the large-scaled KITTI dataset [4] for general vision tasks, the collection of endoscopic datasets is challenged by the limited freedom of instruments and the scenarios. In this case, the visual variations of lumen structures and texture appearances cannot be fully explored. While road datasets mostly exhibit 3DOF motion(2DOF translation and 1DOF rotation in the road plane), endoscopy involves 6DOF motion within 3D anatomical structures. Therefore, SfM algorithms for endoscopic imaging are designed to estimate complicated trajectories with limited data diversity. General data augmentation methods, including random flipping and cropping, cannot generate synthesis with sufficient variance of camera views, and the realistic camera motion cannot be fully guaranteed. Recent works have tried to mimic the camera motion to improve the diversity of samples. For example, PDA [16] generated new samples for supervised depth estimation, and 3DCC [6] used synthetic samples to test the robustness of the models. However, the perspective view synthesis for unsupervised SfM framework, especially the transformation on image triplets is still underexplored.The second issue is the appearance inconsistency in image triplets. Due to the complicated environment of endoluminal structures, the illumination changes, motion blurness and specular artefacts are frequently observed in endoscopy images. The appearance-inconsistent area may generate substantial photometric losses even in the well-aligned adjacent frames. These photometric losses caused by the inconsistent appearance impede the training process and remain unable to optimize. To handle this problem, AF-SfMLearner [12] adopted the flow network to predict appearance flow to correct inconsistency between consecutive frames. RNNSLAM [9] used an encoder network to predict masks with supervised HSV signals from the original images. Consequently, these methods adopted auxiliary modules to handle the visual inconsistency, involving more parameters to learn.In this paper, we propose a triplet-consistency-learning framework (TCL) for unsupervised depth and pose estimation of monocular endoscopes. To improve the visual diversity of image triplets, the perspective view synthesis is introduced, considering the geometric consistency of camera motion. Specifically, the depthconsistency and pose-consistency are preserved via specific losses. To reduce the appearance inconsistency in the image triplets, a triplet-masking strategy is proposed by measuring the differences between the triplet-level and the framelevel representations. The proposed framework does not involve additional model parameters, which can be easily embedded into previous SfM methods. Experiments on public datasets demonstrate that TCL can effectively improve the accuracy of depth and pose estimation even with small amounts of training samples."
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,2,Methodology,
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,2.1,Unsupervised SfM with Triplet Consistency Learning,"Unsupervised SfM Method. The unsupervised SfM methods adopt Depth-Net and PoseNet to predict the depth and the pose, respectively. With depth and pose prediction, the reference frames I ri (i = 0, 1) are warped to the warped frames I ti (i = 0, 1). The photometric loss, denoted by L P , is introduced to measure the differences between I ti (i = 0, 1) and the target frame I t . The loss can be implemented by L 1 norm [17] and further improved with SSIM metrics [13].In addition to L P , auxiliary regularization loss functions, such as depth map smoothing loss [5] and depth scale consistency loss [2], are also introduced to improve the performance. In this work, these regularization terms are denoted by L Reg . Therefore, the final loss functions L of unsupervised methods can be summarized as follows:Previous unsupervised methods [2,5,17] have achieved excellent results on realistic road datasets such as KITTI dataset [4]. However, endoscopic datasets lack sufficient diversity of visual variations, and appearance inconsistency is also frequently observed in image triplets. Therefore, the unsupervised SfM methods based on L p require further considerations to address the issues above.Framework Architecture. As shown in Fig. 1, we propose a tripletconsistency-learning framework (TCL) based on unsupervised SfM with image triplets. TCL can be easily embedded into SfM variants without adding model parameters. To enrich the diversity of endoscopic samples, the Geometric Consistency module (GC) performs the perspective view synthesis method to generate synthesis triplets. Additionally, we introduce the Depth Consistent Loss L dc and the Pose Consistent Loss L pc to preserve the depth-consistency and the poseconsistency between raw and synthesis triplets. To reduce the affect of appearance inconsistency in the triplet, we propose Appearance Inconsistency module (AiC) where the Triplet Masks of reference frames are generated to reduce the inconsistent warping in the photometric loss."
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,2.2,Learning with Geometric Consistency,"Since the general data augmentation methods cannot generate synthesis with sufficient variance of camera views, 3DCC [6] and PDA [16] mimic the camera motion to generate the samples by applying perspective view synthesis. However, raw and novel samples are used separately in the previous works. To enrich the diversity of endoscopic datasets, we perform the synthesis on triplets. Furthermore, the loss functions are introduced to preserve the depth-consistency and the pose-consistency."
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,,Synthesis Triplet Generation.,"The perspective view synthesis method aims to warp the original image I to generate a new image I . The warping process is based on the camera intrinsic matrix K, the depth map D of the original image and perturbation pose P 0 . For any point q in I, its depth value is denoted as z in D. The corresponding point q on the new image I is calculated by Eq.( 2):Given the depth maps generated from a pre-trained model, we perform perspective view synthesis with the same pose transformation P 0 on the three frames of raw triplet respectively. Pose Consistent Loss. As shown in Fig. 1, the inner pose predictions of the adjacent frames of the raw and synthesis triplet are P tri (i = 0, 1) and P tri (i = 0, 1). Since the three frames of the synthesis triplet are warped from the same pose perturbation P 0 , the inner poses of the synthesis triplet remain the same as the raw triplet. To preserve inner pose-consistency between raw and synthesis triplets, we propose Pose Consistent Loss L pc , defined as the weighted L1 loss function of the inner pose prediction of the raw and synthesis triplets. Specifically, we use the translational t tri , t tri ∈ R 3×1 (i = 0, 1) and rotational R tri , R tri ∈ R 3×3 (i = 0, 1) components of the pose transformation matrix to calculate L pc . We weight the translation component by λ pt , Pose Consistent Loss is written as Eq.( 4)."
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,2.3,Learning with Appearance Inconsistency,"The complicated environment of endoluminal structures may cause appearance inconsistency in image triplets, leading to the misalignment of reference and target frames. Previous works [9,12] proposed auxiliary modules to handle the appearance inconsistency, involving more parameters of models. Since the movement of camera is normally slow, the same appearance inconsistency is unlikely to exist multiple times within an endoscopic image triplet. Therefore, we can measure the differences between the triplet-level and the frame-level representations to eliminate the appearance inconsistency.Specifically, for the selected triplet [ I r0 , I t , I r1 ], two warped frames [ I t0 , I t1 ] are generated from two reference frames. To obtain the frame-level representations, we used the encoder of DepthNet to extract the feature maps of the three frames [ I t0 , I t , I t1 ] respectively. The feature maps are upsampled to the size of the original image, denoted by [F r0 , F t , F r1 ]. As in Eq.( 5), the triplet-level representations F R are generated by the weighted aggregation of the feature maps, which is dominated by the feature of the target frame with weight λ t . To measure the differences between the triplet-level and the frame-level representations, we calculate feature difference maps Df i (i = 0, 1) by weighting direct subtraction and SSIM similarity [13] with weight λ sub . The Triplet Mask of each reference frame is generated by reverse normalizing the difference map to [β, 1].where N (a,b) (•) normalizes the input to the range [a, b]."
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,2.4,Overall Loss,"The final loss of TCL L t is formulated as follows:where denotes that the Triplet Mask MT i (i = 0, 1) is applied to the photometric loss calculation of the two reference frames respectively. The final photometric loss is obtained by averaging the photometric losses of the two reference frames after applying MT i (i = 0, 1). λ d , λ p are weights of L dc and L pc . Since the early adoption of L dc and L pc may lead to overfitting, the DepthNet and PoseNet are warmed up with N w epochs before adding the two loss functions. The synthesis method may inherently generate invalid (black) areas in the augmented samples. This arises from the single-image-based augmentation process, which lacks the additional information to fill the new areas generated from the viewpoint transformation. The invalid regions should be masked in the related loss functions."
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,3,Experiments,"Dataset and Implementation Details. The public datasets, including SCARED [1] with ground truth of both depth and pose and SERV-CT [3] with only depth ground truth, were used to evaluate the proposed method. Following the settings in [12], we trained on SCARED and tested on SCARED and SERV-CT. The depth metrics (Abs Rel, Sq Rel, RMSE, RMSE log and δ), and pose metrics (ATE, t RP E , r RP E ) were used to measure the difference of predictions and the ground truth1 . ATE is noted as the weighted average of the RMSE of sub-trajectories, and the rest metrics are noted as Mean ± Standard Deviation of error. We implemented networks using PyTorch [11] and trained the networks on 1 NVIDIA RTX 3090 GPU with Adam [7] for 100 epochs with a learning rate of 1e -4 , dropped by a scale factor of 10 after 10 epochs. Given the SCARED dataset, we divided 5/2/2 subsets for training/validation/testing. We finally obtained ∼ 8k frames for training. The batch size was 12 and all images were downsampled to 256 × 320. λ pt , λ t , λ sub , β, λ d , λ p , N w in the loss function were empirically set to 1, 0.5, 0.2, 0.5, 0.001, 0.5, 5 which were tuned on validation set. For more details of the experiments, please refer to the supplementary material."
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,,Results.,"To evaluate the effectiveness of the proposed method, TCL was applied to MonoDepth2 [5] and SC-SfMLearner [2] by exactly using the same architectures of DepthNet and PoseNet. For comparisons, SfMLearner [17], Mon-oDepth2 [5], AF-SfMLearner [12], SC-SfMLearner [2] and Endo-SfMLearner [10] were adopted as baseline methods. Specifically, AF-SfMLearner improved the MonoDepth2 by predicting the appearance flow, while Endo-SfMLearner is the alternative of SC-SfMLearner with better model architectures.Table 1 presents the quantitative results on SCARED and SERV-CT. After TCL is applied to the series baselines(MonoDepth2 and SC-SfMLearner), most depth and pose metrics are significantly improved, and most metrics achieve the best performance in their series. Our method not only outperforms the series baseline on SERV-CT, but also achieves all the best values of the MonoDepth2-Based and SC-SfMLearner-Based series. For visualization and further ablations, we present the results of TCL applied to MonoDepth2, which is denoted as Ours below. Figure 2(a) presents the comparisons of the depth prediction. In the first image, our method predicts the depth most accurately. In the second image, the ground truth reveals that the dark area in the upper right corner is closer, and only our method accurately predicts this area, resulting in a detailed and accurate full-image depth map. Despite the improvement, our prediction results still remain inaccurate compared to the ground truths. Figure 2(b) visualizes the trajectory and the projection on three planes. Our predicted trajectory is the closest to the ground truth compared with baselines. The SfMlearner predicts almost all trajectories as straight lines, a phenomenon also observed in [10], in which case a low r RP E metric is ineffective.Ablations on Proposed Modules. We introduce two proposed modules to the baseline (MonoDepth2) separately. We additionally propose a simple version of AiC that computes Triplet Masks directly using two reference frames without warping, denoted as AiC*. From Table 2, GC and AiC can significantly improve the effect of the baseline when introduced separately. AiC outperforms AiC* as the warping process can provide greater benefits for pixel-level alignment. Figure 3(a) intuitively demonstrates the effect of the proposed Triplet Mask(AiC), which effectively covers pixels with apparent appearance inconsistency between reference and target frames.Ablations on Different Dataset Amounts. To verify the effect of our proposed method on different amounts of training and validation sets, we utilize the main depth metric RMSE and pose metric ATE for comparison. In Fig. 3(b), our method achieves significant improvements over MonoDepth2 with different dataset amounts. The performance of our approach is almost optimal for depth and pose at 11k and 8k training samples, respectively. Therefore, our proposed framework has the potential to effectively enhance the performance of various unsupervised SfM methods, even with limited training data.  "
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,4,Conclusion,"We present a triplet-consistency-learning framework (TCL) to improve the effect of monocular endoscopy unsupervised depth and pose estimation. The GC module generates synthesis triplets to increase the diversity of the endoscopic samples. Furthermore, we constrain the depth and pose consistency using two loss functions. The AiC module generates Triplet Mask(MT) based on the triplet information. MT can effectively mask the appearance inconsistency in the triplet, which leads to more efficient training of the photometric loss. Extensive experiments demonstrate the effectiveness of TCL, which can be easily embedded into various SfM methods without additional model parameters."
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,,Fig. 1 .,
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,,Fig. 2 .,
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,,Table 2 .Fig. 3 .,
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,,,"Then we obtain the synthesis triplet [I r0 , I t , I r1 ]. Selected triplet [ I r0 , I t , I r1 ] is randomly selected from raw and synthesis triplets as the unsupervised training triplet to calculate L in Eq.(1). Depth Consistent Loss."
TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope,,Table 1 .,
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,1,Introduction,"Colorectal cancer (CRC) is the third most commonly diagnosed cancer but ranks second in terms of mortality worldwide [11]. Intestinal lesions, particularly polyps and adenomas, are usually developed to CRC in many years. Therefore, diagnosis and treatment of colorectal polyps and adenomas at their early stages are essential to reduce morbidity and mortality of CRC. Interventional colonoscopy is routinely performed by surgeons to visually examine colorectal lesions. However these lesions in colonoscopic images are easily omitted and wrongly classified due to limited knowledge and experiences of surgeons. Automatic and accurate segmentation is a promising way to improve colorectal examination.Many researchers employ U-shaped network [7,13,18] for colonoscopic polyp segmentation. ResUNet++ [7] combines residual blocks and atrous spatial pyramid pooling and Zhao et al. [18] designed a subtraction unit to generate the difference features at multiple levels and constructed a training-free network to supervise polyp-aware features. Unlike a family of U-Net driven segmentation methods, numerous papers have been worked on boundary constraints to segment colorectal polyps. Fan et al. [2] introduced PraNet with reverse attention to establish the relationship between boundary cues from global feature maps generated by a parallel partial decoder. Both polyp boundary-aware segmentation methods work well but still introduce much false positive. Based on PraNet [2] and HardNet [1], Huang et al. [6] removed the attention mechanism and replaced Res2Net50 by HardNet to build HardNet-MSEG that can achieve faster segmentation. In addition, Kim et al. [9] modified PraNet to construct UACANet with parallel axial attention and uncertainty augmented context attention to compute uncertain boundary regions. Although PraNet and UACANet aim to extract ambiguous boundary regions from both saliency and reverse saliency features, they simply set the saliency score to 0.5 that cannot sufficiently detect complete boundaries to separate foreground and background regions. More recently, Shen et al. [10] introduced task-relevant feature replenishment networks for crosscenter polyp segmentation, while Tian et al. [12] combined transformers and multiple instance learning to detect polyps in a weakly supervised way.Unfortunately, limited field of view and illumination variations usually result in insufficient boundary contrast between intestinal lesions and their surrounding tissues. On the other hand, various polyps and adenomas with different pathological features have similar visual characteristics to intestinal folds. To address these issues mentioned above, we explore a new deep learning architecture called cascade transformer encoded boundary-aware multibranch fusion (CTBMF) networks with cascade transformers and multibranch fusion for polyp and adenoma segmentation in colonoscopic white-light and narrow-band video images. Several technical highlights of this work are summarized as follows. First, we construct cascade transformers that can extract global semantic and subtle boundary features at different resolutions and establish weighted links between global semantic cues and local spatial ones for intermediate reasoning, providing long-range dependencies and a global receptive field for pixel-level segmentation. Next, a hybrid spatial-frequency loss function is defined to compensate for loss features in the spatial domain but available in the frequency domain. Additionally, we built a new colonoscopic lesion image database and will make it publicly available, while this work also conducts a thorough evaluation and comparison on our new database and four publicly available ones (Fig. 2).  "
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,2,Approaches,"This section details our CTBMF networks that can refine inaccurate lesion location, rough or blurred boundaries, and unclear textures. Figure 1 illustrates the encoder-decoder architecture of CTMBF with three main modules."
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,2.1,Transformer Cascade Encoding,"This work employs a pyramid transformer [15] to build a transformer cascaded encoder. Let X 0 and X i be input patches and the feature map at stage i, respectively. Overlapping patch embedding (OPE) separates an image into fixed-size patches and linearly embeds them into tokenized images while making adjacent windows overlap by half of a patch. Either key K i or value V i is the input sequence of linear spatial reduction (LSR) that implements layer normalization (LN) and average pooling (AP) to reduce the input dimension:where Ω(•) denotes the output parameters of position embedding, ⊕ is the element-wise addition, W Ki indicates the parameters that reduces the dimension of K i or V i , and R i is the reduction ratio of the attention layers at stage i.As the output of LSR is fed into multihead attention, we can obtain attention feature map A j i from head j (j = 1, 2, • • • , N, N is the head number of the attention layer) at stage i:where Attention(•) is calculated as the original transformer [14]. Subsequently, the output LSRAwhere is the concatenation and W Ai is the linear projection parameters. Then,where DC is a 3 × 3 depth-wise convolution [5] with padding size of between the fully-connected (FC) layer and the Gaussian error linear unit (GELU) [4] in the feed-forward networks. Eventually, the output feature map X i of the pyramid transformer at stage i can be represented by"
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,2.2,Boundary-Aware Multibranch Fusion Decoding,"Boundary-Aware Attention Module. Current methods [2,9] detect ambiguous boundaries from both saliency and reverse-saliency maps by predefining a saliency score of 0.5. Unfortunately, a predefined score cannot distinguish foreground and background of different colonoscopic lesions [3]. Based on [17], this work explores an effective boundary-aware attention mechanism to adaptively extract boundary regions. Given the feature map X i with semantic cues and rough appearance details, we perform convolution (Conv) on it and obtain Xi = Conv(X i ), which is further augmented by channel and spatial attentions. The channel attention performs channel maxpooling (CMP), multilayer perceptron (MLP), and sigmoid (SIG) to obtain the intermediate feature map Y i :where ⊗ indicates the elementwise product. Subsequently, the detail enhanced feature map Z i of the channel-spatial attention iswhere SMP indicates spatial maxpooling. We subtract the feature map Xi from the enhanced map Z i to obtain the augmented boundary attention map B i , and also establish the correlation between the neighbor layers X i+1 and X i to generate multilevel boundary map G i :where and US indicate subtraction and upsampling.Residual Multibranch Fusion Module. To highlight salient regions and suppress task-independent feature responses (e.g., blurring), we linearly aggregate B i and G i to generate discriminative boundary attention map D i :where i = 1, 2, 3 and RELU is the rectified linear unit function.We obtain the fused feature representation map M i (i = 1, 2, 3, 4) from the elementwise addition or summation of M i+1 , D i , and the residual feature Xi byEventually, the output M 1 of the boundary-aware multibranch fusion decoder is represented by the following equation:which precisely combines global semantic features with boundary or appearance details of colorectal lesions."
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,2.3,Hybrid Spatial-Frequency Loss,"This work proposes a hybrid spatial-frequency loss function H L to train our network architecture for colorectal polyp and adenoma segmentation:where S L and F L are a spatial-domain loss and a frequency-domain loss to calculate the total difference between prediction P and ground truth G, respectively. The spatial-domain loss S L consists of a weighted intersection over union loss and a weighted binary cross entropy loss [16].The frequency-domain loss F L can be computed by [8] where W × H is the image size, λ is the coefficient of F L , G(u, v) and P(u, v) are a frequency representation of ground truth G and prediction P using 2-D discrete Fourier transform. γ(u, v) is a spectrum weight matrix that is dynamically determined by a non-uniform distribution on the current loss of each frequency."
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,3,Experiments,"Our clinical in-house colonoscopic videos were acquired from various colonoscopic procedures under a protocol approved by the research ethics committee of the university. These white-light and narrow-band colonoscopic images contain four types of colorectal lesions with different pathological features classified by surgeons: (1) 268 cases of hyperplastic polyp, (2) 815 cases of inflammatory polyp, (3) 1363 cases of tubular adenoma, and (4) 143 cases of tubulovillous adenoma. Additionally, four public datasets including Kvasir, ETIS-LaribPolypDB, CVC-ColonDB, and CVC-ClinicDB were also used to evaluate our network model. We implemented CTBMF on PyTorch and trained it with a single NVIDIA RTX3090 to accelerate the calculations for 100 epochs at mini-batch size 16. Factors λ (Eq. ( 14)) were set to 0.1. We employ the stochastic gradient descent  algorithm to optimize the overall parameters with an original learning rate of 0.0001 for cascade transformer encoding and 0.05 for other parts and use warmup and linear decay strategies to adjust it. The momentum and weight decay were set as 0.9 and 0.0005. Further, we resized input images to 352 × 352 for training and testing and the training time was nearly 1.5 h to achieve the convergence. We employ three metrics to evaluate the segmentation: Dice similarity coefficient (DSC), intersection over union (IoU), and weighted F-measure (F β ).  "
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,4,Results and Discussion,"Figure 3 visually compares the segmentation results of the four methods tested on our in-house and public databases. Our method can accurately segment polyps in white-light and narrow-band colonoscopic images under various scenarios, and CTBMF can successfully extract small, textureless and weak boundary and colorectal lesions. The segmented boundaries of our method are sharper and clear than others especially in textureless lesions that resemble intestinal lining.Figure 4 shows the DSC-boxplots to evaluate the quality of segmented polyps and adenomas, which still demonstrate that our method works much better than the others. Figure 5 displays the enhanced feature maps using the boundary-aware attention module. Evidently, small and weak-boundary or textureless lesions can be enhanced with good boundary feature representation. Table 1 summarizes the quantitative results in accordance with the three metrics and computational time of four methods. Evidently, CTBMF generally works better than the compared methods on the in-house database with four types of colorectal lesions. Furthermore, we also summarizes the average three metrics computed from all the five databases (the in-house dataset and four public datasets). Our method attains much higher average DSC and IoU of (0.870, 0.805) than the others on the five databases. We performed an ablation study to evaluate the effectiveness of each module used in CTBMF. The baseline is the standard version of cascade pyramid transformers. Modules D 1 , D 2 , D 3 , residual connections, and frequency loss F L are gradually added into the baseline, evaluating the effectiveness of each module and comparing the variants with each other. We tested these modules on the four public databases. Table 2 shows all the ablation study results. Each module can improve the segmentation performance. Particularly, the boundary-aware attention module critically improves the average DSC, IoU, and F β .Our method generally works better than the other three methods. Several reasons are behind this. First, the cascade-transformer encoder can extract local and global semantic features of colorectal lesions with different pathological characteristics due to its pyramid representation and linear spatial reduction attention. While the pyramid operation extracts multiscale local features, the attention mechanism builds global semantic cues. Both pyramid and attention strategies facilitate the representation of small and textureless intestinal lesions in encoding, enabling to characterize the difference between intestinal folds (linings) and subtle-texture polyps or small adenomas. Next, the boundary-aware attention mechanism drives the multibranch fusion, enhancing the representation of intestinal lesions in weak boundary and nonuniform lighting. Such a mechanism first extracts the channel-spatial attention feature map, from which subtracts the current pyramid transformer's feature map to enhance the boundary information. Also, the multibranch fusion generates multilevel boundary maps by subtracting the next pyramid transformer's upsampling output from the current pyramid transformer's output, further improving the boundary contrast. Additionally, the hybrid spatial-frequency loss was also contributed to the improvement of colorectal lesion segmentation. The frequency-domain information can compensate loss feature information in the spatial domain, leading to a better supervision in training."
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,5,Conclusion,"This work proposes a new deep learning model of cascade pyramid transformer encoded boundary-aware multibranch fusion networks to automatically segment different colorectal lesions of polyps and adenomas in colonoscopic imaging. While such an architecture employs simple and convolution-free cascade transformers as an encoder to effectively and accurately extract global semantic features, it introduces a boundary-aware attention multibranch fusion module as a decoder to preserve local and global features and enhance structural and boundary information of polyps and adenomas, as well as it uses a hybrid spatialfrequency loss function for training. The thorough experimental results show that our method outperforms the current segmentation models without any preprocessing. In particular, our method attains much higher accuracy on colonoscopic images with small, illumination changes, weak-boundary, textureless, and motion blurring lesions, improving the average dice similarity coefficient and intersection over union from (89.5%, 84.1%) to (90.3%, 84.4%) on our in-house database, from (78.9%, 72.6%) to (83.4%, 76.5%) on the four public databases, and from (84.3%, 78.4%) to (87.0%, 80.5%) on the five databases."
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,,Fig. 1 .,
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,,Fig. 2 .,
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,,Fig. 3 .,
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,,Fig. 4 .,
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,,Fig. 5 .,
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,,Table 1 .,CTBMF (Ours) 0.870 0.805 0.846 33.1 FPS 33.6 FPS 33.4 FPS
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,,Table 2 .,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,1,Introduction,"Automation in robot-assisted minimally invasive surgery (RMIS) may reduce human error that is linked to fatigue, lack of attention and cognitive overload [8]. It could help surgeons operate such systems by reducing the learning curve [29]. And in an ageing society with shrinking workforce, it could help to retain accessibility to healthcare. It is therefore expected that parts of RMIS will be ultimately automated [5,30]. On the continuous transition towards different levels of autonomy, camera motion automation is likely to happen first [14]. Initial attempts to automate camera motion in RMIS include rule-based approaches that keep surgical tools in the center of the field of view [4,9,21].The assumption that surgical tools remain centrally is, however, simplistic, as in many cases the surgeon may want to observe the surrounding anatomy to decide their next course of action.Contrary to rule-based approaches, data-driven methods are capable to capture more complex control policies. Example data-driven methods suitable for camera motion automation include reinforcement learning (RL) and imitation learning (IL). The sample inefficiency and potential harm to the patient currently restrict RL approaches to simulation [1,22,23], where a domain gap remains. Work to bridge the domain gap and make RL algorithms deployable in real setups have been proposed [3,20], but clinical translation has not yet been achieved. For IL, on the other hand, camera motion automation could be learned from real data, thereby implicitly tackling the domain-gap challenge. The downside is that sufficient data may be difficult to collect. Many works highlight that lack of expert annotated data hinders progress towards camera motion automation in RMIS [7,13,19]. It is thus not surprising that existing literature on IL for camera motion automation utilizes data from mock setups [12,26].Recent efforts to make vast amounts of laparoscopic intervention videos publicly available [19] drastically change how IL for camera motion automation can be approached. So far, this data is leveraged mainly to solve auxiliary tasks that could contribute to camera motion automation. As reviewed in [18], these tasks include tool and organ segmentation, as well as surgical phase recognition. For camera motion automation specifically, however, there exist no publicly available image-action pairs. Some work, therefore, continues to focus on the tools to infer camera motion [15], or learns on a robotic setup altogether [17] where camera motion is accessible. The realization, however, that camera motion is intrinsic to the videos of laparoscopic interventions and that camera motion could be learned on harvested actions was first realized in [11], and later in [16]. This comes with the additional advantage that no robot is necessary to learn behaviors and that one can directly learn from human demonstrations.In this work, we build on [11] for computationally efficient image-action pair extraction from publicly available datasets of laparoscopic interventions, which yields more than 20× the amount of data that was used in the closed source data of [16]. Contrary to [16], our camera motion extraction does not rely on image features, which are sparse in surgical videos, and is intrinsically capable to differentiate between camera and object motion. We further propose a novel importance sampling and data augmentation step for achieving camera motion automation IL."
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,2,Materials and Methods,"The proposed approach to learning camera motion prediction is summarized in Fig. 1. The following sections will describe its key components in more detail. Training pipeline, refer to Sect. 2.3. From left to right: Image sequences are importance sampled from the video database and random augmentations are applied per sequence online. The lower branch estimates camera motion between subsequent frames, which is taken as pseudo-ground-truth for the upper branch, which learns to predict camera motion on a preview horizon."
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,2.1,Theoretical Background,"Points on a plane, as observed from a moving camera, transform by means of the 3 × 3 projective homography matrix G in image space. Thus, predicting future camera motion (up to scale) may be equivalently treated as predicting future projective homographies.It has been shown in [6] that the four point representation of the projective homography, i.e., taking the difference between four points in homogeneous coordinatesis better behaved for deep learning applications than the 3 × 3 matrix representation of a homography. Therefore, in this work, we treat camera motion C as a sequence of four point homographies on a time horizon [T 0 , T N +M ), N being the recall horizon's length, M being the preview horizon's length. Time points lie Δt apart, that is T i+1 = T i + Δt. For image sequences of length N+M, we work with four point homography sequences"
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,2.2,Data and Data Preparation,"Three datasets are curated to train and evaluate the proposed method: two cholecystectomy datasets (laparoscopic gallbladder removal), namely Cholec80 [25] and HeiChole [27], and one hysterectomy dataset (laparoscopic uterus removal), namely AutoLaparo [28].To remove status indicator overlays from the laparoscopic videos, which may hinder the camera motion estimator, we identify the bounding circle of the circular field of view using [2]. We crop the view about the center point of the bounding circle to a shape of 240 × 320, so that no black regions are prominent in the images.All three datasets are split into training, validation, and testing datasets. We split the videos by frame count into 80 ± 1% training and 20 ± 1% testing. Training and testing videos never intersect. We repeat this step to further split the training dataset into (pure) training and validation datasets.Due to errors during processing the raw data, we exclude videos 19, 21, and 23 from HeiChole, as well as videos 22, 40, 65, and 80 from Cholec80. This results in dataset sizes of: Cholec80 -4.4e6 frames at 25 fps, HeiChole -9.5e5 frames at 25 fps, and AutoLaparo -7.1e4 frames at 25 fps."
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,2.3,Proposed Pipeline,"Video Database and Importance Sampling. The curated data from Sect. 2.2 is accumulated into a video database. Image sequences of length N + M are sampled at a frame increment of Δn between subsequent frames and with Δc frames between the sequence's initial frames. Prior to adding the videos to the database, an initial offline run is performed to estimate camera motion Δuv between the frames. This creates image-motion correspondences of the form (I n , I n+Δn , Δuv n ). Image-motion correspondences where E(||Δuv n || 2 ) > σ, with sigma being the standard deviation over all motions in the respective dataset, define anchor indices n. Image sequences are sampled such that the last image in the recall horizon lies at index n = N -1, marking the start of a motion. The importance sampling samples indices from the intersection of all anchor indices, shifted by -N , with all possible starting indices for image sequences.Geometric and Photometric Transforms. The importance sampled image sequences are fed to a data augmentation stage. This stage entails geometric and photometric transforms. The distinction is made because downstream, the pipeline is split into two branches. The upper branch serves as camera motion prediction whereas the lower branch serves as camera motion estimation, also refer to the next section. As it acts as the source of pseudo-ground-truth, it is crucial that the camera motion estimator performs under optimal conditions, hence no photometric transforms, i.e. transforms that change brightness/contrast/fog etc., are applied. Photometrically transformed images shall further be denoted as Ĩ. To encourage same behavior under different perspectives, geometric transforms are applied, i.e. transforms that change orientation/up to down/left to right etc. Transforms are always sampled randomly, and applied consistently to the entire image sequence. "
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,,Camera Motion,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,3,Experiments and Evaluation Methodology,"The following two sections elaborate the experiments we conduct to investigate the proposed pipeline from Fig. 1 in Sect. 2.3. First the camera motion estimator is investigated, followed by the camera motion predictor."
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,3.1,Camera Motion Estimator,"Camera Motion Distribution. To extract the camera motion distribution, we run the camera motion estimator from [11] with a ResNet-34 backbone over all datasets from Sect. 2.2. We map the estimated four point homographies to up/down/left/right/zoom-in/zoom-out for interpretability. Left/right/up/down corresponds to all four point displacements Δuv consistently pointing left/right/ up/down respectively. Zoom-in/out corresponds to all four point displacements Δuv consistently pointing inwards/outwards. Rotation left corresponds to all four point displacements pointing up right, bottom right, and so on. Same for rotation right. Camera motion is defined static if it lies below the standard deviation in the dataset. The frame increment is set to 0.25 s, corresponding to Δn = 5 for the 25 fps videos.Online Camera Motion Estimation. Since the camera motion estimator is executed online, memory footprint and computational efficiency are of importance. Therefore, we evaluate the estimator from [11] with a ResNet-34 backbone, SURF & RANSAC, and LoFTR [24] & RANSAC. Each estimator is run 1000 times on a single image sequence of length N +M = 15 with an NVIDIA GeForce RTX 2070 GPU and an Intel(R) Core(TM) i7-9750H CPU @ 2.60 GHz."
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,3.2,Camera Motion Predictor,"Model Architecture. For all experiments, the camera motion predictor is a ResNet-18/34/50, with the number of input features equal to the recall horizon N × 3 (RGB), where N = 14. We set the preview horizon M = 1. The frame increment is set to 0.25 s, or Δn = 5 for the 25 fps videos. The number of frames between clips is also set to 0.25 s, or Δc = 5.Training Details. The camera motion predictor is trained on each dataset from Sect. 2.2 individually. For training on Cholec80/HeiChole/AutoLaparo, we run 80/50/50 epochs on a batch size of 64 with a learning rate of 2.5e -5/1.e -4/1.e -4. The learning rates for Cholec80 and HeiChole relate approximately to the dataset's training sizes, see Table 2. For Cholec80, we reduce the learning rate by a factor 0.5 at epochs 50, 75. For Heichole/AutoLaparo we drop the learning rate by a factor 0.5 at epoch 35. The loss in Fig. 1 is set to the mean pairwise distance between estimation and prediction E(||Δ ũv t -Δuv t || 2 )+λE(||Δ ũv t || 2 ) with a regularizer that discourages the identity Δ ũv t = 0 (i.e. no motion). We set λ = 0.1.Evaluation Metrics. For evaluation we compute the mean pairwise distance between estimated and predicted motion E(||Δ ũv t -Δuv t || 2 ). All camera motion predictors are benchmarked against a baseline, that is a O(1)/O(2)-Taylor expansion of the estimated camera motion Δuv t . Furthermore, the model that is found to perform best is evaluated on the multi-class labels (left, right, up, down) that are provided in AutoLaparo."
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,4,Results,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,4.1,Camera Motion Estimator,"Camera Motion Distribution. The camera motion distributions for all datasets are shown in Fig. 2. It is observed that for a large fraction of the sequences there is no significant camera motion (Cholec80 76.21%, HeiChole 76.2%, AutoLaparo 71.29%). This finding supports the importance sampling that was introduced in Sect. 2.3. It can further be seen that e.g. left/right and up/down motions are equally distributed. Fig. 2. Camera motion distribution, refer to Sect. 3.1. AutoLaparo: 2.81% -up, 1.88%down, 4.48% -left, 3.38% -right, 0.45% -zoom_in, 0.2% -zoom_out, 0.3% -rotate_left 0.3%, -rotate_right 14.9% -mixed, 71.29% -static.Online Camera Motion Estimation. The results of the online camera motion estimation are summarized in Table 1. The deep homography estimation with a Resnet34 backbone executes 11× quicker and has the lowest GPU memory footprint of the GPU accelerated methods. This allows for efficient implementation of the proposed online camera motion estimation in Fig. 1."
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,4.2,Camera Motion Prediction,"The camera motion prediction results for all datasets are highlighted in Table 2. It can be seen that significant improvements over the baseline are achieved on the Cholec80 and HeiChole datasets. Whilst the learned prediction performs better on average than the baseline, no significant improvement is found for the AutoLaparo dataset. The displacement of the image center point under the predicted camera motion for AutoLaparo is plotted against the provided multi-class motion annotations and shown in Fig. 3. It can be seen that the camera motion predictions align well with the ground truth labels.     "
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,5,Conclusion and Outlook,"To the best of our knowledge, this work is the first to demonstrate that camera motion can indeed be learned from retrospective videos of laparoscopic interventions, with no manual annotation. Self-supervision is achieved by harvesting image-motion correspondences using a camera motion estimator, see Fig. 1. The camera motion predictor is shown to generate statistically significant better predictions over a baseline in Table 2 as measured using pseudo-ground-truth and on multi-class manually annotated motion labels from AutoLaparo in Fig. 3. An exemplary image sequence in Fig. 4 demonstrates successful camera motion prediction on HeiChole. These results were achieved through the key finding from Fig. 2, which states that most image sequences, i.e. static ones, are irrelevant to learning camera motion. Consequentially, we contribute a novel importance sampling method, as described in Sect. 2.3. Finally, we hope that our open-source commitment will help the community explore this area of research further.A current limitations of this work is the preview horizon M of length 1. One might want to extend it for model predictive control. Furthermore, to improve explainability to the surgeon, but also to improve the prediction in general, it would be beneficial to include auxiliary tasks, e.g. tool and organ segmentation, surgical phase recognition, and audio. There also exist limitations for the camera motion estimator. The utilized camera motion estimator is efficient and isolates object motion well from camera motion, but is limited to relatively small camera motions. Improving the camera motion estimator to large camera motions would help increase the preview horizon M .In future work, we will execute this model in a real setup for investigating transferability. This endeavor is backed by [10], which demonstrates how the learned homography could immediately be deployed on a robotic laparoscope holder. It might proof necessary to fine-tune the presented policy through reinforcement learning with human feedback."
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,,Fig. 1 .,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,,,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,,( a ),
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,,Fig.,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,,Fig. 4 .,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,,Table 1 .,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,,Table 2 .,
Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning,,,"left/right, the center point is predicted to move left/right and for up/down labels, the predicted left/right motion is centered around zero (a). Same is observed for up/down motion in (b), where left/right motion is zero-centered."
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,1,Introduction,"Augmentation of intra-operative X-ray images using the pre-operative data (e.g., treatment plan) has the potential to reduce procedure time and improve patient outcomes in minimally invasive procedures. However, surgeons must rely on their clinical knowledge to perform a mental mapping between pre-and intraoperative information, since the pre-and intra-operative images are based on different coordinate systems. To utilize pre-operative data efficiently, accurate pose estimation of the X-ray source relative to pre-operative images (or called registration between pre-and intra-operative data) is necessary and beneficial to relieve surgeon's mental load and improve patient outcomes.Although 2D/3D registration methods for medical images have been widely researched and systematically reviewed [10,17], developing an automatic endto-end registration method remains an open issue. For conventional intensitybased registration, it is formulated as an iterative optimization problem based on similarity measures. A novel similarity measure called weighted local mutual information is proposed to perform solid vascular 2D-3D registration [11] but has a limited capture range, becoming inefficient and prone to local minima if initial registration error is large. A good approach that directly predicts the spatial mapping relationship between simulated X-rays and real X-rays using a neural network is put forward [12] but requires an initialization. Some supervised learning tasks, such as anatomical landmark detection [1,3], are defined to develop a robust initialization scheme. When used to initialize an optimizer [14] for refinement, they can lead to a fully automatic 2D/3D registration solution [3]. But extensive manual annotation [1,3] or pairwise clinical data [12] is needed for training, which is time-and labor-consuming when expanding to new anatomies, and the robustness of these methods might be challenged due to the neglect of patient's anatomical specificity [1]. A patient-specific landmark refinement scheme is then proposed, which contributes to model's robustness when applied intraoperatively [2]. Nevertheless, the final performance of this automatic registration method still relies on conventional refinement step based on derivative-free optimizer (i.e., BOBYQA), which limits the computational efficiency of deep learning-based registration. Meanwhile, some studies employ regression neural networks to directly predict slice's pose relative to pre-operative 3D image data [4,8,15]. While the simplicity of these approaches is appealing, the applicability of these methods is constrained by their performance and are more suitable as initialization.In this paper, we propose a purely self-supervised and patient-specific end-toend framework for fully automatic registration of single-view X-ray to preoperative CT. Our main contributions are as follows: (1) The proposed method eliminates the need for manual annotation, relying instead on self-supervision from simulated patient-specific X-rays and corresponding automatically labeled poses, which makes the registration method easier to extend to new medical applications. (2) Regularized autoencoder and multi-head self-attention mechanism are embedded to encourage the model to capture patient-specific salient information automatically, therefore improving the robustness of registration; and an novel refinement model is proposed to further improve registration accuracy.(3) The proposed method has been successfully evaluated on X-rays, achieving an average run-time of around 2.5 s, which meets the requirements for clinical applications. "
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,2,Method,"An overview of the proposed framework is illustrated in Fig. 1, which can be mainly divided into two parts that are introduced hereafter."
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,2.1,Pose Regression Model,"Network Architecture. A regression neural network architecture is developed to estimate the six degrees of freedom (DOF) pose from an input X-ray image, which consists of a regularized autoencoder, a multi-head self-attention block (MHSAB), a learnable 2D position embedding, and a multilayer perceptron (MLP), as shown in Fig. 1f .Regularized Autoencoder. The autoencoder consists of a carefully selected backbone and a decoder. The first six layers of EfficientNet-b0 [16] are chosen as the backbone, where the first convolution layer is adapted by setting the input channel to 1 (shown as blue trapezoidal block in Fig. 1f ). For image-based pose estimation, edge, as an important structural information, is more advantageous than intensity value. Hence, a structure aware loss function L s is defined based on zero-normalized cross-correlation(ZNCC) to constrain the features extracted by encoder to contain necessary structure information, which is formulated as,Here, Y is the output of the decoder, I is the input image, G I is the normalized gradient of the input image, and N is the number of pixels in the input image. E() is the expectation operator, and σ is standard deviation operator.Multi-head Self-attention Block and Positional Embedding. First, inspired by [19], a 3 × 1 convolutional layer, a 1 × 3 convolutional layer, and a 1 × 1 convolutional layer are used to generate the query (q ∈ R n×hw×C/n ), key (k ∈ R n×hw×C/n ), and value (v ∈ R n×hw×C/n ) representations in the features (f ∈ R h×w×C ) extracted by backbone respectively, which capture the edge information in the horizontal and vertical orientation. And the attention weight (A ∈ R n×hw×hw ) is computed by measuring the similarity between q and k according to,where n is the number of heads, C is number of channels, and h, w are the height and width of features. Using the computed attention weights, the output of MHSAB f a is computed as,Second, in order to make the proposed method more sensitive to spatial transformation, 2D learnable positional embedding is employed to explicitly incorporate the object's position information.Incremental Learning Strategy. Based on Incremental Learning strategy, the network is trained for 100 epochs with a batch size of 32 using the Adam optimizer (learning rate is 0.002, decay of 0.5 per 10 epochs). After training for 40 epochs, the training dataset will be automatically updated if the loss computed on the validation set does not change frequently (i.e., less than 10 percent of the maximum of loss) within 20 epochs, which allows the network to observe a wider range of poses while avoiding over-fitting. Final loss function is a weighted sum of the structure aware loss L s for autoencoder and L2 Loss between the predicted pose and the ground truth, which is formulated as,where α is set as 5, I is the input DRR image, p is the ground truth of pose, P (I) is predicted pose, and D(I) is the output of autoencoder.Pre-processing and Data Augmentation. In order to improve the contrast of the digitally reconstructed radiographs (DRRs) and reduce the gap to real X-rays, the DRR is first normalized by Z-score and then normalized using the sigmoid function, which maps the image into the interval [0, 1] with a mean of 0.5. Then contrast-limited adaptive histogram equalization (CLAHE) is employed to enhance the contrast. For data augmentation, random brightness adjustment, random adding offset, adding Gaussian noise, and adding Poisson noise are adopted."
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,2.2,Refinement Model,"A novel refinement model is proposed to further refine the pose predicted by regression network as shown in Fig. 1(d). Specifically, inspired by DeepDRR [18], a differentiable DeepDRR (Diff-DeepDRR) generator is developed. Our proposed Diff-DeepDRR generator offers two main advantages compared with DeepDRR, including the ability to generate a 256 2 pixel DRR image in just 15.6 ms, making it fast enough for online refinement, and providing a way to obtain gradients with respect to the pose parameters. The refinement model takes the input Xray image with an unknown pose as the fixed image and has six learnable pose parameters initialized by the prediction of pose regression network. Then the proposed Diff-DeepDRR generator utilizes the six learnable pose parameters to generate DRR image online, which is considered as the moving image. The ZNCC is then used as the loss function to minimize the distance between the fixed image and the moving image. With the powerful PyTorch auto-grad engine, the six pose parameters are learned iteratively. For each refinement process, the refinement model is online optimized for 100 iterations using an Adam optimizer with a learning rate of 5.0 for translational parameters and 0.05 for rotational parameters, and outputs the pose with the minimal loss score.3 Experiments"
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,3.1,Dataset,"Our method is evaluated on six DRR datasets and one X-ray dataset. The six DRR datasets are generated from five common preoperative CTs and one specific CT with previous surgical implants, while the X-ray dataset is obtained from a Pelvis phantom containing a metal bead landmark inside. The X-ray dataset includes a CBCT volume and ten X-ray images with varying poses. For each CT or CBCT, 12800 DRR images with a reduced resolution of 256 2 pixels are generated using the DeepDRR method, which are divided into training (50%), validation (25%), and test (25%) sets. The DRR generation system is configured based on the geometry of a mobile C-arm, with a 432 mm detector, 0. "
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,3.2,Experimental Design and Evaluation Metrics,"To better understand our work, we conduct a series of experiments, for example, 1) A typical pose regression model consists of a CNN backbone and an MLP. To find an efficient backbone for this task, EfficientNet, ResNet [5], and DenseNet [6] were studied. 2) A detailed ablation experiment was performed, where you can learn the evolution process and the superiority of our proposed method. 3) Through the experiment on X-ray data, you can know whether the proposed method trained only on DRRs can be generalized to X-ray applications.To validate the performance of our method on DRR datasets, we employed five measurements including 2D mean projection distance (mPD), 3D mean target registration error (mTRE) [7], Mean Absolute Error (MAE), and success rate (SR) [2], where success is defined as an mTRE of less than 10 mm. Cases with mTRE exceeding 10 mm were excluded from average mPD and mTRE measurements. For the validation on X-rays, projection distance (PD) is used to measure the positional difference of the bead between the X-ray and the final registered DRR. In addition, NCC [13], structural similarity index measure (SSIM), and contrast-structure similarity (CSS) [9] are employed to evaluate the similarity between the input X-ray and final registered DRR."
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,4,Results,
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,4.1,The Choice of Backbone and Ablation Study,"The performance of three models with different backbones were evaluated on the first DRR dataset as reported in Table 1. Res-backbone [15] means that the first four layers of ResNet-50 as the backbone; Dense-backbone means that the first six layers of DenseNet-121 as the backbone; Ef-backbone means that the first six layers of EfficientNet-b0 as the backbone. Compared with the other two networks, Ef-backbone achieves a higher SR of 88.66%, and reduces parameter size and FLOPs by an order of magnitude. Therefore, Ef-backbone is chosen and regarded as baseline for further studies. Then, a second set of models was trained and evaluated for a detailed ablation study, and the experimental results are shown in Table 2. A means the aforementioned baseline; B means adding regularized autoencoder; C means adding multi-head self-attention block and position embedding; D means using refinement model; * means the network is trained via Incremental Learning strategy. It is clear that each module we proposed plays a positive role in this task, and our method makes arresting improvements, e.g., compared with baseline, the SR increased from 88.66% to 100%. In addition, observing the visualized self-attention map shown in Fig. 2(b), we find that the proposed network does automatically capture some salient anatomical regions, which pays more attention to bony structures. Meanwhile, the distribution of attention changes with the view pose, and it seems that the closer to the detector, the bone region gets more attention."
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,4.2,Experimental Results on DRR Datasets and X-Ray Dataset,"The experimental results of our proposed method on six DRR datasets are shown in Table 2. It is worth noticing that the success rate of our method has achieved 100% on all datasets and the average of mTRE on six datasets achieves 2.67 mm.  For the X-ray dataset, the quantitative evaluation results on 10 X-ray cases are shown in Table 3, achieving a mPD of 1.55 mm, which demonstrates that our method can be successfully generalized to X-ray. More intuitive results are shown in Fig. 2. Observing the dotted circled areas on Fig. 2(e&f), we find that the network only makes coarse predictions for X-rays due to unavoidable artifacts on the CBCT boundaries, and the proposed refinement model facilitates accurate pose estimation, which can be confirmed by the overlapping of edges from the X-rays and the final registered DRRs."
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,5,Conclusion and Discussion,"In this paper, we present a patient-specific and self-supervised end-to-end approach for automatic X-ray/CT rigid registration. Our method effectively addresses the primary limitations of existing methods, such as requirement of manual annotation, dependency on conventional derivative-free optimization, and patient-specific concerns. When field of view of CT is not smaller than that of X-ray, which is often satisfied in clinical routines, our proposed method would perform very well without any additional post-process. The quantitative and qualitative evaluation results of our proposed method illustrates its superiority and its ability to generalize to X-rays even when trained solely on DRRs.For our experiments, the validation on X-ray of phantom cannot fully represent the performance on X-ray of real patients, but it shows that the proposed method has high potential. Meanwhile, domain randomization could reduce the gap between DRR and real X-ray images, which would allow methods validated on phantoms to perform better also on real X-ray. For the runtime aspect, our patient-specific regression network can complete the training phase within one hour using an NVIDIA GPU (Quadro RTX A6000), which meets the requirement of clinical application during pre-operative planning phase. Meanwhile, the proposed network achieves an average inference time of 6ms per image with a size of 256 2 , when considering the run-time of the proposed refinement model, the total cost is approximately 2.5 s, which also fully satisfies the requirement for clinical application during intra-operative phase."
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,,Fig. 1 .,
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,,,
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,,Fig. 2 .,
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,,Table 1 .,
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,,Table 2 .,
A Patient-Specific Self-supervised Model for Automatic X-Ray/CT Registration,,Table 3 .,
