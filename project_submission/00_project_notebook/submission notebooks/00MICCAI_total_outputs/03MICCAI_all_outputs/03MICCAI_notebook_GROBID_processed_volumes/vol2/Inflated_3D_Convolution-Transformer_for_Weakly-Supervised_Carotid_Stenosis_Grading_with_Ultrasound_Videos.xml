<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos</title>
				<funder ref="#_EFCDjyY">
					<orgName type="full">Shenzhen-Hong Kong Joint Research Program</orgName>
				</funder>
				<funder ref="#_A7B5YEH #_d2grQxv">
					<orgName type="full">Shenzhen College Stable Support Plan</orgName>
				</funder>
				<funder ref="#_a6SBs4k">
					<orgName type="full">Shenzhen Science and Technology Innovations Committee</orgName>
				</funder>
				<funder ref="#_FtTPpuy #_wCVHJTA #_a8aTZ83">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinrui</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wufeng</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxin</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qilong</forename><surname>Ying</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanji</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen RayShape Medical Technology Co. Ltd</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">The Third Affiliated Hospital</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">The Third Affiliated Hospital</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dong</forename><surname>Ni</surname></persName>
							<email>nidong@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="511" to="520"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">17665ED30751A3B875DC78584C552AF4</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ultrasound video</term>
					<term>Carotid stenosis grading</term>
					<term>Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Localization of the narrowest position of the vessel and corresponding vessel and remnant vessel delineation in carotid ultrasound (US) are essential for carotid stenosis grading (CSG) in clinical practice. However, the pipeline is time-consuming and tough due to the ambiguous boundaries of plaque and temporal variation. To automatize this procedure, a large number of manual delineations are usually required, which is not only laborious but also not reliable given the annotation difficulty. In this study, we present the first video classification framework for automatic CSG. Our contribution is three-fold. First, to avoid the requirement of laborious and unreliable annotation, we propose a novel and effective video classification network for weakly-supervised CSG. Second, to ease the model training, we adopt an inflation strategy for the network, where pre-trained 2D convolution weights can be adapted into the 3D counterpart in our network for an effective warm start. Third, to enhance the feature discrimination of the video, we propose a novel attention-guided multi-dimension fusion (AMDF) transformer encoder to model and integrate global dependencies within and across spatial and temporal dimensions, where two lightweight cross-dimensional attention mechanisms are designed. Our approach is extensively validated on a large clinically collected carotid US video dataset, demonstrating stateof-the-art performance compared with strong competitors.</p><p>X. Zhou and Y. Huang-Contribute equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Carotid stenosis grading (CSG) represents the severity of carotid atherosclerosis, which is highly related to stroke risk <ref type="bibr" target="#b10">[11]</ref>. In clinical practice, sonographers need to first visually locate the frame with the largest degree of vascular stenosis (i.e., minimal area of remnant vessels) in a dynamic plaque video clip based on B-mode ultrasound (US), then manually delineate the contours of both vessels and remnant vessels on it to perform CSG. However, the two-stage pipeline is time-consuming and the diagnostic results heavily rely on operator experience and expertise due to ambiguous plaque boundaries and temporal variation (see Fig. <ref type="figure" target="#fig_0">1</ref>). Fully-supervised segmentation models can automatize this procedure, but require numerous pixel-level masks laboriously annotated by sonographers and face the risk of training failure due to unreliable annotation. Hence, tackling this task via weak supervision, i.e., video classification, is desired to avoid the requirement of tedious and unreliable annotation.</p><p>Achieving accurate automatic CSG with US videos is challenging. First, the plaque clips often have extremely high intra-class variation due to changeable plaque echo intensity, shapes, sizes, and positions (Fig. <ref type="figure" target="#fig_0">1(d-e</ref>)). The second challenge lies in the inter-class similarity of important measurement indicators (i.e., diameter and area stenosis rate) for CSG among cases with borderlines of mild and severe, which makes designing automatic algorithms difficult (Fig. <ref type="figure" target="#fig_0">1(c-d</ref>)).</p><p>A typical approach for this video classification task is CNN-LSTM <ref type="bibr" target="#b6">[7]</ref>. Whereas, such 2D + 1D paradigm lacks interaction with temporal semantics of input frames in the early stage. Instead, a more efficient way is to build 3D networks that handle spatial and temporal (ST) information simultaneously <ref type="bibr" target="#b20">[21]</ref>.</p><p>There are several types of 3D networks that have been widely used in visual tasks: (1) Pure 3D convolution neural networks (3D CNNs) refer to capturing local ST features using convolution operations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>. However, most current 3D CNNs suffer from the lack of good initialization and capacity for extracting global representations <ref type="bibr" target="#b15">[16]</ref>. (2) Pure 3D transformer networks (3D Trans) aim to exploit global ST features by applying self-attention mechanisms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. However, their ability in extracting local ST information is weaker than 3D CNNs. Moreover, such designs have not deeply explored lightweight crossdimensional attention mechanisms to gain refined fused features for classification.</p><p>Recently, Wang et al. <ref type="bibr" target="#b17">[18]</ref> first introduced the self-attention mechanism in 3D CNN for video classification. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref> then proposed Convolution-Transformer hybrid networks for image classification. Li et al. <ref type="bibr" target="#b11">[12]</ref> further extended such hybrid design to 3D for video recognition by seamlessly integrating 3D convolution and self-attention. Thanks to both operations, such networks can fully exploit and integrate local and global features, and thus achieve state-of-the-art results. However, current limited 3D hybrid frameworks are designed in cascade, which may lead to semantic misalignment between CNN-and Transformer-style features and thus degrade accuracy of video classification.</p><p>In this study, we present the first video classification framework based on 3D Convolution-Transformer design for CSG (named CSG-3DCT). Our contribution is three-fold. First, we propose a novel and effective video classification network for weakly-supervised CSG, which can avoid the need of laborious and unreliable mask annotation. Second, we adopt an inflation strategy to ease the model training, where pre-trained 2D convolution weights can be adapted into the 3D counterpart. In this case, our network can implicitly gain the pre-trained weights of existing large models to achieve an effective warm start. Third, we propose a novel play-and-plug attention-guided multi-dimension fusion (AMDF) transformer encoder to integrate global dependencies within and across ST dimensions. Two lightweight cross-dimensional attention mechanisms are devised in AMDF to model ST interactions, which merely use class (CLS) token <ref type="bibr" target="#b7">[8]</ref> as Query. Extensive experiments show that CSG-3DCT achieve state-of-the-art performance in CSG task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Figure <ref type="figure" target="#fig_1">2</ref>(a) shows the pipeline of our proposed framework. Note that the proposed CSG-3DCT is inflated from the 2D architecture. Thus, it can implicitly gain the pre-trained weights of current large model for effective initialization. In CSG-3DCT, given a video clip, vessel regions are first detected by the pre-trained detection model <ref type="bibr" target="#b13">[14]</ref> for reducing redundant background information. Then, the cropped regions are concatenated to form a volumetric vessel and input to the 3D CNN and Transformer encoders. Specifically, to better model the global knowledge, ST features are decoupled and fused by the proposed AMDF transformer encoder. CNN-and Transformer-style features are integrated by the 3D feature coupling unit (3D FCU) <ref type="bibr" target="#b15">[16]</ref> orderly. Finally, by combining the CNN features and the CLS token, the model will output the label prediction.</p><p>3D Mix-Architecture for Video Classification. CNN and Transformer have been validated that they specialize in extracting local and global features, respectively. Besides, compared to the traditional 2D video classifiers, 3D systems have shown the potential to improve classification accuracy due to their powerful capacity of encoding multi-dimensional information. Thus, in CSG-3DCT, we propose to leverage the advantages of both CNN and Transformer and extend the whole framework to a 3D version.</p><p>The meta-architecture of our proposed CSG-3DCT follows the 2D Convolution-Transformer (Conformer) model <ref type="bibr" target="#b15">[16]</ref>. It mainly has 5 stages (termed c1-c5 ). Extending it to 3D represents that both CNN and Transformer should be modified to adapt the 3D input. In specific, we tend to inflate the 2D k ×k convolution kernels to 3D ones with the size of t × k 2 by adding a temporal dimension, which is similar to <ref type="bibr" target="#b3">[4]</ref>. Such kernels can be implicitly pre-trained on ImageNet through bootstrapping operation <ref type="bibr" target="#b3">[4]</ref>. While translating the 2D transformer only requires adjusting the token number according to the input dimension.</p><p>Inflation Strategy for 3D CNN Encoder. We devise an inflation strategy for the 3D CNN encoder to relieve the model training and enhance the representation ability. For achieving 2D-to-3D inflation, a feasible scheme is to expand all the 2D convolution kernels at temporal dimension with t&gt;1 <ref type="bibr" target="#b3">[4]</ref>. However, multi-temporal (t&gt;1) 3D convolutions are computationally complex and hard to train. Thus, we only select part of the convolution kernels for inflating their temporal dimension larger than 1, while others restrict the temporal dimension to 1. By adapting pre-trained 2D convolution weights into the 3D counterpart, our network can achieve good initialization from existing large model. Moreover, we notice that performing convolutions at a temporal level in early layers may degrade accuracy due to the over-neglect of spatial learning <ref type="bibr" target="#b9">[10]</ref>. Therefore, instead of taking a whole-stage temporal convolution, we only perform it on 3D Conv blocks of the last three stages (i.e., c3-c5 ). We highlight that our temporal convolutions are length-invariant, which indicates that we will not down-sample at the temporal dimension. It can benefit the maintenance of both video fidelity and time-series knowledge, especially for short videos. See supplementary material for more details.</p><p>Transformer Encoder with Play-and-plug AMDF Design. Simply translating the 2D transformer encoder into the 3D standard version mainly has two limitations: (1) It blindly compares the similarity of all ST tokens by selfattention, which tends to inaccurate predictions. Moreover, such video-based computation handles t× tokens simultaneously compared to image-based methods, leading to much computational cost. ( <ref type="formula">2</ref>) It also has no ability to decide which information is more important during different learning stages. Thus, we propose to enhance the decoupled ST features and their interactions using different attention manners. The proposed encoder can improve computational efficiency, and can be flexibly integrated into 2D or 3D transformer-based networks.</p><p>Before the transformer encoder, we first decompose the feature maps X produced by the stem module into t × n 2 embeddings without overlap. A CLS token X cls ∈ R d is then added in the start position of X to obtain merged embeddings Z ∈ R d×(t×n 2 +1) . n 2 and d denote the number of spatial patch tokens and hidden dimensions, respectively. Then, the multiple AMDF Trans blocks in the transformer encoder drive Z to produce multi-dimensional enhanced representations. Specifically, the AMDF block has the following main components.</p><p>1) Intra-dimension ST Learning Module. Different from the cascade structure in <ref type="bibr" target="#b2">[3]</ref>, CSG-3DCT constructs two parallel branches to learn global ST features, respectively. As shown in Fig. <ref type="figure" target="#fig_1">2(b</ref>), the proposed module is following ViT <ref type="bibr" target="#b7">[8]</ref>, which consists of a multi-head self-attention (MHSA) module and a feed-forward network (FFN). Query-Key-Value (QKV) projection after Layer-Norms <ref type="bibr" target="#b1">[2]</ref> is conducted before each MHSA module. Besides, the residual connections are performed in MHSA module and FFN. Taking token embeddings as input, the two branches can extract the ST features well by parallel spatial and temporal attention (see Fig. <ref type="figure" target="#fig_1">2(c</ref>) for visualization of computation process).</p><p>2) Inter-dimension ST Fusion Module. To boost interactions between S and T dimensions, we build the inter-dimension fusion module after the intradimension learning module. The only difference between the two types of modules is the calculation mode of attention. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, we consider the following two methods to interact the ST features: (i) Switched Attention (SWA) Fusion and (ii) Cross Attention (CA) Fusion. Here, we define one branch as the target dimension and the other branch as the complementary dimension. For example, when the temporal features are flowing to the spatial features, the spatial branch is the target and the temporal branch is the complementary one. SWA is an intuitive way for information interaction. It uses the attention weights (i.e., generated by Q and K) as the bridge to directly swap the information. For computing the target features in CA, K and V are from the complementary one, and Q is from its own. Intuition behind CA is that the target branch can query the useful information from the given K and V <ref type="bibr" target="#b5">[6]</ref>. Thus, the querying process in CA can better encourage the knowledge flowing.</p><p>To improve computing efficiency in CA, Chen et al. <ref type="bibr" target="#b4">[5]</ref> proposed to adopt the CLS token of the target branch to compute the CLS-Q to replace the common Q from token embeddings. Then, they transferred the target CLS token to the complementary branch to obtain the K and V and perform CA. However, such a design may lead to overfitting due to the query-queried feature dependency. Motivated by <ref type="bibr" target="#b4">[5]</ref>, we introduce a simple yet efficient attention strategy in interdimension ST fusion module. Specifically, the target dimension adopts its CLS token as a query to mine rich information, and this CLS token will not be inserted into the complementary dimension. Besides, using one token only can reduce the computation time quadratically compared to all tokens attention.</p><p>3) Learnable Mechanism for Adaptive Updating. Multi-dimensional features commonly have distinct degrees of contribution for prediction. For example, supposing the size of carotid plaque does not vary significantly in a dynamic segment, the spatial information may play a dominant role in making the final diagnosis. Thus, we introduce a learnable parameter to make the network adaptively adjust the weights of different branches and learn the more important features (see Fig. <ref type="figure" target="#fig_1">2(b)</ref>). We highlight that this idea is easy to implement and general to be equipped with any existing feature-fusion modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Dataset and Implementations. We validated the CSG-3DCT on a large inhouse carotid transverse US video dataset. Approved by the local IRB, a total Table <ref type="table">1</ref>. Quantitative results of methods. "MTV(B/2+S/8)" means to use the larger "B" model to encode shorter temporal information (2 frames), and the smaller "S" model to encode longer temporal information (8 frames) <ref type="bibr" target="#b19">[20]</ref>. † denotes random initialization. * indicates removing the learnable mechanism from AMDF encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy F1-score Precision Recall I3D <ref type="bibr" target="#b3">[4]</ref> 78.8% 78.8% 81.0% 80.8% SlowFast <ref type="bibr" target="#b9">[10]</ref> 70.3% 70.2% 70.3% 70.8% TPN <ref type="bibr" target="#b20">[21]</ref> 78 of 200 videos (63225 images with size 560×560 and 380×380) were collected from 169 patients with carotid plaque. In clinic, sonographers often focus on a relatively narrow short plaque video clip instead of the long video. Thus, we remade the dataset by using the key plaque video clips instead of original long videos. Specifically, sonographers with 7-year experience manually annotated 8/16 frames for a plaque clip and labeled the corresponding stenosis grading (mild/severe) using the Pair annotation software package <ref type="bibr" target="#b12">[13]</ref>. The final dataset was split randomly into 318, 23, and 118 plaque clips with 8 frames or into 278, 23, and 109 ones with 16 frames for training, validation, and independent testing set at the patient level with no overlap.</p><p>In this study, we implemented CSG-3DCT in Pytorch, using an NVIDIA A40 GPU. Unless specified, we trained our model using 8-frame input plaque clips. All frames were resized to 256 × 256. The learnable weights of QKV projection and LayerNorm weights in spatial dimension branch of intra-dimension ST learning module were initialized with those from transformer branch in Conformer <ref type="bibr" target="#b15">[16]</ref>, while other parameters in AMDF transformer encoder performed random initialization. We trained CSG-3DCT using Adam optimizer with the learning rate (lr ) of 1e-4 and weight decay of 1e-4 for 100 epochs. Batch size was set as 4. Inspired by <ref type="bibr" target="#b17">[18]</ref>, CSG-3DCT with 16-frame inputs was initialized with 8-frame model and fine-tuned using an initial lr of 0.0025 for 40 epochs. Quantitative and Qualitative Analysis. We conducted extensive experiments to evaluate CSG-3DCT. Accuracy, F1-score, precision and recall were evaluation metrics. Table <ref type="table">1</ref> compares CSG-3DCT with other 8 strong competitors, including 3D CNNs, 3D Trans, and 3D Mix-architecture. Note that "-Base" is directly inflated from Conformer <ref type="bibr" target="#b15">[16]</ref>. Among all the competitors, -Base achieves the best results on accuracy and f1-score. It can also be observed that our proposed CSG-3DCT achieves state-of-the-art results (at least 4.3% improvement in accuracy).</p><p>Figure <ref type="figure" target="#fig_3">4</ref> visualizes feature maps of different typical networks using Grad-CAM <ref type="bibr" target="#b16">[17]</ref>. We use models without temporal downsampling (i.e., TimeSformer <ref type="bibr" target="#b2">[3]</ref> and the "fast" branch of SlowFast <ref type="bibr" target="#b9">[10]</ref>) to observe attention changes along temporal dimension. Both models ignore capturing equally important local and global ST features simultaneously, resulting in imprecise and coarse attention to the key object, i.e., the plaque area. Compared to both cases, CSG-3DCT can progressively learn the ST contexts in an interactive fashion. As a result, the attention area is more accurate and complete, indicating the stronger discriminative ability of the learned features by CSG-3DCT, which proves the efficacy of our framework.</p><p>Ablation Study. We performed ablation experiments in the last 6 rows of Table <ref type="table">1</ref>. "-SWA * " uses SWA in AMDF transformer encoder, while "-CA * " uses CA instead. "-Base-16" denotes our "-Base" model with 16-frame inputs.</p><p>1) Effects of Different Key Components of Our Model Design. We compared CSG-3DCT with three variants (i.e., -Base, -SWA * , and -CA * ) to analyze the effects of different key components. Compared with -Base, each of our proposed modules and their combination can help improve the accuracy. We adopt CA in our final model for its good performance.</p><p>2) Effects of Plaque Clip Length. We only investigated the effects of our model on 8-frame and 16-frame input clips due to limited GPU memory. We can find in Table <ref type="table">1</ref> that longer input clips slightly degrade the performance. This is reasonable since the frame-extracting method has been applied in the original videos, causing the covered range of plaque from a longer plaque clip is relatively wider, which is not beneficial to stenosis grading.</p><p>3) Effectiveness of Initialization with ImageNet. We evaluated the value of training models starting from ImageNet-pretrained weights compared with scratch. It can be seen in Table <ref type="table">1</ref> that model with pretraining significantly boosts +4.2% Acc., demonstrating the efficacy of good initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a novel and effective video classification network for automatic weakly-supervised CSG. To the best of our knowledge, this is the first work to tackle this task. By adopting an inflation strategy, our network can achieve effective warm start and make more accurate predictions. Moreover, we develop a novel AMDF Transformer encoder to enhance the feature discrimination of the video with reduced computational complexity. Experiments on our large inhouse dataset demonstrate the superiority of our method. In the future, we will explore to validate the generalization capability of CSG-3DCT on more large datasets and extend two-grade classification to four-grade of carotid stenosis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Carotid plaque images in US with annotated vessel (green contour) and remnant vessel (red contour). (b) An original video with vessels and plaques dynamically displayed. (c-e): Cropped plaque clips with annotated vessel (green box), remnant vessel (red box), and corresponding label. (Color figure online)</figDesc><graphic coords="2,91,80,383,21,240,46,161,53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Overview of CSG-3DCT. It contains L+1 and L repeated AMDF Trans and 3D Conv Blocks, respectively. A 3D Conv Block consists of two sub-blocks. (b) Pipeline of the AMDF Trans Block. (c) Visualization of the space-time attention used in the intra-dimension ST learning module. Yellow, blue and red patches indicate query, and attention separately adopted along temporal and spatial dimensions, respectively. (Color figure online)</figDesc><graphic coords="4,41,79,54,59,340,21,168,10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustration of our proposed multi-dimension fusion methods.</figDesc><graphic coords="6,92,31,54,59,239,20,84,10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Attention maps of one carotid severe stenosis testing case (shown in cropped volumetric vessel). Red box denotes remnant vessel annotated by sonographers. (Color figure online)</figDesc><graphic coords="8,65,79,53,90,292,30,150,79" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the grant from <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62171290</rs>, <rs type="grantNumber">62101343</rs>), <rs type="funder">Shenzhen-Hong Kong Joint Research Program</rs> (No. <rs type="grantNumber">SGDX20201103095613036</rs>), <rs type="funder">Shenzhen Science and Technology Innovations Committee</rs> (No. <rs type="grantNumber">20200812143441001</rs>), <rs type="funder">Shenzhen College Stable Support Plan</rs> (Nos. <rs type="grantNumber">20220810145705001</rs>, <rs type="grantNumber">20200812162245001</rs>), and <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">No 81971632</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FtTPpuy">
					<idno type="grant-number">62171290</idno>
				</org>
				<org type="funding" xml:id="_wCVHJTA">
					<idno type="grant-number">62101343</idno>
				</org>
				<org type="funding" xml:id="_EFCDjyY">
					<idno type="grant-number">SGDX20201103095613036</idno>
				</org>
				<org type="funding" xml:id="_a6SBs4k">
					<idno type="grant-number">20200812143441001</idno>
				</org>
				<org type="funding" xml:id="_A7B5YEH">
					<idno type="grant-number">20220810145705001</idno>
				</org>
				<org type="funding" xml:id="_d2grQxv">
					<idno type="grant-number">20200812162245001</idno>
				</org>
				<org type="funding" xml:id="_a8aTZ83">
					<idno type="grant-number">No 81971632</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_48.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ViViT: a video vision transformer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference On Computer Vision</title>
		<meeting>the IEEE/CVF International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CrossViT: cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dyadformer: a multi-modal transformer for long-range modeling of dyadic interactions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Curto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2177" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6824" to="6835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Risk of stroke in relation to degree of asymptomatic carotid stenosis: a population-based cohort study, systematic review, and meta-analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gaziano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Rothwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Neurol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UniFormer: unified transformer for efficient spatial-temporal representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102461</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning based on carotid transverse B-mode scan videos for the diagnosis of carotid plaque: a prospective multicenter study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video swin transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3202" to="3211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conformer: local features coupling global representations for visual recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ViTAE: vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28522" to="28535" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiview transformers for video recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3333" to="3343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
