<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation</title>
				<funder ref="#_tDr4UeR">
					<orgName type="full">Ministry of Science and ICT of KOREA</orgName>
				</funder>
				<funder ref="#_Amu5txc">
					<orgName type="full">National Research Foundation of Korea</orgName>
				</funder>
				<funder ref="#_8Uk2KJh #_bcF8D6r #_Nw5fnVy">
					<orgName type="full">National Institute of Health</orgName>
				</funder>
				<funder>
					<orgName type="full">NRF</orgName>
				</funder>
				<funder ref="#_tA9PQ3Y">
					<orgName type="full">Korean Government (MSIT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Myeongkyun</forename><surname>Kang</surname></persName>
							<email>mkkang@dgist.ac.kr</email>
							<idno type="ORCID">0000-0002-9165-870X</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Robotics and Mechatronics Engineering</orgName>
								<orgName type="department" key="dep2">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</orgName>
								<address>
									<settlement>Daegu</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><surname>Chikontwe</surname></persName>
							<idno type="ORCID">0000-0002-6995-2312</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Robotics and Mechatronics Engineering</orgName>
								<orgName type="department" key="dep2">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</orgName>
								<address>
									<settlement>Daegu</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soopil</forename><surname>Kim</surname></persName>
							<idno type="ORCID">0000-0001-8937-6263</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Robotics and Mechatronics Engineering</orgName>
								<orgName type="department" key="dep2">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</orgName>
								<address>
									<settlement>Daegu</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyong</forename><forename type="middle">Hwan</forename><surname>Jin</surname></persName>
							<idno type="ORCID">0000-0001-7885-4792</idno>
						</author>
						<author>
							<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
							<idno type="ORCID">0000-0002-0579-7763</idno>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kilian</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
							<idno type="ORCID">0000-0001-5416-5159</idno>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sang</forename><forename type="middle">Hyun</forename><surname>Park</surname></persName>
							<idno type="ORCID">0000-0001-7476-1046</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Robotics and Mechatronics Engineering</orgName>
								<orgName type="department" key="dep2">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</orgName>
								<address>
									<settlement>Daegu</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="521" to="531"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">466B305AF0F3E45EEE6C6A45D2B9BD0A</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>One-Shot Federated Learning</term>
					<term>Knowledge Distillation</term>
					<term>Noise</term>
					<term>Image Synthesis</term>
					<term>Client Model Adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-shot federated learning (FL) has emerged as a promising solution in scenarios where multiple communication rounds are not practical. Notably, as feature distributions in medical data are less discriminative than those of natural images, robust global model training with FL is non-trivial and can lead to overfitting. To address this issue, we propose a novel one-shot FL framework leveraging Image Synthesis and Client model Adaptation (FedISCA) with knowledge distillation (KD). To prevent overfitting, we generate diverse synthetic images ranging from random noise to realistic images. This approach (i) alleviates data privacy concerns and (ii) facilitates robust global model training using KD with decentralized client models. To mitigate domain disparity in the early stages of synthesis, we design noise-adapted client models where batch normalization statistics on random noise (synthetic images) are updated to enhance KD. Lastly, the global model is trained with both the original and noise-adapted client models via KD and synthetic images. This process is repeated till global model convergence. Extensive evaluation of this design on five small-and three large-scale medical image classification datasets reveals superior accuracy over prior methods. Code is available at https://github.com/myeongkyunkang/FedISCA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One-shot federated learning (FL) allows a global model to be trained through a single communication round without sharing data between clients <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. This approach significantly reduces the risk of attack and communication costs compared to FL <ref type="bibr" target="#b20">[21]</ref> and allows for decentralized training under extreme conditions. For instance, one-shot FL has emerged as a viable solution for reducing significant transmission costs in scenarios where patient data is only accessible within an isolated network requiring in-person transfer of client models. Since one-shot FL can only access clients' models once during training, recent oneshot FL suggests generating images and using them to transfer knowledge from multiple client models for global model training using knowledge distillation (KD) <ref type="bibr" target="#b32">[33]</ref>. However, the lack of diversity in the generated images often leads to overfitting, posing a significant challenge for one-shot FL. To address this issue, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref> propose to enhance the transferability of client models by generating diverse natural images near the decision boundary. Compared to natural images, the decision boundaries in medical data are often more complex (e.g., less discriminative as shown in Fig. <ref type="figure" target="#fig_0">1</ref>), which limits the applicability of existing one-shot FL approaches to this application. Note, while the challenges in medical data and client heterogeneity can be mitigated through multiple communication rounds <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>, the one-shot scenario presents a unique difficulty. Through this study, we reveal the inherent drawbacks of existing one-shot FL methods for medical data (see Table <ref type="table" target="#tab_2">1</ref>), and suggest a more suitable approach to address existing challenges e.g., overfitting.</p><p>To prevent global model overfitting, we attempt to leverage random noise as a training source for KD (see Fig. <ref type="figure" target="#fig_1">2</ref>). Baradad et al. <ref type="bibr" target="#b0">[1]</ref> employs diverse types of structured noise for training in order to account for the difference between real images and random noise. However, due to the diversity of medical data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, seeking a common noise space is more challenging than in natural images. Hence, we exploit DeepInversion <ref type="bibr" target="#b29">[30]</ref>, which synthesizes structured proxy noise specific to a task and thus ensures that generated noise matches the properties of medical data. Specifically, we first gather client models on the central server, where each client model is trained on its own dataset. Next, we synthesize images from random noise and store all intermediate samples in memory. Also, as images in the early stages of synthesis (i.e., close to random noise) are different from real images, we design noise-adapted client models that employ adaptive batch normalization (AdaBN) <ref type="bibr" target="#b15">[16]</ref>. AdaBN is based on the assumption that domainrelated knowledge is represented by the statistics of the batch normalization (BN) <ref type="bibr" target="#b10">[11]</ref> and label-related knowledge is stored in the weight matrix of each layer, ultimately enhancing the KD signal for random noise. Lastly, we train a global model through KD with both the original-and noise-adapted client models using memory-stored images, repeating until global model convergences.</p><p>The contributions are as follows: (i) We propose one-shot FL leveraging image synthesis with client model adaptation. This allows to transfer knowledge from client models to the global model with synthesized images ranging from random noise to realistic images and contributes to preventing overfitting. (ii) We employ noise-adapted client models using AdaBN to produce a better KD signal for random noise. (iii) Comprehensive experiments on five small-and three large-scale medical image classification datasets consisting of microscopy, dermatoscopy, oct, histology, x-ray, and retinal images reveal that our method outperforms state-of-the-art one-shot FL methods.</p><p>Related Work. Due to the challenges of one-shot FL, prior methods were trained on public data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, applying dataset distillation <ref type="bibr" target="#b34">[35]</ref>, or sharing additional information <ref type="bibr" target="#b5">[6]</ref>. However, these assumptions may not hold for several real world scenarios, posing a challenge for their practical application. Recently, Zhang et al. <ref type="bibr" target="#b32">[33]</ref> proposed the one-shot FL DENSE, which transfers knowledge from an ensemble of client models using KD and generated images. To enhance the transferability of client models, DENSE generates diverse images near the decision boundary to improve its accuracy. However, DENSE does not perform well in one-shot FL for medical data due to the complexity of decision boundaries. While DENSE diversifies generation using a generator, we propose to avoid overfitting by using synthesized images ranging from random noise to realistic images. For data-free KD <ref type="bibr" target="#b18">[19]</ref>, DeepInversion <ref type="bibr" target="#b29">[30]</ref> synthesizes images by optimizing RGB pixels with cross-entropy and regularization losses and improves synthesis quality by minimizing feature statistics in BN layers. DAFL <ref type="bibr" target="#b1">[2]</ref> uses a generator for image synthesis with a teacher model as a discriminator. To prevent student model overfitting, ZSKT <ref type="bibr" target="#b21">[22]</ref> synthesizes images that exhibit mismatch between the student and teacher models. Unlike the methods that choose the best image as a training source for KD, our approach utilizes all intermediate synthesized images to prevent overfitting. Also, while Raikward et al. <ref type="bibr" target="#b23">[24]</ref> proposed a method for KD that uses random noise as a training source, it requires real images during training and needs to adjust BN layer statistics multiple times iteratively. In contrast, our method performs one-shot FL without requiring real images during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The overall training processes are shown in Fig. <ref type="figure" target="#fig_1">2</ref> and<ref type="figure">Algorithm</ref> </p><formula xml:id="formula_0">1. Given K client models W c = {W c 1 , . . . , W c k } with corresponding BN statistics μ k and σ 2</formula><p>k with respect to data D k , the objective of FL is to train a global model W g , which represents all data D = {D 1 , . . . , D k }. Motivated by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, KD enables the transfer of knowledge from client models W c to the global model W g . Due to restricted access of D, prior works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> use synthetic images x as a training source for KD. However, since x may be monotonous for robust training, overfitting is a significant challenge in one-shot FL. To address this, we employ random Gaussian noise N (0, 1) as a training source for KD <ref type="bibr" target="#b0">[1]</ref>. However, in contrast with <ref type="bibr" target="#b0">[1]</ref>, N (0, 1) does not capture common medical properties. Hence, we employ DeepInversion <ref type="bibr" target="#b29">[30]</ref> to ensure random noise retains characteristics of D. Details regarding image synthesis with DeepInversion are described in the following section. Image Synthesis. Given random noise x ∈ R H×W ×C initialized from N (0, 1), where H, W , and C denote height, width, and channels; the objective of image synthesis is to ensure x possesses a certain property of D. To achieve this, we optimize RGB pixels of x to synthesize a class-conditioned image with respect to a specific label y for I iterations. Formally,</p><formula xml:id="formula_1">L s (x, y; W c ) = L CE (x, y; W c ) + λ BN L BN (x; W c ) + λ T V L T V (x; W c ),<label>(1)</label></formula><p>where L CE , L BN , and L T V are cross-entropy, BN, and total variation losses <ref type="bibr" target="#b19">[20]</ref>. Hyper-parameters λ BN and λ T V are used to balance the losses. Cross-entropy loss enables the synthesis of an image with respect to the label y, and total variation loss encourages image synthesis consistency. Additionally,  training source, our method employs all intermediate synthesized samples for KD. Thus we store all intermediate samples and the corresponding noise level λ (e.g., 1i/I for i steps) in memory during I iterations. Due to the visual difference between N (0, 1) and D, we design noise-adapted client models using AdaBN <ref type="bibr" target="#b15">[16]</ref> to provide better KD signals for x. The following section will describe more details regarding noise-adapted client models.</p><formula xml:id="formula_2">L BN (x) = ( μ(x) -μ + σ 2 (x) -σ 2 ), where μ(x) &amp; σ 2 (x)</formula><formula xml:id="formula_3">for i = 1, • • • , I do x ← x -η s∇Ls(x, y; W c ) // Synthesize image memory.append((x, 1 -i/I)) end for for i = 1, • • • , I do x, λ ← memory[I -i] μ ← αμ + (1 -α)μ(x), σ2 ← ασ 2 + (1 -α)σ 2 (x) // Adapt noise for Ŵ c end for for i = 1, • • • , I do x, λ ← memory[i] W g ← W g -η d ∇L d (x, λ; W c , Ŵ c , W g ) //</formula><p>Noise Adaptation. BN <ref type="bibr" target="#b10">[11]</ref> was proposed to mitigate internal covariate shifts, allowing to provide consistent input distributions to subsequent layers. Due to the existing discrepancy between N (0, 1) and D, there is no guarantee BN will provide consistent input to subsequent parameters and may lead to poor model predictions. Thus we adapt N (0, 1) by iteratively adjusting the running statistics of BN using AdaBN <ref type="bibr" target="#b15">[16]</ref>, producing better logit signals for KD. Formally,</p><formula xml:id="formula_4">μ = αμ + (1 -α)μ(x), σ2 = ασ 2 + (1 -α)σ 2 (x),<label>(2)</label></formula><p>where α represents momentum and x is a sample stored in memory. Initially, μ and σ2 are set to μ and σ 2 . The samples in memory ranging from characteristic images for D to N (0, 1) by gradually adjusting μ and σ2 towards N (0, 1) through Eq. 2 for I steps. With this in mind, we now describe how to train the global model. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. We denote W c with original μ and σ 2 as W c , and denote W c with μ and σ2 as Ŵ c . Since x, W c , and Ŵ c are used for KD, this enables the model to avoid overfitting without being negatively impacted during global model training. Formally,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Model Training. KD allows to train a global model with multiple client models</head><formula xml:id="formula_5">L d (x, λ; W c , Ŵ c , W g ) = λL KD (x; Ŵ c , W g ) + (1 -λ)L KD (x; W c , W g ),<label>(3)</label></formula><p>where λ denotes a noise level stored in memory. L KD (x; W c , W g ) denotes the Kullback-Leibler divergence between p(x; W c ) and p(x; W g ) where p(•) is an ensemble (averaging) prediction of given models with a temperature on softmax inputs <ref type="bibr" target="#b9">[10]</ref>. Overall, W g is trained for I steps. To clarify, random noise contributes to avoiding overfitting, while noise-adapted client models help to produce a better KD signal for random noise, improving robust global model training. These processes i.e., Image Synthesis, Noise Adaptation, and Global Model Training are repeated until the global model W g converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. For evaluation, we use five small-scale (28×28) medical image classification datasets i.e., Blood, Derma, Oct, Path, and Tissue from MedMNIST <ref type="bibr" target="#b28">[29]</ref>. Additionally, we use three large-scale (224×224) datasets i.e., RSNA, Diabetic, and ISIC from RSNA Pneumonia Detection <ref type="bibr" target="#b24">[25]</ref>, Diabetic Retinopathy Detection <ref type="bibr" target="#b6">[7]</ref>, and ISIC2019-HAM-BCN20000 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Experimental Settings. We explore three scenarios i.e., (i) data heterogeneity levels, (ii) impact on large-scale datasets, and (iii) model heterogeneity i.e., each client has different architectures. In (i), Blood, Derma, Oct, Path, and Tissue datasets are used with Independent and Identically Distributed (IID) clients and Dirichlet distributed <ref type="bibr" target="#b30">[31]</ref> clients with α = 0.6 and α = 0.3. For (ii), RSNA, Diabetic, and ISIC datasets are used with IID clients, including ISIC where each client has a different image acquisition system <ref type="bibr" target="#b26">[27]</ref>. For (iii), client models used either ResNet18 <ref type="bibr" target="#b8">[9]</ref>, ResNet34 <ref type="bibr" target="#b8">[9]</ref>, WRN-16-2 <ref type="bibr" target="#b31">[32]</ref>, VGG16(with BN) <ref type="bibr" target="#b25">[26]</ref>, and VGG8(with BN) <ref type="bibr" target="#b25">[26]</ref>, respectively.</p><p>Comparison Methods. We employ three one-shot FL methods: FedAvg <ref type="bibr" target="#b20">[21]</ref> with single communication, DAFL <ref type="bibr" target="#b1">[2]</ref>, and DENSE <ref type="bibr" target="#b32">[33]</ref>, each evaluated using global model accuracy obtained on test data. For the upper bound, we report the FedAvg with 100 communications. For ablations, we evaluate (a) without image synthesis (w/o IS), (b) without image synthesis and noise adaptation (w/o IS&amp;Ada) with only N (0, 1) used for training, (c) without noise adaptation (w/o Ada), and (d) without intermediate random noise (w/o N ), this is equivalent to DeepInversion <ref type="bibr" target="#b29">[30]</ref> in a one-shot FL scenario. For w/o N , we synthesize all images and perform KD. For a fair comparison, we follow each method's original implementation and matched all training/parameter settings. For DAFL, an ensemble of client models was used as the teacher model following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> with KD used for global model training. On large-scale datasets, an ImageNet pre-trained model was used with balanced classification accuracy reported for evaluation as in <ref type="bibr" target="#b26">[27]</ref>.</p><p>Implementation Details. We used ResNet18 <ref type="bibr" target="#b8">[9]</ref> for our experiments with five clients by default. Client models were trained for 100 epochs with SGD optimizer using learning rate (LR) 1e-3 and batch size 128. For image synthesis, we used Adam optimizer with LR 5e-2 for 100 epochs with 500 and 1,000 synthesis iterations (i.e., I) for small-and large-scale datasets, with batch sizes 256 and 50, respectively. Following <ref type="bibr" target="#b29">[30]</ref>, λ T V = 0.000025 and λ BN = 10, with KD temperature T = 20 and momentum α = 0.9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Results</head><p>Table <ref type="table" target="#tab_2">1</ref> shows the accuracy on five datasets with different heterogeneity levels.</p><p>FedISCA outperforms all one-shot FL methods across all datasets regardless of the level of heterogeneity. In Table <ref type="table" target="#tab_3">2</ref>, FedISCA also reports improved performance against the compared methods, validating the viability of our approach on real-world large-scale data. On the contrary, DAFL and DENSE performed poorly on medical data since significant accuracy gaps exist between the upper bound and each competitor (except Derma). Additionally, though FedAvg reports higher accuracy for multiple communication rounds, it shows significantly lower accuracy for single communication (FedAvg(1)). To better explain this phenomenon, we analyzed the accuracy of FedAvg(1) by comparing the variance between client model parameters i.e., client models with high variance e.g., Path IID(=36.10), yield lower accuracy compared to those with low variance e.g., Derma IID(=0.01). This suggests that the variance of client models is correlated with the accuracy of FedAvg(1). In Fig. <ref type="figure" target="#fig_2">3</ref>, we show the synthesized images of FedISCA, DAFL <ref type="bibr" target="#b1">[2]</ref>, and DENSE <ref type="bibr" target="#b32">[33]</ref> on eight datasets. FedISCA generates more realistic images compared to the competitors. Note that DENSE aims to generate a diverse image (e.g., generating highly transferable samples) distributed near the decision boundary, which may not be realistic. Although these methods have achieved higher accuracy on natural data, our experiments reveal that this assumption does not hold in the medical domain. In addition, DENSE outperforms FedAvg(1) on small-scale  Ablations. We report ablation results in Table <ref type="table" target="#tab_2">1</ref> and 2. In the medical field, generating realistic images is crucial for one-shot FL, as the accuracy of w/o IS, and w/o IS&amp;Ada is significantly lower compared to FedISCA; this validates the need for image synthesis. However, relying on image synthesis alone is not enough to achieve high accuracy, as neither w/o Ada nor w/o N achieve the best accuracy across all datasets. w/o N performs worse than w/o Ada in most datasets (except Blood and Derma), showing that solely relying on the best image is not sufficient for robust training. On the contrary, the accuracy of FedISCA suggests that noise-adapted client models alleviate the negative effects of random noise, resulting in high accuracy. Overall, the experimental results support the idea that both components play an essential role in medical oneshot FL. Additionally, we also evaluate the variance in BN statistics between the original and noise-adapted client models. Here, we found that high variance (e.g., RSNA(=0.0018)), yields improved accuracy compared to those with lower variance (e.g., Diabetic(=0.0008)). Finally, Table <ref type="table" target="#tab_4">3</ref> shows the accuracy of a global model trained on client models with model heterogeneity. The proposed method reports the best accuracy among all competitors, equally demonstrating the effectiveness of our method in one-shot FL with diverse types of model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present a novel one-shot FL framework that uses image synthesis and client model adaptation with KD. We demonstrate that (i) random noise significantly reduces the risk of overfitting, resulting in robust global model training; (ii) noiseadapted client models enhance the KD signal leading to high accuracy; and (iii) through experiments on eight datasets, our method outperforms the state-ofthe-art one-shot FL methods on medical data. Further investigation into severe heterogeneity in clients will be a topic of future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Feature visualization on natural (MNIST and Cifar10) and medical (Blood, Derma, Oct, Path, and Tissue) images. For visualization, we placed a bottleneck layer before the class prediction layer, reducing the feature dimension to 2. Each color represents a classification label. Notably, the feature distribution in medical data is more complex.</figDesc><graphic coords="2,41,79,54,02,340,21,50,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our proposed method. W c k denotes a client model with respect to data D k and W g denotes a global model. W c denotes original client models and Ŵ c denotes noise-adapted client models. x indicates random noise and λ indicates noise level. x is optimized to have a property of all D k using LCE, LBN , and LTV . Afterward, it is used as a training source for KD in global model training.</figDesc><graphic coords="4,41,79,53,81,340,21,168,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The synthesized images of (a) FedISCA, (b) DAFL [2], and (c) DENSE [33] on eight datasets. Overall, FedISCA synthesizes more realistic images.</figDesc><graphic coords="8,41,79,54,50,340,21,70,21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>are the batch-wise mean &amp; variance features of x and μ &amp; σ 2 of the stored statistics of the BN layer. Since BN enforces feature similarity at all levels, this improves the quality of image synthesis significantly. Algorithm 1. Training process of our proposed method. Client models W c with corresponding μ and σ 2 , a global model W g , a iteration I, a learning rate of image synthesis ηs, a learning rate of KD η d , a momentum α. Ŵ c ← W c , μ ← μ, σ2 ← σ 2 // Initialize noise-adapted client models Repeat Initialize a batch of random noise x and arbitrary labels y.</figDesc><table /><note><p>Recall that our method employs random noise x that has D's characteristics for training. In contrast to DeepInversion which selects the best image as a Input: memory ← [ ]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Train global model end for until convergence. Output: Trained global model W g .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Classification accuracy on five datasets with different heterogeneity levels. The first and second sub-rows show the accuracy of the upper bound and one-shot FL methods. The third sub-row shows ablation performance with IS, Ada, and N denoting w/o image synthesis, noise adaptation, and random noise, respectively. Bold indicates the best accuracy among one-shot FL methods. Ada 81.61 68.33 70.30 82.08 59.34 63.67 68.18 61.90 78.61 51.99 29.73 14.36 54.30 77.69 50.40 w/o N [30] 87.02 68.73 60.20 77.90 57.86 80.62 69.58 60.30 75.54 49.06 45.69 13.87 49.20 70.53 46.73</figDesc><table><row><cell></cell><cell>IID</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dirichlet (α = 0.6)</cell><cell>Dirichlet (α = 0.3)</cell></row><row><cell></cell><cell cols="2">Blood Derma Oct</cell><cell cols="4">Path Tissue Blood Derma Oct</cell><cell>Path Tissue Blood Derma Oct</cell><cell>Path Tissue</cell></row><row><cell cols="8">FedAvg [21] 93.51 74.61 75.60 84.54 63.64 93.60 72.72 76.50 81.48 55.61 87.49 69.88 73.50 77.52 53.26</cell></row><row><cell>FedAvg(1)</cell><cell cols="3">13.74 66.88 25.00 5.86</cell><cell cols="4">32.07 18.24 66.88 25.00 5.86</cell><cell>32.07 16.92 10.97 25.00 5.86</cell><cell>32.07</cell></row><row><cell>DAFL [2]</cell><cell>7.13</cell><cell cols="2">66.43 25.00 7.63</cell><cell cols="2">11.55 7.13</cell><cell cols="2">66.88 34.40 14.97 39.15 7.13</cell><cell>13.62 25.00 18.64 45.00</cell></row><row><cell cols="8">DENSE [33] 39.37 66.93 33.80 21.89 21.35 34.52 67.78 39.40 30.31 9.47</cell><cell>30.78 12.77 25.80 19.87 9.33</cell></row><row><cell>FedISCA</cell><cell cols="7">87.99 70.12 70.20 84.18 61.90 82.90 69.83 68.60 82.92 53.04 46.59 15.91 60.50 79.25 51.00</cell></row><row><cell>w/o IS</cell><cell>9.09</cell><cell cols="4">66.88 25.20 24.69 23.70 9.09</cell><cell cols="2">66.88 26.10 22.41 9.31</cell><cell>23.27 11.02 27.10 18.70 9.31</cell></row><row><cell cols="2">w/o IS&amp;Ada 7.13</cell><cell cols="2">11.12 35.80 4.72</cell><cell>7.13</cell><cell>7.13</cell><cell cols="2">66.88 25.00 14.15 32.07 7.13</cell><cell>11.12 25.00 4.72</cell><cell>7.13</cell></row><row><cell>w/o</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Balanced classification accuracy on large-scale datasets.</figDesc><table><row><cell></cell><cell cols="8">FedAvg [21] FedAvg(1) DAFL [2] DENSE [33] FedISCA w/o IS w/o IS&amp;Ada w/o Ada w/o N [30]</cell></row><row><cell>RSNA</cell><cell>88.16</cell><cell>78.65</cell><cell>50.55</cell><cell>55.06</cell><cell>85.34</cell><cell>50.00 50.00</cell><cell>81.56</cell><cell>50.61</cell></row><row><cell cols="2">Diabetic 49.04</cell><cell>35.60</cell><cell>22.63</cell><cell>23.51</cell><cell>40.08</cell><cell>20.07 20.02</cell><cell>40.91</cell><cell>28.30</cell></row><row><cell>ISIC</cell><cell>62.88</cell><cell>38.05</cell><cell>14.51</cell><cell>13.69</cell><cell>48.39</cell><cell>12.50 12.50</cell><cell>47.21</cell><cell>25.61</cell></row><row><cell>ISIC</cell><cell>57.15</cell><cell>18.08</cell><cell>18.37</cell><cell>16.46</cell><cell>22.47</cell><cell>11.29 12.52</cell><cell>21.72</cell><cell>14.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Classification accuracy on five datasets with model heterogeneity. .00 15.72 35.66 7.13 67.23 37.10 28.15 39.54 7.13 13.47 45.30 29.68 19.54 DENSE [33] 46.86 66.88 44.00 33.08 38.28 23.47 67.93 40.70 28.68 36.70 34.67 13.42 44.00 39.37 38.37 FedISCA 87.96 71.17 70.00 83.02 61.74 73.43 69.23 64.80 82.73 51.95 44.20 16.61 62.00 72.26 43.80 w/o N [30] 87.55 69.93 51.20 74.05 57.90 68.78 68.93 61.00 72.79 46.91 43.85 15.61 51.50 64.65 39.89</figDesc><table><row><cell></cell><cell>IID</cell><cell></cell><cell>Dirichlet (α = 0.6)</cell><cell>Dirichlet (α = 0.3)</cell></row><row><cell></cell><cell cols="2">Blood Derma Oct</cell><cell>Path Tissue Blood Derma Oct</cell><cell>Path Tissue Blood Derma Oct</cell><cell>Path Tissue</cell></row><row><cell>DAFL [2]</cell><cell>7.13</cell><cell>65.69 25</cell><cell></cell></row></table><note><p>datasets (except Tissue), but its accuracy is lower than FedAvg(1) on large-scale datasets. This suggests a difficulty in large-scale image generation i.e., the generator in DENSE deteriorates global model training and leads to lower accuracy, while FedAvg(1) achieves high accuracy due to the low client model variance e.g., RSNA(=0.61), Diabetic(=0.04), and ISIC(=0.09).</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by funding from the <rs type="programName">DGIST R&amp;D program</rs> of the <rs type="funder">Ministry of Science and ICT of KOREA</rs> (<rs type="grantNumber">22-KUJoint-02</rs>) and the framework of international cooperation program managed by the <rs type="funder">National Research Foundation of Korea</rs> (<rs type="grantNumber">NRF-2022K2A9A1A01097840</rs>) and the <rs type="funder">NRF</rs> grant funded by the <rs type="funder">Korean Government (MSIT)</rs>(No. <rs type="grantNumber">2019R1C1C1008727</rs>) and the <rs type="funder">National Institute of Health</rs> (<rs type="grantNumber">MH113406</rs>, <rs type="grantNumber">DA057567</rs>, <rs type="grantNumber">AA021697</rs>) and by the <rs type="institution">Stanford HAI Google Cloud Credit</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tDr4UeR">
					<idno type="grant-number">22-KUJoint-02</idno>
					<orgName type="program" subtype="full">DGIST R&amp;D program</orgName>
				</org>
				<org type="funding" xml:id="_Amu5txc">
					<idno type="grant-number">NRF-2022K2A9A1A01097840</idno>
				</org>
				<org type="funding" xml:id="_tA9PQ3Y">
					<idno type="grant-number">2019R1C1C1008727</idno>
				</org>
				<org type="funding" xml:id="_8Uk2KJh">
					<idno type="grant-number">MH113406</idno>
				</org>
				<org type="funding" xml:id="_bcF8D6r">
					<idno type="grant-number">DA057567</idno>
				</org>
				<org type="funding" xml:id="_Nw5fnVy">
					<idno type="grant-number">AA021697</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_49.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by looking at noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baradad Jurjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2556" to="2569" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data-free learning of student networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3514" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature re-calibration based multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chikontwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_41" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="420" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02288</idno>
		<title level="m">Bcn20000: dermoscopic lesions in the wild</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Heterogeneity for the win: one-shot federated clustering</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2611" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m">EyePACS: Diabetic retinopathy detection</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.11175</idno>
		<title level="m">One-shot federated learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic bank learning for semi-supervised federated image diagnosis with class imbalance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_19" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="196" to="206" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Conditional gan with 3d discriminator for MRI generation of Alzheimer&apos;s disease progression. Pattern Recogn</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">109061</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional rnn-based few shot learning for 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chikontwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1808" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical one-shot federated learning for cross-silo setting</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adaptive batch normalization for practical domain adaptation. Pattern Recogn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ensemble distillation for robust model fusion in federated learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2351" to="2363" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Intervention &amp; interaction federated abnormality detection with noisy clients</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_30" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="309" to="319" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.15278</idno>
		<title level="m">Data-free knowledge transfer: a survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="1273" to="1282" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Zero-shot knowledge transfer via adversarial belief matching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contrastive re-localization and history distillation in federated cmr segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_25" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="256" to="265" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discovering and overcoming limitations of noiseengineered data-free knowledge distillation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Raikwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m">RSNA: Rsna pneumonia detection challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Datasets and benchmarks for cross-silo federated learning in realistic healthcare settings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ogier Du Terrail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14795</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dreaming to distill: data-free knowledge transfer via deepinversion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8715" to="8724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric federated learning of neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Khazaeni</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7252" to="7261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVC</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dense: data-free one-shot federated learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The diversified ensemble neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16001" to="16011" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Distilled one-shot federated learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07999</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Federated medical image analysis with virtual sample synthesis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_70</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_70" />
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="728" to="738" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
