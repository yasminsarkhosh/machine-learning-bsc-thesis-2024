<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Solving Low-Dose CT Reconstruction via GAN with Local Coherence</title>
				<funder ref="#_Eh8ym2Q">
					<orgName type="full">Provincial NSF of Anhui</orgName>
				</funder>
				<funder ref="#_2kzF5dv">
					<orgName type="full">National Key R&amp;D program of China</orgName>
				</funder>
				<funder ref="#_4KxHVRX">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenjie</forename><surname>Liu</surname></persName>
							<idno type="ORCID">0000-0002-4524-8507</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hu</forename><surname>Ding</surname></persName>
							<email>huding@ustc.edu.cn</email>
							<idno type="ORCID">0000-0002-1307-6077</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Solving Low-Dose CT Reconstruction via GAN with Local Coherence</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="524" to="534"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CE1683FD2A09EEE9EA1636EFC46DE054</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_50</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CT reconstruction</term>
					<term>Low-dose</term>
					<term>Generative adversarial networks</term>
					<term>Local coherence</term>
					<term>Optical flow</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Computed Tomography (CT) for diagnosis of lesions in human internal organs is one of the most fundamental topics in medical imaging. Low-dose CT, which offers reduced radiation exposure, is preferred over standard-dose CT, and therefore its reconstruction approaches have been extensively studied. However, current low-dose CT reconstruction techniques mainly rely on model-based methods or deep-learning-based techniques, which often ignore the coherence and smoothness for sequential CT slices. To address this issue, we propose a novel approach using generative adversarial networks (GANs) with enhanced local coherence. The proposed method can capture the local coherence of adjacent images by optical flow, which yields significant improvements in the precision and stability of the constructed images. We evaluate our proposed method on real datasets and the experimental results suggest that it can outperform existing state-of-the-art reconstruction approaches significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computed Tomography (CT) is one of the most widely used technologies in medical imaging, which can assist doctors for diagnosing the lesions in human internal organs. Due to harmful radiation exposure of standard-dose CT, the low dose CT is more preferable in clinical application <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">34]</ref>. However, when the dose is low together with the issues like sparse-view or limited angles, it becomes quite challenging to reconstruct high-quality CT images. The high-quality CT images are important to improve the performance of diagnosis in clinic <ref type="bibr" target="#b26">[27]</ref>. In mathematics, we model the CT imaging as the following procedure y = T (x r ) + δ, <ref type="bibr" target="#b0">(1)</ref> where x r ∈ R d denotes the unknown ground-truth picture, y ∈ R m denotes the received measurement, and δ is the noise. The function T represents the forward operator that is analogous to the Radon transform, which is widely used in medical imaging <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>. The problem of CT reconstruction is to recover x r from the received y.</p><p>Solving the inverse problem of ( <ref type="formula">1</ref>) is often very challenging if there is no any additional information. If the forward operator T is well-posed and δ is neglectable, we know that an approximate x r can be easily obtained by directly computing T -1 (y). However, T is often ill-posed, which means the inverse function T -1 does not exist and the inverse problem of (1) may have multiple solutions. Moreover, when the CT imaging is low-dose, the filter backward projection (FBP) <ref type="bibr" target="#b10">[11]</ref> can produce serious detrimental artifact. Therefore, most of existing approaches usually incorporate some prior knowledge during the reconstruction <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>. For example, a commonly used method is based on regularization:</p><formula xml:id="formula_0">x = argmin x T (x) -y p + λR(x), (<label>2</label></formula><formula xml:id="formula_1">)</formula><p>where • p denotes the p-norm and R(x) denotes the penalty item from some prior knowledge.</p><p>In the past years, a number of methods have been proposed for designing the regularization R. The traditional model-based algorithms, e.g., the ones using total variation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>, usually apply the sparse gradient assumptions and run an iterative algorithm to learn the regularizers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref>. Another popular line for learning the regularizers comes from deep learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>; the advantage of the deep learning methods is that they can achieve an end-to-end recovery of the true image x r from the measurement y <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>. Recent researches reveal that convolutional neural networks (CNNs) are quite effective for image denoising, e.g., the CNN based algorithms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">34]</ref> can directly learn the reconstructed mapping from initial measurement reconstructions (e.g., FBP) to the ground-truth images. The dual-domain network that combines the sinograms with reconstructed low-dose CT images were also proposed to enhance the generalizability <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>A major drawback of the aforementioned reconstruction methods is that they deal with the input CT 2D slices independently (note that the goal of CT reconstruction is to build the 3D model of the organ). Namely, the neighborhood correlations among the 2D slices are often ignored, which may affect the reconstruction performance in practice. In the field of computer vision, "optical flow" is a common technique for tracking the motion of object between consecutive frames, which has been applied to many different tasks like video generation <ref type="bibr" target="#b35">[35]</ref>, prediction of next frames <ref type="bibr" target="#b21">[22]</ref> and super resolution synthesis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">31]</ref>. To estimate the optical flow field, existing approaches include the traditional brightness gradient methods <ref type="bibr" target="#b1">[2]</ref> and the deep networks <ref type="bibr" target="#b6">[7]</ref>. The idea of optical flow has also been used for tracking the organs movement in medical imaging <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">33]</ref>. However, to the best of our knowledge, there is no work considering GANs with using optical flow to capture neighbor slices coherence for low dose 3D CT reconstruction.</p><p>In this paper, we propose a novel optical flow based generative adversarial network for 3D CT reconstruction. Our intuition is as follows. When a patient is located in a CT equipment, a set of consecutive cross-sectional images are generated. If the vertical axial sampling space of transverse planes is small, the corresponding CT slices should be highly similar. So we apply optical flow, though there exist several technical issues waiting to solve for the design and implementation, to capture the local coherence of adjacent CT images for reducing the artifacts in low-dose CT reconstruction. Our contributions are summarized below:</p><p>1. We introduce the "local coherence" by characterizing the correlation of consecutive CT images, which plays a key role for suppressing the artifacts. 2. Together with the local coherence, our proposed generative adversarial networks (GANs) can yield significant improvement for texture quality and stability of the reconstructed images. 3. To illustrate the efficiency of our proposed approach, we conduct rigorous experiments on several real clinical datasets; the experimental results reveal the advantages of our approach over several state-of-the-art CT reconstruction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we briefly review the framework of the ordinary generative adversarial network, and also introduce the local coherence of CT slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Adversarial Network.</head><p>Traditional generative adversarial network <ref type="bibr" target="#b7">[8]</ref> consists of two main modules, a generator and a discriminator. The generator G is a mapping from a latent-space Gaussian distribution P Z to the synthetic sample distribution P XG , which is expected to be close to the real sample distribution P X . On the other hand, the discriminator D aims to maximize the distance between the distributions P XG and P X . The game between the generator and discriminator actually is an adversarial process, where the overall optimization objective follows a min-max principle:</p><formula xml:id="formula_2">min G max D E x r ∼PX ,z∼PZ (log(D(x r )) + log(1 -D(G(z))). (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>Local Coherence. As mentioned in Sect. 1, optical flow can capture the temporal coherence of object movements, which plays a crucial role in many videorelated tasks. More specifically, the optical flow refers to the instantaneous velocity of pixels of moving objects on consecutive frames over a short period of time <ref type="bibr" target="#b1">[2]</ref>. The main idea relies on the practical assumptions that the brightness of the object more likely remains stable across consecutive frames, and the brightness of the pixels in a local region are usually changed consistently <ref type="bibr" target="#b8">[9]</ref>.</p><p>Based on these assumptions, the brightness of optical flow can be described by the following equation:</p><formula xml:id="formula_4">∇I w • v w + ∇I h • v h + ∇I t = 0,<label>(4)</label></formula><p>where v = (v w , v h ) represents the optical flow of the position (w, h) in the image. ∇I = (∇I w , ∇I h ) denotes spatial gradients of image brightness, and ∇I t denotes the temporal partial derivative of the corresponding region. Following the Eq. ( <ref type="formula" target="#formula_4">4</ref>), we consider the question that whether the optical flow idea can be applied to 3D CT reconstruction. In practice, the brightness of adjacent CT images often has very tiny difference, due to the inherent continuity and structural integrity of human body. Therefore, we introduce the "local coherence" that indicates the correlation between adjacent images of a tissue. Namely, adjacent CT images often exhibit significant similarities within a certain local range along the vertical axis of the human body. Due to the local coherence, the noticeable variations observed in CT slices within the local range often occur at the edges of organs. We can substitute the temporal partial derivative ∇I t by the vertical axial partial derivative ∇I z in the Eq. ( <ref type="formula" target="#formula_4">4</ref>), where "z" indicates the index of the vertical axis. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the local coherence can be captured by the optical flow between adjacent CT slices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GANs with Local Coherence</head><p>In this section, we introduce our low-dose CT image generation framework with local coherence in detail.</p><p>The Framework of Our Network. The proposed framework comprises three components, including a generator G, a discriminator D and an optical flow estimator F. The generator is the core component, and the flow estimator provides auxiliary warping images for the generation process.</p><p>Suppose we have a sequence of measurements y 1 , y 2 , • • • , y n ; for each y i , 1 ≤ i ≤ n, we want to reconstruct its ground truth image x r i as the Eq. ( <ref type="formula">1</ref>). Before performing the reconstruction in the generator G, we apply some prior knowledge in physics and run filter backward projection on the measurement y i in Eq. ( <ref type="formula">1</ref>) to obtain an initial recovery solution s i . Usually s i contains significant noise comparing with the ground truth x r i . Then the network has two input components, i.e., the initial backward projected image s i that serves as an approximation of the ground truth x r i , and a set of neighbor CT slices N (s i ) = {s i-1 , s i+1 }<ref type="foot" target="#foot_0">1</ref> for preserving the local coherence. The overall structure of our framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Below, we introduce the three key parts of our framework separately. Optical Flow Estimator. The optical flow F(N (s i ), s i ) denotes the brightness changes of pixels from N (s i ) to s i , where it captures their local coherence. The estimator is derived by the network architecture of FlowNet <ref type="bibr" target="#b6">[7]</ref>. The FlowNet is an autoencoder architecture with extraction of features of two input frames to learn the corresponding flow, which is consist of 6 (de)convolutional layers for both encoder and decoder.</p><p>Discriminator. The discriminator D assigns the label "1" to real standarddose CT images and "0" to generated images. The goal of D is to maximize the separation between the distributions of real images and generated images:</p><formula xml:id="formula_5">L D = n i=1 -(log(D(x r i )) + log(1 -D(x g i ))),<label>(5)</label></formula><p>where x g i is the image generated by G (the formal definition for x g i will be introduced below). The discriminator includes 3 residual blocks, with 4 convolutional layers in each residual block.</p><p>Generator. We use the generator G to reconstruct the high-quality CT image for the ground truth x r i from the low-dose image s i . The generated image is obtained by</p><formula xml:id="formula_6">x g i = G(s i , W(N (x g i ))); N (x g i ) = G(N (s i )),<label>(6)</label></formula><p>where W(•) is the warping operator. Before generating x g i , N (x g i ) is reconstructed from N (s i ) by the generator without considering local coherence. Subsequently, according to the optical flow F(N (s i ), s i ), we warp the reconstructed images N (x g i ) to align with the current slice by adjusting the brightness values. The warping operator W utilizes bi-linear interpolation to obtain W(N (x g i )), which enables the model to capture subtle variations in the tissue from the generated N (x g i ); also, the warping operator can reduce the influence of artifacts for the reconstruction. Finally, x g i is generated by combining s i and W(N (x g i )). Since x r i is our target for reconstruction in the i-th batch, we consider the difference between x g i and x r i in the loss. Our generator is mainly based on the network architecture of Unet <ref type="bibr" target="#b24">[25]</ref>. Partly inspired by the loss in <ref type="bibr" target="#b4">[5]</ref>, the optimization objective of the generator G comprises three items with the coefficients λ pix , λ adv , λ per ∈ (0, 1):</p><formula xml:id="formula_7">L G = λ pix L pixel + λ adv L adv + λ per L percept . (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>In <ref type="bibr" target="#b6">(7)</ref>, "L pixel " is the loss measuring the pixel-wise mean square error of the generated image x g i with respect to the ground-truth x r i . "L adv " represents the adversarial loss of the discriminator D, which is designed to minimize the distance between the generated standard-dose CT image distribution P XG and the real standard-dose CT image distribution P X . "L percept " denotes the perceptual loss, which quantifies the dissimilarity between the feature maps of x r i and x g i ; the feature maps denote the feature representation extracted from the hidden layers in the discriminator D (suppose there are t hidden layers):</p><formula xml:id="formula_9">L percept = n i=1 t j=1 D j (x r i ) -D j (x g i ) 1<label>(8)</label></formula><p>where D j (•) refers to the feature extraction performed on the j-th hidden layer. Through capturing the high frequency differences in CT images, L percept can enhance the sharpness for edges and increase the contrast for the reconstructed images. L pixel and L adv are designed to recover global structure, and L percept is utilized to incorporate additional texture details into the reconstruction process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>Datasets. First, our proposed approaches are evaluated on the "Mayo-Clinic low-dose CT Grand Challenge" (Mayo-Clinic) dataset of lung CT images <ref type="bibr" target="#b18">[19]</ref>.</p><p>The dataset contains 2250 two dimensional slices from 9 patients for training, and the remaining 128 slices from 1 patient are reserved for testing. The lowdose measurements are simulated by parallel-beam X-ray with 200 (or 150) uniform views, i.e., N v = 200 (or N v = 150), and 400 (or 300) detectors, i.e., N d = 400 (or N d = 300). In order to further verify the denoising ability of our approaches, we add the Gaussian noise with standard deviation σ = 2.0 to the sinograms after X-ray projection in 50% of the experiments. To evaluate the generalization of our model, we also consider another dataset RIDER with nonsmall cell lung cancer under two CT scans <ref type="bibr" target="#b36">[36]</ref> for testing. We randomly select 4 patients with 1827 slices from the dataset. The simulation process is identical to that of Mayo-Clinic. The proposed networks were implemented in the PyTorch framework and trained on Nvidia 3090 GPU with 100 epochs.</p><p>Baselines and Evaluation Metrics. We consider several existing popular algorithms for comparison. ( <ref type="formula">1</ref>) FBP <ref type="bibr" target="#b10">[11]</ref>: the classical filter backward projection on low-dose sinograms. ( <ref type="formula" target="#formula_0">2</ref>) FBPConvNet <ref type="bibr" target="#b9">[10]</ref>: a direct inversion network followed by the CNN after initial FBP reconstruction. ( <ref type="formula" target="#formula_2">3</ref>) LPD <ref type="bibr" target="#b0">[1]</ref>: a deep learning method based on proximal primal-dual optimization. ( <ref type="formula" target="#formula_4">4</ref>) UAR <ref type="bibr" target="#b20">[21]</ref>: an end-toend reconstruction method based on learning unrolled reconstruction operators and adversarial regularizers. Our proposed method is denoted by GAN-LC.</p><p>We set λ pix = 1.0, λ adv = 0.01 and λ per = 1.0 for the optimization objective in Eq. ( <ref type="formula" target="#formula_7">7</ref>) during our training process. Following most of the previous articles on 3D CT reconstruction, we evaluate the experimental performance by two metrics: the peak signal-to-noise ratio (PSNR) and the structural similarity index (SSIM) <ref type="bibr" target="#b32">[32]</ref>. PSNR measures the pixel differences of two images, which is negatively correlated with mean square error. SSIM measures the structure similarity between two images, which is related to the variances of the input images. For both two measures, the higher the better.</p><p>Results. a similar increasing trend with our approach across different settings but has worse reconstruction quality. To evaluate the stability and generalization of our model and the baselines trained on Mayo-Clinic dataset, we also test them on the RIDER dataset. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. Due to the bias in the datasets collected from different facilities, the performances of all the models are declined to some extents. But our proposed approach still outperforms the other models for most testing cases.</p><p>To illustrate the reconstruction performances more clearly, we also show the reconstruction results for testing images in Fig. <ref type="figure" target="#fig_2">3</ref>. We can see that our network can reconstruct the CT image with higher quality. Due to the space limit, the experimental results of different views N v and more visualized results are placed in our supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel approach for low-dose CT reconstruction using generative adversarial networks with local coherence. By considering the inherent continuity of human body, local coherence can be captured through optical flow, which is small deformations and structural differences between consecutive CT slices. The experimental results on real datasets demonstrate the advantages of our proposed network over several popular approaches. In future, we will evaluate our network on real-world CT images from local hospital and use the reconstructed images to support doctors for the diagnosis and recognition of lung nodules. Our code is publicly available at https://github.com/lwjie595/GANLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Registration</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The optical flow between two adjacent CT slices. The scanning window of Xray slides from the position of the left image to the position of the right image. The directions and lengths of the red arrows represent the optical flow field. The left and right images share the local coherence and thus the optical flows are small. (Color figure online)</figDesc><graphic coords="4,104,97,312,35,242,29,116,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of our generate adversarial network with local coherence for CT reconstruction.</figDesc><graphic coords="5,42,30,244,31,297,97,156,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Reconstruction results on Mayo-Clinic dataset. The sparse view setting of sinograms is Nv = 200, N d = 400 and σ = 2.0. "Ground Truth" is the standard-dose CT image.</figDesc><graphic coords="9,90,81,54,50,242,29,180,22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Table1presents the results on the Mayo-Clinic dataset, where the first row represents different parameter settings (i.e., the number of uniform views N v , the number of detectors N d and the standard deviation of Gaussian noise σ) for simulating low-dose sinograms. Our proposed approach GAN-LC consistently outperforms the baselines under almost all the low-dose parameter settings. The methods FBP and UAR are very sensitive to noise; the performance of LPD is relatively stable but with low reconstruction accuracy. FBPConvNet has Experimental results for Mayo-Clinic dataset. The value in first row of the table represents Nv, N d and σ for simulating low-dose sinograms, respectively.</figDesc><table><row><cell>Sinograms</cell><cell>200, 400, 0.0</cell><cell>200, 400, 2.0</cell><cell>150, 300, 0.0</cell><cell>150, 300, 2.0</cell></row><row><cell></cell><cell cols="4">PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell>FBP</cell><cell cols="4">26.449 0.721 13.517 0.191 21.460 0.616 12.593 0.168</cell></row><row><cell cols="5">FBPConvNet 38.213 0.918 30.148 0.743 35.263 0.869 29.095 0.723</cell></row><row><cell>LPD</cell><cell cols="4">28.050 0.844 28.357 0.794 28.376 0.826 27.409 0.801</cell></row><row><cell>UAR</cell><cell cols="4">33.248 0.902 22.048 0.272 29.829 0.848 21.227 0.238</cell></row><row><cell>GAN-LC</cell><cell cols="4">39.548 0.950 32.437 0.819 36.542 0.899 31.586 0.725</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental results for RIDER dataset. The value in first row of the table represents Nv, N d and σ for simulating low-dose sinograms, respectively.</figDesc><table><row><cell>Sinograms</cell><cell cols="2">200, 400, 0.0</cell><cell>200, 400, 2.0</cell><cell cols="2">150, 300, 0.0</cell><cell>150, 300, 2.0</cell></row><row><cell></cell><cell>PSNR</cell><cell cols="4">SSIM PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell>FBP</cell><cell>21.398</cell><cell cols="3">0.647 15.609 0.233 19.49</cell><cell>0.597 14.845 0.203</cell></row><row><cell cols="2">FBPConvNet 27.256</cell><cell cols="4">0.671 19.520 0.444 27.504 0.650 18.517 0.431</cell></row><row><cell>LPD</cell><cell>22.341</cell><cell cols="4">0.615 12.196 0.466 22.172 0.556 12.215 0.455</cell></row><row><cell>UAR</cell><cell>24.915</cell><cell cols="4">0.667 20.943 0.207 21.136 0.557 19.873 0.176</cell></row><row><cell>GAN-LC</cell><cell cols="5">28.861 0.721 22.624 0.517 29.171 0.705 19.607 0.470</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>If i = 1, N (s i ) = {s2}; if i = n, N (s i ) = {sn-1}.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The research of this work was supported in part by <rs type="funder">National Key R&amp;D program of China</rs> through grant <rs type="grantNumber">2021YFA1000900</rs>, the <rs type="funder">NSFC</rs> throught Grant <rs type="grantNumber">62272432</rs>, and the <rs type="funder">Provincial NSF of Anhui</rs> through grant <rs type="grantNumber">2208085MF163</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2kzF5dv">
					<idno type="grant-number">2021YFA1000900</idno>
				</org>
				<org type="funding" xml:id="_4KxHVRX">
					<idno type="grant-number">62272432</idno>
				</org>
				<org type="funding" xml:id="_Eh8ym2Q">
					<idno type="grant-number">2208085MF163</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 50.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learned primal-dual reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Öktem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1322" to="1332" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The computation of optical flow</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Beauchemin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv. (CSUR)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="466" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An algorithm for total variation minimization and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low-dose CT via convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="679" to="694" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning temporal coherence via self-supervision for GAN-based video generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="75" to="76" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning with adaptive hyper-parameters for low-dose CT image reconstruction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="648" to="660" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flownet: learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>CoRR abs/1406.2661</idno>
		<ptr target="https://arxiv.org/abs/1406.2661" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4509" to="4522" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Principles of computerized tomographic imaging</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Second order total generalized variation (TGV) for MRI</title>
		<author>
			<persName><forename type="first">F</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stollberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="480" to="491" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Total deep variation for linear inverse problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Effland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kunisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7549" to="7558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">NETT: solving inverse problems with deep neural networks. Inverse Prob</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antholzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haltmeier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">65005</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dudonet: dual domain network for CT metal artifact reduction</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10512" to="10521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low dose 4D-CT super-resolution reconstruction via inter-plane motion estimation based on optical flow</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ibragimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. Control</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page">102085</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial regularizers in inverse problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Öktem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Schönlieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast 3D reconstruction method for differential phase contrast X-ray CT</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nilchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stampanoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Express</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="14564" to="14581" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TU-FG-207A-04: overview of the low dose CT grand challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mccollough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3759" to="3760" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Part35</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3D hermite transform optical flow estimation in left ventricle CT sequences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moya-Albor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Escalante-Ramírez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Olveres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vallejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">595</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end reconstruction meets data-driven regularization for inverse problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Öktem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Schönlieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21413" to="21425" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06309</idno>
		<title level="m">Spatio-temporal video autoencoder with differentiable memory</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Ramm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Katsevich</surname></persName>
		</author>
		<title level="m">The Radon Transform and Local Tomography</title>
		<meeting><address><addrLine>Boca Raton</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The little engine that could: regularization by denoising (RED)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imag. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1804" to="1844" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DFD-Net: lung cancer detection from denoised CT scan image using deep learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Sori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Godana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Gelmecha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Comp. Sci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Toft</surname></persName>
		</author>
		<title level="m">The radon transform. Theory and Implementation</title>
		<meeting><address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Technical University of Denmark</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Plug-and-play priors for model based reconstruction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Global Conference on Signal and Information Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="945" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DuDoTrans: dual-domain transformer for sparse-view CT reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLMIR 2022</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Haq</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Johnson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Würfl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13587</biblScope>
			<biblScope unit="page" from="84" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-17247-2_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-17247-29" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06601</idno>
		<title level="m">Video-to-video synthesis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Three-dimensional surface reconstruction using optical flow for medical imaging</title>
		<author>
			<persName><forename type="first">N</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pierson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="630" to="641" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for noise reduction in low-dose CT</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2536" to="2545" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual dynamics: probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating variability in tumor measurements from same-day repeat CT scans of patients with non-small cell lung cancer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">252</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
