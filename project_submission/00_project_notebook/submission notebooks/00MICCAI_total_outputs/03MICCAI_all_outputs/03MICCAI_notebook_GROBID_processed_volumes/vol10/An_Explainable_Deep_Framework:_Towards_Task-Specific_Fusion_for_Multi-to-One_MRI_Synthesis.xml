<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis</title>
				<funder ref="#_7dmv53C">
					<orgName type="full">Guangzhou Elite Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luyi</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Nuclear Medicine</orgName>
								<orgName type="institution">Radboud University Medical Centre</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Nuclear Medicine</orgName>
								<orgName type="institution">Radboud University Medical Centre</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">GROW School for Oncology and Development Biology</orgName>
								<orgName type="institution">Maastricht University</orgName>
								<address>
									<postBox>P. O. Box 616</postBox>
									<postCode>6200 MD</postCode>
									<settlement>Maastricht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunzhi</forename><surname>Huang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Institute for AI in Medicine</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoran</forename><surname>Dou</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">School of Computing</orgName>
								<orgName type="laboratory">Centre for Computational Imaging and Simulation Technologies in Biomedicine (CISTIB)</orgName>
								<orgName type="institution">University of Leeds</orgName>
								<address>
									<settlement>Leeds</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">GROW School for Oncology and Development Biology</orgName>
								<orgName type="institution">Maastricht University</orgName>
								<address>
									<postBox>P. O. Box 616</postBox>
									<postCode>6200 MD</postCode>
									<settlement>Maastricht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">GROW School for Oncology and Development Biology</orgName>
								<orgName type="institution">Maastricht University</orgName>
								<address>
									<postBox>P. O. Box 616</postBox>
									<postCode>6200 MD</postCode>
									<settlement>Maastricht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunyao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Nuclear Medicine</orgName>
								<orgName type="institution">Radboud University Medical Centre</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tao</forename><surname>Tan</surname></persName>
							<email>taotanjs@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Applied Sciences</orgName>
								<orgName type="institution">Macao Polytechnic University</orgName>
								<address>
									<postCode>999078</postCode>
									<settlement>Macao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ritse</forename><surname>Mann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and Nuclear Medicine</orgName>
								<orgName type="institution">Radboud University Medical Centre</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="45" to="55"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">EA4F38A29F006926DC2B552C49CE51DA</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Missing-Sequence MRI Synthesis</term>
					<term>Explainable Synthesis</term>
					<term>Multi-Sequence Fusion</term>
					<term>Task-Specific Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-sequence MRI is valuable in clinical settings for reliable diagnosis and treatment prognosis, but some sequences may be unusable or missing for various reasons. To address this issue, MRI synthesis is a potential solution. Recent deep learning-based methods have achieved good performance in combining multiple available sequences for missing sequence synthesis. Despite their success, these methods lack the ability to quantify the contributions of different input sequences and estimate region-specific quality in generated images, making it hard to be practical. Hence, we propose an explainable task-specific synthesis network, which adapts weights automatically for specific sequence generation tasks and provides interpretability and reliability from two sides: (1) visualize and quantify the contribution of each input sequence in the fusion stage by a trainable task-specific weighted average module; (2) highlight the area the network tried to refine during synthesizing by a task-specific attention module. We conduct experiments on the BraTS2021 dataset of 1251 subjects, and results on arbitrary sequence synthesis indicate that the proposed method achieves better performance than the state-of-the-art methods. Our code is available at https://github.com/fiy2W/mri seq2seq.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic resonance imaging (MRI) consists of a series of pulse sequences, e.g. T1-weighted (T1), contrast-enhanced (T1Gd), T2-weighted (T2), and T2-fluidattenuated inversion recovery (Flair), each showing various contrast of water and fat tissues. The intensity contrast combination of multi-sequence MRI provides clinicians with different characteristics of tissues, extensively used in disease diagnosis <ref type="bibr" target="#b15">[16]</ref>, lesion segmentation <ref type="bibr" target="#b16">[17]</ref>, treatment prognosis <ref type="bibr" target="#b6">[7]</ref>, etc. However, some acquired sequences are unusable or missing in clinical settings due to incorrect machine settings, imaging artifacts, high scanning costs, time constraints, contrast agents allergies, and different acquisition protocols between hospitals <ref type="bibr" target="#b4">[5]</ref>. Without rescanning or affecting the downstream pipelines, the MRI synthesis technique can generate missing sequences by leveraging redundant shared information between multiple sequences <ref type="bibr" target="#b17">[18]</ref>.</p><p>Many studies have demonstrated the potential of deep learning methods for image-to-image synthesis in the field of both nature images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> and medical images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>. Most of these works introduce an autoencoder-like architecture for image-to-image translation and employ adversarial loss to generate more realistic images. Unlike these one-to-one approaches, MRI synthesis faces the challenge of fusing complementary information from multiple input sequences. Recent studies about multi-sequence fusion can specifically be divided into two groups: (1) image fusion and (2) feature fusion. The image fusion approach is to concatenate sequences as a multi-channel input. Sharma et al. <ref type="bibr" target="#b17">[18]</ref> design a network with multi-channel input and output, which combines all the available sequences and reconstructs the complete sequences at once. Li et al. <ref type="bibr" target="#b13">[14]</ref> add an availability condition branch to guide the model to adapt features for different input combinations. Dalmaz et al. <ref type="bibr" target="#b8">[9]</ref> equip the synthesis model with residual transformer blocks to learn contextual features. Image-level fusion is simple and efficient but unstable -zero-padding inputs for missing sequences lead to training unstable and slight misalignment between images can easily cause artifacts. In contrast, efforts have been made on feature fusion, which can alleviate the discrepancy across multiple sequences, as high-level features focus on the semantic regions and are less affected by input misalignment compared to images. Zhou et al. <ref type="bibr" target="#b22">[23]</ref> design operation-based (e.g. summation, product, maximization) fusion blocks to densely combine the hierarchical features. And Li et al. <ref type="bibr" target="#b14">[15]</ref> employ self-attention modules to integrate multi-level features. The model architectures of these methods are not flexible and difficult to adapt to various sequence combinations. More importantly, recent studies only focus on proposing end-to-end models, lacking quantifying the contributions for different sequences and estimating the qualities of generated images.</p><p>In this work, we propose an explainable task-specific fusion sequence-tosequence (TSF-Seq2Seq) network, which has adaptive weights for specific synthesis tasks with different input combinations and targets. Specially, this framework can be easily extended to other tasks, such as segmentation. Our primary contributions are as follows: <ref type="bibr" target="#b0">(1)</ref> We propose a flexible network to synthesize the target MRI sequence from an arbitrary combination of inputs; <ref type="bibr" target="#b1">(2)</ref> The network shows interpretability for fusion by quantifying the contribution of each input sequence; (3) The network provides reliability for synthesis by highlighting the area the network tried to refine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the overview of the proposed TSF-Seq2Seq network. Our network has an autoencoder-like architecture including an encoder E, a multisequence fusion module, and a decoder G. Available MRI sequences are first encoded to features by E, respectively. Then features from multiple input sequences are fused by giving the task-specific code, which identifies sources and targets with a binary code. Finally, the fused features are decoded to the target sequence by G. Furthermore, to explain the mechanism of multi-sequence fusion, our network can quantify the contributions of different input sequences with the task-specific weighted average module and visualize the TSEM with the task-specific attention module.</p><p>To leverage shared information between sequences, we use E and G from Seq2Seq <ref type="bibr" target="#b9">[10]</ref>, which is a one-to-one synthetic model that integrates arbitrary sequence synthesis into single E and G. They can reduce the distance between different sequences at the feature level to help more stable fusion. Details of the multi-sequence fusion module and TSEM are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-sequence Fusion</head><p>Define a set of N sequences MRI: X = {X i |i = 1, ..., N } and corresponding available indicator A ⊂ {1, ..., N } and A = ∅. Our goal is to predict the target set</p><formula xml:id="formula_0">X T = {X i |i /</formula><p>∈ A} by giving the available set X A = {X i |i ∈ A} and the corresponding task-specific code c = {c src , c tgt } ∈ Z 2N . As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, c src and c tgt are zero-one codes for the source and the target set, respectively. To fuse multiple sequences at the feature level, we first encode images and concatenate the features as f = {E(X i )|i = 1, ..., N }. Specifically, we use zero-filled placeholders with the same shape as E(X i ) to replace features of i / ∈ A to handle arbitrary input sequence combinations. The multi-sequence fusion module includes: (1) a task-specific weighted average module for the linear combination of available features; (2) a task-specific attention module to refine the fused features.</p><p>Task-Specific Weighted Average. The weighted average is an intuitive fusion strategy that can quantify the contribution of different sequences directly. To learn the weight automatically, we use a trainable fully connected (FC) layer to predict the initial weight ω 0 ∈ R N from c.</p><formula xml:id="formula_1">ω 0 = softmax(cW + b) + (1)</formula><p>where W and b are weights and bias for the FC layer, = 10 -5 to avoid dividing 0 in the following equation. To eliminate distractions and accelerate training, we force the weights of missing sequences in ω 0 to be 0 and guarantee the output</p><formula xml:id="formula_2">ω ∈ R N to sum to 1. ω = ω 0 • c src ω 0 , c src<label>(2)</label></formula><p>where • refers to the element-wise product and •, • indicates the inner product.</p><p>With the weights ω, we can fuse multi-sequence features as f by the linear combination.</p><formula xml:id="formula_3">f = f, ω<label>(3)</label></formula><p>Specially, f ≡ E(X i ) when only one sequence i is available, i.e. A = {i}. It demonstrates that the designed ω can help the network excellently inherit the synthesis performance of pre-trained E and G. In this work, we use ω to quantify the contribution of different input combinations.</p><p>Task-Specific Attention. Apart from the sequence-level fusion of f , a taskspecific attention module G A is introduced to refine the fused features at the pixel level. The weights of G A can adapt to the specific fusion task with the given target code. To build a conditional attention module, we replace convolutional layers in convolutional block attention module (CBAM) <ref type="bibr" target="#b19">[20]</ref> with Hyper-Conv <ref type="bibr" target="#b9">[10]</ref>. HyperConv is a dynamic filter whose kernel is mapped from a shared weight bank, and the mapping function is generated by the given target code.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, channel attention and spatial attention can provide adaptive feature refinement guided by the task-specific code c to generate residual attentional fused features f A .</p><formula xml:id="formula_4">f A = G A ( f |c) (4)</formula><p>Loss Function. To force both f and f + f A can be reconstructed to the target sequence by the conditional G, a supervised reconstruction loss is given as,</p><formula xml:id="formula_5">L rec =λ r • X -X tgt 1 + λ p • L p (X , X tgt ) +λ r • X A -X tgt 1 + λ p • L p (X A , X tgt )<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">X = G( f |c tgt ), X A = G( f + f A |c tgt ), X tgt ∈ X T ,</formula><p>• 1 refers to a L 1 loss, and L p indicates the perceptual loss based on pre-trained VGG19. λ r and λ p are weight terms and are experimentally set to be 10 and 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task-Specific Enhanced Map</head><p>As f A is a task-specific contextual refinement for fused features, analyzing it can help us understand more what the network tried to do. Many studies focus on visualizing the attention maps to interpret the principle of the network, especially for the transformer modules <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. However, visualization of the attention map is limited by its low resolution and rough boundary. Thus, we proposed the TSEM by subtracting the reconstructed target sequences with and without f A , which has the same resolution as the original images and clear interpretation for specific tasks. TSEM</p><formula xml:id="formula_7">= |X A -X | (6)</formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metrics</head><p>We use brain MRI images of 1,251 subjects from Brain Tumor Segmentation 2021 (BraTS2021) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref>, which includes four aligned sequences, T1, T1Gd, T2, and Flair, for each subject. We select 830 subjects for training, 93 for validation, and 328 for testing. All the images are intensity normalized to [-1, 1] and central cropped to 128 × 192 × 192. During training, for each subject, a random number of sequences are selected as inputs and the rest as targets. For validation and testing, we fixed the input combinations and the target for each subject. The synthesis performance is quantified using the metrics of peak signal noise rate (PSNR), structural similarity index measure (SSIM), and learned perceptual image patch similarity (LPIPS) <ref type="bibr" target="#b20">[21]</ref>, which evaluate from intensity, structure, and perceptual aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The models are implemented with PyTorch and trained on the NVIDIA GeForce RTX 3090 Ti GPU. E comprises three convolutional layers and six residual blocks. The initial convolutional layer is responsible for encoding intensities to features, while the second and third convolutional layers downsample images by a factor of four. The residual blocks then extract the high-level representation. The 28.5 ± 2.5 0.883 ± 0.040 9.65 ± 3.57 DiamondGAN <ref type="bibr" target="#b13">[14]</ref> 28.2 ± 2.5 0.877 ± 0.041 10.20 ± 3.33 ResViT <ref type="bibr" target="#b8">[9]</ref> 28.3 ± 2.4 0.882 ± 0.039 9.87 ± 3.30 Seq2Seq <ref type="bibr" target="#b9">[10]</ref> (Average) 28.5 ± 2.3 0.880 ± 0.038 11.61 ± 3.87 TSF-Seq2Seq (w/o fA) 28.3 ± 2.6 0.876 ± 0.044 9.61 ± 4.00 TSF-Seq2Seq 28.8 ± 2.6 0.887 ± 0.042 8.89 ± 3.80 channels are 64, 128, 256, and 256, respectively. G has an inverse architecture with E, and all the convolutional layers are replaced with HyperConv. The E and G from Seq2Seq are pre-trained using the Adam optimizer with an initial learning rate of 2 × 10 -4 and a batch size of 1 for 1,000,000 steps, taking about 60 h. Then we finetune the TSF-Seq2Seq with the frozen E using the Adam optimizer with an initial learning rate of 10 -4 and a batch size of 1 for another 300,000 steps, taking about 40 h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quantitative Results</head><p>We compare our method with one-to-one translation, image-level fusion, and feature-level fusion methods. One-to-one translation methods include Pix2Pix <ref type="bibr" target="#b11">[12]</ref> and Seq2Seq <ref type="bibr" target="#b9">[10]</ref>. Image-level fusion methods consist of MM-GAN <ref type="bibr" target="#b17">[18]</ref>, DiamondGAN <ref type="bibr" target="#b13">[14]</ref>, and ResViT <ref type="bibr" target="#b8">[9]</ref>. Feature-level fusion methods include Hi-Net <ref type="bibr" target="#b22">[23]</ref> and MMgSN-Net <ref type="bibr" target="#b14">[15]</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows the examples of synthetic T2 of comparison methods input with the combinations of T1Gd and Flair. Table <ref type="table" target="#tab_0">1</ref> reports the sequence synthesis performance for comparison methods organized by the different numbers of input combinations. Note that, for multiple inputs, one-to-one translation methods synthesize multiple outputs separately and average them as one. And Hi-Net <ref type="bibr" target="#b22">[23]</ref> and MMgSN-Net <ref type="bibr" target="#b14">[15]</ref> only test on the subset with two inputs due to fixed network architectures. As shown in Table <ref type="table" target="#tab_0">1</ref>, the proposed method achieves the best performance in different input combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>We compare two components of our method, including (1) task-specific weighted average and (2) task-specific attention, by conducting an ablation study between Seq2Seq, TSF-Seq2Seq (w/o f A ), and TSF-Seq2Seq. TSF-Seq2Seq (w/o f A ) refers to the model removing the task-specific attention module. As shown in Table <ref type="table" target="#tab_0">1</ref>, when only one sequence is available, our method can inherit the performance of Seq2Seq and achieve slight improvements. For multi-input situations, the task-specific weighted average can decrease LPIPS to achieve better perceptual performance. And task-specific attention can refine the fused features to achieve the best synthesis results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Interpretability Visualization</head><p>The proposed method not only achieves superior synthesis performance but also has good interpretability. In this section, we will visualize the contribution of different input combinations and TSEM.  Sequence Contribution. We use ω in Eq. 2 to quantify the contribution of different input combinations for synthesizing different target sequences. Figure <ref type="figure" target="#fig_2">3</ref> shows the bar chart for the sequence contribution weight ω with different taskspecific code c. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, both T1 and T1Gd contribute greatly to the sequence synthesis of each other, which is expected because T1Gd are T1weighted scanning after contrast agent injection, and the enhancement between these two sequences is indispensable for cancer detection and diagnosis. The less contribution of T2, when combined with T1 and/or T1Gd, is consistent with the clinical findings <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> that T2 can be well-synthesized by T1 and/or T1Gd.</p><p>TSEM vs. Attention Map. Figure <ref type="figure" target="#fig_3">4</ref> shows the proposed TSEM and the attention maps extracted by ResViT <ref type="bibr" target="#b8">[9]</ref>. As shown in Fig. <ref type="figure" target="#fig_3">4</ref>, TSEM has a higher resolution than the attention maps and can highlight the tumor area which is hard to be synthesized by the networks. Table <ref type="table" target="#tab_1">2</ref> reports the results of PSNR for regions highlighted or not highlighted by TSEM with a threshold of the 99th percentile. To assist the synthesis models deploying in clinical settings, TSEM can be used as an attention and uncertainty map to remind clinicians of the possible unreliable synthesized area. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we introduce an explainable network for multi-to-one synthesis with extensive experiments and interpretability visualization. Experimental results based on BraTS2021 demonstrate the superiority of our approach compared with the state-of-the-art methods. And we will explore the proposed method in assisting downstream applications for multi-sequence analysis in future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the TSF-Seq2Seq network. By giving the task-specific code, TSF-Seq2Seq can synthesize a target sequence from existing sequences, and meanwhile, output the weight of input sequences ω and the task-specific enhanced map (TSEM).</figDesc><graphic coords="3,105,96,53,72,240,40,128,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of synthetic T2 of comparison methods given the combination of T1Gd and Flair.</figDesc><graphic coords="7,55,98,136,82,340,96,66,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Bar chart for the weights of the input set of sequences to synthesize different target sequences: (a) T1; (b) T1Gd; (c) T2; (d) Flair.</figDesc><graphic coords="8,41,79,53,93,340,72,193,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of the proposed TSEM and the attention maps extracted byResViT<ref type="bibr" target="#b8">[9]</ref> when generating T1Gd by given with T1, T2, and Flair.</figDesc><graphic coords="9,101,88,53,87,286,72,209,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results for a set of sequences to a target sequence synthesis on BraTS2021.</figDesc><table><row><cell>Number</cell><cell>Methods</cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>LPIPS↓</cell></row><row><cell>of inputs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>Pix2Pix [12]</cell><cell cols="3">25.6 ± 3.1 0.819 ± 0.086 15.85 ± 9.41</cell></row><row><cell></cell><cell>MM-GAN [18]</cell><cell cols="3">27.3 ± 2.4 0.864 ± 0.039 11.47 ± 3.76</cell></row><row><cell></cell><cell>DiamondGAN [14]</cell><cell cols="3">27.0 ± 2.3 0.857 ± 0.040 11.95 ± 3.65</cell></row><row><cell></cell><cell>ResViT [9]</cell><cell cols="3">26.8 ± 2.1 0.857 ± 0.037 11.82 ± 3.54</cell></row><row><cell></cell><cell>Seq2Seq [10]</cell><cell cols="3">27.7 ± 2.4 0.869 ± 0.038 10.49 ± 3.63</cell></row><row><cell></cell><cell cols="4">TSF-Seq2Seq (w/o fA) 27.8 ± 2.4 0.871 ± 0.039 10.15 ± 3.67</cell></row><row><cell></cell><cell>TSF-Seq2Seq</cell><cell cols="3">27.8 ± 2.4 0.872 ± 0.039 10.16 ± 3.69</cell></row><row><cell>2</cell><cell cols="4">Pix2Pix [12] (Average) 26.2 ± 2.7 0.834 ± 0.054 15.84 ± 6.05</cell></row><row><cell></cell><cell>MM-GAN [18]</cell><cell cols="3">28.0 ± 2.3 0.878 ± 0.037 10.33 ± 3.58</cell></row><row><cell></cell><cell>DiamondGAN [14]</cell><cell cols="3">27.7 ± 2.3 0.872 ± 0.038 10.82 ± 3.36</cell></row><row><cell></cell><cell>ResViT [9]</cell><cell cols="3">27.7 ± 2.2 0.875 ± 0.035 10.53 ± 3.26</cell></row><row><cell></cell><cell>Hi-Net [23]</cell><cell cols="3">27.1 ± 2.3 0.866 ± 0.039 11.11 ± 3.76</cell></row><row><cell></cell><cell>MMgSN-Net [15]</cell><cell cols="3">27.1 ± 2.7 0.865 ± 0.044 11.38 ± 4.37</cell></row><row><cell></cell><cell cols="4">Seq2Seq [10] (Average) 28.2 ± 2.2 0.879 ± 0.035 11.11 ± 3.72</cell></row><row><cell></cell><cell cols="4">TSF-Seq2Seq (w/o fA) 28.0 ± 2.4 0.875 ± 0.039 9.89 ± 3.63</cell></row><row><cell></cell><cell>TSF-Seq2Seq</cell><cell cols="3">28.3 ± 2.4 0.882 ± 0.038 9.48 ± 3.58</cell></row><row><cell>3</cell><cell cols="4">Pix2Pix [12] (Average) 26.6 ± 2.5 0.842 ± 0.041 15.77 ± 5.08</cell></row><row><cell></cell><cell>MM-GAN [18]</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of PSNR for regions highlighted or not highlighted by TSEM.</figDesc><table><row><cell>1</cell><cell>18.0 ± 3.2</cell><cell>28.3 ± 2.4</cell><cell>27.8 ± 2.4</cell></row><row><cell>2</cell><cell>18.8 ± 3.7</cell><cell>28.8 ± 2.4</cell><cell>28.3 ± 2.4</cell></row><row><cell>3</cell><cell>19.5 ± 4.0</cell><cell>29.3 ± 2.5</cell><cell>28.8 ± 2.6</cell></row></table><note><p>Number of inputs TSEM &gt; 99% TSEM &lt; 99% Total</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. <rs type="person">Luyi Han</rs> was funded by <rs type="grantName">Chinese Scholarship Council (CSC) scholarship</rs>. <rs type="person">Tianyu Zhang</rs> was supported by the <rs type="funder">Guangzhou Elite Project</rs> (<rs type="grantNumber">TZ-JY201948</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7dmv53C">
					<idno type="grant-number">TZ-JY201948</idno>
					<orgName type="grant-name">Chinese Scholarship Council (CSC) scholarship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 5.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00928</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Medgan: medical image translation using gans</title>
		<author>
			<persName><forename type="first">K</forename><surname>Armanious</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">101684</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification</title>
		<author>
			<persName><forename type="first">U</forename><surname>Baid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal mr synthesis via modality-invariant latent representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="803" to="814" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transformer interpretability beyond attention visualization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="782" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clinical application of magnetic resonance imaging in management of breast cancer patients receiving neoadjuvant chemotherapy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed Res. Int</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stargan: unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Resvit: residual vision transformers for multimodal medical image synthesis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dalmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2598" to="2614" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Synthesis-based imaging-differentiation representation learning for multi-sequence 3d/4d mri</title>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00517</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised imageto-image translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional GAN with an attention-based generator and a 3D discriminator for 3D medical image generation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-131" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DiamondGAN: unified multi-modal generative adversarial networks for MRI sequences synthesis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_87</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-987" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="795" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Virtual contrast-enhanced magnetic resonance images synthesis for patients with nasopharyngeal carcinoma using multimodality-guided synergistic neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Radiat. Oncol.* Biol.* Phys</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1033" to="1044" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Breast mri: state of the art</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="520" to="536" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Missing mri pulse sequence synthesis using multimodal generative adversarial network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1170" to="1183" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Memory-efficient gan-based domain translation of high resolution 3d medical images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Uzunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Handels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">101801</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cbam: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Important-net: integrated mri multi-parameter reinforcement fusion generator with attention network for synthesizing absent data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01788</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hi-net: hybrid-fusion network for multi-modal mr image synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2772" to="2781" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
