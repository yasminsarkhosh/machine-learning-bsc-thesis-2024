<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior</title>
				<funder ref="#_MnvHaCx">
					<orgName type="full">Shenzhen Fundamental Research Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kangrong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Guangdong Provincial Key Laboratory of Popular High Performance Computers</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Key Laboratory of Service Computing and Application</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qirui</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Guangdong Provincial Key Laboratory of Popular High Performance Computers</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Key Laboratory of Service Computing and Application</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
							<email>yangxuan@szu.edu.cn</email>
							<idno type="ORCID">0000-0002-6680-6934</idno>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Guangdong Provincial Key Laboratory of Popular High Performance Computers</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Key Laboratory of Service Computing and Application</orgName>
								<address>
									<settlement>Shenzhen, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="581" to="591"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">85CED3B88D5A1C4BF5370432483D5BFD</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Variational inference</term>
					<term>Transformer</term>
					<term>Cross attention</term>
					<term>Compact support radial basis function (CSRBF)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The variational registration model takes advantage of explaining uncertainties of registration results. However, most existing variational registration models are based on convolutional neural networks (CNNs), which cannot capture distant information in images. Besides, the evidence lower bound (ELBO) and the commonly used standard prior cannot close the gap between the real posterior and the variational posterior in the vanilla variational registration model. This paper proposes a network in a variational image registration model for cardiac motion estimation to effectively capture the spatial correspondence of long-distance images and solve the shortcomings of CNNs. Our proposed network comprises a Transformer with a T2T module and the cross attention between the moving and the fixed images. To close the gap between the real posterior and the variational posterior, the importance-weighted evidence lower bound (iwELBO) is introduced into the variational registration model with an implicit prior. The coefficients of a parametric transformation using multi-supports CSRBFs are latent variables in our variational registration model, which improve registration accuracy significantly. Experimental results show that the proposed method outperforms state-of-arts research on public cardiac datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cardiac motion estimation is vital in evaluating cardiac function, detecting heart diseases, and understanding cardiac biomechanics. Deformable image registration (DIR) is the critical technique of cardiac motion estimation. It minimizes the differences between the warped moving and fixed images to estimate a displacement vector field (DVF). Unsupervised deep-learning-based image registration has recently become mainstream due to the required non-annotation data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> and rapid inference performance when the network is well-trained.</p><p>The probabilistic generative model shows potential in the unsupervised learning registration <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">22]</ref>. It allows the registration framework to be highly adaptable and can be applied to cases with a small amount of data and anatomical variability. Another advantage of the probabilistic generative registration model is that it can provide registration uncertainties <ref type="bibr" target="#b0">[1]</ref>, which plays a vital role in clinical application <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>. However, several issues exist with variational image registration approaches. The first is that traditional convolutions are limited in representing long-range relationships between image features. The second issue is a gap between the objective function ELBO and the log-likelihood of input image pairs, which deteriorates registration accuracy. Besides, nonparametric transformation is commonly used <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">27]</ref>, which has the challenge of regularizing the DVF smooth and topology-preserving.</p><p>This paper proposed a novel variational image registration model to cope with the above issues by employing the Transformers with the cross-attention mechanism and introducing an importance-weighted ELBO (iwELBO) <ref type="bibr" target="#b6">[7]</ref> with an implicit prior. Detailed contributions of our work include:</p><p>-A novel VAE network architecture is proposed, which employs the Transformer architecture to focus on cross-attention between the moving and fixed images. The predictive results of the transformation parameter distribution using our architecture are more accurate than traditional VAE architecture. -We optimized the importance-weighted ELBO in the variational image registration model. We use approximated aggregated posterior as the prior to regularizing posterior. To our best knowledge, we are the first to combine the iwELBO and aggregated posterior to close the gap between the real and variational posterior. -A parametric transformation based on multi-supports CSRBFs is embedded in our variational registration model. By imposing a sparse constraint, the coefficients of multi-CSRBFs are regularized to be sparse to select the optimal support for multi-support CSRBFs. The parametric transformation model improves registration accuracy significantly and makes it easy to regularize the smoothness of DVFs.</p><p>2 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Importance-Weighted Variational Image Registration Model</head><p>Given the moving and fixed images M, F , and n control points {p i } n i=1 , the parametric transformation based on multi-supports CSRBFs is</p><formula xml:id="formula_0">f z (υ) = υ + n i=1 s k=1 z i,k φ( υ-pi 2 c k ), where φ( • c</formula><p>) is a CSRBF with support c; υ -p i 2 is the Euclidean distance between the pixel υ and p i . s different supports {c k } s are provided for each CSRBF. z = {z k } s k=1 , z k = {z k,i } n i=1 is the latent variable whose distribution is required to be estimated. The parametric transformation can control deformations using different supports. By imposing sparse constraints, selecting the optimal support from these given supports is possible, leading to more flexible deformation results.</p><p>The variational registration model aims to estimate the posterior of p(z|F, M ). In the vanilla variational registration model, q(z|F, M ) is estimated to approximate p(z|F, M ) by optimizing ELBO = E z ∼q(z|F,M ) log p(F |z ,M )p(z ) q(z|F,M ) , where p(F |z, M) is the probability of occurring F when the moving image M is deformed using a transformation f z with latent variables z. It can be expressed as Boltzmann distribution p(F |z, M) ∝ e sim(F,M (fz )) using a similarity metric sim. p(z) is the prior of z.</p><p>The importance-weighted evidence lower bound (iwELBO) <ref type="bibr" target="#b6">[7]</ref> is defined as,</p><formula xml:id="formula_1">iwELBO = E z 1 ,...,z K ∼q(z|F,M ) log 1 K K k=1 p(F |z k , M)p(z k ) q(z k |F,M )<label>(1)</label></formula><p>where z 1 , . . . , z K are K-samples of latent variable z sampled from q(z|F, M ). It is assumed that q(z|F, M ) ∼ N (μ(F, M ), Σ(F, M )), and z 1 , . . . , z K is sampled as</p><formula xml:id="formula_2">z k = μ(F, M ) + Σ(F, M ) k , k ∼ N (0, I) using the reparametrization trick. Denote w k = p(F |z k ,M )p(z k ) q(z k |F,M )</formula><p>, the gradient of iwELBO can be interpreted as normalized importance-weighted average gradients of each sample, which implies the sample with larger w k contributes more to iwELBO. It is challenging to compute w k directly due to the high dimensional latent variable z k . We tackle this problem by a trick.</p><formula xml:id="formula_3">arg max iwELBO = arg max E z 1 ,...,z K ∼q(z|F,M ) log 1 K K k=1 e log w k 1 λ , λ &gt; 0. (2) Because log w k ∝ sim(F, M (f z k )) + log p(z k ) q(z k |F,M ) , our objective function is E z 1 ,...,z K ∼q(z|F,M ) log 1 K K k=1 e sim(F,M (f z k ))+ 1 λ log p(z k ) q(z k |F,M ) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>iwELBO is a tighter evidence lower bound of the log-likelihood of data; it approaches the log-likelihood log p(F |M ) as K → ∞. From the view of image registration, the iwELBO tends to converge to a transformation with the optimal w k from K samples. When a large hyperparameter λ is used,</p><formula xml:id="formula_5">1 λ log p(z k |M ) q(z k |F,M</formula><p>) is relative smaller compared with the similarity term. That implies the iwELBO prefers the sample z k leading to the optimal similarity between the warped moving and fixed images, which is beneficial to push the network to predict more accurate z. Besides, Huang et al. <ref type="bibr" target="#b14">[15]</ref> pointed out that when only one sample corresponding to a high loss is drawn to estimate the iwELBO, iwELBO tolerates this mistake due to the importance of weight. On the contrary, the sample with high loss will be highly penalized in traditional ELBO because the decoder treats the sample as real, observed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Aggregate Posterior as the Prior</head><p>A simple prior, such as the standard Normal in VAE, incurred over-regularization on the posterior and widened the gap between the variational posterior and the real posterior. Many researchers resolve this mismatch by proposing various priors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref>. Tomczak et al. stated that the optimal prior in VAE is the aggregated posterior of data. Takahashi et al. <ref type="bibr" target="#b23">[23]</ref> introduced the density ratio trick to estimate this aggregated posterior implicitly. However, all these works are evaluated based on ELBO instead of iwELBO. We derive and approximate the optimal prior based on iwELBO, please refer to part 1 in the supplement.</p><p>The optimal prior should maximize the expectation of iwELBO:</p><formula xml:id="formula_6">arg max p(z) p(F,M )E z 1 ,...,z K ∼q(z|F,M ) log 1 K K k=1 e sim(F,M (f z k ))+ 1 λ log p(z k ) q(z k |F,M ) d(F,M )<label>(4</label></formula><p>) It can be derived that the optimal prior p * (z) is approximated as the aggregated posterior E p(F,M ) q(z|F, M ). Substituting the optimal prior p * (z) into Eq. ( <ref type="formula" target="#formula_3">3</ref>), the objective function is rewritten as</p><formula xml:id="formula_7">E z 1 ,...,z K ∼q(z|F,M ) log 1 K K k=1 e sim(F,M (f z k ))+ 1 λ log p 0 (z k ) q(z k |F,M ) + 1 λ log p * (z k ) p 0 (z k ) . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where p 0 (z) is a simple given prior. To estimate the density ratio log p * (z ) p0(z ) , a binary discriminator T (z) is trained by maximizing <ref type="bibr" target="#b23">[23]</ref>,</p><formula xml:id="formula_9">max E p * (z ) [σ(T (z))] + E p0(z ) [log(1 -σ(T (z)))]<label>(6)</label></formula><p>where σ is the sigmoid function. The discriminator is a neural network composed of four fully connected layers, with the final layer outputting density ratio. A dropout layer is added before the output to prevent the discriminator network from overfitting. When T (z) is well trained,</p><formula xml:id="formula_10">log p * (z k ) p0(z k ) ≈ T (z k ). The given prior p 0 (z) is defined as N (0, B -1 ), where B = diag(B 1 , • • • , B s ), B k is a n × n matirx with the entries B k ij = φ( pi-pj c k ), k = 1, . . . , s. Then, log p 0 (z k ) q(z k |F,M ) = log |Σ(F, M )| + log |B| - 1 2 z k T Bz k -(z k -μ(F,M )) T Σ -1 (F,M )(z k -μ(F,M )) . (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where z k T Bz k is the bending energy of DVF using multi-supports CSRBFs aiming to regularize DVF smooth. The sampling size for k is 5; λ is 110000.</p><p>We optimize our network by iterating a two-step procedure. The encoder is updated using Eq. ( <ref type="formula" target="#formula_10">7</ref>) by fixing the discriminator. Next, the discriminator is updated using Eq. ( <ref type="formula" target="#formula_9">6</ref>) by fixing the encoder. Above two steps are performed alternatively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network</head><p>Our network architecture consists of an encoder and a decoder, as shown in Fig. <ref type="figure">1</ref>. The encoder composes of T2T modules <ref type="bibr" target="#b26">[26]</ref> and a Transformer to predict μ(F, M )), Σ(F, M ). T2T modules preprocess M and F and then input to our Transformer's T-encoder and T-decoder, which pays self-attention and cross-attention of M and F , respectively. Outputs of the cross-MSA (marked by orange) are fused information of M and F , weighted by similarity. More details of the Transformer Network can be seen in part 2 of the supplementary materials. The inherent ability to capture the correlation between two images makes Transformers easier to extract effective features for image registration. The decoder of our network generates the warped moving image using the DVF obtained by transformation based on multi-supports CSRBFs. The number of Transformer blocks N is 3. T2T modules tackle the partitioning issue in Transformer by modeling the local structure information iteratively. As shown in Fig. <ref type="figure">2</ref>, overlapped patches are encoded by an unfold operation as a vector token T i and re-encoded as T i using a ViT. T i is reshaped as M i+1 with the size of overlapped patches. The above process is repeated three times to obtain the final vector T i+2 , which is the input of our Transformer.</p><p>The total loss function of our network L is combing the iwELBO and sparse constraints on z as L = iwELBO -z 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Four public cardiac datasets are used to evaluate our method, including MIC-CAI2009 <ref type="bibr" target="#b18">[19]</ref>, York <ref type="bibr" target="#b2">[3]</ref>, ACDC <ref type="bibr" target="#b5">[6]</ref>, and M&amp;Ms <ref type="bibr" target="#b7">[8]</ref>. We combine MICCAI, York, and ACDC as a dataset denoted as ACDC+. Contours of the left ventricle (LV), the right ventricle (RV), or myocardium (MYO) at the end-diastolic (ED) phase or the end-systolic (ES) phase are provided by experts for different datasets. We take the ED and ES images as moving and fixed images, respectively. The training, validation, and testing slices are 1257, 130, 698 for ACDC+ and 1134, 266, 1030 for M&amp;Ms. All images are cropped into the size of 128 × 128 containing the heart. Data augmentations such as flips and rotations are used. The LCC with the size of 9 × 9 is employed as the similarity metric. 64 global control points are evenly spaced on the 128 × 128 image, while 100 local control points are evenly spaced in an area of 64 × 64 in the center of the image that contains the heart. Our network is trained using PyTorch on a computer equipped with an Intel(R) Xeon(R) Silver 4210 CPU and Nvidia RTX 2080Ti GPU. The Adam optimizer with a learning rate of 5e -4 is employed. We use the estimated DVFs to map the contours of the moving image and compare the mapped contours with the contours of the fixed image using various metrics, including the Dice score, the average perpendicular distance (APD, in mm), and 95%-tile Hausdorff Distance (HD, in mm). Moreover, we count the number of anomalies to measure the topological property of the DVFs |J fz | ≤ 0 and calculate the bending energy (BE, ×10 -4 ) of DVFs to measure their smoothness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>To compare the performance of our method with unsupervised registration networks KrebsDiff <ref type="bibr" target="#b16">[17]</ref>, DalcaDiff <ref type="bibr" target="#b10">[11]</ref>, NetGI <ref type="bibr" target="#b12">[13]</ref>, VoxelMorph <ref type="bibr" target="#b3">[4]</ref>, and Trans-morph <ref type="bibr" target="#b8">[9]</ref>. KrebsDiff, DalcaDiff, and NetGI are networks of variational registration. Transmorph is a network embedding Transformers. VoxelMorph is a vanilla unsupervised registration network. Registration results of two datasets using different networks are listed in Table <ref type="table" target="#tab_0">1</ref>. Our network outperforms other networks regarding Dice, HD, and APD. NetGI is the most similar registration model to our method, achieving the smoothest DVFs, while DalcaDiff preserved the topology of DVF best due to the diffeomorphism deformation it used. The bending energy and topology-preserving of DVFs using our network are close to that of NetGI and DalcaDiff, which implies that the transformation model based on multi-supports CSRBFs is good at generating smooth and topology-preserving DVFs. Visualization of the registration results using different networks is shown in Fig. <ref type="figure" target="#fig_1">3</ref>. The myocardium of the fixed image is marked by green, while the warped moving image is marked by blue. The overlap of the myocardium is marked by red. Here, registration results of the basal, middle, and apical slices are provided. Note that objects in apical slices are small, while our network matches the small myocardium better than other networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>Ablation experiments are performed on the ACDC+ dataset to validate the influence of different components in our method. Table <ref type="table" target="#tab_2">2</ref> lists evaluation results using the different combinations of components. Using ELBO as the objective function, the transformation based on multi-supports CSRBFs improves Dice 5%. Dice is improved 3% when the aggregated posterior is used as the prior. Whether the standard normal or the aggregated posterior as prior, the importance-weighted ELBO improves about 2-3% in Dice compared with ELBO. It is noted that when iwELBO is used, the aggregated posterior cannot improve registration compared with the standard Normal prior. The reason might be that the registration accuracy has a value close to its limit due to iwELBO; in this case, there is no space for the aggregated posterior as prior to improve registration accuracy further. Moreover, we find improvement in registration accuracy using iwELBO comes from the improvement of apical slices registration, details can be referred to in part 3 of the supplement. Further, we replaced the encoder of our network using ViT. Since only the Tencoder existed in ViT, we concatenated the moving and fixed images as input of ViT. In this experiment, different loops in ViT and our Transformer, denoted as ViT-n and Ours-n (n is the number of loops), are employed to compare the performance of self-attention and cross-attention. Table <ref type="table" target="#tab_4">3</ref> lists comparison results of ViT and our Transformer. It can be seen that cross-attention outperforms selfattention, and more loops are not beneficial in predicting the posterior parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a novel variational registration model using Transformer to pay attention to cross-attention between images. The importanceweighted ELBO and the aggregated posterior as prior close the gap between the real posterior and the variational posterior. Our transformation using multisupports CSRBFs generates flexible DVFs. Evaluation results on public cardiac datasets show that our method outperforms the state-of-art networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. The architecture of our network. The moving and fixed images M and F are preprocessed by T2T modules first and then input to our Transformer's T-encoder and T-decoder, respectively. Self-MSA and cross-MSA In our Transformer are marked by green and orange, respectively. (Color figure online)</figDesc><graphic coords="5,58,98,124,49,334,87,153,46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Demonstration of registration results using different networks. (Color figure online)</figDesc><graphic coords="8,43,80,60,86,336,19,126,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of registration results on ACDC+ and M&amp;Ms datasets using different networks. Data format: mean (standard deviations)</figDesc><table><row><cell>Dataset Method</cell><cell>BE</cell><cell>|J fz | ≤ 0</cell><cell>Dice</cell><cell>HD</cell><cell>APD</cell></row><row><cell>ACDC+ KrebsDiff [17]</cell><cell cols="5">27.71 (15.97) 2.01 (14.67) 0.840 (0.063) 5.66 (1.88) 2.21 (0.77)</cell></row><row><cell cols="6">DalcaDiff [11] 165.60 (40.34) 0.01 (0.04) 0.849 (0.064) 5.78 (1.93) 2.09 (0.81)</cell></row><row><cell cols="6">VoxelMorph [4] 149.26 (32.05) 26.17 (21.08) 0.845 (0.066) 5.84 (1.95) 2.15 (0.84)</cell></row><row><cell>NetGI [13]</cell><cell cols="5">12.68 (5.91) 4.96 (14.45) 0.854 (0.057) 5.46 (1.87) 2.00 (0.67)</cell></row><row><cell cols="6">TransMorph [9] 104.78 (27.16) 20.12 (27.55) 0.850 (0.064) 5.71 (1.89) 2.08 (0.82)</cell></row><row><cell>Ours</cell><cell cols="5">24.09 (6.04) 0.84 (4.28) 0.867 (0.049) 5.17 (1.57) 1.87(0.58)</cell></row><row><cell>M&amp;Ms KrebsDiff [17]</cell><cell cols="5">27.90 (15.97) 4.92 (10.09) 0.828 (0.054) 4.57 (2.24) 1.82 (0.93)</cell></row><row><cell>DalcaDiff [11]</cell><cell cols="5">90.15 (55.86) 0.01 (0.05) 0.853 (0.050) 4.21 (2.07) 1.53 (0.82)</cell></row><row><cell cols="6">VoxelMorph [4] 253.47 (59.40) 36.73 (25.40) 0.848 (0.059) 4.48 (2.34) 1.60 (0.92)</cell></row><row><cell>NetGI [13]</cell><cell cols="5">12.14 (4.47) 0.18 (0.93) 0.847 (0.052) 4.32 (2.38) 1.63 (0.94)</cell></row><row><cell cols="6">TransMorph [9] 169.36 (44.52) 33.08 (38.52) 0.860 (0.052) 4.13 (2.05) 1.48 (0.74)</cell></row><row><cell>Ours</cell><cell cols="3">21.23 (5.64) 0.77 (1.56) 0</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.869 (0.042) 3.84 (1.81) 1.41(0.65)</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the influence of different parts in our method on ACDC+ dataset. Data format: mean (stand deviation)</figDesc><table><row><cell>ELBO iwELBO Single</cell><cell>Multi-</cell><cell>N (0, I )</cell><cell>Aggregated</cell><cell>Dice</cell><cell>BE</cell><cell>|J fz | ≤ 0</cell></row><row><cell>support</cell><cell>supports</cell><cell>prior</cell><cell>prior</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.858 (0.053) 7</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>.48 (3.16) 0.36 (1.31)</head><label></label><figDesc></figDesc><table><row><cell>0.861 (0.051) 8.48 (3.36) 0.50 (2.09)</cell></row><row><cell>0.863 (0.053) 22.99 (5.88) 0.77 (3.52)</cell></row><row><cell>0.865 (0.051) 21.16 (6.01) 0.62 (2.33)</cell></row><row><cell>0.866 (0.053) 20.63 (5.31) 0.69 (1.20)</cell></row><row><cell>0.867 (0.049) 24.09 (6.04) 0.84 (4.28)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison of registration results on ACDC+ dataset using self-attention and cross-attention mechanisms, respectively. Data format: mean (standard deviations)</figDesc><table><row><cell cols="2">Method BE</cell><cell>|J fz | ≤ 0</cell><cell>Dice</cell><cell>HD</cell><cell>APD</cell></row><row><cell>ViT-3</cell><cell cols="5">26.28 (6.61) 0.68 (1.76) 0.865 (0.054) 5.23 (1.73) 1.86 (0.63)</cell></row><row><cell>ViT-6</cell><cell cols="5">19.58 (5.60) 0.70 (1.85) 0.862 (0.053) 5.30 (1.70) 1.91 (0.62)</cell></row><row><cell cols="6">Ours-3 24.09 (6.04) 0.84 (4.28) 0.867 (0.049) 5.17 (1.57) 1.87 (0.58)</cell></row><row><cell cols="6">Ours-6 18.81 (5.26) 0.53 (1.32) 0.862 (0.053) 5.33 (1.66) 1.92 (0.64)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This paper is supported by the <rs type="funder">Shenzhen Fundamental Research Program</rs> (<rs type="grantNumber">JCYJ20220531102407018</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MnvHaCx">
					<idno type="grant-number">JCYJ20220531102407018</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_55.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review of uncertainty quantification in deep learning: techniques, applications and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="243" to="297" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data-targeted prior distribution for variational autoencoder</title>
		<author>
			<persName><forename type="first">N</forename><surname>Akkari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Casenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ryckelynck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fluids</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient and generalizable statistical models of shape and appearance for analysis of cardiac MRI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andreopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="357" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An unsupervised learning model for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9252" to="9260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VoxelMorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<title level="m">Importance weighted autoencoders</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-centre, multi-vendor and multi-disease cardiac segmentation: the M&amp;Ms challenge</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">9458279</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">TransMorph: transformer for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Segars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10480</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Variational autoencoder with learned latent structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Canal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Rozell</surname></persName>
		</author>
		<idno>ArXiv abs/2006.10597</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning for fast probabilistic diffeomorphic registration</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_82</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00928-1_82" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial similarity network for evaluating image alignment in deep learning based registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_83</idno>
		<idno>978-3-030-00928-1_83</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic modeling for image registration using radial basis functions: Application to cardiac motion estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uncertainty learning towards unsupervised deformable medical image registration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khaidem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2484" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical importance weighted autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dhekane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CycleMorph: cycle consistent unsupervised deformable image registration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102036</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised probabilistic deformation modeling for robust diffeomorphic registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mailhé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-5_12" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bi-level probabilistic feature learning for deformable image registration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluation framework for algorithms segmenting short axis cardiac MRI</title>
		<author>
			<persName><forename type="first">P</forename><surname>Radau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Connelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The MIDAS J.-Cardiac MR Left Ventricle Segment. Challenge</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent registration neural networks for deformable image registration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Andermatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nyilas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic image registration via deep multi-class classification: characterizing uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLIP/UNSURE -2019</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11840</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32689-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32689-0_2" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sheikhjafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Noga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Punithakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<title level="m">Unsupervised deformable image registration with fully connected generative neural network</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variational autoencoder with implicit optimal priors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VAE with a VampPrior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the necessity and effectiveness of learning the prior of variational auto-encoder</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pei</surname></persName>
		</author>
		<idno>ArXiv abs/1905.13452</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tokens-to-token ViT: training vision transformers from scratch on ImageNet</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recursive cascaded networks for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10600" to="10610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
