<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model</title>
				<funder ref="#_BKtqQA7 #_DDb8cma #_YfseNh8 #_ehQ2kZa #_MCMdxWK">
					<orgName type="full">King Abdullah University of Science and Technology</orgName>
					<orgName type="abbreviated">KAUST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuetan</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computational Bioscience Research Center (CBRC)</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Longxi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computational Bioscience Research Center (CBRC)</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gongning</forename><surname>Luo</surname></persName>
							<email>gongning.luo@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Bioscience Research Center (CBRC)</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaowen</forename><surname>Qiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information and Computer Engineering</orgName>
								<orgName type="institution">Northeast Forestry University</orgName>
								<address>
									<postCode>150040</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
							<email>xin.gao@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">Computational Bioscience Research Center (CBRC)</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="260" to="270"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">26B690FAE979F26E1E0556251EB41281</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_25</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computed tomography</term>
					<term>Super resolution</term>
					<term>Diffusion model</term>
					<term>Image enhancement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>X-ray computed tomography (CT) is indispensable for modern medical diagnosis, but the degradation of spatial resolution and image quality can adversely affect analysis and diagnosis. Although super-resolution (SR) techniques can help restore lost spatial information and improve imaging resolution for low-resolution CT (LRCT), they are always criticized for topology distortions and secondary artifacts. To address this challenge, we propose a dual-stream diffusion model for super-resolution with topology preservation and structure fidelity. The diffusion model employs a dual-stream structure-preserving network and an imaging enhancement operator in the denoising process for image information and structural feature recovery. The imaging enhancement operator can achieve simultaneous enhancement of vascular and blob structures in CT scans, providing the structure priors in the superresolution process. The final super-resolved CT is optimized in both the convolutional imaging domain and the proposed vascular structure domain. Furthermore, for the first time, we constructed an ultra-high resolution CT scan dataset with a spatial resolution of 0.34 × 0.34 mm 2 and an image size of 1024×1024 as a super-resolution training set. Quantitative and qualitative evaluations show that our proposed model can achieve comparable information recovery and much better structure fidelity compared to the other state-of-the-art methods. The performance of high-level tasks, including vascular segmentation and lesion detection on super-resolved CT scans, is comparable to or even better than that of raw HRCT. The source code is publicly available at https://github.com/Arturia-Pendragon-Iris/UHRCT SR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computed tomography (CT) is a prevalent imaging modality with applications in biology, disease diagnosis, interventional imaging, and other areas. Highresolution CT (HRCT) is beneficial for clinical diagnosis and surgical planning because it can provide detailed spatial information and specific features, usually employed in advanced clinical routines <ref type="bibr" target="#b0">[1]</ref>. HRCT usually requires high-precision CT machines to scan for a long time with high radiation doses to capture the internal structures, which is expensive and can impose the risk of radiation exposure <ref type="bibr" target="#b1">[2]</ref>. These factors make HRCT relatively less available, especially in towns and villages, compared to low-resolution CT (LRCT). However, degradation in spatial resolution and imaging quality brought by LRCT can interfere with the original physiological and pathological information, adversely affecting the diagnosis <ref type="bibr" target="#b2">[3]</ref>. Consequently, how to produce high-resolution CT scans at a smaller radiation dose level with lower scanning costs is a holy grail of the medical imaging field (Fig. <ref type="figure" target="#fig_0">1</ref>). With the advancement of artificial intelligence, super-resolution (SR) techniques based on neural networks indicate new approaches to this problem. By inferring detailed high-frequency features from LRCT, super-resolution can introduce additional knowledge and restore lost information due to lowresolution scanning. Deep-learning (DL) based methods, compared to traditional methods, can incorporate hierarchical features and representations from prior knowledge, resulting in improved results in SR tasks <ref type="bibr" target="#b3">[4]</ref>. According to different neural-network frameworks, these SR methods can be broadly categorized into two classes: 1) convolutional neural network (CNN) based model <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, and 2) generative adversarial network (GAN) based model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Very recently, the diffusion model is emerging as the most promising deep generative model <ref type="bibr" target="#b10">[11]</ref>, which usually consists of two stages: a forward stage to add noises and a reverse stage to separate noises and recover the original images. The diffusion model shows impressive generative capabilities for many tasks, including image generation, inpainting, translation, and super-resolution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>While DL-based methods can generate promising results, there can still be geometric distortions and artifacts along with structural edges in the superresolved results <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. These structural features always represent essential physiological structures, including vasculature, fibrosis, tumor, and other lesions. The distortion and infidelity of these features can lead to potential misjudgment for diagnosis, which is unacceptable for clinical application. Moreover, the target image size and spatial resolution of HRCT for most existing SR methods is about 512 × 512 and 0.8 × 0.8 mm 2 . With the progress in hardware settings, ultra-high-resolution CT (UHRCT) with an image size of 1024 × 1024 and spatial resolution of 0.3 × 0.3 mm 2 can be available very recently <ref type="bibr" target="#b16">[17]</ref>. Though UHRCT can provide much more detailed information, to our best knowledge, SR tasks targeting UHRCT have rarely been discussed and reported.</p><p>In this paper, we propose a novel dual-stream conditional diffusion model for CT scan super-resolution to generate UHRCT results with high image quality and structure fidelity. The conditional diffusion model takes the form p(y|x), where x is the LRCT, and y is the targeted UHRCT <ref type="bibr" target="#b13">[14]</ref>. The novel diffusion model incorporates a dual-stream structure-preserving network and a novel imaging enhancement operator in the denoising process. The imaging enhancement operator can simultaneously extract the vascular and blob structures in the CT scans and provide structure prior to the dual-stream network. The dualstream network can fully exploit the prior information with two branches. One branch optimizes the SR results in the image domain, and the other branch optimizes the results in the structure domain. In practice, we use a convolution-based lightweight module to simulate the filtering operations, which enables faster and easier back-propagation in the training process. Furthermore, we constructed a new ultra-high resolution CT scan dataset obtained with the most advanced CT machines. The dataset contained 87 UHRCT scans with a spatial resolution of 0.34×0.34 mm 2 and an image size of 1024×1024. Extensive experiments, including qualitative and quantitative comparisons in both image consistency, structure fidelity, and high-level tasks, demonstrated the superiority of our method. Our contributions can be summarized as follows:</p><p>1) We proposed a novel dual-stream diffusion model framework for CT superresolution. The framework incorporates a dual-stream structure-preserving network in the denoising process to realize better physiological structure restoration. 2) We designed a new image enhancement operator to model the vascular and blob structures in medical images. To avoid non-derivative operations in image enhancement, we proposed a novel enhancement module consisting of lightweight convolutional layers to replace the filtering operation for faster and easier back-propagation in structural domain optimization.</p><p>3) We established an ultra-high-resolution CT scan dataset with a spatial resolution of 0.34 × 0.34 mm 2 and an image size of 1024 × 1024 for training and testing the SR task. 4) We have conducted extensive experiments and demonstrated the excellent performance of the proposed SR methods in both the image and structure domains. In addition, we have evaluated our proposed method on high-level tasks, including vascular-system segmentation and lesion detection on the SRCT, indicating the reliability of our SR results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dual-Stream Diffusion Model for Structure-Preserving Super-Resolution</head><p>To preserve the structure and topology relationship in the denoising progress, we designed a novel Dual-Stream Diffusion Model (DSDM) for better superresolution and topology restoration (Fig. <ref type="figure" target="#fig_1">2</ref>). In the DSDM framework, given a HRCT slice y, we generate a noisy version ỹ, and train the network G DSSP to denoise ỹ with the corresponding LRCT slice x and a noise level indicator γ.</p><p>The optimization is defined as</p><formula xml:id="formula_0">E (x,y) E N (0,I) E γ G DSSP x, √ γy + 1 -γ , γ - p<label>(1)</label></formula><p>In the denoising process, we used a dual-stream structure-preserving (DSSP) network for supervised structure restoration. The DSSP network optimizes the denoised results in the image domain and the structure domain, respectively. The structural domain, obtained with the image enhancement operator, is concatenated with the LRCT slice as the input of the structure branch. The final SR results are obtained after the feature fusion model between the image map and the structure map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Imaging Enhancement Operator for Vascular and Blob Structure</head><p>We introduced an enhancement operator in the DSSP network to model the vascular and blob structures, which can represent important physiological information according to clinical experience, and provide the prior structural information for the DSSP network. For one pixel</p><formula xml:id="formula_1">x = [x 1 , x 2 ]</formula><p>T in the CT slice, let I (x) denote the imaging intensity at this point. The 2 × 2 Hessian matrix at the scale s is defined as <ref type="bibr" target="#b17">[18]</ref> </p><formula xml:id="formula_2">H i,j (x) = s 2 I (x) ∂ 2 G (x, s) ∂x i ∂x j<label>(2)</label></formula><p>where G (x, s) is the 2D Gaussian kernel. The two eigenvalues of the Hessian matrix are denoted as λ = (λ 1 , λ 2 ) and here we agree that |λ 1 | &lt;= |λ 2 |. The eigenvalues of the Hessian matrix can reflect the geometric shape, curvature, and brightness of the local images. For the blob-like structures, the three eigenvalues are about the same, λ 1 ≈ λ 2 ; for the vascular-like structures, λ 2 can be much larger than the absolute value of λ 1 , |λ 2 | &gt;&gt; |λ 1 | <ref type="bibr" target="#b18">[19]</ref>. The eigenvalue relations at scale s can be indicated by several different functions. Here we proposed a novel structure kernel function, which is defined as</p><formula xml:id="formula_3">V C = λ 2 (κ 1 λ 1 + λ τ ) (κ 2 λ 2 + λ τ ) (λ 1 + λ 2 + λ τ ) 3<label>(3)</label></formula><p>κ 1 and κ 2 are the parameters to control the sensitivity for the vascular-like structures and blob-like structures, respectively. λ τ is the self-regulating factor.</p><formula xml:id="formula_4">λ τ = (λ 2 -λ 1 ) e γ -e -γ e γ + e -γ + λ 1 (4) γ = λ 2 λ 1 -1<label>(5)</label></formula><p>When λ 1 is about the same with λ 2 , λ τ is closed to λ 1 ; and when λ 1 is much smaller to λ 2 , λ τ is closed to λ 2 , which can achieve a balance between two conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Objective Function</head><p>We designed a new loss function to ensure that the final SR result can be optimized in both the image domain and the structure domain. Denoting the reference image as y t , the pixel-wise loss in the imaging domain is formulated as</p><formula xml:id="formula_5">L pixel SR = G image DSSP y t -y t 2 + λ 1 G image DSSP y t -y t 1<label>(6)</label></formula><p>G image DSSP (y t ) is the recovered image from the image-domain branch. L1 loss yields a significantly higher consistency and lower diversity, while L2 loss can better capture the outliers. Here we used a parameter λ L1 to balance these two losses.</p><p>In the meantime, a structure-constraint loss is also necessary to help the network achieve better performance in structure consistency. The loss function consists of two parts, which measure the consistency of the image-domain branch and the structure-domain branch. Denoting the structure-domain output as G struct DSSP (y t ), the structure-constraint loss can be presented as</p><formula xml:id="formula_6">L struct SR = F c • G image DSSP y t -F c • y t 1 + G struct DSSP y t -F c • y t 1 (7)</formula><p>However, the image enhancement described above involves overly complex calculations, making back-propagation difficult in the training process. Here we utilized a convolution-based operator O Fc (•) to simplify the calculation, which consists of several lightweight convolutional layers to simulate the operation of image enhancement. In this way, we transform the complex filtering operation into a simple convolution operation, thus back-propagation can be easily processed. The loss function is then modified as</p><formula xml:id="formula_7">L struct SR = O Fc • G image DSSP y t -O Fc • y t 1 + G struct DSSP y t -O Fc • y t 1 (8)</formula><p>The total objective function is the sum of two losses.</p><formula xml:id="formula_8">L SR = L pixel SR + λ 2 L struct SR<label>(9)</label></formula><p>3 Experiments and Conclusion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation</head><p>We constructed three datasets for framework training and evaluation; two of them were in-house data collected from two CT scanners(the ethics number is 20220359), and the other was the public Luna16 dataset <ref type="bibr" target="#b21">[22]</ref>. More details about the two in-house datasets are described in the Supplementary Materials.</p><p>We evaluated our SR model on three CT datasets:</p><p>• Dataset 1: 2D super-resolution from 256×256 to 1024×1024, with the spatial resolution from 1.36 × 1.36 mm 2 to 0.34 × 0.34 mm 2 .</p><p>• Dataset 2: 3D super-resolution from 256 × 256 × 1X to 512 × 512 × 5X, with the spatial resolution from 1.60 × 1.60 × 5.00 mm 3 to 0.80 × 0.80 × 1.00 mm 3 . • Dataset 3: 2D super-resolution from 256 × 256 to 512 × 512 on the Luna16 dataset.</p><p>We compare our model with other SOTA super-resolution methods, including bicubic interpolation, SRCNN <ref type="bibr" target="#b6">[7]</ref>, SRResNet <ref type="bibr" target="#b5">[6]</ref>, Cycle-GAN <ref type="bibr" target="#b1">[2]</ref>, and SR3 <ref type="bibr" target="#b11">[12]</ref>. Performance is assessed qualitatively and quantitatively, using PSNR, SSIM <ref type="bibr" target="#b22">[23]</ref>, Visual Information Fidelity (VIF) <ref type="bibr" target="#b23">[24]</ref>, and Structure Mean Square Error (SMSE). VIF value is correlated well with the human perception of SR images, which can measure diagnostic acceptance and information maintenance. SMSE is proposed to evaluate the structure difference between the ground truth and SRCT. Specially, we obtained the structural features of the ground truth and SRCT with Frangi filtering and then calculated the pixel-wise difference <ref type="bibr" target="#b19">[20]</ref>.  </p><formula xml:id="formula_9">SM SE = F Frangi (HRCT ) -F Frangi (SRCT )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Qualitative and Quantitative Results</head><p>Qualitative comparisons are shown in Fig. <ref type="figure" target="#fig_3">3</ref> and the quantitative results are shown in Table <ref type="table" target="#tab_0">1</ref>. The super-resolution results with our proposed methods achieve the highest scores in both image restoration and structure consistency for most indices, and there are no obvious secondary artifacts introduced in the SR results. Although the GAN-based methods and SR3 can produce sharp details, they tend to generate artifacts for the vascular systems, which is more evident in the structure-enhanced figures. The problem of inconsistent structure is also reflected in the value of VIF and SMSE on both GAN-based methods and SR3.</p><p>Lesion Detection and Vessel Segmentation on Super-Resolved CT. To further evaluate the information maintenance of our SR methods, we conducted some high-level tasks, including lung nodules detection and pulmonary airway and blood vessel segmentation on the super-resolved CT scans. We compared the performance of different methods on SRCT and the ground truth. For nodule detection, these methods included U-Net, V-Net <ref type="bibr" target="#b24">[25]</ref>, ResNet <ref type="bibr" target="#b25">[26]</ref>, DCNN <ref type="bibr" target="#b26">[27]</ref> and 3D-DCNN <ref type="bibr" target="#b27">[28]</ref>. For the vessel segmentation, these methods included 3D U-Net, V-Net <ref type="bibr" target="#b24">[25]</ref>, nnUNet <ref type="bibr" target="#b30">[31]</ref>, Nardelli et al. <ref type="bibr" target="#b29">[30]</ref> and Qin et al. <ref type="bibr" target="#b28">[29]</ref>. Figure <ref type="figure" target="#fig_4">4</ref> shows the quantitative results of the performance comparison. The performance of these high-level tasks on the SR results is comparable to or even better than that on the ground-truth CT. Such results, to some extent, demonstrated that our SR method does not introduce artifacts or structural inconsistencies and cause misjudgment, while the improved spatial resolution and image quality generated by our proposed results shows great potential in improving the performance of high-level tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we have established a dual-stream diffusion model framework to address the problem of topology distortion and artifact introduction that generally exists in the medical super-resolution results. We first propose a novel image enhancement operator to model the vessel and blob structures in the CT slice, which can provide a structure prior to the SR framework. Then, we design a dualstream diffusion model that employs a dual-stream ream structure-preserving network in the denoising process. The final SR outputs are optimized not only by convolutional image-space losses but also by the proposed structure-space losses. Extensive experiments have shown that our SR methods can achieve high performance in both image restoration and structure fidelity, demonstrating the promising performance of information preservation and the potential of applying our SR results to downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Low-resolution CT scans can adversely affect the diagnosis. Restoring the original structural information while improving the spatial resolution is a great challenge.</figDesc><graphic coords="2,58,98,267,17,334,48,128,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.Overview of the proposed Dual-stream Diffusion Model. The forward diffusion process q gradually adds Gaussian noises to the target HRCT. The reverse process p iteratively denoises the target image, conditioned on the source LRCT x, which is realized with the dual-stream structure-preserving network.</figDesc><graphic coords="4,58,98,182,30,334,48,174,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparison of super-resolved CT from the ultra-high-resolution dataset. The display window is [-600, 400] HU. The restored images are shown in the first and third lines, and the structural features are shown in the second and fourth lines.</figDesc><graphic coords="7,73,80,274,43,276,76,190,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of the detection and segmentation performances on HRCT and SRCT.</figDesc><graphic coords="8,73,98,444,35,304,96,124,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation of state-of-the-art super-resolution algorithms.</figDesc><table><row><cell></cell><cell>Dataset 1</cell><cell></cell><cell></cell><cell>Dataset 2</cell><cell>Dataset 3</cell></row><row><cell></cell><cell cols="2">PSNR SSIM VIF</cell><cell cols="2">SMSE PSNR SSIM VIF</cell><cell>SMSE PSNR SSIM VIF</cell><cell>SMSE</cell></row><row><cell cols="4">Interpolation 28.27 0.926 0.583 1.46</cell><cell>22.73 0.817 0.516 1.76</cell><cell>23.84 0.776 0.679 1.51</cell></row><row><cell>SRCNN [7]</cell><cell cols="3">31.71 0.957 0.743 1.43</cell><cell>28.39 0.875 0.559 1.27</cell><cell>31.62 0.842 0.738 1.43</cell></row><row><cell cols="2">SRResNet [6] 32.69 0.96</cell><cell cols="3">0.762 0.992 29.6</cell><cell>0.854 0.685 1.03</cell><cell>32.84 0.897 0.796 0.892</cell></row></table><note><p><p><p><p><p>Cycle-GAN</p><ref type="bibr" target="#b1">[2]</ref> </p>37.32 0.993 0.901 0.462 32.31 0.918 0.881 0.822 37.82 0.921 0.915 0.282 SR3</p><ref type="bibr" target="#b11">[12]</ref> </p>37.18 0.974 0.812 0.474 36.85 0.957 0.916 0.859 39.57 0.968 0.902 0.274 Our Proposed 40.75 0.992 0.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>977 0.162 38.76 0.979 0.967 0.274 38.91 0.977 0.974 0.162</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This publication is based upon work supported by the <rs type="funder">King Abdullah University of Science and Technology (KAUST) Office of Research Administration (ORA)</rs> under Award No <rs type="grantNumber">URF/1/4352-01-01</rs>, <rs type="grantNumber">FCC/1/1976-44-01</rs>, <rs type="grantNumber">FCC/1/1976-45-01</rs>, <rs type="grantNumber">REI/1/5234-01-01</rs>, <rs type="grantNumber">REI/1/5414-01-01</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BKtqQA7">
					<idno type="grant-number">URF/1/4352-01-01</idno>
				</org>
				<org type="funding" xml:id="_DDb8cma">
					<idno type="grant-number">FCC/1/1976-44-01</idno>
				</org>
				<org type="funding" xml:id="_YfseNh8">
					<idno type="grant-number">FCC/1/1976-45-01</idno>
				</org>
				<org type="funding" xml:id="_ehQ2kZa">
					<idno type="grant-number">REI/1/5234-01-01</idno>
				</org>
				<org type="funding" xml:id="_MCMdxWK">
					<idno type="grant-number">REI/1/5414-01-01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 25.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning reconstruction improves image quality of abdominal ultra-high-resolution CT</title>
		<author>
			<persName><forename type="first">M</forename><surname>Akagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Higaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="6163" to="6171" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CT super-resolution GAN constrained by the identical, residual, and cycle learning ensemble (GAN-CIRCLE)</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="203" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for noise reduction in low-dose CT</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2536" to="2545" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning for image super-resolution: a survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3365" to="3387" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CT-SRCNN: cascade trained and trimmed deep convolutional neural networks for image super resolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1423" to="1431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wider or deeper: revisiting the resnet model for visual recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks with intermediate loss for 3D super-resolution of CT and MRI scans</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Verga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="49112" to="49124" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">tempoGAN: a temporally coherent, volumetric GAN for super-resolution fluid flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Super-resolution MRI and CT through GANcircle</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Developments in X-ray tomography XII</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11113</biblScope>
			<biblScope unit="page" from="202" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dual diffusion implicit bridges for image-to-image translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">At-ddpm: restoring faces degraded by atmospheric turbulence using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3434" to="3443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Palette: Image-to-image diffusion models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH on Conference Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.12104</idno>
		<title level="m">Conversion Between CT and MRI Images Using Diffusion and Score-Matching Models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structure-preserving image super-resolution via contextualized multitask learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2804" to="2815" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure-preserving image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7898" to="7911" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Physical evaluation of an ultrahigh-resolution CT scanner</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Oostveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boedeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2552" to="2560" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Blob enhancement and visualization for improved intracranial aneurysm detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pernuš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Likar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visual Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1705" to="1717" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancement of vascular structures in 3D and 2D angiographic images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pernuš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Likar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2107" to="2118" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiscale vessel enhancement filtering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<idno type="DOI">10.1007/BFb0056195</idno>
		<ptr target="https://doi.org/10.1007/BFb0056195" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 1998</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Colchester</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Delp</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1496</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blind deblurring of spiral CT images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Vannier</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2003.815075</idno>
		<ptr target="https://doi.org/10.1109/TMI.2003.815075" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="837" to="845" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="915" to="931" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image Quality Metrics: PSNR vs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Horé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2010.579</idno>
		<ptr target="https://doi.org/10.1109/ICPR.2010.579" />
	</analytic>
	<monogr>
		<title level="m">SSIM. In: 2010 20th International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A study on the relationship between depth map quality and stereoscopic image quality using upsampled depth maps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mahmoudpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
	<note>Emerging Trends in Image Processing</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VNet: an end-to-end fully convolutional neural network for road extraction from high-resolution remote sensing data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alamri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="179424" to="179436" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46493-038" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A deep 3D residual CNN for false-positive reduction in pulmonary nodule detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2097" to="2107" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance analysis of state-of-the-art CNN architectures for luna16</title>
		<author>
			<persName><forename type="first">I</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">4426</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning tubule-sensitive cnns for pulmonary airway and artery-vein segmentation in ct</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1603" to="1617" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pulmonary arteryvein classification in CT images using deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez-Carretero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bermejo-Pelaez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2428" to="2440" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A A</forename><surname>Kohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Novel example-based method for superresolution and denoising of medical images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dibos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1882" to="1895" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
