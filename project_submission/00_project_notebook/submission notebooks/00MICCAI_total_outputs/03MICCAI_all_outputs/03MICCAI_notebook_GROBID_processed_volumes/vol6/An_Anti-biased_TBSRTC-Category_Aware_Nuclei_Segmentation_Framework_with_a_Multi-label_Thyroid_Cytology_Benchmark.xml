<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark</title>
				<funder ref="#_kagXyap">
					<orgName type="full">Natural Science Foundation of Shanghai</orgName>
				</funder>
				<funder ref="#_XqcvvQ2">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junchao</forename><surname>Zhu</surname></persName>
							<email>junchaozhu@sjtu.edu.cn</email>
							<idno type="ORCID">0000-0001-6610-9808</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Life Sciences and Biotechnology</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiqing</forename><surname>Shen</surname></persName>
							<idno type="ORCID">0000-0001-7866-3339</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haolin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Shanghai Ocean University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Ke</surname></persName>
							<email>kejing@sjtu.edu.cn</email>
							<idno type="ORCID">0000-0001-7459-257X</idno>
							<affiliation key="aff3">
								<orgName type="department">School of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="580" to="590"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">BB98D4150A9AD0423BEDF4EEFAF2053B</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unbalanced Nuclei Segmentation</term>
					<term>Semi-Supervised Learning</term>
					<term>Thyroid Cytology</term>
					<term>TBSRTC Diagnostic Category</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Bethesda System for Reporting Thyroid Cytopathology (TBSRTC) has been widely accepted as a reliable criterion for thyroid cytology diagnosis, where extensive diagnostic information can be deduced from the allocation and boundary of cell nuclei. However, two major challenges hinder accurate nuclei segmentation from thyroid cytology. Firstly, unbalanced distribution of nuclei morphology across different TBSRTC categories can lead to a biased model. Secondly, the insufficiency of densely annotated images results in a less generalized model. In contrast, image-wise TBSRTC labels, while containing lightweight information, can be deeply explored for segmentation guidance. To this end, we propose a TBSRTC-category aware nuclei segmentation framework (TCSegNet). To top up the small amount of pixel-wise annotations and eliminate the category preference, a larger amount of image-wise labels are taken in as the complementary supervision signal in TCSegNet. This integration of data can effectively guide the pixel-wise nuclei segmentation task with a latent global context. We also propose a semi-supervised extension of TCSegNet that leverages images with only TBSRTC-category labels. To evaluate the proposed framework and also for further cytology cell studies, we curated and elaborately annotated a multi-label thyroid cytology benchmark, collected clinically from 2019 to 2022, which will be made public upon acceptance. Our TCSegNet outperforms state-of-the-art segmentation approaches with an improvement of 2.0% Dice and 2.7% IoU; besides, the semi-supervised extension can further boost this margin. In conclusion, our study explores the weak annotations by constructing an image-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Thyroid cancer is the most common cancer of the endocrine system, accounting for 2.1% of all malignant cancers <ref type="bibr" target="#b0">[1]</ref>. Clinically, pathologists rely on the six-category of "The Bethesda System for Reporting Thyroid Cytopathology" (TBSRTC) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> to distinguish the cell morphology in the stained cytopathologic sections. The emergence of computational pathology allows automatic diagnosis of thyroid cancer, and nuclei segmentation becomes one of the most critical diagnostic tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, as the shapes of nuclei, whether round, oval, or elongated, can provide valuable information for further analysis <ref type="bibr" target="#b5">[6]</ref>. For example, small and scattered thyroid cells with a light hue and relatively low cell density are usually low-grade and indicative of early-stage cancer; whereas large and dark cells with extreme-dense agglomeration are usually middle-or late-grade <ref type="bibr" target="#b2">[3]</ref>. Correspondingly, accurate location of cell boundaries is essential for both pathologists and computer-aided diagnosis (CAD) systems to assist decision <ref type="bibr" target="#b6">[7]</ref>.</p><p>However, nuclei segmentation in thyroid cytopathology is still challenged by the varying cellularity of images from different TBSRTC categories <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. For example, benign cells (I &amp; II) present high sparsity and are difficult to be distinguished from background tissues, thus may account for a relatively small proportion when equal images are involved in a training set <ref type="bibr" target="#b2">[3]</ref>. By contrast, high-grade cells (V &amp; VI) are densely packed and severely clustered, thus much more are presented in a training set. In this way, an unbalanced distribution across different categories resulted, correspondingly, the training leads to biased models with lower accuracy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Such distinct morphological differences can be characterized by the TBSRTC category, which thus inspires us to utilize the handy image-wise grading labels to guide the nuclei segmentation model learning from unbalanced datasets. We also noticed that another challenge for accurate nuclei identification is the heavy reliance on large-scale high-quality annotations <ref type="bibr" target="#b10">[11]</ref>. Moreover, amongst multiple annotation paradigms <ref type="bibr" target="#b11">[12]</ref>, pixellevel labeling is the most time-consuming and laborious, whereas the image-wise diagnostic labels, i.e. TBSRTC categories, are comparatively simpler. Despite the labeling intensity, prevalent nuclei segmentation methods, e.g., CIA-Net <ref type="bibr" target="#b12">[13]</ref>, CA 2.5 -Net <ref type="bibr" target="#b13">[14]</ref>, and ClusterSeg <ref type="bibr" target="#b14">[15]</ref>, are limited to pixel-wise annotations, where the potential benefits of integrating accessible image-wise labels are unaware.</p><p>To narrow the gap discussed, we propose a novel TBSRTC-category-aware nuclei segmentation framework. Our contributions are three-fold. <ref type="bibr" target="#b0">(1)</ref> We propose a cytopathology nuclei segmentation network named TCSegNet, to provide supplementary guidance to facilitate the learning of nuclei boundaries. Innovatively, our approach can help reduce bias in the learning process of the segmentation model with the routine unbalanced training set. <ref type="bibr" target="#b1">(2)</ref> We expand TCSegNet to Semi-TCSegNet to leverage image-wise labels in a semi-supervised learning manner, which significantly reduces the reliance on annotation-intensive pixel-wise labels. Additionally, an HSV-intensity noise is designed specifically for cytopathology images to boost the generalization ability. (3) We establish a dataset of thyroid cytopathology image patches of 224 × 224, where 4,965 image labels are provided following TBSRTC, and 1,473 of them are densely annotated <ref type="bibr" target="#b2">[3]</ref> (to be on GitHub upon acceptance). To the best of our knowledge, it is the first publicized thyroid cytopathology dataset of both image-wise and pixel-wise labels. The annotated dataset well alleviates the insufficiency of an open cytopathology dataset for computer-assisted analysis (Fig. <ref type="figure" target="#fig_0">1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Overview. We propose a novel TBSRTC-category Aware Segmentation Network (TCSegNet) to segment nuclei boundaries in cytopathology images, which is guided by TBSRTC-category label to learn from unbalanced data. Our model uses a CNN and Transformer dual-path U-shape architecture, where the CNN captures the local features, and the Transformer extracts the global features for a more comprehensive representation of nuclei allocation <ref type="bibr" target="#b15">[16]</ref>. Considering the spatial distributions of thyroid cells in cytopathology images, our design provides extended global information for more accurate segmentation. Our approach employs short connections to allow effective communication of local and global representations <ref type="bibr" target="#b16">[17]</ref>. Formally, the overall segmentation loss L seg to train our model is a combination of the binary cross-entropy loss (BCE), i.e.</p><formula xml:id="formula_0">L seg = γ ni • BCE(ŷ cnn ni , y ni ) + γ ni • BCE(ŷ trans ni , y ni ) + γ nb • BCE(ŷ cnn nb , y nb ), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where ŷ is the prediction from TCSegNet and y is and pixel-wise annotation, respectively. Subscript ni and nb denote the nuclei area and boundary, respectively. Superscripts cnn and trans write for the CNN branch and Transformer branch, respectively. We set the balancing coefficient γ ni to 1 and γ nb to 10. Additionally, to ensure the consistency between the two branches, we impose a dice consistency loss (L cons ) between the nuclei instance predictions from the CNN branch and the Transformer branch, namely</p><formula xml:id="formula_2">L cons = Dice(ŷ cnn ni , ŷtrans ni ).</formula><p>TBSRTC-Category Label Guidance Block. In TCSegNet, we introduce a TBSRTC-category label guidance block to address the learning issue from unbalanced routine datasets. This block consists of two learnable fully connected layers that process the feature extracted by the CNN and Transformer branches separately, which obtains image-wise TBSRTC-category prediction denoted as ŷcnn cls and ŷtrans cls . Correspondingly, to train this block, we use a cross-entropy loss function (CE) that provides an extra supervision signal to help the network learn from unbalanced datasets, defined as follows:</p><formula xml:id="formula_3">L cls = CE(ŷ cnn cls , y cls ) + γ cls • CE(ŷ trans cls , y cls ),<label>(2)</label></formula><p>where y cls is the image-wise TBSRTC-category label, and the balancing coefficient γ cls is set to 3, as the global feature captured by the Transformer branch is tightly correlated with the image-level classification tag. Finally, the overall loss for TCSegNet becomes</p><formula xml:id="formula_4">L s = L seg + L cls + L cons . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>Extension to Semi-supervised Learning. To leverage images that only have image-wise labels, we extend to a semi-supervised mean teacher <ref type="bibr" target="#b17">[18]</ref> framework called Semi-TCSegNet. In this framework, both the student and teacher share the same full-supervised nuclei segmentation architecture of TCSegNet. The weights of the teacher θ t are updated with the exponential moving average (EMA) of the weights of student θ s , and smoothing coefficient α = 0.99, following the previous work <ref type="bibr" target="#b18">[19]</ref>. Formally, the weights of the teacher at e-th epoch are updated by </p><formula xml:id="formula_6">θ e t = αθ e-1 t + (1 -α)θ e s . (<label>4</label></formula><p>where e max is the maximum epoch number.</p><p>HSV-Intensity Noise. The traditional method of integrating Gaussian noise in the mean teacher <ref type="bibr" target="#b17">[18]</ref> may be problematic when working with cytopathology images that have an imbalanced color distribution. To address this issue, we generate a novel intensity-based noise, which can adaptively behave stronger in the dark nuclei areas and weaker in bright cytoplasm or background regions. We first sample η from a Gaussian distribution N 0, σ 2 , where σ is the standard deviation computed from the pixel values of the V channel in HSV space. The Gaussian noise η serves as the basis for generating the intensity-based noise, which is obtained by</p><formula xml:id="formula_8">η v = λ v • η • (1 -X v ).</formula><p>Specifically, X v is the pixel value of the image's V channel in HSV space, and hyper-parameter λ v is set as 0.5 to control the amplitude of the intensity-based noise. Finally, the value of the obtained noise is clamped to [-0.2, 0.2] before being added to the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Image Dataset. We construct a clinical thyroid cytopathology dataset with images of both image-wise and pixel-wise labels as a benchmark (appear in GitHub upon acceptance) Some representative images are presented in Fig. <ref type="figure" target="#fig_2">2</ref>, together with the profile of the dataset. The dataset comprises 4,965 H&amp;E stained image patches and labels of TBSRTC, where a subset of 1,473 images was densely annotated for nuclei boundaries by three experienced cytopathologists and reached a total number of 31,064 elaborately annotated nuclei. Patient-level images were partitioned first for training and test images, and patch-level curation was performed. We divided the dataset with image-wise labels into 80% training samples and the remaining 20% testing samples. Our collection of thyroid cytopathology images was granted with an Ethics Approval document. Table <ref type="table">1</ref>. Quantitative comparisons in both fully-supervised and semi-supervised manners. The best performance is highlighted in bold, where we can observe that both TCSegNet and its semi-supervised extension outperform state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Dice IoU</head><p>Fully-supervised Mask R-CNN <ref type="bibr" target="#b19">[20]</ref> 0.657 0.500 Swin-Unet <ref type="bibr" target="#b20">[21]</ref> 0.671 0.516 SegNet <ref type="bibr" target="#b21">[22]</ref> 0.676 0.587 UNet++ <ref type="bibr" target="#b22">[23]</ref> 0.784 0.691 Ca 2.5 -Net <ref type="bibr" target="#b13">[14]</ref> 0.838 0.732 CIA-Net <ref type="bibr" target="#b12">[13]</ref> 0.854 0.775 ClusterSeg <ref type="bibr" target="#b14">[15]</ref> 0.857 0.761 TCSegNet (Ours) 0.877 0.788</p><p>Semi-supervised PseudoSeg <ref type="bibr" target="#b23">[24]</ref> 0.734 0.612 Cross Pseudo Seg <ref type="bibr" target="#b24">[25]</ref> 0.737 0.618 Cross Teaching <ref type="bibr" target="#b25">[26]</ref> 0.795 0.704 PS-ClusterSeg <ref type="bibr" target="#b14">[15]</ref> 0.866 0.775 MTMT-Net <ref type="bibr" target="#b26">[27]</ref> 0.878 0.789 Semi-TCSegNet (Ours) 0.889 0.805 Implementations. The proposed method and compared methods are implemented on a single NVIDIA GeForce RTX 3090 GPU card. We employ a conformer <ref type="bibr" target="#b15">[16]</ref> with 12 Transformer layers and 5 CNN blocks as the encoder in TCSegNet. Both TCSegNet and Semi-TCSegNet use SGD optimizer with a momentum of 0.9 and a weight decay of 10 -4 . The initial learning rate lr 0 is set to 5 × 10 -3 , and the learning rate for e th epoch is determined by the poly strategy <ref type="bibr" target="#b27">[28]</ref>, i.e., lr e = lr 0 × (1 -e/e max ) 0.9 , where e max = 280 is the total epoch number. We set the batch size for TCSegNet to 8, and for Semi-TCSegNet to 10, i.e. 8 fully-annotated images and 2 partially-annotated images per batch.</p><p>Compared Methods and Evaluation Metrics. We compared TCSegNet with the fully-supervised counterparts, including method specific for segmentation in general image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, medical image <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref>, and nuclei <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. We also compared Semi-TCSegNet with semi-supervised methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. We used the officially released code published along with their papers for all the compared methods. Intersection over Union (IoU) and Dice score were applied as the evaluation metrics, where a higher value indicated a better semantic segmentation performance. Experimental Results. The results in Table <ref type="table">1</ref> indicated that TCSegNet can achieve the highest performance by a Dice score of 87.7% and an IoU of 78.8%. The performance values in the challenging regions are highlighted with red boxes in Fig. <ref type="figure" target="#fig_3">3</ref>, together with the line charts in Fig. <ref type="figure" target="#fig_4">4 (A,</ref><ref type="figure">B</ref>). Our approach is capable to address the current issue in the recognition and segmentation of small isolated cells graded in the I category, which is always ignored by the unbalanced pixel-wise cell morphology with other approaches. Also, it yields that the incorporation of TBSRTC-category can contribute to a partial alleviation of a biased model, resulting in more satisfying segmentation performance experimentally. Furthermore, the fact that the TBSRTC-category label is easy to obtain endows the applicability of our model to various circumstances that nuclei in various sizes, shapes, and dyeing styles can be accurately recognized and segmented. Consequently, it can serve as a guarantee for the validity and accuracy of the subsequent analysis in real clinical practice. Moreover, with the semi-supervised learning, Semi-TCSegNet can further boost the performance to an 88.9% Dice score, and 80.5% IoU, by leveraging additional data with image-wise TBSRTCcategory labels solely. The performance improvement of 1.2% Dice, 1.7% IoU, together with the general improvement is shown in the boxplot in Fig. <ref type="figure" target="#fig_4">4 (C,</ref><ref type="figure">D</ref>), as a demonstration of the advantage using full data resources with Semi-TCSegNet. Ablation Study. To evaluate the effectiveness of each functional block and demonstrate the functionality of semi-supervised learning, we illustrate the ablation study in Table <ref type="table" target="#tab_0">2</ref>. The results indicate that performance improvement is accumulated with increasing data size. Besides, training with a classificationlearning block alone can increase the nuclei segmentation performance by 1.7% and 2.6% in the Dice score and IoU, respectively. Meanwhile, trained with specially designed HSV-Intensity noise can also increase the performance by 0.9% Dice and 1.4% IoU, showing its potential for generation ability improvement. Importantly, the benefits from the two blocks are orthonormal, where Semi-TCSegNet achieves the optimal performance with the utilization of both. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a TBSRTC-category aware nuclei segmentation framework TCSegNet, that leverages easy-to-obtain image-wise diagnostic category to facilitate nuclei segmentation. Importantly, it addresses the challenge of distinguishing nuclei across different cell scales in an unbalanced dataset. We also extend the framework to a semi-supervised learning fashion to overcome the issue of lacking annotated training samples. Moreover, we construct the first thyroid cytopathology dataset with both image-wise and pixel-wise labels, which we believe can it facilitate future research in this field. As the spatial distribution, shape, and area information from nuclear segmentation is supportive of diagnostic decisions, we will further leverage the segmentation result for malignancy analysis and also explore the potential of spatial information for unlabeled data exploration in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the proposed TCSegNet and Semi-TCSegNet. TCSegNet utilizes annotation-lightweight image-wise TBSRTC-category labels to aid in the learning of unbalanced nuclei morphology in segmentation. Semi-TCSegNet reduces the heavy reliance on annotation-intensive pixel-wise labels through the use of a semi-supervised framework.</figDesc><graphic coords="3,53,31,220,49,317,05,222,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>)</head><label></label><figDesc>During the training stage, the teacher model assigns pixel-wise soft labels to the images with exclusive image-wise labels, thus expanding the scale of labeled data to the student model. The overall loss function for training the student is a combination of the supervised loss Ls in Eq. (3) computed on fully-annotated data, and the semi-supervised loss Lss computed on data with only image-wise labels. It follows that the overall training objective in Semi-TCSegNet is to minimize the loss function L = L s + λ ss • L ss , where L ss measures the segmentation consistency between the teacher and student models via L 2 -norm. The balancing coefficient λ ss changes for every epoch e, namely λ e ss = exp(-5 (1 -e/e max )2 ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The annotated thyroid cytopathology benchmark. A) Examples from I to VI of TBSRTC six diagnostic categories with pixel-wise nuclei mask and boundary annotations; B) The profile of the dataset.</figDesc><graphic coords="6,55,98,54,38,340,18,104,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of segmented nuclei in thyroid cytopathology images by TCSegNet and prevent fully-supervised models methods. Observably, TCSegNet is more aware of scattered, small, or clustered nuclei.</figDesc><graphic coords="7,41,79,169,91,340,21,231,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Quantitative comparison presented by boxplot and line chart. A and B are the variation tendency of Dice score and IoU of six diagnostic categories in fully-supervised methods. C and D are the distribution of the Dice score and IoU of all mentioned models respectively. Our models presented a general improvement across the metrics.</figDesc><graphic coords="8,57,96,184,25,336,85,177,94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Ablation study for our Semi-TCSegNet and functional blocks.</figDesc><table><row><cell>Classification</cell><cell>HSV-Intensity</cell><cell>Dice IoU</cell></row><row><cell>Learning</cell><cell>Noise</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.867 0.771</cell></row><row><cell></cell><cell></cell><cell>0.876 0.785</cell></row><row><cell></cell><cell></cell><cell>0.884 0.797</cell></row><row><cell></cell><cell></cell><cell>0.889 0.805</cell></row><row><cell>W. image-wise data</cell><cell>+1k data</cell><cell>0.879 0.790</cell></row><row><cell></cell><cell>+2k data</cell><cell>0.882 0.795</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">62102247</rs>) and <rs type="funder">Natural Science Foundation of Shanghai</rs> (No. <rs type="grantNumber">23ZR1430700</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XqcvvQ2">
					<idno type="grant-number">62102247</idno>
				</org>
				<org type="funding" xml:id="_kagXyap">
					<idno type="grant-number">23ZR1430700</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cancer incidence and mortality worldwide: sources, methods and major patterns in globocan 2012</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ferlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Cancer</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="E386" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">American thyroid association management guidelines for adult patients with thyroid nodules and differentiated thyroid cancer: the american thyroid association guidelines task force on thyroid nodules and differentiated thyroid cancer</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Haugen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thyroid</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="133" />
			<date type="published" when="2015">2015. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Cibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The bethesda system for reporting thyroid cytopathology</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1159" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multi-organ nucleus segmentation challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1380" to="1391" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prognostic value of automatically extracted nuclear morphometric features in whole slide images of male breast cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mod. Pathol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1559" to="1565" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Thyroid FNA Cytology: Differential Diagnoses and Pitfalls</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kakudo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-13-1897-9</idno>
		<ptr target="https://doi.org/10.1007/978-981-13-1897-9" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="234" to="263" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Cibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 bethesda system for reporting thyroid cytopathology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1341" to="1346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pan-Nuke: an open pan-cancer histology dataset for nuclei instance segmentation and classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alemi Koohbanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Benet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-23937-4_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-23937-42" />
	</analytic>
	<monogr>
		<title level="m">ECDP 2019</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Reyes-Aldasoro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Janowczyk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Veta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bankhead</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11435</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Greenwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="555" to="565" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: a retrospective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CIA-Net: robust nuclei instance segmentation with contour-aware information aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">F</forename><surname>Onder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsougenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20351-1_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-20351-153" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2019</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11492</biblScope>
			<biblScope unit="page" from="682" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CA 2.5 -net nuclei segmentation framework with a microscopy cell benchmark collection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-343" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="445" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clusterseg: a crowd cluster pinpointed nucleus segmentation framework with cross-modality datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102758</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conformer: local features coupling global representations for visual recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swin-unet: unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-89" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022 Workshops: Tel</title>
		<meeting><address><addrLine>Aviv, Israel; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-10-27">23-27 October 2022. 2023</date>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Segnet: a deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">UNet++: a nested u-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-51" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pseudoseg: designing pseudo labels for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09713</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation via cross teaching between cnn and transformer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="820" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A multi-task mean teacher for semi-supervised shadow detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5611" to="5620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: looking wider to see better</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
