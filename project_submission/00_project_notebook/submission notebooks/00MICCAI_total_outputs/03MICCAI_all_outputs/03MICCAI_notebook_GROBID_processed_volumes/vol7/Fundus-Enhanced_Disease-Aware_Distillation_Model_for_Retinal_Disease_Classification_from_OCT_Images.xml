<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images</title>
				<funder ref="#_qKqUEEz #_PHgQMea">
					<orgName type="full">Foshan HKUST</orgName>
				</funder>
				<funder ref="#_wvnQyXG">
					<orgName type="full">Hong Kong Innovation and Technology Fund</orgName>
				</funder>
				<funder ref="#_9RPCmvv">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lehan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weihang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mei</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep2">Guangdong Provincial Hospital of Integrated Traditional Chinese and Western Medicine</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chubin</forename><surname>Ou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Guangdong Weiren Meditech Co., Ltd</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">44B6208A453374DE25E00E8193897AB9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Retinal Disease Classification</term>
					<term>Knowledge Distillation</term>
					<term>OCT Images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optical Coherence Tomography (OCT) is a novel and effective screening tool for ophthalmic examination. Since collecting OCT images is relatively more expensive than fundus photographs, existing methods use multi-modal learning to complement limited OCT data with additional context from fundus images. However, the multi-modal framework requires eye-paired datasets of both modalities, which is impractical for clinical use. To address this problem, we propose a novel fundusenhanced disease-aware distillation model (FDDM), for retinal disease classification from OCT images. Our framework enhances the OCT model during training by utilizing unpaired fundus images and does not require the use of fundus images during testing, which greatly improves the practicality and efficiency of our method for clinical use. Specifically, we propose a novel class prototype matching to distill disease-related information from the fundus model to the OCT model and a novel class similarity alignment to enforce consistency between disease distribution of both modalities. Experimental results show that our proposed approach outperforms single-modal, multi-modal, and state-of-the-art distillation methods for retinal disease classification. Code is available at https://github.com/xmed-lab/FDDM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Retinal diseases are one of the most common eye disorders, which can lead to vision impairment and blindness if left untreated. Computer-aided diagnosis has been increasingly used as a tool to detect ophthalmic diseases at the earliest possible time and to ensure rapid treatment. Optical Coherence Tomography (OCT) <ref type="bibr" target="#b5">[6]</ref> is an innovative imaging technique with the ability to capture micrometer-resolution images of retina layers, which provides a deeper view compared to alternative methods, such as fundus photographs <ref type="bibr" target="#b16">[17]</ref>, thereby allowing diseases to be detected earlier and more accurately. Because of this, OCT imaging has become the primary diagnostic test for many diseases, such as age-related macular degeneration, central serous chorioretinopathy, and retinal vascular occlusion <ref type="bibr" target="#b1">[2]</ref>.</p><p>Traditional methods manually design OCT features and adopt machine learning classifiers for prediction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. In recent years, deep learning methods have achieved outstanding performance on various medical imaging analysis tasks and have also been successfully applied to retinal disease classification with OCT images <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. However, diagnosing disease with a single OCT modality, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> (a), is still challenging since OCT scans are inadequate compared with fundus photos due to their more expensive cost in data collection. Some methods attempt to use extra layer-related knowledge from the segmentation task to improve prediction despite limited OCT data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>, but this leads to increased training costs since an additional segmentation model is required.</p><p>Recent works have attempted to include additional modalities for classification through multi-modal learning shown in Fig. <ref type="figure" target="#fig_0">1 (b)</ref>, where fundus and OCT images are jointly used to detect various retinal diseases and achieve promising results <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. Wang et al. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> used a two-stream structure to extract fundus and OCT features, which are then concatenated for prediction. He et al. <ref type="bibr" target="#b3">[4]</ref> designed modality-specific attention networks to tackle differences in modal characteristics. Nevertheless, there are still limitations in these existing approaches. Firstly, existing multi-modal learning approaches require strictly paired images from both modalities for training and testing. This necessitates the collection of multi-modal images for the same patients, which can be laborious, costly, and not easily achievable in real-world clinical practice. Secondly, previous works mostly focused on a limited set of diseases, such as age-related macular degeneration (AMD), diabetic retinopathy, and glaucoma, which cannot reflect the complexity and diversity of real-world clinical settings.</p><p>To this end, we propose Fundus-enhanced Disease-aware Distillation Model (FDDM) for retinal disease classification from OCT images, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> (c). FDDM is motivated by the observation that fundus images and OCT images provide complementary information for disease classification. For instance, in the case of AMD detection, fundus images can provide information on the number and area of drusen or atrophic lesions of AMD, while OCT can reveal the aggressiveness of subretinal and intraretinal fluid lesions <ref type="bibr" target="#b25">[26]</ref>. Utilizing this complementary information from both modalities can enhance AMD detection accuracy.</p><p>Our main goal is to extract disease-related information from a fundus teacher model and transfer it to an OCT student model, all without relying on paired training data. To achieve this, we propose a class prototype matching method to align the general disease characteristics between the two modalities while also eliminating the adverse effects of a single unreliable fundus instance. Moreover, we introduce a novel class similarity alignment method to encourage the student to learn similar inter-class relationships with the teacher, thereby obtaining additional label co-occurrence information. Unlike existing works, our method is capable of extracting valuable knowledge from any accessible fundus dataset without additional costs or requirements. Moreover, our approach only needs one modality during the inference process, which can help greatly reduce the prerequisites for clinical application.</p><p>To summarize, our main contributions include 1) We propose a novel fundusenhanced disease-aware distillation model for retinal disease classification via class prototype matching and class similarity alignment; 2) Our proposed method offers flexible knowledge transfer from any publicly available fundus dataset, which can significantly reduce the cost of collecting expensive multi-modal data. This makes our approach more accessible and cost-effective for retinal disease diagnosis; 3) We validated our proposed method using a clinical dataset and other publicly available datasets. The results demonstrate superior performance when compared to state-of-the-art alternatives, confirming the effectiveness of our approach for retinal disease classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Our approach is based on two ideas: class prototype matching, which distills generalized disease-specific knowledge unaffected by individual sample noise, and class similarity alignment, which transfers additional label co-occurrence information from the teacher to the student. Details of both components are discussed in the sections below. An overview of our framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>We denote the fundus dataset as</p><formula xml:id="formula_0">D f = {x f,i , y f,i } N i=1</formula><p>, and the OCT dataset as</p><formula xml:id="formula_1">D o = {x o,j , y o,j } M j=1 .</formula><p>To utilize knowledge from the fundus modality during training, we build a teacher model, denoted F t , trained on D f . Similarly, an OCT model F s is built to learn from OCT images D o using the same backbone architecture as the fundus model. We use binary cross-entropy loss as the classification loss L CLS for optimization, to allow the same input to be associated with multiple classes. During inference time, only OCT data is fed into the OCT model to compute the probabilities p = {p c } C c=1 for each disease, c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Class Prototype Matching</head><p>To distill features from the teacher model into the student model, we aim to ensure that features belonging to the same class are similar. However, we note that individual sample features can be noisy since they contain variations specific to the sample instance instead of the class. In order to reduce noise and ensure disease-specific features are learnt, we compress features of each class into a class prototype vector to represent the general characteristics of the disease. During the training per batch, the class prototype vector is the average of all the feature vectors belonging to each category, which is formulated as:</p><formula xml:id="formula_2">e c f = B i=1 v f,i * y c f,i B i=1 y c f,i , e c o = B j=1 P (v o,j ) * y c o,j B j=1 y c o,j ,<label>(1)</label></formula><p>where e c f and e c o denote the prototype vector for class c of the fundus and OCT modality respectively, v f,i (v o,j ) represents the feature vector of the input image, and y c f,i y c o,j is a binary number which indicates whether the instance belongs to class c or not. P demotes an MLP projector that projects OCT features into the same space as fundus features.</p><p>In the class prototype matching stage, we apply softmax loss to the prototype vectors of fundus modality to formulate soft targets E c f = σ(e c f /τ ), where τ is the temperature scale that controls the strength to soften the distribution. Student class prototypes E c o are obtained in the same way. KL divergence is then used to encourage OCT student to learn matched class prototypes with fundus teacher:</p><formula xml:id="formula_3">L CP M = C c=1 E c f log E c f E c o ,<label>(2)</label></formula><p>By doing so, the OCT model is able to use the global information from fundus modality for additional supervision. Overall, our approach adopts class prototypes from fundus modality instead of noisy features from individual samples, which provides more specific knowledge for OCT student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Class Similarity Alignment</head><p>We also note that for multi-label classification tasks, relationships among different classes also contain important information, especially since label cooccurrence is common for eye diseases. Based on this observation, we additionally propose a class similarity alignment scheme to distill knowledge concerning inter-class relationships from fundus model to OCT model. First, we estimate the disease distribution by averaging the obtained logits of fundus and OCT model in a class-wise manner to get</p><formula xml:id="formula_4">q f = {q c f } C c=1 , q o = {q c o } C c=1 .</formula><p>Then, to transfer information on inter-class relationships, we enforce cosine similarity matrices of the averaged logits to be consistent between teacher and student model. The similarity matrix for teacher model is calculated as <ref type="figure">(q c</ref> f , q f )/τ ) and is obtained similarly for student model, Q c o . KL divergence loss is used to encourage alignment between the two similarity matrices:</p><formula xml:id="formula_5">Q c f = σ(sim</formula><formula xml:id="formula_6">L CSA = C c=1 Q c f log Q c f Q c o ,<label>(3)</label></formula><p>In this way, disease distribution knowledge is distilled from fundus teacher model, forcing OCT student model to learn additional knowledge concerning inter-class relationships, which is highly important in multi-label scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Overall Framework</head><p>The overall loss is the combination of classification loss and distillation enhancement loss:</p><formula xml:id="formula_7">L OCT = L CLS + αL CP M + βL CSA , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where α and β are loss weights that control the contribution of each distillation loss. Admittedly, knowledge distillation strategies in computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b26">27]</ref> can be applied to share multi-modal information as well. Unlike classical distillation methods, our two novel distillation losses allow knowledge about disease-specific features and inter-class relationships to be transferred, thereby allowing knowledge distillation to be conducted with unpaired data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Dataset. To evaluate the effectiveness of our approach, we collect a new dataset TOPCON-MM with paired fundus and OCT images from 369 eyes of 203 patients in Guangdong Provincial Hospital of Integrated Traditional Chinese and Western Medicine using a Topcon Triton swept-source OCT featuring multimodal fundus imaging. For fundus images, they are acquired at a resolution of 2576 × 1934. For OCT scans, the resolution ranges from 320 × 992 to 1024 × 992. Specifically, multiple fundus and OCT images are obtained for each eye, Implementation Details. Following prior work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, we use contrastlimited adaptive histogram equalization for fundus images and median filter for OCT images as data preprocessing. We adopt data augmentation including random crop, flip, rotation, and changes in contrast, saturation, and brightness.</p><p>All the images are resized to 448 × 448 before feeding into the network. For a fair comparison, we apply identical data processing steps, data augmentation operations, model backbones and running epochs in all the experiments. We use SGD to optimize parameters with a learning rate of 1e-3, a momentum of 0.9, and a weight decay of 1e-4. The batch size is set to 8. For weight parameters, τ is set to 4, α is set to 2 and β is set to 1. All the models are implemented on an NVIDIA RTX 3090 GPU. We split the dataset into training and test subsets according to the patient's identity and maintained a training-to-test set ratio of approximately 8:2. To ensure the robustness of the model, the result was reported by five-fold cross-validation.</p><p>Evaluation Metrics. We follow previous work <ref type="bibr" target="#b11">[12]</ref> to evaluate image-level performance. As each eye in our dataset was scanned multiple times, we use the ensemble results from all the images of the same eye to determine the final prediction. More specifically, if any image indicates an abnormality, the eye is predicted to have the disease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compare with State-of-the-Arts</head><p>To prove the effectiveness of our proposed method, we compare our approach with single-modal, multi-modal, and knowledge distillation methods. From Table <ref type="table" target="#tab_1">2</ref>, it is apparent that the model trained with OCT alone performs better than the fundus models. It is noteworthy that current multi-modality methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref> and knowledge distillation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref> do not yield improved results on our dataset. Table <ref type="table" target="#tab_1">2</ref> also demonstrates that compared with the single-modal OCT baseline, our method improves MAP from 66.44% to 69.06%, F1 from 64.16% to 69.17%. This shows that it is still possible to learn valuable information from the fundus modality to assist the OCT model, despite being a weaker modality. It can be observed that our approach outperforms the state-of-the-art multi-modal retinal image classification method <ref type="bibr" target="#b3">[4]</ref> by 9.57% in MAP (69.06% v.s. 59.49%). Notably, our method excels the best-performing knowledge distillation method [1] by 3.96% in MAP (69.06% v.s. 65.10%). We also note that the alternative methods are limited to training with eye-paired fundus and OCT images only, whilst our approach does not face such restrictions.</p><p>To further demonstrate the efficiency of our proposed distillation enhancement approach, we validate our method on a publicly available multi-modal dataset with fundus and OCT images, MMC-AMD <ref type="bibr" target="#b23">[24]</ref>. MMC-AMD dataset contains four classes: normal, dry AMD, PCV, and wet AMD. We reproduce single-modal ResNet, Two-Stream CNN <ref type="bibr" target="#b24">[25]</ref>, and KD methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref> as baselines and show results in Fig. <ref type="figure" target="#fig_2">3 (a)</ref>. It can be seen that our method improves MAP to 92.29%, largely surpassing existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results Trained with Other Fundus Datasets</head><p>Since we implement distillation in a disease-aware manner, multi-modal fundus and OCT training data do not need to be paired. Theoretically, any publicly available fundus dataset could be applied as long as it shares a label space that overlaps with our OCT data. To verify this hypothesis, we separately reproduce our methods with fundus images from two datasets, MMC-AMD <ref type="bibr" target="#b23">[24]</ref> and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Studies</head><p>Table <ref type="table" target="#tab_2">3</ref> shows the ablation study of our method. To provide additional insight, we also show the results on majority classes, which contain over 10% images of the dataset, and minority classes with less than 10%. It can be seen that individually using CPM and CSA can improve the overall result by 1.32% and 1.06% in MAP, respectively. Removing either of the components degrades the performance. Results also show that CPM improves classification performance in majority classes by distilling disease-specific knowledge, while CSA benefits minority classes by attending to inter-disease relationships. By simultaneously adopting CPM and CSA, the overall score of all the classes is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our work proposes a novel fundus-enhanced disease-aware distillation module, FDDM, for retinal disease classification. The module incorporates class prototype matching to distill global disease information from the fundus teacher to the OCT student, while also utilizing class similarity alignment to ensure the consistency of disease relationships between both modalities. Our approach deviates from the existing models that rely on paired instances for multi-modal training and inference, making it possible to extract knowledge from any available fundus data and render predictions with only OCT modality. As a result, our approach significantly reduces the prerequisites for clinical applications. Our extensive experiments demonstrate that our method outperforms existing baselines by a considerable margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison between (a) single-modal, (b) multi-modal learning, and (c) our proposed distillation enhancement method. Under our setting, images from an additional modalitiy are only used for model training and are not required for inference.</figDesc><graphic coords="2,57,30,54,23,310,00,70,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our proposed FDDM. Our method is based on class prototype matching, which distills disease-specific features, and class similarity alignment, which distills inter-class relationships.</figDesc><graphic coords="4,48,30,54,11,327,40,169,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Results on other publicly available datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of our collected TOPCON-MM dataset.</figDesc><table><row><cell>Category</cell><cell cols="11">Normal dAMD wAMD DR CSC PED MEM FLD EXU CNV RVO Total</cell></row><row><cell>Eyes</cell><cell>153</cell><cell>52</cell><cell>30</cell><cell>72 15</cell><cell>23</cell><cell>38</cell><cell>93</cell><cell>90</cell><cell>14</cell><cell>10</cell><cell>369</cell></row><row><cell cols="2">Fundus Images 299</cell><cell>178</cell><cell>171</cell><cell>502 95</cell><cell cols="2">134 200</cell><cell cols="4">638 576 143 34</cell><cell>1520</cell></row><row><cell>OCT Images</cell><cell>278</cell><cell>160</cell><cell>145</cell><cell>502 95</cell><cell cols="2">133 196</cell><cell cols="4">613 573 138 34</cell><cell>1435</cell></row><row><cell cols="12">and each image may reveal multiple diseases with consistent labels specific to</cell></row><row><cell cols="12">that eye. All cases were examined by two ophthalmologists independently to</cell></row><row><cell cols="12">determine the diagnosis label. If the diagnosis from two ophthalmologists dis-</cell></row><row><cell cols="12">agreed with each other, a third senior ophthalmologist with more than 15 years of</cell></row><row><cell cols="12">experience was consulted to determine the final diagnosis. As shown in Table 1,</cell></row><row><cell cols="12">there are eleven classes, including normal, dry age-related macular degeneration</cell></row><row><cell cols="12">(dAMD), wet age-related macular degeneration (wAMD), diabetic retinopathy</cell></row><row><cell cols="12">(DR), central serous chorioretinopathy (CSC), pigment epithelial detachment</cell></row><row><cell cols="12">(PED), macular epiretinal membrane (MEM), fluid (FLD), exudation (EXU),</cell></row><row><cell cols="11">choroid neovascularization (CNV) and retinal vascular occlusion (RVO).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on our collected TOPCON-MM dataset. "Training" and "Inference" indicate which modalities are required for both phases. "Paired" indicates whether paired fundus-OCT images are required in training. All the experiments use ResNet50 as the backbone. For multi-modal methods, two ResNet50 without shared weights are applied separately for each modality. "Late Fusion" refers to the direct ensemble of the results from models trained with two single modalities. †: we implement multi-modal methods in retinal disease classification. : we run KD methods in computer vision. ± 3.81 53.14 ± 6.60 95.28 ± 0.85 64.16 ± 6.24 87.73 ± 1.44 Multi-Modal Methods ± 1.34 54.45 ± 2.72 94.29 ± 0.74 64.93 ± 3.00 86.92 ± 1.48 Two-Stream CNN † [25] Both Both ✓ 58.75 ± 2.71 53.47 ± 3.82 92.97 ± 0.91 61.82 ± 4.02 84.79 ± 2.77 MSAN † [4] Both Both ✓ 59.49 ± 3.43 56.44 ± 3.13 93.37 ± 0.59 63.95 ± 3.77 84.51 ± 1.91 ± 2.63 53.13 ± 5.49 95.09 ± 0.92 63.19 ± 6.53 87.97 ± 1.32</figDesc><table><row><cell>Method</cell><cell cols="4">Training Inference Paired MAP</cell><cell>Sensitivity Specificity F1 Score AUC</cell></row><row><cell>Single-Modal Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet50</cell><cell>Fundus</cell><cell>Fundus</cell><cell>-</cell><cell cols="2">50.56 ± 3.05 43.68 ± 6.58 92.24 ± 0.77 54.95 ± 7.09 79.97 ± 1.63</cell></row><row><cell cols="6">ResNet50 66.44 Late Fusion OCT OCT -Both Both ✓ 63.83 FitNet [21] B o t h O C T ✓ 63.41 ± 3.45 54.44 ± 4.04 94.87 ± 0.58 65.00 ± 4.32 87.17 ± 1.85</cell></row><row><cell>KD [5]</cell><cell>B o t h</cell><cell>O C T</cell><cell>✓</cell><cell cols="2">63.69 ± 2.04 51.70 ± 3.10 95.75 ± 0.62 63.56 ± 2.32 87.90 ± 1.03</cell></row><row><cell>RKD [20]</cell><cell>B o t h</cell><cell>O C T</cell><cell>✓</cell><cell cols="2">63.59 ± 3.04 53.42 ± 1.71 94.42 ± 1.81 63.70 ± 0.72 87.36 ± 2.08</cell></row><row><cell>DKD [27]</cell><cell>B o t h</cell><cell>O C T</cell><cell>✓</cell><cell cols="2">64.40 ± 2.09 53.83 ± 5.23 95.24 ± 0.11 64.00 ± 4.44 87.52 ± 0.58</cell></row><row><cell cols="6">SimKD [1] 65.10 Ours B o t h O C T ✓ Both OCT ✗ 69.06 ± 3.39 57.15 ± 5.93 95.93 ± 0.57 69.17 ± 6.07 89.06 ± 0.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of our method. "Majority" and "Minority" refers to the average score of classes that represent more than 10% or less than 10% of the total number of images, respectively. "Overall" indicates overall performance on all the classes. ± 3.81 71.12 ± 4.04 58.26 ± 7.42 64.16 ± 6.24 87.73 ± 1.44 ✗ ✓ 67.50 ± 3.00 70.60 ± 4.91 62.08 ± 8.72 65.40 ± 5.21 88.09 ± 1.28 ✓ ✗ 67.76 ± 2.34 72.26 ± 3.20 59.90 ± 8.32 65.58 ± 2.58 88.73 ± 1.32 ✓ ✓ 69.06 ± 3.39 73.34 ± 3.48 61.47 ± 8.17 69.17 ± 6.07 89.06 ± 0.97</figDesc><table><row><cell>Method</cell><cell>MAP</cell><cell></cell><cell>F1 Score AUC</cell></row><row><cell cols="2">CPM CSA Overall</cell><cell>Majority</cell><cell>Minority</cell></row><row><cell cols="4">✗ 66.44 RFMiD [19]. To ensure label overlap, we only select fundus and OCT images from ✗</cell></row><row><cell cols="4">common classes for training and validation, namely, 3 classes for MMC-AMD</cell></row><row><cell cols="4">and 6 classes for RFMiD. The results are reported in Fig. 3 (b). Compared with</cell></row><row><cell cols="4">single-modal OCT model, our distillation enhancement can achieve an increase</cell></row><row><cell cols="4">of 4.26% (84.06% v.s. 79.80%) and 2.21% (75.47% v.s. 73.26%) in MAP. Our</cell></row><row><cell cols="4">results prove that our method has the flexibility to use any existing fundus dataset</cell></row><row><cell cols="3">to enhance OCT classification.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 H. Greenspan et al. (Eds.): MICCAI 2023, LNCS 14226, pp. 639-648, 2023. https://doi.org/10.1007/978-3-031-43990-2_60</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by grants from <rs type="funder">Foshan HKUST</rs> Projects under Grants <rs type="grantNumber">FSUST21-HKUST10E</rs> and <rs type="grantNumber">FSUST21-HKUST11E</rs>, as well as by the <rs type="funder">Hong Kong Innovation and Technology Fund</rs> under Projects <rs type="grantNumber">PRP/041/22FX</rs> and <rs type="grantNumber">ITS/030/21</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qKqUEEz">
					<idno type="grant-number">FSUST21-HKUST10E</idno>
				</org>
				<org type="funding" xml:id="_PHgQMea">
					<idno type="grant-number">FSUST21-HKUST11E</idno>
				</org>
				<org type="funding" xml:id="_wvnQyXG">
					<idno type="grant-number">PRP/041/22FX</idno>
				</org>
				<org type="funding" xml:id="_9RPCmvv">
					<idno type="grant-number">ITS/030/21</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 60.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Knowledge distillation with the reused teacher classifier</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11933" to="11942" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Retina Illustrated</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ehlers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thieme Medical Publishers</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention to lesion: lesion-aware convolutional neural network for retinal optical coherence tomography image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1959" to="1970" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-modal retinal image classification with modality-specific attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1591" to="1602" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optical coherence tomography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="issue">5035</biblScope>
			<biblScope unit="page" from="1178" to="1181" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic classification of retinal optical coherence tomography images with layer guided convolutional neural network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1026" to="1030" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transfer learning based classification of optical coherence tomography images with diabetic macular edema and dry age-related macular degeneration</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P K</forename><surname>Karri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="579" to="592" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying medical diagnoses and treatable diseases by image-based deep learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kermany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1122" to="1131" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning is effective for classifying normal versus age-related macular degeneration oct images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Baughman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmol. Retina</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="322" to="327" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification of SD-OCT volumes using local binary patterns: experimental validation for DME detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lemaître</surname></persName>
		</author>
		<idno type="DOI">10.1155/2016/3298606</idno>
		<ptr target="https://doi.org/10.1155/2016/3298606" />
	</analytic>
	<monogr>
		<title level="j">J. Ophthalmol</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-modal multi-instance learning for retinal disease recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACMMM</publisher>
			<biblScope unit="page" from="2474" to="2482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning based early stage diabetic retinopathy detection using optical coherence tomography</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page" from="134" to="144" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal information fusion for glaucoma and diabetic retinopathy classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16525-2_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16525-26" />
	</analytic>
	<monogr>
		<title level="m">Ophthalmic Medical Image Analysis: 9th International Workshop</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Antony</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Macgillivray</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore, Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022-09-22">September 22, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="53" to="62" />
		</imprint>
	</monogr>
	<note>OMIA</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">One-stage attention-based network for image classification and segmentation on optical coherence tomography image</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3025" to="3029" />
		</imprint>
		<respStmt>
			<orgName>SMC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated macular pathology diagnosis in retinal oct images using multi-scale spatial pyramid and local binary patterns in texture and shape encoding</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wollstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Schuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="748" to="759" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ophthalmic Diagnostic Imaging: Retina</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dolz-Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tafreshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmitz-Valckenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Holz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-16638-0_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-16638-04" />
	</analytic>
	<monogr>
		<title level="m">High Resolution Imaging in Microscopy and Ophthalmology</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Bille</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="87" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">M 2 LC-Net: A multi-modal multi-disease long-tailed classification network for real clinical scenes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">China Commun.D</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="210" to="220" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Retinal fundus multi-disease image dataset (RFMid): a dataset for multi-disease detection research</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pachade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Channel-wise knowledge distillation for dense prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5311" to="5320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully automated detection of diabetic macular edema and dry age-related macular degeneration from optical coherence tomography images</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3568" to="3577" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning two-stream CNN for multi-modal age-related macular degeneration categorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4111" to="4122" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stream CNN with loose pair training for multi-modal AMD categorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-718" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The possibility of the combination of oct and fundus images for improving the diagnostic accuracy of deep learning for age-related macular degeneration: a preliminary experiment</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Selvaperumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Biol. Eng. Comput</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="677" to="687" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Decoupled knowledge distillation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11953" to="11962" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
