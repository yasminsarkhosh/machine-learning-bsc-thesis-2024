<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos</title>
				<funder ref="#_WjkpBDB">
					<orgName type="full">ARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ruyi</forename><surname>Zha</surname></persName>
							<email>ruyi.zha@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">AIM for Health Lab</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Monash Medical AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">AIM for Health Lab</orgName>
								<orgName type="institution" key="instit2">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Monash Medical AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Airdoc-Monash Research Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="13" to="23"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E9557441A14162AD0283A33EC6D6F36C</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D Reconstructon</term>
					<term>Neural Fields</term>
					<term>Robotic Surgery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reconstructing soft tissues from stereo endoscope videos is an essential prerequisite for many medical applications. Previous methods struggle to produce high-quality geometry and appearance due to their inadequate representations of 3D scenes. To address this issue, we propose a novel neural-field-based method, called EndoSurf, which effectively learns to represent a deforming surface from an RGBD sequence. In EndoSurf, we model surface dynamics, shape, and texture with three neural fields. First, 3D points are transformed from the observed space to the canonical space using the deformation field. The signed distance function (SDF) field and radiance field then predict their SDFs and colors, respectively, with which RGBD images can be synthesized via differentiable volume rendering. We constrain the learned shape by tailoring multiple regularization strategies and disentangling geometry and appearance. Experiments on public endoscope datasets demonstrate that EndoSurf significantly outperforms existing solutions, particularly in reconstructing high-fidelity shapes. Code is available at https://github.com/Ruyi-Zha/endosurf.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Surgical scene reconstruction using stereo endoscopes is crucial to Robotic-Assisted Minimally Invasive Surgery (RAMIS). It aims to recover a 3D model of R. Zha and X. Cheng-Equal contribution. Fig. <ref type="figure">1</ref>. 3D meshes extracted from EndoNeRF <ref type="bibr" target="#b27">[26]</ref> and our method. EndoNeRF cannot recover a smooth and accurate surface even with post-processing filters.</p><p>the observed tissues from a stereo endoscope video. Compared with traditional 2D monitoring, 3D reconstruction offers notable advantages because it allows users to observe the surgical site from any viewpoint. Therefore, it dramatically benefits downstream medical applications such as surgical navigation <ref type="bibr" target="#b22">[21]</ref>, surgeon-centered augmented reality <ref type="bibr" target="#b19">[18]</ref>, and virtual reality <ref type="bibr" target="#b8">[7]</ref>. General reconstruction pipelines first estimate depth maps with stereo-matching <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">13]</ref> and then fuse RGBD images into a 3D model <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b30">28]</ref>. Our work focuses on the latter, i.e., how to accurately reconstruct the shape and appearance of deforming surfaces from RGBD sequences.</p><p>Existing approaches represent a 3D scene in two ways: discretely or continuously. Discrete representations include point clouds <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b30">28]</ref> and mesh grids <ref type="bibr" target="#b18">[17]</ref>. Additional warp fields <ref type="bibr" target="#b10">[9]</ref> are usually utilized to compensate for tissue deformation. Discrete representation methods produce surfaces efficiently due to their sparsity property. However, this property also limits their ability to handle complex high-dimensional changes, e.g., non-topology deformation and color alteration resulting from cutting or pulling tissues.</p><p>Recently, continuous representations have become popular with the blossoming of neural fields, i.e., neural networks that take space-time inputs and return the required quantities. Neural-field-based methods <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr">[25]</ref><ref type="bibr" target="#b27">[26]</ref><ref type="bibr" target="#b29">[27]</ref> exploit deep neural networks to implicitly model complex geometry and appearance, outperforming discrete-representation-based methods. A good representative is EndoNeRF <ref type="bibr" target="#b27">[26]</ref>. It trains two neural fields: one for tissue deformation and the other for canonical density and color. EndoNeRF can synthesize reasonable RGBD images with post-processing filters. However, the ill-constrained properties of the density field deter the network from learning a solid surface shape. Figure <ref type="figure">1</ref> shows that EndoNeRF can not accurately recover the surface even with filters. While there have been attempts to parameterize other geometry fields, e.g., occupancy fields <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b21">20]</ref> and signed distance function (SDF) fields <ref type="bibr">[25,</ref><ref type="bibr" target="#b29">27]</ref>, they hypothesize static scenes and diverse viewpoints. Adapting them to surgical scenarios where surfaces undergo deformation and camera movement is confined is non-trivial.</p><p>We propose EndoSurf: neural implicit fields for Endoscope-based Surf ace reconstruction, a novel neural-field-based method that effectively learns to represent dynamic scenes. Specifically, we model deformation, geometry, and appearance with three separate multi-layer perceptrons (MLP). The deformation network transforms points from the observation space to the canonical space. The  geometry network represents the canonical scene as an SDF field. Compared with density, SDF is more self-contained as it explicitly defines the surface as the zero-level set. We enforce the geometry network to learn a solid surface by designing various regularization strategies. Regarding the appearance network, we involve positions and normals as extra clues to disentangle the appearance from the geometry. Following [25], we adopt unbiased volume rendering to synthesize color images and depth maps. The network is optimized with gradient descent by minimizing the error between the real and rendered results. We evaluate EndoSurf quantitatively and qualitatively on public endoscope datasets. Our work demonstrates superior performance over existing solutions, especially in reconstructing smooth and accurate shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Problem Setting. Given a stereo video of deforming tissues, we aim to reconstruct the surface shape S and texture C. Similar to EndoNeRF <ref type="bibr" target="#b27">[26]</ref>, we take as inputs a sequence of frame data</p><formula xml:id="formula_0">{(I i , D i , M i , P i , t i )} T i=1 .</formula><p>Here T stands for the total number of frames. I i ∈ R H×W ×3 and D i ∈ R H×W refer to the ith left RGB image and depth map with height H and width W . Foreground mask M i ∈ R H×W is utilized to exclude unwanted pixels, such as surgical tools, blood, and smoke. Projection matrix P i ∈ R 4×4 maps 3D coordinates to 2D pixels. t i = i/T is each frame's timestamp normalized to [0, 1]. While stereo matching, surgical tool tracking, and pose estimation are also practical clinical concerns, in this work we prioritize 3D reconstruction and thus take depth maps, foreground masks, and projection matrices as provided by software or hardware solutions.</p><p>Pipeline. Figure <ref type="figure" target="#fig_1">2</ref>(a) illustrates the overall pipeline of our approach. Similar to <ref type="bibr">[25,</ref><ref type="bibr" target="#b27">26]</ref>, we incorporate our EndoSurf network into a volume rendering scheme. Specifically, we begin by adopting a mask-guided sampling strategy <ref type="bibr" target="#b27">[26]</ref> to select valuable pixels from a video frame. We then cast 3D rays from these pixels and hierarchically sample points along the rays <ref type="bibr">[25]</ref>. The EndoSurf network utilizes these sampled points and predicts their SDFs and colors. After that, we adopt the unbiased volume rendering method [25] to synthesize pixel colors and depths used for network training. We tailor loss functions to enhance the network's learning of geometry and appearance. In the following subsections, we will describe the EndoSurf network (cf. Sect. 2.2) and the optimization process (cf. Sect. 2.3) in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">EndoSurf: Representing Scenes as Deformable Neural Fields</head><p>We represent a dynamic scene as canonical neural fields warped to an observed pose. Separating the learning of deformation and canonical shapes has been proven more effective than directly modeling dynamic shapes <ref type="bibr" target="#b23">[22]</ref>. Particularly, we propose a neural deformation field Ψ d to transform 3D points from the observed space to the canonical space. The geometry and appearance of the canonical scene are described by a neural SDF field Ψ s and a neural radiance field Ψ r , respectively. All neural fields are modeled with MLPs with position encoding <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b25">24]</ref>. Neural SDF Field. The shape of the canonical scene is represented by a neural field Ψ s (x c ) → (ρ, f ) that maps a spatial position x c ∈ R 3 to its signed distance function ρ ∈ R and a geometry feature vector f ∈ R F with feature size F .</p><formula xml:id="formula_1">Neural Deformation Field. Provided a 3D point x o ∈ R 3 in the observed space at time t ∈ [0, 1], the neural deformation field Ψ d (x o , t) → Δx returns the displacement Δx ∈ R</formula><p>In 3D vision, SDF is the orthogonal distance of a point x to a watertight object's surface, with the sign determined by whether or not x is outside the object. In our case, we slightly abuse the term SDF since we are interested in a segment of an object rather than the whole thing. We extend the definition of SDF by imagining that the surface of interest divides the surrounding space into two distinct regions, as shown in Fig. <ref type="figure" target="#fig_1">2 (b)</ref>. SDF is positive if x falls into the region which includes the camera and negative if it is in the other. As x approaches the surface, the SDF value gets smaller until it reaches zero at the surface. Therefore, the surface of interest S is the zero-level set of SDF, i.e.,</p><formula xml:id="formula_2">S = {p ∈ R 3 |Ψ s (p) = 0}.</formula><p>Compared with the density field used in <ref type="bibr" target="#b27">[26]</ref>, the SDF field provides a more precise representation of surface geometry because it explicitly defines the surface as the zero-level set. The density field, however, encodes the probability of an object occupying a position, making it unclear which iso-surface defines the object's boundary. As a result, density-field-based methods <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b27">26]</ref> can not directly identify a depth via ray marching but rather render it by integrating the depths of sampled points with density-related weights. Such a rendering method can lead to potential depth ambiguity, i.e., camera rays pointing to the same surface produce different surface positions (Fig. <ref type="figure" target="#fig_1">2(b)</ref>).</p><p>Given a surface point p c ∈ R 3 in the canonical space, the surface normal n c ∈ R 3 is the gradient of the neural SDF field Ψ s : n c = ∇ Ψs (p c ). Normal n o of the deformed surface point p o can also be obtained with the chain rule.</p><p>Neural Radiance Field. We model the appearance of the canonical scene as a neural radiance field Ψ r (x c , v c , n c , f ) → c c that returns the color c c ∈ R 3 of a viewpoint (x c , v c ). Unlike <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b27">26]</ref>, which only take the view direction v c and feature vector f as inputs, we also feed the normal n c and position x c to the radiance field as extra geometric clues. Although the feature vector implies the normal and position information, it is validated that directly incorporating them benefits the disentanglement of geometry, i.e., allowing the network to learn appearance independently from the geometry [25,27].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization</head><p>Unbiased Volume Rendering. Given a camera ray r(h) = o o + hv o at time t in the observed space, we sample N points x i in a hierarchical manner along this ray [25] and predict their SDFs ρ i and colors c i via EndoSurf. The color Ĉ and depth D of the ray can be approximated by unbiased volume rendering [25]:</p><formula xml:id="formula_3">Ĉ(r(h)) = N i=1 T i α i c i , D(r(h)) = N i=1 T i α i h i ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_4">T i = i-1 j=1 (1 -α j ), α i = max((φ(ρ i ) -φ(ρ i+1</formula><p>))/φ(ρ i ), 0) and φ(ρ) = (1 + e -ρ/s ) -1 . Note that s is a trainable standard deviation, which approaches zero as the network training converges.</p><p>Loss. We train the network with two objectives: 1) to minimize the difference between the actual and rendered results and 2) to impose constraints on the neural SDF field such that it aligns with its definition. Accordingly, we design two categories of losses: rendering constraints and geometry constraints:</p><formula xml:id="formula_5">L = λ 1 L color + λ 2 L depth rendering + λ 3 L eikonal + λ 4 L sdf + λ 5 L visible + λ 6 L smooth geometry , (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>where λ i=1,••• ,6 are balancing weights. The rendering constraints include the color reconstruction loss L color and depth reconstruction loss L depth :</p><formula xml:id="formula_7">L color = r∈R M (r)( Ĉ(r) -C(r)) 1 , L depth = r∈R M (r)( D(r) -D(r)) 1 , (3)</formula><p>where M (r), { Ĉ, D}, {C, D} and R are ray masks, rendered colors and depths, real colors and depths, and ray batch, respectively.</p><p>We regularize the neural SDF field Ψ s with four losses: Eikonal loss L eikonal , SDF loss L sdf , visibility loss L visible , and smoothness loss L smooth .</p><formula xml:id="formula_8">L eikonal = x∈X ( ∇ Ψs (x) 2 -1) 2 , L sdf = p∈D Ψ s (p) 1 , L visible = p∈D max( ∇ Ψs (p), v c , 0), L smooth = p∈D ∇ Ψs (p) -∇ Ψs (p + ) 1 .</formula><p>(4) Here the Eikonal loss L eikonal <ref type="bibr" target="#b11">[10]</ref> encourages Ψ s to satisfy the Eikonal equation <ref type="bibr" target="#b9">[8]</ref>. Points x are sampled from the canonical space X . The SDF loss L sdf restricts the SDF value of points lying on the ground truth depths D to zero. The visibility loss L visible limits the angle between the canonical surface normal and the viewing direction v c to be greater than 90 • . The smoothness loss L smooth encourages a surface point and its neighbor to be similar, where is a random uniform perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Settings</head><p>Datasets and Evaluation. We conduct experiments on two public endoscope datasets, namely ENDONERF <ref type="bibr" target="#b27">[26]</ref> and SCARED <ref type="bibr" target="#b0">[1]</ref> (See statistical details in the supplementary material). ENDONERF provides two cases of in-vivo prostatectomy data with estimated depth maps <ref type="bibr" target="#b14">[13]</ref> and manually labeled tool masks. SCARED <ref type="bibr" target="#b0">[1]</ref> collects the ground truth RGBD images of five porcine cadaver abdominal anatomies. We pre-process the datasets by normalizing the scene into a unit sphere and splitting the frame data into 7:1 training and test sets.</p><p>Our approach is compared with EndoNeRF <ref type="bibr" target="#b27">[26]</ref>, the state-of-the-art neuralfield-based method. There are three outputs for test frames: RGB images, depth maps, and 3D meshes. The first two outputs are rendered the same way as the training process. We use marching cubes <ref type="bibr" target="#b16">[15]</ref> to extract 3D meshes from the density and SDF fields. The threshold is set to 5 for the density field and 0 for the SDF field. See the supplementary material for the validation of threshold selection. Five evaluation metrics are used: PSNR, SSIM, LPIPS, RMSE, and point cloud distance (PCD). The first three metrics assess the similarity between the actual and rendered RGB images <ref type="bibr" target="#b27">[26]</ref>, while RMSE and PCD measure depth map <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">14]</ref> and 3D mesh <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> reconstruction quality, respectively. Implementation Details. We train neural networks per scene, i.e., one model for each case. All neural fields consist of 8-layer 256-channel MLPs with a skip connection at the 4th layer. Position encoding frequencies in all fields are 6, Fig. <ref type="figure">3</ref>. 2D rendering results on the dynamic case "ENDONERF-cutting" and static case "SCARED-d1k1". Our method yields high-quality depth and normal maps, whereas those of EndoNeRF exhibit jagged noise, over-smoothed edges (white boxes), and noticeable artifacts (white rings).</p><p>except those in the radiance field are 10 and 4 for location and direction, respectively. The SDF network is initialized <ref type="bibr" target="#b1">[2]</ref> for better training convergence. We use Adam optimizer <ref type="bibr" target="#b12">[11]</ref> with a learning rate of 0.0005, which warms up for 5k iterations and then decays with a rate of 0.05. We sample 1024 rays per batch and 64 points per ray. The initial standard deviation s is 0.3. The weights in Eq. 2 are λ 1 = 1.0, λ 2 = 1.0, λ 3 = 0.1, λ 4 = 1.0, λ 5 = 0.1 and λ 6 = 0.1. We train our model with 100K iterations for 9 h on an NVIDIA RTX 3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Qualitative and Quantitative Results</head><p>As listed in Table <ref type="table" target="#tab_0">1</ref>, EndoSurf yields superior results against EndoNeRF. On the one hand, EndoSurf produces better appearance quality than EndoNeRF by ↑ 0.571 PSNR, ↑ 0.025 SSIM, and ↑ 0.020 LPIPS. On the other hand, EndoSurf dramatically outperforms EndoNeRF in terms of geometry recovery by ↓ 0.190 RMSE and ↓ 1.625 PCD. Note that both methods perform better on ENDON-ERF than on SCARED. This is because ENDONERF fixes the camera pose, leading to easier network fitting. Figure <ref type="figure">3</ref> shows the 2D rendering results. While both methods synthesize highfidelity RGB images, only EndoSurf succeeds in recovering depth maps with a smoother shape, more details, and fewer artifacts. First, the geometry constraints in EndoSurf prevent the network from overfitting depth supervision, suppressing rough surfaces as observed in the EndoNeRF's normal maps. Second, the brutal post-processing filtering in EndoNeRF cannot preserve sharp details (white boxes in Fig. <ref type="figure">3</ref>). Moreover, the texture and shape of EndoNeRF are not disentangled, causing depth artifacts in some color change areas (white rings in Fig. <ref type="figure">3</ref>).</p><p>Figure <ref type="figure" target="#fig_3">4</ref> depicts the shapes of extracted 3D meshes. Surfaces reconstructed by EndoSurf are accurate and smooth, while those from EndoNeRF are quite noisy. There are two reasons for the poor quality of EndoNeRF's meshes. First, the density field without regularization tends to describe the scene as a volumetric fog rather than a solid surface. Second, the traditional volume rendering causes discernible depth bias <ref type="bibr">[25]</ref>. In contrast, we force the neural SDF field to conform to its definition via multiple geometry constraints. Furthermore, we use unbiased volume rendering to prevent depth ambiguity <ref type="bibr">[25]</ref>.</p><p>We present a qualitative ablation study on how geometry constraints can influence the reconstruction quality in Fig. <ref type="figure" target="#fig_4">5</ref>. The Eikonal loss L eikonal and SDF loss L sdf play important roles in improving geometry recovery, while the visibility loss L visible and smoothness loss L smooth help refine the surface. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents a novel neural-field-based approach, called EndoSurf, to reconstruct the deforming surgical sites from stereo endoscope videos. Our approach overcomes the geometry limitations of prior work by utilizing a neural SDF field to represent the shape, which is constrained by customized regularization techniques. In addition, we employ neural deformation and radiance fields to model surface dynamics and appearance. To disentangle the appearance learning from geometry, we incorporate normals and locations as extra clues for the radiance field. Experiments on public datasets demonstrate that our method achieves state-of-the-art results compared with existing solutions, particularly in retrieving high-fidelity shapes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The overall pipeline of EndoSurf. (b) Density field v.s. SDF field. Red lines represent surfaces. The density field is depth ambiguous, while the SDF field clearly defines the surface as the zero-level set. (Color figure online)</figDesc><graphic coords="3,58,53,61,67,256,21,92,38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3 that transforms x o to its canonical position x c = x o + Δx. The canonical view direction v c ∈ R 3 of point x c can be obtained by transforming the raw view direction v o with the Jacobian of the deformation field J Ψ d (x o ) = ∂Ψ d /∂x o , i.e., v c = (I + J Ψ d (x o ))v o .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Extracted meshes on one dynamic scene and three static scenes. Our method produces accurate and smooth surfaces.</figDesc><graphic coords="8,43,29,54,29,337,12,147,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Ablation study on four geometry constraints, i.e., visibility loss L visible , Eikonal loss L eikonal , SDF loss L sdf , and smoothness loss L smooth .</figDesc><graphic coords="9,59,46,54,14,333,40,71,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,57,96,206,54,336,28,162,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative metrics of appearance (PSNR/SSIM/LPIPS) and geometry (RMSE/PCD) on two datasets. The unit for RMSE/PCD is millimeter.</figDesc><table><row><cell>Methods</cell><cell>EndoNeRF [26]</cell><cell></cell><cell>EndoSurf (Ours)</cell><cell></cell></row><row><cell>Metrics</cell><cell cols="4">PSNR↑ SSIM↑ LPIPS↓ RMSE↓ PCD↓ PSNR↑ SSIM↑ LPIPS↓ RMSE↓ PCD↓</cell></row><row><cell cols="2">ENDONERF-cutting 34.186 0.932 0.151</cell><cell>0.930</cell><cell>1.030 34.981 0.953 0.106</cell><cell>0.835 0.559</cell></row><row><cell cols="2">ENDONERF-pulling 34.212 0.938 0.161</cell><cell>1.485</cell><cell>2.260 35.004 0.956 0.120</cell><cell>1.165 0.841</cell></row><row><cell>SCARED-d1k1</cell><cell>24.365 0.763 0.326</cell><cell>0.697</cell><cell>2.982 24.395 0.769 0.319</cell><cell>0.522 0.741</cell></row><row><cell>SCARED-d2k1</cell><cell>25.733 0.828 0.240</cell><cell>0.583</cell><cell>1.788 26.237 0.829 0.254</cell><cell>0.352 0.515</cell></row><row><cell>SCARED-d3k1</cell><cell>19.004 0.599 0.467</cell><cell>1.809</cell><cell>3.244 20.041 0.649 0.441</cell><cell>1.576 1.091</cell></row><row><cell>SCARED-d6k1</cell><cell>24.041 0.833 0.464</cell><cell>1.194</cell><cell>3.268 24.094 0.866 0.461</cell><cell>1.065 1.331</cell></row><row><cell>SCARED-d7k1</cell><cell>22.637 0.813 0.312</cell><cell>2.272</cell><cell>3.465 23.421 0.861 0.282</cell><cell>2.123 1.589</cell></row><row><cell>Average</cell><cell>26.311 0.815 0.303</cell><cell>1.281</cell><cell>2.577 26.882 0.840 0.283</cell><cell>1.091 0.952</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research is funded in part via an <rs type="funder">ARC</rs> <rs type="grantName">Discovery project research grant</rs> (<rs type="grantNumber">DP220100800</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WjkpBDB">
					<idno type="grant-number">DP220100800</idno>
					<orgName type="grant-name">Discovery project research grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 2.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01133</idno>
		<title level="m">Stereo correspondence and reconstruction of endoscopic data challenge</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SAL: sign agnostic learning of shapes from raw data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2565" to="2574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepDeform: learning non-rigid rgb-d reconstruction with semi-supervised data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bozic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7002" to="7012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural surface reconstruction of dynamic scenes with monocular RGB-D camera</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.15258</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical neural architecture search for deep stereo matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22158" to="22169" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep laparoscopic stereo matching with transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="464" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-144" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Zha</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Virtual reality application for laparoscope in clinical surgery based on Siamese network and census transformation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-16-3880-0_7</idno>
		<ptr target="https://doi.org/10.1007/978-981-16-3880-07" />
	</analytic>
	<monogr>
		<title level="m">MICAD 2021</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Su</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y.-D</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">784</biblScope>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Viscosity solutions of Hamilton-Jacobi equations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Lions</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Am. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">SurfelWarp: efficient non-volumetric single view dynamic reconstruction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.13073</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Implicit geometric regularization for learning shapes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yariv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10099</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SuPer: a surgical perception framework for endoscopic tissue manipulation with surgical robotics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2294" to="2301" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6197" to="6206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">E-DSSR: efficient dynamic surgical scene reconstruction with transformer-based stereoscopic depth perception</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-140" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part IV</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="415" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Marching cubes: a high resolution 3D surface construction algorithm</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NeRF: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-824" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020, Part I</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DynamicFusion: reconstruction and tracking of non-rigid scenes in real-time</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="343" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Augmented reality in laparoscopic surgical oncology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nicolau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surg. Oncol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="201" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Differentiable volumetric rendering: learning implicit 3D representations without 3D supervision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3504" to="3515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UNISURF: unifying neural implicit surfaces and radiance fields for multi-view reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5589" to="5599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Navigation and robotics in spinal surgery: where are we now?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Overley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurosurgery</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3S</biblScope>
			<biblScope unit="page" from="86" to="S99" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">D-NeRF: neural radiance fields for dynamic scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10318" to="10327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic reconstruction of deformable soft-tissue with stereo scope in minimal invasive surgery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dissanayake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">NeuS: learning neural implicit surfaces by volume rendering for multi-view reconstruction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10689</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural rendering for stereo 3D reconstruction of deformable tissues in robotic surgery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="431" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-141" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiview neural surface reconstruction by disentangling geometry and appearance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yariv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2492" to="2502" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">EMDQ-SLAM: real-time high-resolution reconstruction of soft tissue surface from stereo laparoscopy videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jayender</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-132" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part IV</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Essert</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
