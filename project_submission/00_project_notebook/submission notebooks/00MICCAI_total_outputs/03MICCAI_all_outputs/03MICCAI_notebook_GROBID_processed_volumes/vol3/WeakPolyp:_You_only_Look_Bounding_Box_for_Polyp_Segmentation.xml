<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WeakPolyp: You only Look Bounding Box for Polyp Segmentation</title>
				<funder ref="#_479JmJB">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_64vsufc">
					<orgName type="full">Shenzhen-Hong Kong</orgName>
				</funder>
				<funder>
					<orgName type="full">Guangdong Provincial Key Laboratory of Big Data Computing</orgName>
				</funder>
				<funder ref="#_atDGFTT #_CPxvEVe">
					<orgName type="full">Shenzhen Outstanding Talents Training Fund</orgName>
				</funder>
				<funder ref="#_W87syDz">
					<orgName type="full">Shenzhen General</orgName>
				</funder>
				<funder>
					<orgName type="full">Tencent Open Fund</orgName>
				</funder>
				<funder ref="#_Bv3VmXK">
					<orgName type="full">Guangdong Provincial Key Laboratory of Future Networks of Intelligence</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Wei</surname></persName>
							<email>junwei@link.cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiwen</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">South China Hospital</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Kevin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Biomedical Engineering and Suzhou Institute for Advanced Research</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WeakPolyp: You only Look Bounding Box for Polyp Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="757" to="766"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">4150D48E0E387768914DE727CA0E03B0</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_72</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Polyp segmentation</term>
					<term>Weak Supervision</term>
					<term>Colorectal cancer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Limited by expensive pixel-level labels, polyp segmentation models are plagued by data shortage and suffer from impaired generalization. In contrast, polyp bounding box annotations are much cheaper and more accessible. Thus, to reduce labeling cost, we propose to learn a weakly supervised polyp segmentation model (i.e.,WeakPolyp) completely based on bounding box annotations. However, coarse bounding boxes contain too much noise. To avoid interference, we introduce the mask-to-box (M2B) transformation. By supervising the outer box mask of the prediction instead of the prediction itself, M2B greatly mitigates the mismatch between the coarse label and the precise prediction. But, M2B only provides sparse supervision, leading to non-unique predictions. Therefore, we further propose a scale consistency (SC) loss for dense supervision. By explicitly aligning predictions across the same image at different scales, the SC loss largely reduces the variation of predictions. Note that our WeakPolyp is a plug-and-play model, which can be easily ported to other appealing backbones. Besides, the proposed modules are only used during training, bringing no computation cost to inference. Extensive experiments demonstrate the effectiveness of our proposed WeakPolyp, which surprisingly achieves a comparable performance with a fully supervised model, requiring no mask annotations at all. Codes are available at https://github.com/weijun88/WeakPolyp.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colorectal Cancer (CRC) has become a major threat to health worldwide. Since most CRCs originate from colorectal polyps, early screening for polyps is necessary. Given its significance, automatic polyp segmentation models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18]</ref>  have been designed to aid in screening. For example, ACSNet <ref type="bibr" target="#b21">[21]</ref>, HRENet <ref type="bibr" target="#b14">[14]</ref>, LDNet <ref type="bibr" target="#b20">[20]</ref> and CCBANet <ref type="bibr" target="#b11">[11]</ref> propose to use convolutional neural networks to extract multi-scale contexts for robust predictions. LODNet <ref type="bibr" target="#b1">[2]</ref>, PraNet <ref type="bibr" target="#b4">[5]</ref>, and MSNet <ref type="bibr" target="#b23">[23]</ref> aim to improve the model's discrimination of polyp boundaries. SANet <ref type="bibr" target="#b19">[19]</ref> eliminates the distribution gap between the training set and the testing set, thus improving the model generalization. Recently, TGANet <ref type="bibr" target="#b15">[15]</ref> introduces text embeddings to enhance the model's discrimination. Furthermore, Transfuse <ref type="bibr" target="#b22">[22]</ref>, PPFormer <ref type="bibr" target="#b0">[1]</ref>, and Polyp-Pvt <ref type="bibr" target="#b2">[3]</ref> introduce the Transformer <ref type="bibr" target="#b3">[4]</ref> backbone to extract global contexts, achieving a significant performance gain.</p><p>All above models are fully supervised and require pixel-level annotations. However, pixel-by-pixel labeling is time-consuming and expensive, which hampers practical clinical usage. Besides, many polyps do not have welldefined boundaries. Pixel-level labeling inevitably introduces subjective noise. To address the above limitations, a generalized polyp segmentation model is urgently needed. In this paper, we achieve this goal by a weakly supervised polyp segmentation model (named WeakPolyp) that only uses coarse bounding box annotations. Figure <ref type="figure" target="#fig_0">1</ref>(a) shows the differences between our WeakPolyp and fully supervised models. Compared with fully supervised ones, WeakPolyp requires only a bounding box for each polyp, thus dramatically reducing the labeling cost. More meaningfully, WeakPolyp can take existing large-scale polyp detection datasets to assist the polyp segmentation task. Finally, WeakPolyp does not require the labeling for polyp boundaries, avoiding the subjective noise at source. All these advantages make WeakPolyp more clinically practical.</p><p>However, bounding box annotations are much coarser than pixel-level ones, which can not describe the shape of polyps. Simply adopting these box annotations as supervision introduces too much background noise, thereby leading to suboptimal models. As a solution, BoxPolyp <ref type="bibr" target="#b18">[18]</ref> only supervises the pixels with high certainty. However, it requires a fully supervised model to predict the uncertainty map. Unlike BoxPolyp, our WeakPolyp completely follows the weakly supervised form that requires no additional models or annotations. Surprisingly, just by redesigning the supervision loss without any changes to the model structure, WeakPolyp achieves comparable performance to its fully supervised counterpart. Figure <ref type="figure" target="#fig_0">1</ref>(b) visualizes some predicted results by WeakPolyp.</p><p>WeakPolyp is mainly enabled by two novel components: mask-to-box (M2B) transformation and scale consistency (SC) loss. In practice, M2B is applied to transform the predicted mask into a box-like mask by projection and backprojection. Then, this transformed mask is supervised by the bounding box annotation. This indirect supervision avoids the misleading of box-shape bias of annotations. However, many regions in the predicted mask are lost in the projection and therefore get no supervision. To fully explore these regions, we propose the SC loss to provide a pixel-level self-supervision while requiring no annotations at all. Specifically, the SC loss explicitly reduces the distance between predictions of the same image at different scales. By forcing feature alignment, it inhibits the excessive diversity of predictions, thus improving the model generalization.</p><p>In summary, our contributions are three-fold: <ref type="bibr" target="#b0">(1)</ref> We build the WeakPolyp model completely based on bounding box annotations, which largely reduces the labeling cost and achieves a comparable performance to full supervision. <ref type="bibr" target="#b1">(2)</ref> We propose the M2B transformation to mitigate the mismatch between the prediction and the supervision, and design the SC loss to improve the robustness of the model against the variability of the predictions. (3) Our proposed WeakPolyp is a plug-and-play option, which can boost the performances of polyp segmentation models under different backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Model Components. Fig. <ref type="figure">2</ref> depicts the components of WeakPolyp, including the segmentation phase and the supervision phase. For the segmentation phase, we adopt Res2Net <ref type="bibr" target="#b6">[6]</ref> as the backbone. For input image I ∈ R H×W , Res2Net extracts four scales of features</p><formula xml:id="formula_0">{f i |i = 1, ..., 4} with the resolutions [ H 2 i+1 , W 2 i+1 ].</formula><p>Considering the computational cost, only f 2 , f 3 and f 4 are utilized. To fuse them, we first apply a 1 × 1 convolutional layer to unify the channels of f 2 , f 3 , f 4 and then use the bilinear upsampling to unify their resolutions. After being transformed to the same size, f 2 , f 3 , f 4 are added together and fed into one 1 × 1 convolutional layer for final prediction. Instead of the segmentation phase, our contributions primarily lie in the supervision phase, including mask-to-box (M2B) transformation and scale consistency (SC) loss. Notably, both M2B and SC are independent of the specific model structure.</p><p>Model Pipeline. For each input image I, we first resize it into two different scales: I 1 ∈ R s1×s1 and I 2 ∈ R s2×s2 . Then, I 1 and I 2 are sent to the segmentation model and get two predicted masks P 1 and P 2 , both of which have been resized to the same size. Next, an SC loss is proposed to reduce the distance between P 1 and P 2 , which helps suppress the variation of the prediction. Finally, to fit the bounding box annotations (B), P 1 and P 2 are sent to M2B and converted into box-like masks T 1 and T 2 . With T 1 /T 2 and B, we calculate the binary cross entropy (BCE) loss and Dice loss, without worrying about noise interference. Fig. <ref type="figure">2</ref>. The framework of our proposed WeakPolyp model, which consists of the segmentation phase and the supervision phase. The segmentation phase predicts the polyp mask for each input firstly, and the supervision phase uses the coarse box annotation to guide previous predicted mask. Note that our contributions mainly lie in the supervision phase, where the proposed M2B transformation converts the predicted mask into a box mask to accommodate the bounding box annotation. Besides, another proposed SC loss is introduced to provide dense supervision from multi-scales, which improves the consistency of predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mask-to-Box (M2B) Transformation</head><p>One naive method to achieve the weakly supervised polyp segmentation is to use the bounding box annotation B to supervise the predicted mask P 1 /P 2 . Unfortunately, models trained in this way show poor generalization. Because there is a strong box-shape bias in B. Training with this bias, the model is forced to predict the box-shape mask, unable to maintain the polyp's contours. To solve this, we innovatively use B to supervise the bounding box mask (i.e.,T 1 /T 2 ) of P 1 /P 2 , rather than P 1 /P 2 itself. This indirect supervision separates P 1 /P 2 from B so that P 1 /P 2 is not affected by the shape bias of B while obtaining the position and extent of polyps. But how to implement the transformation from P 1 /P 2 to T 1 /T 2 ? We design the M2B module, which consists of two steps: projection and back-projection, as shown in Fig. <ref type="figure">2</ref>.</p><p>Projection. As shown in Eq. 1, given a predicted mask P ∈ [0, 1] H×W , we project it horizontally and vertically into two vectors P w ∈ [0, 1] 1×W and P h ∈ [0, 1] H×1 . In this projection, instead of using mean pooling, we use max pooling to pick the maximum value for each row/column in P . Because max pooling can completely remove the shape information of the polyp. After projection, only the position and scope of the polyp are stored in P w and P h . Back-projection. Based on P w and P h , we construct the bounding box mask of the polyp by back-projection. As shown in Eq. 2, P w and P h are first repeated into P w and P h with the same size as P . Then, we element-wisely take the minimum of P w and P h to achieve the bounding box mask T . As shown in Fig. <ref type="figure">2</ref>, T no longer contains the contours of the polyp.</p><formula xml:id="formula_1">P w = max(P, axis = 0) ∈ [0, 1] 1×W , P h = max(P, axis = 1) ∈ [0, 1] H×1 (1)</formula><formula xml:id="formula_2">P w = repeat(P w , H, axis = 0) ∈ [0, 1] H×W P h = repeat(P h , W, axis = 1) ∈ [0, 1] H×W T = min(P w , P h ) ∈ [0, 1] H×W (2)</formula><p>Supervision. By M2B, P 1 and P 2 are transformed into T 1 and T 2 , respectively. Because both T 1 /T 2 and B are box-like masks, we directly calculate the supervision loss between them without worrying about the misguidance of box-shape bias. Specifically, we follow <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">19]</ref> to adopt BCE loss L BCE and Dice loss L Dice for model supervision, as shown in Eq. 3.</p><formula xml:id="formula_3">L Sum = L BCE (T 1 , B) + L BCE (T 2 , B) 2 + L Dice (T 1 , B) + L Dice (T 2 , B) 2<label>(3)</label></formula><p>Priority. By simple transformation, M2B turns the noisy supervision into a noise-free one, so that the predicted mask is able to preserve the contours of the polyp. Notably, M2B is differentiable, which can be easily implemented with PyTorch and plugged into the model to participate in gradient backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scale Consistency (SC) Loss</head><p>In M2B, most pixels in P are ignored in the projection, thus only a few pixels with high response values are involved in the supervision loss. This sparse supervision may lead to non-unique predictions. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, after M2B projection, five predicted masks with different response values can be transformed into the same bounding box mask. Therefore, we consider introducing the SC loss to achieve dense supervision without annotations, which reduces the degree of freedom of predictions.</p><p>Method. As shown in Fig. <ref type="figure">2</ref>, due to the non-uniqueness of the prediction and the scale difference between I 1 and I 2 , P 1 and P 2 differ in response values. But come from the same image I 1 . They should be exactly the same. Given this, as shown in Eq. 4, we build the dense supervision L SC by explicitly reducing the distance between P 1 and P 2 , where (i, j) is the pixel coordinates. Note that only pixels inside bounding box are involved in L SC to emphasize more on polyp regions. Despite its simplicity, L SC brings pixel-level constraints to compensate for the sparsity of L Sum , thus reducing the variety of predictions.</p><formula xml:id="formula_4">L SC = (i,j)∈box |P i,j 1 -P i,j 2 | (i,j)∈box 1<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Total Loss</head><p>As shown in Eq. 5, combining L Sum and L SC together, we get WeakPolyp model. Note that WeakPolyp simply replaces the supervision loss without making any changes to the model structure. Therefore, it is general and can be ported to other models. Besides, L Sum and L SC are only used during training. In inference, they will be removed, thus having no effect on the speed of the model.</p><formula xml:id="formula_5">L T otal = L Sum + L SC (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. Two large polyp datasets are adopted to evaluate the model performance, including SUN-SEG <ref type="bibr" target="#b9">[9]</ref> and POLYP-SEG. SUN-SEG originates   Compared with Fully Supervised Methods. Table . 3 shows our WeakPolyp is even superior to many previous fully supervised methods: PraNet <ref type="bibr" target="#b4">[5]</ref>, SANet <ref type="bibr" target="#b19">[19]</ref>, 2/3D <ref type="bibr" target="#b12">[12]</ref> and PNS+ <ref type="bibr" target="#b9">[9]</ref>, which shows the excellent application prospect of weakly supervised learning in the polyp field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Limited by expensive labeling cost, pixel-level annotations are not readily available, which hinders the development of the polyp segmentation field. In this paper, we propose the WeakPolyp model completely based on bounding box annotations. WeakPolyp requires no pixel-level annotations, thus avoiding the interference of subjective noise labels. More importantly, WeakPolyp even achieves a comparable performance to the fully supervised models, showing the great potential of weakly supervised learning in the polyp segmentation field.</p><p>In future, we will introduce temporal information into weakly supervised polyp segmentation to further reduce the model's dependence on labeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Comparison between the fully supervised model and our proposed WeakPolyp using box mask only. (b) Visualization of prediction from WeakPolyp.</figDesc><graphic coords="2,51,30,54,47,322,00,115,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Different predictions may correspond to the same bounding box mask.</figDesc><graphic coords="5,57,96,53,84,336,61,59,86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization comparison between predictions based on different supervisions.</figDesc><graphic coords="7,57,96,53,87,336,28,142,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,44,31,54,47,335,56,179,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison between different baselines and our WeakPolyp, involving two datasets (SUN-SEG and POLYP-SEG) and two backbones (Res2Net-50<ref type="bibr" target="#b6">[6]</ref> and PVTv2-B2<ref type="bibr" target="#b17">[17]</ref>). The gt row is the performance upper bound. The box row is the performance lower bound. 'Bac.' means backbone. 'Sup.' means supervision. The highest and second-highest scores are marked in red and blue, respectively</figDesc><table><row><cell></cell><cell></cell><cell>SUN-SEG</cell><cell cols="2">POLYP-SEG</cell></row><row><cell cols="2">Bac. Sup.</cell><cell>Easy Testing Hard Testing Training</cell><cell>Testing</cell><cell>Training</cell></row><row><cell></cell><cell></cell><cell cols="3">Dice IoU Dice IoU Dice IoU Dice IoU Dice IoU</cell></row><row><cell></cell><cell>gt</cell><cell cols="3">.772 .693 .798 .716 .931 .876 .761 .684 .936 .884</cell></row><row><cell>Res.</cell><cell cols="4">grabcut .595 .514 .617 .530 .706 .608 .660 .579 .778 .687 box .715 .601 .718 .599 .806 .685 .686 .566 .804 .683</cell></row><row><cell></cell><cell cols="4">Ours .792 .715 .807 .727 .899 .826 .760 .680 .909 .842</cell></row><row><cell></cell><cell>gt</cell><cell cols="3">.851 .780 .858 .784 .932 .878 .793 .715 .936 .883</cell></row><row><cell>Pvt.</cell><cell cols="4">grabcut .741 .648 .747 .649 .766 .670 .644 .559 .780 .683 box .769 .652 .770 .648 .804 .681 .734 .611 .824 .705</cell></row><row><cell></cell><cell cols="4">Ours .853 .781 .854 .777 .907 .839 .792 .707 .922 .859</cell></row><row><cell cols="2">P 1 and P 2</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies on the SUN-SEG testing set under different backbones. WeakPolyp is implemented using PyTorch. All input images are uniformly resized to 352×352. For data augmentation, random flip, random rotation, and multi-scale training are adopted. The whole network is trained in an end-to-end way with an AdamW optimizer. Initial learning rate and batch size are set to 1e-4 and 16, respectively. We train the entire model for 16 epochs.Quantitative Comparison. Table.1 compares the model performance under different supervisions, backbones, and datasets. The overall performance order is gt&gt;WeakPolyp&gt;box&gt;grabcut. The model supervised by grabcut<ref type="bibr" target="#b13">[13]</ref> masks performs the worst, because the foreground and background of polyp images are similar. Grabcut can not well distinguish between them, resulting in poor masks. Our WeakPolyp predictably outperforms the model supervised by box masks because it is not affected by the box-shape bias of the annotations.</figDesc><table><row><cell>Modules</cell><cell>Res2Net-50</cell><cell></cell><cell>PVTv2-B2</cell><cell></cell></row><row><cell></cell><cell cols="4">Easy Testing Hard Testing Easy Testing Hard Testing</cell></row><row><cell></cell><cell>Dice IoU</cell><cell>Dice IoU</cell><cell>Dice IoU</cell><cell>Dice IoU</cell></row><row><cell>Base</cell><cell>.715 .601</cell><cell>.718 .599</cell><cell>.769 .652</cell><cell>.770 .648</cell></row><row><cell>Base+M2B</cell><cell>.748 .654</cell><cell>.768 .673</cell><cell>.822 .738</cell><cell>.822 .735</cell></row><row><cell cols="2">Base+M2B+SC .792 .715</cell><cell>.807 .727</cell><cell>.853 .781</cell><cell>.854 .777</cell></row><row><cell cols="5">from [7,10], which consists of 19,544 training images, 17,070 easy tesing images,</cell></row><row><cell cols="5">and 12,522 hard testing images. POLYP-SEG is our private polyp segmenta-</cell></row><row><cell cols="5">tion dataset, which contains 15,916 training images and 4,040 testing images.</cell></row><row><cell cols="5">Note that, during training, only bounding box annotations are adopted in our</cell></row><row><cell>WeakPolyp.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training Settings.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Interestingly, WeakPolyp even surpasses the fully supervised model on SUN-SEG, which indicates that there is a lot of noise in the pixel-level annota-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison with previous fully supervised models on SUN-SEG.Visual Comparison. Fig.4visualizes some predictions based on different supervisions. Compared with other counterparts, WeakPolyp not only highlights the polyp shapes but also suppresses the background noise. Even for challenging scenarios, WeakPolyp still handles well and generates accurate masks.Ablation Study. To investigate the importance of each component in WeakPolyp, we evaluate the model on both Res2Net-50 and PVTv2-B2 for ablation studies. As shown in Table2, all proposed modules are beneficial for the final predictions. Combining all these modules, our model achieves the highest performance.</figDesc><table><row><cell>Model</cell><cell cols="2">Conference Backbone</cell><cell>Easy Testing Hard Testing</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dice IoU</cell><cell>Dice IoU</cell></row><row><cell cols="4">PraNet [5] MICCAI 2020 Res2Net-50 .689 .608</cell><cell>.660 .569</cell></row><row><cell cols="4">2/3D [12] MICCAI 2020 ResNet-101 .755 .668</cell><cell>.737 .643</cell></row><row><cell cols="4">SANet [19] MICCAI 2021 Res2Net-50 .693 .595</cell><cell>.640 .543</cell></row><row><cell cols="2">PNS+ [9] MIR 2022</cell><cell cols="2">Res2Net-50 .787 .704</cell><cell>.770 .679</cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">Res2Net-50 .792 .715</cell><cell>.807 .727</cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">PVTv2-B2 .853 .781</cell><cell>.854 .777</cell></row><row><cell cols="4">tions. But WeakPolyp does not require pixel-level annotations so it avoids noise</cell></row><row><cell>interference.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J.Wei  and Y. Hu-Equal contributions. c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 H. Greenspan et al. (Eds.): MICCAI 2023, LNCS 14222, pp. 757-766, 2023. https://doi.org/10.1007/978-3-031-43898-1_72</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by <rs type="funder">Shenzhen General</rs> Program No.<rs type="grantNumber">JCYJ20220530143600001</rs>, by the <rs type="programName">Basic Research Project No. HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S&amp;T Cooperation Zone</rs>, by <rs type="funder">Shenzhen-Hong Kong</rs> Joint Funding No. <rs type="grantNumber">SGDX20211123112401002</rs>, by <rs type="funder">Shenzhen Outstanding Talents Training Fund</rs>, by Guangdong Research Project No. <rs type="grantNumber">2017ZT07X152</rs> and No. <rs type="grantNumber">2019CX01X104</rs>, by the <rs type="funder">Guangdong Provincial Key Laboratory of Future Networks of Intelligence</rs> (Grant No. <rs type="grantNumber">2022B1212010001</rs>), by the <rs type="funder">Guangdong Provincial Key Laboratory of Big Data Computing</rs>, <rs type="affiliation">The Chinese University of Hong Kong, Shenzhen</rs>, by the <rs type="funder">NSFC</rs> <rs type="grantNumber">61931024&amp;81922046</rs>, by zelixir biotechnology company Fund, by <rs type="funder">Tencent Open Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_W87syDz">
					<idno type="grant-number">JCYJ20220530143600001</idno>
					<orgName type="program" subtype="full">Basic Research Project No. HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S&amp;T Cooperation Zone</orgName>
				</org>
				<org type="funding" xml:id="_64vsufc">
					<idno type="grant-number">SGDX20211123112401002</idno>
				</org>
				<org type="funding" xml:id="_atDGFTT">
					<idno type="grant-number">2017ZT07X152</idno>
				</org>
				<org type="funding" xml:id="_CPxvEVe">
					<idno type="grant-number">2019CX01X104</idno>
				</org>
				<org type="funding" xml:id="_Bv3VmXK">
					<idno type="grant-number">2022B1212010001</idno>
				</org>
				<org type="funding" xml:id="_479JmJB">
					<idno type="grant-number">61931024&amp;81922046</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using guided self-attention with local information for polyp segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="629" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learnable Oriented-Derivative Network for Polyp Segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_68</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_68" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06932</idno>
		<title level="m">Polyp-PVT: polyp segmentation with pyramid vision transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PraNet: Parallel Reverse Attention Network for Polyp Segmentation</title>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_26" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<ptr target="http://amed8k.sundatabase.org/" />
		<title level="m">Sun colonoscopy video database</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Progressively normalized self-attention network for video polyp segmentation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video polyp segmentation: a deep learning perspective</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11633-022-1371-y</idno>
		<ptr target="https://doi.org/10.1007/s11633-022-1371-y" />
	</analytic>
	<monogr>
		<title level="j">Mach. Intell. Res</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Development of a computer-aided detection system for colonoscopy and a publicly accessible large colonoscopy video database (with video)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Misawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="960" to="967" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CCBANet: Cascading Context and Balancing Attention for Polyp Segmentation</title>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Diep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-H</forename><surname>Tran-Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_60" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="633" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Endoscopic polyp segmentation using a hybrid 2D/3D CNN</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G B</forename><surname>Puyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GrabCut interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HRENet: A Hard Region Enhancement Network for Polyp Segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_53" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TGANet: text-guided attention for improved polyp segmentation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bagci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_15" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stepwise feature fusion: local guides global</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_11" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PVT v2: improved baselines with pyramid vision transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Visual Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BoxPolyp: boost generalized polyp segmentation using extra coarse bounding box annotations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="67" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shallow Attention Network for Polyp Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_66" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="699" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lesion-aware dynamic kernel for polyp segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_10" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="99" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive Context Selection for Polyp Segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_25" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfuse: fusing transformers and CNNs for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic Polyp Segmentation via Multi-scale Subtraction Network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_12" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
