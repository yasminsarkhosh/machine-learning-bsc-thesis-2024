<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images</title>
				<funder>
					<orgName type="full">Helmholtz Association</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Science, Research and the Arts Baden-Württemberg</orgName>
				</funder>
				<funder>
					<orgName type="full">Federal Ministry of Education and Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zdravko</forename><surname>Marinov</surname></persName>
							<email>zdravko.marinov@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">HIDSS4Health -Helmholtz Information and Data Science School for Health</orgName>
								<address>
									<settlement>Karlsruhe, Heidelberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
							<email>rainer.stiefelhagen@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Karlsruhe</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jens</forename><surname>Kleesiek</surname></persName>
							<email>jens.kleesiek@uk-essen.de</email>
							<affiliation key="aff2">
								<orgName type="department">Institute for AI in Medicine</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Cancer Research Center Cologne Essen (CCCE)</orgName>
								<orgName type="institution">University Medicine Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="637" to="647"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A1E9D3908467C42C03E1C595CCED3816</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Interactive Segmentation</term>
					<term>Comparative Study</term>
					<term>Click Guidance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interactive segmentation reduces the annotation time of medical images and allows annotators to iteratively refine labels with corrective interactions, such as clicks. While existing interactive models transform clicks into user guidance signals, which are combined with images to form (image, guidance) pairs, the question of how to best represent the guidance has not been fully explored. To address this, we conduct a comparative study of existing guidance signals by training interactive models with different signals and parameter settings to identify crucial parameters for the model's design. Based on our findings, we design a guidance signal that retains the benefits of other signals while addressing their limitations. We propose an adaptive Gaussian heatmap guidance signal that utilizes the geodesic distance transform to dynamically adapt the radius of each heatmap when encoding clicks. We conduct our study on the MSD Spleen and the AutoPET datasets to explore the segmentation of both anatomy (spleen) and pathology (tumor lesions). Our results show that choosing the guidance signal is crucial for interactive segmentation as we improve the performance by 14% Dice with our adaptive heatmaps on the challenging AutoPET dataset when compared to non-interactive models. This brings interactive models one step closer to deployment in clinical workflows. Code: https://github.com/ Zrrr1997/Guiding-The-Guidance/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning models have achieved remarkable success in segmenting anatomy and lesions from medical images but often rely on large-scale manually annotated datasets <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. This is challenging when working with volumetric medical data as voxelwise labeling requires a lot of time and expertise. Interactive segmentation models address this issue by utilizing weak labels, such as clicks, instead of voxelwise annotations <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. The clicks are transformed into guidance signals, e.g., Gaussian heatmaps or Euclidean/Geodesic distance maps, and used together with the image as a joint input for the interactive model. Annotators can make additional clicks in missegmented areas to iteratively refine the segmentation mask, which often significantly improves the prediction compared to non-interactive models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>. However, prior research on choosing guidance signals for interactive models is limited to small ablation studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. There is also no systematic framework for comparing guidance signals, which includes not only accuracy but also efficiency and the ability to iteratively improve predictions with new clicks, which are all important aspects of interactive models <ref type="bibr" target="#b6">[7]</ref>. We address these challenges with the following contributions:</p><p>1. We compare 5 existing guidance signals on the AutoPET <ref type="bibr" target="#b0">[1]</ref> and MSD Spleen <ref type="bibr" target="#b1">[2]</ref> datasets and vary various hyperparameters. We show which parameters are essential to tune for each guidance and suggest default values. 2. We introduce 5 guidance evaluation metrics (M1)-(M5), which evaluate the performance, efficiency, and ability to improve with new clicks. This provides a systematic framework for comparing guidance signals in future research. 3. Based on our insights from 1., we propose novel adaptive Gaussian heatmaps, which use geodesic distance values around each click to set the radius of each heatmap. Our adaptive heatmaps mitigate the weaknesses of the 5 guidances and achieve the best performance on AutoPET <ref type="bibr" target="#b0">[1]</ref> and MSD Spleen <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work.</head><p>Previous work comparing guidance signals has mostly been limited to small ablation studies. Sofiiuk et al. <ref type="bibr" target="#b8">[9]</ref> and Benenson et al. <ref type="bibr" target="#b7">[8]</ref> both compare Euclidean distance maps with solid disks and find that disks perform better. However, neither of them explore different parameter settings for each guidance and both work with natural 2D images. Dupont et al. <ref type="bibr" target="#b11">[12]</ref> note that a comprehensive comparison of existing guidance signals would be helpful in designing interactive models. The closest work to ours is MIDeepSeg <ref type="bibr" target="#b4">[5]</ref>, which proposes a user guidance based on exponentialized geodesic distances and compare it to existing guidance signals. However, they use only initial clicks and do not add iterative corrective clicks to refine the segmentation. In contrast to previous work, our research evaluates the influence of hyperparameters for guidance signals and assesses the guidances' efficiency and ability to improve with new clicks, in addition to accuracy. While some previous works <ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref> propose using a larger radius for the first click's heatmap, our adaptive heatmaps offer a greater flexibility by adjusting the radius at each new click dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Guidance Signals</head><p>We define the five guidance signals over a set of N clicks C = {c 1 , c 2 , ..., c N } where c i = (x i , y i , z i ) is the i th click. As disks and heatmaps can be computed independently for each click, they are defined for a single click c i over 3D voxels v = (x, y, z) in the volume. The disk signal fills spheres with a radius σ centered around each click c i , which is represented by the equation in Eq. <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">disk(v, c i , σ) = 1, if ||v -c i || 2 ≤ σ 0, otherwise<label>(1)</label></formula><p>The Gaussian heatmap applies Gaussian filters centered around each click to create softer edges with an exponential decrease away from the click (Eq. ( <ref type="formula" target="#formula_1">2</ref>)).</p><formula xml:id="formula_1">heatmap(v, c i , σ) = exp(- ||v -c i || 2 2σ 2 )<label>(2)</label></formula><p>The Euclidean distance transform (EDT) is defined in Eq. ( <ref type="formula" target="#formula_2">3</ref>) as the minimum Euclidean distance between a voxel v and the set of clicks C. It is similar to the disk signal in Eq. ( <ref type="formula" target="#formula_0">1</ref>), but instead of filling the sphere with a constant value it computes the distance of each voxel to the closest click point.</p><formula xml:id="formula_2">EDT(v, C) = min ci∈C ||v -c i || 2<label>(3)</label></formula><p>The Geodesic distance transform (GDT) is defined in Eq. (4) as the shortest path distance between each voxel in the volume and the closest click in the set C <ref type="bibr" target="#b13">[14]</ref>. The shortest path in GDT also takes into account intensity differences between voxels along the path. We use the method of Asad et al. <ref type="bibr" target="#b9">[10]</ref> to compute the shortest path which is denoted as Φ in Eq. <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_3">GDT(v, C) = min ci∈C Φ(v, c i )<label>(4)</label></formula><p>We also examine the exponentialized Geodesic distance (exp-GDT) proposed in MIDeepSeg <ref type="bibr" target="#b4">[5]</ref> that is defined in Eq. ( <ref type="formula" target="#formula_4">5</ref>) as an exponentiation of GDT:</p><formula xml:id="formula_4">exp-GDT(v, C) = 1 -exp(-GDT(v, C)) (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>Note: We normalize signals to [0, 1] and invert intensity values for Euclidean and Geodesic distances d(x) by 1d(x) for better highlighting of small distances. We define our adaptive Gaussian heatmaps ad-heatmap(v, c i , σ i ) via:</p><formula xml:id="formula_6">σ i = ae -bx , where x = 1 |N ci | v∈Nc i GDT(v, C) (6)</formula><p>Here, N ci is the 9-neighborhood of c i , a = 13 limits the maximum radius to 13, and b = 0.15 is set empirically 1 (details in supplementary). The radius σ i is smaller for higher x, i.e., when the mean geodesic distance in the neighboring voxels is high, indicating large intensity changes such as edges. This leads to a more precise guidance with a smaller radius σ i near edges and a larger radius in homogeneous areas such as clicks in the center of the object of interest. An example of this process and each guidance signal can be seen in Fig. <ref type="figure" target="#fig_1">1a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Backbone and Datasets</head><p>We use the DeepEdit <ref type="bibr" target="#b10">[11]</ref> model with a U-Net backbone <ref type="bibr" target="#b15">[15]</ref> and simulate a fixed number of clicks N during training and evaluation. For each volume, N clicks are iteratively sampled from over-and undersegmented predictions of the model as in <ref type="bibr" target="#b16">[16]</ref> and represented as foreground and background guidance signals. We implemented our experiments with MONAI Label <ref type="bibr" target="#b23">[23]</ref> and will release our code.</p><p>We trained and evaluated all of our models on the openly available AutoPET <ref type="bibr" target="#b0">[1]</ref> and MSD Spleen <ref type="bibr" target="#b1">[2]</ref> datasets. MSD Spleen <ref type="bibr" target="#b1">[2]</ref> contains 41 CT volumes with voxel size 0.79×0.79×5.00mm 3 and average resolution of 512×512×89 voxels with dense annotations of the spleen. AutoPET <ref type="bibr" target="#b0">[1]</ref> consists of 1014 PET/CT volumes with annotated tumor lesions of melanoma, lung cancer, or lymphoma. We discard the 513 tumor-free patients, leaving us with 501 volumes. We also only use PET data for our experiments. The PET volumes have a voxel size of 2.0 × 2.0 × 2.0mm 3 and an average resolution of 400 × 400 × 352 voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hyperparameters: Experiments</head><p>We keep these parameters constant for all models: learning rate = 10 -5 , #clicks N = 10, Dice Cross-Entropy Loss <ref type="bibr" target="#b24">[24]</ref>, and a fixed 80-20 training-validation split (D train /D val ). We apply the same data augmentation transforms to all models and simulate clicks as proposed in Sakinis et al. <ref type="bibr" target="#b16">[16]</ref>. We train using one A100 GPU for 20 and 100 epochs on AutoPET <ref type="bibr" target="#b0">[1]</ref> and MSD Spleen <ref type="bibr" target="#b1">[2]</ref> respectively.</p><p>We vary the following four hyperparameters (H1)-(H4): (H1) Sigma. We vary the radius σ of disks and heatmaps in Eq. ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>) and also explore how this parameter influences the performance of the distancebased signals in Eq. ( <ref type="formula" target="#formula_2">3</ref>)- <ref type="bibr" target="#b4">(5)</ref>. Instead of initializing the seed clicks C as individual voxels c i , we initialize the set of seed clicks C as all voxels within a radius σ centered at each c i and then compute the distance transform as in Eq. ( <ref type="formula" target="#formula_2">3</ref>)-( <ref type="formula" target="#formula_4">5</ref>).</p><p>(H2) Theta. We explore how truncating the values of distance-based signals in Eq. ( <ref type="formula" target="#formula_2">3</ref>)-( <ref type="formula" target="#formula_4">5</ref>) affects the performance. We discard the top θ ∈ {10%, 30%, 50%} of the distance values and keep only smaller distances closer to the clicks making the guidance more precise. Unlike MIDeepSeg <ref type="bibr" target="#b4">[5]</ref>, we compute the θ threshold for each image individually, as fixed thresholds may not be suitable for all images.</p><p>(H3) Input Adaptor. We test three methods for combining guidance signals with input volumes proposed by Sofiuuk et al. <ref type="bibr" target="#b8">[9]</ref> -Concat, Distance Maps Fusion (DMF), and Conv1S. Concat combines input and guidance by concatenating their channels. DMF additionally includes 1 × 1 conv. layers to adjust the channels to match the original size in the backbone. Conv1S has two branches for the guidance and volume, which are summed and fed to the backbone. (H4) Probability of Interaction. We randomly decide for each volume whether to add the N clicks or not, with a probability of p, in order to make the model more independent of interactions and improve its initial segmentation. All the hyperparameters we vary are summarized in Table <ref type="table" target="#tab_0">1</ref>. Each combination of hyperparameters corresponds to a separately trained DeepEdit <ref type="bibr" target="#b10">[11]</ref> model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Additional Evaluation Metrics</head><p>We use 5 metrics (M1)-(M5) (Table <ref type="table" target="#tab_1">2</ref>) to evaluate the validation performance.  Overlap of the guidance G with the ground-truth mask M : |M ∩G| |G| . This estimates the guidance precision as corrective clicks are often near boundaries, and if guidances are too large, such as disks with a large σ, there is a large overlap with the background outside the boundary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hyperparameters: Results</head><p>We first train a DeepEdit <ref type="bibr" target="#b10">[11]</ref> model for each (σ, θ) pair and set p = 100% and the input adaptor to Concat to constrain the parameter space.</p><p>(H1) Sigma. Results in Fig. <ref type="figure" target="#fig_1">1b</ref>) show that on MSD Spleen <ref type="bibr" target="#b1">[2]</ref>, the highest Dice scores are at σ = 5, with a slight improvement for two samples at σ = 1, but performance decreases for higher values σ &gt; 5. On AutoPET <ref type="bibr" target="#b0">[1]</ref>, σ = 5 and two samples with σ = 0 show the best performance, while higher values again demonstrate a significant performance drop. Figure <ref type="figure" target="#fig_1">1c</ref>) shows that the best final  and initial Dice for disks and heatmaps are with σ = 1 and σ = 0. Geodesic maps exhibit lower Dice scores for small σ &lt; 5 and achieve the best performance for σ = 5 on both datasets. Larger σ values lead to a worse initial Dice for all guidance signals. Differences in results for different σ values are more pronounced in AutoPET <ref type="bibr" target="#b0">[1]</ref> as it is a more challenging dataset <ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b25">25]</ref>.</p><p>(H2) Theta. We examine the impact of truncating large distance values for the EDT and GDT guidances from Eq. ( <ref type="formula" target="#formula_2">3</ref>) and ( <ref type="formula" target="#formula_3">4</ref>). Figure <ref type="figure" target="#fig_1">1a</ref>) shows that the highest final Dice scores are achieved with θ = 10 for MSD Spleen <ref type="bibr" target="#b1">[2]</ref>. On AutoPET <ref type="bibr" target="#b0">[1]</ref>, the scores are relatively similar when varying θ with a slight improvement at θ = 10. The results in Fig. <ref type="figure" target="#fig_1">1d</ref>) also confirm that θ = 10 is the optimal parameter for both datasets and that not truncating values on MSD Spleen <ref type="bibr" target="#b1">[2]</ref>, i.e. θ = 0, leads to a sharp drop in performance.</p><p>For our next experiments, we fix the optimal (σ, θ) pair for each of the five guidances (see Table <ref type="table" target="#tab_2">3</ref>) and train a DeepEdit <ref type="bibr" target="#b10">[11]</ref> model for all combinations of input adaptors and probability of interaction.</p><p>(H3) Input Adaptor. We look into different ways of combining guidance signals with input volumes using the input adaptors proposed by Sofiuuk et al. <ref type="bibr" target="#b8">[9]</ref>. The results in Fig. <ref type="figure" target="#fig_1">1e</ref>) indicate that the best performance is achieved by simply concatenating the guidance signal with the input volume. This holds true for both datasets and the difference in performance is substantial.</p><p>(H4) Probability of Interaction. Figure <ref type="figure" target="#fig_1">1e</ref>) shows that p ∈ {75%, 100%} results in the best performance on MSD Spleen <ref type="bibr" target="#b1">[2]</ref>, with a faster convergence rate for p = 75%. However, with p = 50%, the performance is worse than the non-interactive baseline (p = 0%). On AutoPET <ref type="bibr" target="#b0">[1]</ref>, the results for all p values are similar, but the highest Dice is achieved with p = 100%. Note that p = 100% results in lower initial Dice scores and requires more interactions to converge, indicating that the models depend more on the interactions. For the rest of our experiments, we use the optimal hyperparameters for each guidance in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Additional Evaluation Metrics: Results</head><p>The comparison of the guidance signals using our five metrics (M1)-(M5) can be seen in Fig. <ref type="figure">2</ref>. Although the concrete values for MSD Spleen <ref type="bibr" target="#b1">[2]</ref>  AutoPET <ref type="bibr" target="#b0">[1]</ref> are different, the five metrics follow the same trend on both datasets.</p><p>(M1) Initial and (M2) Final Dice. Overall, all guidance signals improve their initial-to-final Dice scores after N clicks, with AutoPET <ref type="bibr" target="#b0">[1]</ref> showing a gap between disks/heatmaps and distance-based signals. Moreover, geodesic-based signals have lower initial scores on both datasets and require more interactions.</p><p>(M3) Consistent Improvement. The consistent improvement is ≈ 65% for both datasets, but it is slightly worse for AutoPET <ref type="bibr" target="#b0">[1]</ref> as it is more challenging. Heatmaps and disks achieve the most consistent improvement, which means they are more precise in correcting errors. In contrast, geodesic distances change globally with new clicks as the whole guidance must be recomputed. These changes may confuse the model and lead to inconsistent improvement.</p><p>(M4) Overlap with Ground Truth. Heatmaps, disks, and EDT have a significantly higher overlap with the ground truth compared to geodesicbased signals, particularly on AutoPET <ref type="bibr" target="#b0">[1]</ref>. GDT incorporates the changes in voxel intensity, which is not a strong signal for lesions with weak boundaries in AutoPET <ref type="bibr" target="#b0">[1]</ref>, resulting in a smaller overlap with the ground truth. The guidances are ranked in the same order in (M3) and in (M4) for both datasets. Thus, a good overlap with the ground truth can be associated with precise corrections.</p><p>(M5) Efficiency. Efficiency is much higher on MSD Spleen [2] compared to AutoPET <ref type="bibr" target="#b0">[1]</ref>, as AutoPET has a ×2.4 larger mean volume size. The time also includes the sampling of new clicks for each simulated interaction. Disks are the most efficient signal, filling up spheres with constant values, while heatmaps are slightly slower due to applying a Gaussian filter over the disks. Distance transform-based guidances are the slowest on both datasets due to their complexity, but all guidance signals are computed in a reasonable time (&lt;1 s).</p><p>Adaptive Heatmaps: Results. Varying (H1)-(H4) and examining (M1)-(M5), we find disks/heatmaps as the best signals, but with inflexibility near edges due to their fixed radius (Fig. <ref type="figure" target="#fig_1">1a</ref>)). Using GDT as a proxy signal to adapt σ i for each click c i mitigates this weakness by imposing large σ i in homogeneous areas and small, precise σ i near edges (Fig. <ref type="figure" target="#fig_1">1a</ref>)). This results in substantially higher consistent improvement and overlap with ground truth and the best initial and final Dice (Table <ref type="table" target="#tab_2">3</ref>). Thus, our comparative study has led to the creation of a more consistent and flexible signal with a slight performance boost, albeit with an efficiency cost due to the need to compute both GDT and heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our comparative experiments yield insights into tuning existing guiding signals and designing new ones. We find that smaller radiuses (σ ≤ 5), a small threshold (θ = 10%), more iterations with interactions (p ≥ 75%), and traditional concatenation should be used. Weaknesses in existing signals include overly large radiuses near edges and inconsistent improvement for geodesic-based signals that change with each click. This analysis inspires our adaptive heatmaps, which adapt the radiuses of the heatmaps according to the geodesic values around the clicks, mitigating the inflexibility and inconsistency of existing guidances. We emphasize the importance of guidance representation in clinical applications, where a consistent and robust model is critical. Our study provides an overview of potential pitfalls, important parameters to tune, and how to design future guidance signals, along with proposed metrics for systematic comparison.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Final Dice Mean Dice score after N = 10 clicks per volume (M2) Initial Dice Mean Dice score before any clicks per volume (N = 0). A higher initial Dice indicates less work for the annotator (M3) Efficiency Inverted * time measurement (1 -T ) in seconds needed to compute the guidance. Low efficiency increases the annotation time with every new click. Note that this metric depends on the volume size and hardware setup. * Our maximum measurement Tmax is shorter than 1 second (M4) Consistent Improvement Ratio of clicks C + that improve the Dice score to the total number of validation clicks: |C + | N •|D val | , where N = 10 and Dval is the validation dataset (M5) Groundtruth Overlap</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. a) Our adaptive heatmaps use the mean local GDT values around each click to compute click-specific radiuses and form larger heatmaps in homogeneous regions and smaller near boundaries. b)-e) Results for varied hyperparameters. b) The final Dice of σ (left) and θ (right) aggregated for all guidance signals. c) The influence of σ and d) the influence of θ on individual guidances. e) Results for the different input adaptors (left) and probability of interaction p (right).</figDesc><graphic coords="6,42,30,54,62,339,52,476,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Variation of hyperparameters (H1) -(H4) in our experiments.</figDesc><table><row><cell>σ</cell><cell>{0, 1, 5, 9, 13}</cell><cell>θ {0%, 10%, 30%, 50%}</cell></row><row><cell cols="3">Input Adaptor {Concat, DMF, Conv1S} p {50%, 75%, 100%}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluation metrics for the comparison of guidance signals.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Optimal parameter settings of our interactive models and the non-interactive baseline (non-int.) and their Dice scores. * Same for both datasets.</figDesc><table><row><cell></cell><cell cols="3">MSD Spleen [2]/AutoPET [1]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>non-int</cell><cell>D</cell><cell>H</cell><cell>EDT</cell><cell>GDT</cell><cell cols="2">exp-GDT Ours</cell></row><row><cell>σ</cell><cell>-/-</cell><cell>1/0</cell><cell>1/0</cell><cell>1  *</cell><cell>5  *</cell><cell>5  *</cell><cell>adaptive  *</cell></row><row><cell>θ</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>10%  *</cell><cell>10%  *</cell><cell>-/-</cell><cell>-/-</cell></row><row><cell cols="3">Adaptor Concat  *  Concat  *</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>p</cell><cell>0%  *</cell><cell>75%/100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dice</cell><cell cols="7">94.9/64.9 95.9/78.2 95.8/78.2 95.8/75.2 95.2/74.5 95.2/73.2 96.9/79.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and Comparison of all guidance signals with our 5 metrics. The circles next to each metric represent the ranking of the guidances (sorted top-to-bottom).</figDesc><table><row><cell></cell><cell></cell><cell>0.8</cell><cell>Final Dice</cell><cell>0.96 0.95 0.95 0.95 0.95 0.95</cell><cell cols="4">Geodesic Maps Euclidean Maps Disks Heatmaps Adaptive Heatmaps (ours)</cell><cell>0.8</cell><cell>Final Dice</cell><cell>0.79 0.78 0.78 0.75 0.73 0.73</cell></row><row><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Exp. Geodesic Maps</cell><cell></cell></row><row><cell>Efficiency</cell><cell>0.61 0.46 0.43 0.42 0.20</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell>Initial Dice</cell><cell>0.90 0.89 0.86 0.85 0.83 0.80</cell><cell>0.31 0.25 0.20 0.17 0.04 0.01</cell><cell>Efficiency</cell><cell>0.6 0.4</cell><cell>0.61 0.60 0.56 0.47 0.47 0.43</cell><cell>Initial Dice</cell></row><row><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell>Overlap with GT</cell><cell>0.80 0.65 0.52 0.42 0.39 0.37</cell><cell cols="2">MSD Spleen</cell><cell></cell><cell>Consistent Improvement</cell><cell>0.81 0.73 0.73 0.72 0.66 0.63</cell><cell>0.96 0.95 0.89 0.81 0.22 0.21</cell><cell>Overlap with GT</cell><cell cols="2">AutoPET</cell><cell>0.78 0.68 0.62 0.60 0.60 0.59</cell><cell>Consistent Improvement</cell></row><row><cell>Fig. 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We note that b can also be automatically learned but we leave this for future work.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The present contribution is supported by the <rs type="funder">Helmholtz Association</rs> under the joint research school "HIDSS4Health -Helmholtz Information and Data Science School for Health. This work was performed on the HoreKa supercomputer funded by the <rs type="funder">Ministry of Science, Research and the Arts Baden-Württemberg</rs> and by the <rs type="funder">Federal Ministry of Education and Research</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 61.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A whole-body FDG-PET/CT Dataset with manually annotated</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gatidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tumor Lesions. Sci. Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A large annotated medical image dataset for the development and evaluation of segmentation algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Simpson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09063</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Virtual raters for reproducible and objective assessments in radiology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MIDeepSeg: minimally interactive segmentation of unseen objects from medical images using deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102102</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">i3Deep: efficient 3D interactive segmentation with the nnU-Net</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gotkowski</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ECONet: efficient convolutional online likelihood network for scribble-based interactive segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning-MIDL</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale interactive object segmentation with human annotators</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reviving iterative training with mask guidance for interactive segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP46576.2022.9897365</idno>
		<ptr target="https://doi.org/10.1109/ICIP46576.2022.9897365" />
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Image Processing (ICIP)</title>
		<meeting><address><addrLine>Bordeaux, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3141" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FastGeodis: fast generalised geodesic distance transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dorent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Source Softw</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">79</biblScope>
			<biblScope unit="page">4532</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeepEdit: deep editable learning for interactive segmentation of 3D medical images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diaz-Pinto</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-17027-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-17027-02" />
	</analytic>
	<monogr>
		<title level="m">Data Augmentation, Labelling, and Imperfections: Second MICCAI Workshop, DALI 2022, Held in Conjunction with MICCAI 2022</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">22 September 2022. 2022</date>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UCP-net: unstructured contour points for instance segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouakrim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iteratively trained interactive segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVC</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GeoS: geodesic image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2008</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5302</biblScope>
			<biblScope unit="page" from="99" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88682-2_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-88682-29" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Interactive segmentation of medical images through fully convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakinis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08205</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Heiliger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.01112</idno>
		<title level="m">AutoPET challenge: combining nn-Unet with swin UNETR augmented by maximum intensity projection classifier</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">AutoPET challenge 2022: automatic segmentation of whole-body tumor lesion based on deep learning and FDG PET/CT</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.01212</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring vanilla U-net for lesion segmentation from whole-body FDG-PET/CT scans</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.07490</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interactive image segmentation with first click attention</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Interactive segmentation using U-Net with weight map and dynamic user interactions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pirabaharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactive object segmentation with dynamic click transform</title>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Diaz-Pinto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12362</idno>
		<title level="m">Monai label: a framework for ai-assisted interactive labeling of 3d medical images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multimodal interactive lung lesion segmentation: a framework for annotating PET/CT images based on physiological and anatomical cues</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Hallitschke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.09914</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
