<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ken</forename><forename type="middle">C L</forename><surname>Wong</surname></persName>
							<email>clwong@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Almaden Research Center</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Wang</surname></persName>
							<email>hongzhiw@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Almaden Research Center</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tanveer</forename><surname>Syeda-Mahmood</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Almaden Research Center</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="364" to="373"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">98FE39A813B571351549AEA6A2AD685D</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image segmentation</term>
					<term>Transformer</term>
					<term>Fourier neural operator</term>
					<term>Hartley transform</term>
					<term>Resolution-robust</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the introduction of Transformers, different attentionbased models have been proposed for image segmentation with promising results. Although self-attention allows capturing of long-range dependencies, it suffers from a quadratic complexity in the image size especially in 3D. To avoid the out-of-memory error during training, input size reduction is usually required for 3D segmentation, but the accuracy can be suboptimal when the trained models are applied on the original image size. To address this limitation, inspired by the Fourier neural operator (FNO), we introduce the HartleyMHA model which is robust to training image resolution with efficient self-attention. FNO is a deep learning framework for learning mappings between functions in partial differential equations, which has the appealing properties of zero-shot superresolution and global receptive field. We modify the FNO by using the Hartley transform with shared parameters to reduce the model size by orders of magnitude, and this allows us to further apply self-attention in the frequency domain for more expressive high-order feature combination with improved efficiency. When tested on the BraTS'19 dataset, it achieved superior robustness to training image resolution than other tested models with less than 1% of their model parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs) have been widely used for medical image segmentation because of their speed and accuracy <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>. Nevertheless, given the local receptive fields of convolutional layers, long-range spatial correlations are mainly captured through consecutive convolutions and pooling. For computationally demanding 3D segmentation, the receptive fields and abstract levels can be more limited than in 2D as fewer layers can be used. To balance between computational complexity and network capability, input size reductions by image downsampling and patch-wise training are common approaches. However, CNNs trained with downsampled images can be suboptimal when applied on the original resolution, and the receptive field of patch-wise training can be largely reduced depending on the patch size.</p><p>With the introduction of Transformers <ref type="bibr" target="#b23">[24]</ref> and their vision alternatives <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, the self-attention mechanism for long-range dependencies has been adopted to medical image segmentation with promising results <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27]</ref>. These approaches form a sequence of samples by either using the pixel values of lowresolution features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> or by dividing an image into smaller patches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>, and the multi-head attention is used to learn the dependencies among samples. Although these self-attention approaches allow capturing of long-range dependencies, as the computational requirements are proportional to sequence lengths and patch sizes which are proportional to image sizes, size-reduction approaches are needed for large images especially in 3D.</p><p>As image size reduction is usually required for large images, it is desirable to have a model that is robust to training image resolution so that the trained model can be applied to higher-resolution images with decent accuracy. Furthermore, as self-attention of Transformers allows better expressiveness through high-order channel and sample mixing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>, incorporating self-attention in an efficient way can be beneficial. To gain these advantages, here we propose the HartleyMHA model which is a resolution-robust and parameter-efficient network architecture with frequency-domain self-attention for 3D image segmentation. This model is based on the Fourier neural operator (FNO) <ref type="bibr" target="#b16">[17]</ref>, which is a deep learning model that learns mappings between functions in partial differential equations (PDEs) and has the appealing properties of zero-shot super-resolution and global receptive field. Our contributions include: 1. To utilize the FNO for computationally expensive 3D segmentation, we modify it by using the Hartley transform with shared model parameters in the frequency domain. Residual connections <ref type="bibr" target="#b9">[10]</ref> and deep supervision <ref type="bibr" target="#b13">[14]</ref> are also introduced. These reduce the number of model parameters by orders of magnitude and improve accuracy. We call it the HNOSeg model. 2. As only low-frequency components are required for decent segmentation results, multi-head self-attention can be efficiently applied in the frequency domain. This allows high-order combination of features to improve the expressiveness of the model. We call it the HartleyMHA model. 3. We compare our proposed models with other models on different training image resolutions to study their robustness. This provides useful insights that are usually unavailable in other studies.</p><p>Experimental results on the BraTS'19 dataset <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref> show that the proposed models have superior robustness to training image resolution than other tested models with less than 1% of their model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fourier Neural Operator</head><p>FNO is a deep learning model for learning mappings between functions in PDEs without the PDEs provided <ref type="bibr" target="#b16">[17]</ref>. By formulating the solution in the continuous space based on the Green's function <ref type="bibr" target="#b15">[16]</ref>, FNO can learn a single set of model parameters for multiple resolutions. For computationally expensive 3D segmentation, such zero-shot super-resolution capability is advantageous as a model trained with lower-resolution images can be applied on higher-resolution images with decent accuracy. The neural operator is formulated as iterative updates:</p><formula xml:id="formula_0">u t+1 (x) := σ (W u t (x) + (Ku t ) (x)) with (Ku t ) (x) := D κ(x -y)u t (y) dy, ∀x ∈ D (1)</formula><p>where u t (x) ∈ R du t is a function of x. W ∈ R du t+1 ×du t is a learnable linear transformation and σ accounts for normalization and activation. In our work, D ⊂ R 3 represents the 3D imaging space, and u t (x) are the outputs of hidden layers with d ut channels. K is the kernel integral operator with κ ∈ R du t+1 ×du t a learnable kernel function. As (Ku t ) is a convolution, it can be efficiently solved by the convolution theorem which states that the Fourier transform (F) of a convolution of two functions is the pointwise product of their Fourier transforms:</p><formula xml:id="formula_1">(Ku t ) (x) = F -1 (F(κ)F(u t )) (x) = F -1 (RU t ) (x), ∀x ∈ D (2) R(k) = (Fκ)(k) ∈ C du t+1 ×du t is a learnable function in the frequency domain and U t (k) = (Fu t ) (k) ∈ C du t .</formula><p>Therefore, each pointwise product at k is realized as a matrix multiplication. When the fast Fourier transform is used in implementation, k ∈ N 3 are non-negative integer coordinates, and each k has a learnable R(k). As mainly low-frequency components are required for image segmentation, only k i ≤ k max,i corresponding to the lower frequencies in each dimension i are used to reduce model parameters and computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hartley Neural Operator (HNO)</head><p>As the FNO requires complex number operations in the frequency domain, the computational requirements such as memory and floating point operations are higher than with real numbers. Therefore, we use the Hartley transform instead, which is an integral transform alternative to the Fourier transform <ref type="bibr" target="#b7">[8]</ref>. The Hartley transform (H) converts real-valued functions to real-valued functions, which is related to the Fourier transform as (Hf ) = Real(Ff ) -Imag(Ff ). The convolution theorem of discrete Hartley transform is more complicated <ref type="bibr" target="#b3">[4]</ref>, and the kernel integration in (1) becomes: 3 is the size of the frequency domain. R and Û are N -periodic in each dimension 1 . Similar to using (2), the models built using (3) have tens of million parameters even with small k max (e.g., <ref type="bibr" target="#b13">(14,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10)</ref>). Therefore, instead of using a different R(k) at each k, we use the same (shared) R for all k and (3) becomes:</p><formula xml:id="formula_2">H (Ku t ) (k) = R(k) Ût (k) + Ût (N -k) + R(N -k) Ût (k) -Ût (N -k) 2 (3) with R(k) = (Hκ)(k) ∈ R du t+1 ×du t and Ût (k) = (Hu t ) (k) ∈ R du t . N ∈ N</formula><formula xml:id="formula_3">H (Ku t ) (k) = R Ût (k)<label>(4)</label></formula><p>This is equivalent to applying a convolution layer with the kernel size of one in the frequency domain. We find that using (4) simplifies the computation and largely reduces the number of parameters without affecting the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hartley Multi-head Attention (MHA)</head><p>As real instead of complex numbers are used in (4), multi-head attention in <ref type="bibr" target="#b23">[24]</ref> can be applied in the frequency domain for high-order feature combination.</p><p>As k max can be much smaller than the image size for image segmentation, the sequence length (number of voxels) can be largely reduced. With (4), the query, key, and value matrices (Q, K, V ) of self-attention can be computed as:</p><formula xml:id="formula_4">Q = Ūt RT Q , K = Ūt RT K , V = Ūt RT V ∈ R N f ×du t+1 (5)</formula><p>where</p><formula xml:id="formula_5">Ūt ∈ R N f ×du t , with N f = 8k max,x k max,y k max,z , is a 2D matrix formed by stacking Ût (k) 2 .</formula><p>Although N f can be relatively small, the computation and memory requirements of computing QK T can still be demanding. For example, k max = (14, 14, 10) corresponds to an attention matrix with around 246M elements. To remedy this, for each Q, K, and V , we group the feature vectors with a patch size of 2 × 2 × 2 voxels in the frequency domain and their matrix sizes become</p><formula xml:id="formula_6">N f 8 × 8d ut+1 .</formula><p>This reduces the number of elements in QK T by 64 times. The self-attention can then be computed as:</p><formula xml:id="formula_7">Attention(Q, K, V ) = SELU QK T / 8d ut+1 V ∈ R N f 8 ×8du t+1 (6)</formula><p>where SELU represents the scaled exponential linear unit <ref type="bibr" target="#b12">[13]</ref>. Similar to <ref type="bibr" target="#b20">[21]</ref>, we find that using softmax in self-attention results in suboptimal segmentations, thus the SELU was chosen after testing with multiple activations. Furthermore, we find that position encoding is unnecessary. The result of ( <ref type="formula">6</ref>) can be rearranged back to the original shape in the frequency domain so that the inverse Hartley transform can be applied. The multi-head attention can be used with (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Network Architectures -HNOSeg and HartleyMHA</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the network architecture. We call it HNOSeg with the HNO blocks and HartleyMHA with the Hartley MHA blocks. Different from the FNO in <ref type="bibr" target="#b16">[17]</ref>, residual connections <ref type="bibr" target="#b9">[10]</ref> and deep supervision <ref type="bibr" target="#b13">[14]</ref> are used to improve the training stability, convergence, and accuracy. As the batch size is usually small for memory demanding 3D segmentation, layer normalization (LN) is used <ref type="bibr" target="#b0">[1]</ref>. The SELU <ref type="bibr" target="#b12">[13]</ref> is used as the activation function, and the softmax function is used to produce the final prediction scores. Similar to the Fourier transform, the Hartley transform provides a global receptive field as all voxels are used to compute the value at each k, thus pooling is not required. As using the original image resolution usually results in out-of-memory errors in 3D segmentation, downsampling the inputs and then upsampling the predictions may be required. Instead of using traditional image resampling methods, we use a convolutional layer with the kernel size and stride of two right after the input layer, and replace the output convolutional layer by a transposed convolutional layer with the kernel size and stride of two (red blocks in Fig. <ref type="figure" target="#fig_0">1</ref>). In this way, the model can learn the optimal resampling approach. In the experiments, k max = (14, 14, 10) so that it can be used with the lowest tested training resolution of 60 × 60 × 39. Other hyperparameters such as d ut+1 , N h , and N B were obtained empirically for decent segmentations when training with the original image resolution. As each HNO block and Hartley MHA block can be implemented as a deep-learning layer in commonly used libraries, they can be easily adopted by other architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training Strategy</head><p>The images of different modalities are stacked along the channel axis to provide a multi-channel input. As the intensity ranges across modalities can be quite different, intensity normalization is performed on each image of each modality. Image augmentation with rotation (axial, ±30 • ), shifting (±20%), and scaling ([0.8, 1.2]) is used and each image has an 80% chance to be transformed. The Adamax optimizer <ref type="bibr" target="#b11">[12]</ref> is used with the cosine annealing learning rate scheduler <ref type="bibr" target="#b19">[20]</ref>, with the maximum and minimum learning rates as 10 -2 and 10 -3 , respectively. The Pearson's correlation coefficient loss is used as it is robust to learning rate for image segmentation <ref type="bibr" target="#b24">[25]</ref>, and it consistently outperformed the Dice loss and weighted cross-entropy in our experiments. An NVIDIA Tesla P100 GPU with 16 GB memory is used with a batch size of one and 100 epochs, and Keras in TensorFlow 2.6.2 is used for implementation. Note that small batch sizes are common in 3D segmentation given the high memory requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Experimental Setups</head><p>The dataset of BraTS'19 with 335 cases of gliomas was used, each with four modalities of T1, post-contrast T1, T2, and T2-FLAIR images with 240 × 240 × 155 voxels <ref type="bibr" target="#b2">[3]</ref>. There is also an official validation dataset of 125 cases in the same format without given annotations. Models were trained with images downsampled by different factors <ref type="bibr">(1, 2, 3, and 4)</ref> to study the robustness to image resolution. In training, we split the training dataset (335 cases) into 90% for training and 10% for validation. In testing, each model was tested on the official validation dataset (125 cases) with 240 × 240 × 155 voxels regardless of the downsampling factor. The predictions were uploaded to the CBICA Image Processing Portal<ref type="foot" target="#foot_2">3</ref> for the results statistics of the "whole tumor" (WT), "tumor core" (TC), and "enhancing tumor" (ET) regions <ref type="bibr" target="#b2">[3]</ref>. We compare our proposed HNOSeg and HartleyMHA models with three other models:</p><p>1. V-Net-DS <ref type="bibr" target="#b25">[26]</ref>: a V-Net with deep supervision representing the commonlyused encoding-decoding architectures. 2. UTNet <ref type="bibr" target="#b6">[7]</ref>: a U-Net enhanced by the Transformer's attention mechanism. 3. FNO <ref type="bibr" target="#b16">[17]</ref>: original FNO without shared parameters, residual connections, and deep supervision. The same hyperparameters as HNOSeg were used. The learnable resampling approach in Sect. 2.4 was applied to all models. Note that our goal is not competing for the best accuracy but studying the robustness to image resolution. Although only the results of a dataset are shown because of the page limit, the characteristics of the proposed models can be demonstrated through this challenging multi-modal brain tumor segmentation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussion</head><p>Figure <ref type="figure" target="#fig_1">2</ref> and Table <ref type="table" target="#tab_0">1</ref> show comparisons of resolution robustness among tested models, and Table <ref type="table" target="#tab_1">2</ref> shows the computational costs during inference. At the original resolution, V-Net-DS and UTNet outperformed HNOSeg and HartleyMHA by less than 3% in the Dice coefficient on average, but HNOSeg and HartleyMHA only had less than 50k model parameters which were less than 1% of V-Net-DS and UTNet. FNO performed worst with the most parameters (144.5M). As the resolution decreased, the accuracies of V-Net-DS and UTNet decreased almost linearly with the downsampling factor, while HNOSeg and HartleyMHA were more robust. When the downsampling factor changed from 1 to 3, the average Dice coefficients of V-Net-DS and UTNet decreased by more than 14.8%, while those of HNOSeg and HartleyMHA only decreased by less than 2.5%. Similar trends can be observed for the 95% Hausdorff distance, except that FNO performed surprisingly well in this aspect. HartleyMHA performed better overall than HNOSeg. Note that we fixed k max for the consistency among models in the experiments, which can be adjusted for better results in other situations.</p><p>For computation cost, Table <ref type="table" target="#tab_1">2</ref> shows that V-Net-DS and UTNet had shorter inference times than HNOSeg and HartleyMHA, though all models used less  Figure <ref type="figure" target="#fig_2">3</ref> shows the visual comparisons of the segmentation results on an unseen case. Consistent to Fig. <ref type="figure" target="#fig_1">2</ref> With such superior robustness to image resolution, HNOSeg and Hart-leyMHA can be trained with lower-resolution images using fewer computational resources to provide decent segmentation results on the original resolution during inference. While HartleyMHA performed better than HNOSeg in general, their similar performance is consistent with the findings in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref> that self-attention is sufficient for good performance but is not crucial. On the other hand, as the use of efficient self-attention improves the expressiveness of the Hartley MHA block, fewer layers can be used to reduce the overall computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, based on the idea of FNO which has the properties of zeroshot super-resolution and global receptive field, we propose the HNOSeg and HartleyMHA models for resolution-robust and parameter-efficient 3D image segmentation. HNOSeg is FNO improved by the Hartley transform, residual connections, deep supervision, and shared parameters in the frequency domain. We further extend this concept for efficient multi-head attention in the frequency domain as HartleyMHA. Experimental results show that HNOSeg and HartleyMHA had similar accuracies as other tested segmentation models when trained with the original image resolution, but had superior performance when trained with images of much lower resolutions. HartleyMHA performed slightly better than HNOSeg and ran faster with less memory. With these advantages, HartleyMHA can be a promising alternative for 3D image segmentation especially when computational resources are limited.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Network architecture. The blocks are (1) with the kernel integral operator implemented by the Hartley transform (HNO block) or the Hartley multi-head attention (Hartley MHA block). N h = 4 is the number of heads. We use du t+1 = du t = 12, kmax = (14, 14, 10), and NB = 32 with the HNO block and NB = 16 with the Hartley MHA block. The red blocks are for learnable resampling.(Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparisons of robustness to training image resolution. Each point represents the average value from WT, TC, and ET of the 125 official validation cases of BraTS'19. The training images were downsampled by different factors while the trained models were tested with the original resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparisons among models trained with different image resolutions, tested on an unseen case of size 240 × 240 × 155. The Dice coefficients were averaged from the WT, TC, and ET regions.</figDesc><graphic coords="8,108,63,60,74,228,55,186,37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>than 0.8 s per image of size 240 × 240 × 155. HartleyMHA ran faster than HNOSeg and used less memory, though HartleyMHA had more parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, except FNO, the accuracies of the models were similar with the original training image resolution. As the training image resolution reduced, HNOSeg and HartleyMHA gradually outperformed V-Net-DS and UTNet. When the training images were downsampled from 240 × 240 × 155 to 60 × 60 × 39 (downsampling factor = 4), the average Dice coefficients of V-Net-DS and UTNet decreased by more than 24%, and HartleyMHA had the least reduction of 5.1%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Numerical comparisons of Dice coefficients (%) and 95% Hausdorff distances (HD95) with different training image resolutions.</figDesc><table><row><cell cols="3">Downsampling factor 1 (240 × 240 × 155)</cell><cell cols="2">3 (80 × 80 × 52)</cell></row><row><cell>Metric</cell><cell>Dice (%)</cell><cell>HD95</cell><cell>Dice (%)</cell><cell>HD95</cell></row><row><cell>Region</cell><cell cols="4">WT TC ET WT TC ET WT TC ET WT TC ET</cell></row><row><cell>V-Net-DS</cell><cell cols="4">88.8 77.7 74.7 7.1 8.9 6.2 70.6 57.9 68.2 51.9 53.1 35.2</cell></row><row><cell>UTNet</cell><cell cols="4">86.9 76.1 74.0 7.5 9.4 6.6 69.9 56.8 63.1 49.4 57.0 42.3</cell></row><row><cell>FNO</cell><cell cols="4">84.0 69.0 62.2 9.4 11.2 8.1 79.3 63.8 54.0 10.0 11.2 9.8</cell></row><row><cell>HNOSeg</cell><cell cols="4">87.7 75.0 73.2 9.1 9.6 6.5 86.7 71.9 69.8 15.0 14.4 12.6</cell></row><row><cell>HartleyMHA</cell><cell cols="4">86.9 73.1 72.5 9.2 9.8 7.3 84.8 72.8 69.8 12.5 13.1 10.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Number of parameters, inference time per image in seconds averaged from images of size 240 × 240 × 155, and memory in GB with a batch size of 1.</figDesc><table><row><cell>V-Net-DS</cell><cell>UTNet</cell><cell>FNO</cell><cell>HNOSeg</cell><cell>HartleyMHA</cell></row><row><cell cols="5">Param Time Mem Param Time Mem Param Time Mem Param Time Mem Param Time Mem</cell></row><row><cell>5.7M 0.33 9.0</cell><cell>7.1M 0.41 4.9</cell><cell>144.5M 0.61 4.8</cell><cell>24.8k 0.71 8.9</cell><cell>47.7k 0.57 4.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In Python, this means Û [Nx, :, :] = Û [0, :, :], etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that kmax = (kmax,x, kmax,y, kmax,z) corresponds to a frequency domain of size 2kmax,x × 2kmax,y × 2kmax,z to cover both positive and negative frequency terms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://ipp.cbica.upenn.edu/.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">170117. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discrete Hartley transform</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bracewell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1832" to="1835" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Swin-Unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-8_9" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13803</biblScope>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UTNet: a hybrid transformer architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="61" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A more symmetrical Fourier analysis applied to transmission problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V L</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IRE</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="144" to="150" />
			<date type="published" when="1942">1942</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning techniques for medical image segmentation: achievements and challenges</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hesamian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="582" to="596" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="972" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<title level="m">FNet: Mixing tokens with Fourier transforms</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03485</idno>
		<title level="m">Neural operator: Graph kernel network for partial differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fourier neural operator for parametric partial differential equations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review of deep-learning-based medical image segmentation methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sustainability</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1224</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SOFT: softmax-free transformer with linear complexity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21297" to="21309" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MLP-Mixer: an all-MLP architecture for vision</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24261" to="24272" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3D segmentation with fully trainable Gabor kernels and Pearson&apos;s correlation coefficient</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning in Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="53" to="61" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D segmentation with exponential logarithmic loss for highly unbalanced object sizes</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00931-1_70</idno>
		<idno>978-3-030-00931-1_70</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11072</biblScope>
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CoTr: efficiently bridging CNN and transformer for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
