<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medical Image Computing and Computer Assisted Intervention â€“ MICCAI 2023</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hu</forename><surname>Wang</surname></persName>
							<email>hu.wang@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Congbo</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianpeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba DAMO Academy</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jodie</forename><surname>Avery</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Louise</forename><surname>Hull</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DB5C22FB8A925E5CA60A32555A55B4A3</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Missing modality issue</term>
					<term>Multi-modal learning</term>
					<term>Learnable cross-modal knowledge distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of missing modalities is both critical and nontrivial to be handled in multi-modal models. It is common for multimodal tasks that certain modalities contribute more compared to other modalities, and if those important modalities are missing, the model performance drops significantly. Such fact remains unexplored by current multi-modal approaches that recover the representation from missing modalities by feature reconstruction or blind feature aggregation from other modalities, instead of extracting useful information from the best performing modalities. In this paper, we propose a Learnable Cross-modal Knowledge Distillation (LCKD) model to adaptively identify important modalities and distil knowledge from them to help other modalities from the cross-modal perspective for solving the missing modality issue. Our approach introduces a teacher election procedure to select the most "qualified" teachers based on their single modality performance on certain tasks. Then, cross-modal knowledge distillation is performed between teacher and student modalities for each task to push the model parameters to a point that is beneficial for all tasks. Hence, even if the teacher modalities for certain tasks are missing during testing, the available student modalities can accomplish the task well enough based on the learned knowledge from their automatically elected teacher modalities. Experiments on the Brain Tumour Segmentation Dataset 2018 (BraTS2018) shows that LCKD outperforms other methods by a considerable margin, improving the state-of-the-art performance by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in terms of segmentation Dice score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal learning has become a popular research area in computer vision and medical image analysis, with modalities spanning across various media types, including texts, audio, images, videos and multiple sensor data. This approach has been utilised in Robot Control <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, Visual Question Answering <ref type="bibr" target="#b11">[12]</ref> and Audio-Visual Speech Recognition <ref type="bibr" target="#b9">[10]</ref>, as well as in the medical field to improve diagnostic system performance <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>. For instance, Magnetic Resonance Imaging (MRI) is a common tool for brain tumour detection that relies on multiple modalities (Flair, T1, T1 contrast-enhanced known as T1c, and T2) rather than a single type of MRI images. However, most existing multi-modal methods require complete modalities during training and testing, which limits their applicability in real-world scenarios, where subsets of modalities may be missing during training and testing.</p><p>The missing modality issue is a significant challenge in the multi-modal domain, and it has motivated the community to develop approaches that attempt to address this problem. Havaei et al. <ref type="bibr" target="#b7">[8]</ref> developed HeMIS, a model that handles missing modalities using statistical features as embeddings for the model decoding process. Taking one step ahead, Dorent et al. <ref type="bibr" target="#b5">[6]</ref> proposed an extension to HeMIS via a multi-modal variational auto-encoder (MVAE) to make predictions based on learned statistical features. In fact, variational auto-encoder (VAE) has been adopted to generate data from other modalities in the image or feature domains <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>. Yin et al. <ref type="bibr" target="#b19">[20]</ref> aimed to learn a unified subspace for incomplete and unlabelled multi-view data. Chen et al. <ref type="bibr" target="#b3">[4]</ref> proposed a feature disentanglement and gated fusion framework to separate modality-robust and modalitysensitive features. Ding et al. <ref type="bibr" target="#b4">[5]</ref> proposed an RFM module to fuse the modal features based on the sensitivity of each modality to different tumor regions and a segmentation-based regularizer to address the imbalanced training problem. Zhang et al. <ref type="bibr" target="#b21">[22]</ref> proposed an MA module to ensure that modality-specific models are interconnected and calibrated with attention weights for adaptive information exchange. Recently, Zhang et al. <ref type="bibr" target="#b20">[21]</ref> introduced a vision transformer architecture, MMFormer, that fuses features from all modalities into a set of comprehensive features. There are several existing works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> proposed to approximate the features from full modalities when one or more modalities are absent. But none work performs cross-modal knowledge distillation. From an other point of view, Wang et al. <ref type="bibr" target="#b18">[19]</ref> introduced a dedicated training strategy that separately trains a series of models specifically for each missing situation, which requires significantly more computation resources compared with a nondedicated training strategy. An interesting fact about multi-modal problems is that there is always one modality that contributes much more than other modalities for a certain task. For instance, for brain tumour segmentation, it is known from domain knowledge that T1c scans clearly display the enhanced tumour, but not edema <ref type="bibr" target="#b3">[4]</ref>. If the knowledge of these modalities can be successfully preserved, the model can produce promising results even when these best performing modalities are not available. However, the aforementioned methods neglect the contribution biases of different modalities and failed to consider keeping that knowledge.</p><p>Aiming at this issue, we propose the non-dedicated training model<ref type="foot" target="#foot_0">1</ref> Learnable Cross-modal Knowledge Distillation (LCKD) for tackling the missing modality issue. LCKD is able to handle missing modalities in both training and testing by automatically identifying important modalities and distilling knowledge from them to learn the parameters that are beneficial for all tasks while training for other modalities (e.g., there are four modalities and three tasks for the three types of tumours in BraTS2018). Our main contributions are:</p><p>-We propose the Learnable Cross-modal Knowledge Distillation (LCKD) model to address missing modality problem in multi-modal learning. It is a simple yet effective model designed from the viewpoint of distilling crossmodal knowledge to maximise the performance for all tasks; -The LCKD approach is designed to automatically identify the important modalities per task, which helps the cross-modal knowledge distillation process. It also can handle missing modality during both training and testing.</p><p>The experiments are conducted on the Brain Tumour Segmentation benchmark BraTS2018 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>, showing that our LCKD model achieves state-of-theart performance. In comparison to recently proposed competing methods on BraTS2018, our model demonstrates better performance in segmentation Dice score by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour, on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Architecture</head><p>Let us represent the N -modality data with M l = {x</p><formula xml:id="formula_0">(i) l } N i=1 , where x (i) l</formula><p>âˆˆ X denotes the l th data sample and the superscript (i) indexes the modality. To simplify the notation, we omit the subscript l when that information is clear from the context. The label for each set M is represented by y âˆˆ Y, where Y represents the ground-truth annotation space. The framework of LCKD is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Multi-modal segmentation is composed not only of multiple modalities, but also of multiple tasks, such as the three types of tumours in BraTS2018 dataset that represent the three tasks. Take one of the tasks for example. Our model undergoes an external Teacher Election Procedure prior to processing all modalities {x (i) } N i=1 âˆˆ M in order to select the modalities that exhibit promising performance as teachers. This is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, where one of the modalities, x (2) , is selected as a teacher, {x (1) , x (3) , ..., x (N ) } are the students, and x (n)  (with n = 2) is assumed to be absent. Subsequently, the modalities are encoded to output features {f (i) } N i=1 , individually. For the modalities that are available, namely x (1) , ..., x (n-1) , x (n+1) , ..., x (N ) , knowledge distillation is carried out between each pair of teacher and student modalities. However, for the absent  i=1 } are processed by the encoder to produce the features {f (i) N i=1 }, which are concatenated and used by the decoder to produce the segmentation. The teacher is elected using a validation process that selects the top-performing modalities as teachers. Cross-modal distillation is performed by approximating the available students' features to the available teachers' features. Features from missing modalities are generated by averaging the other modalities' features. modality x (n) , its features f (n) are produced through a missing modality feature generation process from the available features f (1) , ..., f (n-1) , f (n+1) , ..., f (N ) .</p><p>In the next sections, we explain each module of the proposed Learnable Crossmodal Knowledge Distillation model training and testing with full and missing modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Teacher Election Procedure</head><p>Usually, one of the modalities is more useful than others for a certain task, e.g. for brain tumour segmentation, T1c scan clearly displays the enhanced tumour, but it does not clearly show edema <ref type="bibr" target="#b3">[4]</ref>. Following knowledge distillation (KD) <ref type="bibr" target="#b8">[9]</ref>, we propose to transfer the knowledge from modalities with promising performance (known as teachers) to other modalities (known as students). The teacher election procedure is further introduced to automatically elect proper teachers for different tasks.</p><p>More specifically, in the teacher election procedure, a validation process is applied: for each task k (for k âˆˆ {1, ..., K}), the modality with the best performance is selected as the teacher t (k) . Formally, we have:</p><formula xml:id="formula_1">t (k) = arg max iâˆˆ{1,...,N } L l=1 d(F (x (i) l ; Î˜), y l ),<label>(1)</label></formula><p>where i indexes different modalities, F (â€¢; Î˜) is the LCKD segmentation model parameterised by Î˜, including the encoder and decoder parameters {Î¸ enc , Î¸ dec } âˆˆ Î˜, and d(â€¢, â€¢) is the function to calculate the Dice score. Based on the elected teachers for different tasks, a list of unique teachers (i.e., repetitions are not allowed in the list, so for BraTS, {T1c, T1c, Flair} would be reduced to {T1c, Flair}) are generated with: T = Ï†(t (1) , t (2) , ..., t (k) , ..., t (K) ), <ref type="bibr" target="#b1">(2)</ref> where Ï† is the function that returns the unique elements from a given list, and T âŠ† {1, ..., N } is the teacher set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-Modal Knowledge Distillation</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, after each modality x (i) is inputted into the encoder parameterised by Î¸ enc , the features f (i) for each modality is fetched, as in:</p><formula xml:id="formula_2">f (i) = f Î¸ enc (x (i) ).</formula><p>(3)</p><p>The cross-modal knowledge distillation (CKD) is defined by a loss function that approximates all available modalities' features to the available teacher modalities in a pairwise manner for all tasks, as follows:</p><formula xml:id="formula_3">ckd (D; Î¸ enc ) = N iâˆˆT;i,j / âˆˆm f (i) -f (j) p, (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where â€¢ p presents the p-norm operation, and here we expended the notation of missing modalities to make it more general by assuming a set of modalities m is missing. The minimisation of this loss pushes the model parameter values to a point in the parameter space that can maximise the performance of all tasks for all modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Missing Modality Feature Generation</head><p>Because of the knowledge distillation between each pair of teachers and students, the features of modalities in the feature space ought to be close to the "genuine" features that can uniformly perform well for different tasks. Still assuming that modality set m is missing, the missing features f (n) can thus be generated from the available features:</p><formula xml:id="formula_5">f (n) = 1 N -|m| N i=1;i / âˆˆm f (i) , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where |m| denotes the number of missing modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training and Testing</head><p>All features encoded from Eq. 3 or generated from Eq. 5 are then concatenated to be fed into the decoder parameterised by Î¸ dec for predicting</p><formula xml:id="formula_7">á»¹ = f Î¸ dec (f (1) , ..., f (N ) ),<label>(6)</label></formula><p>where á»¹ âˆˆ Y is the prediction of the task.</p><p>The training of the whole model is achieved by minimising the following objective function: tot (D, Î˜) = task (D, Î¸ enc , Î¸ dec ) + Î± ckd (D; Î¸ enc ), <ref type="bibr" target="#b6">(7)</ref> where task (D, Î¸ enc , Î¸ dec ) is the objective function for the whole task (e.g., Cross-Entropy and Dice losses are adopted for brain tumour segmentation), and Î± is the trade-off factor between the task objective and cross-modal KD objective.</p><p>Testing is based on taking all image modalities available in the input to produce the features from Eq. 3, and generating the features from the missing modalities with Eq. 5, which are then provided to the decoder to predict the segmentation with Eq. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Implementation Details</head><p>Our model and competing methods are evaluated on the BraTS2018 Segmentation Challenge dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. The task involves segmentation of three subregions of brain tumours, namely enhancing tumour (ET), tumour core (TC), and whole tumour (WT). The dataset consists of 3D multi-modal brain MRIs, including Flair, T1, T1 contrast-enhanced (T1c), and T2, with ground-truth annotations. The dataset comprises 285 cases for training, and 66 cases for evaluation. The ground-truth annotations for the training set are publicly available, while the validation set annotations are hidden 2 .</p><p>3D UNet architecture (with 3D convolution and normalisation) is adopted as our backbone network, where the CKD process occurs at the bottom stage of the UNet structure. To optimise our model, we adopt a stochastic gradient descent optimiser with Nesterov momentum <ref type="bibr" target="#b1">[2]</ref> set to 0.99. L1 loss is adopted for ckd (.) in Eq. 4. Batch-size is set to 2. The learning rate is initially set to 10 -2 and gradually decreased via the cosine annealing <ref type="bibr" target="#b12">[13]</ref> strategy. We trained the LCKD model for 115,000 iterations and use 20% of the training data as the validation task for teacher election. To simulate modality-missing situations with non-dedicated training of models, we randomly dropped 0 to 3 modalities for each iteration. Our training time is 70.12 h and testing time is 6.43 s per case on one Nvidia 3090 GPU. 19795 MiB GPU memory is used for model training with batch-size 2 and 3789 MiB GPU memory is consumed for model testing with batch-size 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Performance</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the overall performance on all 15 possible combinations of missing modalities for three sub-regions of brain tumours. Our models are compared with several strong baseline models: U-HeMIS (abbreviated as HMIS in the figure) <ref type="bibr" target="#b7">[8]</ref>, U-HVED (HVED) <ref type="bibr" target="#b5">[6]</ref>, Robust-MSeg (RSeg) <ref type="bibr" target="#b3">[4]</ref> and mmFormer (mmFm) <ref type="bibr" target="#b20">[21]</ref>. We can clearly observe that with T1c, the model performs considerably better than other modalities for ET. Similarly, T1c for TC and Flair for WT contribute the most, which confirm our motivation.</p><p>The LCKD model significantly outperforms (as shown by the one-tailed paired t-test for each task between models in the last row of Table <ref type="table" target="#tab_1">1</ref>) U-HeMIS, U-HVED, Robust-MSeg and mmFormer in terms of the segmentation Dice for enhancing tumour and whole tumour on all 15 combinations and the tumour core on 14 out of 15. It is observed that, on average, the proposed LCKD model improves the state-of-the-art performance by 3.61% for enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in terms of the segmentation Dice score. Especially in some combinations without the best modality, e.g. ET/TC without T1c and WT without Flair, LCKD has a 6.15% improvement with only Flair and 10.69% with only T1 over the second best model for ET segmentation; 10.8% and 10.03% improvement with only Flair and T1 for TC; 8.96% and 5.01% improvement with only T1 and T1c for WT, respectively. These results demonstrate that useful knowledge of the best modality has been successfully distilled into the model by LCKD for multimodal learning with missing modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analyses</head><p>Single Teacher vs. Multi-teacher. To analyse the effectiveness of knowledge distillation from multiple teachers of all tasks in the proposed LCKD model, we perform a study to compare the model performance of adopting single teacher and multi-teachers for knowledge distillation. We enable multi-teachers for LCKD by default to encourage the model parameters to move to a point that can perform well for all tasks. However, for single teacher, we modify the  <ref type="table" target="#tab_2">2</ref>, compared with multi-teacher model LCKD-m, we found that the single teacher model LCKD-s receives comparable results for ET and TC segmentation (it even has better average performance on ET), but it cannot outperform LCKD-m on WT. This phenomenon, also shown in Fig. <ref type="figure">2</ref>, demonstrates that LCKD-m has better overall segmentation performance. This resonates with our expectations because there are 3 tasks in BraTS, and the best teachers for ET and TC are the same, which is T1c, but for WT, Flair is the best one. Therefore, for LCKD-s, the knowledge of the best teacher for ET and TC can be distilled into the model, but not for WT. The LCKD-m model can overcome this issue since it attempts to find a point in the parameter space that is beneficial for all tasks. Empirically, we observed that both models found the correct teacher(s) quickly: the best teacher of the single teacher model alternated between T1c and Flair for a few validation rounds and stabilised at T1c; while the multi-teacher model found the best teachers (T1c and Flair) from the first validation round.  Role of Î± and CKD Loss Function. As shown in Fig. <ref type="figure">3</ref>, we set Î± in Eq. 7 to {0, 0.1, 0.5, 1} using T1 input only and L1 loss for ckd (.) in <ref type="bibr" target="#b3">(4)</ref>. If Î± = 0, the model performance drops greatly, but when Î± &gt; 0, results improve, where Î± = 0.1 produces the best result. This shows the importance of the cross-modal knowledge distillation loss in <ref type="bibr" target="#b6">(7)</ref>. To study the effect of a different CKD loss, we show Dice score with L2 loss for ckd (.) in ( <ref type="formula" target="#formula_3">4</ref>), with Î± = 0.1. Compared with the L1 loss, we note that Dice decreases slightly with the L2 loss, especially for TC and WT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we introduced the Learnable Cross-modal Knowledge Distillation (LCKD), which is the first method that can handle missing modality during training and testing by distilling knowledge from automatically selected important modalities for all training tasks to train other modalities. Experiments on BraTS2018 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> show that LCKD reaches state-of-the-art performance in missing modality segmentation problems. We believe that our proposed LCKD has the potential to allow the use of multimodal data for training and missingmodality data per testing. One point to improve about LCKD is the greedy teacher selection per task. We plan to improve this point by transforming this problem into a meta-learning strategy, where the meta parameter is the weight for each modality, which will be optimised per task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. LCKD model framework for training and testing. The N modalities {x (i) Ni=1 } are processed by the encoder to produce the features {f (i) N i=1 }, which are concatenated and used by the decoder to produce the segmentation. The teacher is elected using a validation process that selects the top-performing modalities as teachers. Cross-modal distillation is performed by approximating the available students' features to the available teachers' features. Features from missing modalities are generated by averaging the other modalities' features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) LCKD-s segmenta on (b) LCKD-m segmenta on (c) GT segmenta on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Segmentation Visualisation with only T2 input available. Light grey, dark grey and white represent different tumour sub-regions. (Color figure online) Fig. 3. T1 Dice score as function of Î± (L1 loss for ckd (.) in (4)). Star markers show the Dice score for L2 loss for ckd (.) (Î± = .1). Colors denote different tumors.</figDesc><graphic coords="9,39,39,79,85,215,89,53,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Model performance comparison of segmentation Dice score (normalised to 100%) on BraTS2018. The best results for each row within tumour types are bolded. "â€¢" and "â€¢" indicate the availability and absence of the modality for testing, respectively. Last row shows p-value from one-tailed paired t-test.</figDesc><table><row><cell>Modalities</cell><cell cols="2">Enhancing Tumour</cell><cell></cell><cell>Tumour Core</cell><cell></cell><cell>Whole Tumour</cell></row><row><cell cols="7">Fl T1 T1c T2 HMIS HVED RSeg mmFm LCKD HMIS HVED RSeg mmFm LCKD HMIS HVED RSeg mmFm LCKD</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 11.78 23.80 25.69 39.33</cell><cell cols="2">45.48 26.06 57.90 53.57 61.21</cell><cell cols="2">72.01 52.48 84.39 85.69 86.10</cell><cell>89.45</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 10.16 8.60</cell><cell>17.29 32.53</cell><cell cols="2">43.22 37.39 33.90 47.90 56.55</cell><cell cols="2">66.58 57.62 49.51 70.11 67.52</cell><cell>76.48</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 62.02 57.64 67.07 72.60</cell><cell cols="2">75.65 65.29 59.59 76.83 75.41</cell><cell cols="2">83.02 61.53 53.62 73.31 72.22</cell><cell>77.23</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 25.63 22.82 28.97 43.05</cell><cell cols="2">47.19 57.20 54.67 57.49 64.20</cell><cell cols="2">70.17 80.96 79.83 82.24 81.15</cell><cell>84.37</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 10.71 27.96 32.13 42.96</cell><cell cols="2">48.30 41.12 61.14 60.68 65.91</cell><cell cols="2">74.58 64.62 85.71 88.24 87.06</cell><cell>89.97</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 66.10 68.36 70.30 75.07</cell><cell cols="2">78.75 71.49 75.07 80.62 77.88</cell><cell cols="2">85.67 68.99 85.93 88.51 87.30</cell><cell>90.47</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 30.22 32.31 33.84 47.52</cell><cell cols="2">49.01 57.68 62.70 61.16 69.75</cell><cell cols="2">75.41 82.95 87.58 88.28 87.59</cell><cell>90.39</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 66.22 61.11 69.06 74.04</cell><cell cols="2">76.09 72.46 67.55 78.72 78.59</cell><cell cols="2">82.49 68.47 64.22 77.18 74.42</cell><cell>80.10</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 32.39 24.29 32.01 44.99</cell><cell cols="2">50.09 60.92 56.26 62.19 69.42</cell><cell cols="2">72.75 82.41 81.56 84.78 82.20</cell><cell>86.05</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 67.83 67.83 69.71 74.51</cell><cell cols="2">76.01 76.64 73.92 80.20 78.61</cell><cell cols="2">84.85 82.48 81.32 85.19 82.99</cell><cell>86.49</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 68.54 68.60 70.78 75.47</cell><cell cols="2">77.78 76.01 77.05 81.06 79.80</cell><cell cols="2">85.24 72.31 86.72 88.73 87.33</cell><cell>90.50</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 31.07 32.34 36.41 47.70</cell><cell cols="2">49.96 60.32 63.14 64.38 71.52</cell><cell cols="2">76.68 83.43 88.07 88.81 87.75</cell><cell>90.46</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 68.72 68.93 70.88 75.67</cell><cell cols="2">77.48 77.53 76.75 80.72 79.55</cell><cell cols="2">85.56 83.85 88.09 89.27 88.14</cell><cell>90.90</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 69.92 67.75 70.10 74.75</cell><cell cols="2">77.60 78.96 75.28 80.33 80.39</cell><cell cols="2">84.02 83.94 82.32 86.01 82.71</cell><cell>86.73</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell cols="2">â€¢ 70.24 69.03 71.13 77.61</cell><cell cols="4">79.33 79.48 77.71 80.86 85.78 85.31 84.74 88.46 89.45 89.64</cell><cell>90.84</cell></row><row><cell>Average</cell><cell cols="2">46.10 46.76 51.02 59.85</cell><cell cols="2">63.46 62.57 64.84 69.78 72.97</cell><cell cols="2">78.96 74.05 79.16 84.39 82.94</cell><cell>86.70</cell></row><row><cell>p-value</cell><cell cols="3">5.3e-6 3.8e-7 8.6e-7 2.8e-5 -</cell><cell cols="2">3.7e-5 4.1e-7 7.2e-6 5.3e-7 -</cell><cell>1.1e-4 1.2e-3 1.1e-5 5.1e-7 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Different LCKD variants Dice score. LCKD-s and LCKD-m represent LCKD with single teacher and multi-teacher, respectively.</figDesc><table><row><cell>Modalities</cell><cell cols="4">Enhancing Tumour Tumour Core</cell><cell cols="2">Whole Tumour</cell></row><row><cell cols="7">Fl T1 T1c T2 LCKD-s LCKD-m LCKD-s LCKD-m LCKD-s LCKD-m</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 46.19</cell><cell>45.48</cell><cell>72.51</cell><cell>72.01</cell><cell>89.38</cell><cell>89.45</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 43.05</cell><cell>43.22</cell><cell>65.79</cell><cell>66.58</cell><cell>75.86</cell><cell>76.48</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 74.26</cell><cell>75.65</cell><cell>81.93</cell><cell>83.02</cell><cell>77.14</cell><cell>77.23</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 48.59</cell><cell>47.19</cell><cell>70.64</cell><cell>70.17</cell><cell>84.25</cell><cell>84.37</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 49.65</cell><cell>48.30</cell><cell>74.98</cell><cell>74.58</cell><cell>90.12</cell><cell>89.97</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 77.72</cell><cell>78.75</cell><cell>85.37</cell><cell>85.67</cell><cell>90.33</cell><cell>90.47</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 49.86</cell><cell>49.01</cell><cell>75.65</cell><cell>75.41</cell><cell>90.28</cell><cell>90.39</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 75.32</cell><cell>76.09</cell><cell>81.77</cell><cell>82.49</cell><cell>79.96</cell><cell>80.10</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 51.65</cell><cell>50.09</cell><cell>73.95</cell><cell>72.75</cell><cell>86.39</cell><cell>86.05</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 75.34</cell><cell>76.01</cell><cell>84.21</cell><cell>84.85</cell><cell>86.05</cell><cell>86.49</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 77.42</cell><cell>77.78</cell><cell>84.79</cell><cell>85.24</cell><cell>90.50</cell><cell>90.50</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 51.05</cell><cell>49.96</cell><cell>76.84</cell><cell>76.68</cell><cell>90.39</cell><cell>90.46</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 76.97</cell><cell>77.48</cell><cell>84.93</cell><cell>85.56</cell><cell>90.83</cell><cell>90.90</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 77.53</cell><cell>77.60</cell><cell>83.95</cell><cell>84.02</cell><cell>86.71</cell><cell>86.73</cell></row><row><cell>â€¢ â€¢ â€¢</cell><cell>â€¢ 78.39</cell><cell>79.33</cell><cell>85.26</cell><cell>85.31</cell><cell>90.74</cell><cell>90.84</cell></row><row><cell>Average</cell><cell>63.53</cell><cell>63.46</cell><cell>78.84</cell><cell>78.96</cell><cell>86.60</cell><cell>86.70</cell></row><row><cell>Best Teacher</cell><cell>T1c</cell><cell></cell><cell>T1c</cell><cell></cell><cell>Fl</cell><cell></cell></row><row><cell cols="7">function Ï†(.) in Eq. 2 to pick the modality with max appearance time (e.g. if we</cell></row><row><cell cols="7">have {T1c, T1c, Flair} then Ï†(.) returns {T1c}), while keeping other settings</cell></row><row><cell>the same.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">From Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We train one model to handle all of the different missing modality situations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Online evaluation is required at https://ipp.cbica.upenn.edu/.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nesterov&apos;s accelerated gradient and momentum as approximations to regularised update descent</title>
		<author>
			<persName><forename type="first">A</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1899" to="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal MR synthesis via modality-invariant latent representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="803" to="814" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust multimodal brain tumor segmentation via feature disentanglement and gated fusion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-950" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RFNet: region-aware fusion network for incomplete multi-modal brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3975" to="3984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hetero-modal variational encoder-decoder for joint modality completion and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dorent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joutard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-89" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unpaired multi-modal segmentation via knowledge distillation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2415" to="2425" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HeMIS: hetero-modal image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guizard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chapados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-854" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge Distillation from Multi-modal to Mono-modal Segmentation Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_75</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-875" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="772" to="781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio-visual deep learning for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7596" to="7599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incomplete cross-modal retrieval with dual-aligned variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3283" to="3291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Show, ask, attend, and answer: a strong baseline for visual question answering</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">SGDR: stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal integration learning of robot behavior using deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. Auton. Syst</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="736" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation on MRI with missing modalities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20351-1_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-20351-132" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2019</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11492</biblScope>
			<biblScope unit="page" from="417" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Soft expert reward learning for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-78" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="126" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Uncertainty-aware multi-modal learning via cross-modal random network prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.10851</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ACN: adversarial co-training network for brain tumor segmentation with missing modalities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-239" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unified subspace learning for incomplete and unlabeled multi-view data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">mmFormer: multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02425</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modality-aware mutual learning for multi-modal medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-256" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="589" to="599" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
