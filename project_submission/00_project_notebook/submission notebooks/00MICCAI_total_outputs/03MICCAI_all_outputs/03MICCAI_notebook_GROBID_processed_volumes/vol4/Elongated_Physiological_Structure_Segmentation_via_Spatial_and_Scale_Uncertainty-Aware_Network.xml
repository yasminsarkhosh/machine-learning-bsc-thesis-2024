<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network</title>
				<funder>
					<orgName type="full">A*STAR Advanced Manufacturing and Engineering (AME) Programmatic Fund (A20H4b0141) Central Research Fund</orgName>
					<orgName type="abbreviated">CRF</orgName>
				</funder>
				<funder ref="#_PkdYArX">
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
				<funder ref="#_WCxeNBJ">
					<orgName type="full">General Program of National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_3hEpPRc">
					<orgName type="full">Guangdong Provincial Department of Education</orgName>
				</funder>
				<funder ref="#_VGN892F">
					<orgName type="full">Shenzhen Natural Science Fund</orgName>
				</funder>
				<funder ref="#_B89jdH8">
					<orgName type="full">Stable Support Plan Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinglin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Nottingham Ningbo China</orgName>
								<address>
									<postCode>315100</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute of Trustworthy Autonomous Systems</orgName>
								<orgName type="institution" key="instit2">Southern University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruiling</forename><surname>Xi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute of Trustworthy Autonomous Systems</orgName>
								<orgName type="institution" key="instit2">Southern University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dave</forename><surname>Towey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Nottingham Ningbo China</orgName>
								<address>
									<postCode>315100</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruibin</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Nottingham Ningbo China</orgName>
								<address>
									<postCode>315100</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Risa</forename><surname>Higashita</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute of Trustworthy Autonomous Systems</orgName>
								<orgName type="institution" key="instit2">Southern University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tomey Corporation</orgName>
								<address>
									<postCode>451-0051</postCode>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jiang</forename><surname>Liu</surname></persName>
							<email>liuj@sustech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Nottingham Ningbo China</orgName>
								<address>
									<postCode>315100</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute of Trustworthy Autonomous Systems</orgName>
								<orgName type="institution" key="instit2">Southern University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tomey Corporation</orgName>
								<address>
									<postCode>451-0051</postCode>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="323" to="332"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A2E75C2E6A8E3C95B934949AD0FE96A2</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Uncertainty</term>
					<term>Medical Image Segmentation</term>
					<term>Elongated Physiological Structure</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust and accurate segmentation for elongated physiological structures is challenging, especially in the ambiguous region, such as the corneal endothelium microscope image with uneven illumination or the fundus image with disease interference. In this paper, we present a spatial and scale uncertainty-aware network (SSU-Net) that fully uses both spatial and scale uncertainty to highlight ambiguous regions and integrate hierarchical structure contexts. First, we estimate epistemic and aleatoric spatial uncertainty maps using Monte Carlo dropout to approximate Bayesian networks. Based on these spatial uncertainty maps, we propose the gated soft uncertainty-aware (GSUA) module to guide the model to focus on ambiguous regions. Second, we extract the uncertainty under different scales and propose the multiscale uncertainty-aware (MSUA) fusion module to integrate structure contexts from hierarchical predictions, strengthening the final prediction. Finally, we visualize the uncertainty map of final prediction, providing interpretability for segmentation results. Experiment results show that the SSU-Net performs best on cornea endothelial cell and retinal vessel segmentation tasks. Moreover, compared with counterpart uncertaintybased methods, SSU-Net is more accurate and robust.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Robust and accurate elongated physiological structure segmentation is crucial for computer-aided diagnosis and quantification of clinical parameters <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Manual delineation is tedious and laborious. Recently, deep learning-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> have been proposed to delineate targets automatically. However, they are not able to outline correctly in ambiguous regions where exist uneven illumination, artifacts, or interference from the disease.</p><p>Many researchers have tried to use uncertainty information to concentrate on the ambiguous region, and to evaluate the reliability of model's prediction. According to the source of prediction errors <ref type="bibr" target="#b2">[3]</ref>, uncertainty is categorized into two types: epistemic and aleatoric. The main methods for uncertainty estimation are as follows. Bayesian neural networks <ref type="bibr" target="#b14">[15]</ref> place a probability distribution over model weights, but are hard to optimize. Monte Carlo dropout <ref type="bibr" target="#b9">[10]</ref> approximates the Gaussian process by embedding the dropout operation into the neural network layers and calculating the variance of several times inference. Deep Ensembles <ref type="bibr" target="#b7">[8]</ref> combine the outputs from a group of independent models to estimate uncertainty. Softmax uncertainty <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> performs well in distinguishing examples that are easy or fallible to classify. Once the uncertainty information has been estimated, we are able to pay more attention to the ambiguous region. Xie et al. <ref type="bibr" target="#b23">[24]</ref> used the cross-attention module to extract influential features for ambiguous regions based on pixel-level uncertainty. Yang et al. <ref type="bibr" target="#b24">[25]</ref> achieved uncertainty awareness by training with a multi-confidence mask, and further used self-attention block with feature aware filter together to highlight uncertain areas. Wang et al. <ref type="bibr" target="#b22">[23]</ref> annotated alpha matte for medical images and used it as a soft label to intuitively promote the network to focus on uncertain areas. Kohl et al. <ref type="bibr" target="#b6">[7]</ref> proposed a generative model to produce multiple reasonable hypotheses for clinical experts to select from, which improved the diagnosis reliability. However, existing works applied the 'hard' attention to utilize uncertainty, which lacks the ability of adaptive adjustment and ignores neighboring uncertain regions. In addition, features at different scales contain rich structural and semantic contexts, which are essential for elongated physiological structure segmentation, such as cobweb corneal endothelial cells and retinal vessels.</p><p>This paper proposes a spatial and scale uncertainty-aware network (SSU-Net) for elongated physiological structure segmentation, which fully uses both spatial and scale uncertainty to highlight ambiguous regions and integrate hierarchical structure contexts. First, we use a gated soft uncertainty-aware (GSUA) module to adaptively highlight ambiguous areas based on spatial uncertainty maps. Second, we extract the uncertainty under different scales and propose the multi-scale uncertainty-aware (MSUA) fusion module to integrate hierarchical predictions for enhancing the final segmentation. Experiment results on segmentation tasks of the cornea endothelium and retinal vessel show the effectiveness of SSU-Net.   . The Bayesian approximate network has two outputs: segmentation prediction ŷ and the estimation of aleatoric uncertainty v. We can calculate the epistemic and aleatoric uncertainty maps, u e and u a , after multiple inferences. Furthermore, we consider the sigmoid probabilities of predictions under different scales as the second uncertainty source, and fuse the predictions {ŷ 1 , ŷ2 , ŷ3 } from multiple scales using the multi-scale uncertainty-aware (MSUA) module. ŷF is the final target output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spatial Uncertainty and Gated Soft Uncertainty-Aware Module</head><p>Spatial Uncertainty. Since the epistemic and aleatoric uncertainty maps are used to find the hard-to-classify spatial areas in this work, we regard them as spatial uncertainty. Referring to <ref type="bibr" target="#b5">[6]</ref>, we add dropout after each UNet block to approximate the Bayesian network, which learns the segmentation ỹ and aleatoric uncertainty v simultaneously. During inference, we sample a group of predictions {ỹ i } N i=1 and {v i } N i=1 by N stochastic forward pass. In this work, we set N = 16. The epistemic u e and aleatoric u a uncertainty are formulated by Eq. ( <ref type="formula">1</ref>), where y is the ground truth.</p><formula xml:id="formula_0">u e = 1 N N i=1 (ỹ i -y) 2 , u a = 1 N N i=1 v i (1)</formula><p>Gated Soft Uncertainty-Aware Module. To endow the uncertainty-aware module with adaptive adjustment ability, we propose the gated soft uncertaintyaware (GSUA) module, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. We extract salient descriptions from uncertainty maps by two parallel pooling [ψ avg , ψ max ] and a 1 × 1 convolution f (•) operation. The relu operation is set as a switch to filter out areas with small uncertainty values, further strengthening our attention on areas with high uncertainty. Since it is also usually difficult to classify the area adjacent to the high-uncertainty regions, we use the Gaussian kernel to soften the boundary in such regions. The GSUA module is formulated by:</p><formula xml:id="formula_1">x o = x i g s (σ(relu(f ([ψ avg (u), ψ max (u)])))) + x i (2)</formula><p>where x i , x o ∈ R N ×c×h×w are the input and output features respectively; u = [u a , u e ] ∈ R N ×2×H×W is a tensor of uncertainty maps; ψ avg and ψ max represent average and max pooling; σ is the sigmoid function; g s denotes a convolution operation with Gaussian kernel and resizes the attention maps to the size of input features; is element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scale Uncertainty and Multi-scale Uncertainty-Aware Module</head><p>Scale Uncertainty. To integrate the predictions from hierarchical layers during model training, we capture the uncertainty under multiple scales. The sigmoid function is a simple and effective way to estimate uncertainty for the binary classification task. We extract the multi-scale uncertainty by Equation (3), where u s is the uncertainty map of prediction ỹs under scale s ∈ {1, 2, 3}.</p><formula xml:id="formula_2">u s = 1 1 + e -ỹs<label>(3)</label></formula><p>Multi-scale Uncertainty-Aware Module. With the uncertainty maps from different scales, all the hierarchical predictions {ỹ 1 , ỹ2 , ỹ3 } are fused by the MSUA module to generate the enhanced prediction ỹF , as illustrate in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The uncertainty map u s provides the classification confidence for each pixel. Therefore, we use u s to highlight the confident region of ỹs and further extract the max value across the different scales. The process is formulated by:</p><formula xml:id="formula_3">ỹF (i, j) = max s∈{1,2,3} (y s (i, j) σ(y s (i, j)) + y s (i, j)) (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where ỹF (i, j) denotes the pixel value at location (i, j) of enhanced prediction; y s (i, j) is the value of prediction under scale s ∈ {1, 2, 3}; and σ denotes the sigmoid operation of Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Objective Function</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we optimize the model with supervision on four segmentation branches simultaneously, including supervision for predicting three scales and the final enhanced output. The loss function is summarized as follows:</p><formula xml:id="formula_5">L total = α 1 L s1 + α 2 L s2 + α 3 L s3 + α F L F (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where α 1 , α 2 , α 3 , and α F are the weight parameters for sub loss L s1 , L s2 , L s3 , and L F . In this experiment, we set all the weight parameters as 1. For these sub-losses, we adopt binary cross-entropy loss, as shown in Eq. ( <ref type="formula" target="#formula_7">6</ref>).</p><formula xml:id="formula_7">L = -[ylog ỹ + (1 -y)log(1 -ỹ)] (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where y is the ground truth; the positive class value of each pixel is 1; the negative class value is 0; and ỹ ∈ (0, 1) is the predicted probability value.</p><p>3 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Two cornea endothelium microscope image datasets, TM-EM3000 and Rodrep, and one retinal fundus image dataset, FIVES, are used in this work. The private dataset TM-EM3000 contains 183 images measured by EM3000 specular microscope (Tomey Corporation, Japan). Following Ruggeri et al. <ref type="bibr" target="#b19">[20]</ref>, we cropped a 192 × 192 pixels sub-region from its 260 × 480 pixels whole image. We used 155 images for model training, ten images for validation, and 18 images for testing. Rodrep <ref type="bibr" target="#b20">[21]</ref> contains 52 in-vivo confocal corneal microscope images, from 23 Fuchs patients with endothelial corneal dystrophy. We used 40 for training, five images for validation, and seven for testing. FIVES <ref type="bibr" target="#b4">[5]</ref> is the largest known high-resolution fundus image dataset: It covers normal eyes and three different eye diseases with a balanced distribution. There are 800 high-resolution images and the corresponding manual annotations, with 550 for training, 50 for validation, and 200 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics and Implementation Details</head><p>There is a class imbalance between foreground and background pixels. To better evaluate the segmentation performance, we choose Dice score <ref type="bibr" target="#b21">[22]</ref>, mIoU , and mAcc as evaluation metrics. We optimized the models using the RMSprop strategy with momentum = 0.9 and weight decay = 1e-8 for 100 epochs. The initial learning rate was 2e-4, and the input size of all networks was uniformly set to 256 × 256. Random shift and rescaling within a range of [-0.3, +0.3] were used for data augmentation. We set the batch size to 1 based on our empirical observations. For uncertainty-based models, we set the dropout rate as 0.5 and no data augmentation. During testing, we inferred N = 16 times and obtained the final</p><formula xml:id="formula_9">prediction ȳ = 1 N N i=1</formula><p>ỹi F and the epistemic uncertainty</p><formula xml:id="formula_10">u e = 1 N N i=1 (ỹ i F -ȳ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We investigated the influence of GSUA and MSUA modules on TM-EM3000, as shown in Table <ref type="table" target="#tab_0">1</ref>. The MSUA increased performance by 0.69% on the Dice score, and GSUA increased by 0.19%. The MSUA module brought more improvement than GSUA, which indicated that multi-scale context is crucial for cornea endothelium cell segmentation. When using both GSUA and MSUA modules simultaneously, we achieved the best performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with State-of-the-Art Methods</head><p>To study the effectiveness of the proposed SSU-Net, we compared it with a series of state-of-the-art methods. On TM-EM3000 and Rodrep, we implemented several popular networks for comparison: UNet <ref type="bibr" target="#b18">[19]</ref>, D-LinkNet <ref type="bibr" target="#b27">[28]</ref>, AttentionUNet <ref type="bibr" target="#b15">[16]</ref>, TransUNet <ref type="bibr" target="#b0">[1]</ref>, and uncertainty-based counterparts, Monte Carlo (MC) BayesianNet [2], Lee's method <ref type="bibr" target="#b8">[9]</ref>. On the fundus image dataset FIVES, we additionally implemented several recent retinal vessel segmentation algorithms: FR-UNet <ref type="bibr" target="#b11">[12]</ref>, SA-UNet <ref type="bibr" target="#b3">[4]</ref>, and IterNet <ref type="bibr" target="#b10">[11]</ref>.</p><p>As shown in Table <ref type="table" target="#tab_1">2</ref>, the proposed SSU-Net achieved the best performance. On the TM-EM3000 dataset, the uncertainty-based methods outperformed the typical convolution and attention methods, which proves that introducing uncertainty is beneficial. On the Rodrep dataset, SSU-Net performed considerably better than Lee's uncertainty method, improving the Dice score by 4.98%, mIoU by 3.48%, and mAcc by 3.14%. The results further suggest that the multi-scale predictions fusion module is crucial to elevate the robustness. According to the indication of uncertainty map u e , we cropped and zoomed in two ambiguous regions of each image, as shown in Fig. <ref type="figure">2</ref>. The visualization results suggested that the proposed SSU-Net effectively improved the segmentation performance in ambiguous regions. On the FIVES dataset, the performance of the specialized network for retinal blood vessel segmentation in the fundus was similar to that of UNet, TransUNet, and AttettionUNet. The uncertainty-based methods are uniformly significantly superior to the above methods. The proposed SSU-Net achieved the best performance, increasing the Dice score by 10.08%, mIoU by 7.29%, and mAcc by 7.51% compared with UNet. Qualitative analysis is shown in Fig. <ref type="figure">3</ref>, further supporting the conclusions of quantitative analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes a spatial and scale uncertainty-aware network (SSU-Net) for elongated physiological structure segmentation. The ablation study shows the effectiveness of core components: the soft gated uncertainty-aware (GSUA) and the multi-scale uncertainty-aware (MSUA) fusion modules. Compared with some SOTA methods on cornea endothelial cell and retinal vessel image segmentation tasks, the proposed SSU-Net achieved the best segmentation performance and is more robust than other uncertainty-based methods. It is noteworthy that the SSU-Net performed considerably better than specialized retinal vessel segmentation networks. In the future, we plan to conduct experiments on various challenging situations to further explore the characteristics of SSU-Net.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The pipeline of the proposed algorithm. (a) The framework of spatial and scale uncertainty-aware network, SSU-Net. (b) We estimate the spatial uncertainty maps with Bayesian approximate network.</figDesc><graphic coords="3,62,46,54,20,327,70,171,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1 (a) illustrates the framework of the proposed spatial and scale uncertainty-aware network, SSU-Net. The gated soft uncertainty-aware (GSUA) module enables the network to focus on the ambiguous region indicated by the spatial uncertainty maps [u e , u a ]. Specifically, we construct a Bayesian approximate network to generate spatial uncertainty maps by introducing Monte Carlo dropout<ref type="bibr" target="#b1">[2]</ref> into U-Net, as shown in Fig.1 (b). The Bayesian approximate network has two outputs: segmentation prediction ŷ and the estimation of aleatoric uncertainty v. We can calculate the epistemic and aleatoric uncertainty maps, u e and u a , after multiple inferences. Furthermore, we consider the sigmoid probabilities of predictions under different scales as the second uncertainty source, and fuse the predictions {ŷ 1 , ŷ2 , ŷ3 } from multiple scales using the multi-scale uncertainty-aware (MSUA) module. ŷF is the final target output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Visualization for cornea endothelial cell segmentation. Red, green, and yellow line presents manual label, predicted segmentation result, and their overlap region. Indicated by the uncertainty map u e of the SSU-Net, we zoomed in two ambiguous local regions for clear observation. (Color figure online)</figDesc><graphic coords="7,73,98,430,58,304,99,104,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study on TM-EM3000. GSUA denotes the gated soft uncertaintyaware module. MSUA means the multi-scale uncertainty-aware fusion module.</figDesc><table><row><cell cols="4">Models GSUA MSUA Dice(%) ↑ mIoU(%) ↑ mAcc(%) ↑</cell></row><row><cell>Variant1</cell><cell>76.04</cell><cell>76.32</cell><cell>85.52</cell></row><row><cell>Variant2</cell><cell>76.73</cell><cell>76.68</cell><cell>87.38</cell></row><row><cell>Variant3</cell><cell>76.23</cell><cell>76.42</cell><cell>86.08</cell></row><row><cell>Variant4</cell><cell>77.16</cell><cell>77.02</cell><cell>87.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of SSU-Net with some SOTA methods on cornea endothelial cell and retinal vessel segmentation tasks.</figDesc><table><row><cell>Dataset</cell><cell>Models</cell><cell cols="3">Dice (%) ↑ mIoU (%) ↑ mAcc (%) ↑</cell></row><row><cell cols="2">TM-EM3000 UNet</cell><cell>71.28</cell><cell>72.67</cell><cell>81.34</cell></row><row><cell></cell><cell>D-LinkNet</cell><cell>72.71</cell><cell>73.60</cell><cell>82.67</cell></row><row><cell></cell><cell cols="2">AttentionUNet 72.29</cell><cell>73.46</cell><cell>82.06</cell></row><row><cell></cell><cell>TransUNet</cell><cell>71.65</cell><cell>72.78</cell><cell>82.20</cell></row><row><cell></cell><cell>BayesianNet</cell><cell>75.42</cell><cell>75.71</cell><cell>85.66</cell></row><row><cell></cell><cell>Lee's</cell><cell>76.40</cell><cell>76.53</cell><cell>85.80</cell></row><row><cell></cell><cell>SSU-Net</cell><cell>77.16</cell><cell>77.02</cell><cell>87.34</cell></row><row><cell>Rodrep</cell><cell>UNet</cell><cell>64.89</cell><cell>67.89</cell><cell>78.50</cell></row><row><cell></cell><cell>D-LinkNet</cell><cell>65.40</cell><cell>68.22</cell><cell>79.21</cell></row><row><cell></cell><cell cols="2">AttentionUNet 60.55</cell><cell>65.68</cell><cell>74.45</cell></row><row><cell></cell><cell>TransUNet</cell><cell>66.56</cell><cell>68.87</cell><cell>80.43</cell></row><row><cell></cell><cell>BayesianNet</cell><cell>65.62</cell><cell>68.15</cell><cell>80.24</cell></row><row><cell></cell><cell>Lee's</cell><cell>63.51</cell><cell>66.62</cell><cell>79.44</cell></row><row><cell></cell><cell>SSU-Net</cell><cell>68.49</cell><cell>70.10</cell><cell>82.58</cell></row><row><cell>FIVES</cell><cell>UNet</cell><cell>78.99</cell><cell>82.59</cell><cell>86.05</cell></row><row><cell></cell><cell>D-LinkNet</cell><cell>73.45</cell><cell>78.03</cell><cell>82.89</cell></row><row><cell></cell><cell cols="2">AttentionUNet 78.09</cell><cell>81.82</cell><cell>85.26</cell></row><row><cell></cell><cell>TransUNet</cell><cell>80.81</cell><cell>83.19</cell><cell>88.18</cell></row><row><cell></cell><cell>FR-UNet</cell><cell>78.47</cell><cell>82.09</cell><cell>85.51</cell></row><row><cell></cell><cell>SA-UNet</cell><cell>79.15</cell><cell>82.05</cell><cell>88.27</cell></row><row><cell></cell><cell>IterNet</cell><cell>79.32</cell><cell>82.54</cell><cell>85.56</cell></row><row><cell></cell><cell>BayesianNet</cell><cell>88.70</cell><cell>89.65</cell><cell>93.01</cell></row><row><cell></cell><cell>Lee's</cell><cell>88.79</cell><cell>89.70</cell><cell>93.37</cell></row><row><cell></cell><cell>SSU-Net</cell><cell>89.07</cell><cell>89.88</cell><cell>93.56</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by <rs type="funder">General Program of National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">82272086</rs>), <rs type="funder">Guangdong Provincial Department of Education</rs> (Grant No. <rs type="grantNumber">2020ZDZX3043</rs>), <rs type="funder">Shenzhen Natural Science Fund</rs> (<rs type="grantNumber">JCYJ20200109140820699</rs>), the <rs type="funder">Stable Support Plan Program</rs> (<rs type="grantNumber">20200925174052004</rs>), the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="programName">AI Singapore Programme (AISG Award</rs> No: <rs type="grantNumber">AISG2-TC-2021-003</rs>), and <rs type="funder">A*STAR Advanced Manufacturing and Engineering (AME) Programmatic Fund (A20H4b0141) Central Research Fund (CRF)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WCxeNBJ">
					<idno type="grant-number">82272086</idno>
				</org>
				<org type="funding" xml:id="_3hEpPRc">
					<idno type="grant-number">2020ZDZX3043</idno>
				</org>
				<org type="funding" xml:id="_VGN892F">
					<idno type="grant-number">JCYJ20200109140820699</idno>
				</org>
				<org type="funding" xml:id="_B89jdH8">
					<idno type="grant-number">20200925174052004</idno>
				</org>
				<org type="funding" xml:id="_PkdYArX">
					<idno type="grant-number">AISG2-TC-2021-003</idno>
					<orgName type="program" subtype="full">AI Singapore Programme (AISG Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Transunet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>ICML. Machine Learning Research<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016-06-22">20-22 Jun 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gawlikowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03342</idno>
		<title level="m">A survey of uncertainty in deep neural networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sa-unet: spatial attention u-net for retinal vessel segmentation</title>
		<author>
			<persName><forename type="first">Changlu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fives: a fundus image dataset for artificial intelligence based vessel segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">475</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in Bayesian deep learning for computer vision?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A probabilistic u-net for segmentation of ambiguous images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of NeurIPS</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Method to minimize the errors of AI: quantifying and exploiting uncertainty of deep learning in brain tumor segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2406</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Leveraging uncertainty information from deep neural networks for disease detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Leibig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iternet: Retinal image segmentation utilizing structural redundancy in vessel networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3656" to="3665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Full-resolution network and dual-threshold iteration for retinal vessel and coronary angiograph segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4623" to="4634" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Confidence calibration and predictive uncertainty estimation for deep medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrtash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3868" to="3878" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cs2-net: deep learning segmentation of curvilinear structures in medical imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101874</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Attention u-net: learning where to look for the pancreas</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Pearce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04972</idno>
		<title level="m">Understanding softmax confidence and uncertainty</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic play segmentation of hockey videos</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pidaparthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4585" to="4593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A system for the automatic estimation of morphometric parameters of corneal endothelium in Alizarine red-stained images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruggeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="643" to="647" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully automatic evaluation of the corneal endothelium from in vivo confocal microscopy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Selig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Medical matting: a new perspective on medical segmentation with uncertainty</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="573" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uncertainty-aware cascade network for ultrasound image segmentation with ambiguous boundary</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Uncertainty-guided lung nodule segmentation with feature-aware attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A multi-branch hybrid transformer network for corneal endothelial cell segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automated tortuosity analysis of nerve fibers in corneal confocal microscopy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2725" to="2737" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">D-linknet: linknet with pretrained encoder and dilated convolution for high resolution satellite imagery road extraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="182" to="186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
