<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation</title>
				<funder ref="#_fMD6vQ8">
					<orgName type="full">NSFC China</orgName>
				</funder>
				<funder ref="#_wHJMZJF #_PVypgpW">
					<orgName type="full">Shanghai Municipal of Science and Technology Project</orgName>
				</funder>
				<funder ref="#_pXYAhtn #_9j8cxqF">
					<orgName type="full">Committee of Science and Technology, Shanghai, China</orgName>
				</funder>
				<funder ref="#_QXdUmCr">
					<orgName type="full">Open Funding of Zhejiang Laboratory</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuncheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Processing and Pattern Recognition</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junjun</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Processing and Pattern Recognition</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Ye</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yun</forename><surname>Gu</surname></persName>
							<email>yungu@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Image Processing and Pattern Recognition</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Medical Robotics</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="674" to="683"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">50757A33754256707057D8907EACA3BD</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_64</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Transferability Estimation</term>
					<term>Model Selection</term>
					<term>Medical Image Analysis</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning is a critical technique in training deep neural networks for the challenging medical image segmentation task that requires enormous resources. With the abundance of medical image data, many research institutions release models trained on various datasets that can form a huge pool of candidate source models to choose from. Hence, it's vital to estimate the source models' transferability (i.e., the ability to generalize across different downstream tasks) for proper and efficient model reuse. To make up for its deficiency when applying transfer learning to medical image segmentation, in this paper, we therefore propose a new Transferability Estimation (TE) method. We first analyze the drawbacks of using the existing TE algorithms for medical image segmentation and then design a source-free TE framework that considers both class consistency and feature variety for better estimation. Extensive experiments show that our method surpasses all current algorithms for transferability estimation in medical image segmentation. Code is available at here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The development of deep neural networks has greatly promoted medical imagingbased computer-aided diagnosis. Due to the large amount of learnable parameters in neural networks, sufficient annotated training samples are required for training. However, the labeling process of medical images is tedious and timeconsuming. To address this problem, the common paradigm of transfer learning, which first pre-trains a model on upstream image datasets and then fine-tunes it on various target tasks, has been widely investigated in recent years <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>. Compared with the distributed training across multiple centers, there are no specific ethical issues or computational design of distributed/federated learning frameworks with the "pre-train-then-fine-tune" workflow.</p><p>Previous works mainly focused on the fine-tuning strategy to effectively adapt the knowledge from the pre-trained models to target tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>. With the increasing number of pre-trained networks provided by the community, model repositories like Hugging Face <ref type="bibr" target="#b24">[25]</ref> and PyTorch Hub <ref type="bibr" target="#b17">[18]</ref> enable researchers to experiment across a large number of downstream datasets and tasks. These pre-trained models require less training time and have better performance and robustness compared with the learning-from-scratch models. However, it has been observed by recent works <ref type="bibr" target="#b22">[23]</ref> that the pre-trained models cannot always benefit the downstream tasks. When the knowledge is transferred from a less relevant source, it may not improve the performance or even negatively affect the intended outcome <ref type="bibr" target="#b23">[24]</ref>. A brute-force method is to fine-tune a set of pretrained models with target datasets to find the optimal one. This process is timeconsuming and laborious. Existing methods also measured the task-relatedness between source and target datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref>. However, most of these works require source information available while medical images have more privacy and ethical issues and fewer datasets are publicly available than natural images.</p><p>Considering the issues mentioned above, this work focused on source-free pre-trained model selection for segmentation tasks in the medical image. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, models pre-trained by upstream data constitute the model bank. The main idea is to directly measure the transferability of the pre-trained models without fully training based on the downstream/target dataset. Among the recent works, LEEP <ref type="bibr" target="#b14">[15]</ref> and its variant <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> were developed to utilize the loglikelihood between the target labels and the predictions from the source model. LogME <ref type="bibr" target="#b26">[27]</ref> computed evidence based on the linear parameters assumption and efficiently leverages the compatibility between features and labels. GBC <ref type="bibr" target="#b16">[17]</ref> applied the Gaussian distribution to each class, and estimate the separability between classes as the basis for transferability estimation. TransRate <ref type="bibr" target="#b8">[9]</ref> evaluated the transferability of models with the compactness and the completeness of embedding space. Cui et.al <ref type="bibr" target="#b4">[5]</ref> contended that discriminability and transferability are crucial properties of representations and introduce the information bottleneck theory for transferability estimation. These methods have achieved promising performance on classification and regression tasks without fully considering the properties of medical image segmentation. First, unlike classification and regression problems that can use a single n-dimensional feature vector to represent each image, segmentation problems lack a global semantic representation, which poses difficulties for direct transferability estimation. In addition, most label-comparison-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref> focus on the relationship between the embeddings and downstream labels without exploring the effectiveness of the features themselves. Third, medical images face severe class imbalance problems, with excessive differences between foreground and background. However, existing algorithms rarely give additional attention to the class imbalance problem. Besides, for semantic segmentation tasks, the feature pyramid is critical for the segmentation output of multi-scale objects while existing works neglect it.</p><p>In our work, we propose a new method using class consistency and feature variety(CC-FV) with an efficient framework to estimate the transferability in medical image segmentation tasks. Class consistency employs the distribution of features extracted from foreground voxels of the same category in each sample to model and calculate their distance, the smaller the distance the better the result; feature diversity utilizes features sampled in the whole global feature map, and the uniformity of the feature distribution obtained by sampling is used to measure the effectiveness of the features themselves. Extensive experiments have proved the superiority of our method compared with baseline methods.</p><p>2 Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>In our work, a model bank M consisting of pre-trained models {M i } K i=1 are available to be fine-tuned and evaluated with a target dataset</p><formula xml:id="formula_0">D = {X j , Y j } N j=1</formula><p>, where X j is the image and Y j is the ground truth of segmentation. After fine-tuning, the performance of M i can be measured with the segmentation metric (e.g. Dice score), which is denoted by P i s→t in this paper. Our work is to directly estimate the transferability score T i s→t without fine-tuning the model on target datasets. A perfect transferability score should preserve the ordering, i.e. T i s→t &gt; T j s→t if and only if P i s→t &gt; P j s→t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Class Consistency with Feature Variety Constraint TE Method</head><p>The transferability of models from a weakly related source domain to a target domain can be compromised if the domains are not sufficiently comparable <ref type="bibr" target="#b23">[24]</ref>. This intrigues us about the question of "what kind of models are transferable". The proposed method is intuitive and straightforward: features extracted by the pre-trained model should be consistent within the class of the target dataset while representative and various globally. Therefore, Class Consistency and Feature Variety are considered to estimate the transferability between models and downstream data.</p><p>Class Consistency. The pre-trained models are trained with specific pretext tasks based on the upstream dataset. Therefore, features extracted by the pretrained models cannot perfectly distinguish the foreground and background of target data. If the features are generalizable, foreground region features will likely follow a similar distribution even without fine-tuning. Given a pair of target data X j and X j , the distribution of the features is modeled with the n-dimensional Gaussian distribution. Since the size of the foreground class varies across the cases, we therefore randomly sample the pixels/voxels of X j and X j for each class and establish the feature distribution F k j , F k j based on the voxels of the k th class to approximate the case-wise distribution of different classes. The class consistency between the data pair is measured by the Wasserstein distance <ref type="bibr" target="#b15">[16]</ref> as follows:</p><formula xml:id="formula_1">W 2 2 (F k j , F k j ) = µ F k j -µ F k j 2 + Tr Σ F k j + Tr Σ F k j -2 Tr Σ F k j Σ F k j 1/2<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">μ F k j , μ F k j are the mean of Gaussian distribution F k j , F k j and Σ F k j and Σ F k j</formula><p>are covariance matrices of F k j and F k j . Compared to some commonly used metrics like KL-divergence or Bhattacharyya distance <ref type="bibr" target="#b16">[17]</ref>, Wasserstein distance is more stable during the computation of high-dimensional matrices because it is unnecessary to compute the determinant or inverse of a high-dimensional matrix, which can easily lead to an overflow in numerical computation. We calculate the Wasserstein distance of the distribution with voxels of the same class in a sample pair comprised of every two samples in the dataset, and obtained the following definition of class consistency C cons</p><formula xml:id="formula_3">C cons = 1 N(N -1) C k=1 i j W 2 (F k i , F k j )<label>(2)</label></formula><p>Given that 3D medical images are computationally intensive, and prone to causing out-of-memory problems, in the sliding window inference process for each case, we do not concatenate the output of each patch into the final prediction result, but directly sample from the patched output and concatenate them into the final sampled feature matrix. In the calculation of class consistency, we only sample the foreground voxels with a pre-defined sampling number which is proportional to the voxel number of each class in the image because of the severe class imbalance problem.</p><p>Feature Variety. Class consistency is not the only criterion for transferability estimation. As a result of learning some trivial solutions, some overfitted models have limited generalization capacity and are difficult to apply to new tasks. We believe that the essential reason for this phenomenon is that class consistency is only concerned with local homogeneity of information while neglecting the integral feature quality assessment. Hence we propose the feature variety constraint, which measures the expressiveness of the features themselves and the uniformity of their probability distribution. Highly complex features are not easily overfitted in the downstream tasks and do not collapse to cause a trivial solution.</p><p>To calculate the variety of features we need to analytically measure the properties of the feature distribution over the full feature space. Besides, to prevent overfitting and trivial features, we expect the distribution of features in the feature space to be as uniform and dispersed as possible. Therefore we employ the following hyperspherical potential energy E s as</p><formula xml:id="formula_4">E s (v) = L i=1 L j=1, j i e s v i -v j = i j v i -v j -s , s &gt; 0 i j log v i -v j -1 , s = 0<label>(3)</label></formula><p>Here v is sampled feature of each image with point-wise embedding v i and L is the length of the feature, which is also the number of sampled voxels. We randomly sample from the whole case so that the features can better express the overall representational power of the model. The feature vectors will be more widely dispersed in the unit sphere if the hyperspherical energy (HSE) is lower <ref type="bibr" target="#b2">[3]</ref>. For the dataset with N cases, we choose s = 1 and the feature variety F v is formulated as</p><formula xml:id="formula_5">F v = 1 N N i=1 E -1 s (v)<label>(4)</label></formula><p>Overall Estimation. As for semantic segmentation problems, the feature pyramid structure is critical for segmentation results <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>. Hence in our framework, different decoders' outputs are upsampled to the size of the output and can be used in the sliding window sampling process. Besides, we decrease the sampling ratio in the decoder layer close to the bottleneck to avoid feature redundancy.</p><p>The final transferability of pre-trained model m to dataset t T m→t is</p><formula xml:id="formula_6">T m→t = 1 D D i=1 log F i v C i cons (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where D is the number of decoder layers used in the estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment on MSD Dataset</head><p>The Medical Segmentation Decathlon (MSD) <ref type="bibr" target="#b1">[2]</ref> dataset is composed of ten different datasets with various challenging characteristics, which are widely used in the medical image analysis field. To evaluate the effectiveness of CC-FV, we conduct extensive experiments on 5 of the MSD dataset, including Task03 Liver(liver and tumor segmentation), Task06 Lung(lung nodule segmentation), Task07 Pancreas(pancreas and pancreas tumor segmentation), Task09 Spleen(spleen segmentation), and Task10 Colon(colon cancer segmentation). All of the datasets are 3D CT images. The public part of the MSD dataset is chosen for our experiments, and each dataset is divided into a training set and a test set at a scale of 80% and 20%. For each dataset, we use the other four datasets to pre-train the model and fine-tune the model on this dataset to evaluate the performance as well as the transferability using the correlation between two ranking sequences of upstream pre-trained models. We load all the pre-trained models' parameters except for the last convolutional layer and no parameters are frozen during the fine-tuning process. On top of that, we follow the nnUNet <ref type="bibr" target="#b10">[11]</ref> with the selfconfiguring method to choose the pre-processing, training, and post-processing strategy. For fair comparisons, the baseline methods including TransRate <ref type="bibr" target="#b8">[9]</ref>, LogME <ref type="bibr" target="#b26">[27]</ref>, GBC <ref type="bibr" target="#b16">[17]</ref> and LEEP <ref type="bibr" target="#b14">[15]</ref> are also implemented. For these currently available methods, we employ the output of the layer before the final convolution as the feature map and sample it through the same sliding window as CC-FV to obtain different classes of features, which can be used for the calculation. Figure <ref type="figure">2</ref> visualizes the average Dice score and the estimation value on Task 03 Liver. The TE results are obtained from the training set only. U-Net <ref type="bibr" target="#b19">[20]</ref> and UNETR <ref type="bibr" target="#b7">[8]</ref> are applied in the experiment and each model is pre-trained for 250k iterations and fine-tuned for 100k iterations with batch size 2 on a single NVIDIA A100 GPU. Besides, we use the model at the end of training for inference and calculate the final DSC performance on the test set. And we use weighted Kendall's τ <ref type="bibr" target="#b26">[27]</ref> and Pearson correlation coefficient for the correlation between the TE results and fine-tuning performance. The Kendall's τ ranges from [-1, 1], and τ=1 means the rank of TE results and performance are perfectly correlated(T i s→t &gt; T j s→t if and only if P i s→t &gt; P j s→t ). Since model selection generally picks the top models and ignores the poor performers, we assign a higher weight to the good models in the calculation, known as weighted Kendall's τ. The Pearson coefficient also ranges from [-1, 1], and measures how well the data can be described by a linear equation. The higher the Pearson coefficient, the higher the correlation between the variables. It is clear that the TE results of our method have a more positive correlation with respect to DSC performance.</p><p>Table <ref type="table" target="#tab_0">1</ref> demonstrates that our method surpasses all the other methods. Most of the existing methods are inferior to ours because they are not designed for segmentation tasks with a serious class imbalance problem. Besides, these methods rely only on single-layer features and do not make good use of the hierarchical structure of the model.  <ref type="figure">2</ref>. Correlation between the fine-tuning performance and transferability metrics using Task03 as an example. The vertical axis represents the average Dice of the model, while the horizontal axis represents the transferability metric results. We have standardized the various metrics uniformly, aiming to observe a positive relationship between higher performance and higher transferability estimations.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>In Table <ref type="table" target="#tab_1">2</ref> we analyze the different parts of our method and compare some other methods. First, we analyze the impact of class consistency C cons and feature variety F v . Though F v can not contribute to the final Kendall's τ directly, C cons with the constraint of F v promotes the total estimation result. Then we compare the performance of our method at single and multiple scales to prove the effectiveness of our multi-scale strategy. Finally, we change the distance metrics in class consistency estimation. KL-divergence and Bha-distance are unstable in high dimension matrics calculation and the performance is also inferior to the Wasserstein distance. Figure <ref type="figure" target="#fig_2">3</ref> visualize the distribution of different classes using t-SNE methods. We can easily find that with models with a pre-training process have a more compact intra-class distance and a higher fine-tuning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In our work, we raise the problem of model selection for upstream and downstream transfer processes in the medical image segmentation task and analyze the practical implications of this problem. In addition, due to the ethical and privacy issues inherent in medical care and the computational load of 3D image segmentation tasks, we design a generic framework for the task and propose a transferability estimation method based on class consistency with feature variety constraint, which outperforms existing model transferability estimation methods as demonstrated by extensive experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Source-free model selection problem and the framework of our Class Consistency with Feature Variety constraint(CC-FV) TE method. Our main goal is to predict the performance of models in the model bank after fine-tuning on downstream tasks without actually fine-tuning. Note that the upstream data are not available in our model selection process.</figDesc><graphic coords="3,43,80,54,11,336,40,202,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.</head><label></label><figDesc>Fig.2. Correlation between the fine-tuning performance and transferability metrics using Task03 as an example. The vertical axis represents the average Dice of the model, while the horizontal axis represents the transferability metric results. We have standardized the various metrics uniformly, aiming to observe a positive relationship between higher performance and higher transferability estimations.</figDesc><graphic coords="7,49,29,54,05,325,60,249,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Visualization of features with same labels using t-SNE. Points with different colors are from different samples. Pre-trained models tend to have a more consistent distribution within a class than the randomly initialized model and after fine-tuning they often have a better Dice performance than the randomly initialized models.</figDesc><graphic coords="8,58,47,53,96,335,56,135,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Pearson coefficient and weighted Kendall's τ for transferability estimation</figDesc><table><row><cell cols="3">Data/Method Metrics Task03 Task06 Task07 Task09 Task10 Avg</cell></row><row><cell>LogME</cell><cell>τ</cell><cell>-0.1628 -0.0988 0.3280 0.2778 -0.2348 0.0218</cell></row><row><cell></cell><cell cols="2">pearson 0.0412 0.5713 0.3236 0.2725 -0.1674 0.2082</cell></row><row><cell>TransRate</cell><cell>τ</cell><cell>-0.1843 -0.1028 0.5923 0.4322 0.6069 0.2688</cell></row><row><cell></cell><cell cols="2">pearson -0.5178 -0.2804 0.7170 0.5573 0.7629 0.2478</cell></row><row><cell>LEEP</cell><cell>τ</cell><cell>0.6008 0.1658 0.2691 0.3516 0.5841 0.3943</cell></row><row><cell></cell><cell cols="2">pearson 0.6765 -0.0073 0.7146 0.1633 0.4979 0.4090</cell></row><row><cell>GBC</cell><cell>τ</cell><cell>0.1233 -0.1569 0.6637 0.7611 0.6643 0.4111</cell></row><row><cell></cell><cell cols="2">pearson -0.2634 -0.3733 0.7948 0.7604 0.7404 0.3317</cell></row><row><cell cols="2">Ours CC-FV τ</cell><cell>0.6374 0.0735 0.6569 0.5700 0.5550 0.4986</cell></row><row><cell></cell><cell cols="2">pearson 0.8608 0.0903 0.9609 0.7491 0.8406 0.7003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation on the effectiveness of different parts in our methods</figDesc><table><row><cell>Data/Method</cell><cell>Task03 Task06 Task07 Task09 Task10 Avg</cell></row><row><cell>Ours CC-FV</cell><cell>0.6374 0.0735 0.6569 0.5700 0.5550 0.4986</cell></row><row><cell cols="2">Ours w/o C cons 0.1871 -0.2210 -0.2810 -0.0289 -0.2710 -0.1230</cell></row><row><cell>Ours w/o F v</cell><cell>0.6165 0.3235 0.6054 0.2761 0.5269 0.4697</cell></row><row><cell>Single-scale</cell><cell>0.4394 0.0252 0.5336 0.5759 0.6007 0.4341</cell></row><row><cell cols="2">KL-divergence -0.5658 -0.0564 0.2319 0.4628 -0.0323 0.0080</cell></row><row><cell>Bha-distance</cell><cell>0.1808 0.0723 0.2295 0.7866 0.4650 0.3468</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">Open Funding of Zhejiang Laboratory</rs> under Grant <rs type="grantNumber">2021KH0AB03</rs>, <rs type="funder">NSFC China</rs> (No. <rs type="grantNumber">62003208</rs>); <rs type="funder">Committee of Science and Technology, Shanghai, China</rs> (No.<rs type="grantNumber">19510711200</rs>); <rs type="programName">Shanghai Sailing Program</rs> (<rs type="grantNumber">20YF1420800</rs>), and <rs type="funder">Shanghai Municipal of Science and Technology Project</rs> (Grant No.<rs type="grantNumber">20JC1419500</rs>, No. <rs type="grantNumber">20DZ2220400</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QXdUmCr">
					<idno type="grant-number">2021KH0AB03</idno>
				</org>
				<org type="funding" xml:id="_fMD6vQ8">
					<idno type="grant-number">62003208</idno>
				</org>
				<org type="funding" xml:id="_pXYAhtn">
					<idno type="grant-number">19510711200</idno>
					<orgName type="program" subtype="full">Shanghai Sailing Program</orgName>
				</org>
				<org type="funding" xml:id="_9j8cxqF">
					<idno type="grant-number">20YF1420800</idno>
				</org>
				<org type="funding" xml:id="_wHJMZJF">
					<idno type="grant-number">20JC1419500</idno>
				</org>
				<org type="funding" xml:id="_PVypgpW">
					<idno type="grant-number">20DZ2220400</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_64.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Transferability metrics for selecting source model ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7936" to="7946" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The medical segmentation decathlon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4128</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02290</idno>
		<title level="m">Contrastive syn-to-real generalization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting meets negative transfer: batch spectral shrinkage for safe transfer learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Discriminability-transferability trade-off: an information-theoretic perspective</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19809-0_2</idno>
		<idno>978- 3-031-19809-0_2</idno>
		<ptr target="https://doi.org/10.1007/" />
		<editor>ECCV</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="20" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Duality diagram similarity: a generic framework for initialization selection in task transfer learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58574-7_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58574-7_30" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12371</biblScope>
			<biblScope unit="page" from="497" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Representation similarity analysis for efficient task taxonomy &amp; transfer learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12387" to="12396" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unetr: transformers for 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Frustratingly easy transferability estimation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9201" to="9225" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">nnu-net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Delta: deep learning transfer using feature map with attention for convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09229</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ranking neural checkpoints</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2663" to="2673" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Leep: a new measure to evaluate transferability of learned representations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Archambeau</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7294" to="7305" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical aspects of wasserstein distances</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Rev. Stat. Appli</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="405" to="431" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transferability estimation using bhattacharyya class separability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pándy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9172" to="9182" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Panda: adapting pretrained features for anomaly detection and segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2806" to="2814" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for medical image analysis: full training or fine tuning?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1312" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A mathematical framework for quantifying transferability in multi-source transfer learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="26103" to="26116" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Characterizing and avoiding negative transfer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11293" to="11302" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Explicit inductive bias for transfer learning with convolutional networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xuhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Davoine</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2825" to="2834" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Logme: practical assessment of pre-trained models for transfer learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12133" to="12143" />
		</imprint>
		<respStmt>
			<orgName>ICM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Taskonomy: disentangling task transfer learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Active, continual fine tuning of convolutional neural networks for reducing annotation efforts</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">101997</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
