<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured State Space Models for Multiple Instance Learning in Digital Pathology</title>
				<funder>
					<orgName type="full">CNRS</orgName>
				</funder>
				<funder>
					<orgName type="full">Région Île-de-France</orgName>
				</funder>
				<funder ref="#_4UEr6cJ #_bZMV4Zk">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Leo</forename><surname>Fillioux</surname></persName>
							<email>leo.fillioux@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MICS Laboratory</orgName>
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>CentraleSupélec, Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">MICS Laboratory</orgName>
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>CentraleSupélec, Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Boyd</surname></persName>
							<email>joseph.boyd@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MICS Laboratory</orgName>
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>CentraleSupélec, Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">MICS Laboratory</orgName>
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>CentraleSupélec, Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>Vakalopoulou</surname></persName>
							<email>maria.vakalopoulou@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MICS Laboratory</orgName>
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>CentraleSupélec, Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul-Henry</forename><surname>Cournède</surname></persName>
							<email>paul-henry.cournede@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MICS Laboratory</orgName>
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>CentraleSupélec, Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stergios</forename><surname>Christodoulidis</surname></persName>
							<email>stergios.christodoulidis@centralesupelec.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MICS Laboratory</orgName>
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<postCode>91190</postCode>
									<settlement>CentraleSupélec, Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structured State Space Models for Multiple Instance Learning in Digital Pathology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DA80018D01CADB5C35A0234B1F20DD1C</idno>
					<idno type="DOI">10.1007/978-3-031-43907-057.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multiple instance learning</term>
					<term>Whole slide images</term>
					<term>State space models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multiple instance learning is an ideal mode of analysis for histopathology data, where vast whole slide images are typically annotated with a single global label. In such cases, a whole slide image is modelled as a collection of tissue patches to be aggregated and classified. Common models for performing this classification include recurrent neural networks and transformers. Although powerful compression algorithms, such as deep pre-trained neural networks, are used to reduce the dimensionality of each patch, the sequences arising from whole slide images remain excessively long, routinely containing tens of thousands of patches. Structured state space models are an emerging alternative for sequence modelling, specifically designed for the efficient modelling of long sequences. These models invoke an optimal projection of an input sequence into memory units that compress the entire sequence. In this paper, we propose the use of state space models as a multiple instance learner to a variety of problems in digital pathology. Across experiments in metastasis detection, cancer subtyping, mutation classification, and multitask learning, we demonstrate the competitiveness of this new class of models with existing state of the art approaches. Our code is available at https://github.com/MICS-Lab/s4 digital pathology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Precision medicine efforts are shifting cancer care standards by providing novel personalised treatment plans with promising outcomes. Patient selection for such treatment regimes is based principally on the assessment of tissue biopsies and the characterisation of the tumor microenvironment. This is typically performed by experienced pathologists, who closely inspect chemically stained histopathological whole slide images (WSIs). Increasingly, clinical centers are investing in the digitisation of such tissue slides to enable both automatic processing as well as research studies to elucidate the underlying biological processes of cancer. The resulting images are of gigapixel size, rendering their computational analysis challenging. To deal with this issue, multiple instance learning (MIL) schemes based on weakly supervised training are used for WSI classification tasks. In such schemes, the WSI is typically divided into a grid of patches, with general purpose features derived from pretrained ImageNet <ref type="bibr" target="#b17">[18]</ref> networks extracted for each patch. These representations are subsequently pooled together using different aggregation functions and attention-based operators for a final slide-level prediction.</p><p>State space models are designed to efficiently model long sequences, such as the sequences of patches that arise in WSI MIL. In this paper, we present the first use of state space models for WSI MIL. Extensive experiments on three publicly available datasets show the potential of such models for the processing of gigapixel-sized images, under both weakly and multi-task schemes. Moreover, comparisons with other commonly used MIL schemes highlight their robust performance, while we demonstrate empirically the superiority of state space models in processing the longest of WSI sequences with respect to commonly used MIL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Using pretrained networks for patch-wise feature extraction is a well established strategy for histopathology analysis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>. An extension of this approach is with MIL, where the patch-wise features of an entire slide are digested simultaneously by an aggregator model, such as attention-based models CLAM <ref type="bibr" target="#b16">[17]</ref> and TransMIL <ref type="bibr" target="#b18">[19]</ref>, the latter being a variant of self-attention transformers <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b2">[3]</ref> proposes another transformer-based method in the form of a hierarchical ViT. Similar to our multitask experiments, <ref type="bibr" target="#b5">[6]</ref> explores combining slide-level and tilelevel annotations with a minimal point-based annotation strategy. One of the key components of MIL methods is the aggregation module that pools together the set of patch representations. Mean or max pooling operations are among the simplest and most effective for aggregating predictions over a whole slide <ref type="bibr" target="#b1">[2]</ref>. In contrast, recurrent neural networks (RNN) with long short-term memory (LSTM) <ref type="bibr" target="#b13">[14]</ref> model the patches more explicitly as a set of tokens in sequence. In particular, LSTM networks have been shown to work well in different MIL settings including both visual cognition <ref type="bibr" target="#b21">[22]</ref> and computational pathology <ref type="bibr" target="#b0">[1]</ref>.</p><p>The state space model is a linear differential equation,</p><formula xml:id="formula_0">ẋ(t) = Ax(t) + Bu(t) y(t) = Cx(t) + Du(t)<label>(1)</label></formula><p>that is widely studied in control theory, and describes a continuous time process for input and output signals u(t) ∈ R p and y(t) ∈ R q , and state signal x(t) ∈ R n , and where the process is governed by matrices</p><formula xml:id="formula_1">A ∈ R n×n , B ∈ R n×p , C ∈ R q×n , D ∈ R q×p .</formula><p>In HiPPO <ref type="bibr" target="#b8">[9]</ref> (high-order polynomial projection operator), continuous time memorisation is posed as a problem of function approximation in a Hilbert space defined by a probability measure μ. For a scaled Legendre probability measure, one obtains the HiPPO matrix A, which enforces uniform weight in the memorisation of all previously observed inputs, in contrast to the exponentially decaying weighting of the constant error carousel of LSTMs <ref type="bibr" target="#b13">[14]</ref>. The HiPPO mode of memorisation is shown empirically to be better suited to modeling long-range dependencies (LRD) than other neural memory layers, for which it serves as a drop-in replacement.</p><p>Whereas in HiPPO, the state matrix A is a fixed constant, the linear state space layer (LSSL) <ref type="bibr" target="#b11">[12]</ref> incorporates A as a learnable parameter. However, this increased expressiveness introduces intractable powers of A. In <ref type="bibr" target="#b9">[10]</ref>, the LSSL is instead reparameterised as the sum of diagonal and low-rank matrices, allowing for the efficient computation of the layer kernel in Fourier space. This updated formulation is known as the structured state space sequence layer (S4). Note that as a linear operator, the inverse discrete Fourier transform is amenable to backpropagation in the context of a neural network. Note also that under this formulation, the hidden state x(t) is only computed implicitly. Finally, <ref type="bibr" target="#b10">[11]</ref> presents a simplification of the S4 layer, known as diagonal S4 (S4D), in which A is approximated by a diagonal matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given that the patch extraction of whole slide images at high magnifications results in long sequences of patches, we propose to incorporate a state space layer in a MIL aggregation network to better represent each patch sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural State Space Models</head><p>In practice, neural state space models (SSM) simulate Eq. 1 in discrete time, invoking a recurrence relation on the discretised hidden state,</p><formula xml:id="formula_2">x t = Ax t-1 + Bu t y t = Cx t + Du t (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where the sequences u t , x t , and y t are the discretised u(t), x(t), and y(t), and the modified model parameters arise from a bilinear discretisation <ref type="bibr" target="#b11">[12]</ref>. As such, SSMs bear an inherent resemblance to RNNs, where the hidden representation x t can be interpreted as a memory cell for the observed sequence over the interval [0, t], and with Du t acting as a skip connection between the input and output at point t. Due to their lack of non-linearities, state space models can also be viewed as a convolution between two discrete sequences. Playing out the recurrence in Eq. 2, one obtains,</p><formula xml:id="formula_4">y = K * u + Du,<label>(3)</label></formula><p>where u ∈ R L and y ∈ R L are the full input and output sequences, and the sequence K ∈ R L is defined as,</p><formula xml:id="formula_5">K = (CB, CAB, . . . , CA L-1 B), (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>which is computed efficiently by the S4D algorithm <ref type="bibr" target="#b10">[11]</ref>. Note that although SSM layers are linear, they may be combined with other, non-linear layers in a neural network. Note also that although Eq. 3 is posed as modeling a onedimensional signal, in practice multi-dimensional inputs are modelled simply by stacking SSM layers together, followed by an affine "mixing" layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MIL Training</head><p>In our pipeline (Fig. <ref type="figure" target="#fig_0">1</ref>) WSIs are first divided into a sequence of L patches {u 1 , u 2 , . . . , u L }, where L will vary by slide. A pretrained ResNet50 is then used to extract a 1024-dimensional feature vector from each patch {u 1 , u 2 , . . . , u L }, which constitute the model inputs. We define a SSM-based neural network F to predict a WSI-level class probability given this input sequence,</p><formula xml:id="formula_7">ŷ = F({u 1 , u 2 , . . . , u L }). (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>The architecture of F is composed of an initial linear projection layer, used to lower the dimensionality of each vector in the input sequence. A SSM layer is then applied feature-wise by applying the S4D algorithm. That is, Eq. 3, including the skip connection, transforms the sequence {u 1,d , u 2,d , . . . , u L,d } for all features d, and the resulting sequences are concatenated. A linear "mixing" layer is applied token-wise, doubling the dimensionality of each token, followed by a gated linear unit <ref type="bibr" target="#b4">[5]</ref> acting as an output gate, which restores the input dimensionality. For the SSM layer, we used the official implementation of S4D<ref type="foot" target="#foot_0">1</ref> . A max pooling layer merges the SSM layer outputs into a single vector, which is projected by a final linear layer and softmax to give the class probabilities ŷ. The model is trained according to,</p><formula xml:id="formula_9">L MIL = - 1 M M m=1 log ŷcm , (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where ŷcm denotes the probability corresponding to c m , the slide-level label of the sequence corresponding to the m th of M whole slide images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multitask Training</head><p>One advantage of processing an entire slide as a sequence is the ease with which additional supervision may be incorporated, when available. A patch-level ground truth creates the opportunity for multitask learning, which can enhance the representations learned for slide-level classification. As an extension of our base model in Eq. 6, we train a multitask model to jointly predict a slide-level and patch-level labels. Prior to the max pooling layer of the base model, an additional linear layer is applied to each sequence token, yielding L additional model outputs. This multitask model is trained according to a sum of log losses,</p><formula xml:id="formula_11">L MT = - 1 M M m=1 log ŷcm + λ L • L l=1 log ŷc m,l ,<label>(7)</label></formula><p>where c m,l indexes the class of the l th patch in the m th training slide and λ is a tunable hyperparameter used to modulate the relative importance of each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>We extracted patches of size 256 × 256 from the tissue regions of WSIs at 20x magnification. Following CLAM <ref type="bibr" target="#b16">[17]</ref>, the third residual block of a pretrained ResNet50 <ref type="bibr" target="#b12">[13]</ref> was used as a feature extractor, followed by a mean pooling operation, resulting in a 1024-dimensional representation for each patch. These features were used as inputs to all models. All model training was performed under a 10-fold cross-validation, and all reported results are averaged over the validation sets of the folds, aside from CAMELYON16, for which the predefined test set was utilized. Thus, for CAMELYON16, we report test set performances averaged over the validation. Baseline models were chosen to be prior art CLAM <ref type="bibr" target="#b16">[17]</ref> and TransMIL <ref type="bibr" target="#b18">[19]</ref>. The official code of these two models was used to perform the comparison. In addition, we included a vanilla transformer, a LSTM RNN, and models based on mean and max pooling. Our vanilla transformer is composed of two stacked self-attention blocks, with four attention heads, a model dimension of 256, and a hidden dimension of 256. For the LSTM, we used an embedding size of 256 and a width of 256. The pooling models applied pooling feature-wise across each sequence, then used a random forest with 200 trees for classification. For the S4 models, the dimension of the state matrix A was tuned to 32 for CAMELYON16 and TCGA-RCC, and 128 for TCGA-LUAD. Our models were trained using the Adam <ref type="bibr" target="#b14">[15]</ref> optimizer with the lookahead method <ref type="bibr" target="#b22">[23]</ref>, with a learning rate of 2 • 10 -4 , and weight decay of 10 -4 for TCGA-LUAD and TCGA-RCC and 10 -3 for CAMELYON16. Early stopping with a patience of 10 was used for all our training. Our implementation is publicly available<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>CAMELYON16 <ref type="bibr" target="#b15">[16]</ref> is a dataset that consists of resections of lymph nodes, where each WSI is annotated with a binary label indicating the presence of tumour tissue in the slide, and all slides containing tumors have a pixel-level annotation indicating the metastatic region. In multitask experiments, we use this annotation to give each patch a label indicating local tumour presence. There are 270 WSIs in the training/validation set, and 130 WSIs in the predefined test set. In our experiments, the average patch sequence length arising from CAMELYON16 is 6129 (ranging from 127 to 27444).</p><p>TCGA-LUAD is a TCGA lung adenocarcinoma dataset that contains 541 WSIs along with genetic information about each patient. We obtained genetic information for this cohort using Xena browser <ref type="bibr" target="#b6">[7]</ref>. As a MIL task, we chose the task of predicting the patient mutation status of TP53, a tumor suppressor gene that is highly relevant in oncology studies. The average sequence length is 10557 (ranging from 85 to 34560).</p><p>TCGA-RCC is a TCGA dataset for three kidney cancer subtypes (denoted KICH, KIRC, and KIRP). It consists of 936 WSIs (121 KICH, 518 KIRC, and 297 KIRP). The average sequence length is 12234 (ranging from 319 to 62235).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Multiple Instance Learning Results. We evaluate our method on each dataset by accuracy and area under receiver operating characteristic curve (AUROC). For multiclass classification, these were computed in a one-versus-rest manner.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarises the comparison between our proposed model and baselines. For the CAMELYON16 dataset, our method performs on par with Trans-MIL and the CLAM models, while it clearly outperforms the other methods. Similarly, in the TCGA-LUAD dataset the proposed model achieves comparable performance with both CLAM models, while outperforming TransMIL and the other methods. We note that TCGA-LUAD proves to be a more challenging dataset for all models. Moreover, our method outperforms CLAM models on the TCGA-RCC dataset, while reporting very similar performance with respect to TransMIL. Overall, looking at the average metrics per model across all three datasets, our proposed method achieves the highest accuracy and the second highest AUROC, only behind CLAM-MB. A pairwise t-test between the proposed method, CLAM, and TransMIL shows that there is no statistical significance performance difference (see supplementary material).</p><p>We further compare our method with respect to model and time complexity. In Table <ref type="table" target="#tab_1">2</ref> we report the number of trainable parameters, as well as the inference time for all models. The number of parameters is computed with all models configured to be binary classifiers, and the inference time is computed as the average time over 100 samples for processing a random sequence of 1024-dimensional vectors of length 30000. For our proposed method, we report both models with the different state dimensions (Ours (SSM 32 )) and (Ours (SSM 128 )). Compared with TransMIL, our method runs four times faster and has less than half the parameters. The CLAM models are more efficient in terms of number of trainable parameters, yet CLAM MB is slower. Table <ref type="table" target="#tab_2">3</ref> shows the effect of modifying parts of the architecture on the results for TCGA-RCC. Most modifications had very little impact on AUROC, but a more significant impact can be seen on the accuracy of the model. Models A and B show that stacking multiple SSM layers results in lower accuracy, which was observed over all three datasets, while models C and D show that modifying the state dimension of the SSM module can have an impact on the accuracy. The optimal state space dimension varies depending on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multitask Learning Results.</head><p>We explored the ability of our model to combine slide-and patch-level information on the CAMEYLON16 dataset. We compared our model with the best performing model on CAMELYON16, TransMIL. Both models were trained according to Eq. 7 with λ = 5 tuned by hand. In Table <ref type="table" target="#tab_3">4</ref> we give slide-level accuracy and AUROC for the two models. We observe that all accuracies and AUROC increase compared with those reported in Table <ref type="table" target="#tab_0">1</ref>. This indicates that the use of patch-level annotations complements the learning of the slide-level label. We furthermore observe that our model outperforms TransMIL when combining slide-and patch-level annotations. We map the sequence of output probabilities to their slide coordinates giving a heatmap localising metastasis (see supplementary material). Performance on Longest Sequences. In order to highlight the inherent ability of SSM models to effectively model long sequences, we performed an experiment on only the largest WSIs of the TCGA-RCC dataset. Indeed, this dataset contains particularly long sequences (up to 62235 patches at 20x). We evaluated the trained models for each fold on a subset of the validation set, only containing sequences with a length in the 85 th percentile. Table <ref type="table" target="#tab_4">5</ref> shows the obtained average accuracy (weighted by the number of long sequences in each validation set) and AUROC on both CLAM models, TransMIL, and our proposed method. Both in terms of AUROC and accuracy, our method outperforms the other methods on long sequences, while the performances are comparable to Table <ref type="table" target="#tab_0">1</ref>, albeit slightly lower, illustrating the challenge of processing large WSIs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work we have explored the ability of state space models to act as multiple instance learners on sequences of patches extracted from histopathology images. These models have been developed for their ability to memorise long sequences, and they have proven competitive with state of the art MIL models across a range of pathology problems. Additionally, we demonstrated the ability of these models to perform multiclass classification, which furthermore allowed us to visualise the localisation of metastasic regions. Finally, we demonstrated that on the longest sequences in our datasets, state space models offer better performance than competing models, confirming their power in modeling long-range dependencies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed pipeline. In the first step, patches are extracted from a regular grid on a WSI. These patches are embedded using a pre-trained ResNet50 and are aggregated by a sequence model based on a state space layer.</figDesc><graphic coords="4,61,98,220,46,328,60,86,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of accuracy and AUROC on three datasets CAMELYON16, TCGA-LUAD, TCGA-RCC, and on average. All metrics in the table are the average of 10 runs. Best performing methods are indicated in bold and second best in italics.</figDesc><table><row><cell>Dataset</cell><cell cols="4">CAMELYON16 TCGA-LUAD</cell><cell cols="2">TCGA-RCC</cell><cell>Average</cell></row><row><cell>Metric</cell><cell>Acc.</cell><cell cols="2">AUROC Acc.</cell><cell cols="2">AUROC Acc.</cell><cell>AUROC Acc.</cell><cell>AUROC</cell></row><row><cell>Mean-pooling</cell><cell cols="2">0.5969 0.5810</cell><cell cols="2">0.6261 0.6735</cell><cell>0.8608</cell><cell>0.9612</cell><cell>0.6946 0.7386</cell></row><row><cell>Max-pooling</cell><cell cols="2">0.7078 0.7205</cell><cell cols="2">0.6328 0.6686</cell><cell>0.8803</cell><cell>0.9659</cell><cell>0.7403 0.7850</cell></row><row><cell cols="3">Transformer [21] 0.5419 0.5202</cell><cell cols="2">0.5774 0.6214</cell><cell>0.7932</cell><cell>0.9147</cell><cell>0.6375 0.6854</cell></row><row><cell>LSTM [8]</cell><cell cols="2">0.5310 0.5053</cell><cell cols="2">0.5389 0.5208</cell><cell>0.6654</cell><cell>0.7853</cell><cell>0.5784 0.6038</cell></row><row><cell cols="3">CLAM SB [17] 0.8147 0.8382</cell><cell cols="2">0.6859 0.7459</cell><cell cols="2">0.8816  *  0.9723  *  0.7941 0.8532</cell></row><row><cell cols="3">CLAM MB [17] 0.8264 0.8523</cell><cell cols="4">0.6901 0.7573 0.8966  *  0.9799  *  0.8044 0.8632</cell></row><row><cell>TransMIL [19]</cell><cell cols="4">0.8287 0.8628 0.6348 0.7015</cell><cell cols="2">0.9466  *  0.9882  *  0.8034 0.8508</cell></row><row><cell>Ours</cell><cell cols="2">0.8217 0.8485</cell><cell cols="2">0.6879 0.7304</cell><cell>0.9426</cell><cell>0.9885 0.8174 0.8558</cell></row></table><note><p><p><p>* indicates results from</p><ref type="bibr" target="#b18">[19]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of parameter count and inference time for all methods.</figDesc><table><row><cell>Model</cell><cell cols="2">Number of parameters Inference time (ms)</cell></row><row><cell>Mean-pooling</cell><cell>1 025</cell><cell>5.60</cell></row><row><cell>Max-pooling</cell><cell>1 025</cell><cell>77.49</cell></row><row><cell cols="2">Transformer [21] 1 054 978</cell><cell>2.60</cell></row><row><cell>LSTM [8]</cell><cell>789 250</cell><cell>320.52</cell></row><row><cell cols="2">CLAM SB [17] 790 791</cell><cell>0.84</cell></row><row><cell cols="2">CLAM MB [17] 791 048</cell><cell>5.85</cell></row><row><cell>TransMIL [19]</cell><cell>2 672 146</cell><cell>8.58</cell></row><row><cell cols="2">Ours (SSM128) 1 184 258</cell><cell>2.01</cell></row><row><cell>Ours (SSM32)</cell><cell>1 085 954</cell><cell>1.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for the different SSM components on the TCGA-RCC dataset. Best results in bold.</figDesc><table><row><cell cols="5">Model SSM layers State dimension Accuracy AUROC</cell></row><row><cell>A</cell><cell>2</cell><cell>32</cell><cell>0.9236</cell><cell>0.9813</cell></row><row><cell>B</cell><cell>3</cell><cell>32</cell><cell>0.9179</cell><cell>0.9834</cell></row><row><cell>C</cell><cell>1</cell><cell>16</cell><cell>0.9352</cell><cell>0.9846</cell></row><row><cell>D</cell><cell>1</cell><cell>64</cell><cell>0.9352</cell><cell>0.9861</cell></row><row><cell cols="2">Ours 1</cell><cell>32</cell><cell>0.9426</cell><cell>0.9885</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of accuracy and AUROC for models trained as multitask classifiers on the CAMELYON16 dataset. Best results in bold.</figDesc><table><row><cell>Model</cell><cell cols="2">Accuracy AUROC</cell></row><row><cell cols="2">TransMIL [19] 0.8403</cell><cell>0.8828</cell></row><row><cell>Ours</cell><cell>0.8488</cell><cell>0.8998</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Results of CLAM SB, CLAM MB, TransMIL, and our proposed method on long sequences. Best results in bold.</figDesc><table><row><cell>Model</cell><cell cols="2">Accuracy AUROC</cell></row><row><cell cols="2">CLAM SB [17] 0.9149</cell><cell>0.9635</cell></row><row><cell cols="2">CLAM MB [17] 0.8936</cell><cell>0.9654</cell></row><row><cell cols="2">TransMIL [19] 0.9007</cell><cell>0.9652</cell></row><row><cell>Ours</cell><cell>0.9220</cell><cell>0.9737</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/HazyResearch/state-spaces.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/MICS-Lab/s4 digital pathology.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work has benefited from state financial aid, managed by the <rs type="funder">Agence Nationale de Recherche</rs> under the <rs type="programName">investment program</rs> integrated into France 2030, project reference <rs type="grantNumber">ANR-21-RHUS-0003</rs>. This work was partially supported by the <rs type="funder">ANR Hagnodice</rs> <rs type="grantNumber">ANR-21-CE45-0007</rs>. Experiments have been conducted using HPC resources from the <rs type="institution">"Mésocentre" computing center of CentraleSupélec and École Normale Supérieure Paris-Saclay</rs> supported by <rs type="funder">CNRS</rs> and <rs type="funder">Région Île-de-France</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4UEr6cJ">
					<idno type="grant-number">ANR-21-RHUS-0003</idno>
					<orgName type="program" subtype="full">investment program</orgName>
				</org>
				<org type="funding" xml:id="_bZMV4Zk">
					<idno type="grant-number">ANR-21-CE45-0007</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation-aggregation networks for segmentation of multi-gigapixel histology images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVC</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Campanella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to gigapixel images via hierarchical self-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16144" to="16155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification and mutation prediction from non-small cell lung cancer histopathology images using deep learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Coudray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1559" to="1567" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A semi-supervised multi-task learning framework for cancer classification with weak annotation in whole-slide images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102652</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualizing and interpreting cancer genomics data via the Xena platform</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="675" to="678" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Long short-term memory. Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HiPPO: recurrent memory with optimal polynomial projections</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1474" to="1487" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00396</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the parameterization and initialization of diagonal state space models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11893</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state space layers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="572" to="585" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">1399 h&amp;e-stained sentinel lymph node sections of breast cancer patients: the camelyon dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GigaScience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-efficient and weakly supervised computational pathology on whole-slide images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TransMIL: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural image compression for gigapixel histopathology image analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tellez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="567" to="578" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In defense of LSTMs for addressing multiple instance learning problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: K steps forward, 1 step back</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
