<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Alina</forename><forename type="middle">F</forename><surname>Dima</surname></persName>
							<email>alina.dima@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Medicine</orgName>
								<orgName type="department" key="dep2">Klinikum Rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Veronika</forename><forename type="middle">A</forename><surname>Zimmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Medicine</orgName>
								<orgName type="department" key="dep2">Klinikum Rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Menten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongwei</forename><forename type="middle">Bran</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Quantitative Biomedicine</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Graf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Medicine</orgName>
								<orgName type="department" key="dep2">Klinikum Rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tristan</forename><surname>Lemke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Medicine</orgName>
								<orgName type="department" key="dep2">Klinikum Rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philipp</forename><surname>Raffler</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Medicine</orgName>
								<orgName type="department" key="dep2">Klinikum Rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Graf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Medicine</orgName>
								<orgName type="department" key="dep2">Klinikum Rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><forename type="middle">S</forename><surname>Kirschke</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Medicine</orgName>
								<orgName type="department" key="dep2">Klinikum Rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rickmer</forename><surname>Braren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Medicine</orgName>
								<orgName type="department" key="dep2">Klinikum Rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computation, Information and Technology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Medicine</orgName>
								<orgName type="department" key="dep2">Klinikum Rechts der Isar</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="141" to="151"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9E22BBB8064B71BEDC836C3254675182</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>vessel segmentation</term>
					<term>3D segmentation</term>
					<term>weakly supervised segmentation</term>
					<term>curvilinear structures</term>
					<term>2D projections</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated segmentation of the blood vessels in 3D volumes is an essential step for the quantitative diagnosis and treatment of many vascular diseases. 3D vessel segmentation is being actively investigated in existing works, mostly in deep learning approaches. However, training 3D deep networks requires large amounts of manual 3D annotations from experts, which are laborious to obtain. This is especially the case for 3D vessel segmentation, as vessels are sparse yet spread out over many slices and disconnected when visualized in 2D slices. In this work, we propose a novel method to segment the 3D peripancreatic arteries solely from one annotated 2D projection per training image with depth supervision. We perform extensive experiments on the segmentation of peripancreatic arteries on 3D contrast-enhanced CT images and demonstrate how well we capture the rich depth information from 2D projections. We demonstrate that by annotating a single, randomly chosen projection for each training sample, we obtain comparable performance to annotating multiple 2D projections, thereby reducing the annotation effort. Furthermore, by mapping the 2D labels to the 3D space using depth information and incorporating this into training, we almost close the performance gap between 3D supervision and 2D supervision. Our code is available at: https://github.com/alinafdima/3Dseg-mip-depth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated segmentation of blood vessels in 3D medical images is a crucial step for the diagnosis and treatment of many diseases, where the segmentation can aid in visualization, help with surgery planning, be used to compute biomarkers, and further downstream tasks. Automatic vessel segmentation has been extensively studied, both using classical computer vision algorithms <ref type="bibr" target="#b16">[16]</ref> such as vesselness filters <ref type="bibr" target="#b8">[8]</ref>, or more recently with deep learning <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b22">21]</ref>, where state-ofthe-art performance has been achieved for various vessel structures. Supervised deep learning typically requires large, well-curated training sets, which are often laborious to obtain. This is especially the case for 3D vessel segmentation.</p><p>Manually delineating 3D vessels typically involves visualizing and annotating a 3D volume through a sequence of 2D cross-sectional slices, which is not a good medium for visualizing 3D vessels. This is because often only the cross-section of a vessel is visible in a 2D slice. In order to segment a vessel, the annotator has to track the cross-section of that vessel through several adjacent slices, which is especially tedious for curved or branching vessel trees. Projecting 3D vessels to a 2D plane allows for the entire vessel tree to be visible within a single 2D image, providing a more robust representation and potentially alleviating the burden of manual annotation. Kozinski et al. <ref type="bibr" target="#b13">[13]</ref> propose to annotate up to three maximum intensity projections (MIP) for the task of centerline segmentation <ref type="bibr" target="#b13">[13]</ref>, obtaining results comparable to full 3D supervision. Compared to centerline segmentation, where the vessel diameter is disregarded, training a 3D vessel segmentation model from 2D annotations poses additional segmentationspecific challenges, as 2D projections only capture the outline of the vessels, providing no information about their interior. Furthermore, the axes of projection are crucial for the model's success, given the sparsity of information in 2D annotations.</p><p>To achieve 3D vessel segmentation with only 2D supervision from projections, we first investigate which viewpoints to annotate in order to maximize segmentation performance. We show that it is feasible to segment the full extent of vessels in 3D images with high accuracy by annotating only a single randomlyselected 2D projection per training image. This approach substantially reduces the annotation effort, even compared to works training only on 2D projections. Secondly, by mapping the 2D annotations to the 3D space using the depth of the MIPs, we obtain a partially segmented 3D volume that can be used as an additional supervision signal. We demonstrate the utility of our method on the challenging task of peripancreatic arterial segmentation on contrast-enhanced arterial-phase computed tomography (CT) images, which feature large variance in vessel diameter. Our contribution to 3D vessel segmentation is three-fold:</p><p>• Our work shows that highly accurate automatic segmentation of 3D vessels can be learned by annotating single MIPs. • Based on extensive experimental results, we determine that the best annotation strategy is to label randomly selected viewpoints, while also substantially reducing the annotation cost.</p><p>• By incorporating additional depth information obtained from 2D annotations at no extra cost to the annotator, we almost close the gap between 3D supervision and 2D supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning from Weak Annotations. Weak annotations have been used in deep learning segmentation to reduce the annotation effort through cheaper, less accurate, or sparser labeling <ref type="bibr" target="#b21">[20]</ref>. Bai et al. <ref type="bibr" target="#b0">[1]</ref> learn to perform aortic image segmentation by sparsely annotating only a subset of the input slices. Multiple instance learning approaches bin pixels together by only providing labels at the bin level. Jia et al. <ref type="bibr" target="#b12">[12]</ref> use this approach to segment cancer on histopathology images successfully. Annotating 2D projections for 3D data is another approach to using weak segmentation labels, which has garnered popularity recently in the medical domain. Bayat et al. <ref type="bibr" target="#b2">[2]</ref> propose to learn the spine posture from 2D radiographs, while Zhou et al. <ref type="bibr" target="#b23">[22]</ref> use multi-planar MIPs for multi-organ segmentation of the abdomen. Kozinski et al. <ref type="bibr" target="#b13">[13]</ref> propose to segment vessel centerlines using as few as 2-3 annotated MIPs. Chen et al. <ref type="bibr" target="#b4">[4]</ref> train a vessel segmentation model from unsupervised 2D labels transferred from a publicly available dataset, however, there is still a gap to be closed between unsupervised and supervised model performance. Our work uses weak annotations in the form of annotations of 2D MIPs for the task of peripancreatic vessel segmentation, where we attempt to reduce the annotation cost to a minimum by only annotating a single projection per training input without sacrificing performance.</p><p>Incorporating Depth Information. Depth is one of the properties of the 3D world. Loss of depth information occurs whenever 3D data is projected onto a lower dimensional space. In natural images, depth loss is inherent through image acquisition, therefore attempts to recover or model depth have been employed for 3D natural data. For instance, Fu et al. <ref type="bibr" target="#b9">[9]</ref> use neural implicit fields to semantically segment images by transferring labels from 3D primitives to 2D images. Lawin et al. <ref type="bibr" target="#b14">[14]</ref> propose to segment 3D point clouds by projecting them onto 2D and training a 2D segmentation network. At inference time, the predicted 2D segmentation labels are remapped back to the original 3D space using the depth information. In the medical domain, depth information has been used in volume rendering techniques <ref type="bibr" target="#b7">[7]</ref> to aid with visualization, but it has so far not been employed when working with 2D projections of 3D volumes to recover information loss. We propose to do the conceptually opposite approach from Lawin et al. <ref type="bibr" target="#b14">[14]</ref>, by projecting 3D volumes onto 2D to facilitate and reduce annotation. We use depth information to map the 2D annotations to the original 3D space at annotation time and generate partial 3D segmentation volumes, which we incorporate in training as an additional loss term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Overview. The maximum intensity projection (MIP) of a 3D volume I ∈ R Nx×Ny×Nz is defined as the highest intensity along a given axis:</p><formula xml:id="formula_0">mip(x, y) = max z I(x, y, z) ∈ R Nx×Ny . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>For simplicity, we only describe MIPs along the z-axis, but they can be performed on any image axis. Exploiting the fact that arteries are hyperintense in arterial phase CTs, we propose to annotate MIPs of the input volume for binary segmentation. The hyperintensities of the arteries ensures their visibility in the MIP, while additional processing removes most occluding nearby tissue (Sect. 4).</p><p>Given a binary 2D annotation of a MIP A ∈ {0, 1} Nx×Ny , we map the foreground pixels in A to the original 3D image space. This is achieved by using the first and last z coordinates where the maximum intensity is observed along any projection ray. Owing to the fact that the vessels in the abdominal cavity are relatively sparse in 2D projections and most of the occluding tissue is removed in postprocessing, this step results in a fairly complete surface of the vessel tree. Furthermore, we can partially fill this surface volume, resulting in a 3D depth map D, which is a partial segmentation of the vessel tree. We use the 2D annotations as well as the depth map to train a 3D segmentation network in a weakly supervised manner.</p><p>An overview of our method is presented in Fig. <ref type="figure" target="#fig_0">1</ref>. In the following, we describe these components and how they are combined to train a 3D segmentation network in more detail.</p><p>Depth Information. We can view MIP as capturing the intensity of the brightest pixel along each ray r xy ∈ R Nz , where r xy (z) = I(x, y, z). Along each projection ray, we denote the first and last z coordinates which have the same intensity as the MIP to be the forward depth z fw = arg max z I(x, y, z) and backward depth z bw = arg min z I(x, y, z). This information can be utilized for the following: (1) enhancing the MIP visualization, or (2) providing a way to map pixels from the 2D MIP back to the 3D space (depth map). The reason why the maximum intensity is achieved multiple times along a ray is because our images are clipped, which removes a lot of the intensity fluctuations. </p><formula xml:id="formula_2">L(Y) = α • CE(A, mip(Y)) + (1 -α) • CE(D, Y) • D,<label>(2)</label></formula><p>where α ∈ [0, 1]. Our final loss is a convex combination between: (a) the crossentropy(CE) of the network output projected to 2D and the 2D annotation, as well as (b) the cross-entropy between the network output and the depth map, but only applied to positive pixels in the depth map. Notably, the 2D loss constrains the shape of the vessels, while the depth loss promotes the segmentation of the vessel interior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Design</head><p>Dataset. We use an in-house dataset of contrast-enhanced abdominal computed tomography images (CTs) in the arterial phase to segment the peripancreatic arteries <ref type="bibr" target="#b6">[6]</ref>. The cohort consists of 141 patients with pancreatic ductal adenocarcinoma, of an equal ratio of male to female patients. Given a 3D arterial CT of the abdominal area, we automatically extract the vertebrae <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b19">18]</ref> and semi-automatically extract the ribs, which have similar intensities as arteries in arterial CTs and would otherwise occlude the vessels. In order to remove as much of the cluttering surrounding tissue and increase the visibility of the vessels in the projections, the input is windowed so that the vessels appear hyperintense. Details of the exact preprocessing steps can be found in Table <ref type="table" target="#tab_1">2</ref> of the supplementary material. The dataset contains binary 3D annotations of the peripancreatic arteries carried out by two radiologists, each having annotated half of the dataset. The 2D annotations we use in our experiments are projections of these 3D annotations. For more information about the dataset, see <ref type="bibr" target="#b6">[6]</ref>.</p><p>Image Augmentation and Transformation. As the annotations lie on a 2D plane, 3D spatial augmentation cannot be used due to the information sparsity in the ground truth. Instead, we apply an invertible transformation T to the input volume and apply the inverse transformation T -1 to the network output before applying the loss, such that the ground truth need not be altered. A detailed description of the augmentations and transformations used can be found in Table <ref type="table" target="#tab_0">1</ref> in the supplementary material.</p><p>Training and Evaluation. We use a 3D U-Net <ref type="bibr" target="#b18">[17]</ref> with four layers as our backbone, together with Xavier initialization <ref type="bibr" target="#b10">[10]</ref>. A diagram of the network architecture can be found in Fig. <ref type="figure">2</ref> in the supplementary material. The loss weight α is tuned at 0.5, as this empirically yields the best performance. Our experiments are averaged over 5-fold cross-validation with 80 train samples, 20 validation samples, and a fixed test set of 41 samples. The network initialization is different for each fold but kept consistent across different experiments run on the same fold. This way, both data variance and initialization variance are accounted for through cross-validation. To measure the performance of our models, we use the Dice score, precision, recall, and mean surface distance (MSD). We also compute the skeleton recall as the percentage of the ground truth skeleton pixels which are present in the prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The Effectiveness of 2D Projections and Depth Supervision. We compare training using single random viewpoints with and without depth information against baselines that use more supervision. Models trained on full 3D ground truth represent the upper bound baseline, which is very expensive to annotate. We implement <ref type="bibr" target="#b13">[13]</ref> as a baseline on our dataset, training on up to 3 fixed orthogonal projections. We distinguish between models selected according to the 2D performance on the validation set (2D) which is a fair baseline, and models selected according to the 3D performance on the validation set (3D), which is an unfair baseline as it requires 3D annotations on the validation set.</p><p>With the exception of the single fixed viewpoint baselines where the models have the tendency to diverge towards over-or segmentation, we perform binary holefilling on the output of all of our other models, as producing hollow objects is a common under-segmentation issue.</p><p>In Table <ref type="table" target="#tab_0">1</ref> we compare our method against the 3D baseline, as well as baselines trained on multiple viewpoints. We see that by using depth information paired with training using a single random viewpoint per sample performs almost at the level of models trained on 3D labels, at a very small fraction of the annotation cost. The depth information also reduces model variance compared to the same setup without depth information. Even without depth information, training the model on single randomly chosen viewpoints offers a robust training signal that the Dice score is on par with training on 2 fixed viewpoints under ideal model selection at only half the annotation cost. Randomly selecting viewpoints for training acts as powerful data augmentation, which is why we are able to obtain performance comparable to using more fixed viewpoints. Under ideal 3D-based model selection, three views would come even closer to full 3D performance; however, with realistic 2D-based model selection, fixed viewpoints are more prone to diverge. This occurs because sometimes 2D-based model selection favors divergent models which only segment hollow objects, which cannot be fixed in postprocessing. Single fixed viewpoints contain so little information on their own that models trained on such input fail to learn how to segment the vessels and generally converge to over-segmenting in the blind spots in the projections. We conclude that using random viewpoints is not only helpful in reducing annotation cost but also decreases model variance.</p><p>In terms of other metrics, randomly chosen projection viewpoints with and without depth improve both recall and skeleton recall even compared to fully 3D annotations, while generally reducing precision. We theorize that this is because the dataset itself contains noisy annotations and fully supervised models better overfit to the type of data annotation, whereas our models converge to following the contrast and segmenting more vessels, which are sometimes wrongfully labeled as background in the ground truth. MSD are not very telling in our dataset due to the noisy annotations and the nature of vessels, as an under-or over-segmented vessel branch can quickly translate into a large surface distance.</p><p>The Effect of Dataset Size. We vary the size of the training set from |D tr | = 80 to as little as |D tr | = 10 samples, while keeping the size of the validation and test sets constant, and train models on single random viewpoints.</p><p>In Table <ref type="table" target="#tab_1">2</ref>, we compare single random projections trained with and without depth information at varying dataset sizes to ilustrate the usefulness of the depth information with different amounts of training data. Our depth loss offers consistent improvement across multiple dataset sizes and reduces the overall performance variance. The performance boost is noticeable across the board, the only exception being precision. The smaller the dataset size is, the greater the performance boost from the depth. We perform a Wilcoxon rank-sum statistical test comparing the individual sample predictions of the models trained at various dataset sizes with single random orthogonal viewpoints with or without depth information, obtaining a statistically significant (p-value of &lt; 0.0001). We conclude that the depth information complements the segmentation effectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we present an approach for 3D segmentation of peripancreatic arteries using very sparse 2D annotations. Using a labeled dataset consisting of single, randomly selected, orthogonal 2D annotations for each training sample and additional depth information obtained at no extra cost, we obtain accuracy almost on par with fully supervised models trained on 3D data at a mere fraction of the annotation cost. Limitations of our work are that the depth information relies on the assumption that the vessels exhibit minimal intensity fluctuations within local neighborhoods, which might not hold on other datasets, where more sophisticated ray-tracing methods would be more effective in locating the front and back of projected objects. Furthermore, careful preprocessing is performed to eliminate occluders, which would limit its transferability to datasets with many occluding objects of similar intensities. Further investigation is needed to quantify how manual 2D annotations compare to our 3D-derived annotations, where we expect occluders to affect the annotation process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Method overview. We train a 3D network to segment vessels from 2D annotations. Given an input image I, depth-encoded MIPs p fw , p bw are generated by projecting the input image to 2D. 2D binary labels A are generated by annotating one 2D projection per image. The 2D annotation is mapped to the 3D space using the depth information, resulting in a partially labeled 3D volume D. During training, both 2D annotations and 3D depth maps are used as supervision signals in a combined loss, which uses both predicted 3D segmentation Y and its 2D projection mip(Y ).</figDesc><graphic coords="4,49,80,173,18,324,88,154,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 . 1 . 2 . 3 .</head><label>2123</label><figDesc>Fig. 2. Example depth-enhanced MIP using (a) forward depth z fw and (b) backward depth z bw visualized in color; (c) binary 2D annotation; a slice view from a 3D volume illustrating: (e) the forward -in green -and backward depth -in blue -, (f) the depth map, (g) 3D ground truth; volume rendering of (h) the depth map and (d) the depth map with only forward and backward depth pixels. The input images are contrast-enhanced.(Color figure online)</figDesc><graphic coords="5,57,48,226,28,337,48,136,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Viewpoint ablation. We compare models trained on single random viewpoints (VPs) with (+D) or without (-D) depth against fixed viewpoint baselines without depth and full 3D supervision. We distinguish between model selection based on 2D annotations vs. 3D annotations on the validation set. The best-performing models for each model selection (2D vs. 3D) are highlighted in bold.</figDesc><table><row><cell>Experiment</cell><cell cols="2">Model Selection Dice ↑</cell><cell>Precision ↑</cell><cell>Recall ↑</cell><cell>Skeleton Recall ↑ MSD ↓</cell></row><row><cell>3D</cell><cell>3D</cell><cell cols="4">92.18 ± 0.35 93.86 ± 0.81 90.64 ± 0.64 76.04 ± 4.51</cell><cell>1.15 ± 0.11</cell></row><row><cell>fixed 3VP</cell><cell>3D</cell><cell cols="4">92.02 ± 0.52 93.05 ± 0.61 91.13 ± 0.79 78.61 ± 1.52</cell><cell>1.13 ± 0.11</cell></row><row><cell>fixed 2VP</cell><cell>3D</cell><cell cols="4">91.29 ± 0.78 91.46 ± 2.13 91.37 ± 1.45 78.51 ± 2.78</cell><cell>1.13 ± 0.09</cell></row><row><cell>fixed 3VP</cell><cell>2D</cell><cell cols="4">90.78 ± 1.30 90.66 ± 1.30 91.18 ± 3.08 81.77 ± 2.13</cell><cell>1.16 ± 0.13</cell></row><row><cell>fixed 2VP</cell><cell>2D</cell><cell cols="4">90.22 ± 1.19 88.16 ± 2.86 92.74 ± 1.63 82.18 ± 2.47</cell><cell>1.14 ± 0.09</cell></row><row><cell>fixed 1VP</cell><cell>2D</cell><cell cols="4">60.76 ± 24.14 50.47 ± 23.21 92.52 ± 3.09 81.19 ± 2.39</cell><cell>2.96 ± 3.15</cell></row><row><cell cols="2">random 1VP-D 2D</cell><cell cols="4">91.29 ± 0.81 91.42 ± 0.92 91.45 ± 1.00 80.16 ± 2.35</cell><cell>1.13 ± 0.04</cell></row><row><cell cols="2">random 1VP+D 2D</cell><cell cols="4">91.69 ± 0.48 90.77 ± 1.76 92.79 ± 0.95 81.27 ± 2.02</cell><cell>1.15 ± 0.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Dataset size ablation. We vary the training dataset size |Dtr| and compare models trained on single random viewpoints, with or without depth. Best performing models in each setting are highlighted.</figDesc><table><row><cell cols="3">|Dtr| Depth Dice ↑</cell><cell>Precision ↑</cell><cell>Recall ↑</cell><cell>Skeleton Recall ↑ MSD ↓</cell></row><row><cell>10</cell><cell>-D</cell><cell cols="3">86.03 ± 2.94 88.23 ± 2.58 84.81 ± 6.42 78.25 ± 2.20</cell><cell>1.92 ± 0.55</cell></row><row><cell>10</cell><cell>+D</cell><cell cols="3">89.06 ± 1.20 88.55 ± 1.73 89.91 ± 1.29 78.95 ± 3.62</cell><cell>1.80 ± 0.28</cell></row><row><cell>20</cell><cell>-D</cell><cell cols="3">88.22 ± 3.89 90.26 ± 1.64 86.74 ± 6.56 80.78 ± 1.66</cell><cell>1.44 ± 0.20</cell></row><row><cell>20</cell><cell>+D</cell><cell cols="3">90.51 ± 0.38 89.84 ± 0.90 91.50 ± 1.23 80.00 ± 1.95</cell><cell>1.33 ± 0.16</cell></row><row><cell>40</cell><cell>-D</cell><cell cols="3">88.07 ± 2.34 89.09 ± 2.01 87.62 ± 4.43 78.38 ± 2.39</cell><cell>1.38 ± 0.10</cell></row><row><cell>40</cell><cell>+D</cell><cell cols="3">90.21 ± 0.89 89.08 ± 2.89 91.82 ± 2.11 79.16 ± 2.36</cell><cell>1.24 ± 0.14</cell></row><row><cell>80</cell><cell>-D</cell><cell cols="3">91.29 ± 0.81 91.42 ± 0.92 91.45 ± 1.00 80.16 ± 2.35</cell><cell>1.13 ± 0.04</cell></row><row><cell>80</cell><cell>+D</cell><cell cols="3">91.69 ± 0.48 90.77 ± 1.76 92.79 ± 0.95 81.27 ± 2.02</cell><cell>1.15 ± 0.11</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_14.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for aortic image sequence segmentation with sparse annotations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_67" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inferring the 3D standing spine posture from 2D radiographs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bayat</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_75</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_75" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="775" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using deep learning: a review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chuah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="111985" to="112004" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3D vessel segmentation with limited guidance of 2D structure-agnostic vessel annotations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03299</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computational methods for liver vessel segmentation in medical imaging: A review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ciecholewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kassjański</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2027</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Segmentation of peripancreatic arteries in multispectral computed tomography imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dima</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87589-3_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87589-3_61" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12966</biblScope>
			<biblScope unit="page" from="596" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Volume rendering</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Drebin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Siggraph Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiscale vessel enhancement filtering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Vincken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<idno type="DOI">10.1007/BFb0056195</idno>
		<ptr target="https://doi.org/10.1007/BFb0056195" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 1998</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Colchester</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Delp</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1496</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Panoptic NeRF: 3D-to-2D label transfer for panoptic urban scene segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision, 3DV 2022</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-09">September 2022. 2022</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constrained deep weak supervision for histopathology image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2376" to="2388" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tracing in 2D to reduce the annotation effort for 3D deep delineation of linear structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koziński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101590</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep projective 3D semantic segmentation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-64689-3_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-64689-3_8" />
	</analytic>
	<monogr>
		<title level="m">CAIP 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Heyden</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Krüger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10424</biblScope>
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A vertebral segmentation dataset with fracture grading</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Löffler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artifi. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">190138</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A segmentation and reconstruction technique for 3D vascular structures</title>
		<author>
			<persName><forename type="first">V</forename><surname>Luboz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2005</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Gerig</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3749</biblScope>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/11566465_6</idno>
		<ptr target="https://doi.org/10.1007/11566465_6" />
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VerSe: a vertebrae labelling and segmentation benchmark for multi-detector CT images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sekuboyina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102166</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Intracranial vessel wall segmentation using convolutional neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2840" to="2847" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jeyaseelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101693</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepvesselnet: vessel segmentation, centerline prediction, and bifurcation detection in 3-d angiographic volumes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tetteh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="page">1285</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised 3D abdominal multi-organ segmentation via deep multi-planar co-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="121" to="140" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
