<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Retrieval of Corresponding US Views in Longitudinal Examinations</title>
				<funder>
					<orgName type="full">King&apos;s College London and King&apos;s College Hospital NHS Foundation Trust</orgName>
				</funder>
				<funder>
					<orgName type="full">National Institute for Health Research</orgName>
					<orgName type="abbreviated">NIHR</orgName>
				</funder>
				<funder ref="#_Mn5UKjg #_XMxR2DP">
					<orgName type="full">Wellcome Trust UK</orgName>
				</funder>
				<funder ref="#_nRwrc6v">
					<orgName type="full">Department of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hamideh</forename><surname>Kerdegari</surname></persName>
							<email>hamideh.kerdegari@kcl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nhat</forename><surname>Phung Tran Huy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Van</forename><forename type="middle">Hao</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Hospital for Tropical Diseases</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thi</forename><forename type="middle">Phuong Thao</forename><surname>Truong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ngoc</forename><forename type="middle">Minh Thu</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thanh</forename><forename type="middle">Phuong</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thi</forename><forename type="middle">Mai Thao</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luigi</forename><surname>Pisani</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Hospital for Tropical Diseases</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Mahidol Oxford Tropical Medicine Research Unit</orgName>
								<address>
									<settlement>Bangkok</settlement>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linda</forename><surname>Denehy</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Melbourne School of Health Sciences</orgName>
								<orgName type="institution" key="instit2">The University of Melbourne</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Razavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Louise</forename><surname>Thwaites</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oxford University Clinical Research Unit</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sophie</forename><surname>Yacoub</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Oxford University Clinical Research Unit</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>King</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><surname>Gomez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Hospital for Tropical Diseases</orgName>
								<address>
									<settlement>Ho Chi Minh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Retrieval of Corresponding US Views in Longitudinal Examinations</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="152" to="161"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">D61920055687A98B8EDDBC2F4DCF8E86</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_15</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Muscle atrophy</term>
					<term>Ultrasound view retrieval</term>
					<term>Self-supervised contrastive learning</term>
					<term>Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeletal muscle atrophy is a common occurrence in critically ill patients in the intensive care unit (ICU) who spend long periods in bed. Muscle mass must be recovered through physiotherapy before patient discharge and ultrasound imaging is frequently used to assess the recovery process by measuring the muscle size over time. However, these manual measurements are subject to large variability, particularly since the scans are typically acquired on different days and potentially by different operators. In this paper, we propose a self-supervised contrastive learning approach to automatically retrieve similar ultrasound muscle views at different scan times. Three different models were compared using data from 67 patients acquired in the ICU. Results indicate that our contrastive model outperformed a supervised baseline model in the task of view retrieval with an AUC of 73.52% and when combined with an automatic segmentation model achieved 5.7% ± 0.24% error in cross-sectional area. Furthermore, a user study survey confirmed the efficacy of our model for muscle view retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Muscle wasting, also known as muscle atrophy (see Fig. <ref type="figure" target="#fig_0">1</ref>), is a common complication in critically ill patients, especially in those who have been hospitalized in the intensive care unit (ICU) for a long period <ref type="bibr" target="#b17">[17]</ref>. Factors contributing to muscle wasting in ICU patients include immobilization, malnutrition, inflammation, and the use of certain medications <ref type="bibr" target="#b13">[13]</ref>. Muscle wasting can result in weakness, impaired mobility, and increased morbidity and mortality. Assessing the degree of muscle wasting in ICU patients is essential for monitoring their progress and tailoring their rehabilitation program to recover muscular mass through physiotherapy before patient discharge. Traditional methods of assessing muscle wasting, such as physical examination, bioelectrical impedance analysis, and dual-energy X-ray absorptiometry, may be limited in ICUs due to the critical illness of patients <ref type="bibr" target="#b15">[15]</ref>. Instead, ultrasound (US) imaging has emerged as a reliable, non-invasive, portable tool for assessing muscle wasting in the ICU <ref type="bibr" target="#b11">[11]</ref>. The accuracy and reliability of US imaging in assessing muscle wasting in ICU patients have been demonstrated by Parry et al. <ref type="bibr" target="#b12">[12]</ref>. US imaging can provide accurate measurements of muscle size, thickness, and architecture, allowing clinicians to track changes over time. However, these measurements are typically performed manually, which is time-consuming, subject to large variability and depends on the expertise of the operator. Furthermore, operators might be different from day to day and/or start scanning from different positions in each scan which will cause further variability.</p><p>In recent years, self-supervised learning (SSL) has gained popularity for automated diagnosis in the field of medical imaging due to its ability to learn from unlabeled data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">16]</ref>. Previous studies on SSL for medical imaging have focused on designing pretext tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b18">18]</ref>. A class of SSL, contrastive learning (CL), aims to learn feature representations via a contrastive loss function to distinguish between negative and positive image samples. A relatively small number of works have applied CL to US imaging, for example to synchronize different cross-sectional views <ref type="bibr" target="#b6">[7]</ref> and to perform view classification <ref type="bibr" target="#b3">[4]</ref> in echocardiography (cardiac US).</p><p>In this paper, we focus on the underinvestigated application of view matching for longitudinal RF muscle US examinations to assess muscle wasting. Our method uses a CL approach (see Fig. <ref type="figure" target="#fig_1">2</ref>) to learn a discriminative representation from muscle US data which facilitates the retrieval of similar muscle views from different scans. The novel contributions of this paper are: 1) the first investigation of the problem of muscle US view matching for longitudinal image analysis, and 2) our approach is able to automatically retrieve similar muscle views between different scans, as shown by quantitative validation and qualitatively through a clinical survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>Muscle wasting assessment requires matching of corresponding cross-sectional US views of the RF over subsequent (days to weeks apart) examinations. The first acquisition is carried out following a protocol to place the transducer half way through the thigh and perpendicular to the skin, but small variations in translation and angulation away from this standard view are common. This scan produces the reference view at time T 1 (RT 1 ). The problem is as follows: given RT 1 , the task is to retrieve the corresponding view (V T 2 ) at a later time (T 2 ) from a sequence of US images captured by the operator using the transducer at approximately the same location and angle as for T 1 . The main challenges of this problem include: (1) the transducer pose and angle might be different, (2) machine settings might be slightly different, and (3) parts of the anatomy (specifically the RF) might change in shape and size over time. As a result, our aim is to develop a model that can select the most similar view acquired during T 2 to the reference view RT 1 acquired at T 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contrastive Learning Framework for Muscle View Matching</head><p>Inspired by the SimCLR algorithm <ref type="bibr" target="#b4">[5]</ref>, our model learns representations by maximizing the similarity between two different augmented views of the same muscle US image via a contrastive loss in the latent space. We randomly sample a minibatch of N images from the video sequences over three times T 1 , T 2 and T 3 , and define the contrastive learning on positive pairs (Xi, Xj) of augmented images derived from the minibatch, resulting in 2N samples. Rather than explicitly sampling negative examples, given a positive pair, we consider the other 2(N -1) augmented image pairs within a minibatch as negative.</p><p>The contrastive loss function for a positive pair (Xi, Xj) is defined as:</p><formula xml:id="formula_0">L i C = -log exp(sim(z i , z j )/τ ) 2n k=1 1 [k =i] exp(sim(z i , z k )/τ ) , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where 1 ∈ (0, 1), τ is a temperature parameter and sim(•) denotes the pairwise cosine similarity. z is a representation vector, calculated by z = g(f (X)), where f(•) indicates a shared encoder and g(•) is a projection head. L i C is computed across all positive pairs in a mini-batch. Then f (•) and g(•) are trained to maximize similarity using this contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Model Architecture</head><p>The model architecture is shown in Fig. <ref type="figure" target="#fig_1">2a</ref>. First, we train the contrastive model to identify the similarity between two images, which are a pair of image augmentations created by horizontal flipping and random cropping (size 10×10) applied on a US image (i.e., they represent different versions of the same image). Each image of this pair (Xi, Xj) is fed into an encoder to extract representation vectors (hi, hj) from them. The encoder architecture (Fig. <ref type="figure" target="#fig_1">2b</ref>) has four conv layers (kernel 3 × 3) with ReLU and two max-poolings. A projection head (a multilayer perceptron with two dense layers of 512 nodes) follows mapping these representations to the space where the contrastive loss is applied.</p><p>Second, we use the trained encoder f (•) for the training of our main task (i.e. the downstream task), which is the classification of positive and negative matches (corresponding and non-corresponding views) of our test set. For that, we feed a reference image X ref , and a candidate frame X j to the encoder to obtain the representations hi, hj and feed these in turn to a classification network (shown in Fig. <ref type="figure" target="#fig_1">2c</ref>) that contains four dense layers with ReLU activation and a softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Materials</head><p>The muscle US exams were performed using GE Venue Go and GE Vivid IQ machines, both with linear probes (4.2-13.0 MHz), by five different doctors. During examination, patients were in supine position with the legs in a neutral rotation with relaxed muscle and passive extension. Measurements were taken at the point three fifths of the way between the anterior superior iliac spine and the patella upper pole. The transducer was placed perpendicular to the skin and to the longitudinal axis of the thigh to get the cross-sectional area of the RF. An excess of US gel was used and pressure on the skin was kept minimal to maximise image quality. US measurements were taken at ICU admission (T 1 ), 2-7 d after admission (T 2 ) and at ICU discharge (T 3 ). For this study, 67 Central Nervous System (CNS) and Tetanus patients were recruited and their data were acquired between June 2020 and Feb 2022. Each patient had an average of six muscle ultrasound examinations, three scans for each leg, totalling 402 examinations. The video resolution was 1080 × 1920 with a frame rate of 30fps. This study was performed in line with the principles of the Declaration of Helsinki. Approval was granted by the Ethics Committee of the Hospital for Tropical Diseases, Ho Chi Minh City and Oxford Tropical Research Ethics Committee. The contrastive learning network was trained without any annotations. However, for the view matching classification task, our test data were annotated automatically as positive and negative pairs based upon manual frame selection by a team of five doctors comprising three radiologists and two ultrasound specialists with expertise in muscle ultrasound. Specifically, each frame in an examination was manually labelled as containing a similar view to the reference RT 1 or not. Based upon these labelings, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the positive pairs are combinations of similar views within each examination (T 1 /T 2 /T 3 ) and between examinations. The rest are considered negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Our model was implemented using Tensorflow 2.7. During training, input videos underwent experimentation with clip sizes of 256 × 256, 128 × 128, and 64 × 64. Eventually, they were resized to 64 × 64 clips, which yielded the best performance. All the hyperparameters were chosen using the validation set. For the CL training, the standard Adam optimizer was used with learning rate =0.00001, kernel size = 3 × 3, batch size = 128, batch normalization, dropout with p = 0.2 and L2 regularization of the model parameters with a weight = 0.00001. The CL model was trained on 80% of the muscle US data for 500 epochs. For the view retrieval model, the standard Adam optimizer with learning rate = 0.0001, batch size = 42 and dropout of p = 0.2 was used. The classifier was trained on the remaining 20% of the data (of which 80% were used for training, 10% for validation and 10% for testing) and the network converged after 60 epochs. For the supervised baseline model, the standard Adam optimizer was used with learning rate =0.00001, kernel size = 3 × 3, batch size = 40, and batch normalization. Here, we used the same data splitting as our view retrieval classifier. The code we used to train and evaluate our models is available at https://github.com/ hamidehkerdegari/Muscle-view-retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Quantitative Results. We carried out two quantitative experiments. First, we evaluated the performance of the view classifier. Second, we evaluated the quality of the resulting cross-sectional areas segmented using a U-Net <ref type="bibr" target="#b14">[14]</ref>.</p><p>The classifier performance was carried out by measuring, for the view retrieval task, the following metrics: Area Under the Curve (AUC), precision, recall, and F1-score. Because there is no existing state of the art for this task, we created two baseline models to compare our proposed model to: first, a naive image-space comparison using normalized cross-correlation (NCC) <ref type="bibr" target="#b2">[3]</ref>, and second, a supervised classifier. The supervised classifier has the same architecture as our CL model, but with the outputs of the two networks being concatenated after the representation h followed by a dense layer with two nodes and a softmax activation function to produce the probabilities of being a positive or negative pair. Table <ref type="table" target="#tab_0">1</ref> shows the classification results on our dataset. As shown in Table <ref type="table" target="#tab_0">1</ref>, our proposed method achieved superior performance in terms of AUC, precision, recall, and F1-score compared to all other models. The NCC method demonstrated the lowest performance, as it lacked the capability to accurately capture dynamic changes and deformations in US images which can result in significant structural differences. A representative example of a modelretrieved view for one case is presented in Fig. <ref type="figure" target="#fig_3">4</ref>. It shows positive, negative, and middle (i.e., images with a probability value between the highest and lowest values predicted by our model) pairs of images generated by our model from a patient's left leg. As reference, on the left we show the user pick (RT 2 ). To assess the quality of the resulting cross-sections, we calculated the mean relative absolute area difference (d) between the ground truth (a GT ) frame and that of the model predicted frame (a pred ) for each examination as follows:</p><formula xml:id="formula_2">d = |a GT -a pred | a GT (2)</formula><p>We applied a trained U-Net model (already trained with 1000 different US muscle images and manual segmentations). Results showed an overall cross-sectional mean relative absolute area error of 5.7% ± 0.24% on the test set (Full details provided in Fig. <ref type="figure" target="#fig_4">5</ref>, right). To put this number into context, Fig. <ref type="figure" target="#fig_4">5</ref>, left visualizes two cases where the relative error is 2.1% and 5.2%.</p><p>Qualitative Results. We conducted a user study survey to qualitatively assess our model's performance. The survey was conducted blindly and independently by four clinicians and consisted of thirty questions. In each, clinicians were shown two different series of three views of the RF: (1) RT 1 , GT match from T 2 and model prediction from T 2 , and (2) RT 1 , a random frame from T 2 and model prediction from T 2 . They were asked to indicate which (second or third) was the best match with the first image. The first question aimed to determine if the model's performance was on par with clinicians, while the second aimed to determine if the model's selection of images was superior to a randomly picked frame. As shown in Fig. <ref type="figure" target="#fig_5">6</ref>, left, clinicians chose the model prediction more often than the GT; however, this difference was not significant (paired Student's t-test, p = 0.44, significance= 0.05). Therefore, our model can retrieve the view as well as clinicians, and significantly better (Fig. <ref type="figure" target="#fig_5">6</ref>, right) than randomly chosen frames (paired Student's t-test, p = 0.02, significance= 0.05). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>This paper has presented a self-supervised CL approach for automatic muscle US view retrieval in ICU patients. We trained a classifier to find positive and negative matches. We also computed the cross-sectional area error between the ground truth frame and the model prediction in each acquisition time to evaluate model performance. The performance of our model was evaluated on our muscle US video dataset and showed AUC of 73.52% and 5.7% ± 0.24% error in cross-sectional view. Results showed that our model outperformed the supervised baseline approach. This is the first work proposed to identify corresponding ultrasound views over time, addressing an unmet clinical need.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of the cross-section of the rectus femoris (RF) on one ICU patient showing muscle mass reduction from admission (9cm 2 , left) to discharge (6cm 2 , right).</figDesc><graphic coords="2,85,47,255,62,281,68,89,53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Proposed architecture for US view retrieval. (a): Overview, a shared encoder and a projection head (two dense layers, each 512 nodes). (b): Encoder subnetwork. (c): The classification subnetwork has four dense layers of 2024, 1024, 512 and 2 features.</figDesc><graphic coords="3,43,80,133,28,337,00,173,65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example of positive and negative pair labeling for US videos acquired at T1 and T2. Positive pairs are either the three views acquired consecutively at the Ti, or a view labeled at T1 and the corresponding view on the same leg at T2 or T3.</figDesc><graphic coords="5,109,80,299,90,204,37,130,45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results showing three sample positive, medium and negative predicted pairs by our model when ground truth (GT) from T1 is compared with the T2 video.</figDesc><graphic coords="7,57,81,185,09,308,44,144,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Left: cross-sectional area error for T1 and T2 examinations (acquisition times). Right: mean relative absolute area difference (d) for T1T1, T1T2, T1T3 (reference frame from T1 and corresponding predicted frames from T1, T2 and T3 respectively) and overall acquisition time.</figDesc><graphic coords="8,63,96,196,70,324,94,109,15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. User study survey results. Left: when T1GT, T2GT and model prediction (from T2) are shown to the users. Right: when T1GT, T2-random frame and model prediction (from T2) are shown to the users.</figDesc><graphic coords="8,68,97,458,57,314,92,111,10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>AUC, precision, recall and F1 score results on the muscle video dataset.</figDesc><table><row><cell>Model</cell><cell>AUC</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell cols="3">Normalized cross-correlation 68.35 % 58.65 %</cell><cell cols="2">63.12 % 60.8 %</cell></row><row><cell>Supervised baseline model</cell><cell cols="2">69.87 % 65.81 %</cell><cell cols="2">60.57 % 63.08 %</cell></row><row><cell>Proposed model</cell><cell cols="2">73.52 % 67.2 %</cell><cell cols="2">68.31 % 67.74 %</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>H. Kerdegari-This work was supported by the <rs type="funder">Wellcome Trust UK</rs> (<rs type="grantNumber">110179/Z/15/Z</rs>, <rs type="grantNumber">203905/Z/16/Z</rs>, <rs type="grantNumber">WT203148/Z/16/Z</rs>). <rs type="person">H. Kerdegari</rs>, <rs type="person">N. Phung</rs>, <rs type="person">R. Razavi</rs>, <rs type="person">A. P King</rs> and <rs type="person">A. Gomez</rs> acknowledge financial support from the <rs type="funder">Department of Health</rs> via the <rs type="funder">National Institute for Health Research (NIHR) comprehensive Biomedical Research Centre award to Guy's and St Thomas' NHS Foundation Trust</rs> in partnership with <rs type="funder">King's College London and King's College Hospital NHS Foundation Trust</rs>. Vital Consortium: Membership of the VITAL Consortium is provided in the Acknowledgments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Mn5UKjg">
					<idno type="grant-number">110179/Z/15/Z</idno>
				</org>
				<org type="funding" xml:id="_XMxR2DP">
					<idno type="grant-number">203905/Z/16/Z</idno>
				</org>
				<org type="funding" xml:id="_nRwrc6v">
					<idno type="grant-number">WT203148/Z/16/Z</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Big self-supervised models advance medical image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3478" to="3488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-supervised learning for cardiac mr image segmentation by anatomical position prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-860" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="541" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross correlation. Cross Correlation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bourke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Auto Correlation-2D Pattern Identification</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contrastive learning for view classification of echocardiograms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8" />
	</analytic>
	<monogr>
		<title level="m">Simplifying Medical Ultrasound: Second International Workshop, ASMUS 2021, Held in Conjunction with MICCAI 2021</title>
		<meeting><address><addrLine>Strasbourg, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-09-27">27 September 2021. 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">USCL: pretraining deep ultrasound image diagnosis model through video contrastive representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_60</idno>
		<idno>978-3-030-87237-3 60</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="627" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Echo-syncnet: self-supervised cardiac view synchronization in echocardiography</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Dezaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2092" to="2104" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A systematic benchmarking analysis of transfer learning for medical image analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Hosseinzadeh Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DART/FAIR -2021</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12968</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87722-4_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87722-41" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised pretraining with dicom metadata in ultrasound imaging</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="732" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-supervised representation learning for ultrasound video</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Droste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drukker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1847" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bedside ultrasound measurement of skeletal muscle</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mourtzakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wischmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion Clinical Nutrition Metabolic Care</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="389" to="395" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ultrasonography in the intensive care setting can be used to detect changes in the quality and quantity of muscle and is related to muscle strength and function</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Parry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Crit. Care</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1151" to="e1159" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Acute skeletal muscle wasting in critical illness</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A</forename><surname>Puthucheary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">310</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1591" to="1600" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Muscular weakness and muscle wasting in the critically ill</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Schefold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wollersheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Grunow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Luedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Z'graggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weber-Carstens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cachexia. Sarcopenia Muscle</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1399" to="1412" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Moco pretraining improves representation and transferability of chest x-ray models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sowrirajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<biblScope unit="page" from="728" to="744" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Functional outcome and muscle wasting in adults with tetanus</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Trung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. R. Soc. Trop. Med. Hyg</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="706" to="713" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning for 3d medical images by playing a rubik&apos;s cube</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-946" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="420" to="428" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
