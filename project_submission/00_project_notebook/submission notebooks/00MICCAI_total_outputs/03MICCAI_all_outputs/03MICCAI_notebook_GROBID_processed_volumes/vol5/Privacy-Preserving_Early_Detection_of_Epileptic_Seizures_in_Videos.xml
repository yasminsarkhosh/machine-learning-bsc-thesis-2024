<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Privacy-Preserving Early Detection of Epileptic Seizures in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Deval</forename><surname>Mehta</surname></persName>
							<email>deval.mehta@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of IT</orgName>
								<orgName type="laboratory">AIM for Health Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Monash Medical AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shobi</forename><surname>Sivathamboo</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Neuroscience</orgName>
								<orgName type="department" key="dep2">Central Clinical School</orgName>
								<orgName type="department" key="dep3">Faculty of Medicine Nursing and Health Sciences</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Neurology</orgName>
								<orgName type="institution">Alfred Health</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Departments of Medicine and Neurology</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Royal Melbourne Hospital</orgName>
								<address>
									<settlement>Parkville</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hugh</forename><surname>Simpson</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Neuroscience</orgName>
								<orgName type="department" key="dep2">Central Clinical School</orgName>
								<orgName type="department" key="dep3">Faculty of Medicine Nursing and Health Sciences</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Neurology</orgName>
								<orgName type="institution">Alfred Health</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Kwan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Neuroscience</orgName>
								<orgName type="department" key="dep2">Central Clinical School</orgName>
								<orgName type="department" key="dep3">Faculty of Medicine Nursing and Health Sciences</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Neurology</orgName>
								<orgName type="institution">Alfred Health</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Departments of Medicine and Neurology</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Royal Melbourne Hospital</orgName>
								<address>
									<settlement>Parkville</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Terence</forename><surname>O’brien</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of IT</orgName>
								<orgName type="laboratory">AIM for Health Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Monash Medical AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Airdoc-Monash Research Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Privacy-Preserving Early Detection of Epileptic Seizures in Videos</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="210" to="219"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A80426FB9C4B0007EB761409EB4CFC71</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_21</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>epilepsy</term>
					<term>early detection</term>
					<term>knowledge distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we contribute towards the development of videobased epileptic seizure classification by introducing a novel framework (SETR-PKD), which could achieve privacy-preserved early detection of seizures in videos. Specifically, our framework has two significant components -(1) It is built upon optical flow features extracted from the video of a seizure, which encodes the seizure motion semiotics while preserving the privacy of the patient; (2) It utilizes a transformer based progressive knowledge distillation, where the knowledge is gradually distilled from networks trained on a longer portion of video samples to the ones which will operate on shorter portions. Thus, our proposed framework addresses the limitations of the current approaches which compromise the privacy of the patients by directly operating on the RGB video of a seizure as well as impede real-time detection of a seizure by utilizing the full video sample to make a prediction. Our SETR-PKD framework could detect tonicclonic seizures (TCSs) in a privacy-preserving manner with an accuracy of 83.9% while they are only half-way into their progression. Our data and code is available at https://github.com/DevD1092/seizure-detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Epilepsy is a chronic neurological condition that affects more than 60 million people worldwide in which patients experience epileptic seizures due to abnormal brain activity <ref type="bibr" target="#b16">[17]</ref>. Different types of seizures are associated with the specific part of the brain involved in the abnormal activity <ref type="bibr" target="#b7">[8]</ref>. Thus, accurate detection of the type of epileptic seizure is essential to epilepsy diagnosis, prognosis, drug selection and treatment. Concurrently, real-time seizure alerts are also essential for caregivers to prevent potential complications, such as related injuries and accidents, that may result from seizures. Particularly, patients suffering from tonic-clonic seizures (TCSs) are at a high risk of sudden unexpected death in epilepsy (SUDEP) <ref type="bibr" target="#b17">[18]</ref>. Studies have shown that SUDEP is caused by severe alteration of cardiac activity actuated by TCS, leading to immediate death or cardiac arrest within minutes after the seizure <ref type="bibr" target="#b4">[5]</ref>. Therefore, it is critical to accurately and promptly detect and classify epileptic seizures to provide better patient care and prevent any potentially catastrophic events.</p><p>The current gold standard practice for detection and classification of epileptic seizures is the hospital-based Video EEG Monitoring (VEM) units <ref type="bibr" target="#b22">[23]</ref>. However, this approach is expensive and time consuming which is only available at specialized centers <ref type="bibr" target="#b2">[3]</ref>. To address this issue, the research community has developed automated methods to detect and classify seizures based on several modalities -EEG <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>, accelerometer <ref type="bibr" target="#b15">[16]</ref>, and even functional neuroimaging modalities such as fMRI <ref type="bibr" target="#b21">[22]</ref> and electrocorticography (ECoG) <ref type="bibr" target="#b23">[24]</ref>. Although, there have been developments of approaches for the above modalities, seizure detection using videos remains highly desirable as it involves no contact with the patient and is easier to setup and acquire data compared to other modalities. Thus, researchers have also developed automated approaches for the video modality.</p><p>Initial works primarily employed hand-crafted features based on patient motion trajectory by attaching infrared reflective markers to specific body key points <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>. However, these approaches were limited in performance due to their inability to generalize to changing luminance (night time seizures) or when the patient is occluded (covered by a bed sheet) <ref type="bibr" target="#b13">[14]</ref>. Thus, very recently deep learning (DL) models have been explored for this task <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. <ref type="bibr" target="#b28">[29]</ref> demonstrated that DL models could detect generalized tonic-clonic seizures (GTCSs) from the RGB video of seizures. Authors in <ref type="bibr" target="#b20">[21]</ref> radically used transfer learning (from action recognition task) to train DL networks for distinguishing focal onset seizures (FOSs) from bilateral TCSs using features extracted from the RGB video of seizures. Whereas, the authors in <ref type="bibr" target="#b11">[12]</ref> developed a DL model to discriminate dystonia and emotion in videos of Hyperkinetic seizures. However, these developed approaches have two crucial limitations -(1) As these approaches directly operate on RGB videos, there is a possibility of privacy leakage of the sensitive patient data from videos. Moreover, obtaining consent from patients to share their raw RGB video data for building inter-cohort validation studies and generalizing these approaches on a large scale becomes challenging; (2) The current approaches consider the full video of a seizure to make predictions, which makes early detection of seizures impossible. The duration of a seizure varies significantly among patients, with some lasting as short as 30 s while others can take minutes to self-terminate. Thus, it is unrealistic to wait until the completion of a long seizure to make a prediction and alert caregivers.</p><p>In this work, we address the above two challenges by building an in-house dataset of privacy-preserved extracted features from a video and propose a framework for early detection of seizures. Specifically, we investigate two aspects - <ref type="bibr" target="#b0">(1)</ref> The feasibility of detecting and classifying seizures based only on optical flow, a modality that captures temporal differences in a scene while being intrinsically privacy-preserving. <ref type="bibr" target="#b1">(2)</ref> The potential of predicting the type of seizure during its progression by analyzing only a fraction of the video sample. Our early detection approach is inspired by recent developments in early action recognition in videos <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. We develop a custom feature extractor-transformer framework, named SEizure TRansformer (SETR) block for processing a single video sample. To achieve early detection from a fraction of the sample, we propose Progressive Knowledge Distillation (PKD), where we gradually distill knowledge from SETR blocks trained on longer portions of a video sample to SETR blocks which will operate on shorter portions. We evaluate our proposed SETR-PKD framework on two datasets -an in-house dataset collected from a VEM unit in a hospital and a publicly available dataset of video-extracted features (GESTURES) <ref type="bibr" target="#b20">[21]</ref>. Our experiments demonstrate that our proposed SETR-PKD framework can detect TCS seizures with an accuracy of 83.9% in a privacy-preserving manner when they are only half-way into their progression. Furthermore, we comprehensively compare the performance of direct knowledge distillation with our PKD approach on both optical flow features (in-house dataset) and raw video features (public dataset). We firmly believe that our proposed method makes the first step towards developing a privacypreserving real-time system for seizure detection in clinical practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>In this section, we first outline the process of extracting privacy-preserving information from RGB video samples to build our in-house dataset. Later, we explain our proposed approach for early detection of seizures in a sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Privacy Preserving Optical Flow Acquisition</head><p>Our in-house dataset of RGB videos of patients experiencing seizures resides on hospital premises and is not exportable due to the hospital's ethics agreement<ref type="foot" target="#foot_0">1</ref> . To work around this limitation, we develop a pipeline to extract optical flow information <ref type="bibr" target="#b10">[11]</ref> from the videos. This pipeline runs locally within the hospital and preserves the privacy of the patients while providing us with motion semiotics of the seizures. An example of the extracted optical flow video sample can be seen in Fig. <ref type="figure" target="#fig_1">1</ref>. We use the TV-L1 algorithm <ref type="bibr" target="#b19">[20]</ref> to extract the optical flow features for each video, which we then export out of the hospital for building our proposed approach. We provide more information about our dataset, including the number of patients and seizures, annotation protocol, etc. in Sect. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Early Detection of Seizures in a Sample</head><p>Consider an input optical flow video sample V i as shown in Fig. <ref type="figure" target="#fig_1">1</ref>(a) with a time period of T i , consisting of N frames -{f 0 , f 1 , ...f N -1 }, and having a ground truth label of y i ∈ {0, 1, ...C} where is C the total number of categories. Then, the task of early detection is to build a framework that could classify the category of the sample correctly by analyzing the least possible partial segment of the sample. Thus, to define the problem of early detection, we split the sample V i into k segments -{0, 1, ...k -1} starting from the beginning to the end as shown in Fig. <ref type="figure" target="#fig_1">1(b</ref>). Here V k-1 i corresponds to the full video sample and the descending segments correspond to the reduced partial video samples. We build these partial segments by equally adding the temporal information throughout the sample i.e. the time period for a partial subset V j i of a sample V i is computed as (j + 1) × T i /k. Thus, the early detection task is to correctly predict the category y i of the sample V i from the lowest possible (j) partial segment V j i of V i . In Fig. <ref type="figure" target="#fig_1">1</ref>, we illustrate our proposed framework where -(a) First, we build a Seizure Transformer (SETR) block for processing a single optical flow video sample (b) Later, we employ SETR based Progressive Knowledge Distillation (SETR-PKD) to achieve early detection in a sample.</p><p>Processing a Single Sample. Since seizure patterns comprise of body movements, we implement transfer learning from a feature extractor pre-trained on action recognition task to extract the spatial features from the optical flow frames. Prior work <ref type="bibr" target="#b20">[21]</ref> has shown that Temporal Segment Networks (TSNs) <ref type="bibr" target="#b26">[27]</ref> pretrained on RGB videos of various actions are effective at extracting features from videos of seizures. We also utilize TSNs but pretrained on the optical flow modality, since we have privacy-preserved optical flow frames. The TSNs extract a 1D feature sequence for each frame f j , referred as spatial features in Fig. <ref type="figure" target="#fig_1">1(a)</ref>. The spatial features are then processed by a linear transformation (1-layer MLP) that maps them into motion tokens ∈ R N ×D , where each token has D-dimensions.</p><p>We leverage transformers to effectively learn temporal relations between the extracted spatial features of the seizure patterns. Following the strategy of ViT <ref type="bibr" target="#b5">[6]</ref>, after extracting the spatial features, we append a trainable class embedding class embed ∈ R D to the motion tokens. This class embedding serves to represent the temporal relationships between the motion tokens and is later used for classification (class token in Fig. <ref type="figure" target="#fig_1">1(a)</ref>). As the order of the motion tokens is not known, we also add a learnable positional encoding L P OS ∈ R (N +1)×D to the combined motion tokens and class embed . This is achieved using an element-wise addition and we term it as the input X i for the input sample V i .</p><p>To enable the interaction between tokens and learn temporal relationships for input sample classification, we employ the Vanilla Multi-Head Self Attention (MHSA) mechanism <ref type="bibr" target="#b25">[26]</ref>. First, we normalize the input sequence X i ∈ R (N +1)×D by passing it through a layer normalization, yielding X i . We then use projection matrices  for query, key, and value respectively. Next, we compute a dot product of Q with K and apply a softmax layer to obtain weights on the values. We repeat this self-attention computation N h times, where N h is the number of heads, and concatenate their outputs. Eq. 1, 2 depict the MHSA process in general.</p><formula xml:id="formula_0">(Q i , K i , V i ) = (X i W Q i , X i W K i , X i W V i ) to project X i into queries (Q), keys<label>(</label></formula><formula xml:id="formula_1">A i = Sof tmax(Q i K i ) (1) MHSA(X i ) = A i × W V i , X i = Norm(X i )<label>(2)</label></formula><p>Subsequently, the output of MHSA is passed to a two-layered MLP with GELU non-linearity while applying layer normalization and residual connections concurrently. Eq. 3, 4 represent this overall process.</p><formula xml:id="formula_2">m l = MHSA(X l-1 ) + X l-1 , l= 1...L<label>(3)</label></formula><formula xml:id="formula_3">m l = MLP (Norm(m l )) + m l , l= 1...L<label>(4)</label></formula><p>where m L ∈ R (N +1)×D are the final output feature representations and L is the total number of encoding layers in the Transformer Encoder. Note that the first R N ×D features correspond to the patch tokens , while the final R D correspond to the class token of the m L as shown in Fig. <ref type="figure" target="#fig_1">1</ref>(a). As mentioned earlier, we then use a one-layer MLP to predict the class label from the class token . We refer to this whole process as a SEizure TRansformer (SETR) block shown in Fig. <ref type="figure" target="#fig_1">1(a)</ref>.</p><p>Progressive Knowledge Distillation. To achieve early detection, we use Knowledge Distillation in a Progressive manner (PKD), starting from a SETR block trained on a full video sample and gradually moving to a SETR block trained on a partial video sample, as shown in Fig. <ref type="figure" target="#fig_1">1(b</ref>). Directly distilling from a SETR block which has seen a significantly longer portion of the video (say V k-1 i ) to a SETR block which has only seen a smaller portion of the video sample (say V 0 i ) will lead to considerable mismatches between the features extracted from the two SETRs as there is a large portion of the input sample that the student 0 SETR has not seen. In contrast, our proposed PKD operates in steps. First we pass the knowledge from teacher (T eacher k-1 in Fig. <ref type="figure" target="#fig_1">1</ref></p><formula xml:id="formula_4">(b)) SETR trained on V k-1 i to a student (Sub -teacher k-2 ) SETR that operates on V k-2 i</formula><p>; Later, the Subteacher k-2 SETR passes its distilled knowledge to its subsequent student (Sub-teacher k-3 ) SETR, and this continues until the final Sub-teacher 1 SETR passes its knowledge to the bottom most Student 0 SETR. Since the consecutive segments of the videos do not differ significantly, PKD is more effective than direct distillation, which is proven by results in Sect. 3.4.</p><p>For distilling knowledge we consider both class token and patch tokens of the teacher and student networks. A standard Kullback-Leibler divergence (L KL ) loss is applied between the probabilities generated from class token of the teacher and student SETR, whereas a mean squared error (L MSE ) loss is computed between the patch tokens of teacher and student SETR. Overall, a student SETR is trained with three losses -L KL and L MSE loss for knowledge distillation, and a cross-entropy (L CE ) loss for classification, given by the equations below.</p><formula xml:id="formula_5">L KL = τ 2 j q T j (log(q T j /q S j ))<label>(5)</label></formula><p>where q S j and q T j are the soft probabilities (moderated by temperature τ ) of the student and teacher SETRs for the j th class, respectively.</p><formula xml:id="formula_6">L mse = ( N i=0 |p T i -p S i 2 )/N (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where N is the number of patches and p T i and p S i are the patches of teacher and student SETRs respectively.</p><formula xml:id="formula_8">L total = L CE + αL KL + βL mse (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where α and β are the weights for L KL and L MSE loss respectively.</p><p>3 Datasets and Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">In-House and Public Dataset</head><p>Our in-house dataset 2 contains optical flow information extracted from highdefinition (1920 × 1080 pixels at 30 frames per second) video recordings of TCS seizures (infrared cameras are used for nighttime seizures) in a VEM unit in hospital. To annotate the dataset, two neurologists examined both the video and corresponding EEG to identify the clinical seizure onset (t ON ) and clinical seizure offset (t OF F ) times for each seizure sample. We curated a dataset comprising of 40 TCSs from 40 epileptic patients, with one sample per patient. The duration (in seconds) of the 40 TCSs in our dataset ranges from 52 to 367 s, with a median duration of 114 s. We also prepared normal samples (no seizure) for each patient by considering the pre-ictal duration from (t ON -300) to (t ON -60) seconds, resulting in dataset of 80 samples (40 normal and 40 TCSs). We refrain from using the 60 s prior to clinical onset as it corresponds to the transition period to the seizure containing preictal activity <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. We use a 5-fold cross validation (split based on patients) for training and testing on our dataset. We also evaluate the effectiveness of our early detection approach on the GESTURES dataset <ref type="bibr" target="#b20">[21]</ref>, which contains features extracted from RGB video samples of seizures. The dataset includes two seizure types -106 focal onset seizures (FOS) and 77 Tonic-Clonic Seizures (TCS). In contrast to our in-house dataset, the features are provided by the authors, and we directly input them into our SETR block without using a feature extractor. To evaluate our method, we adopt the stratified 10-fold cross-validation protocol as used in GESTURES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Implementation and Evaluation Metrics</head><p>We implement all experiments in PyTorch 1.8.1 on a single A100 GPU. The SETR block takes in a total of 64 frames (N ) with 512 1-D spatial feature per frame, has 8 MHSA heads (N h ) with a dropout rate of 0.1, 3 encoder layers (L), and 256 hidden dimensions (D). For early detection, we experiment by progressively segmenting a sample into -{4,8,16} parts (k). We employ a grid search to select the weight of 0.2 and 0.5 for KL divergence (τ = 10) and MSE loss respectively. We train all methods with a batch size of 16, a learning rate of 1e-3 and use the AdamW optimizer with a weight decay of 1e-4 for a total 50 epochs. For GESTURES dataset, we implement a weighted BCE loss to deal with the dataset imbalance, whereas for our in-house dataset we implement the standard BCE loss. We use precision, recall and f1-score for benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance for Early Detection</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the benchmarking performance of all techniques with varying fractions of input video samples on both datasets. We observed three key findings from the results in Table <ref type="table" target="#tab_0">1</ref>. First, transformer-based methods such as our proposed SETR-PKD and OaDTR exhibit better performance retention compared to LSTM-based techniques (RULSTM, Slowfast RULSTM, EgoAKD, GESTURES) with a reduction in the fraction of input sample. Second, SETR-PKD performance increases with k=8 from k=4, but saturates at k=16 for inhouse dataset, whereas it achieves the best performance for k=4 for GESTURES dataset. The median seizure length for the in-house dataset and GESTURES dataset is 114 s and 71 s, respectively. As a result, PKD using relatively longer partial segments (k=4) is sufficient for GESTURES, while shorter partial segments (k=8) are required for our dataset. Thus, the optimal value of k for PKD may vary depending on a dataset. Finally, we observed better performance on the GESTURES dataset, which is expected given the more detailed and refined features extracted from RGB video compared to optical flow information. To validate our approach of progressive knowledge distillation in a fair manner, we conducted an ablation study to compare it with direct knowledge distillation. Figure <ref type="figure" target="#fig_2">2</ref> shows the comparison of the accuracy of the two approaches for different fractions of the input video sample on both datasets. The results indicate that although direct knowledge distillation can increase performance, it is less effective when the knowledge gap is wide, i.e., from a SETR block trained on a full input sample to a SETR block trained on a minimal fraction of the input sample (1/8, 1/4, .. 1/2) compared to when the knowledge gap is small (5/8, .. 7/8). On the other hand, our SETR-PKD approach significantly improves performance for minimal fractions of input samples on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Progressive V/s Direct Knowledge Distillation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we show that it is possible to detect epileptic seizures from optical flow modality in a privacy-preserving manner. Moreover, to achieve real-time seizure detection, we specifically develop a novel approach using progressive knowledge distillation which proves to detect seizures more accurately during their progression itself. We believe that our proposed privacy-preserving early detection of seizures will inspire the research community to pursue real-time seizure detection in videos as well as facilitate inter-cohort studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>K), and values (V), whereW Q/K/V i ∈ R D×D are the projection matrices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our proposed framework -(a) SEizure TRansformer (SETR) block for a single optical flow video sample (b) SETR based Progressive Knowledge Distillation (SETR-PKD) for early detection of seizures in a sample. (Best viewed in zoom and color).</figDesc><graphic coords="5,52,29,53,72,319,60,199,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Performance comparison of direct knowledge distillation and progressive knowledge distillation between SETR blocks for different fractions of input video sample.</figDesc><graphic coords="8,76,98,186,50,298,60,121,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Benchmarking of different techniques for different fraction {1/4, 1/2, 3/4, Full} of input video sample. The performance is presented as mean of -{Precision/Recall/F1-score} across the 5-folds &amp; 10-folds for in-house and GES-TURES dataset respectively. (Best viewed in zoom). .60/0.59 0.74/0.73/0.73 0.82/0.83/0.82 0.94/0.94/0.94 0.68/0.66/0.66 0.74/0.72/0.73 0.86/0.85/0.85 0.97/0.99/0.98 SETR 0.61/0.60/0.60 0.75/0.73/0.74 0.84/0.83/0.83 0.96/0.95/0.95 0.67/0.66/0.66 0.73/0.74/0.73 0.88/0.88/0.88 0.98/0.99/0.98 SETR-PKD (k=4) 0.63/0.62/0.62 0.78/0.79/0.78 0.89/0.90/0.89 0.96/0.95/0.95 0.74/0.73/0.73 0.86/0.85/0.85 0.96/0.95/0.95 0.98/0.99/0.98 SETR-PKD (k=8) 0.70/0.69/0.69 0.86/0.84/0.85 0.92/0.93/0.92 0.96/0.95/0.95 0.73/0.74/0.73 0.85/0.85/0.85 0.95/0.96/0.95 0.98/0.99/0.98 SETR-PKD (k=16) 0.69/0.69/0.69 0.85/0.84/0.84 0.92/0.92/0.92 0.96/0.95/0.95 0.72/0.73/0.72 0.85/0.84/0.84 0.96/0.95/0.95 0.98/0.99/0.98</figDesc><table><row><cell>Method/Dataset</cell><cell>In-house dataset 1/4 1/2</cell><cell>3/4</cell><cell>Full</cell><cell>GESTURES 1/4</cell><cell>1/2</cell><cell>3/4</cell><cell>Full</cell></row><row><cell>RULSTM [9]</cell><cell cols="7">0.57/0.56/0.56 0.72/0.71/0.71 0.79/0.79/0.79 0.95/0.93/0.94 0.65/0.64/0.64 0.71/0.73/0.72 0.84/0.85/0.84 0.93/0.94/0.93</cell></row><row><cell cols="8">Slowfast RULSTM [19] 0.57/0.56/0.56 0.73/0.72/0.72 0.81/0.80/0.80 0.94/0.94/0.94 0.67/0.65/0.66 0.73/0.72/0.72 0.86/0.84/0.85 0.97/0.95/0.96</cell></row><row><cell>EgoAKD [31]</cell><cell cols="7">0.64/0.65/0.64 0.79/0.80/0.79 0.89/0.90/0.89 0.95/0.94/0.94 0.70/0.69/0.69 0.80/0.79/0.79 0.93/0.90/91 0.97/0.94/0.95</cell></row><row><cell>OaDTR [28]</cell><cell cols="7">0.66/0.65/0.65 0.82/0.83/0.82 0.90/0.90/0.90 0.95/0.95/0.95 0.72/0.69/0.70 0.82/0.83/0.82 0.91/0.92/0.91 0.99/0.99/0.99</cell></row><row><cell>GESTURES [21]</cell><cell>0.59/0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We have a data ethics agreement approved for collection of data at hospital.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We plan to release the in-house optical flow dataset and corresponding code.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_21.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A hierarchical multimodal system for motion analysis in patients with epilepsy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ahmedt-Aristizabal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsy Behav</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="46" to="58" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep motion analysis for epileptic seizure classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ahmedt-Aristizabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dionisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3578" to="3581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video-EEG monitoring in adults</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Cascino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="80" to="93" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NeuroKinect: a novel low-cost 3Dvideo-EEG system for epileptic seizure motion quantification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P S</forename><surname>Cunha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">145669</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sudden unexpected death in epilepsy: epidemiology, mechanisms, and prevention</title>
		<author>
			<persName><forename type="first">O</forename><surname>Devinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Hesdorffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Thurman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lhatoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Neurol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1075" to="1088" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16 × 16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting abnormal pattern of epileptic seizures via temporal synchronization of EEG signals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="601" to="608" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Operational classification of seizure types by the international league against epilepsy: position paper of the ILAE commission for classification and terminology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="522" to="530" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rolling-unrolling LSTMs for action anticipation from first-person video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell. (PAMI)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="4021" to="4036" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Egocentric early action prediction via multimodal transformerbased dual action prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4472" to="4483" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated video analysis of emotion and dystonia in epileptic seizures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bartolomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgonigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsy Res</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page">106953</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glutamatergic pre-ictal discharges emerge at the transition to seizure in human epilepsy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huberfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="627" to="634" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic segmentation of episodes containing epileptic clonic seizures in video sequences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kalitzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Velis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vledder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Da Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3379" to="3385" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated detection of videotaped neonatal seizures based on motion segmentation methods</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Karayiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Wise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Hrachovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Mizrahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Neurophys</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1585" to="1594" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated detection of convulsive seizures using a wearable accelerometer device</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kusmakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Karmakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muthuganapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palaniswami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="421" to="432" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Epilepsy: new advances</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Moshé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perucca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ryvlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="issue">9971</biblScope>
			<biblScope unit="page" from="884" to="898" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unifying the definitions of sudden unexpected death in epilepsy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nashef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ryvlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="233" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SlowFast rolling-unrolling LSTMs for action anticipation in egocentric videos</title>
		<author>
			<persName><forename type="first">N</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Camporese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coscia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3437" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tv-l1 optical flow estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meinhardt-Llopis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Process. On Line</title>
		<imprint>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer learning of deep spatiotemporal networks to model arbitrarily long videos of seizures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_32" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="334" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The use of machine learning and deep learning algorithms in functional magnetic resonance imaging-a systematic review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12644</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Indications and methodology for video-electroencephalographic studies in the epilepsy monitoring unit</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="36" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel quick seizure detection and localization through brain data mining on ECoG dataset</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kabir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5595" to="5608" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cardiorespiratory and autonomic function in epileptic seizures: a video-EEG monitoring study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sivathamboo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsy Behav</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">107271</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2740" to="2755" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">OadTR: online action detection with transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7565" to="7575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video-based detection of generalized tonic-clonic seizures using deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Sarkis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El Atrache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Loddenkemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meisel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2997" to="3008" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A multi-context learning approach for EEG epileptic seizure detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC syst. Biol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Egocentric early action prediction via adversarial knowledge distillation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
