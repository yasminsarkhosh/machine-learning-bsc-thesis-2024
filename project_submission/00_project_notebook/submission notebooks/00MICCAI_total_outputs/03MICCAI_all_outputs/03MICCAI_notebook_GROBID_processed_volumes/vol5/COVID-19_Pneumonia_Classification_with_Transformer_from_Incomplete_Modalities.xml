<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities</title>
				<funder ref="#_eNRMFYg">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_sAdbRN6">
					<orgName type="full">Natural Science Foundation of Shanghai</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eduard</forename><surname>Lloret Carbonell</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yiqing</forename><surname>Shen</surname></persName>
							<idno type="ORCID">0000-0001-7866-3339</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
							<email>yang_xin@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Ke</surname></persName>
							<email>kejing@sjtu.edu.cn</email>
							<idno type="ORCID">0000-0001-7459-257X</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="379" to="388"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">700F4AAF092F7C2B4C858D305504BA7F</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_37</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>COVID-19 Pneumonia Classification</term>
					<term>Hybrid CNN-Transformer</term>
					<term>Multi-modality E. L. Carbonell and Y. Shen-Equal contributions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>COVID-19 is a viral disease that causes severe acute respiratory inflammation. Although with less death rate, its increasing infectivity rate, together with its acute symptoms and high number of infections, is still attracting growing interests in the image analysis of COVID-19 pneumonia. Current accurate diagnosis by radiologists requires two modalities of X-Ray and Computed Tomography (CT) images from one patient. However, one modality might miss in clinical practice. In this study, we propose a novel multi-modality model to integrate X-Ray and CT data to further increase the versatility and robustness of the AI-assisted COVID-19 pneumonia diagnosis that can tackle incomplete modalities. We develop a Convolutional Neural Networks (CNN) and Transformers hybrid architecture, which extracts extensive features from the distinct data modalities. This classifier is designed to be able to predict COVID-19 images with X-Ray image, or CT image, or both, while at the same time preserving the robustness when missing modalities are found. Conjointly, a new method is proposed to fuse three-dimensional and two-dimensional images, which further increase the feature extraction and feature correlation of the input data. Thus, verified with a real-world public dataset of BIMCV-COVID19, the model outperform state-of-the-arts with the AUC score of 79.93%. Clinically, the model has important medical significance for COVID-19 examination when some image modalities are missing, offering relevant flexibility to medical teams. Besides, the structure may be extended to other chest abnormalities to be detected by X-ray or CT examinations. Code is available at https://github.com/edurbi/MICCAI2023.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coronavirus disease 2019 (COVID-19) has been a highly infectious viral disease, that can affect people of all ages, with a persistently high incidence after the outbreak in 2019 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Early detection of the lung inflammatory reaction is crucial to initiate prompt treatment decisions, where the clinical assessment typically depends on two imaging techniques in conjunction, namely X-ray and CT scans <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Specifically, CT scans can provide a three-dimensional volumetric characterization of the patient's lung; while X-rays offer a two-dimensional landscape <ref type="bibr" target="#b4">[5]</ref>. Recently, the use of multimodality data in COVID-19 diagnosis has received increasing interest as it can significantly improve prediction accuracy with complementary information <ref type="bibr" target="#b5">[6]</ref>.</p><p>During the inference stage, modalities can be incomplete amongst some test samples, which is known as incomplete multimodal learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. To address this, it is necessary to implement a strategy that reduces the impact of missing modalities during the inference stage while also offering clinicians the flexibility to use any possible combination of data. Various strategies have been proposed to address this issue, such as generating the missing data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. However, this approach requires training a generative model for each possible missing modality and a larger training set, making it computationally expensive. Therefore, more compact models have emerged that reduces the number of generators <ref type="bibr" target="#b6">[7]</ref>. However, these models are prone to be biased when dealing with multiple modalities. Consequently, a modality-invariant embedding model that makes use of Transformers has been introduced <ref type="bibr" target="#b10">[11]</ref>. Despite the excellent performance, all of the models discussed above have been applied to the datasets, where the modalities used were restricted to three-dimensional, and the structures are similar. However, in the case of COVID-19 pneumonia detection, the clinicians practically employ different dimensional data to interpret the results. One of the major problems during the analysis of medical data is that, sometimes, some data modalities are missing, e.g., the case reported negative, or the examination device was unavailable. <ref type="bibr" target="#b11">[12]</ref> Therefore being able to have a model that is able to adapt to all possible conditions will be greatly beneficial for the detection of COVID-19 pneumonia or any other disease. To this end, a novel method to diagnose COVID-19 pneumonia which can take incomplete CT and X-Ray multimodal data is proposed. The main contributions in this model are three-fold: <ref type="bibr" target="#b0">(1)</ref> We propose a dual feature fusion across different dimensionality data. Instead of sole pre-fusion or post-fusion, both are performed in the multi-modality prediction model. <ref type="bibr" target="#b1">(2)</ref> We design a feature matrix dropout regularization method to improve the reliability and generalization of the model. (3) A feature correlation block is proposed between two of the given modalities to extract latent dependencies. This attention layer further improves the understanding of the patients' evolution when multi-modality images are acquired at different stages of the disease. Overview of the proposed model. This model is composed by three image modalities, on the first modality we use a convolutional-based feature extractor, followed by a Transformer encoder and a random features dropout. The other two modalities consist of an early fusion with the first modality features followed by a transformer encoder. Finally, all of the features have a features dropout layer followed by a Convolutional layer and a fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture Overview</head><p>We propose a model that can detect the COVID-19 pneumonia status from incomplete multi-modalities of CT scans and X-ray images. Specifically, CT scans and X-ray images are first embedded using convolutional layers and then processed by Transformer layers to obtain the global feature correlations. Then, a novel feature fusion layer can simulate incomplete modalities in the latent space while learning to fuse the features. Finally, the predictions are made using a ResNet based classification model followed by a learnable MLP layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Fusion Layer</head><p>The objective of this layer is to combine features from different modalities, which have varying dimensions. To achieve this, we need to reduce the threedimensional CT data to two-dimensional by convolutional layers before fusing them with two-dimensional X-ray data. We then reshape the data into smaller patches and apply a Transformer encoder to it, resulting in a two-dimensional matrix of the desired size. In this study, we investigate the data fusion at two stages, namely the early fusion and late fusion <ref type="bibr" target="#b12">[13]</ref>, where the data are fused twice in our model. Empirically, finding that dual fusion has some slight improvement compared to only early and late fusion as shown in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Matrix Dropout Layer</head><p>This layer helps regularize the model towards learning a modality agnostic representation. Specifically, during the feature fusion layer, random features are dropped from the feature matrix ∈ R X,Y <ref type="bibr" target="#b13">[14]</ref>. In our design certain percentage of data in the form of patches are dropped, where a patch is defined as a subpart of the feature matrix where all values have been set to 0. To generate the patches, we start by creating a random matrix Mδ ∈ R X N , Y N with random values between 0 and 1, where N is the size of the dropout patches on the output images. We then round the values of Mδ using a threshold T , where all values below T are set to 0, and all values equal to or above T are set to 1. This gives us a binary matrix M δ that indicates which parts of the feature matrix should be dropped, i.e.</p><formula xml:id="formula_0">M D = R T (M δ )<label>(1)</label></formula><p>where R T (•) is the round function that converts all values of M δ into 0 if they are under the threshold T and 1 otherwise. Finally the obtained matrix is interpolated or upsampled by a given scale N to match the size of the feature matrices</p><formula xml:id="formula_1">F M ∈ R X,Y . F D = I M (M D ) F M (2)</formula><p>where F D represents the final feature map after the patch dropout, F M is the initial feature map, I M (•) represents a nearest interpolation and (•) (•) represents an element-wise matrix multiplication. This gives us the final feature map F D , which has a similar structure to F M but with some parts of size N × N converted to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Transformer Layer</head><p>Transformers helps finding the different dependencies between the different modalities making use of their attention-based nature. Therefore, in this paper we will make use of the benefits that the transformers offers by implementing a ViT based transformer layer <ref type="bibr" target="#b14">[15]</ref>. In this case we will use the transformer layer as a feature extractor and find the dependencies between the different embedded features. To extract the features, we will first split the image into patches of a fixed size, to afterwards be processed by an embedding layer, in this case the embedding layer will be a convolutional layer. Then each one of the embedded feature will pass through a different Self Attention Layer (SA) formulated as</p><formula xml:id="formula_2">SA L = σ Q L K T √ d L V L , MSA L = Concat(SA 1 L , • • • , SA n L ) • W 0 ,<label>(3)</label></formula><p>where σ(•) represents the softmax function, d represents the size of each one of the heads, and W 0 denotes a value embedding. Meanwhile, Q, K, V ∈ R X,d represent the Query, key and embedding respectively. To constitute the ViT the values of each of the SA are concatenated forming the Multi-Headed Self Attention Layer (MSA) to afterward be normalized by a layer normalization. Finally, a Multi-Layer Perceptron (MLP) is applied to give non-linearity to the data, obtaining</p><formula xml:id="formula_3">A k = MSA(LN (A k-1 )) + A k-1 A k = LN (MLP (A k )) + A k (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where A k-1 is the previous transformer layer, LN(•) is a Layer Normalization. In the vanilla ViT a final MLP is introduced to obtain the probabilities of each class however, in this paper the transformer layer is only used for feature extraction, therefore, the final MLP is deleted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Dual X-Ray Attention</head><p>The Dual X-Ray attention block main idea is to find the different dependencies between the two input X-Ray images. Transformers have showed great results when looking for dependencies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. Therefore, this paper introduced a new attention layer that extracts the dependencies between two X-Ray images. The dependencies are extracted using the Transformer mentioned above Layer having as input both X-Ray images in the multimodality. This layer can also be seen as a fusion layer between two of the input modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We leverage images from the BIMCV-COVID19 dataset <ref type="bibr" target="#b15">[16]</ref>, a public dataset with thousands of different positive and negative cases, for performance evaluation. This dataset is composed of 1200 unique cases from a combination of 1 CT Scan, 1 CT Scan and 1 X-Ray, 1 CT Scan and 2 X-Ray, 1 X-Ray or 2 X-Ray. One patient may have more than one image modality of CT or X-Ray. Regarding the image size of the dataset, the dimension of the CT scans is non-fixed per image, with the average image size around 500 × 500 × 400. To facilitate the training on GPUs, the images perform dimension reduction with factor 2 and resulted in a final image size of 250 × 250 × 200. The dataset is composed of approximately 2900 cases. From which around 1700 are negative, and 1200 are positive. The distribution is unbalanced because we wanted to extract as much data as possible with a balanced number of miss-modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>This framework is implemented with Pytorch 1.13.0 using an NVIDIA A10 GPU with 24 GB of VRAM. The input size is 250 × 250 × 200 for the CT scan and 2048 × 2048 with a batch size of 4 for the X-Ray. For the proposed model, we applied Adam optimizer with an cosine annealing scheduler of an initial learning rate 1e-5, and 100 epochs for training. The data partition for training, validation, and testing is 80%, 10%, and 10% on patient level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We use the following metrics to evaluate the performance of the binary classification model: the AUC score, Recall and Precision. To obtain the values on the Table <ref type="table" target="#tab_0">1</ref>, we made a split form the original dataset without miss-modalities. This table shows the difference between the possible combinations of models we can build to interpret the data. Taking in account the AUC, the worst model is the one using the CT as its only input with only a 65.74% in the AUC score, followed by the model with two X-Ray images as its input, obtaining a 67.98%. The result obtained in the Dual X-Ray multimodality performs around 2% worse than just having one X-Ray as an input. Finally, when having all the modalities performs 3% better in the AUC score than when only having one CT and one X-Ray which has around 71.26% in the AUC metric. A comparison is made between our model and others with different possible variations, shown in the Table <ref type="table" target="#tab_1">2</ref>. The first model is used as a comparison is a fully convolutional model where all of its transformer components are converted into convolutions. This variant shows a significant reduction in all of its parameters with a sharp decrease in the performance of 70.18%, 59.9% and 68.93% in the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>We show the classification performance on Table <ref type="table" target="#tab_2">3</ref>, of the different model components that introduced in the Fig. <ref type="figure" target="#fig_0">1</ref>. We make use of the original dataset with missing modalities. From the Table <ref type="table" target="#tab_2">3</ref>, the addition of the Fusion Layer gives an slight increase of 0.19% in the AUC-score metric compared with the baseline. Meanwhile, we see an observable increase in the recall metric of 2.93%. The addition of the Dual X-Ray layer further increases the performance of the model increasing of 1.8% and 3.43% in the AUC-score and Precision respectively, yet a slight reduction in the recall metric by around 1.35%. Finally, the Regularization layer is topped up to further increases the AUC by a 2.66% and recall by a 13.90%, yet lower Precision score of 5.44%. Thus, compared to the baseline, the final model shows an increase of 4.65% in the AUC metric, a boost of 15.58% in the recall score and only a 2.47% decrease in the precision metric.</p><p>The saliency maps are visualized to pinpoint the diagnostic areas in a CT or X-ray image. Clinically, the saliency maps are helpful to assist radiologists. In the Fig. <ref type="figure" target="#fig_1">2,</ref><ref type="figure">(a)</ref> shows an example of a COVID-19 positive slice extracted from a complete CT scan. Its saliency map is situated next to the image, where a big opacity is found in the left patient's lung. The top-right figure gives an example of COVID-19 positive X-Ray which its correspondent saliency map, which mainly focus on the bottom contour of the lung. Moving to the bottom-left figure we can see a COVID-19 negative CT scan using a different angle compared to the first introduced figure. In this case, similar to what has been seen in the positive case, some opacities have been found however, in this case, the image is negative. Finally, the bottom-right figure is a negative X-Ray case, in this case, similar  to what has been found in the positive X-Ray figure the model mainly looks for the contours of the lungs. Therefore, through this images, a difference can be seen between the extracted features using the CT scans and the ones extracted using the X-Ray images. Finally, in the Fig. <ref type="figure" target="#fig_2">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel multi-modality framework for COVID-19 pneumonia image diagnosis. A Dual fusion layer is introduced to help establish the dependencies between the different input modalities, as well as to take in different dimensionality inputs. The proposed Dual X-Ray attention layer makes it possible to effectively extract dependencies from the two X-Ray images focusing on the salient features between multi-modality images, whereas irrelevant features are ignored. The feature deletion layer helps to regularize the model dropping random features and improving the generalization of the model. Consequently, we provide the possibility to use one modality of CT or X-Ray for COVID-19 pneumonia diagnosis. Moreover, this model has the potential to be applied to other chest abnormalities in clinical practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overview of the proposed model. This model is composed by three image modalities, on the first modality we use a convolutional-based feature extractor, followed by a Transformer encoder and a random features dropout. The other two modalities consist of an early fusion with the first modality features followed by a transformer encoder. Finally, all of the features have a features dropout layer followed by a Convolutional layer and a fully connected layer.</figDesc><graphic coords="3,63,48,54,14,325,60,188,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. X-ray and CT images with their saliency visualization.</figDesc><graphic coords="8,51,36,274,82,319,00,111,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance of proposed model. (a) AUC Curve, (b) Confusion matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the AUC Curve obtained with the proposed model together with the confusion matrix, obtained as a result of classifying the test dataset with the model, are shown as Sub-figure a and b respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The performance of AUC-score, Precision and Recall of the different modalities is reported. The baseline does not include any additional model proposed by us.</figDesc><table><row><cell cols="4">CT X-Ray 1 X-Ray 2 AUC Recall Precision</cell></row><row><cell>X</cell><cell>X</cell><cell cols="2">70.15 66.66 64.28</cell></row><row><cell>X</cell><cell></cell><cell>67.98 65.9</cell><cell>70.58</cell></row><row><cell>X</cell><cell>X</cell><cell cols="2">65.74 61.11 75.0</cell></row><row><cell></cell><cell>X</cell><cell cols="2">71.26 74.28 69.76</cell></row><row><cell></cell><cell></cell><cell cols="2">74.49 66.66 62.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The result comparisons between different variations of the proposed model. AUC, Recall, and Precision metrics, respectively. Another model is tested, where the final convolutional block is changed into a ViT base block with a final MLP layer. The AUC score remains similar to the one found in the previously tested model, obtaining 71.38%, 76.80% and 59.84% in AUC, Recall and Precision respectively. Due to the multi-modality and missing modality nature of this model and this dataset, the comparison between this model and other existing models was not feasible.</figDesc><table><row><cell>Model</cell><cell cols="2">AUC Recall Precision</cell></row><row><cell cols="2">Convolutional Model 70.18 59.9</cell><cell>68.93</cell></row><row><cell>ViT Classificator</cell><cell cols="2">71.38 76.80 59.84</cell></row><row><cell>Final model</cell><cell cols="2">79.93 86.62 67.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study between the three different proposed methods by adding Dual Fusion block, Dual X-Ray attention block, and Feature matrix dropout in our method.</figDesc><table><row><cell cols="4">Dual Fusion Dual X-Ray attention Feature matrix dropout AUC Recall Precision</cell></row><row><cell>X</cell><cell>X</cell><cell>X</cell><cell>75.28 71.14 69.87</cell></row><row><cell></cell><cell>X</cell><cell>X</cell><cell>76.49 61.43 83.55</cell></row><row><cell></cell><cell></cell><cell>X</cell><cell>77.27 72.72 72.84</cell></row><row><cell></cell><cell></cell><cell></cell><cell>79.93 86.62 67.40</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">62102247</rs>) and <rs type="funder">Natural Science Foundation of Shanghai</rs> (No. <rs type="grantNumber">23ZR1430700</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eNRMFYg">
					<idno type="grant-number">62102247</idno>
				</org>
				<org type="funding" xml:id="_sAdbRN6">
					<idno type="grant-number">23ZR1430700</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.cdc.gov/coronavirus/2019-ncov/covid-data/covidview/index.html" />
		<title level="m">Percentage of Visits for COVID-19-Like Illness: Covid data page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The importance of understanding the stages of covid-19 in treatment and trials</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="DOI">10.24875/aidsrev.200001261</idno>
		<ptr target="https://doi.org/10.24875/aidsrev" />
	</analytic>
	<monogr>
		<title level="j">AIDS Rev</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="47" />
			<date type="published" when="2021">2021. 200001261</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Utility of chest CT in diagnosis of covid-19 pneumonia</title>
		<author>
			<persName><forename type="first">N</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.5152/dir.2020.20144</idno>
		<idno type="PMCID">PMC7490028</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7490028/" />
	</analytic>
	<monogr>
		<title level="j">Diagn. Interv. Radiol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="437" to="442" />
			<date type="published" when="2020">2020. 2020. 20144</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Review on covid-19 diagnosis models based on machine learning and deep learning approaches</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">A A</forename><surname>Alyasseri</surname></persName>
		</author>
		<idno type="DOI">10.1111/exsy.12759</idno>
		<ptr target="https://doi.org/10.1111/exsy.12759.https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12759" />
	</analytic>
	<monogr>
		<title level="j">Exp. Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12759</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diagnostic value and key features of computed tomography in coronavirus disease</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1080/22221751.2020.1750307</idno>
		<ptr target="https://doi.org/10.1080/22221751.2020.1750307" />
	</analytic>
	<monogr>
		<title level="j">Emerg. Microbes Infect</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">32241244</biblScope>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s disease diagnosis framework from incomplete multimodal data using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elazab</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2021.103863</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1532046421001921" />
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Informatics</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">103863</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Medical image segmentation on MRI images with missing modalities: a review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghanmanshadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Merhof</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2203.06217</idno>
		<ptr target="https://arxiv.org/abs/2203.06217" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SMIL: multimodal learning with severely missing modality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i3.16330</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/16330" />
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2302" to="2310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hybrid deep learning method for early and late mild cognitive impairment diagnosis with incomplete multimodal data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3389/fninf.2022.843566</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8965366/" />
	</analytic>
	<monogr>
		<title level="j">Frontiers Neuroinf</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Task-induced pyramid and attention Gan for multimodal brain image imputation and classification in Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2021.3097721</idno>
		<ptr target="https://doi.org/10.1109/JBHI.2021.3097721" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">mmFormer: multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2206.02425</idno>
		<ptr target="https://arxiv.org/abs/2206.02425" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Missing data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bland</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmj.38977.682025.2C</idno>
		<ptr target="https://doi.org/10.1136/bmj.38977.682025.2C.https://www.bmj.com/content/334/7590/424" />
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="page">424</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Early vs late fusion in multimodal convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gadzicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khamsehashari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zetzsche</surname></persName>
		</author>
		<idno type="DOI">10.23919/FUSION45008.2020.9190246</idno>
		<ptr target="https://doi.org/10.23919/FUSION45008.2020.9190246" />
	</analytic>
	<monogr>
		<title level="m">IEEE 23rd International Conference on Information Fusion (FUSION)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">EmbraceNet: a robust deep learning architecture for multimodal classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2019.02.010</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1566253517308242" />
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="259" to="270" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2010.11929</idno>
		<ptr target="https://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">BIMCV covid-19+: a large annotated dataset of RX and CT images from covid-19 patients with extension Part II</title>
		<author>
			<persName><surname>De La Iglesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vayá</surname></persName>
		</author>
		<idno type="DOI">10.21227/mpqg-j236</idno>
		<ptr target="https://doi.org/10.21227/mpqg-j236" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
