<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chaoyi</forename><surname>Li</surname></persName>
							<email>chaoyi.li@uq.net.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<settlement>St. Lucia</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Li</surname></persName>
							<email>meng.li@uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<settlement>St. Lucia</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CSIRO DATA 61, Robotics and Autonomous Systems Group</orgName>
								<address>
									<settlement>Pullenvale</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
							<email>lovell@eecs.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<postCode>4072</postCode>
									<settlement>St. Lucia</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8BC63557FC425F1D71E38A4ACF887610</idno>
					<idno type="DOI">10.1007/978-3-031-43904-972.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image classification</term>
					<term>Curriculum learning</term>
					<term>Uncertainty estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an innovative approach to curriculum learning, which is a technique used to train learning models. Curriculum learning is inspired by the way humans learn, starting with simple examples and gradually progressing to more challenging ones. There are currently two main types of curriculum learning: fixed curriculum generated by transfer learning, and self-paced learning based on loss functions. However, these methods have limitations that can hinder their effectiveness. To overcome these limitations, this article proposes a new approach called Dynamic Curriculum Learning via In-Domain Uncertainty (DCLU), which is derived from uncertainty estimation. The proposed approach utilizes a Dirichlet distribution classifier to obtain prediction and uncertainty estimates from the network, which can be used as a metric to quantify the difficulty level of the data. An uncertainty-aware sampling pacing function is also introduced to adapt the curriculum according to the difficulty metric. This new approach has been evaluated on two medical image datasets, and the results show that it outperforms other curriculum learning methods. The source code for this approach will be released at https://github.com/Joey2117/DCLU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Curriculum learning methods in deep learning are inspired by human education and involve structuring the training data from easy to hard to teach networks progressively. However, developing a good difficulty metric or measurer for curriculum learning is a challenge. Recently, there are two main categories of approaches to designing the difficulty metrics. The first is that human experts quantify data difficulty based on data characteristics such as complexity <ref type="bibr" target="#b18">[19]</ref>. These metrics not only require sufficient domain knowledge, but they also run the risk that metrics of difficulty from a human perspective may not be applicable to a learning model due to different decision boundaries between the model and the human <ref type="bibr" target="#b24">[25]</ref>.</p><p>Another popular approach is difficulty measurers based on the network, including transfer learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref> and loss function <ref type="bibr" target="#b13">[14]</ref>. Transfer learning is the scoring of samples using predictions of a reference model on the same training data. These measurers are fixed and do not take feedback from the progress of the current model into account during the training process. Difficulty measurers based on loss function tend to select samples with small training losses as a priority to train the model. However, this type of approach suffers from the uncertainty of quantifying the difficulty of data due to insufficient training in the early stages. In addition, while deep learning models have achieved impressive performance in the medical image analysis field, there remain challenges in measuring and developing a model with low in-domain uncertainty. Recently, uncertainty estimation has emerged as an effective tool for measuring the indomain uncertainty of models. However, reducing the in-domain uncertainty of models is an active research direction <ref type="bibr" target="#b5">[6]</ref>. Then, based on uncertainty estimates, we use UAS to sort all data from easy to hard and select easier samples to update the parameters of the network. This process ensures that the network is trained on easier samples at the beginning when it is less mature and gradually moves towards harder samples as the network improves.</p><p>We propose a new approach to address the challenges of curriculum learning, which we call Dynamic Curriculum Learning via In-Domain Uncertainty (DCLU). Our approach is motivated by two key observations: 1) sample difficulty is influenced by both the complexity of the data and the model's inability to explain data, related to in-domain uncertainty, and 2) reducing in-domain uncertainty by improving the learning process can boost model performance. To estimate in-domain uncertainty, we use a Dirichlet distribution classifier, which provides uncertainty estimates and predictions simultaneously. DCLU then sorts the training data from easy to hard based on in-domain uncertainty estimation, allowing the model to focus on easier samples first. Our approach does not require additional networks to capture uncertainty and is end-to-end.</p><p>In particular, Our dynamic difficulty measurer (DDM) generates uncertainties and predictions for each image simultaneously. Uncertainties reflect the difficulty and we use these as the criteria for data rearrangement. Uncertainty estimation runs at each iteration to ingest the feedback of the current network. We also propose an effective uncertainty-aware sampling pacing function (UAS) to sort all training data according to the latest results of DDM and gradually introduce progressively harder samples to learn new parameter vectors and update the network until the entire dataset is covered. The full process of the proposed method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We evaluate our method on two medical image datasets ISIC 2018 task 3 and Chest-Xray8 (COVID- <ref type="bibr" target="#b18">19)</ref>. Results indicate that our method outperforms other curriculum learning works. In addition, with our proposed approach, the uncertainty of the model can be mitigated effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In-Domain Uncertainty. In-domain uncertainty represents the uncertainty associated with inputs extracted from a data distribution equivalent to the training data distribution <ref type="bibr" target="#b5">[6]</ref>. Deep learning models can experience in-domain uncertainty as they lack the necessary in-domain knowledge to interpret indomain samples. In-domain uncertainty is caused by two types of uncertainty: data uncertainty and model uncertainty. Data uncertainty represents the complexity of data, which is related to noise and variations in observations <ref type="bibr" target="#b5">[6]</ref>. On the other hand, model uncertainty arises due to shortcomings of the model such as a poor fit to the training dataset or lack of knowledge <ref type="bibr" target="#b5">[6]</ref>.</p><p>In-Domain Uncertainty Estimate. MC dropout <ref type="bibr" target="#b3">[4]</ref> and deep ensemble <ref type="bibr" target="#b14">[15]</ref> have emerged as two widely adopted techniques for estimating uncertainty in recent years. These methods need significant computing sources and extra metrics to quantify in-domain uncertainty. As our proposal exploits in-domain uncertainties from the current network at every iteration, we want to avoid using additional modules and metrics to obtain similar uncertainty estimates from the above methods and thereby reduce computational resource requirements. Therefore, we apply a classifier with Dirichlet distribution to gain direct both the predictions and uncertainty estimation simultaneously.</p><p>Curriculum Learning. Bengio et al. <ref type="bibr" target="#b1">[2]</ref> first brought curriculum learning into the field of machine learning, which has prompted quite a bit of interest in the field of computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Hacohen et al. <ref type="bibr" target="#b6">[7]</ref> used the confidence score obtained from transfer learning or bootstrapping to determine a fixed curriculum. Self-paced learning(SPL) was proposed by Kumar et al. <ref type="bibr" target="#b13">[14]</ref>, which applied example-wise training loss at each iteration as the difficulty measurer. The drawback of SPL is that some easier data appear all the time since SPL tended to select data with lower current losses. Jiang et al. <ref type="bibr" target="#b10">[11]</ref> indicated a novel curriculum learning method named self-paced curriculum learning (SPCL) that combined predefined curriculum learning and self-paced learning so that prior knowledge before training and information during training are used effectively. In addition, Kong et al. <ref type="bibr" target="#b11">[12]</ref> presented a linear combination of the current model loss and prior knowledge to adapt the difficulty measurer of the current model. In our work, we want to build a dynamic difficulty measurer through the uncertainty estimates generated by each iteration of the network. This means our approach does not require the prior knowledge provided by the pre-trained model and is able to update the curricula during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>In DCLU, we randomly input data into the dynamic difficulty measurer (DDM) at the first epoch to obtain the uncertainty for each data point. Next, we apply the uncertainty-aware sampling pacing function (UAS) to present the training data to the DDM in order of the ascending uncertainties to synchronously produce predictions and new uncertainty estimates. New uncertainties can be used as a criterion for sorting data in the next iteration. Additionally, our pacing function selects a fraction of easier samples and learns a parameter vector to update the network. With the training process progressing, the proportion of selected samples increases until it eventually comprises the entire dataset. The pseudo-code for our method is given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Difficulty Measurer</head><p>The difficulty measurer is used to measure the difficulty of the data to decide on the order of the training data, which is a crucial component of curriculum learning. Our dynamic difficulty measurer (DDM) is a multi-class classifier with Dirichlet distribution, based on evidential deep learning <ref type="bibr" target="#b21">[22]</ref>. The setting of our work is focusing on the K-classes classification task. In detail, first of all, we assume e k ≥ 0 to be evidence of k-th output of the activation function (e.g. softplus <ref type="bibr" target="#b16">[17]</ref>) of our DDM. Then, DDM assigns a belief mass b k for each category and an overall uncertainty mass u, which can be defined as (1):</p><formula xml:id="formula_0">u + K k=1 b k = 1, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where K is the total number of classes. We allocate b k in correspondence to Dirichlet distribution with parameters α k = e k + 1. So, the belief mass and uncertainty can be formulated as</p><formula xml:id="formula_2">b k = α k -1 S and u = K S , where S = K k=1 (α k ).</formula><p>Additionally, the Dirichlet distribution of DDM can be defined with parameters α = [α 1 , ..., α K ] as (2):</p><formula xml:id="formula_3">D(p k |α k ) = 1 B(α) K k=1 p α k -1 k , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where K k=1 p k = 1 and 0 ≤ p 1 , ..., p K ≤ 1. B(α) is a K-dimensional multinomial beta function <ref type="bibr" target="#b12">[13]</ref>. It represents the density of each probability distribution, which is based on parameters derived from the evidence vector <ref type="bibr" target="#b9">[10]</ref>. The expected probability of the k-th output of the classifier can be formulated as (3):</p><formula xml:id="formula_5">pk = α k S .<label>(3)</label></formula><p>Thus, DDM can generate both predictions and in-domain uncertainty estimates for each sample simultaneously. To be specific, in-domain uncertainty estimates include data and model uncertainty. Data uncertainty cannot be eliminated with training and model uncertainty can be reduced by improving the learning process <ref type="bibr" target="#b5">[6]</ref>. Inspired by this phenomenon, we employ in-domain uncertainty to measure the difficulty of data at each iteration. This not only allows the data uncertainty as a prior for the criterion of difficulty measure but also allows the measure of data difficulty to be updated efficiently based on model uncertainty estimated from the current state of the model. The method can achieve a dynamic curriculum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Uncertainty-Aware Sampling Pacing Function</head><p>The pacing function is based on using the difficulty measurer to determine how training examples (data) are fed into the network during the training process. Our dynamic difficulty measurer (DDM) can provide difficulty scores for all data at each iteration. However, some existing pacing functions attempt to partition the dataset into multiple subsets and gradually feed them into the network during training -this fails to satisfy the requirement of our difficulty measurer. To address this problem, we propose the uncertainty-aware sampling pacing function (UAS) consisting of two modules: the reorder and sampling modules. Within the reorder module, UAS sorts all training data from easy to hard according to ascending uncertainties from DDM at the last epoch and sends them into the network to yield predictions and new uncertainty estimates. The sampling module specifies a fraction of easier samples to update to the network. The weight assigned to the selected examples is set to 1, while the weight for other data is set to 0. These weights α will be applied to the objective function, which allows the parameters learned from the specified examples to update the network. We increase the fraction exponentially in each epoch until it eventually comprises the entire dataset.</p><p>In our work, we implement UAS through two approaches. Firstly, UAS (exponential) incorporates both the reorder and sampling modules, prompting the network to prioritize learning easier examples during the initial stages of training and then gradually learn more difficult examples. In each epoch, UAS (exponential) first utilizes the reorder module to sort all training data from easy to hard. Then, it applies the sampling module to select a fraction of easier examples to update the network. Additionally, UAS (full) only contains the reorder module to enable the network to learn all sorted data, ranging from easy to hard examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>Assume D(p k |α k ) is the prior on the cross-entropy loss. The classification loss function for each sample can be formulated by the Bayes risk by (4):</p><formula xml:id="formula_6">L cls i (Θ) = K k=1 -y ik log (p ik ) D(p ik |α ik )dp i = K k=1 y ik (ψ (S i ) -ψ (α ik )) , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where ψ is the digamma function, p ik is the estimated probability of the k-th class of the i-th sample. Additionally, Kullback-Leibler (KL) divergence is used to reduce the total evidence to zero under the condition of incorrect classification, which can be denoted by ( <ref type="formula">5</ref>):</p><formula xml:id="formula_8">L KL i = log ⎛ ⎝ Γ K k=1 αik Γ (K) K k=1 Γ (α ik ) ⎞ ⎠ + K k=1 (α ik -1) ψ (α ik ) -ψ K k=1 αik ,</formula><p>(5) where Γ denotes the gamma function. To achieve pace control and reduce the effects of overfitting due to easier data that may always appear at the beginning of training, the final objective function is:</p><formula xml:id="formula_9">L DCLU = N i=1 α i * (L cls i + λ t L KL i ) + 1 2 ||w| | 2 2 , (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where α i is the weight from the UAS pacing function to control the pace, λ t = min(1, t/50) is the annealing parameter, t is the current training epoch and w is weights of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Experimental Setup</head><p>Dataset. We evaluated our method on two public medical image datasets including ISIC 2018 Task 3 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>  Evaluation Metrics. For both datasets, the performance of diagnosis is evaluated with both accuracy and F1 score. Moreover, for the assessment of uncertainty estimation, we apply expected calibration error (ECE) as the metric.</p><p>Implementation Details. We employ ResNet-18 <ref type="bibr" target="#b8">[9]</ref> as the backbone for both tasks. In our experiments, we used the TensorFlow framework and trained on an NVIDIA 3090 GPU with 32G of RAM. An Adam optimizer with β = 0 and α = 0.99 and a linear learning rate scheduler are used to tune the network. The initial learning rate is 0.0001 and the learning rate is decreased to 0.00001 after 20 epochs with batch size 16. The total epoch is 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Comparison with State-of-the-Art. Our method is compared with various curriculum learning methods: 1) Vanilla samples batches randomly on the entire dataset without any curriculum learning techniques; 2) Fixed curriculum learning (FCL) <ref type="bibr" target="#b6">[7]</ref> sorts data with the confidence score derived from transfer learning; 3) Self-pace learning (SPL) <ref type="bibr" target="#b13">[14]</ref> chooses the data with minimum losses to train the network in advance; 4) Self-pace curriculum learning (SPCL) <ref type="bibr" target="#b10">[11]</ref> is the combination of self-paced learning and predefined curriculum learning; 5. Adaptive Curriculum learning (Adaptive CL) <ref type="bibr" target="#b11">[12]</ref> adjusts the difficulty measurer by incorporating the prior knowledge and feedback from the current model. The results in Table <ref type="table" target="#tab_1">1</ref> show that our method (DDM) with UAS (full) outperforms both datasets. Our method with UAS (exponential) performs better than other methods on Chest-Xray 8 (COVID-19) and has the second best performance on ISIC 2018 Task 3. The performance gap between our two methods may be Evaluation In-Domain Uncertainty. To verify that our method is effective in reducing in-domain uncertainty, our method is compared with vanilla. Figure <ref type="figure">2</ref> shows that our method is more robust than vanilla. Details of the comparison with other curriculum learning methods are referred to in the Appendix. our method performs 8.93% better than FCL using the same exponential pacing functions. This indicates that DDM performs quite well compared to the fixed curriculum obtained by transfer learning. Moreover, In order to demonstrate that the order generated by DDM is robust, we have conducted experiments on different backbones, details of which can be found in the Appendix. Next, we compare our proposed pacing function to three pacing functions, which are fixed exponential, single-step and linear pacing functions <ref type="bibr" target="#b6">[7]</ref>. Both single-step and linear pacing functions divide the entire data set into subsets and put them into the model progressively. The difference between the two is in the number of subsets. Single-step pacing function divides into two subsets and the linear pacing function splits into several subsets (e.g. 3 subsets). The results in Table <ref type="table" target="#tab_2">2</ref> show that UAS is more suited to DDM than the other pacing functions. When the other three pacing functions are used, new samples added to the current subset will disrupt the existing order of data when the size of the subset is increased. It causes the network to have to relearn and reorder the new subset. Finally, We compare the effect of using the loss function with and without L2 regular term on the model performance, finding that using the loss function with L2 can eliminate the effect of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our approach, called Dynamic Curriculum Learning via In-Domain Uncertainty (DCLU), introduces a new perspective on curriculum design by utilizing the current stage of the network to estimate the difficulty of data based on its in-domain uncertainty. To support this, we also introduce an uncertainty-aware sampling pacing function that is compatible with our dynamic difficulty measurer. Our experimental results confirm that DCLU is successful in reducing uncertainty, and we demonstrate the effectiveness of our approach on two medical image datasets through extensive experimentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The pipeline of our DCLU with the dynamic difficulty measurer (DDM) and uncertainty-aware sampling pacing function (UAS). First, DDM provides both uncertainty estimates and predictions for all training data simultaneously in every iteration.Then, based on uncertainty estimates, we use UAS to sort all data from easy to hard and select easier samples to update the parameters of the network. This process ensures that the network is trained on easier samples at the beginning when it is less mature and gradually moves towards harder samples as the network improves.</figDesc><graphic coords="2,42,30,304,07,339,43,140,62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Chest-Xray8 (COVID-19)<ref type="bibr" target="#b17">[18]</ref>. ISIC 2018 Task 3 dataset has 10,015 training images and 194 images for validation. It aims to classify 7 skin cancer types including Melanoma, Melanocytic nevus, Basal cell carcinoma, Actinic keratosis, Benign keratosis Dermatofibroma and Vascular lesion. Chest-Xray 8 (COVID-19) contains 1125 X-ray images of the chest of the individuals studied, including 125 images labeled COVID-19 taken from<ref type="bibr" target="#b2">[3]</ref>, 500 images labeled pneumonia and 500 images labeled no findings were randomly taken from the ChestX-ray8<ref type="bibr" target="#b23">[24]</ref>. We employ training and validation data from ISIC 2018 Task 3 dataset as our training and test sets. Chest-Xray 8 (COVID-19) dataset is randomly divided into two parts, with 80% of the images being the training set and 20% of the images being the test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art curriculum learning methods on ISIC 2018 Task 3 and Chest-Xray 8 (COVID-19)due to the fact that our objective function is constructed based on the Dirichlet distribution and the presence of class imbalance within both datasets. When the samples are selected by UAS (exponential) at the early training stage, samples from smaller numbered classes may not be chosen which leads to the optimization may tend to fall into local extrema. Our approach clearly outperforms FCL, demonstrating the necessity to design a dynamic curriculum. Compared with loss function-based curriculum learning (SPL, SPCL and Adaptive CL), our methods obtain better performance, showing the importance of using uncertainty estimates as the criterion for difficulty measurement of data. Furthermore, our method is more effective than SPL at an early training stage. Details are referred to in the Appendix.</figDesc><table><row><cell>Method</cell><cell cols="2">ISIC 2018 Task 3</cell><cell cols="2">Chest-Xray8 (COVID-19)</cell></row><row><cell></cell><cell cols="4">Accuracy F1 Score Accuracy F1 Score</cell></row><row><cell>Vanilla</cell><cell>81.73</cell><cell>38.8</cell><cell>71.88</cell><cell>57.51</cell></row><row><cell>FCL [7]</cell><cell>82.85</cell><cell>39.87</cell><cell>72.32</cell><cell>58.86</cell></row><row><cell>SPL [14]</cell><cell>82.69</cell><cell>39.49</cell><cell>74.55</cell><cell>68.77</cell></row><row><cell>SPCL [11]</cell><cell>82.21</cell><cell>39.19</cell><cell>73.21</cell><cell>63.08</cell></row><row><cell>Adaptive CL [12]</cell><cell>83.17</cell><cell>37.64</cell><cell>72.77</cell><cell>64.13</cell></row><row><cell cols="2">Ours (Exponential) 82.93</cell><cell>39.26</cell><cell>81.7</cell><cell>73.69</cell></row><row><cell>Ours (Full)</cell><cell>85.1</cell><cell>40.58</cell><cell>83.04</cell><cell>82.03</cell></row></table><note><p>Fig. 2. Compare our method and vanilla under ECE metric on Chest-Xray 8 (COVID-19)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation analysis of our method on Chest-Xray 8 (COVID-19) dataset.</figDesc><table><row><cell>Method</cell><cell>DDM Exp Single-step Linear UAS Accuracy</cell></row><row><cell>Vanilla</cell><cell>71.88</cell></row><row><cell>FCL</cell><cell>72.32</cell></row><row><cell>Ours</cell><cell>81.25</cell></row><row><cell>Ours</cell><cell>80.36</cell></row><row><cell>Ours</cell><cell>77.23</cell></row><row><cell>Ours with full UAS(w/o L2)</cell><cell>82.14</cell></row><row><cell>Ours with full UAS(w L2)</cell><cell>83.04</cell></row></table><note><p><p><p>Ablation Study. We present the results of ablation studies on Chest-Xray 8 (COVID-19) in Table</p>2</p>to show the effectiveness of key components, i.e., DDM, UAS pacing function and the loss function with L2 regular term. First of all,</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Image similarity using deep CNN and curriculum learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Appalaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaoji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08761</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11597</idno>
		<title level="m">Covid-19 image data collection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking curriculum learning with incremental labels and adaptive compensation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04529</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gawlikowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03342</idno>
		<title level="m">A survey of uncertainty in deep neural networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the power of curriculum learning in training deep networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2535" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Skin lesion segmentation and classification for ISIC 2018 using traditional classifiers with hand-crafted features</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kebede</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07001</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deal: Deep evidential active learning for image classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schöffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Learn. Appl</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="171" to="192" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5067" to="5076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous multivariate distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Models and applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2004">2004</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Curriculum dropout</title>
		<author>
			<persName><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3544" to="3552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated detection of covid-19 cases using deep neural networks with x-ray images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ozturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Talo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">B</forename><surname>Baloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">103792</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Competencebased curriculum learning for neural machine translation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stretcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09848</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Curriculum learning for heterogeneous star network embedding via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="468" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-induced curriculum learning in neural machine translation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ruiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>España-Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Genabith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on curriculum learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Curriculum learning by transfer learning: theory and experiments with deep networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amir</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5238" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
