<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning</title>
				<funder ref="#_S6gJCjC">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">Stony Brook University Provost Funds</orgName>
				</funder>
				<funder ref="#_vxhZZXx #_GQ9zzNH">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_PKhxpvB">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingwei</forename><surname>Zhang</surname></persName>
							<email>jingwezhang@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saarthak</forename><surname>Kapse</surname></persName>
							<email>saarthak.kapse@stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Ma</surname></persName>
							<email>kemma@cs.stonybrook.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Snap Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prateek</forename><surname>Prasanna</surname></persName>
							<email>prateek.prasanna@stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joel</forename><surname>Saltz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maria</forename><surname>Vakalopoulou</surname></persName>
							<email>maria.vakalopoulou@centralesupelec.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CentraleSup√©lec</orgName>
								<orgName type="institution" key="instit2">University of Paris-Saclay</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
							<email>samaras@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="624" to="634"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F9AF76B4E495A9484CEAE789EC4D29EF</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_60</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Whole slide image classification</term>
					<term>Multiple instance learning</term>
					<term>Prompt tuning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Whole slide image (WSI) classification is a critical task in computational pathology, requiring the processing of gigapixel-sized images, which is challenging for current deep-learning methods. Current state of the art methods are based on multi-instance learning schemes (MIL), which usually rely on pretrained features to represent the instances. Due to the lack of task-specific annotated data, these features are either obtained from well-established backbones on natural images, or, more recently from self-supervised models pretrained on histopathology. However, both approaches yield task-agnostic features, resulting in performance loss compared to the appropriate task-related supervision, if available. In this paper, we show that when task-specific annotations are limited, we can inject such supervision into downstream task training, to reduce the gap between fully task-tuned and task agnostic features. We propose Prompt-MIL, an MIL framework that integrates prompts into WSI classification. Prompt-MIL adopts a prompt tuning mechanism, where only a small fraction of parameters calibrates the pretrained features to encode task-specific information, rather than the conventional full fine-tuning approaches. Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC, and BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL methods, achieving a relative improvement of 1.49%-4.03% in accuracy and 0.25%-8.97% in AUROC while using fewer than 0.3% additional parameters. Compared to conventional full fine-tuning approaches, we fine-tune less than 1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in accuracy and 3.22%-27.18% in AUROC and reduce GPU memory consumption by 38%-45% while training 21%-27% faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Whole slide image (WSI) classification is a critical task in computational pathology enabling disease diagnosis and subtyping using automatic tools. Owing to the paucity of patch-level annotations, multiple instance learning (MIL) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref> techniques have become a staple in WSI classification. Under an MIL scheme, WSIs are divided into tissue patches or instances, and a feature extractor is used to generate features for each instance. These features are then aggregated using different pooling or attention-based operators to provide a WSI-level prediction. ImageNet pretrained networks have been widely used as MIL feature extractors. More recently, self-supervised learning (SSL), using a large amount of unlabeled histopathology data, has become quite popular for WSI classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> as it outperforms ImageNet feature encoders.</p><p>Most existing MIL methods do not fine-tune their feature extractor together with their classification task; this stems from the requirement for far larger GPU memory than is available currently due to the gigapixel nature of WSIs, e.g. training a WSI at 10x magnification may require more than 300 Gb of GPU memory. Recently, researchers have started to explore optimization methods to enable end-to-end training of the entire network and entire WSI within GPU memory <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. These methods show better performance compared to conventional MIL; they suffer, however, from two limitations. First, they are ImageNet-pretrained and do not leverage the powerful learning capabilities of histology-trained SSL models. Second, these are mostly limited to convolutional architectures rather than more effective attention-based architectures such as vision transformers <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation:</head><p>To improve WSI-level analysis, we explore end-to-end training of the entire network using SSL pretrained ViTs. To achieve this, we use the patch batching and gradient retaining techniques in <ref type="bibr" target="#b24">[25]</ref>. However, we find that conventional fine-tuning approaches, where the entire network is fine-tuned, achieve low performance. For example, on the BRIGHT dataset <ref type="bibr" target="#b1">[2]</ref>, the accuracy drops more than 5% compared to the conventional MIL approaches. The poor performance is probably caused by the large network over-fitted to the limited downstream training data, leading to suboptimal feature representation. Indeed, especially for weakly supervised WSI classification, where annotated data for downstream tasks is significantly less compared to natural image datasets, conventional finetuning schemes can prove to be quite challenging.</p><p>To address the subpar performance of SSL-pretrained vision transformers, we utilize the prompt tuning techniques. Initially proposed in natural language processing, a prompt is a trainable or a pre-defined natural language statement that is provided as additional input to a transformer to guide the neural network towards learning a specific task or objective <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>. Using prompt tuning we fine-tune only the prompt and downstream network without re-training the large backbone (e.g. GPT-3 with 17B parameters). This approach is parameter efficient <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> and has been shown to better inject task-specific information and reduce the overfitting in downstream tasks, particularly in limited data scenarios <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>. Recently, prompts have also been adopted in computer vision and demonstrated superior performance compared to conventional fine-tuning methods <ref type="bibr" target="#b9">[10]</ref>. Prompt tuning performs well even when only limited labeled data is available for training, making it particularly attractive in computational pathology. The process of prompt tuning thus involves providing a form of limited guidance during the training of downstream tasks, with the goal of minimizing the discrepancy between feature representations that are fully tuned to the task and those that are not task-specific.</p><p>In this paper, we propose a novel framework, Prompt-MIL, which uses prompts for WSI-level classification tasks within an MIL paradigm. Our contributions are:</p><p>-Fine-tuning: Unlike existing works in histopathology image analysis, Prompt-MIL is fine-tuned using prompts rather than conventional full finetuning methods. -Task-specific representation learning: Our framework employs an SSL pretrained ViT feature extractor with a trainable prompt that calibrates the representations making them task-specific. By doing so, only the prompt parameters together with the classifier, are optimized. This avoids potential overfitting while still injecting task-specific knowledge into the learned representations.</p><p>Extensive experiments on three public WSI datasets, TCGA-BRCA, TCGA-CRC, and BRIGHT demonstrate the superiority of Prompt-MIL over conventional MIL methods, achieving a relative improvement of 1.49%-4.03% in accuracy and 0.25%-8.97% in AUROC by using only less than 0.3% additional parameters. Compared to the conventional full fine-tuning approach, we finetune less than 1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in accuracy and 3.22%-27.18% in AUROC. Moreover, compared to the full fine-tuning approach, our method reduces GPU memory consumption by 38%-45% and trains 21%-27% faster. To the best of our knowledge, this is the first work where prompts are explored for WSI classification. While our method is quite simple, it is versatile as it is agnostic to the MIL scheme and can be easily applied to different MIL methods. Our code is available at https://github.com/cvlab-stonybrook/PromptMIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our Prompt-MIL framework consists of three components: a frozen feature model to extract features of tissue patches, a classifier that performs an MIL scheme of feature aggregation and classification of the WSIs, and a trainable prompt. Given a WSI and its label y, the image is tiled into n tissue patches/instances {x 1 , x 2 , . . . , x n } at a predefined magnification. As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, the feature model F (‚Ä¢) computes n feature representations from the corresponding n patches:</p><formula xml:id="formula_0">h = [h 1 , h 2 , . . . , h n ] = [F (x 1 , P), F (x 2 , P), . . . , F (x n , P)],<label>(1)</label></formula><p>where h i denotes the feature of the i th patch, h is the concatenation of all h i , and P = {p i , i = 1, 2, . . . , k} is the trainable prompt consisting of k trainable tokens.</p><p>The classifier G(‚Ä¢) applies an MIL scheme to predict the label ≈∑ and calculate the loss L as:</p><formula xml:id="formula_1">L = L cls (≈∑, y) = L cls (G(h), y), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where the L cls is a classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Prompt Tuning</head><p>The visual prompt tuning is the key component of our framework. As shown in Fig.  where t 0 i is the embedding token of z i and T 0 z is the collection of such tokens. These tokens T 0 z are concatenated with a class token t 0 cls and a prompt P: The class token is used to aggregate information from all other tokens. The prompt consists of k trainable tokens P = {p i |i = 1, 2, . . . , k}. The concatenation is fed into l layers of the Transformer encoders:</p><formula xml:id="formula_3">T 0 z = L 0 ([z 1 , z 2 , . . . , z w ]) = {t 0 1 , t 0 2 , . . . , t 0 w },<label>(3)</label></formula><formula xml:id="formula_4">T 1 z , T 1 P , t 1 cls = L 1 ([T 0 z , P, t 0 cls ])<label>(4)</label></formula><formula xml:id="formula_5">T i z , T i P , t i cls = L i ([T i-1 z , T i-1 P , t i-1 cls ]), i = 2, 3, . . . , l<label>(5)</label></formula><formula xml:id="formula_6">T i P = {p i j |j = 1, 2, . . . , k},<label>(6)</label></formula><p>where p i j is the j th output prompt token of the i th Transformer encoder and T i P is the collection of all k such output prompt tokens, which are not trainable. The output feature of x i is defined as the last class token:</p><formula xml:id="formula_7">h i = t l cls .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Optimization</head><p>Our overall loss function is defined as</p><formula xml:id="formula_8">L = L cls (G(H), y) = L cls (G([F (x 1 , P), F (x 2 , P), . . . , F (x n , P)]), y),<label>(7)</label></formula><p>where only the parameters of the G(‚Ä¢) and the prompt P are optimized, while the feature extractor model F (‚Ä¢) is frozen.</p><p>Training the entire pipeline in an end-to-end fashion on gigapixel images is infeasible using the current hardware. To address this issue, we utilize the patch batching and gradient retaining techniques from <ref type="bibr" target="#b24">[25]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">1(a)</ref>, to reduce the GPU memory consumption, the n tissue patches {x 1 , x 2 , . . . , x n } are grouped into m batches. The first step (step‚ë† in the figure) of our optimization is to sequentially feed m batches of tissue patches forward to the feature model to compute its respective features which are subsequently concatenated into the h matrix. In this step, we just conduct a forward pass like the inference stage, without storing the memory-intensive computational graph for back-propagation.</p><p>In the second step (step‚ë°), we feed h into the classifier G(‚Ä¢) to calculate the loss L and update the parameters of G(‚Ä¢) by back-propagate the loss. The back-propagated gradients g = ‚àÇL/‚àÇh on h are retained for the next step.</p><p>Finally (step‚ë¢), we feed the input batches into the feature model F (‚Ä¢) again and use the output h and the retained gradients g from the last step to update the trainable prompt tokens. In particular, the gradients on the j th prompt token p j are calculated as:</p><formula xml:id="formula_9">‚àÇL ‚àÇp j = ‚àÇL ‚àÇh ‚àÇh ‚àÇp j = i ‚àÇL ‚àÇh i ‚àÇh i ‚àÇp j = i g i ‚àÇh i ‚àÇp j , (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where g i is the gradient calculated with respect to h i .</p><p>To sum up, in each step, we only update either F or G given the current batch, which avoid storing the gradients of the whole framework for all the input patches. This patch batching and gradient retaining techniques make the end-to-end training feasible.</p><p>In this study, we use DSMIL <ref type="bibr" target="#b12">[13]</ref> as the classifier and binary cross entropy as the classification loss L cls when the task is a tumor sub-type classification or cross entropy otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We assessed Prompt-MIL using three histopathological WSI datasets: TCGA-BRCA <ref type="bibr" target="#b13">[14]</ref>, TCGA-CRC <ref type="bibr" target="#b18">[19]</ref>, and BRIGHT <ref type="bibr" target="#b1">[2]</ref>. These datasets were utilized for both the self-supervised feature extractor pretraining and the end-to-end finetuning (with or without prompts), including the MIL component. Note that the testing data were not used in the SSL pretraining. TCGA-BRCA contains 1034 diagnostic digital slides of two breast cancer subtypes: invasive ductal carcinoma (IDC) and invasive lobular carcinoma (ILC). We used the same training, validation, and test split as that in the first fold cross validation in <ref type="bibr" target="#b4">[5]</ref>. The cropped patches (790K training, 90K test) were extracted at 5√ó magnification. TCGA-CRC contains 430 diagnostic digital slides of colorectal cancer for a binary classification task: chromosomal instability (CIN) or genome stable (GS). Following the common 4-fold data split <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, we used the first three folds for training (236 GS, 89 CIN), and the fourth for testing (77 GS, 28 CIN). We further split 20% (65 slides) training data as a validation set. The cropped patches (1.07M training, 370K test) were extracted at 10√ó magnification. BRIGHT contains 503 diagnostic slides of breast tissues. We used the official training (423 WSIs) and test (80 WSIs) splits. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We cropped non-overlapping 224 √ó 224 sized patches in all our experiments and used ViT-Tiny (ViT-T/16) <ref type="bibr" target="#b6">[7]</ref> for feature extraction. For SSL pretraining, we leveraged the DINO framework <ref type="bibr" target="#b3">[4]</ref> with the default hyperparameters, but adjusted the batch size to 256 and employed the global average pooling for token aggregation. We pretrained separate ViT models on the TCGA-CRC datasets for 50 epochs, on the BRIGHT dataset for 50 epochs, and on the BRCA dataset for 30 epochs. For TCGA-BRCA, we used the AdamW <ref type="bibr" target="#b16">[17]</ref> optimizer with a learning rate of 1e -4, 1e -2 weight decay, and trained for 40 epochs. For TCGA-CRC, we also used the AdamW optimizer with a learning rate of 5e -4 and trained for 40 epochs. For Bright, we used the Adam <ref type="bibr" target="#b10">[11]</ref> optimizer with a learning rate of 1e -4, 5e -2 weight decay and trained for 40 epochs. We applied a cosine annealing learning rate decay policy in all our experiments. For the MIL baselines, we employed the same hyperparameters as above. For all full fine-tuning experiments, we used the learning rate in the corresponding prompt experiment as the base learning rate. For parameters in the feature model F (‚Ä¢), which are SSL pretrained, we use 1/10 of the base learning rate. For parameters in the Classifier G(‚Ä¢), which are randomly initialized, we use the base learning rate. We train the full tuning model for 10 more epochs than our prompt training to allow full convergence. This training strategy is optimized using the validation datasets. All model implementations were in PyTorch <ref type="bibr" target="#b19">[20]</ref> on a NVIDIA Tesla V100 or a Nvidia Quadro RTX 8000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We chose overall accuracy and Area Under Receiver Operating Characteristic curve (AUROC) as the evaluation metrics.</p><p>Evaluation of Prompt Tuning Performance: We compared the proposed Prompt-MIL with two baselines: 1) a conventional MIL model with a frozen feature extractor <ref type="bibr" target="#b12">[13]</ref>, 2) fine-tuning all parameters in the feature model (full fine-tuning). Table <ref type="table" target="#tab_0">1</ref> highlights that our Prompt-MIL consistently outperformed both. Compared to the conventional MIL method, Prompt-MIL added negligible parameters (192, less than 0.3% of the total parameters), achieving a relative improvement of 1.49% in accuracy and 0.25% in AUROC on TCGA-BRCA, 3.36% in accuracy and 8.97% in AUROC on TCGA-CRC, and 4.03% in accuracy and 0.43% in AUROC on BRIGHT. The observed improvement can be attributed to a more optimal alignment between the feature representation learned during the SSL pretraining and the downstream task, i.e., the prompt explicitly calibrated the features toward the downstream task.</p><p>The computationally intensive full fine-tuning method under-performed conventional MIL and Prompt-MIL. Compared to the full fine-tuning method, our method achieved a relative improvement of 1.29% to 13.61% in accuracy and 3.22% to 27.18% in AUROC on the three datasets. Due to the relatively small amount of slide-level labels (few hundred to a few thousands) fully fine tuning 5M parameters in the feature model might suffer from overfitting. In contrast, our method contained less than 1.3% of parameters compared to full fine-tuning, leading to robust training.  Evaluation of Time and GPU Memory Efficiency: Prompt-MIL is an efficient method requiring less GPU memory to train and running much faster than full fine-tuning methods. We evaluated the training speed and memory consumption of our method and compared to the full fine-tuning baseline on four different sized WSIs in the BRIGHT dataset. As shown in Table <ref type="table" target="#tab_1">2</ref>, our method consumed around 38% to 45% less GPU memory compared to full finetuning and was 21% to 27% faster. As we scaled up the WSI size (i.e. WSIs with more number of patches), the memory cost difference between Prompt-MIL and full fine-tuning further widened.</p><p>Evaluation on the Pathological Foundation Models: We demonstrated our Prompt-MIL also had a better performance when used with the pathological foundation model. Foundational models refer to those trained on large-scale pathology datasets (e.g. the entire TCGA Pan-cancer dataset <ref type="bibr" target="#b27">[28]</ref>). We utilized the publicly available <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> ViT-Small network pretrained using MoCo v3 <ref type="bibr" target="#b5">[6]</ref> on all the slides from TCGA <ref type="bibr" target="#b27">[28]</ref> and PAIP <ref type="bibr" target="#b21">[22]</ref>. In Table <ref type="table" target="#tab_2">3</ref>, we showed that our method robustly boosted the performance on both TCGA (the same domain as the foundation model trained on) and BRIGHT (a different domain). The improvement is more prominent in BRIGHT, which further confirmed that Prompt-MIL aligns the feature extractor to be more task-specific. Ablation Study: An ablation was performed to study the effect of the number of trainable prompt tokens on downstream tasks. Table <ref type="table" target="#tab_3">4</ref> shows the accuracy and AUROC of our Prompt-MIL model with 1, 2 and 3 trainable prompt tokens (k = 1, 2, 3) on the TCGA-BRCA and the BRIGHT datasets. On the TCGA-BRCA dataset, our Prompt-MIL model with 1 to 3 prompt tokens reported similar performance. On the BRIGHT dataset, the performance of our model dropped with the increased number of prompt tokens. Empirically, this ablation study shows that for classification tasks, one prompt token is sufficient to boost the performance of conventional MIL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we introduced a new framework, Prompt-MIL, which combines the use of Multiple Instance Learning (MIL) with prompts to improve the performance of WSI classification. Prompt-MIL adopts a prompt tuning mechanism rather than a conventional full fine-tuning of the entire feature representation.</p><p>In such a scheme, only a small fraction of parameters calibrates the pretrained representations to encode task-specific information, so the entire training can be performed in an end-to-end manner. We applied our proposed method to three publicly available datasets. Extensive experiments demonstrated the superiority of Prompt-MIL over the conventional MIL as well as the conventional fully finetuning methods. Moreover, by fine-tuning much fewer parameters compared to fully fine-tuning, our method is GPU memory efficient and fast. Our proposed approach also showed promising potentials in transferring foundation models. We will further explore the task-specific features that are captured by our prompt toward explainability of these models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1(b), our feature model F (‚Ä¢) is a ViT based architecture. It consists of a patch embedding layer L 0 and l sequential encoding layers {L 1 , L 2 , . . . , L l }. The ViT first divides an input image x i into w smaller patches [z 1 , z 2 , . . . , z w ] and embeds them into w tokens:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed method. (a) Overall structure of our training pipeline. Tissue patches tiled from the input WSI are grouped into separate batches, which are fed into a frozen feature model F (‚Ä¢) to compute their respective features. The features are subsequently concatenated into the feature h and a classifier G(‚Ä¢) applies an MIL scheme on h to predict the label and calculate the loss L. (b) Structure of the feature model F (‚Ä¢) with the additional prompt. An input image xi is cropped into w small patches z1, . . . , zw. k trainable prompt tokens, together with the embedding of small patches and a class token t 0 cls , are fed into l layers of Transformer encoders. The output feature corresponding to xi is the last class token t l cls . The feature model F (‚Ä¢) is frozen and only the prompt is trainable.</figDesc><graphic coords="4,58,98,356,93,334,60,122,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The task involves classifying non-cancerous (196 training, 25 test) vs. pre-cancerous (66 training, 23 test) vs. cancerous (161 training, 32 test). We further used 20% (85 slides) training slides for validation. The cropped patches (1.24M training, 195K test) were extracted at 10√ó magnification. Comparison of accuracy and AUROC on three datasets. Reported metrics (in %age) are the average across 3 runs. "Num. of Parameters" represents the number of optimized parameters</figDesc><table><row><cell>Dataset</cell><cell cols="2">TCGA-BRCA</cell><cell cols="2">TCGA-CRC</cell><cell>BRIGHT</cell><cell></cell><cell>Num. of</cell></row><row><cell>Metric</cell><cell cols="7">Accuracy AUROC Accuracy AUROC Accuracy AUROC Parameters</cell></row><row><cell cols="2">Conventional MIL 92.10</cell><cell>96.65</cell><cell>73.02</cell><cell>69.24</cell><cell>62.08</cell><cell>80.96</cell><cell>70k</cell></row><row><cell>Full fine-tuning</cell><cell>88.14</cell><cell>93.78</cell><cell>74.53</cell><cell>56.63</cell><cell>56.13</cell><cell>75.87</cell><cell>5.6M</cell></row><row><cell cols="2">Prompt-MIL (ours) 93.47</cell><cell>96.89</cell><cell>75.47</cell><cell>75.45</cell><cell>64.58</cell><cell>81.31</cell><cell>70k + 192</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of GPU memory consumption and training speed per slide benchmarked on the BRIGHT dataset between the full fine-tuning and our prompt tuning on four slides with different sizes. Our prompt method requires far less memory and is significantly faster.</figDesc><table><row><cell></cell><cell>WSI size</cell><cell cols="4">44k √ó 21k 26k √ó 21k 22k √ó 17k 11k √ó 16k</cell></row><row><cell></cell><cell>#Tissue patches</cell><cell>9212</cell><cell>4765</cell><cell>2307</cell><cell>1108</cell></row><row><cell>GPU Mem.</cell><cell>Full fine-tuning</cell><cell>21.81G</cell><cell>18.22G</cell><cell>16.37G</cell><cell>12.71G</cell></row><row><cell></cell><cell>Prompt (ours)</cell><cell>12.04G</cell><cell>10.66G</cell><cell>10.00G</cell><cell>7.90G</cell></row><row><cell></cell><cell cols="2">Reduction percentage 44.79%</cell><cell>41.50%</cell><cell>38.92%</cell><cell>37.84%</cell></row><row><cell cols="2">Time per slide Full fine-tuning</cell><cell>17.73 s</cell><cell>8.92 s</cell><cell>4.37 s</cell><cell>2.15 s</cell></row><row><cell></cell><cell>Prompt (ours)</cell><cell>13.92 s</cell><cell>7.09 s</cell><cell>3.35s</cell><cell>1.56 s</cell></row><row><cell></cell><cell cols="2">Reduction percentage 21.49%</cell><cell>20.51%</cell><cell>23.32%</cell><cell>27.27%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of accuracy and AUROC on three datasets for a pathological foundation model.</figDesc><table><row><cell>Dataset</cell><cell cols="2">TCGA-BRCA</cell><cell>BRIGHT</cell><cell></cell></row><row><cell>Metric</cell><cell cols="4">Accuracy AUROC Accuracy AUROC</cell></row><row><cell>ViT-small [27]</cell><cell>91.75</cell><cell>97.03</cell><cell>54.17</cell><cell>76.76</cell></row><row><cell cols="2">ViT-small w/ Prompt-MIL 92.78</cell><cell>97.53</cell><cell>57.50</cell><cell>78.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance with a different number of prompt tokens. For two different WSI classification tasks, one token was enough to boost the performance of the conventional MIL schemes.</figDesc><table><row><cell>Dataset</cell><cell cols="2">TCGA-BRCA</cell><cell>BRIGHT</cell><cell></cell></row><row><cell cols="5">#prompt tokens k Accuracy AUROC Accuracy AUROC</cell></row><row><cell>k = 1</cell><cell>93.47</cell><cell>96.89</cell><cell>64.58</cell><cell>81.31</cell></row><row><cell>k = 2</cell><cell>93.13</cell><cell>96.93</cell><cell>60.41</cell><cell>79.74</cell></row><row><cell>k = 3</cell><cell>93.47</cell><cell>96.86</cell><cell>59.17</cell><cell>76.75</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partially supported by the <rs type="funder">ANR Hagnodice</rs> <rs type="grantNumber">ANR-21-CE45-0007</rs>, the <rs type="funder">NSF</rs> <rs type="grantNumber">IIS-2212046</rs>, the <rs type="funder">NSF</rs> <rs type="grantNumber">IIS-2123920</rs>, the <rs type="funder">NIH</rs> <rs type="grantNumber">1R21CA258493-01A1</rs>, the <rs type="institution">NCI UH3CA225021</rs> and <rs type="funder">Stony Brook University Provost Funds</rs>. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_S6gJCjC">
					<idno type="grant-number">ANR-21-CE45-0007</idno>
				</org>
				<org type="funding" xml:id="_vxhZZXx">
					<idno type="grant-number">IIS-2212046</idno>
				</org>
				<org type="funding" xml:id="_GQ9zzNH">
					<idno type="grant-number">IIS-2123920</idno>
				</org>
				<org type="funding" xml:id="_PKhxpvB">
					<idno type="grant-number">1R21CA258493-01A1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Development and validation of a weakly supervised deep learning framework to predict the status of molecular pathways and key mutations in colorectal cancer from routine histology images: a retrospective study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bilal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="763" to="e772" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BRACS: a dataset for breast carcinoma subtyping in H&amp;E histology images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brancati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page">93</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to gigapixel images via hierarchical self-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="16144" to="16155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16 √ó 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PPT: pre-trained prompt tuning for fewshot learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8410" to="8423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Patch-based convolutional neural network for whole slide tissue image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2424" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual prompt tuning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19827-4_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19827-4_41" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ciss√©</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Tel Aviv, Israel; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-10-27">23-27 October 2022. 2022</date>
			<biblScope unit="volume">XXXIII</biblScope>
			<biblScope unit="page" from="709" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-09">7-9 May 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Radiology data from the Cancer Genome Atlas Breast Invasive Carcinoma [TCGA-BRCA] collection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lingle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Imaging Arch</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">P-tuning: prompt tuning can be comparable to fine-tuning across scales and tasks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparative molecular analysis of gastrointestinal adenocarcinomas</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="721" to="735" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data-efficient and weakly supervised computational pathology on whole-slide images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comprehensive molecular characterization of human colon and rectal cancer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G A</forename><surname>Network</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">487</biblScope>
			<biblScope unit="page" from="330" to="337" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Streaming convolutional neural networks for end-to-end learning with multi-megapixel images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pinckaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1581" to="1590" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Platform</surname></persName>
		</author>
		<ptr target="http://www.wisepaip.org/paip/" />
		<title level="m">Data retrieved from PAIP</title>
		<imprint>
			<publisher>PAIP</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The power of prompt tuning for low-resource semantic parsing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TransMIL: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-stage pathological image classification using semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Takahama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10702" to="10711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TransPath: transformer-based self-supervised learning for histopathological image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_18" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part VIII</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transformer-based unsupervised contrastive learning for histopathological image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102559</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The cancer genome atlas pan-cancer analysis project</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Genet</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1113" to="1120" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gigapixel whole-slide images classification using locally supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_19" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
