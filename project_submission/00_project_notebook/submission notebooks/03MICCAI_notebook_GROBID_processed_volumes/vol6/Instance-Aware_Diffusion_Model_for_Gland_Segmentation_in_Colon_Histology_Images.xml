<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images</title>
				<funder ref="#_ZSfKaBQ #_3zqpsbK #_MXb5BZW #_e3d2pQq">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_V9gyQw6">
					<orgName type="full">Provincial Natural Science Foundation of Shandong Province of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mengxue</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Shandong Normal University</orgName>
								<address>
									<settlement>Jinan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenhui</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Shandong Normal University</orgName>
								<address>
									<settlement>Jinan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanjie</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Shandong Normal University</orgName>
								<address>
									<settlement>Jinan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="662" to="672"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6A7FD51F45BA48D1D86145A12D33F34B</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_64</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gland segmentation</term>
					<term>Diffusion model</term>
					<term>Colon histology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In pathological image analysis, determination of gland morphology in histology images of the colon is essential to determine the grade of colon cancer. However, manual segmentation of glands is extremely challenging and there is a need to develop automatic methods for segmenting gland instances. Recently, due to the powerful noise-toimage denoising pipeline, the diffusion model has become one of the hot spots in computer vision research and has been explored in the field of image segmentation. In this paper, we propose an instance segmentation method based on the diffusion model that can perform automatic gland instance segmentation. Firstly, we model the instance segmentation process for colon histology images as a denoising process based on the diffusion model. Secondly, to recover details lost during denoising, we use Instance Aware Filters and multi-scale Mask Branch to construct global mask instead of predicting only local masks. Thirdly, to improve the distinction between the object and the background, we apply Conditional Encoding to enhance the intermediate features with the original image encoding. To objectively validate the proposed method, we compared state-of-the-art deep learning model on the 2015 MICCAI Gland Segmentation challenge (GlaS) dataset and the Colorectal Adenocarcinoma Gland (CRAG) dataset. The experimental results show that our method improves the accuracy of segmentation and proves the efficacy of the method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colorectal cancer is a prevalent form of cancer characterized by colorectal adenocarcinoma, which develops in the colon or rectum's inner lining and exhibits glandular structures <ref type="bibr" target="#b4">[5]</ref>. These glands play a critical role in protein and carbohydrate secretion across various organ systems. Histological examinations using Hematoxylin and Eosin staining are commonly conducted by pathologists to evaluate the differentiation of colorectal adenocarcinoma <ref type="bibr" target="#b14">[15]</ref>. The extent of gland formation is a crucial factor in determining tumor grade and differentiation. Accurate segmentation of glandular instances on histological images is essential for evaluating glandular morphology and assessing colorectal adenocarcinoma malignancy. However, manual annotation of glandular instances is a time-consuming and expertise-demanding process. Hence, automated methods for glandular instance segmentation hold significant value in clinical practice. Automated segmentation has been explored using deep learning techniques <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b33">33]</ref>, including U-Net <ref type="bibr" target="#b16">[17]</ref>, FCN <ref type="bibr" target="#b12">[13]</ref>, Siamese network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and their variations for semantic segmentation <ref type="bibr" target="#b31">[31]</ref>. There are also methods that combine information bottleneck for detection and segmentation <ref type="bibr" target="#b23">[23]</ref>. Additionally, twostage instance segmentation methods like Mask R-CNN <ref type="bibr" target="#b6">[7]</ref> and BlendMask <ref type="bibr" target="#b2">[3]</ref> have been utilized, combining object detection and segmentation sub-networks. However, these methods may face difficulties in capturing different cell shapes and distinguishing tightly positioned gland boundaries. Limitations arise from image scaling and cropping, leading to information loss or distortion, resulting in ineffective boundary recognition and over-/under-segmentation. To overcome these limitations, we aim to perform gland instance segmentation to accurately identify the target location and prevent misclassification of background tissue.</p><p>Recently, diffusion model <ref type="bibr" target="#b8">[9]</ref> has gained popularity as efficient generative models <ref type="bibr" target="#b15">[16]</ref>. In the task of image synthesis, diffusion model has evolved to achieve state-of-the-art performance in terms of quality and mode coverage compared with GAN <ref type="bibr" target="#b32">[32]</ref>. Furthermore, diffusion model has been applied to various other tasks <ref type="bibr" target="#b17">[18]</ref>. DiffusionDet <ref type="bibr" target="#b3">[4]</ref> treats the object detection task as a generative task on the bounding box space in images to handle projection detection. Several studies have explored the feasibility of using diffusion model in image segmentation <ref type="bibr" target="#b26">[26]</ref>. These methods generate segmentation maps from noisy images and demonstrate better representation of segmentation details compared to previous deep learning methods.</p><p>In this paper, we propose a new method for gland instance segmentation based on the diffusion model. (1) Our method utilizes a diffusion model to perform denoising and tackle the task of gland instance segmentation in histology images. The noise boxes are generated from Gaussian noise, and the predicted ground truth (GT) boxes and segmentation masks are performed during the diffusion process. (2) To improve segmentation, we use instance-aware techniques to recover lost details during denoising. This includes employing a filter and a multi-scale Mask Branch to create a global mask and refine finer segmentation details. (3) To enhance object-background differentiation, we utilize Conditional Encoding to augment intermediate features with the original image encoding. This method effectively integrates the abundant information from the original image, thereby enhancing the distinction between the objects and the surrounding background. Our proposed method was trained and tested on the 2015 MIC-CAI Gland Segmentation (GlaS) Challenge dataset <ref type="bibr" target="#b20">[20]</ref> and Colorectal Adenocarcinoma Gland (CRAG) dataset <ref type="bibr" target="#b5">[6]</ref> (as shown in Fig. <ref type="figure" target="#fig_0">1</ref>), and the experiment results demonstrate the efficacy of the method. To preserve multi-scale information, we introduce a Mask Branch that operates on F mask . By applying convolutions with weights assigned from filters to F mask , we obtain instance masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we present the architecture of our proposed method, which includes an Image Encoder, an Image Decoder, and a Mask Branch. The network structure is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Encoder</head><p>We propose to perform subsequent operations on the features of the original image, so we use an Image Encoder for advanced feature extraction. The Image Encoder takes the original image as input and we use a convolutional neural network such as ResNet <ref type="bibr" target="#b7">[8]</ref> for feature extraction and a Feaure Pyramid Network (FPN) <ref type="bibr" target="#b11">[12]</ref> is used to generate a multi-scale feature map for ResNet backbone following.</p><p>The input image is x and the output is a high-level feature F R .</p><formula xml:id="formula_0">F R = F(x) (1)</formula><p>where F is the ResNet. The Image Encoder operates only once and uses the F R as condition to progressively refine and generate predictions from the noisy boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Decoder</head><p>We designed our model based on the diffusion model <ref type="bibr" target="#b30">[30]</ref>, which typically uses two Markov chains divided into two phases: a forward diffusion process and a reverse denoising process. The components of diffusion model are a learning reverse process called p θ (z t-1 |z t ) that creates samples by converting noise into samples from q(z 0 ) and a forward diffusion process called q(z t |z t-1 ) that gradually corrupts data from some target distribution into a normal distribution. The forward diffusion process is defined as:</p><formula xml:id="formula_1">q(z t |z t-1 ) = N (z t , 1 -β t z t-1 , β t I)<label>(2)</label></formula><p>A variance schedule β t ∈ (0, 1), t ∈ {1, ..., T } determines the amount of noise that is introduced at each stage. Alternatively, we can obtain a sample of z t from direct z 0 as follows:</p><formula xml:id="formula_2">z t = √ ᾱt z 0 + (1 -ᾱt )<label>(3)</label></formula><p>where ᾱt = t s=0 (1β s ), ∼ N (0, I). Our Image Decoder is based on diffusion model, which can be viewed as a noise-to-GT denoising process. In this setting, the data samples consist of a set of bounding boxes represented as z 0 , where z 0 is a set of N boxes.</p><p>The neural network f θ (z t , t) is trained to predict z 0 from the z t based on the corresponding image x. In addition, to achieve complementary information by integrating the segmentation information from z t into the original image encoding, we introduce Conditional Encoding, which uses the encoding features of the current step to enhance its intermediate features.</p><formula xml:id="formula_3">f θ (z t , F R , t) = D((Concat(E(z t , F R ), F R ), t), t) (<label>4</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">z 0 = f (• • •(f (z T -m , F R , T -m)) (5)</formula><p>where D represent the decoder, E represent the encoder and m ∈ {1, ..., T }. We use Instance Aware Filters (IAF) during iterative sampling, which allows sharing parameters between steps.</p><formula xml:id="formula_6">F t f = IAF (f θ (z t , F R , t), t) (6)</formula><p>where F t f is the output feature of the filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mask Branch</head><p>We have also utilized dynamic mask head <ref type="bibr" target="#b22">[22]</ref> to predict masks in our study.</p><p>In this stage, we use the Mask Branch to fuse the different scale information of the FPN and output the mask feature F mask . The diffusion process decodes RoI features into local masks, and multi-scale features can be supplemented with more detailed information for predicting global masks to compensate for the detail lost in the diffusion process, and we believe that instance masks require a larger perceptual domain because of the higher demands on instance edges. Specifically, the instance mask can be generated by convolving the feature map F mask from the Mask Branch and F t f from the IAF , which is calculated as follows:</p><formula xml:id="formula_7">s = MF H(F mask , F t f )<label>(7)</label></formula><p>where the predicted instance mask is denoted by s ∈ R H×W . The Mask FCN Head, denoted by MF H, is comprised of three 1 × 1 convolutional layers. We enhance our loss function by incorporating two components, L d and L s , and utilize the γ parameter to optimize the balance between these two losses.</p><formula xml:id="formula_8">L = L d + γL s (s, s GT ) (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>where the L s in our model represents the measure of overlap between the predicted instance mask and the ground truth s GT <ref type="bibr" target="#b13">[14]</ref>, and the L d is the loss of DiffusionDet. The optimal value for the parameter γ is usually determined based on achieving the best overall performance on the validation set. In this work, we chose γ = 5 to balance these two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We presented the segmentation results of our model compared to the ground truth in Fig. <ref type="figure" target="#fig_2">3</ref>, and provided both qualitative and quantitative evaluations that validate the effectiveness of our proposed network for gland instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and Evaluation Metrics:</head><p>We evaluated the effectiveness of the proposed model on two datasets: the GlaS dataset and the CRAG dataset. The GlaS dataset comprises 85 training and 80 testing images, divided into 60 images in Test A and 20 images in Test B. The CRAG dataset consists of 173 training and 40 testing images. We have adopted Vahadane method for stain normalization <ref type="bibr" target="#b0">[1]</ref>. Furthermore, to enhance the training dataset and mitigate the risk of overfitting, we employed random combinations of image flipping, translation, Gaussian blur, brightness variation, and other augmentation techniques.</p><p>We assessed the segmentation results using three metrics from the GlaS Challenge: (1) Object F1, which measures the accuracy of detecting individual glands, (2) Object Dice, which evaluates the volume-based accuracy of gland segmentation, and (3) Object Hausdorff, which assesses the shape similarity between the segmentation result and the ground truth. We assigned each method three ranking numbers based on these metrics and computed their sum to determine the final ranking for each method's overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details:</head><p>In our experiments, we choose the ResNet-50 with FPN as the backbone in the proposed method. The backbone is pretrained on ImageNet. Image decoder, Mask Branch and Mask FCN Head are trained end-toend. We trained on the GlaS and CRAG datasets in a Python 3.8.3 environment on Ubuntu 18.04, using PyTorch 1.10 and CUDA 11.4. During training, we utilized an SGD optimizer with a learning rate of 2.5 × 10 -5 and the weight decay as 10 -4 . We set diffusion timesteps T = 1000 and chose a linear schedule from β 1 = 10 -4 to β T = 0.02. Training was performed on A100 GPU with a batch size of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on the GlaS Challenge Dataset:</head><p>We conducted experiments to evaluate the performance of our proposed model by comparing it with the DSE model <ref type="bibr" target="#b27">[27]</ref>, the DMCN <ref type="bibr" target="#b28">[28]</ref>, the DCAN <ref type="bibr" target="#b1">[2]</ref>, the SPL-Net <ref type="bibr" target="#b29">[29]</ref>, the DoubleU-Net <ref type="bibr" target="#b24">[24]</ref>, the MILD-Net <ref type="bibr" target="#b5">[6]</ref>, the GCSBA-Net <ref type="bibr" target="#b25">[25]</ref>, and the MPCNN <ref type="bibr" target="#b19">[19]</ref>. Table <ref type="table" target="#tab_0">1</ref> provides an overview of the average performance of these models.</p><p>Our proposed model demonstrated a enhancement in performance, surpassing the second-best method on both Test A and Test B datasets. Specifically, on Test A, we observed an improvement of 0.006, 0.01, and 1.793 in Object F1, Object Dice, and Object Hausdorf. Similarly, on Test B, resulting in an improvement of 0.022, 0.014 and 3.694 in Object F1, Object Dice, and Object Hausdorf, respectively. Although Test B presented a more challenging task due to the presence of complex morphology in the images, our proposed model demonstrated accurate segmentation in all cases. The experimental results highlighted the effectiveness of our approach in improving the accuracy of gland instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on the CRAG Dataset:</head><p>The proposed model was additionally evaluated on the CRAG dataset by comparing it against the GCSBA-Net, DoubleU-Net, DSE model, MILD-Net, and DCAN. The average performance of these models is shown in Table <ref type="table" target="#tab_1">2</ref>. Our experimental results demonstrate that our proposed method achieves superior performance, with improvements of 0.017, 0.012, and 4.026 for Object F1, Object Dice, and Object Hausdorff, respectively, compared to the second-best method. These results demonstrate the effectiveness of our method in segmenting different datasets.</p><p>Ablation Studies: Our network utilizes the Mask Branch and Conditional Encoding to enhance performance and segmentation quality. Ablation studies on the GlaS and CRAG datasets confirm the effectiveness of these modules (Table <ref type="table" target="#tab_2">3</ref>). The Mask Branch is responsible for multi-scale feature extraction and fusion with the backbone network, as well as refining the Image Decoder's output. Without the Mask Branch, direct usage of original image features lacks multi-scale information and results in less accurate segmentation. Conditional Encoding is employed to establish a connection between input image features  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>In this paper, we propose a diffusion model based method for gland instance segmentation. By considering instance segmentation as a denoising process based on diffusion model. Our model contains three main parts: Image Encoder, Image Decoder, and Mask Branch. By utilizing a diffusion model with Conditional Encoding for denoising, we are able to improve the precision of instance localization while compensating for the missing details in the diffusion model. By incorporating multi-scale information fusion, our approach results in more accurate segmentation outcomes. Experimental results on the GlaS dataset and CRAG dataset show that our method surpasses state-of-the-art approach, demonstrating its effectiveness. Although our method demonstrates excellent performance in gland instance segmentation, challenges arise in certain scenarios characterized by irregular shapes, flattening, and overlapping. In such cases, our network tends to classify multiple small targets with unclear boundaries as a single object, indicating limitations in segmentation accuracy when dealing with high aggregation or overlap. This limitation may stem from the difficulty in accurately distinguishing fine details between instances and the incorrect identification of boundaries.</p><p>To address these limitations, future work will focus on improving segmentation performance in challenging scenarios by specifically targeting three identified limitations: (1) Incorporate random noise during training to reduce reliance on bounding box information for denoising; (2) Explore more efficient methods for cross-step denoising in the diffusion model to improve processing time without compromising segmentation accuracy; and (3) Develop a more effective Conditional Encoding method to provide accurate instance context for noise filtering in discriminative tasks like nuclear segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a-b) Example images from the CRAG dataset. (c-d) Example images from the GlaS dataset.</figDesc><graphic coords="2,107,97,145,76,236,11,70,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The network architecture diagram of our model. The Image Encoder consists of a backbone that extracts multi-scale features from the input image. The Image Decoder based on a diffusion model incorporates the original image features as conditions to enhance the intermediate features.To preserve multi-scale information, we introduce a Mask Branch that operates on F mask . By applying convolutions with weights assigned from filters to F mask , we obtain instance masks.</figDesc><graphic coords="3,44,79,201,02,327,88,104,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The instance segmentation results on the GlaS dataset and CRAG dataset.From top to bottom: the original images, the ground truth, and the segmentation results produced by our method.</figDesc><graphic coords="6,56,46,54,02,339,49,105,67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The experimental results on GlaS Challenge dataset. The S represents the score, R represents the rank and Rank Sum refers to the sum of rank for each evaluation metric.</figDesc><table><row><cell>Models</cell><cell cols="2">Object F1</cell><cell cols="2">Object Dice</cell><cell cols="3">Object Hausdorff (pixels) Rank Sum</cell></row><row><cell></cell><cell>Test A</cell><cell>Test B</cell><cell>Test A</cell><cell>Test B</cell><cell>Test A</cell><cell>Test B</cell></row><row><cell></cell><cell>S</cell><cell>R S</cell><cell>R S</cell><cell>R S</cell><cell>R S</cell><cell>R S</cell><cell>R</cell></row><row><cell>Proposed</cell><cell cols="6">0.941 1 0.893 1 0.939 1 0.889 1 26.042 1 72.351</cell><cell>1</cell><cell>6</cell></row><row><cell cols="7">DoubleU-Net [24] 0.935 2 0.871 2 0.929 2 0.875 2 27.835 2 76.045</cell><cell>2</cell><cell>12</cell></row><row><cell>DSE model [27]</cell><cell cols="6">0.926 3 0.862 3 0.927 3 0.871 3 31.209 3 80.509</cell><cell>3</cell><cell>18</cell></row><row><cell>GCSBA-Net [25]</cell><cell cols="5">0.916 5 0.832 6 0.914 4 0.834 6 41.49</cell><cell>4 102.88</cell><cell>4</cell><cell>29</cell></row><row><cell>MILD-Net [6]</cell><cell cols="7">0.914 6 0.844 4 0.913 5 0.836 5 41.540 5 105.890 5</cell><cell>30</cell></row><row><cell>SPL-Net [29]</cell><cell cols="7">0.924 4 0.844 4 0.902 7 0.840 4 49.881 8 106.075 6</cell><cell>33</cell></row><row><cell>DMCN [28]</cell><cell cols="7">0.893 8 0.843 5 0.908 6 0.833 7 44.129 6 116.821 7</cell><cell>39</cell></row><row><cell>DCAN [2]</cell><cell cols="7">0.912 7 0.716 7 0.897 8 0.781 9 45.418 7 160.347 9</cell><cell>47</cell></row><row><cell>MPCNN [19]</cell><cell cols="7">0.891 9 0.703 8 0.882 9 0.786 8 57.413 9 145.575 8</cell><cell>51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The experimental results on the CRAG dataset. and the diffusion model. Performing reverse diffusion without any reference condition can introduce numerous errors and require multiple iterations to achieve the desired outcome. When employing Mask Branch, our approach resulted in an improvement of 0.082, 0.09, 0.07 in Object F1, and 0.07, 0.078, 0.07 in Object Dice, while Object Hausdorff decreased by10.29, 11.11, 24.47 on GlaS Test A,</figDesc><table><row><cell>Models</cell><cell cols="5">Object F1 Object Dice Object Hausdorff (pixels) Rank Sum</cell></row><row><cell></cell><cell>S</cell><cell cols="2">R S</cell><cell>R</cell><cell>S</cell><cell>R</cell></row><row><cell>Proposed</cell><cell cols="2">0.853 1</cell><cell cols="2">0.906 1</cell><cell>113.224 1</cell><cell>3</cell></row><row><cell cols="3">GCSBA-Net [25] 0.836 2</cell><cell cols="2">0.894 2</cell><cell>146.77 4</cell><cell>8</cell></row><row><cell cols="3">DoubleU-Net [24] 0.835 3</cell><cell cols="2">0.890 3</cell><cell>117.25 2</cell><cell>8</cell></row><row><cell>DSE model [27]</cell><cell cols="2">0.835 3</cell><cell cols="2">0.889 4</cell><cell>120.127 3</cell><cell>10</cell></row><row><cell>MILD-Net [6]</cell><cell cols="2">0.825 4</cell><cell cols="2">0.875 5</cell><cell>160.140 5</cell><cell>14</cell></row><row><cell>DCAN [2]</cell><cell cols="2">0.736 5</cell><cell cols="2">0.794 6</cell><cell>218.76 6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The ablation study results on the CRAG and GlaS datasets demonstrate the impact of different modules on performance. The Mask Branch module contributes to multi-scale feature extraction, while the Conditional Encoding module establishes the connection between input image features and the diffusion model.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Object F1</cell><cell>Object Dice Object Hausdorff (pixels)</cell></row><row><cell></cell><cell>Mask Branch Conditional Encoding</cell><cell></cell><cell></cell></row><row><cell>GlaS Test A</cell><cell></cell><cell cols="2">0.941±0.041 0.939±0.057 26.042±1.688</cell></row><row><cell></cell><cell></cell><cell cols="2">0.893±0.105 0.913±0.092 32.813±2.731</cell></row><row><cell></cell><cell></cell><cell cols="2">0.859±0.098 0.869±0.107 36.332±1.460</cell></row><row><cell>GlaS Test B</cell><cell></cell><cell cols="2">0.893±0.046 0.889±0.072 72.351±1.271</cell></row><row><cell></cell><cell></cell><cell cols="2">0.859±0.061 0.847±0.072 80.466±4.604</cell></row><row><cell></cell><cell></cell><cell cols="2">0.803±0.077 0.811±0.082 83.461±3.704</cell></row><row><cell>CRAG</cell><cell></cell><cell cols="2">0.853±0.045 0.906±0.066 113.224±2.464</cell></row><row><cell></cell><cell></cell><cell cols="2">0.801±0.062 0.849±0.071 125.365±3.599</cell></row><row><cell></cell><cell></cell><cell cols="2">0.783±0.073 0.836±0.083 137.694±5.934</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is supported by funds from the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62003196</rs>, <rs type="grantNumber">62076249</rs>, <rs type="grantNumber">62072289</rs>, and <rs type="grantNumber">62073201</rs>) and the <rs type="funder">Provincial Natural Science Foundation of Shandong Province of China</rs> (<rs type="grantNumber">ZR2020QF032</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZSfKaBQ">
					<idno type="grant-number">62003196</idno>
				</org>
				<org type="funding" xml:id="_3zqpsbK">
					<idno type="grant-number">62076249</idno>
				</org>
				<org type="funding" xml:id="_MXb5BZW">
					<idno type="grant-number">62072289</idno>
				</org>
				<org type="funding" xml:id="_e3d2pQq">
					<idno type="grant-number">62073201</idno>
				</org>
				<org type="funding" xml:id="_V9gyQw6">
					<idno type="grant-number">ZR2020QF032</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast gpu-enabled color normalization for digital pathology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Systems, Signals and Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="219" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dcan: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blendmask: top-down meets bottom-up for instance segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8573" to="8581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09788</idno>
		<title level="m">Diffusiondet: diffusion model for object detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Colorectal carcinoma: pathologic aspects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Tatishchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Gastrointesti. Oncol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mild-net: minimal information loss dilated network for gland instance segmentation in colon histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="199" to="211" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting probabilistic siamese visual tracking with a conditional variational autoencoder</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14213" to="14219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end multitask siamese network with residual hierarchical attention for real-time object tracking</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1908" to="1921" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Digital pathology and artificial intelligence</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K K</forename><surname>Niazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Parwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gurcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Oncol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="253" to="e261" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What is healthy? generative counterfactual diffusion for lesion localization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kascenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09-22">22 September 2022</date>
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-18576-2_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-18576-24" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: the glas challenge contest</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A stochastic polygons model for glandular structures in colon histology images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2366" to="2378" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving the classification ability of network utilizing fusion technique in contrast-enhanced spectral mammography</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="966" to="977" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-817" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Information bottleneck-based interpretable multitask network for breast cancer classification and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102687</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DoubleU-net: colorectal cancer diagnosis and gland instance segmentation with text-guided feature control</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-66415-2_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-66415-222" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Fusiello</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12535</biblScope>
			<biblScope unit="page" from="338" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gcsba-net: gabor-based and cascade squeeze bi-attention network for gland segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1185" to="1196" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diffusion models for implicit image segmentation ensembles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valmaggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1336" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep segmentation-emendation model for gland instance segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-752" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gland instance segmentation using deep multichannel neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2901" to="2912" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A deep model with shape-preserving loss for gland instance segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-216" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="138" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.00796</idno>
		<title level="m">Diffusion models: a comprehensive survey of methods and applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative error prediction network for semi-supervised colon gland segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102458</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Symreg-gan: symmetric image registration with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5631" to="5646" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image matting with deep gaussian process</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
