<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning</title>
				<funder ref="#_ZdPVTQg">
					<orgName type="full">Fundamental Research Funds for the Central Universities of China</orgName>
				</funder>
				<funder ref="#_QjpebzS #_6dapejD #_vkQ6tYg">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kun</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Image Processing Center</orgName>
								<orgName type="department" key="dep2">School of Astronautics</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>102206</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yushan</forename><surname>Zheng</surname></persName>
							<email>yszheng@buaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Engineering Medicine</orgName>
								<orgName type="department" key="dep2">Advanced Innovation Center on Biomedical Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Shi</surname></persName>
							<email>juns@hfut.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<postCode>230601</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fengying</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Image Processing Center</orgName>
								<orgName type="department" key="dep2">School of Astronautics</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>102206</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiguo</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Image Processing Center</orgName>
								<orgName type="department" key="dep2">School of Astronautics</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>102206</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="714" to="724"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">46C2B5FFBD124502289983D192D074E3</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_69</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>WSI representation learning â€¢ Self-supervised learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer-based multiple instance learning (MIL) framework has been proven advanced for whole slide image (WSI) analysis. However, existing spatial embedding strategies in Transformer can only represent fixed structural information, which are hard to tackle the scalevarying and isotropic characteristics of WSIs. Moreover, the current MIL cannot take advantage of a large number of unlabeled WSIs for training. In this paper, we propose a novel self-supervised whole slide image representation learning framework named position-aware masked autoencoder (PAMA), which can make full use of abundant unlabeled WSIs to improve the discrimination of slide features. Moreover, we propose a position-aware cross-attention (PACA) module with a kernel reorientation (KRO) strategy, which makes PAMA able to maintain spatial integrity and semantic enrichment during the training. We evaluated the proposed method on a public TCGA-Lung dataset with 3,064 WSIs and an in-house Endometrial dataset with 3,654 WSIs, and compared it with 6 state-of-the-art methods. The results of experiments show our PAMA is superior to SOTA MIL methods and SSL methods. The code will be available at https://github.com/WkEEn/PAMA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, the development of histopathological whole slide image (WSI) analysis methods has dramatically contributed to the intelligent cancer diagnosis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>. However, due to the limitation of hardware resources, it is difficult to directly process gigapixel WSIs in an end-to-end framework. Recent studies usually divide the WSI analysis into multiple stages.</p><p>Generally, multiple instance learning (MIL) is one of the most popular solutions for WSI analysis <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. MIL methods regard WSI recognition as a weakly supervised learning problem and focus on how to effectively and efficiently aggregate histopathological local features into a global representation. Several studies introduced attention mechanisms <ref type="bibr" target="#b8">[9]</ref>, recurrent neural networks <ref type="bibr" target="#b1">[2]</ref> and graph neural network <ref type="bibr" target="#b7">[8]</ref> to enhance the capacity of MIL in structural information mining. More recently, Transformer-based structures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref> are proposed to aggregate long-term relationships of tissue regions, especially for large-scale WSIs. These Transformer-based models achieved state-of-the-art performance in sub-type classification, survival prediction, gene mutant prediction, etc. However, these methods still rely on at least patient-level annotations. In the networkbased consultation and communication platforms, there is a vast quantity of unlabeled WSIs not effectively utilized. These WSIs are usually without any annotations or definite diagnosis descriptions but are available for unsupervised learning. In this case, self-supervised learning (SSL) is gradually introduced into the MIL-based framework and is becoming a new paradigm for WSI analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. Typically, Chen et al. <ref type="bibr" target="#b4">[5]</ref> explored and posed a new challenge referred to as slide-level self-learning and proposed HIPT, which leveraged the hierarchical structure inherent in WSIs and constructed multiple levels of the self-supervised learning framework to learn high-resolution image representations. This approach enables MIL-based frameworks to take advantage of abundant unlabeled WSIs, further improving the accuracy and robustness of tumor recognition.</p><p>However, HIPT is a hierarchical learning framework based on a greedy training strategy. The bias and error generated in each level of the representation model will accumulate in the final decision model. Moreover, the ViT <ref type="bibr" target="#b5">[6]</ref> backbone used in HIPT is originally designed for nature sense images in fixed sizes whose positional information is consistent. However, histopathological WSIs are scale-varying and isotropic. The positional embedding strategy of ViT will bring ambiguity into the structural modeling. To relieve this problem, KAT <ref type="bibr" target="#b18">[19]</ref> built hierarchical masks based on local anchors to maintain multi-scale relative distance information in the training. But these masks are manually defined which is not trainable and lacked orientation information. The current embedding strategy for WSI structural description is not complete.</p><p>In this paper, we propose a novel whole slide image representation learning framework named position-aware masked autoencoder (PAMA), which achieves slide-level representation learning by reconstructing the local representations of the WSI in the patch feature space. PAMA can be trained end-to-end from the local features to the WSI-level representation. Moreover, we designed a position-aware cross-attention mechanism to guarantee the correlation of localto-global information in the WSIs while saving computational resources. The proposed approach was evaluated on a public TCGA-Lung dataset and an in-house Endometrial dataset and compared with 6 state-of-the-art methods. The results have demonstrated the effectiveness of the proposed method.</p><p>The contribution of this paper can be summarized into three aspects. (1) We propose a novel whole slide image representation learning framework named position-aware masked autoencoder (PAMA). PAMA can make full use of abundant unlabeled WSIs to learn discriminative WSI representations. (2) We propose a position-aware cross-attention (PACA) module with a kernel reorientation (KRO) strategy, which makes the framework able to maintain the spatial integrity and semantic enrichment of slide representation during the selfsupervised training. <ref type="bibr" target="#b2">(3)</ref> The experiments on two datasets show our PAMA can achieve competitive performance compared with SOTA MIL methods and SSL methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation and Data Preparation</head><p>MAE <ref type="bibr" target="#b6">[7]</ref> is a successful SSL framework that learns image presentations by reconstructing the masked image in the original pixel space. We introduced this paradigm to WSI-level representation learning. The flowchart of the proposed work is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. First, we divided WSIs into non-overlapping image patches and meanwhile removed the background without tissue regions based on a threshold (as shown in Fig. <ref type="figure" target="#fig_0">1(I)</ref>). Then, we applied the self-supervised learning framework DINO <ref type="bibr" target="#b2">[3]</ref> for patch feature learning and extraction. Afterward, the features for a WSI are represented as X âˆˆ R npÃ—d f , where d f is the dimension of the feature and n p is the number of patches in the WSI. Inspired by KAT <ref type="bibr" target="#b18">[19]</ref>, we extracted multiple anchors by clustering the location coordinates of patches for the auxiliary description of the WSI structure. We assigned trainable representations for these anchors, which are formulated as K âˆˆ R n k Ã—d f , where n k is the number of anchors in the WSI. Here, we regard each anchor as an observation point of the tissue and assess the relative distance and orientation from the patch positions to the anchor positions. Specifically, a polar coordinate system is built on each anchor position, and the polar coordinates of all the patches on the system are recorded. Finally, a relative distance matrix D âˆˆ N n k Ã—np and relative polar angle matrix P âˆˆ N n k Ã—np are obtained, where D ij âˆˆ D and P ij âˆˆ P respectively represent the distance and polar angle of the i-th patch in the polar coordinate system that takes the position of the j-th anchor as the pole. Then, we can formulate a WSI as S = {X, K, D, P}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Masked WSI Representation Autoencoder</head><p>Figure <ref type="figure" target="#fig_0">1</ref>(II) illustrates the procedure of WSI representation learning. Referring to MAE <ref type="bibr" target="#b6">[7]</ref>, we random mask patch tokens with a high masking ratio (i.e. 75% in our experiments). The remaining tokens (as shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>) are fed into the encoder. Each encoder block sequentially consists of LayerNorm, PACA module, LayerNorm, and multilayer perceptron (MLP), as shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>). Then, masked tokens are appended into encoded tokens to conduct the full set of tokens, which is shown in Fig. <ref type="figure" target="#fig_0">1(d)</ref>. Next, the decoder reconstructs the slide representation in feature space. Finally, mean squared error (MSE) loss is built between the reconstructed patch features and the original patch features. Referring to MAE <ref type="bibr" target="#b6">[7]</ref>, a trainable token is appended to the patch tokens to extract the global representation. After training, the pre-trained encoder will be employed as the backbone for various downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Position-Aware Cross-Attention</head><p>To preserve the structure information of the tissue, we propose the positionaware cross-attention (PACA) module, which is the core of the encoder and decoder blocks. The structure of PACA is shown in Fig. <ref type="figure" target="#fig_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(III).</head><p>The message passing between the anchors and patches is achieved by a bidirectional cross-attention between the patches and anchors. First, the anchors collect the local information from the patches, which is formulated as</p><formula xml:id="formula_0">K (n+1) = Ïƒ( K (n) W (n) q â€¢ (X (n) W (n) k ) T âˆš d e +Ï• d (D (n) )+Ï• p (P (n) ))â€¢(X (n) W (n) v ),<label>(1)</label></formula><p>where W l âˆˆ R d f Ã—de , l = q, k, v are learnable parameters with d e denoting the dimension of the head output, Ïƒ represents the softmax function, and Ï• d and Ï• p are the embedding functions that respectively take the distance and polar angle as input and output the corresponding trainable embedding values. Symmetrically, each patch token catches the information of all anchors into their own local representations by the equations</p><formula xml:id="formula_1">X (n+1) = Ïƒ( X (n) W (n) q â€¢ (K (n) W (n) k ) T âˆš d e +Ï• T d (D (n) )+Ï• T p (P (n) ))â€¢(K (n) W (n) v ). (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>The two-way communication makes the patches and anchors timely transmit local information and perceive the dynamic change of global information. The embedding of relative distance and polar angle information helps the model maintain the semantic and structural integrity of the WSI and meanwhile prevents the WSI representation from collapsing to the local area throughout the training process.</p><p>In terms of efficiency, the computational complexity of self-attention is O(n p 2 ) where n p is the number of patch tokens. In contrast, our proposed PACA's complexity is O(n k Ã— n p ) where n k is the number of anchors. Notice that n k &lt;&lt; n p , the complexity is close to O(n p ), i.e. linear correlation with the size of the WSI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Kernel Reorientation</head><p>As for the polar angle matrix P âˆˆ N n k Ã—np , we specify the horizontal direction of all the anchors as the initial polar axis. In natural scene images, there is natural directional conspicuousness of semantics. For instance, in the case of a church, it is most likely to find a door below the windows rather than be located above them. But histopathology images have no absolute definition of direction. The semantics of WSI will not change with rotation and flip. Namely, it is isotropic. Embedding the orientation information with a fixed polar axis will lead to ambiguities in various slides.</p><p>To address this problem, we design a kernel reorientation (KRO) strategy to dynamically update the polar axis during the training. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(IV), we equally divide the polar coordinate system into N bins and calculate the sum of the attention scores from each bin. Then, the orientation with the highest score is recognized as the new polar axis for the anchor. Based on the updated polar axis, we can then amend P (n) to P (n+1) . The detailed algorithm is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluated the proposed method on two datasets, the public TCGA-Lung and the in-house Endometrial dataset, which are introduced as follows.</p><p>Algorithm 1: Kernel Reorientation algorithm.</p><p>Input: P (n) âˆˆ N HÃ—n k Ã—np : The relative polar angle matrix of n-th block, where H is the head number of multi-head attention, n k is the number of anchors in the WSI, np is the number of patches in the WSI; A (n) âˆˆ R HÃ—n k Ã—np : The attention matrix from anchors to patches, defined as</p><formula xml:id="formula_3">A (n) = K (n) W (n) q â€¢(X (n) W (n) k ) T âˆš de ;</formula><p>D score : A dictionary taking the angle as KEY for storing attention scores; Output: P (n+1) âˆˆ R HÃ—n k Ã—np : The updated polar angle matrix.</p><formula xml:id="formula_4">for h in H do for i in n k do D score = 0 for j in np do D score [P (n) h,i,j ] += A (n) h,i,j ; end P (n)</formula><p>h,i,max = arg max D score ; // Find the orientation that has the highest attention score. for j in np do P</p><formula xml:id="formula_5">(n+1) h,i,j = P (n) h,i,j -P (n)</formula><p>h,i,max ;// Reorientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end end end</head><p>TCGA-Lung dataset is collected from The Cancer Genome Atlas (TCGA) Data Portal. The dataset includes a total of 3,064 WSIs, which consist of three categories, namely Tumor-free (Normal), Lung Adenocarcinoma (LUAD), and Lung Squamous Cancer (LUSC), Endometrial dataset includes 3,654 WSIs of endometrial pathology, which includes 8 categories, namely Well/Moderately/Low-differentiated endometrioid adenocarcinoma, Squamous differentiation carcinoma, Plasmacytoid carcinoma, Clear cell carcinoma, Mixed-cell adenocarcinoma, and benign tumor.</p><p>Each dataset was randomly divided into training, validation and test sets according to 6:1:3 while keeping each category of data proportionally. We conducted WSI multi-type classification experiments on the two datasets. The validation set was used to perform an early stop. The results of the test set were reported for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The WSI representation pre-training stage uses all training data and does not involve any supervised information. During the downstream classification task,  the pre-trained encoder is utilized as the slide representation extractor, and the [CLS ] token is fed into the following classifier consisting of a multilayer perceptron (MLP) and a fully connected layer. Following the protocol in selfsupervised learning <ref type="bibr" target="#b6">[7]</ref>, we evaluated the quality of pre-training with the two approaches: 1) Fine-tuning is to train the whole network parameters, including WSI encoder and classifier; 2) Linear probing is to freeze the encoder and only train the classifier. The usage of [CLS ] token refers to the MAE <ref type="bibr" target="#b6">[7]</ref> framework, which was concatenated with patch tokens. During pre-training, the [CLS ] token is not involved in loss computation, but it continuously interacts with kernels and receives global information. After pre-training, the pre-trained parameters of the [CLS ] token will be loaded for fine-tuning and linear probing.</p><p>To ensure the uniformity of patch features, we choose DINO <ref type="bibr" target="#b2">[3]</ref> to extract patch features on the magnification under 20Ã— lenses. Accuracy (ACC) and area under the ROC curve (AUC) are employed as evaluation metrics. We implemented all the models in Python 3.8 with PyTorch 1.7 and Cuda 10.2 and run the experiments on a computer with 4 GPUs of Nvidia Geforce 2080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effectiveness of the WSI Representation Learning</head><p>We first conducted experiments on the Endometrial dataset to verify the effectiveness of self-supervised learning for WSI analysis under label-limited conditions. The results are shown in Fig. <ref type="figure" target="#fig_1">2</ref>, where the performance obtained with different ratios of labeled training WSIs are compared. MAE <ref type="bibr" target="#b6">[7]</ref> based on the patch features is implemented as the baseline. Furthermore, we applied the proposed distance and polar angle embedding to the self-attention module of MAE <ref type="bibr" target="#b6">[7]</ref>, which is referred to as MAE+ in Fig. <ref type="figure" target="#fig_1">2</ref>. Overall, PAMA consistently achieves significantly better performance across all the label ratios than MAE <ref type="bibr" target="#b6">[7]</ref> and HIPT <ref type="bibr" target="#b4">[5]</ref>. These results have demonstrated the effectiveness of PAMA in WSI representation pre-training. Moreover, PAMA achieves the best stability in AUCs and ACCs when the label ratios are reduced from 85% to 10%. This is of practical importance as it reduces the dependence on a large number of labeled WSIs for training robust WSI analysis models. Meanwhile, it means that we can utilize the unlabeled WSIs to improve the capacity of the models with the help of PAMA. HIPT <ref type="bibr" target="#b4">[5]</ref> is a two-stage self-learning framework, which first leverages DINO <ref type="bibr" target="#b2">[3]</ref> to pre-train patches (256 Ã— 256) divided from regions (4096 Ã— 4096) and then utilizes DINO-4k <ref type="bibr" target="#b4">[5]</ref> to pre-train regions of WSIs. The multi-stage framework accumulated the training bias and noise, which caused an AUC gap of HIPT <ref type="bibr" target="#b4">[5]</ref> to MAE <ref type="bibr" target="#b6">[7]</ref> and PAMA, especially trained with only 10% labeled WSIs. We also observed a significant improvement when comparing MAE+ with MAE <ref type="bibr" target="#b6">[7]</ref>. It indicates the proposed distance and polar angle embedding strategy is more appreciated than the positional embedding of ViT <ref type="bibr" target="#b5">[6]</ref> to describe the structure of histopathological WSIs. Please refer to the supplementary materials for more detailed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>Then, we conducted ablation experiments to verify the necessity of the proposed structural embedding strategy. The detailed results are shown in Table <ref type="table" target="#tab_0">1</ref>, where all the models were fine-tuned with 35% training WSIs. It shows that the AUC decreases by 0.019 and 0.021, respectively, when the distance or polar angle embedding is discarded. And, when removing both the distance and polar angle embedding, the AUC drops by 0.034. These results demonstrate that local and global spatial information is crucial for PAMA to learn WSI representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison with SOTA Methods</head><p>Finally, we additionally compared the proposed PAMA with four weaklysupervised methods, DSMIL <ref type="bibr" target="#b11">[12]</ref>, TransMIL <ref type="bibr" target="#b12">[13]</ref>, SETMIL <ref type="bibr" target="#b17">[18]</ref> and KAT <ref type="bibr" target="#b18">[19]</ref>. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. Overall, PAMA consistently achieves the best performance. In comparison with the second-best methods, PAMA achieves an increase of 0.015/0.011 and 0.025/0.009 in AUCs on TCGA and Endometrial datasets, respectively, by using 35%/100% labeled WSIs. Moreover, PAMA reveals the most robust capacity when reducing the training data from 100% to 35%, with AUC decreasing slightly from 0.988 to 0.982 and from 0.851 to 0.829 on the two datasets. TransMIL <ref type="bibr" target="#b12">[13]</ref>, SETMIL <ref type="bibr" target="#b17">[18]</ref> and KAT <ref type="bibr" target="#b18">[19]</ref> are state-ofthe-art methods for histopathological image classification. They all considered the spatial adjacency of patches but neglected the orientation relationships of the patches. It is the main reason that the three methods cannot surpass our method even with 100% training WSIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed an effective self-supervised representation learning framework for WSI analysis. The experiments on two large-scale datasets have demonstrated the effectiveness of PAMA in the condition of limited-label. The results have shown superiority to the existing weakly-supervised and selfsupervised MIL methods. Future work will focus on training the WSI representation model based on datasets across multiple organs, thus promoting the generalization ability of the model for different downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overview of the proposed whole slide image representation with positionaware masked autoencoder (PAMA), where (I) shows the data preprocessing including the patch embedding and anchors clustering, (II) describes the workflow of WSI representation self-supervised learning with PAMA, (III) is the structure of the positionaware cross-attention (PACA) module which is the core of the encoder and decoder, and (IV) shows the kernel reorientation (KRO) strategy and the detailed process is described in algorithm 1.</figDesc><graphic coords="3,44,79,218,42,334,51,269,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Semi-supervised experiments with 10%, 35%, 60% and 85% of labelled data on the Endometrial dataset. Solid lines represent fine-tuning results and dotted lines represent liner probing results.</figDesc><graphic coords="7,68,79,54,32,286,60,135,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study on 35% of labelled Endometrial dataset.</figDesc><table><row><cell cols="2">NO. Dis Polar KRO AUC</cell><cell>ACC</cell></row><row><cell>1</cell><cell>0.829</cell><cell>43.38</cell></row><row><cell>2</cell><cell cols="2">0.809 (â†“0.020) 39.52 (â†“3.86)</cell></row><row><cell>3</cell><cell cols="2">0.808 (â†“0.021) 40.82 (â†“2.56)</cell></row><row><cell>4</cell><cell cols="2">0.810 (â†“0.019) 39.76 (â†“3.62)</cell></row><row><cell>5</cell><cell cols="2">0.795 (â†“0.034) 36.33 (â†“7.05)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with weakly-supervised MIL and slide-level self-learning study on the two datasets for sub-type classification.</figDesc><table><row><cell>Methods</cell><cell>TCGA-Lung</cell><cell></cell><cell>Endometrial</cell></row><row><cell></cell><cell>35%</cell><cell>100%</cell><cell>35%</cell><cell>100%</cell></row><row><cell></cell><cell cols="4">AUC ACC AUC ACC AUC ACC AUC ACC</cell></row><row><cell>DSMIL [12]</cell><cell cols="4">0.911 75.00 0.938 80.11 0.761 38.21 0.786 39.32</cell></row><row><cell cols="5">TransMIL [13] 0.932 79.62 0.959 84.35 0.783 38.43 0.798 40.01</cell></row><row><cell cols="5">SETMIL [18] 0.937 80.21 0.962 84.95 0.795 38.71 0.831 40.84</cell></row><row><cell>KAT [19]</cell><cell cols="4">0.951 83.37 0.965 85.81 0.799 38.89 0.835 41.93</cell></row><row><cell>HIPT [5]</cell><cell cols="4">0.967 84.23 0.977 87.83 0.804 38.69 0.842 40.63</cell></row><row><cell>MAE [7]</cell><cell cols="4">0.965 83.90 0.970 87.50 0.801 38.87 0.832 41.95</cell></row><row><cell>MAE+</cell><cell cols="4">0.969 85.07 0.981 88.25 0.811 37.91 0.845 42.85</cell></row><row><cell>PAMA</cell><cell cols="4">0.982 90.84 0.988 92.48 0.829 43.38 0.851 43.64</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partly supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">62171007</rs>, <rs type="grantNumber">61901018</rs>, and <rs type="grantNumber">61906058</rs>), and partly supported by the <rs type="funder">Fundamental Research Funds for the Central Universities of China</rs> (grant No. <rs type="grantNumber">JZ2022HGTB0285</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QjpebzS">
					<idno type="grant-number">62171007</idno>
				</org>
				<org type="funding" xml:id="_6dapejD">
					<idno type="grant-number">61901018</idno>
				</org>
				<org type="funding" xml:id="_vkQ6tYg">
					<idno type="grant-number">61906058</idno>
				</org>
				<org type="funding" xml:id="_ZdPVTQg">
					<idno type="grant-number">JZ2022HGTB0285</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 69.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Robust and efficient medical imaging with self-supervision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09723</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Campanella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and scalable search of whole-slide images via self-supervised deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Schaumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1420" to="1434" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to gigapixel images via hierarchical self-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Trister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16144" to="16155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integration of patch features through self-supervised learning and transformer for survival analysis on whole slide images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integrating context for superior cancer prognosis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-path: self-supervision for classification of pathology images with limited annotations</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Koohbanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Khurram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2845" to="2856" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transmil: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention2majority: weak multiple instance learning for regenerative kidney grading on whole slide images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Tavolara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carreno-Galeano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gurcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102462</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph deep learning for the characterization of tumour microenvironments from spatial protein profiles in tissue specimens</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised visual representation learning for histopathological images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-35" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prototypical multiple instance learning for predicting lymph node metastasis of breast cancer from whole-slide pathological images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="page">102748</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Setmil: spatial encoding transformer-based multiple instance learning for pathological image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-77" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kernel attention transformer (KAT) for histopathology whole slide image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<title level="s">LNCS</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 18-22, 2022</date>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-728" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
