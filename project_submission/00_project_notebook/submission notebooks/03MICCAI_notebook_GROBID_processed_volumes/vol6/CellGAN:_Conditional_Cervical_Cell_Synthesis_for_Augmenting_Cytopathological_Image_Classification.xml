<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification</title>
				<funder ref="#_MdKQe8f">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenrong</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lichi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<email>qianwang@shanghaitech.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="487" to="496"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">357CD3B19EC6AD6558AAAB22099EE49A</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_47</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conditional Image Synthesis</term>
					<term>Generative Adversarial Network</term>
					<term>Cytopathological Image Classification</term>
					<term>Data Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic examination of thin-prep cytologic test (TCT) slides can assist pathologists in finding cervical abnormality for accurate and efficient cancer screening. Current solutions mostly need to localize suspicious cells and classify abnormality based on local patches, concerning the fact that whole slide images of TCT are extremely large. It thus requires many annotations of normal and abnormal cervical cells, to supervise the training of the patch-level classifier for promising performance. In this paper, we propose CellGAN to synthesize cytopathological images of various cervical cell types for augmenting patch-level cell classification. Built upon a lightweight backbone, CellGAN is equipped with a non-linear class mapping network to effectively incorporate cell type information into image generation. We also propose the Skip-layer Global Context module to model the complex spatial relationship of the cells, and attain high fidelity of the synthesized images through adversarial learning. Our experiments demonstrate that CellGAN can produce visually plausible TCT cytopathological images for different cell types. We also validate the effectiveness of using CellGAN to greatly augment patch-level cell classification performance. Our code and model checkpoint are available at https://github.com/ZhenrongShen/CellGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cervical cancer accounts for 6.6% of the total cancer deaths in females worldwide, making it a global threat to healthcare <ref type="bibr" target="#b5">[6]</ref>. Early cytology screening is highly effective for the prevention and timely treatment of cervical cancer <ref type="bibr" target="#b22">[23]</ref>.</p><p>Nowadays, thin-prep cytologic test (TCT) <ref type="bibr" target="#b0">[1]</ref> is widely used to screen cervical cancers according to the Bethesda system (TBS) rules <ref type="bibr" target="#b20">[21]</ref>. Typically there are five types of cervical squamous cells under TCT examinations <ref type="bibr" target="#b4">[5]</ref>, including normal class or negative for intraepithelial malignancy (NILM), atypical squamous cells of undetermined significance (ASC-US), low-grade squamous intraepithelial lesion (LSIL), atypical squamous cells that cannot exclude HSIL (ASC-H), and high-grade squamous intraepithelial lesion (HSIL). The NILM cells have no cytological abnormalities while the others are manifestations of cervical abnormality to a different extent. By observing cellular features (e.g., nucleus-cytoplasm ratio) and judging cell types, pathologists can provide a diagnosis that is critical to the clinical management of cervical abnormality.</p><p>After scanning whole-slide images (WSIs) from TCT samples, automatic TCT screening is highly desired due to the large population versus the limited number of pathologists. As the WSI data per sample has a huge size, the idea of identifying abnormal cells in a hierarchical manner has been proposed and investigated by several studies using deep learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. In general, these solutions start with the extraction of suspicious cell patches and then conduct patch-level classification. The promising performance of cell classification at the patch level is critical, which contributes to sample-level diagnosis after integrating outcomes from many patches in a WSI. However, such a patchlevel classification task requires a large number of annotated training data. And the efforts in collecting reliably annotated data can hardly be negligible, which requires high expertise due to the intrinsic difficulty of visually reading WSIs.</p><p>To alleviate the shortage of sufficient data to supervise classification, one may adopt traditional data augmentation techniques, which yet may bring little improvement due to scarcely expanded data diversity <ref type="bibr" target="#b25">[26]</ref>. Thus, synthesizing cytopathological images for cervical cells is highly desired to effectively augment training data. Existing literature on pathological image synthesis has explored the generation of histopathological images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. In cytopathological images, on the contrary, cervical cells can be spatially isolated from each other, or are highly squeezed and even overlapped. The spatial relationship of individual cells is complex, adding diversity to the image appearance of color, morphology, texture, etc. In addition, the differences between cell types are mainly related to nuanced cellular attributes, thus requiring fine granularity in modulating synthesized images toward the expected cell types. Therefore, the task to synthesize realistic cytopathological images becomes very challenging.</p><p>Aiming at augmenting the performance of cervical abnormality screening, we develop a novel conditional generative adversarial network in this paper, namely CellGAN, to synthesize cytopathological images for various cell types. We leverage FastGAN <ref type="bibr" target="#b15">[16]</ref> as the backbone for the sake of training stability and computational efficiency. To inject cell type for fine-grained conditioning, a non-linear mapping network embeds the class labels to perform layer-wise feature modulation in the generator. Meanwhile, we introduce the Skip-layer Global Context (SGC) module to capture the long-range dependency of cells for precisely modeling their spatial relationship. We adopt an adversarial learning scheme, where the discriminator is modified in a projection-based way <ref type="bibr" target="#b19">[20]</ref> for matching condi- tional data distribution. To the best of our knowledge, our proposed CellGAN is the first generative model with the capability to synthesize realistic cytopathological images for various cervical cell types. The experimental results validate the visual plausibility of CellGAN synthesized images, as well as demonstrate their data augmentation effectiveness on patch-level cell classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The dilemma of medical image synthesis lies in the conflict between the limited availability of medical image data and the high demand for data amount to train reliable generative models. To ensure the synthesized image quality given relatively limited training samples, the proposed CellGAN is built upon FastGAN <ref type="bibr" target="#b15">[16]</ref> towards stabilized and fast training for few-shot image synthesis. By working in a class-conditional manner, CellGAN can explicitly control the cervical squamous cell types in the synthesized cytopathological images, which is critical to augment the downstream classification task. The overall architecture of CellGAN is presented in Fig. <ref type="figure" target="#fig_0">1</ref>, and more detailed structures of the key components are displayed in Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture of the Generator</head><p>The generator of CellGAN has two input vectors. The first input of the class label y, which adopts one-hot encoding, provides class-conditional information to indicate the expected cervical cell type in the synthesized image I syn . The second input of the 128-dimensional latent vector z represents the remaining image information, from which I syn is gradually expanded. We stack six UpBlocks to form the main branch of the generator.</p><p>To inject cell class label y into each UpBlock, we follow a similar design to StyleGAN <ref type="bibr" target="#b12">[13]</ref>. Specifically, the class label y is first projected to a class embedding c via a non-linear mapping network, which is implemented using four groups of fully connected layers and LeakyReLU activations. We set the dimensions of class embedding c to the same as the latent vector z. Then, we pass c through learnable affine transformations, such that the class embedding is specialized to the scaling and bias parameters controlling Adaptive Instance Normalization (AdaIN) <ref type="bibr" target="#b12">[13]</ref> in each UpBlock. The motivation for the design above comes from our hypothesis that the class-conditional information mainly encodes cellular attributes related to cell types, rather than common image appearance. Therefore, by modulating the feature maps at multiple scales, the input class label can better control the generation of cellular attributes.</p><p>We further introduce the Skip-layer Global Context (SGC) module into the generator (see Fig. <ref type="figure" target="#fig_1">2</ref> in Supplementary Materials), to better handle the diversity of the spatial relationship of the cells. Our SGC module reformulates the idea of GCNet <ref type="bibr" target="#b3">[4]</ref> with the design of SLE module from FastGAN <ref type="bibr" target="#b15">[16]</ref>. It first performs global context modeling on the low-resolution feature maps, then transforms global context to capture channel-wise dependency, and finally merges the transformed features into high-resolution feature maps. In this way, the proposed SGC module learns a global understanding of the cell-to-cell spatial relationship and injects it into image generation via computationally efficient modeling of long-range dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discriminator and Adversarial Training</head><p>In an adversarial training setting, the discriminator forces the generator to faithfully match the conditional data distribution of real cervical cytopathological images, thus prompting the generator to produce visually and semantically realistic images. For training stability, the discriminator is trained as a feature encoder with two extra decoders. In particular, five ResNet-like <ref type="bibr" target="#b6">[7]</ref> Down-Blocks are employed to convert the input image into an 8 × 8 × 512 feature map. Two simple decoders reconstruct downscaled and randomly cropped versions of input images I crop and I resize from 8 2 and 16 2 feature maps, respectively. These decoders are optimized together with the discriminator by using a reconstruction loss L recon that is represented below:</p><formula xml:id="formula_0">L recon = E f ∼Dis(x),x∼I real Dec(f ) -T (x) 1 ,<label>(1)</label></formula><p>where T denotes the image processing (i.e., 1 2 downsampling and 1 4 random cropping) on real image I real , f is the processed intermediate feature map from the discriminator Dis, and Dec stands for the reconstruction decoder. This simple self-supervised technique provides a strong regularization in forcing the discriminator to extract a good image representation.</p><p>To provide more detailed feedback from the discriminator, PatchGAN <ref type="bibr" target="#b11">[12]</ref> architecture is adopted to output an 8 × 8 logit map by using a 1 × 1 convolution on the last feature map. By penalizing image content at the scale of patches, the color fidelity of synthesized images is guaranteed as illustrated in our ablation study (see Fig. <ref type="figure" target="#fig_2">3</ref>). To align the class-conditional fake and real data distributions in the adversarial setting, the discriminator directly incorporates class labels as additional inputs in the manner of projection discriminator <ref type="bibr" target="#b19">[20]</ref>. The class label is projected to a learned 512-dimensional class embedding and takes innerproduct at every spatial position of the 8 × 8 × 512 feature map. The resulting 8×8 feature map is then added to the aforementioned 8×8 logit map, composing the final output of the discriminator.</p><p>For the objective function, we use the hinge version <ref type="bibr" target="#b14">[15]</ref> of the standard adversarial loss L adv . We also employ R 1 regularization L reg <ref type="bibr" target="#b16">[17]</ref> as a slight gradient penalty for the discriminator. Combining all the loss functions above, the total objective L total to train the proposed CellGAN in an adversarial manner can be expressed as:</p><formula xml:id="formula_1">L total = L adv + L recon + λ reg L reg ,<label>(2)</label></formula><p>where λ reg is empirically set to 0.01 in our experiments.</p><p>3 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experimental Setup</head><p>Dataset. In this study, we collect 14,477 images with 256 × 256 pixels from three collaborative clinical centers. All the images are manually inspected to contain different cervical squamous cell types. In total, there are 7,662 NILM, 2,275 ASC-US, 2,480 LSIL, 1,638 ASC-H, and 422 HSIL images. All the 256×256 images with their class labels are selected as the training data.</p><p>Implementation Details. We use the learning rate of 2.5 × 10 -4 , batch size of 64, and Adam optimizer <ref type="bibr" target="#b13">[14]</ref> to train both the generator and the discriminator for 100k iterations. Spectral normalization <ref type="bibr" target="#b18">[19]</ref>, differentiable augmentation <ref type="bibr" target="#b29">[30]</ref> and exponential-moving-average optimization <ref type="bibr" target="#b28">[29]</ref> are included in the training process. Fréchet Inception Distance (FID) <ref type="bibr" target="#b7">[8]</ref> is used to measure the overall semantic realism of the synthesized images. All the experiments are conducted using an NVIDIA GeForce RTX 3090 GPU with PyTorch <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of Image Synthesis Quality</head><p>We compare CellGAN with the state-of-the-art generative models for classconditional image synthesis, i.e., BigGAN <ref type="bibr" target="#b1">[2]</ref> from cGANs <ref type="bibr" target="#b17">[18]</ref> and Latent Diffusion Model (LDM) <ref type="bibr" target="#b24">[25]</ref> from diffusion models <ref type="bibr" target="#b8">[9]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, BigGAN  To verify the effects of key components in the proposed CellGAN, we conduct an ablation study on four model settings in Table <ref type="table">2</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>. We denote the models in Fig. <ref type="figure" target="#fig_2">3</ref> from left to right as Model i, Model ii, Model iii, and CellGAN. The visual results of Model i suffer from severe color distortions while the other models do not, indicating that the PatchGAN-based discriminator can guarantee color fidelity by patch-level image content penalty.  The abnormal cells generated by Model i and Model ii tend to have highly similar cellular features. In contrast, Model iii and CellGAN can accurately capture the morphological characteristics of different cell types. This phenomenon suggests that the implementation of the class mapping network facilitates more distinguishable feature representations for different cell types. By comparing the synthesized images from Model iii with CellGAN, it is observed that adopting SGC modules can yield more clear cell boundaries, which demonstrates the capability of SGC module in modeling complicated cell-to-cell relationships in image space. The quantitative results further state the effects of the components above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation of Augmentation Effectiveness</head><p>To validate the data augmentation capacity of the proposed CellGAN, we conduct 5-fold cross-validations on the cell classification performances of two classi- In each fold, one group is selected as the testing data while the other four are used for training. For different data settings, we synthesize 2,000 images for each cell type using the corresponding generative method, and add them to the training data of each fold. We use the learning rate of 1.0 × 10 -4 , batch size of 64, and SGD optimizer <ref type="bibr" target="#b23">[24]</ref> to train all the classifiers for 30 epochs. Random flip is applied to all data settings since it is reasonable to use traditional data augmentation techniques simultaneously in practice.</p><p>The experimental accuracy, precision, recall, and F1 score are listed in Table <ref type="table" target="#tab_2">3</ref>. It is shown that both the classifiers achieve the best scores in all metrics using the additional synthesized data from CellGAN. Compared with the baselines, the accuracy values of ResNet-34 and DenseNet-121 are improved by 5.25% and 4.05%, respectively. Meanwhile, the scores of other metrics are all improved by more than 4%, indicating that our synthesized data can significantly enhance the overall classification performance. Thanks to the visually plausible and semantically realistic synthesized data, CellGAN is conducive to the improvement of cell classification, thus serving as an efficient tool for augmenting automatic abnormal cervical cell screening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>In this paper, we propose CellGAN for class-conditional cytopathological image synthesis of different cervical cell types. Built upon FastGAN for training stability and computational efficiency, incorporating class-conditional information of cell types via non-linear mapping can better represent distinguishable cellular features. The proposed SGC module provides the global contexts of cell spatial relationships by capturing long-range dependencies. We have also found that the PatchGAN-based discriminator can prevent potential color distortion. Qualitative and quantitative experiments validate the semantic realism as well as the data augmentation effectiveness of the synthesized images from CellGAN.</p><p>Meanwhile, our current CellGAN still has several limitations. First, we cannot explicitly control the detailed attributes of the synthesized cell type, e.g., nucleus size, and nucleus-cytoplasm ratio. Second, in this paper, the synthesized image size is limited to 256×256. It is worth conducting more studies for expanding synthesized image size to contain much more cells, such that the potential applications can be extended to other clinical scenes (e.g., interactively training pathologists) in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall architecture of the proposed CellGAN. The numbers in the center and the bottom right corner of each square indicate the feature map size and the channel number, respectively.</figDesc><graphic coords="3,55,98,54,14,340,18,231,55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative comparison between state-of-the-art generative models and the proposed CellGAN. Different rows stand for different cervical squamous cell types.</figDesc><graphic coords="6,41,79,54,53,340,21,193,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Generated images from ablation study of the following key components: (a) PatchGAN architecture, (b) class mapping network, (c) SGC module. Table 2. Quantitative ablation study of the following key component: (a) PatchGAN architecture, (b) class mapping network, (c) SGC module. (↓: Lower is better).</figDesc><graphic coords="7,55,98,54,26,340,18,193,45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitativecannot generate individual cells with clearly defined cell boundaries. And it also fails to capture the morphological features of HSIL cells that are relatively limited in training data quantity. LDM only yields half-baked cell structures since the generated cells are mixed, and there exists negligible class separability among abnormal cell types. On the contrary, our proposed CellGAN is able to synthesize visually plausible cervical cells and accurately model distinguishable cellular features for each cell type. The quantitative comparison by FID in Table1also demonstrates the superiority of CellGAN in synthesized image quality.</figDesc><table><row><cell>Method</cell><cell>FID↓</cell><cell></cell><cell></cell></row><row><cell></cell><cell>NILM</cell><cell>ASC-US LSIL</cell><cell>ASC-H HSIL</cell><cell>Mean</cell></row><row><cell>BigGAN</cell><cell cols="4">29.5076 37.9543 35.5058 48.0228 85.6230 47.3227</cell></row><row><cell>LDM</cell><cell cols="4">53.4307 56.1689 49.0969 59.6406 84.9522 60.6579</cell></row><row><cell cols="5">CellGAN(Ours) 26.0135 33.5718 33.3401 46.2965 68.3458 41.5136</cell></row></table><note><p>comparison between state-of-the-art generative models and the proposed CellGAN (↓: Lower is better).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Data augmentation comparison between the proposed CellGAN and other synthesis-based methods (↑: Higher is better).</figDesc><table><row><cell cols="2">Classifier Method</cell><cell>Accuracy↑ Precision↑ Recall↑</cell><cell>F1-Score↑</cell></row><row><cell>ResNet</cell><cell>baseline</cell><cell cols="2">74.30±1.69 68.00±1.79 70.94±2.28 68.88±1.78</cell></row><row><cell></cell><cell cols="3">+ BigGAN 76.30±2.80 72.96±2.58 75.11±2.20 73.89±2.44</cell></row><row><cell></cell><cell>+ LDM</cell><cell cols="2">75.80±1.12 71.14±0.72 73.89±1.36 72.29±0.91</cell></row><row><cell></cell><cell cols="3">+ CellGAN 79.55±1.20 74.88±1.60 75.42±1.74 74.70±1.79</cell></row><row><cell cols="2">DenseNet baseline</cell><cell cols="2">72.10±0.66 65.23±1.17 68.28±1.26 66.33±1.25</cell></row><row><cell></cell><cell cols="3">+ BigGAN 75.40±1.73 68.47±1.78 70.13±1.21 68.94±1.97</cell></row><row><cell></cell><cell>+ LDM</cell><cell cols="2">74.95±1.94 68.03±2.11 69.32±1.65 68.55±2.37</cell></row><row><cell></cell><cell cols="3">+ CellGAN 76.15±1.38 70.37±1.52 72.42±1.95 70.99±1.75</cell></row><row><cell cols="4">fiers (ResNet-34 [7] and DenseNet-121 [11]) using four training data settings for</cell></row><row><cell cols="4">comparison: (1) real data only (the baseline); (2) baseline + BigGAN synthe-</cell></row><row><cell cols="4">sized images; (3) baseline + LDM synthesized images; (4) baseline + CellGAN</cell></row><row><cell cols="4">synthesized images. For each cell type, we randomly select 400 real images and</cell></row><row><cell cols="2">divide them into 5 groups.</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62001292</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MdKQe8f">
					<idno type="grant-number">62001292</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_47.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance of ThinPrep liquid-based cervical cytology in comparison with conventionally prepared Papanicolaou smears: a quantitative survey</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abulafia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pezzullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Sherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gynecol. Oncol</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="144" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel attention-guided convolutional network for the detection of abnormal cervical cells in cervical cancer screening</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102197</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GCNet: non-local networks meet squeezeexcitation networks and beyond</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Atypical squamous cells of undetermined significance: interlaboratory comparison and quality assurance monitors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Davey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naryshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Kline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagn. Cytopathol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="390" to="396" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">World health organization call for action to eliminate cervical cancer globally</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gultekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Broutet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hutubessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Gynecol. Cancer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="426" to="427" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust histopathology image analysis: to label or to synthesize?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8533" to="8542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<title level="m">Geometric GAN</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards faster and stabilized GAN training for high-fidelity few-shot image synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3481" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wilbur</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-11074-5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-11074-5" />
		<title level="m">The Bethesda System for Reporting Cervical Cytology: Definitions, Criteria, and Explanatory Notes</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cervical pap smear study and its utility in cancer screening, to specify the strategy for cervical cancer control</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natl. J. Community Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="49" to="51" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A novel automationassisted cervical cancer reading method based on convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biocybernetics Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="611" to="623" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selective synthetic augmentation with histoGAN for improved histopathology image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101816</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The unusual effectiveness of averaging in GAN training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yazici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piliouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<editor>ICLR (Poster</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Differentiable augmentation for dataefficient GAN training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7559" to="7570" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical pathology screening for cervical abnormality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">101892</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
