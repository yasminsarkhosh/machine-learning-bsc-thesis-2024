<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair</title>
				<funder ref="#_GfXQAV7">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhonghang</forename><surname>Zhu</surname></persName>
							<email>zzhonghang@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science at School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qichang</forename><surname>Chen</surname></persName>
							<email>qcchen@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science at School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lequan</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lianxin</forename><surname>Wang</surname></persName>
							<email>lswang@xmu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Orthopedics</orgName>
								<orgName type="institution">The First Affiliated Hospital of Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Defu</forename><surname>Zhang</surname></persName>
							<email>dfzhang@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science at School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baptiste</forename><surname>Magnier</surname></persName>
							<email>baptiste.magnier@mines-ales.fr</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Euromov Digital Health in Motion</orgName>
								<orgName type="institution" key="instit2">University Montpellier</orgName>
								<orgName type="institution" key="instit3">IMT Mines Ales</orgName>
								<address>
									<settlement>Ales</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science at School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="444" to="453"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">07FC5FD818DE62931C9C20A03DD67157</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hip fracture diagnosis</term>
					<term>X-ray image</term>
					<term>Deformable transformer</term>
					<term>Cross-view correspondence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hip fractures are a common cause of morbidity and mortality and are usually diagnosed from the X-ray images in clinical routine. Deep learning has achieved promising progress for automatic hip fracture detection. However, for fractures where displacement appears not obvious (i.e., non-displaced fracture), the single-view X-ray image can only provide limited diagnostic information and integrating features from crossview X-ray images (i.e., Frontal/Lateral-view) is needed for an accurate diagnosis. Nevertheless, it remains a technically challenging task to find reliable and discriminative cross-view representations for automatic diagnosis. First, it is difficult to locate discriminative task-related features in each X-ray view due to the weak supervision of image-level classification labels. Second, it is hard to extract reliable complementary information between different X-ray views as there is a displacement between them. To address the above challenges, this paper presents a novel crossview deformable transformer framework to model relations of critical representations between different views for non-displaced hip fracture identification. Specifically, we adopt a deformable self-attention module to localize discriminative task-related features for each X-ray view only with the image-level label. Moreover, the located discriminative features are further adopted to explore correlated representations across views by taking advantage of the query of the dominated view as guidance. Furthermore, we build a dataset including 768 hip cases, in which each case has paired hip X-ray images (Frontal/Lateral-view), to evaluate our</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hip fractures represent a life-changing event and carry a substantial risk of decreased functional status and death, especially in elderly patients <ref type="bibr" target="#b16">[17]</ref>. Usually, they are diagnosed from X-ray images in clinical practice. Currently, proper X-ray fracture identification relies on the manual observation of board-certified radiologists, which leads to increased workload pressures to radiologists. However, accurate and timely diagnosis of hip fractures is critical, especially in emergency situations such as non-displaced hip fractures <ref type="bibr" target="#b12">[13]</ref>. Therefore, automated X-ray image classification is of great significance to support the clinical assistant diagnosis.</p><p>Recently, Deep Learning (DL) methods for radiography analysis have gained popularity and shown promising results <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>, which aims to distinguish normal radiography or prioritize urgent/critical cases with the goal of reducing the radiologist workload or improving the reporting time. For example, a triaging pipeline based on the urgency of exams has been proposed in <ref type="bibr" target="#b0">[1]</ref> and Tang et al. <ref type="bibr" target="#b20">[21]</ref> compared different DL models applied to several public chest radiography datasets for distinguishing abnormal cases. However, these works only focus on single-view radiography analysis. When the fracture displacement in the X-ray image is not apparent,i.e., a non-displaced hip fracture as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, these methods may fail in extracting enough fracture features represented by a ridge <ref type="bibr" target="#b19">[20]</ref> in the image and result in misdiagnosis. Therefore, it is necessary to develop cross-view learning approaches to diagnose fracture from paired views (Frontal/Lateral-images), which have been demonstrated to provide complementary features to promote the diagnostic performance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>. Recent studies have been investigated for cross-view learning of X-ray images, which aims to exploit the value of paired X-ray images and fuse them to get a comprehensive anatomical representation for diagnosis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. However, these methods do not consider cross-view feature relations which is a quite important issue for accurate cross-view feature fusion.</p><p>Since the introduction of vision transformer models <ref type="bibr" target="#b8">[9]</ref>, more researches have been developed in the tokenization process and relation modeling among tokens in an image <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. Recently, deformable self-attention has been proposed to refine visual tokens <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>, which is powerful in focusing on relevant regions and capturing more informative features. Motivated by this, we propose a novel cross-view deformable transformer framework for hip fracture detection from cross-view X-ray images. Firstly, deformable self-attention modules are utilized to localize reliable task-related features of each view. Secondly, the dominatedview characteristics are used to explore informative representations in the other view for effective feature fusion of cross-view X-ray images. Specifically, our contributions are three folds:</p><p>1. We propose a cross-view deformable transformer framework for non-displaced hip fracture classification, in which we take advantage of discriminative features of Frontal-view as a guidance to localize informative representations of Lateral-view for cross-view feature fusion. 2. For each view, we adopt the deformable self-attention module to select pivotal tokens in a data-dependent way. 3. We build a new non-displaced hip fracture X-ray dataset which includes both Frontal and Lateral views for each case to valid the proposed method. Our approach surpasses the state of the art in accuracy by over 1.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>The detailed architecture of the proposed cross-view deformable attention framework is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. To model the relations among features across different views, the framework is designed as a joint of two view-specific deformable transformer branches with four stages. For each view-specific branch, the input image is firstly processed by shifted window attention modules presented in the left of Fig. <ref type="figure" target="#fig_1">2</ref> to aggregate information locally, followed by the last two stages to model the global relations among the locally augmented tokens with deformable self-attention modules. In the last two stages, the query features of the Frontalview are adopted as the guidance to detect the relations among the Lateral-view tokens. The detailed design of each component is introduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">View-Specific Deformable Transformer Network</head><p>To discover the task-related regions of each view, the view-specific branch is designed as a deformable transformer network consisting of four stages. In each branch, the first two stages explore the local representations of the input images with shift-window attention modules, followed by the last two stages exploit local tokens relation using deformable self-attention modules. Specifically, our framework takes an image of size H×W ×3 as input. After the first two stages, the input image will be embedded into feature maps</p><formula xml:id="formula_0">f layer3 ∈ H/4 × W/4 × C,</formula><p>where the C denotes the channel number. The f layer3 will be passed to a query projection network W q , which is a light network to obtain the query feature maps f layer3;q . Moreover, a uniform grid p original ∈ R H/4×W/4×2 is generated as a position reference of points in f layer3 . The values of p original are linearly spaced and normalized to 2D coordinates range in</p><formula xml:id="formula_1">(-1, -1), • • • , (1, 1), in which the (-1, •), • • • , (1, •) and the (•, -1), • • • , (•, 1</formula><p>) refers to the horizontal and vertical coordinates for reference points respectively. In the meanwhile, reference points offset p offsets ∈ R H/4×W/4×2 are generated from the f layer3;q by a light offset network consisted of two convolutional layers followed normalization layer, which are also normalized into (-4/H, -4/W ), ). Then the deformed features of each point are sampled at the shifted position, which could be denoted as f = S(f, p), where S represents a bilinear interpolation function. Therefore, the deformed multi-head self-attention module with M heads can be described as:</p><formula xml:id="formula_2">q = fW q , k = fW k , v = fW v , (<label>1</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">z m = σ(q (m) k(m)T / √ d)v (m) , m = 1, • • • , M,<label>(2)</label></formula><formula xml:id="formula_5">z = Concat z 1 , • • • , z M W o ,<label>(3)</label></formula><p>where σ(•) denotes the softmax function, and d is the dimension of each head. z (m) is the embedding output from the m-th attention head, and {q (m) , k(m) , v(m) } ∈ R N ×d represents query, deformed key and value embeddings, respectively. Also, W q , W k , W v , W o are the projection networks. Features passed to the 4th stage are conducted a same operation as in 3rd stage with different feature dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-View Deformable Transformer Framework</head><p>The proposed cross-view framework is consisted of two joint view-specific branches, with a pair of X-ray images (Frontal-view and Lateral-view) from the same patient taken as the input of two individual view-specific branches, respectively. These input images will be embedded into primary representations in the first stage of view-specific network, then these primary features will be sent to the second stage to get representations with larger receptive field. To observe correlations between Frontal-view and Lateral-view, we opt for a simple solution to share queries from the Frontal-view to model token relations of Lateral-view in a self-attention manner as the Frontal-view contains dominated diagnosis features <ref type="bibr" target="#b23">[24]</ref>. In this way, the focused regions of the Lateral-view are determined by the discriminative features of the Frontal-view. So for the Lateral-view branch, the multi-head self-attention can be denoted as:</p><formula xml:id="formula_6">q fr = f fr W q;fr , kla = fla W k;la , vla = fla W v;la , (<label>4</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">z m la = σ(q (m) fr kla (m)T / √ d) vla (m) , m = 1, • • • , M,<label>(5)</label></formula><formula xml:id="formula_9">z la = Concat z 1 la , • • • , z M la W o;la ,<label>(6)</label></formula><p>in which (•) fr and (•) la represent the features of Frontal-view and Lateral-view, respectively. While for the Frontal-view branch, the multi-head self-attention can be denoted as:</p><formula xml:id="formula_10">q fr = f fr W q;fr , k fr = f fr W k;fr , v fr = f fr W v;fr , (<label>7</label></formula><formula xml:id="formula_11">)</formula><formula xml:id="formula_12">z m fr = σ(q (m) fr k fr (m)T / √ d) v fr (m) , m = 1, • • • , M, (<label>8</label></formula><formula xml:id="formula_13">)</formula><formula xml:id="formula_14">z fr = Concat z 1 fr , • • • , z M fr W o;fr ,<label>(9)</label></formula><p>It is worth noting that view-specific reference points offset are derived from corresponding view-specific query feature maps which contain global view-specific position relations. By taking query feature maps from the Frontal-view as an informative clue, it makes sense to search relevant task-related features in the Lateral-view deformed values and keys embedding which are also discriminative features of Lateral-view. In this way, the cross-view transformer framework manages to localize task-related features in both views while exploring the crossview related representations for feature aggregation. Then the final output can be denoted as outputs =MLP(Concat(f fr , f la )), where the f fr and f la represent the output features of the last layer of the Frontal-view and Lateral-view branches, respectively. The Concat is a concatenation operation and MLP is a projection head consisted of two fully connected layers to generate logit predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Technical Details</head><p>The view-specific model shares a similar pyramid structure with DTA-T <ref type="bibr" target="#b22">[23]</ref>.</p><p>The first stage consists of one shift-window block whose head number is set as 3, followed the second stage with one shift-window block whose head number is set as 6. We adopt three deformable attention block with 12 heads in the 3rd stage and one deformable attention block with 24 heads in the 4th stage. To optimize the whole framework, we calculate the cross entropy loss between the label and final output of the cross-view deformable transformer for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Setup</head><p>Dataset. The dataset used in this study includes 768 paired hip X-ray images (329 non-displaced fractures, 439 normal hips) from 4 different manufacturers of radiologic data sources: GE Healthcare, Philips Medical Systems, Kodak and Canon. All the hip radiographs are collected and labeled by experts with nondisplaced fractures or normal for classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details.</head><p>For experiments of our dataset, we manually locate the hip region and crop a 224 × 224 image that is centered on the original hip region whose size is 400 × 600. The learning rate is set as 3e-3 for the endto-end training of the framework with a batch size of 32. We adopt a 10-fold cross-validation and report the average performance of 10 folds. For each fold, we further divide the data (the other 9 folds) into a training set (90%) and a validation set (10%) and take the best model on the validation part for testing.</p><p>Evaluation Metric. We evaluate our method with Accuracy (Acc), Precision, Recall and F1 score. The Precision and Recall are calculated with one-classversus-all-other-classes and then calculate F1 score F 1 = 2•P recision•Recall P recision+Recall .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>Comparisons with the State of the Art. The proposed method is also compared with other cross-view fusion methods; results are reported in Table <ref type="table" target="#tab_1">1</ref>. 1) MVC-NET: a network with back projection transposition branch to explicitly incorporate the spatial information from two views at the feature level.</p><p>2) DualNet: an ensemble of two DenseNet-121 <ref type="bibr" target="#b11">[12]</ref> networks followed a global average pooling operation of the final convolutional layer before a fully connected layer to simultaneously process multi-view images. 3) Auloss: a DualNet regularized by auxiliary view-specific classification losses. 4) ResNet18-dual: an ensemble of two ResNet18 <ref type="bibr" target="#b10">[11]</ref> networks, and the predicted results are generated by concatenating logit outputs from each ResNet18 network. 5) Densenet-dual: an ensemble of two DenseNet networks, and the predicted results are generated by concatenating logit outputs from each DenseNet network. 6) Swin-dual: an ensemble of two swin-transformer networks <ref type="bibr" target="#b15">[16]</ref>, and the predicted results are generated by concatenating logit outputs from each swin-transformer network.</p><p>As shown in Table <ref type="table" target="#tab_1">1</ref>, we compare our method to different dual frameworks. It can be observed that the proposed method achieves better performance than others (compare Ours with ResNet18-dual, Densenet-dual and Swin-dual), which demonstrates that the accuracy boost is due to the deformable transformer network and the feature interaction design not the increased backbone size. In addition, the MVC-NET shares a similar feature-level interaction motivation with Ours, and the 19.1% accuracy improvement indicates that our cross-view deformable attention gains better performance. Otherwise, we demonstrate the effectiveness of the deformable transformer network by comparing the Our w/o q to Swin-dual, as the only difference between these two frameworks is that the Our w/o q change the last two stages of Swin-dual to deformable transformer modules.</p><p>Ablation Study. We also conduct ablation experiments to validate the design of our proposed different components. We compare the following different settings. 1) Frontal: take the Frontal image as input of the view-specific deformable transformer network to generate the prediction. 2) Frontal swin: take the Frontal image as input of the swin-transformer network to generate the prediction.</p><p>3) Lateral: take the Lateral image as input of the view-specific deformable transformer network to generate the prediction. 4) Lateral swin: take the Lateral image as input of the swin-transformer network to generate the prediction. 5) Ours w/o q: the proposed framework without cross-view deformable attention.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows the ablation results. It is observed from Ours w/o q and Ours that the proposed method improves the performance of classification by adopting the proposed cross-view deformable attention, which demonstrates that the query of Frontal-view has a positive effect on mining the discrimination features of Lateral representations. Especially, cross-view learning contributes a minimum accuracy improvement of 3% compared Ours to Frontal and Lateral as discriminative features between different views can be complementary. Moreover, we  present the performance of different view-specific networks by comparing Frontal to Frontal swin, the results show that the deformable transformer network gains higher accuracy with about 1.7% increment. For the Lateral-view, the deformable transformer network also has comparable performance to Swin-transformer.</p><p>Visualization Results. To verify the effectiveness of the proposed framework, we visualize the interest regions of the model as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. It shows that the model could concentrate on the interested region of the diagnosis as labeled by expert. In addition, for the diagnosis of non-displaced hip fracture, the smoothness of the bone edge is a very important reference. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, our model is also very good at focusing on bone smoothness in the same area from different perspectives in the same patient, indicating that the features from the Frontal view actually have a guidance to feature selection of the Lateral view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper innovatively introduces a cross-view deformable transformer framework for non-displaced hip fracture classification from paired hip X-ray images.</p><p>We adopt the deformable self-attention module to locate the interested regions of each view, while exploring feature relations among Lateral-view with the guidance of Frontal-view characteristics. In addition, the proposed deformable crossview learning method is general and has great potential to boost the performance of detecting other complicated disease. Our future work will focus on more effective training strategies and extend our framework to other cross-view medical image analysis problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparisons of non-displaced/displaced hip fracture and normal hip X-ray images. The fracture regions are marked by green arrows and red arrows for nondisplaced/displaced fractures, respectively. (Color figure online)</figDesc><graphic coords="3,41,79,54,02,340,33,84,70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Illustration of the proposed framework depicted in two view-specific branches with four stages. A pair of X-ray images, i.e., Frontal and Lateral images are fed into two branches, respectively. The input images are processed with shifted window attention modules to aggregate discriminative local features (first two stages), while deformable self-attention modules are utilized to model the relations among tokens (last two stages). Moreover, Frontal queries are passed to model relations among Lateral features for cross-view deformable attention. The p original represents the original feature position of each view, while p of f sets denotes the position offsets. The W q/q , W k/ k and W v/v are projection matrices for queries, keys and values, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization results of different patients. The interest area is annotated by expert with green arrows and rectangles, whereas the highlighted areas show the interest regions of the model. (Color figure online)</figDesc><graphic coords="8,93,48,286,16,296,68,143,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results (mean±standard deviation)% of different methods.</figDesc><table><row><cell>Method</cell><cell>Acc</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 score</cell></row><row><cell cols="5">MVC-NET [26] 77.35±4.81 71.96±10.23 75.69±9.09 73.30±7.66</cell></row><row><cell>DualNet [19]</cell><cell cols="4">80.87±5.98 76.21±10.31 82.37±8.01 78.41±5.49</cell></row><row><cell>Auloss [10]</cell><cell cols="4">82.30±4.51 78.42±8.26 79.69±6.23 78.86±6.15</cell></row><row><cell cols="5">ResNet18-dual 88.41±2.86 95.87±3.27 76.14±8.29 84.58±4.61</cell></row><row><cell cols="5">Densenet-dual 90.36±3.42 94.37±3.11 82.20±7.95 87.66±4.60</cell></row><row><cell>Swin-dual</cell><cell cols="4">95.05±2.01 95.44±5.00 92.85±3.77 94.01±2.68</cell></row><row><cell>Lateral swin</cell><cell cols="4">85.16±3.86 85.36±6.89 78.77±8.35 81.56±5.42</cell></row><row><cell>Lateral</cell><cell cols="4">84.37±4.43 84.51±7.78 78.01±9.47 80.68±6.41</cell></row><row><cell>Frontal swin</cell><cell cols="4">91.67±3.53 94.12±5.22 85.55±8.64 89.33±5.34</cell></row></table><note><p>Frontal 93.36±2.24 93.19±5.34 91.34±3.94 92.11±2.76 Ours w/o q 95.83±1.59 96.10±4.82 93.96±3.57 96.36±1.31 Ours 96.48±1.83 95.65±4.31 96.22±3.69 95.83±2.26</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2019YFE0113900</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GfXQAV7">
					<idno type="grant-number">2019YFE0113900</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Pathology</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated triaging of adult chest radiographs with deep artificial neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annarumma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Withey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Bakewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pesce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="196" to="202" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view probabilistic classification of breast microcalcifications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bekker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shalhon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="645" to="653" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08534</idno>
		<title level="m">Do lateral views help automated chest X-ray predictions? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning for chest X-ray analysis: a survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sogancioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102125</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning models for classifying mammogram exams containing unregistered multi-view images and segmentation maps of lesions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep learning for medical image analysis</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="321" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DPT: deformable patch-based transformer for visual recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2899" to="2907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting COVID-19 pneumonia severity on chest X-ray with deep learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cureus</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cswin transformer: a general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12124" to="12134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quantifying the value of lateral views in deep learning for chest X-rays</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<biblScope unit="page" from="288" to="303" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Laurens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic hip fracture identification and functional subclassification with deep learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Krogue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">190023</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automated assessment of covid-19 pulmonary disease severity on chest radiographs using convolutional siamese neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedRxiv</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2025" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-view correspondence reasoning based on bipartite graph convolutional network for mammogram mass detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3812" to="3822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Advanced deep learning techniques applied to automated femoral neck fracture detection and classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mutasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Varada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasiej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1209" to="1217" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional architectures for multiclass segmentation in chest radiographs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lenis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hladvka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bühler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1865" to="1876" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Large scale automated reading of frontal and lateral chest X-rays using dual convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu-Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07839</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ridge detection by image filtering techniques: a review and an objective analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Shokouh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Image Anal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="551" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated abnormality classification of chest radiographs using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digit. Med</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-view analysis of unregistered medical images using cross-view transformers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Tulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchiori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vision transformer with deformable attention</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4794" to="4803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated classification of hip fractures using deep convolutional neural networks with orthopedic surgeon-level accuracy: ensemble decisionmaking with antero-posterior and lateral radiographs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Orthop</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="699" to="704" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Vision transformer with progressive sampling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mvc-Net: Multi-view chest radiograph classification network with deep fusion</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="554" to="558" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
