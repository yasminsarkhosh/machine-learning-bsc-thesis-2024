<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
							<idno type="ORCID">0009-0000-5631-164X</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biological Science and Medical Engineering</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="department" key="dep3">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
							<idno type="ORCID">0000-0003-2848-7642</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biological Science and Medical Engineering</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="department" key="dep3">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiya</forename><surname>Saiyin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biological Science and Medical Engineering</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="department" key="dep3">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bingzheng</forename><surname>Wei</surname></persName>
							<idno type="ORCID">0000-0001-6979-0459</idno>
							<affiliation key="aff1">
								<orgName type="institution">Xiaomi Corporation</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maode</forename><surname>Lai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Pathology</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="laboratory">Zhejiang Provincial Key Laboratory of Disease Proteomics and Alibaba-Zhejiang University Joint Research Center of Future Digital Healthcare</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310053</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianzhong</forename><surname>Shou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution" key="instit2">Peking Union Medical College</orgName>
								<address>
									<postCode>100021</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yubo</forename><surname>Fan</surname></persName>
							<idno type="ORCID">0000-0002-3480-4395</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biological Science and Medical Engineering</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="department" key="dep3">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
							<idno type="ORCID">0000-0002-2636-7594</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biological Science and Medical Engineering</orgName>
								<orgName type="department" key="dep2">Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education</orgName>
								<orgName type="department" key="dep3">Advanced Innovation Center for Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="693" to="703"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">C1411D462400DAD1E26A9BFC29D15E34</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_67</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Nuclei Detection</term>
					<term>Unsupervised Learning</term>
					<term>Visual-Language Pre-trained Models</term>
					<term>Prompt Designing</term>
					<term>Zero-shot Learning Y. Wu and Y. Zhou-Equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale visual-language pre-trained models (VLPM) have proven their excellent performance in downstream object detection for natural scenes. However, zero-shot nuclei detection on H&amp;E images via VLPMs remains underexplored. The large gap between medical images and the web-originated text-image pairs used for pre-training makes it a challenging task. In this paper, we attempt to explore the potential of the object-level VLPM, Grounded Language-Image Pretraining (GLIP) model, for zero-shot nuclei detection. Concretely, an automatic prompts design pipeline is devised based on the association binding trait of VLPM and the image-to-text VLPM BLIP, avoiding empirical manual prompts engineering. We further establish a selftraining framework, using the automatically designed prompts to generate the preliminary results as pseudo labels from GLIP and refine the predicted boxes in an iterative manner. Our method achieves a remarkable performance for label-free nuclei detection, surpassing other comparison methods. Foremost, our work demonstrates that the VLPM pretrained on natural image-text pairs exhibits astonishing potential for downstream tasks in the medical field as well. Code will be released at github.com/VLPMNuD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the field of medical image processing, nuclei detection on Hematoxylin and Eosin (H&amp;E)-stained images plays a crucial role in various areas of biomedical research and clinical applications <ref type="bibr" target="#b7">[8]</ref>. While fully-supervised methods have been proposed for this task <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>, the annotation remains labor-intensive and expensive. To address the aforementioned issue, several unsupervised methods have been proposed, including thresh-holding-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>, selfsupervised-based methods <ref type="bibr" target="#b25">[26]</ref>, and domain adaptation-based methods <ref type="bibr" target="#b13">[14]</ref>. Among these, domain adaptation methods are mainstream and have demonstrated favorable performance by achieving adaptation through aligning the source and target domains <ref type="bibr" target="#b13">[14]</ref>. However, current unsupervised methods exhibit strong empirical design and introduce subjective biases during the model design process, thus current unsupervised methods may lead to suboptimal results.</p><p>Yet, newly developed large-scale visual-language pre-trained models (VLPMs) have provided another possible unsupervised learning paradigm <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>. VLPM learns aligned text and image features from massive text-image pairs acquired from the internet, making the learned visual features semantic-rich, general, and transferable. Zero-shot learning methods based on VLPM for downstream tasks such as text-driven image manipulation <ref type="bibr" target="#b22">[23]</ref>, image captioning <ref type="bibr" target="#b14">[15]</ref>, view synthesis <ref type="bibr" target="#b9">[10]</ref>, and object detection <ref type="bibr" target="#b15">[16]</ref>, have achieved excellent results.</p><p>Among VLPMs, Grounded Language-Image Pre-training (GLIP) model <ref type="bibr" target="#b15">[16]</ref>, pre-trained at the object level, can even rival fully-supervised counterparts in zero-shot object detection and phrase grounding tasks. Although VLPM has been utilized for object detection in natural scenes, zero-shot nuclei detection on H&amp;E images via VLPM remains underexplored. The significant domain differences between medical H&amp;E images and the natural images used for pre-training make this task challenging. It is wondered whether VLPM, with its rich semantic information, can facilitate direct prediction of nuclei detection through semanticdriven prompts, establishing an elegant, concise, clear but more efficient and transferable unsupervised system for label-free nuclei detection.</p><p>Building upon this concept, our goal is to establish a zero-shot nuclei detection framework based on VLPM. However, directly applying VLPM for this task poses two challenges. <ref type="bibr" target="#b0">(1)</ref> Due to the gap between medical images and the web-originated text-image pairs used for pre-training, the text-encoder may lack prior knowledge of medical concept words, thus making the prompt design for zero-shot detection a challenging task. (2) Different from the objects in natural images, the high density and specialized morphology of nuclei in H&amp;E stained images may lead to missed detection, false detection, and overlapping during zero-shot transfer solely with prompts.</p><p>To address the first challenge, Yamada et al. have analyzed and revealed that under the pre-training of vast text-image pairs, VLPM establishes a strong association binding between the object and its semantic attributes regardless of image domain, i.e., associated attribute text can fully describe the corresponding objects in an image through VLPM <ref type="bibr" target="#b26">[27]</ref>. Therefore, it is feasible for VLPM to detect unseen medical objects in a label-free manner by constructing appropriate attribute texts. Manual prompting is a cumbersome and subjective process, which may lead to considerable bias. Yet, the VLPM network BLIP <ref type="bibr" target="#b14">[15]</ref> has the capability to generate automatic descriptions for images. Therefore, we first use BLIP to automatically generate attribute words to describe the unseen nuclei object. This approach avoids the empirical manual prompt design and fully leverages the text-to-image aligning trait of VLPMs. We subsequently integrate these attribute words with medical nouns, i.e. "[shape][color][noun]", to create detection prompts. These prompts are then inputted into GLIP to realize zero-shot detection of nuclei. Our proposed automatic prompt designing method fully utilizes the text-to-image alignment of VLPM, and enables the automatic generation of the most suitable attribute text words describing the corresponding domain. Our approach offers excellent interpretability.</p><p>Through GLIP's strong object retrieval performance, we can obtain preliminary boxes. The precision of these preliminary boxes is relatively high, but there is still considerable room for improvement in recall. Therefore, we further establish a self-training framework. We use the preliminary boxes generated by GLIP as pseudo labels for further training YOLOX <ref type="bibr" target="#b6">[7]</ref>, to refine and polish the predicted boxes in an iterative manner. Together with the self-training strategy, the resulting model achieves a remarkable performance for label-free nuclei detection, surpassing other comparison methods. We demonstrate that VLPM, which is pre-trained on natural image-text pairs, also exhibits astonishing potential for downstream tasks in the medical field.</p><p>The contributions of this paper are threefold. (1) A novel zero-shot labelfree nuclei detection framework is proposed based on VLPMs. Our method outperforms all existing unsupervised methods and demonstrates excellent transferability. (2) We leverage GLIP, which places more emphasis on object-level representation learning and generates more high-quality language-aware visual representations compared to Contrastive Language-Image Pre-training (CLIP) model, to achieve better nuclei retrieval. (3) An automatic prompt design process is established based on the association binding trait of VLPM to avoid non-trivial empirical manual prompt engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our approach aims to establish a zero-shot nuclei detection framework based on VLPMs by directly using text prompts. We utilize GLIP for better object-level representation extraction. The overview of our framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Object-Level VLPM -GLIP</head><p>Recently, large VLPMs such as CLIP <ref type="bibr" target="#b23">[24]</ref> and ALIGN <ref type="bibr" target="#b10">[11]</ref> have made great progress in generic visual representation learning and demonstrated the enormous potential of utilizing prompts for zero-shot transfer.</p><p>A typical VLPM framework comprises two encoders: the text encoder, which encodes text prompts to semantic-rich text embeddings, and the image encoder, which encodes images to visual embeddings. These VLPMs use a vast amount of web-originated text-image pairs {(X, T )} i to learn the text-to-image alignment through contrastive loss over text and visual embeddings. Denoting the image encoder as E I , the text encoder as E T , the cosine similarity function as cos(•), and assuming that K text-image pairs are used in each training epoch, the objective of the contrastive learning can be formulated as:</p><formula xml:id="formula_0">L c = - K i=1 log exp (cos (•E I (X i ) • E T (T i )) /τ ) K j=1 exp (cos (E I (X i ) • E T (T j )) /τ ) . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Through this aligning contrastive learning, VLPM aligns text and image in a common feature space, allowing one to directly transfer a trained VLPM to downstream tasks via manipulating text, i.e., prompt engineering. The visual representations of input images are semantic-rich and interpretability-friendly with the help of aligned text-image pairs. However, the conventional VLPM pre-training process only aligns the image with the text from a whole perspective, which results in a lack of emphasis on object-level representations. Therefore, Li et al. proposed GLIP <ref type="bibr" target="#b15">[16]</ref>, whose image encoder generates visual embeddings for each object of different regions in the image to align with object words present in the text. Moreover, GLIP utilizes web-originated phrase grounding text-image pairs to extract novel object word entities, expanding the object concept in the text encoder. Additionally, unlike CLIP, which only aligns embeddings at the end of the model, GLIP builds a deep cross-modality fusion based on cross-attention <ref type="bibr" target="#b2">[3]</ref> for multi-level alignment. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), GLIP leverages DyheadModules <ref type="bibr" target="#b3">[4]</ref> and BERTLayer <ref type="bibr" target="#b4">[5]</ref> as the image and text encoding layers, respectively. With text embedding represented as R and visual embedding as P , the deep fusion process can be represented as:</p><formula xml:id="formula_2">R i t2i , P i i2t = X-MHA R i , P i , • • • i ∈ {0, 1, . . . , L -1},<label>(2)</label></formula><formula xml:id="formula_3">R i+1 = DyHeadModule R i + R i t2i , • • • R = R L , (<label>3</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">P i+1 = BERTLayer P i + P i t2i , • • • P = P L , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where L is the total number of layers, R 0 denotes the visual features from swintransformer-large <ref type="bibr" target="#b18">[19]</ref>, and P 0 denotes the token features from BERT <ref type="bibr" target="#b4">[5]</ref>. X-MHA represents cross-attention. This architecture enables GLIP to attain superior object-level performance and semantic aggregation. Consequently, GLIP is better suited for object-level zero-shot transfer than conventional VLPMs. Thus, we adopt GLIP to extract better object-level representations for nuclei detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Automatic Prompt Design</head><p>The text input, known as the prompt, plays a crucial role in the zero-shot transfer of VLPM for downstream tasks. GLIP originally uses concatenated object nouns such as "object noun 1. Object noun 2..." as default text prompts for detection, and also allows for manual engineering to improve performance <ref type="bibr" target="#b15">[16]</ref>. However, manual prompt engineering is a non-trivial challenge, demanding substantial effort and expertise. Furthermore, a notable disparity exists between the weboriginated pre-training text-image pairs and medical images. Thus simple noun concatenation is insufficient for GLIP to retrieve nuclei. We note that Yamada et al. prove that the pre-training on extensive textimage pairs has allowed VLPMs to establish a strong association binding between objects and their semantic attributes <ref type="bibr" target="#b26">[27]</ref>. Thus, through VLPM, the associated attribute text can accurately depict the corresponding object. Based on this research, we propose that VLPM has the potential to detect unlabelled medical objects in a zero-shot manner by constructing relevant attribute text.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), we first use the image captioning VLPM BLIP <ref type="bibr" target="#b14">[15]</ref> to automatically generate attribute words to describe the unseen nuclei object. BLIP allows us to avoid manual attribute design and generates attribute vocabulary that conforms to the text-to-image alignment of VLPM. This process involves three steps. (1) Directly input target medical nouns into GLIP for coarse box prediction. (2) Use the coarse boxes to squarely crop the image, the cropped objects are fed into a frozen BLIP to automatically generate attribute words that describe the object. (3) Word frequency statistics and classification are adopted to find the top M words that describe the object's shape and color, respectively, for that "shape" and "color" are the most relative two attributes that depict nuclei. For a thorough description, we augment the attribute words with synonyms retrieved by a pre-trained language model <ref type="bibr" target="#b24">[25]</ref>. All these attribute words are combined with those medical nouns to automatically generate a triplet detection prompt of "[shape][color][noun]". Finally, all generated triplets were put into GLIP for refined detection. This method avoids the empirical manual prompt design and fully utilizes the text-to-image aligning trait of VLPMs.</p><p>Our automatic prompt design leverages the powerful text-to-image aligning capabilities of GLIP and BLIP. This approach also enables the automatic generation of the most appropriate attribute words for the specific domain, embodying excellent interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-training Boosting</head><p>Leveraging the strong object retrieval performance of GLIP, we obtain preliminary detection boxes with high precision but low recall. These boxes suffer from missed detection, false detection, and overlapping. To fully exploit the zeroshot potential of GLIP, a self-training framework is established. The automatic prompts are inputted into GLIP to generate the initial results which served as pseudo labels for training YOLOX <ref type="bibr" target="#b6">[7]</ref>. Then, the converged YOLOX is used as a teacher to generate new pseudo labels, and iteratively trains students YOLOX. As self-training is based on the EM optimization algorithm <ref type="bibr" target="#b20">[21]</ref>, it propels our system to continuously refine the predicted boxes and achieve a better optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation</head><p>The dataset used in this study is the MoNuSeg dataset <ref type="bibr" target="#b12">[13]</ref>, which consists of 30 nuclei images of size 1000 × 1000, with an average of 658 nuclei per image. Following Kumar et al. <ref type="bibr" target="#b12">[13]</ref>, the dataset was split into training and testing sets with a ratio of 16:14. 16 training images served as inputs of GLIP to generate pseudo-labels for self-training, with 4 images randomly selected for validation. Annotations were solely employed for evaluation purposes on the test images. 16 overlapped image patches of size 256 × 256 were extracted from each image and randomly cropped into 224 × 224 as inputs.</p><p>In terms of experimental settings, four Nvidia RTX 3090 GPUs were utilized, each with 24 GB of memory. In the automatic prompt generating process, the VQA weights of BLIP finetuned on ViT-B and CapFilt-L <ref type="bibr" target="#b14">[15]</ref> were used to generate [shape] and [color] attributes. These attribute words are augmented with synonyms by GPT <ref type="bibr" target="#b24">[25]</ref>, i.e. attribute augmentation. The target medical noun list was first set to ["nuclei"] straightforwardly, and was also augmented by GPT to ["nuclei", "nucleus", "cyteblast", "karyon"], i.e. noun augmentation. Attribute words were subsequently combined with the target medical nouns to "[shape][color][noun]" format as inputs of GLIP to generate bounding boxes as pseudo labels. The weights used for GLIP is GLIP-L. For self-training refinement, we used the default setting of YOLOX and followed the standard self-training methodology described in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Table <ref type="table">1</ref>. Comparison results on MoNuSeg <ref type="bibr" target="#b12">[13]</ref>. The best results of unsupervised methods are marked in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison</head><p>Our proposed method was compared with the representative fully-supervised method YOLOX <ref type="bibr" target="#b6">[7]</ref>, as well as the current state-of-the-art (SOTA) methods in unsupervised object detection, including SSNS <ref type="bibr" target="#b25">[26]</ref>, SOP <ref type="bibr" target="#b13">[14]</ref>, and VLDet <ref type="bibr" target="#b16">[17]</ref>. Among them, the fully-supervised method YOLOX represents the current SOTA on natural images, and VLDet is a newly proposed zero-shot object detection method based on CLIP. For evaluation, mAP, AP50, AP75, and AR are chosen as metrics, following COCO <ref type="bibr" target="#b17">[18]</ref>. The final results are shown in Table <ref type="table">1</ref>.</p><p>Referring to the table, it is evident that our GLIP-based approach outperforms all unsupervised techniques, including domain adaptation-based and clipbased methods. Figure <ref type="figure" target="#fig_1">2</ref> depicts the visualization of the detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>Automatic Prompt Design. Firstly, we conducted an ablation study specifically targeting the prompt while ensuring that other conditions remained constant, the results are presented in Table <ref type="table" target="#tab_0">2</ref>.</p><p>The first row of the table displays the default noun-concatenation prompt GLIP originally adopted, i.e. "nuclei. nucleus. cyteblast. karyon". The second row represents the same set of nouns with some manual property descriptions added, like "Nuclei. Nucleus. cyteblast. karyon, which are round or oval, and purple or magenta". It is noteworthy that this manual approach is empirically  subjective and therefore prone to significant biases. The subsequent rows in the table demonstrate the combination of attributes generated by our automatic prompt design method. In these experiments, M was set to 3.</p><p>It is worth noting that the predictions generated by the prompts shown in the first and second rows also employed the self-training strategy until convergence. However, the results of the first row contain a significant amount of noise, implying that the intrinsic gap between medical nouns and natural nouns impedes the directly zero-shot transfer of GLIP. The second row improves obviously, indicating the effectiveness of attribute description. But manual design is empirical and tedious. As for the automatically generated prompts, it is evident that as the description of attributes becomes comprehensive, from only including shape or color solely to encompassing both, GLIP's performance improves gradually. Furthermore, the second-to-last row indicates that even without nouns, attribute words alone can achieve good results, which also demonstrates the ability of BLIP-generated attribute words to effectively describe the target nuclei. Through the utilization of VLPM's text-to-image alignment capabilities, the proposed automatic prompt design method generates the most suitable attribute words for a given domain automatically, with a high degree of interpretability. Please refer to the supplement for a detailed list of [shape] and [color] attributes.</p><p>We further looked into the effect of word augmentation. The results are shown in Table <ref type="table" target="#tab_1">3</ref>. Without and with noun augmentation, the noun lists were ["nuclei"] and ["nuclei", "nucleus", "cyteblast", "karyon"], respectively. The first row of Table <ref type="table" target="#tab_1">3</ref> uses non-augmented "[shape][color][noun]", while the first row of Table <ref type="table" target="#tab_0">2</ref> uses noun-augmented concatenation. It is intriguing that applying noun augmentation may lead to suboptimum results compared with the counterparts using the straightforward ["nuclei"]. This is most likely because the augmented new synonym nouns are uncommon medical words and did not appear in the pretraining data that GLIP used. However, applying attribute word augmentation is generally effective because augmented attribute words are also common descriptions for natural scenes. These results suggest a general approach for leveraging the VLPM's potential in downstream medical tasks, that is identifying common attribute words that can be used for describing the target nouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-training and YOLOX.</head><p>The box optimization process of the self-training stage is recorded, and the corresponding results are presented in the supplement. YOLOX and self-training are not the essential reasons for the superior performance of our method. The true key is the utilization of semantic-information-rich VLPMs. To illustrate this point, we employed another commonly used unsupervised detection method, superpixels <ref type="bibr" target="#b0">[1]</ref>, to generate pseudo labels in a zero-shot manner for a fair comparison. These pseudo labels were then fed into the selftraining framework based on the YOLOX segmentation architecture, keeping the settings consistent with our approach except for the pseudo label generation. Additionally, we also used DETR <ref type="bibr" target="#b1">[2]</ref> instead of YOLOX in our method. The results are shown in the supplement and demonstrate that the high performance of our method lies in the effective utilization of the knowledge from VLPMs rather than YOLOX or self-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose to use the object-level VLPM, GLIP, to realize zeroshot nuclei detection on H&amp;E images. An automatic prompt design pipeline is proposed to avoid empirical manual prompt design. It fully utilizes the text-toimage alignment of BLIP and GLIP, and enables the automatic generation of the most suitable attribute describing words, offering excellent interpretability. Furthermore, we utilize the self-training strategy to polish the predicted boxes in an iterative manner. Our method achieves a remarkable performance for labelfree nuclei detection, surpassing other comparison methods. We demonstrate that VLPMs pre-trained on natural image-text pairs still exhibit astonishing potential for downstream tasks in the medical field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of our zero-shot nuclei detection framework based on the frozen object-level VLPM GLIP. (a) Given the original target nouns of the task, prompts are designed automatically to avoid non-trivial empirical manual prompt engineering, based on the association binding of VLPM and the image-to-text VLPM BLIP. (b) A self-training strategy is further adopted to refine and polish the predicted boxes in an iterative manner. The automatically designed prompts are used by GLIP to generate the preliminary results as pseudo labels.</figDesc><graphic coords="4,44,79,53,96,334,57,163,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Output visualizations of different models. The boxes are shown in white.</figDesc><graphic coords="7,58,98,181,52,334,54,101,71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Results of adopting different prompt design methods. The pompts of last 4 rows are automatically generated. The best results are marked in bold.</figDesc><table><row><cell>prompt design</cell><cell>mAP AP50 AP75 AR</cell></row><row><cell>noun. [16]</cell><cell>0.064 0.152 0.036 0.150</cell></row><row><cell>manual design [16]</cell><cell>0.414 0.757 0.422 0.509</cell></row><row><cell>auto: [shape][noun.]</cell><cell>0.336 0.604 0.350 0.434</cell></row><row><cell>auto: [color][noun.]</cell><cell>0.213 0.447 0.176 0.325</cell></row><row><cell>auto: [shape][color]</cell><cell>0.413 0.726 0.438 0.498</cell></row><row><cell cols="2">auto:[shape][color][noun.] 0.416 0.808 0.382 0.502</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on word augmentation. "A Aug." and "N Aug." refer to attribute augmentation and noun augmentation, respectively.</figDesc><table><row><cell>A Aug N Aug mAP AP50 AP75 AR</cell></row><row><cell>0.372 0.725 0.337 0.464</cell></row><row><cell>0.416 0.808 0.382 0.502</cell></row><row><cell>0.316 0.754 0.182 0.419</cell></row><row><cell>0.332 0.659 0.296 0.434</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 67.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-813" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Crossvit: cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic head: unifying object detection heads with attentions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7373" to="7382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semisupervised self-learning for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dópido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Marpu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M B</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4032" to="4044" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">Yolox: exceeding yolo series in 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histologic grading of prostate cancer: a perspective</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Gleason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Pathol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="279" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hover-net: simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Putting nerf on a diet: semantically consistent fewshot view synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5885" to="5894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An improved OSTU method for image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 8th International Conference on Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised nuclei segmentation using spatial organization priors</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Bescond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-732" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="325" to="335" />
		</imprint>
	</monogr>
	<note>MICCAI 2022</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blip: bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grounded language-image pre-training</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10965" to="10975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning object-language alignments for open-vocabulary object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.14843</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-148" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">IHC-net: a fully convolutional neural network for automated nuclear segmentation and ensemble classification for allred scoring in breast pathology</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Mahanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kakoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">107136</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The expectation-maximization algorithm</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Magaz</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast unsupervised nuclear segmentation and classification scheme for automatic allred cancer scoring in immunohistochemical breast tissue images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mouelhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rmili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Prog. Biomed</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="37" to="51" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Styleclip: text-driven manipulation of stylegan imagery</title>
		<author>
			<persName><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2085" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-supervised nuclei segmentation in histopathological images using attention</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahasrabudhe</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-138" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">When are lemons purple? The concept association bias of clip</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yildirim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.12043</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-scale cell instance segmentation with keypoint graph based bounding boxes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-741" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="369" to="377" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
