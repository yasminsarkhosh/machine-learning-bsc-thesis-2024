<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aman</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><forename type="middle">Thomas</forename><surname>Fletcher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="786" to="796"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6F618AA8EA010AF1A805EA0B393F2E40</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_76</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative Modeling</term>
					<term>Histopathology</term>
					<term>Diffusion Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, computational pathology has seen tremendous progress driven by deep learning methods in segmentation and classification tasks aiding prognostic and diagnostic settings. Nuclei segmentation, for instance, is an important task for diagnosing different cancers. However, training deep learning models for nuclei segmentation requires large amounts of annotated data, which is expensive to collect and label. This necessitates explorations into generative modeling of histopathological images. In this work, we use recent advances in conditional diffusion modeling to formulate a first-of-its-kind nuclei-aware semantic tissue generation framework (NASDM) which can synthesize realistic tissue samples given a semantic instance mask of up to six different nuclei types, enabling pixel-perfect nuclei localization in generated samples. These synthetic images are useful in applications in pathology pedagogy, validation of models, and supplementation of existing nuclei segmentation datasets. We demonstrate that NASDM is able to synthesize high-quality histopathology images of the colon with superior quality and semantic controllability over existing generative methods. Implementation: https://github.com/4m4n5/NASDM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Histopathology relies on hematoxylin and eosin (H&amp;E) stained biopsies for microscopic inspection to identify visual evidence of diseases. Hematoxylin has a deep blue-purple color and stains acidic structures such as DNA in cell nuclei. Eosin, alternatively, is red-pink and stains nonspecific proteins in the cytoplasm and the stromal matrix. Pathologists then examine highlighted tissue characteristics to diagnose diseases, including different cancers. A correct diagnosis, therefore, is dependent on the pathologist's training and prior exposure to a wide variety of disease subtypes <ref type="bibr" target="#b30">[30]</ref>. This presents a challenge, as some disease variants are extremely rare, making visual identification difficult. In recent years, deep learning methods have aimed to alleviate this problem by designing discriminative frameworks that aid diagnosis <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">28]</ref>. Segmentation models find applications in spatial identification of different nuclei types <ref type="bibr" target="#b5">[6]</ref>. However, generative modeling in histopathology is relatively unexplored. Generative models can be used to generate histopathology images with specific characteristics, such as visual patterns identifying rare cancer subtypes <ref type="bibr" target="#b3">[4]</ref>. As such, generative models can be sampled to emphasize each disease subtype equally and generate more balanced datasets, thus preventing dataset biases getting amplified by the models <ref type="bibr" target="#b6">[7]</ref>. Generative models have the potential to improve the pedagogy, trustworthiness, generalization, and coverage of disease diagnosis in the field of histology by aiding both deep learning models and human pathologists. Synthetic datasets can also tackle privacy concerns surrounding medical data sharing. Additionally, conditional generation of annotated data adds even further value to the proposition as labeling medical images involves tremendous time, labor, and training costs. Recently, denoising diffusion probabilistic models (DDPMs) <ref type="bibr" target="#b7">[8]</ref> have achieved tremendous success in conditional and unconditional generation of real-world images <ref type="bibr" target="#b2">[3]</ref>. Further, the semantic diffusion model (SDM) demonstrated the use of DDPMs for generating images given semantic layout <ref type="bibr" target="#b27">[27]</ref>. In this work, (1) we leverage recently discovered capabilities of DDPMs to design a first-of-its-kind nuclei-aware semantic diffusion model (NASDM) that can generate realistic tissue patches given a semantic mask comprising of multiple nuclei types, (2) we train our framework on the Lizard dataset <ref type="bibr" target="#b4">[5]</ref> consisting of colon histology images and achieve state-of-the-art generation capabilities, and (3) we perform extensive ablative, qualitative, and quantitative analyses to establish the proficiency of our framework on this tissue generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep learning based generative models for histopathology images have seen tremendous progress in recent years due to advances in digital pathology, compute power, and neural network architectures. Several GAN-based generative models have been proposed to generate histology patches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33]</ref>. However, GANs suffer from problems of frequent mode collapse and overfitting their discriminator <ref type="bibr" target="#b29">[29]</ref>. It is also challenging to capture long-tailed distributions and synthesize rare samples from imbalanced datasets using GANs. More recently, denoising diffusion models have been shown to generate highly compelling images by incrementally adding information to noise <ref type="bibr" target="#b7">[8]</ref>. Success of diffusion models in generating realistic images led to various conditional <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> and unconditional <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref> diffusion models that generate realistic samples with high fidelity. Following this, a morphology-focused diffusion model has been presented for generating tissue patches based on genotype <ref type="bibr" target="#b17">[18]</ref>. Semantic image synthesis is a task involving generating diverse realistic images from semantic layouts. GAN-based semantic image synthesis works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref> generally struggled at generating high quality and enforcing semantic correspondence at the same time. To this end, a semantic diffusion model has been proposed that uses conditional denoising diffusion probabilistic model and achieves both better fidelity and diversity <ref type="bibr" target="#b27">[27]</ref>. We use this progress in the field of conditional diffusion models and semantic image synthesis to formulate our NASDM framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this paper, we describe our framework for generating tissue patches conditioned on semantic layouts of nuclei. Given a nuclei segmentation mask, we intend to generate realistic synthetic patches. In this section, we (1) describe our data preparation, (2) detail our stain-normalization strategy, (3) review conditional denoising diffusion probabilistic models, (4) outline the network architecture used to condition on semantic label map, and (5) highlight the classifier-free guidance mechanism that we employ at sampling time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Processing</head><p>We use the Lizard dataset <ref type="bibr" target="#b4">[5]</ref> to demonstrate our framework. This dataset consists of histology image regions of colon tissue from six different data sources at 20× objective magnification. The images are accompanied by full segmentation annotation for different types of nuclei, namely, epithelial cells, connective tissue cells, lymphocytes, plasma cells, neutrophils, and eosinophils. A generative model trained on this dataset can be used to effectively synthesize the colonic tumor micro-environments. The dataset contains 238 image regions, with an average size of 1055 × 934 pixels. As there are substantial visual variations across images, we construct a representative test set by randomly sampling a 7.5% area from each image and its corresponding mask to be held-out for testing. The test and train image regions are further divided into smaller image patches of 128 × 128 pixels at two different objective magnifications: (1) at 20×, the images are directly split into 128 × 128 pixels patches, whereas (2) at 10×, we generate 256 × 256 patches and resize them to 128 × 128 for training. To use the data exhaustively, patching is performed with a 50% overlap in neighboring patches. As such, at (1) 20× we extract a total of 54,735 patches for training and 4,991 patches as a held-out set, while at (2) 20× magnification we generate 12,409 training patches and 655 patches are held out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stain Normalization</head><p>A common issue in deep learning with H&amp;E stained histopathology slides is the visual bias introduced by variations in the staining protocol and the raw materials of chemicals leading to different colors across slides prepared at different labs <ref type="bibr" target="#b0">[1]</ref>. As such, several stain-normalization methods have been proposed to tackle this issue by normalizing all the tissue samples to mimic the stain distribution of a given target slide <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">26]</ref>. In this work, we use the structure preserving color normalization scheme introduce by Vahadane et al. <ref type="bibr" target="#b26">[26]</ref> to transform all the slides to match the stain distribution of an empirically chosen slide from the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Conditional Denoising Diffusion Probabilistic Model</head><p>In this section, we describe the theory of conditional denoising diffusion probabilistic models, which serves as the backbone of our framework. A conditional diffusion model aims to maximize the likelihood p θ (x 0 | y), where data x 0 is sampled from the conditional data distribution, x 0 ∼ q(x 0 | y), and y represents the conditioning signal. A diffusion model consists of two intrinsic processes. The forward process is defined as a Markov chain, where Gaussian noise is gradually added to the data over T timesteps as</p><formula xml:id="formula_0">q(x t | x t-1 ) = N (x t ; 1 -β t x t-1 , β t I), q(x 1:T | x 0 ) = T t=1 q(x t | x t-1 ),<label>(1)</label></formula><p>where {β} t=1:T are constants defined based on the noise schedule. An interesting property of the Gaussian forward process is that we can sample x t directly from x 0 in closed form. Now, the reverse process, p θ (x 0:T | y), is defined as a Markov chain with learned Gaussian transitions starting from pure noise, p(x T ) ∼ N(0, I), and is parameterized as a neural network with parameters θ as</p><formula xml:id="formula_1">p θ (x 0:T | y) = p(y T ) T t=1 p θ (x t-1 | x t , y).</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>Hence, for each denoising step from t to t -1,</p><formula xml:id="formula_3">p θ (x t-1 | x t , y) = N (x t-1 ; μ θ (x t , y, t), Σ θ (x t , y, t)). (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>It has been shown that the combination of q and p here is a form of a variational auto-encoder <ref type="bibr" target="#b12">[13]</ref>, and hence the variational lower bound (VLB) can be described as a sum of independent terms, L vlb := L 0 + ... + L T -1 + L T , where each term corresponds to a noising step. As described in Ho et al. <ref type="bibr" target="#b7">[8]</ref>, we can randomly sample timestep t during training and use the expectation E t,x0,y, to estimate L vlb and optimize parameters θ. The denoising neural network can be parameterized in several ways, however, it has been observed that using a noiseprediction based formulation results in the best image quality <ref type="bibr" target="#b7">[8]</ref>. Overall, our NASDM denoising model is trained to predicting the noise added to the input image given the semantic layout y and the timestep t using the loss described as follows:</p><formula xml:id="formula_5">L simple = E t,x, [ -θ (x t , y, t) 2 ] . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Note that the above loss function provides no signal for training Σ θ (x t , y, t). Therefore, following the strategy in improved DDPMs <ref type="bibr" target="#b7">[8]</ref>, we train a network to directly predict an interpolation coefficient v per dimension, which is turned into variances and optimized directly using the KL divergence between the estimated distribution p θ (x t-1 | x t , y) and the diffusion posterior q(x t-1 | x t , x 0 ) as</p><formula xml:id="formula_7">L vlb = D KL (p θ (x t-1 | x t , y) q(x t-1 | x t , x 0 )</formula><p>). This optimization is done while applying a stop gradient to (x t , y, t) such that L vlb can guide Σ θ (x t , y, t) and L simple is the main guidance for (x t , y, t). Overall, the loss is a weighted summation of the two objectives described above as follows:</p><formula xml:id="formula_8">L hybrid = L simple + λL vlb .</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conditioning on Semantic Mask</head><p>NASDM requires our neural network noise-predictor θ (x t , y, t) to effectively process the information from the nuclei semantic map. For this purpose, we leverage a modified U-Net architecture described in Wang et al. <ref type="bibr" target="#b27">[27]</ref>, where semantic information is injected into the decoder of the denoising network using multi-layer, spatially-adaptive normalization operators. As denoted in Fig. <ref type="figure" target="#fig_0">1</ref>, we construct the semantic mask such that each channel of the mask corresponds to a unique nuclei type. In addition, we also concatenate a mask comprising of the edges of all nuclei to further demarcate nuclei instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Classifier-Free Guidance</head><p>To improve the sample quality and agreement with the conditioning signal, we employ classifier-free guidance <ref type="bibr" target="#b9">[10]</ref>, which essentially amplifies the conditional distribution using unconditional outputs while sampling. During training, the conditioning signal, i.e., the semantic label map, is randomly replaced with a null mask for a certain percentage of samples. This leads to the diffusion model becoming stronger at generating samples both conditionally as well as unconditionally and can be used to implicitly infer the gradients of the log probability required for guidance as follows: where ∅ denotes an empty semantic mask. During sampling, the conditional distribution is amplified using a guidance scale s as follows:</p><formula xml:id="formula_9">θ (x t | y) -θ (x t | ∅) ∝ ∇ xt log p(x t | y) -∇ xt log p(x t ), ∝ ∇ xt log p(y | x t ),<label>(6)</label></formula><formula xml:id="formula_10">ˆ θ (x t | y) = θ (x t | y) + s • [ θ (x t | y) -θ (x t | ∅)] . (<label>7</label></formula><formula xml:id="formula_11">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first describe our implementation details and training procedure. Further, we establish the robustness of our model by performing an ablative study over objective magnification and classifier-guidance scale. We then perform quantitative and qualitative assessments to demonstrate the efficacy of our nuclei-aware semantic histopathology generation model. In all following experiments, we synthesize images using the semantic masks of the held-out dataset at the concerned objective magnification. We then compute Fréchet Inception Distance (FID) and Inception Score (IS) metrics between the synthetic and real images in the held-out set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Our diffusion model is implemented using a semantic UNet architecture (Sect. 3.4), trained using the objective in <ref type="bibr" target="#b4">(5)</ref>. Following previous works <ref type="bibr" target="#b18">[19]</ref>, we set the trade-off parameter λ as 0.001. We use the AdamW optimizer to train our model. Additionally, we adopt an exponential moving average (EMA) of the denoising network weights with 0.999 decay. Following DDPM <ref type="bibr" target="#b7">[8]</ref>, we set the total number of diffusion steps as 1000 and use a linear noising schedule with respect to timestep t for the forward process. After normal training with a learning rate of 1e -4, we decay the learning rate to 2e -5 to further finetune the model with a drop rate of 0.2 to enhance the classifier-free guidance capability during sampling. The whole framework is implemented using Pytorch and trained on 4 NVIDIA Tesla A100 GPUs with a batch-size of 40 per GPU. Code will be made public on publication or request. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation over Guidance Scale (s)</head><p>In this study, we test the effectiveness of the classifier-free guidance strategy. We consider the variant without guidance as our baseline. As seen in Fig. <ref type="figure" target="#fig_1">2</ref>, increase in guidance scale initially results in better image quality as more detail is added to visual structures of nuclei. However, with further increase, the image quality degrades as the model overemphasizes the nuclei and staining textures. As described in Sect. 3.1, we generate patches at two different objective magnifications of 10× and 20×. In this section, we contrast the generative performance of the models trained on these magnification levels respectively. From the table on right, we observe that the model trained at 20× objective magnification produces better generative metrics. Note that we only train on a subset on 20× mag. to keep the size of the training data constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation over Objective Magnification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative Analysis</head><p>To the best of our knowledge, ours is the only work that is able to synthesize histology images given a semantic mask, making a direct quantitative comparison tricky. However, the standard generative metric Fréchet Inception Distance (FID) measures the distance between distributions of generated and real images in the Inception-V3 <ref type="bibr" target="#b13">[14]</ref> latent space, where a lower FID indicates that the model is able to generate images that are very similar to real data. Therefore, we compare FID and IS metrics with the values reported in existing works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">32]</ref> (ref.</p><p>Table <ref type="table" target="#tab_0">1</ref>) in their own settings. We can observe that our method outperforms all existing methods including both GANs-based methods as well as the recently proposed morphology-focused generative diffusion model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Analysis</head><p>We perform an expert pathologist review of the patches generated by the model. We use 30 patches, 17 synthetic and 13 real for this review. We have two experts assess the overall medical quality of the patches as well as their consistency with the associated nuclei masks on likert scale. The survey used for the review can be found on a public google survey<ref type="foot" target="#foot_0">1</ref> . It can be seen from this survey (Fig. <ref type="figure">4</ref>) that the patches generated by the model are found to be more realistic than even the patches in our real set. We now qualitatively discuss the proficiency of our model in generating realistic visual patterns in synthetic histopathology images (refer Fig. <ref type="figure" target="#fig_3">3</ref>). We can see that the model is able to capture convincing visual structure for each type of nuclei. In the synthetic images, we can see that the lymphocytes are accurately circular, while neutrophils and eosinophils have a more lobed structure. We also observe that the model is able to mimic correct nucleus-to-cytoplasm ratios for each type of nuclei. Epithelial cells are less dense, have a distinct chromatin structure, and are larger compared to other white blood cells. Epithelial cells are most difficult to generate in a convincing manner, however, we can see that model is able to capture the nuances well and generates accurate chromatin distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Works</head><p>In this work, we present NASDM, a nuclei-aware semantic tissue generation framework. We demonstrate the model on a colon dataset and qualitatively Fig. <ref type="figure">4</ref>. Qualitative Review: Compiled results from a pathologist review. We have experts assess patches for, their overall medical quality (left), as well as, their consistency with the associated mask (right). We observe that the patches generated by the model do better on all metrics and majority are imperceptible from real patches.</p><p>and quantitatively establish the proficiency of the framework at this task. In future works, further conditioning on properties like stain-distribution, tissuetype, disease-type, etc. would enable patch generation in varied histopathological settings. Additionally, this framework can be extended to also generate semantic masks enabling an end-to-end tissue generation framework that first generates a mask and then synthesizes the corresponding patch. Further, future works can explore generation of patches conditioned on neighboring patches, as this enables generation of larger tissue areas by composing patches together.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. NASDM training framework: Given a real image x0 and semantic mask y, we construct the conditioning signal by expanding the mask and adding an instance edge map. We sample timestep t and noise to perform forward diffusion and generate the noised input xt. The corrupted image xt, timestep t, and semantic condition y are then fed into the denoising model which predicts ˆ as the amount of noise added to the model. Original noise and prediction ˆ are used to compute the loss in (4).</figDesc><graphic coords="4,63,18,60,68,317,98,99,46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Guidance Scale Ablation: For a given mask, we generate images using different values of the guidance scale, s. The FID and IS metrics are computed by generating images for all masks in the test set at 20× magnification.</figDesc><graphic coords="6,58,98,58,91,332,23,91,90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative Results: We generate synthetic images given masks with each type of nuclei in different environments to demonstrate the proficiency of the model to generate realistic nuclei arrangements. Legend at bottom denotes the mask color for each type of nuclei.</figDesc><graphic coords="8,60,99,63,08,330,88,143,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative Assessment: We report the performance of our method using Fréchet Inception Distance (FID) and Inception Score (IS) with the metrics reported in existing works. (-) denotes that corresponding information was not reported in original work. * Note that performance reported for best competing method on the colon data is from our own implementation, performances for both this and our method should improve with better tuning. Please refer to our github repo for updated statistics.</figDesc><table><row><cell>Method</cell><cell cols="4">Tissue type Conditioning FID(↓) IS(↑)</cell></row><row><cell>BigGAN [2]</cell><cell>bladder</cell><cell>none</cell><cell>158.4</cell><cell>-</cell></row><row><cell>AttributeGAN [32]</cell><cell>bladder</cell><cell>attributes</cell><cell>53.6</cell><cell>-</cell></row><row><cell>ProGAN [11]</cell><cell>glioma</cell><cell>morphology</cell><cell>53.8</cell><cell>1.7</cell></row><row><cell cols="2">Morph-Diffusion [18] glioma</cell><cell>morphology</cell><cell>20.1</cell><cell>2.1</cell></row><row><cell cols="2">Morph-Diffusion  *  [18] colon</cell><cell>morphology</cell><cell>18.8</cell><cell>2.2</cell></row><row><cell>NASDM (Ours)</cell><cell>colon</cell><cell cols="2">semantic mask 14.1</cell><cell>2.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://forms.gle/1dLAdk9XKhp6FWMY6.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements:. We would like to thank <rs type="person">Dr. Shyam Raghavan, M.D.</rs>, <rs type="person">Fisher Rhoads</rs>, B.S., and <rs type="person">Dr. Lubaina Ehsan, M.D.</rs> for their invaluable inputs for our qualitative analysis.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantitative analysis of stain variability in histology slides and an algorithm for standardization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Timofeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Otte-Höller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Pathology</title>
		<imprint>
			<biblScope unit="volume">9041</biblScope>
			<biblScope unit="page" from="45" to="51" />
			<date type="published" when="2014">2014. 2014</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On oversampling imbalanced data with deep conditional generative models</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Fajardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Findlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houmanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page">114463</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lizard: a large-scale dataset for colonic nuclear instance segmentation and classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="684" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hover-net: simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adcock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11706</idno>
		<title level="m">A systematic study of bias amplification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">47</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Kawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11793</idno>
		<title level="m">Denoising diffusion restoration models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06026</idno>
		<title level="m">The role of imagenet classes in fr\&apos;echet inception distance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning in histopathology: the path to the clinic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="775" to="784" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synthesis of diagnostic quality cancer pathology images by generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pathol</title>
		<imprint>
			<biblScope unit="volume">252</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="188" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A method for normalizing histology slides for quantitative analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Macenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging: from Nano to Macro</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1107" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A morphology focused diffusion probabilistic model for synthesis of histopathology images</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Moghadam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2000" to="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Palette: Image-to-image diffusion models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2022 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image superresolution via iterative refinement</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4713" to="4726" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-attentive adversarial stain normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">January 10-15, 2021</date>
			<biblScope unit="page" from="120" to="140" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-68763-2_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-68763-210" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diverse semantic image synthesis via probability distribution modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7962" to="7971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient semantic image synthesis via class-adaptive normalization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4852" to="4866" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structure-preserving color normalization and sparse stain separation for histological images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1962" to="1971" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.00050</idno>
		<title level="m">Semantic image synthesis via diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recent advances of deep learning for computational histopathology: principles and applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1199</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Tackling the generative learning trilemma with denoising diffusion gans</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07804</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrating deep convolutional neural networks with marker-controlled watershed for overlapping nuclei segmentation in histopathology images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page" from="166" to="179" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selective synthetic augmentation with Histogan for improved histopathology image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101816</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A multi-attribute controllable generative model for histopathology image synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zaino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-359" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021: 24th International Conference</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Strasbourg, France; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27 -October 1, 2021. 2021</date>
			<biblScope unit="page" from="613" to="623" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A u-net based progressive gan for microscopic image augmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-12053-4_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-12053-434" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Understanding and Analysis: 26th Annual Conference</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">July 27-29, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="458" to="468" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
