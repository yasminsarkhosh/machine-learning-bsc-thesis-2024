<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions</title>
				<funder ref="#_p5EMZjd">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lanfeng</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Pathology</orgName>
								<orgName type="institution" key="instit1">West China Second University Hospital</orgName>
								<orgName type="institution" key="instit2">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guotai</forename><surname>Wang</surname></persName>
							<email>guotai.wang@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="570" to="579"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">8F9202B928AF3F68C619FB7821D9A736</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semi-supervised learning</term>
					<term>Knowledge distillation</term>
					<term>Attention</term>
					<term>Uncertainty</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segmentation of pathological images is a crucial step for accurate cancer diagnosis. However, acquiring dense annotations of such images for training is labor-intensive and time-consuming. To address this issue, Semi-Supervised Learning (SSL) has the potential for reducing the annotation cost, but it is challenged by a large number of unlabeled training images. In this paper, we propose a novel SSL method based on Cross Distillation of Multiple Attentions (CDMA) to effectively leverage unlabeled images. Firstly, we propose a Multi-attention Tri-branch Network (MTNet) that consists of an encoder and a three-branch decoder, with each branch using a different attention mechanism that calibrates features in different aspects to generate diverse outputs. Secondly, we introduce Cross Decoder Knowledge Distillation (CDKD) between the three decoder branches, allowing them to learn from each other's soft labels to mitigate the negative impact of incorrect pseudo labels in training. Additionally, uncertainty minimization is applied to the average prediction of the three branches, which further regularizes predictions on unlabeled images and encourages inter-branch consistency. Our proposed CDMA was compared with eight state-of-the-art SSL methods on the public DigestPath dataset, and the experimental results showed that our method outperforms the other approaches under different annotation ratios. The code is available at https://github.com/HiLab-git/CDMA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic segmentation of tumor lesions from pathological images plays an important role in accurate diagnosis and quantitative evaluation of cancers. Recently, deep learning has achieved remarkable performance in pathological image segmentation when trained with a large and well-annotated dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b20">20]</ref>. However, obtaining dense annotations for pathological images is challenging and time-consuming, due to the extremely large image size (e.g., 10000 × 10000 pixels), scattered spatial distribution, and complex shape of lesions.</p><p>Semi-Supervised Learning (SSL) is a potential technique to reduce the annotation cost via learning from a limited number of labeled data along with a large amount of unlabeled data. Existing SSL methods can be roughly divided into two categories: consistency-based <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b23">23]</ref> and pseudo label-based <ref type="bibr" target="#b1">[2]</ref> methods. The consistency-based methods impose consistency constraints on the predictions of an unlabeled image under some perturbations. For example, Mean Teacher (MT)-based methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b23">23]</ref> encourage consistent predictions between a teacher and a student model with noises added to the input. Xie et al. <ref type="bibr" target="#b21">[21]</ref> introduced a pairwise relation network to exploit semantic consistency between each pair of images in the feature space. Luo et al. <ref type="bibr" target="#b9">[9]</ref> proposed an uncertainty rectified pyramid consistency between multi-scale predictions. Jin et al. <ref type="bibr" target="#b7">[7]</ref> proposed to encourage the predictions of auxiliary decoders and a main decoder to be consistent under perturbed hierarchical features. Pseudo label-based methods typically generate pseudo labels for labeled images to supervise the network <ref type="bibr" target="#b3">[4]</ref>. Since using a model's prediction to supervise itself may over-fit its bias, Chen et al. <ref type="bibr" target="#b1">[2]</ref> proposed Cross Pseudo Supervision (CPS) where two networks learn from each other's pseudo labels generated by argmax of the output prediction. MC-Net+ <ref type="bibr" target="#b19">[19]</ref> utilized multiple decoders with different upsampling strategies to obtain slightly different outputs, and each decoder's probability output was sharpened to serve as pseudo labels to supervise the others. However, the pseudo labels are not accurate and contain a lot of noise, using argmax or sharpening operation will lead to over-confidence of potentially wrong predictions, which limits the performance of the models. Additionally, some related works advocated the entropy-minimization methods. Typical entropy Minimization (EM) <ref type="bibr" target="#b15">[15]</ref> that aims to reduce the uncertainty or entropy in a system. Wu et al. <ref type="bibr" target="#b17">[17]</ref> directly applied entropy minimization to the segmentation results.</p><p>In this work, we propose a novel and efficient method based on Cross Distillation with Multiple Attentions (CDMA) for semi-supervised pathological image segmentation. Firstly, a Multi-attention Tri-branch Network (MTNet) is proposed to efficiently obtain diverse outputs for a given input. Unlike MC-Net+ <ref type="bibr" target="#b19">[19]</ref> that is based on different upsampling strategies, our MTNet uses different attention mechanisms in three decoder branches that calibrate features in different aspects to obtain diverse and complementary outputs. Secondly, inspired by the observation that smoothed labels are more effective for noise-robust learning found in recent studies <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b22">22]</ref>, we propose a Cross Decoder Knowledge Distillation (CDKD) strategy to better leverage the diverse predictions of unlabeled images. In CDKD, each branch serves as a teacher of the other two branches using soft label supervision, which reduces the effect of noise for more robust learning from inaccurate pseudo labels than argmax <ref type="bibr" target="#b1">[2]</ref> and sharpening-based <ref type="bibr" target="#b19">[19]</ref> pseudo supervision in existing methods. Differently from typical Knowledge Distillation (KD) methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">24]</ref> that require a pre-trained teacher to generate soft predictions, our method efficiently obtains the teacher and student's soft predictions simultaneously in a single forward pass. In addition, we apply an uncertainty minimization-based regularization to the average probability prediction across the decoders, which not only increases the network's confidence, but also improves the inter-decoder consistency for leveraging labeled images.</p><p>The contribution of this work is three-fold: 1) A novel framework named CDMA based on MTNet is introduced for semi-supervised pathological image segmentation, which leverages different attention mechanisms for generating diverse and complementary predictions for unlabeled images; 2) A Cross Decoder Knowledge Distillation method is proposed for robust and efficient learning from noisy pseudo labels, which is combined with an average prediction-based uncertainty minimization to improve the model's performance; 3) Experimental results show that the proposed CDMA outperforms eight state-of-the-art SSL methods on the public DigestPath dataset <ref type="bibr" target="#b2">[3]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the proposed Cross Distillation of Multiple Attentions (CDMA) framework for semi-supervised pathological image segmentation consists of three core modules: 1) a tri-branch network MTNet that uses three different attention mechanisms to obtain diverse outputs, 2) a Cross Decoder Knowledge Distillation (CDKD) module to reduce the effect of noisy pseudo labels based on soft supervision, and 3) an average prediction-based uncertainty minimization loss to further regularize the predictions on unlabeled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-attention Tri-Branch Network (MTNet)</head><p>Attention is an effective network structure design in fully supervised image segmentation <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b16">16]</ref>. It can calibrate the feature maps for better performance by paying more attention to the important spatial positions or channels with only a few extra parameters. However, it has been rarely investigated in semi-supervised segmentation tasks. To more effectively exploit attention mechanisms for semisupervised pathological image segmentation, our proposed MTNet consists of a shared encoder and three decoder branches that are based on Channel Attention (CA), Spatial Attention (SA) and simultaneous Channel and Spatial Attention (CSA), respectively. The encoder consists of multiple convolutional blocks that are sequentially connected to a down-sampling layer, and each decoder has multiple convolutional blocks that are sequentially connected by an up-sampling layer. For a certain decoder, it uses CA, SA or SCA at the convolutional block at each resolution level to calibrate the features.</p><p>CA branch uses channel attention blocks to calibrate the features in the first decoder. A channel attention block highlights important channels in a feature map and it is formulated as:</p><formula xml:id="formula_0">F c = F • σ MLP P ool S avg (F ) + MLP P ool S max (F )<label>(1)</label></formula><p>where F represents an input feature map. P ool S avg and P ool S max represent average pooling and max-pooling across the spatial dimension, respectively. MLP and σ denote multi-layer perception and the sigmoid activation function respectively. F c is the output feature map calibrated by channel attention.</p><p>SA branch leverages spatial attention to highlight the most relevant spatial positions and suppress the irrelevant regions in a feature map. An SA block is:</p><formula xml:id="formula_1">F s = F • σ Conv P ool C avg (F ) ⊕ P ool C max (F )<label>(2)</label></formula><p>where Conv denotes a convolutional layer. P ool C avg and P ool C max are average and max-pooling across the channel dimension, respectively. ⊕ means concatenation.</p><p>CSA branch calibrates the feature maps using a CSA block for each convolutional block. A CSA block consists of a CA block followed by an SA block, taking advantage of channel and spatial attention simultaneously.</p><p>Due to the different attention mechanisms, the three decoder branches pay attention to different aspects of feature maps and lead to different outputs. To further improve the diversity of the outputs and alleviate over-fitting, we add a dropout layer and a feature noise layer η <ref type="bibr" target="#b11">[11]</ref> before each of the three decoders. For an input image, the logit predictions obtained by the three branches are denoted as Z CA , Z SA and Z CSA , respectively. After using a standard Softmax operation, their corresponding probability prediction maps are denoted as P CA , P SA and P CSA , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross Decoder Knowledge Distillation (CDKD)</head><p>Since the three branches have different decision boundaries, using the predictions from one branch as pseudo labels to supervise the others would avoid each branch over-fitting its bias. However, as the predictions for unlabeled training images are noisy and inaccurate, using hard or sharpened pseudo labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">19]</ref> would strengthen the confidence on incorrect predictions, leading the model to overfit the noise <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b22">22]</ref>. To address this problem, we introduce CDKD to enhance the ability of our MTNet to leverage unlabeled images and eliminate the negative impact of noisy pseudo labels. It forces each decoder to be supervised by the other two decoders' soft predictions. Following the practice of KD <ref type="bibr" target="#b4">[5]</ref>, a temperature calibrated Softmax (T-Softmax) is used to soften the probability maps:</p><formula xml:id="formula_2">pc = exp(z c /T ) c exp(z c /T )<label>(3)</label></formula><p>where z c represents the logit prediction for class c of a pixel, and pc is the soft probability value for class c. Temperature T is a parameter to control the softness of the output probability. Note that T = 1 corresponds to a standard Softmax function, and a larger T value leads to a softer probability distribution with higher entropy. When T &lt; 1, Eq. 3 is a sharpening function.</p><p>Let PCA , PSA and PCSA represent the soft probability map obtained by T-Softmax for the three branches, respectively. With the other two branches being the teachers, the KD loss for the CSA branch is:</p><formula xml:id="formula_3">L CSA kd = KL( PCSA , PCA ) + KL( PCSA , PSA )<label>(4)</label></formula><p>where KL() is the Kullback-Leibler divergence function. Note that the gradient of L CSA kd is only back-propagated to the CSA branch, so that the knowledge is distilled from the teachers to the student. Similarly, the KD losses for the CA and SA branches are denoted as L CA kd and L SA kd , respectively. Then, the total distillation loss is defined as:</p><formula xml:id="formula_4">L cdkd = 1 3 (L CSA kd + L CA kd + L SA kd )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Average Prediction-Based Uncertainty Minimization</head><p>Minimizing the uncertainty (e.g., entropy) <ref type="bibr" target="#b15">[15]</ref> has been shown to be an effective regularization for predictions on unlabeled images, which increases the model's confidence on its predictions. However, applying uncertainty minimization to each branch independently may lead to inconsistent predictions between the decoders where each of them is very confident, e.g., two branches predict the foreground probability of a pixel as 0.0 and 1.0 respectively. To avoid this problem and further encourage inter-decoder consistency for regularization, we propose an average prediction-based uncertainty minimization:</p><formula xml:id="formula_5">L um = - 1 N N i=0 C c=0 P c i log( P c i )<label>(6)</label></formula><p>where P = (P CSA + P CA + P SA )/3 is the average probability map. C and N are the class number and pixel number respectively. P c i is the average probability for class c at pixel i. Note that when L um for a pixel is close to zero, the average probability for class c of that pixel is close to 0.0 (1.0), which drives all the decoders to predict it as 0.0 (1.0) and encourages inter-decoder consistency.</p><p>Finally, the overall loss function for our CDMA is:</p><formula xml:id="formula_6">L = L sup + λ 1 L cdkd + λ 2 L um (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where L sup = (L CSA sup + L CA sup + L SA sup )/3 is the average supervised learning loss for the three branches on the labeled training images, and the supervised loss for each branch calculates the Dice loss and cross entropy loss between the probability prediction (P CSA , P CA and P SA ) and the ground truth label. λ 1 and λ 2 are the weights of L cdkd and L um respectively. Note that L cdkd and L um are applied on both labeled and unlabeled training images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Dataset and Implementation Details. We used the public DigestPath dataset <ref type="bibr" target="#b2">[3]</ref> for binary segmentation of colonoscopy tumor lesions from Whole Slide Images (WSI) in the experiment. The WSIs were collected from four medical institutions of ×20 magnification (0.475 μm/pixel) with an average size of 5000 × 5000. We randomly split 130 malignant WSIs into 100, 10, and 20 for training, validation and testing, respectively. For SSL, we investigated two annotation ratios: 5% and 10%, where only 5 and 10 WSIs in the training set were taken as annotated respectively. Labeled WSIs were randomly selected. For computational feasibility, we cropped the WSIs into patches with a size of 256 × 256.</p><p>At inference time for segmenting a WSI, we used a sliding window of size 256×256 with a stride of 192 × 192.</p><p>The CDMA framework was implemented in PyTorch, and all experiments were performed on one NVIDIA 2080Ti GPU. MTNet was implemented by extending DeepLabv3+ <ref type="bibr" target="#b0">[1]</ref> into a tri-branch network, where the three decoders were equipped with CA, SA and CSA blocks respectively. The encoder used a backbone of ResNet50 pre-trained on ImageNet. The kernel size of Conv in the SA block is 7 × 7. SGD optimizer was used for training, with weight decay 5 × 10 -4 , momentum 0.9 and epoch number 150. The learning rate was initialized to 10 -3 and decayed by 0.1 every 50 epochs. The hyper-parameter setting was λ 1 = λ 2 = 0.1, T = 10 based on the best results on the validation set. The batch size was 16 (8 labeled and 8 unlabeled patches). For data augmentation, we adopted random flipping, random rotation, and random Gaussian noise. For inference, only the CSA branch was used due to the similar performance of the three branches after converge and the increased inference time of their ensemble, and no post-processing was used. Dice Similarity Coefficient (DSC) and Jaccard Index (JI) were used for quantitative evaluation. Comparison with State-of-the-Art Methods. Our CDMA was compared with eight existing SSL methods: 1) Entropy Minimization (EM) <ref type="bibr" target="#b15">[15]</ref>; 2) Mean Teacher (MT) <ref type="bibr" target="#b14">[14]</ref>; 3) Uncertaitny-Aware Mean Teacher (UAMT) <ref type="bibr" target="#b23">[23]</ref>; 4) R-Drop <ref type="bibr" target="#b18">[18]</ref> that introduces a dropout-based consistency regularization between two networks; 5) CPS <ref type="bibr" target="#b1">[2]</ref>; 6) Hierarchical Consistency Enforcement (HCE) <ref type="bibr" target="#b7">[7]</ref>; 7) CNN&amp;Transformer <ref type="bibr" target="#b8">[8]</ref> that introduces cross-supervision between CNN and Transformer; 8) MC-Net+ <ref type="bibr" target="#b19">[19]</ref> that imposes mutual consistency between multiple slightly different decoders. They were also compared with the lower bound of Supervised Learning (SL) that only learns from the labeled images. All these methods used the same backbone of DeepLabv3+ <ref type="bibr" target="#b0">[1]</ref> for a fair comparison.</p><p>Quantitative evaluation of these methods is shown in Table <ref type="table" target="#tab_0">1</ref>. In the existing methods, MC-Net+ <ref type="bibr" target="#b19">[19]</ref> and CPS <ref type="bibr" target="#b1">[2]</ref> showed the best performance for both of the two annotation ratios. Our proposed CDMA achieved a better performance than all the existing methods, with a DSC score of 69.72% and 72.24% when the annotation ratio was 5% and 10%, respectively. Figure <ref type="figure" target="#fig_1">2</ref> shows a qualitative comparison between different methods. It can be observed that our CDMA yields less mis-segmentation compared with CPS <ref type="bibr" target="#b1">[2]</ref> and MC-Net+ <ref type="bibr" target="#b19">[19]</ref>. Ablation Study. For ablation study, we set the baseline as using the proposed MTNet with three different decoders for supervised learning from labeled images only. It obtained an average DSC of 65.02% and 68.61% under the two annotation ratios respectively. The proposed L cdkd was compared with two variants: L cdkd (argmax) and L cdkd (T =1) that represent using hard pseudo labels and standard probability output obtained by Softmax for CDKD respectively. Table <ref type="table" target="#tab_2">2</ref> shows that our L cdkd obtained an average DSC of 68.84% and 71.49% under the two annotation ratios respectively, and it outperformed L cdkd (argmax) and L cdkd (T =1), demonstrating that our CDKD based on softened probability prediction is more effective in dealing with noisy pseudo labels. By introducing our average prediction-based uncertainty minimization L um , the DSC was further improved to 69.72% and 72.24% under the two annotation ratios respectively. In addition, replacing our L um by applying entropy minimization to each branch respectively (L um ) led to a DSC drop by around 0.65%. Then, we compared different MTNet variants: 1) MTNet(dual) means a dualbranch structure (removing the CSA branch); 2) MTNet(csa×3) means all the three branches use CSA blocks; 3) MTNet(-atten) means no attention block is used in all the branches; and 4) MTNet(ensb) means using an ensemble of the three branches for inference. Note that all these variants were trained with L cdkd and L um . The results in the second section of Table <ref type="table" target="#tab_2">2</ref> show that using the same structures for different branches, i.e., MTNet(-atten) and MTNet(csa×3), had a lower performance than using different attention blocks, and using three attention branches outperformed just using two attention branches. It can also be found that using CSA branch for inference had a very close performance to MTNet(ensb), and it is more efficient than the later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented a novel semi-supervised framework based on Cross Distillation of Multiple Attentions (CDMA) for pathological image segmentation. It employs a Multi-attention Tri-branch network to generate diverse predictions based on channel attention, spatial attention, and simultaneous channel and spatial attention, respectively. Different attention-based decoder branches focus on various aspects of feature maps, resulting in disparate outputs, which is beneficial to semi-supervised learning. To eliminate the negative impact of incorrect pseudo labels in training, we employ a Cross Decoder Knowledge Distillation (CDKD) to enforce each branch to learn from soft labels generated by the other two branches. Experimental results on a colonoscopy tissue segmentation dataset demonstrated that our CDMA outperformed eight state-of-the-art SSL methods. In the future, it is of interest to apply our method to multi-class segmentation tasks and pathological images from different organs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our CDMA for semi-supervised segmentation. Three decoder branches use different attentions to obtain diverse outputs. Cross Decoder Knowledge Distillation (CDKD) is proposed to better deal with noisy pseudo labels, and an uncertainty minimization is applied to the average probability prediction of the three branches. Lsup is only for labeled images.</figDesc><graphic coords="3,42,81,231,62,338,29,157,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparison between our proposed CDMA with state-of-the-art methods for semi-supervised semantic segmentation of WSIs. The green regions are lesions.</figDesc><graphic coords="6,68,49,249,92,327,82,140,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between different SSL methods on the DigestPath dataset. * denotes p-value &lt; 0.05 (significance level) when comparing the proposed CDMA with the others under t-test hypothesis testing.</figDesc><table><row><cell>Methods</cell><cell>DSC</cell><cell></cell><cell>Jaccard Index</cell></row><row><cell></cell><cell>5% labeled</cell><cell>10% labeled</cell><cell>5% labeled</cell><cell>10% labeled</cell></row><row><cell cols="2">SL lower bound 64.74±23.24  UAMT [23] 67.76±23.44</cell><cell cols="2">69.64±22.41  *  55.16±22.24</cell><cell>57.22±22.25  *</cell></row><row><cell>R-Drop [18]</cell><cell cols="4">67.22±24.05  *  70.37±23.58  *  54.70±22.63  *  57.39±22.94  *</cell></row><row><cell>CPS [2]</cell><cell cols="2">67.71±22.50  *  70.46±23.75</cell><cell cols="2">54.73±20.92  *  58.67±23.30</cell></row><row><cell>HCE [7]</cell><cell cols="2">67.34±22.32  *  70.29±22.62</cell><cell cols="2">54.58±20.37  *  58.04±21.11</cell></row><row><cell cols="2">CNN&amp;Transformer [8] 67.66±25.12</cell><cell cols="2">70.43±18.84  *  55.74±23.38</cell><cell>57.89±19.48  *</cell></row><row><cell>MC-Net+ [19]</cell><cell>67.81±24.22</cell><cell></cell><cell></cell></row></table><note><p>* 68.32±21.18 * 52.35±21.53 * 53.62±20.32 * EM [15] 67.09±24.28 * 70.01±22.24 * 54.55±22.40 * 56.96±21.70 * MT [14] 67.46±23.10 * 70.19±21.72 * 54.68±21.27 * 56.38±21.21 * * 70.09±22.07 * 55.40±22.54 * 57.64±21.80 * Ours (CSA branch)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>69.72±22.06 72.24±21.21 57.09±21.23 60.17±21.98</head><label></label><figDesc></figDesc><table><row><cell>Full Supervision</cell><cell>77.47±12.49</cell><cell>64.97±14.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablative analysis of our proposed method.</figDesc><table><row><cell>Methods</cell><cell>Mean DSC</cell><cell></cell><cell>Mean JI</cell><cell></cell></row><row><cell></cell><cell>5% labeled</cell><cell>10% labeled</cell><cell>5% labeled</cell><cell>10% labeled</cell></row><row><cell>MTNet (Baseline)</cell><cell>65.02±23.94</cell><cell>68.61±22.10</cell><cell>52.59±22.54</cell><cell>55.47±21.81</cell></row><row><cell>MTNet + L cdkd (argmax)</cell><cell>68.20±23.42</cell><cell>70.61±21.03</cell><cell>55.46±21.49</cell><cell>58.71±21.23</cell></row><row><cell>MTNet + L cdkd (T =1)</cell><cell>68.22±23.55</cell><cell>70.32±21.67</cell><cell>55.48±21.57</cell><cell>58.45±21.32</cell></row><row><cell>MTNet + L cdkd</cell><cell>68.84±22.89</cell><cell>71.49±20.74</cell><cell>55.92±21.44</cell><cell>59.02±21.13</cell></row><row><cell>MTNet + L cdkd + L um</cell><cell>69.11±23.43</cell><cell>71.56±22.02</cell><cell>56.57±21.49</cell><cell>59.52±22.46</cell></row><row><cell>MTNet + L cdkd + Lum</cell><cell cols="2">69.72±22.06 72.24±21.21</cell><cell cols="2">57.09±21.23 60.17±21.98</cell></row><row><cell cols="2">MTNet(dual) +L cdkd +Lum 69.49±22.42</cell><cell>71.65±20.48</cell><cell>56.96±21.85</cell><cell>59.13±21.10</cell></row><row><cell cols="2">MTNet(csa×3)+L cdkd +Lum 69.24±23.57</cell><cell>71.50±20.54</cell><cell>56.93±22.34</cell><cell>59.04±21.25</cell></row><row><cell cols="2">MTNet(-atten)+L cdkd +Lum 68.92±23.42</cell><cell>71.37±20.68</cell><cell>56.03±22.13</cell><cell>58.81±21.46</cell></row><row><cell cols="2">MTNet(ensb) +L cdkd +Lum 69.66±22.08</cell><cell cols="2">72.25±21.19 57.01±21.25</cell><cell>60.18±21.98</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62271115</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_p5EMZjd">
					<idno type="grant-number">62271115</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Digestpath: a benchmark dataset with challenge review for the pathological detection and segmentation of digestive-system</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Da</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102485</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inf-Net: automatic covid-19 lung infection segmentation from CT images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2626" to="2637" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dual adaptive pyramid network for cross-stain histopathology image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-812" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised histological image segmentation via hierarchical consistency enforcement</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-71" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised medical image segmentation via cross teaching between CNN and transformer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="820" to="833" />
		</imprint>
		<respStmt>
			<orgName>MIDL</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation via uncertainty rectified pyramid consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102517</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12674" to="12684" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recalibrating fully convolutional networks with spatial and channel &quot;squeeze and excitation&quot; blocks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="540" to="549" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep active learning for breast cancer segmentation on immunohistochemistry images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-1" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NeurIPS</publisher>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Advent: adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cbam: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cross-patch dense contrastive learning for semi-supervised segmentation of cellular nuclei in histopathologic images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11666" to="11675" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">R-drop: regularized dropout for neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10890" to="10905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mutual consistency learning for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102530</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep segmentation-emendation model for gland instance segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-752" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pairwise relation learning for semi-supervised gland segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_40</idno>
		<idno>978-3-030-59722-1 40</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="417" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature normalized knowledge distillation for image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58595-2_40</idno>
		<idno>978-3-030-58595-2 40</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12370</biblScope>
			<biblScope unit="page" from="664" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-867" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Decoupled knowledge distillation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11953" to="11962" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
