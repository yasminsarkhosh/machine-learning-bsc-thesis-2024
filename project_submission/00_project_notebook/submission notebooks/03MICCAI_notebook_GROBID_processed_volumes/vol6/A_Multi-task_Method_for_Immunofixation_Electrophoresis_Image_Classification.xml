<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medical Image Computing and Computer Assisted Intervention – MICCAI 2023</title>
				<funder ref="#_NwA6aXn #_JSqMDuW #_aBEwkwZ">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_JhCN2Xk">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui-Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wen-Qi</forename><surname>Shao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Laboratory Medicine</orgName>
								<orgName type="institution" key="instit1">Zhongshan Hospital</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin-Cen</forename><surname>Duan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Laboratory Medicine</orgName>
								<orgName type="institution" key="instit1">Zhongshan Hospital</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">De-Chuan</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bai-Shen</forename><surname>Pan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Laboratory Medicine</orgName>
								<orgName type="institution" key="instit1">Zhongshan Hospital</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bei-Li</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Laboratory Medicine</orgName>
								<orgName type="institution" key="instit1">Zhongshan Hospital</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Laboratory Medicine</orgName>
								<orgName type="institution" key="instit1">Zhongshan Hospital</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main"></title>
					</analytic>
					<monogr>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B98AF35E9F6BB9AB3C78C26918A95DF2</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>IFE image classification</term>
					<term>Multi-task learning</term>
					<term>Task-related regularization</term>
					<term>Class imbalance</term>
					<term>Expert knowledge</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the field of plasma cell disorders diagnosis, the detection of abnormal monoclonal (M) proteins through Immunofixation Electrophoresis (IFE) is a widely accepted practice. However, the classification of IFE images into nine distinct categories is a complex task due to the significant class imbalance problem. To address this challenge, a two-sub-task classification approach is proposed, which divides the classification task into the determination of severe and mild cases, followed by their combination to produce the final result. This strategy is based on the expert understanding that the nine classes are different combinations of severe and mild cases. Additionally, the examination of the dense band co-location on the electrophoresis lane and other lanes is crucial in the expert evaluation of the image class. To incorporate this expert knowledge into the model training, inner-task and inter-task regularization is introduced. The effectiveness of the proposed method is demonstrated through experiments conducted on approximately 15,000 IFE images, resulting in interpretable visualization outcomes that are in alignment with expert expectations. Codes are available at https:// github.com/shiy19/IFE-classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The utilization of immunofixation electrophoresis (IFE) as a laboratory technique has been widely adopted for the identification and characterization of abnormal proteins in blood or urine specimens <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>. This technique is commonly employed to detect monoclonal gammopathy, a condition characterized by the presence of deviant monoclonal (M) proteins, which can aid in the diagnosis of multiple myeloma, Waldenstrom's macroglobulinemia, and other plasma cell disorders <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>. The electrophoresis process separates serum proteins into distinct lanes, which are then treated with specific antisera against 'IgG', 'IgA', 'IgM', 'κ', and 'λ'. Precipitin bands indicative of abnormal M-proteins exhibit greater density and darkness than their normal counterparts. The electrophoresis (ELP) lane, which represents a mixture of multiple proteins, serves as a reference for the recognition of various M-proteins. For human experts, identification of an M-protein is achieved by observing the co-location of bands between the ELP lane, heavy chain lanes ('G', 'A', and 'M'), and light chain lanes ('κ' and 'λ'). For example, as shown in the top of Fig. <ref type="figure" target="#fig_0">1</ref>(a), if the dense (dark) band of the ELP lane can be aligned with that of the heavy chain lane 'G' and the light chain lane 'κ', this sample can be identified as 'IgG-κ positive'. In the same way, we can identify the other two samples in Fig. <ref type="figure" target="#fig_0">1</ref>(a) as 'λ positive' and 'Negative'. Recently, the application of machine learning for image recognition has gained attention as a means of handling large-scale samples and providing objective and quality results <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>. While machine learning has been successfully applied to a wide range of medical images, such as Computed Tomography (CT) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, histopathological images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>, magnetic resonance images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>, ultrasound images <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>, and mammograms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>, its application to IFE presents unique challenges. IFE images encompass nine distinct yet interrelated classes, including 'Negative', two types of positive with mild cases ('κ', 'λ'), and six types of positive with both severe and mild cases ('IgG-κ', 'IgG-λ', 'IgA-κ', 'IgA-λ', 'IgM-κ', 'IgM-λ') <ref type="foot" target="#foot_0">1</ref> , which can be identified as different combinations of three severe cases ('IgG', 'IgA', and 'IgM') and two mild cases ('κ' and 'λ'). The class imbalance phenomenon <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref> is common in IFE images, as demonstrated by our dataset collected from a large hospital, where the predominant class 'Negative' comprises over 10,000 samples, while the minor class 'κ positive' has only 63 samples. Conventional multi-class classification approaches struggle to accurately learn the features of minor classes, leading to subpar performance, particularly for minor classes. Previous studies have approached IFE image classification as a multi-class problem, such as the two-stage strategy proposed by Hu et al. <ref type="bibr" target="#b5">[6]</ref> which first classifies samples into positive and negative categories, followed by classification into different positive categories. Wei et al. <ref type="bibr" target="#b23">[24]</ref> employed a dynamic programming based method to segment IFE images into lanes and strips, calculating the similarity between them as inputs to the model. However, these approaches necessitate additional processing or complex data preprocessing, and they overlook the class imbalance problem.</p><p>Inspired by the expert knowledge that all the nine classes are different combinations of severe cases and mild cases, we propose a novel end-to-end multi-task learning method for IFE image classification, as depicted in Fig. <ref type="figure" target="#fig_0">1(c</ref>). Our approach employs a decompose-and-fuse strategy, where the classification task is first divided into two sub-tasks: one for the classification of severe cases, and the other for mild cases. This decomposition effectively mitigates the class imbalance present in the original multi-class problem. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), while 'IgG-κ positive', 'IgA-κ positive', 'IgM-κ positive', and 'κ positive' are considered minor classes compared to class 'Negative' in the original task, they all contribute to 'κ'. Thus, in the sub-task that classifies mild cases, class 'κ' has a higher number of samples, reducing the class imbalance issue. The same holds for the sub-task that categorizes severe cases. Due to the reduced class imbalance in the two subtasks, improved performance can be achieved. The results from the two sub-tasks are then combined to produce good performance in the original 9-class classification problem. Additionally, we have integrated an attention module into the convolution layers at the height level of the image to enhance the model's focus on relevant image features and improve visualization and explanation. Drawing upon expert knowledge, we have implemented two forms of regularization to more accurately model the relationships within and between tasks, as depicted in Fig. <ref type="figure" target="#fig_1">2(b,</ref><ref type="figure">c</ref>). The inner-task regularization smoothens the attention map of negative samples and sharpens that of positive samples, which aligns with the expert understanding that only positive samples exhibit co-located dense bands. For a sample with both severe and mild cases simultaneously (namely S&amp;M-sample), the inter-task regularization reduces the gap between its attention maps from the two sub-tasks, which corresponds to the expert knowledge that dense bands co-exist on both the severe lane and the mild lane for a S&amp;M-sample.</p><p>In summary, our contributions are three-fold: <ref type="bibr" target="#b0">(1)</ref> we propose an end-toend multi-task method for IFE image classification that effectively addresses the class imbalance issue, (2) we implement two types of regularization mechanisms to more accurately model relationships within and between tasks, and (3) our extensive experiments and visualization demonstrate the effectiveness and explainability of our proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we describe the methodology for the development of a novel multi-task learning approach for the classification of IFE images. Based on the two-sub-task framework, our approach employs both within-task and betweentask regularization on the attention module to address several critical challenges, such as alleviating class imbalance and modeling different patterns of severe and mild cases, which can incorporate expert knowledge into the model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-task Framework</head><p>Directly addressing the classification of IFE images as a multi-class (9-class) problem may face a severe class imbalance phenomenon, which makes the classification of IFE images a challenging task. Inspired by expert knowledge, we decompose the multi-class problem into two sub-tasks to mitigate this issue.</p><p>The first sub-task is to classify heavy lanes into four classes ('None', 'G', 'A', and 'M'), while the second is to classify light lanes into three classes ('None', 'κ', and 'λ'). If a sample does not possess the corresponding dense bands in its heavy or light lane that match with the ELP lane, it is classified as 'None'. This decomposition approach ensures that different minor classes of the original 9-class task that shares the same severe or mild cases are combined into larger minor classes for the two sub-tasks, which can be proven by the histogram in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. Define imbalance ratio (IR) as follows:</p><formula xml:id="formula_0">IR = N min /N max<label>(1)</label></formula><p>Here, N min and N max represent minority class size and majority class size, respectively. In reviewing our dataset, the initial task demonstrates an IR of 0.06, whereas the two sub-tasks exhibit an IR of 0.188 and 0.042, respectively. Utilizing the metric of IR provides an academically rigorous, quantitative methodology to discern the mitigation of the class imbalance issue within the given dataset.</p><p>The final classification result is obtained by combining the prediction results from two sub-tasks. As demonstrated in Fig. <ref type="figure" target="#fig_0">1(c</ref>), when two sub-tasks yield predictions of 'G' and 'κ', the final output is identified as 'IgG-κ positive'. In each sub-task, the ResNet18 architecture <ref type="bibr" target="#b4">[5]</ref> is employed as the feature extractor for raw images, while a fully connected layer serves as the classifier. To capture the relationship between the two sub-tasks, and to ensure that they both learn the basic features of the raw image, such as the dense band in the lane, the first three blocks of the ResNet18 model are shared between the two sub-tasks. This sharing of blocks was found to provide the best performance empirically, and further details can be found in the supplementary material. Formally speaking, for an instance x i ∈ R D , we split its ground-truth label y i ∈ S(9) into y is ∈ S(4) and y im ∈ S(3). Here, S(k) means the set of {1, 2, • • • , k}, and subscripts s and m represent the task that classifies severe cases and mild cases, respectively. Let φ(•) and h(•) represent the feature extractor and the classifier, respectively. The logit vectors of a sample x i in two sub-tasks are obtained with fis = h s (φ s (x i )) and fim = h m (φ m (x i )). The predicted label for each sub-task is then obtained by taking the maximum value from the corresponding logit vector. φ s and h s (φ m and h m ) of the sub-task are trained with cross-entropy loss L s CE (L m CE ), which minimizes the discrepancy between the posterior class probability with the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task-Related Regularization</head><p>Horizontal Attention. As depicted in Fig. <ref type="figure" target="#fig_0">1(a)</ref>, human experts identify the valid part of an IFE image as a horizontal rectangular region that displays the alignment of dense bands between the ELP lane and the other lanes. To encourage the model to focus on this area, an attention mechanism has been integrated into the convolutional layer of the proposed method. Attention mechanisms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref> are widely used in Convolutional Neural Network (CNN) models to focus on specific channels or spatial regions. However, for IFE image classification, only attention to the horizontal direction is required. The horizontal attention mechanism used in this study differs from conventional spatial attention in that it only focuses on a 1D direction (the horizontal direction). To calculate the horizontal attention, average-pooling AvgPool(F) and max-pooling MaxPool(F) are applied along the channel axis and the width axis, followed by a standard convolution layer and a sigmoid function σ to generate a horizontal attention map M(F) ∈ R H :M(F) = σ (Conv([AvgPool(F); MaxPool(F)])). Here, F represents the feature vector, and H is the height of the image. Compared to spatial attention, this attention map encodes the horizontal region to be emphasized, enabling the model to better capture the relationship between the dense bands in different lanes. Furthermore, the horizontal attention mechanism provides a convenient way to visualize the model results, making it more interpretable, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner-Task Regularization.</head><p>To effectively model the patterns in different samples, it is crucial to understand the desired appearance of the attention map for positive and negative samples. As previously discussed, for positive samples, the model should emphasize the horizontal region that displays the alignment of dense bands. However, obtaining human-labeled attention areas for the training phase is a time-consuming and labor-intensive task. To address this issue, the attention map is differentiated between positive and negative samples. The goal is to sharpen the attention map for positive samples and smoothen it for negative samples. The attention map M(F) can be viewed as the distribution of attention intensity along the height dimension of the image, as depicted in Fig. <ref type="figure" target="#fig_1">2(b</ref>). The distribution is adjusted to be sharper or flatter by applying a Softmax operator with a temperature parameter τ : M(F) = Softmax (M(F)/τ ). The inner-task regularization Reg s inner and Reg m inner can be designed using the Kullback-Leibler divergence (KLdiv) <ref type="bibr" target="#b9">[10]</ref> as follows:</p><formula xml:id="formula_1">Reg s inner = KLdiv(M s (F s ) • M s (F s )) Reg m inner = KLdiv(M m (F m ) • M m (F m )) . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Here, KLdiv(A • B) = A • (log Alog B). The inner-task attention mechanism allows the model to differentiate the attention map between positive and negative samples.</p><p>Inter-Task Regularization. It is important to note that the alignment of dense bands between heavy lanes, light lanes, and the ELP lane is a crucial factor in the identification of S&amp;M-samples by human experts. To model this relationship in the proposed method, the two sub-tasks are encouraged to emphasize similar horizontal areas. Specifically, for all S&amp;M-samples, the inter-task regularization can be designed with the Jensen-Shannon Divergence (JSdiv) <ref type="bibr" target="#b13">[14]</ref> as follows :</p><formula xml:id="formula_3">Reg inter = JSdiv(M s (F s ) • M m (F m ))<label>(3)</label></formula><p>Here, JSdiv(A</p><formula xml:id="formula_4">• B) = (KLdiv(A • C) + KLdiv(B • C)) /2, and C = (A + B)/2.</formula><p>The inter-task regularizer enables the model to better capture the patterns of S&amp;M-samples by modeling the relationship between the two sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Overall Objective</head><p>The overall optimization objective for the proposed method can be expressed as:</p><formula xml:id="formula_5">L s CE + L m CE + λ inner • ( Reg s inner + Reg m inner ) + λ inter • Reg inter . (4)</formula><p>Here, λ inner and λ inter are hyper-parameters.</p><p>In conclusion, the proposed multi-task method mitigates the class imbalance problem in the classification of IFE images by decomposing the task into two separate sub-tasks. The use of inner-task and inter-task regularization on the horizontal attention mechanism allows the model to capture the relationships between sub-tasks and learn discriminative patterns for both positive and negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>Dataset. We introduce a new IFE image dataset, which comprises 14540 images along with their respective protein presence class labels. As defined in Eq. 1, the dataset exhibits an imbalance ratio of 0.006, which underscores a pronounced instance of class imbalance within the dataset. Our dataset is collected from a large hospital between January 2020 and December 2022, and it reflects the realworld environment, as the images usually contain certain types of noise, such as left and right offset of lanes and non-pure white background color. Unlike previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref> that perform preprocessing operations, such as cutting the images into different horizontal or vertical pieces, the raw images are utilized in the proposed method, resulting in an end-to-end approach. The training and test sets are stratified based on a 4:1 ratio, and the dataset characteristics can be found in the supplementary material. The images are resized to 224 × 224 as the input for the model.  Implementation Details. All models are implemented using the PyTorch framework and trained on an NVIDIA GeforceRTX 2080ti GPU with 11 GB memory. The network is trained for 100 epochs with a batch size of 64, and the optimization is performed using Stochastic Gradient Descent (SGD) with momentum as the optimizer. The initial learning rate is set to 0.03 for the first 5 epochs and then cosine-decayed for the remaining epochs. The training stage takes 1.2 h on one GPU and occupies 4G display memory. The hyper-parameters of regularization are tuned based on cross-validation on the training set, with the best hyper-parameters set to λ inner = 0.3, τ = 5, λ inter = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metric and Comparison</head><p>Methods. We assess the efficacy via a suite of evaluation metrics, including accuracy (acc), F1-score, false negative rate (fnr), top-1 minor class accuracy (M1-Acc), and top-3 minor class accuracy (M3-Acc). These metrics are widely adopted for classification problems, with the false negative rate being of particular significance in the medical domain as it quantifies the likelihood of misidentifying positive samples as negative. The top-1 and top-3 minor class accuracy metrics evaluate the model's performance on underrepresented classes. A ResNet18-based multi-class classification approach is selected as the baseline and tuned using an identical range of hyper-parameters as the proposed method. The baseline is trained for 100 epochs with a batch size of 64, an initial learning rate of 0.1, and a decay scheduler consistent with the proposed method. Additionally, the proposed method is compared against Two Stage DNN <ref type="bibr" target="#b5">[6]</ref> and Collocative Net <ref type="bibr" target="#b23">[24]</ref>, the state-of-the-art methods in the IFE recognition domain, using our dataset. The results are obtained by adhering to the code released by the authors. The dataset is preprocessed manually prior to network training in accordance with the methodology described in the respective papers.</p><p>Performance Evaluation. The methodology proposed in this study is thoroughly evaluated and compared to both the baseline method and other stateof-the-art techniques on the IFE dataset, as presented in Table <ref type="table" target="#tab_0">1</ref>. The results reveal that our method outperforms all other methods in terms of all evaluation metrics, particularly with respect to top1 and top3 minor class accuracy, thus highlighting its efficacy in addressing the class imbalance problem and achieving improved performance for underrepresented classes. Our empirical analysis demonstrates that Collocative Net <ref type="bibr" target="#b23">[24]</ref> and Two Stage DNN <ref type="bibr" target="#b5">[6]</ref> exhibit inferior performance compared to the conventional multi-class approach. This can be attributed to the excessive manual intervention employed during preprocessing and model development, which is not suitable for handling real-world IFE datasets containing various forms of noise, such as lane offsets and impurity pixels in the background.</p><p>Visualization. The proposed horizontal attention mechanism provides a useful tool for verifying the consistency of model-learned knowledge with expert experience. As illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>, the areas with the highest attention intensity are highlighted in orange, while red boxes indicate the judgment areas of our experts. The results show that the attention regions of positive samples largely overlap with the expert judgment areas, while the attention regions of negative samples are more dispersed, indicating a lack of specific areas of interest. These findings demonstrate that the knowledge learned by our model is consistent with expert experience.</p><p>Ablation Study. An ablation study is conducted to examine the contribution of key components of the proposed model, including the multi-task framework and two regularization methods. The results, presented in Table <ref type="table" target="#tab_1">2</ref>, indicate that the use of the multi-task framework results in significant performance gains, particularly for minority classes, and that the introduction of the two regularization methods further improves the overall performance of the model. Further exploration of the effects of different hyper-parameters can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we investigate the issue of classifying immunofixation electrophoresis (IFE) images and contend that conventional multi-class classification strategies may not be adequate due to the considerable class imbalance issue. To address this issue, we introduce a multi-task framework that leverages expert knowledge. Our method effectively captures different patterns of both severe and mild cases through the use of inner-task and inter-task regularization. The empirical evaluation of our method, conducted on a dataset of approximately 15,000 IFE images, demonstrates its superiority over other approaches. The results also provide interpretable visualizations that align with expert annotations and highlight the potential for improved accuracy and reliability in the classification of IFE images. This development has the potential to significantly advance the diagnostic capabilities of monoclonal protein disorders in the medical field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Three kinds of IFE images: 'IgG-κ positive' (severe case and mild case simultaneously), 'λ positive' (only mild case), and 'Negative'. The area in the red box is the judgment area marked by our experts, and the area in the green box is the dense band. (b) An illustration of vanilla multi-class framework, which directly performs a 9-class classification for IFE images. (c) An illustration of our proposed multi-task framework, which splits the task into two sub-tasks: Ts and Tm to deal with the classification of severe cases and mild cases, respectively. Ts and Tm can focus on corresponding areas, as shown in the orange boxes. φ and h represent feature extractors and classifiers, respectively. (Color figure online)</figDesc><graphic coords="2,55,98,256,10,340,30,185,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Histogram of class numbers for the original multi-class task and our sub-task that classifies mild cases. (b) An illustration of our proposed inner-task regularizer. We sharpen/smoothen the attention map of positive/negative samples first, and minimize its KL-divergence with the original attention map. (c) An illustration of our proposed inter-task regularizer. For positive samples with severe and mild case simultaneously, we minimize the JS-divergence between the attention maps of sub-task Ts and sub-task Tm.</figDesc><graphic coords="4,55,98,207,62,340,18,86,53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of horizontal attention maps on different types of IFE images. The orange areas are the most attention part learned by our model, and the areas in red boxes are the judgment areas marked by our experts. (Color figure online)</figDesc><graphic coords="9,69,81,54,02,284,74,72,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on IFE dataset. The best performances are in bold.</figDesc><table><row><cell>Methods</cell><cell>Acc (%)</cell><cell cols="2">F1-score(%) Fnr (%)</cell><cell cols="2">M1-Acc (%) M3-Acc (%)</cell></row><row><cell>Multi-class</cell><cell>95.12 ± 0.04</cell><cell>85.16 ± 0.60</cell><cell cols="2">10.10 ± 0.41 88.46 ± 3.84</cell><cell>73.83 ± 2.04</cell></row><row><cell cols="2">Collocative Net [24] 93.07 ± 0.05</cell><cell>81.80 ± 0.08</cell><cell cols="2">14.22 ± 0.48 80.77 ± 3.84</cell><cell>59.97 ± 0.65</cell></row><row><cell cols="2">Two stage DNN [6] 92.54 ± 0.63</cell><cell>75.68 ± 0.96</cell><cell>9.31 ± 0.60</cell><cell>42.30 ± 3.85</cell><cell>42.03 ± 0.64</cell></row><row><cell>Our Method</cell><cell cols="5">95.43 ± 0.10 86.68 ± 0.24 8.54 ± 0.18 92.31 ± 0.00 76.10 ± 1.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the multi-task framework and two regularization.</figDesc><table><row><cell cols="4">Multi-task Reg inner Reg inter Acc (%)</cell><cell cols="2">F1-score (%) Fnr (%)</cell><cell>M1-Acc (%)</cell><cell>M3-Acc (%)</cell></row><row><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>95.12 ± 0.04</cell><cell>85.16 ± 0.60</cell><cell cols="2">10.10 ± 0.41 88.46 ± 3.84</cell><cell>73.83 ± 2.04</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>95.23 ± 0.01</cell><cell>85.79 ± 0.35</cell><cell>9.80 ± 0.12</cell><cell>92.31 ± 0.00</cell><cell>73.60 ± 0.46</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>95.37 ± 0.05</cell><cell>86.16 ± 0.15</cell><cell>9.14 ± 0.42</cell><cell>92.31 ± 0.00</cell><cell>75.99 ± 1.39</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>95.24 ± 0.08</cell><cell>85.70 ± 0.85</cell><cell>9.26 ± 0.06</cell><cell>88.46 ± 3.84</cell><cell>75.00 ± 0.01</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell cols="5">95.43 ± 0.10 86.68 ± 0.24 8.54 ± 0.18 92.31 ± 0.00 76.10 ± 1.28</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Samples with multiple positives are rare, e.g., with 'IgG-κ' and 'IgM-λ' simultaneously. These samples are not considered in this study.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was supported by <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2020AAA0109401</rs>), <rs type="funder">NSFC</rs> (<rs type="grantNumber">62176117</rs>, <rs type="grantNumber">61921006</rs>, <rs type="grantNumber">62006112</rs>), <rs type="institution">Collaborative Innovation Center of Novel Software Technology and Industrialization</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JhCN2Xk">
					<idno type="grant-number">2020AAA0109401</idno>
				</org>
				<org type="funding" xml:id="_NwA6aXn">
					<idno type="grant-number">62176117</idno>
				</org>
				<org type="funding" xml:id="_JSqMDuW">
					<idno type="grant-number">61921006</idno>
				</org>
				<org type="funding" xml:id="_aBEwkwZ">
					<idno type="grant-number">62006112</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Multi-task Method for Immunofixation Electrophoresis Image Classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Abd Elrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Network Innovative Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="332" to="340" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diagnosis and management of multiple myeloma: a review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">327</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="464" to="477" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-scale super-resolution magnetic resonance spectroscopic imaging with adjustable sharpness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-039" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BMD-GAN: bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-061" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="644" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Expert-level immunofixation electrophoresis image recognition based on explainable and generalizable deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Chem</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="130" to="139" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual deep learning-based explanation for Neuritic plaques segmentation in Alzheimer&apos;s disease using weakly annotated whole slide histopathological images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jimenez</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-733" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40537-019-0192-5</idno>
		<ptr target="https://doi.org/10.1186/s40537-019-0192-5" />
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kullback-leibler divergence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Joyce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Encyclopedia of Statistical Science</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="720" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Keren</surname></persName>
		</author>
		<title level="m">High-Resolution Electrophoresis and Immunofixation: Techniques and Interpretation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Autofocusing+: noise-resilient motion correction in magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kuzmina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Y</forename><surname>Rogov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adalsteinsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_35</idno>
		<idno>978-3-031-16446-0 35</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="365" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for detection of inner ear abnormal anatomy in computed tomography</title>
		<author>
			<persName><forename type="first">P</forename><surname>López Diez</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-867" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The jensen-shannon divergence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Menéndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Franklin Inst</title>
		<imprint>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="307" to="318" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Protein characterization using electrophoresis and immunofixation; a case-based review of dogs and cats</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Avery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Veterinary Clin. Pathol</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Treatment of relapsed and refractory multiple myeloma: recommendations from the international myeloma working group</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Oncol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="e118" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sensor geometry generalization to untrained conditions in quantitative ultrasound imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Bae</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_74</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-074" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="780" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiple myeloma current treatment algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blood Cancer J</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">94</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Digestive organ recognition in video capsule endoscopy based on temporal segmentation network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-114" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="136" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end cell recognition by point annotation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-811" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Leveraging multi-task learning to cope with poor and missing labels of mammograms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Radiol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">796078</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Key-frame guided network for thyroid nodule recognition using ultrasound videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-823" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep collocative learning for immunofixation electrophoresis image analysis</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1898" to="1910" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiple myeloma: detection of free monoclonal light chains by modified immunofixation electrophoresis with antisera against free light chains</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wilhite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Bollag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pract. Lab. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">256</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Laboratory testing for monoclonal gammopathies: focus on monoclonal gammopathy of undetermined significance and smoldering multiple myeloma</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Willrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Biochem</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="38" to="47" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An end-to-end combinatorial optimization method for r-band chromosome recognition with grouping guided attention</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-81" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2023</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Uncertainty-aware cascade network for ultrasound image segmentation with ambiguous boundary</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-826" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mammographic image classification with deep fusion learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14361</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised PR virtual staining for breast histopathological images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
