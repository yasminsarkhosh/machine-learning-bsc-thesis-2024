<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery</title>
				<funder>
					<orgName type="full">Carl Zeiss Meditec</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><surname>Sommersperger</surname></persName>
							<email>michael.sommersperger@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shervin</forename><surname>Dehghani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philipp</forename><surname>Matten</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kristina</forename><surname>Mach</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">Ali</forename><surname>Nasseri</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Klinikum Rechts der Isar</orgName>
								<address>
									<settlement>Augenklinik, Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hessam</forename><surname>Roodaki</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Carl Zeiss Meditec AG</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ulrich</forename><surname>Eck</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="408" to="417"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9F9D608EE3C3143E623F70AE3D564661</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Optical Coherence Tomography</term>
					<term>Real-Time Visualization</term>
					<term>Visual Perception</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Swept-Source Optical Coherence Tomography (SS-OCT) integrated with surgical microscopes has enabled fast, high-resolution, and volumetric visualization of delicate tissue-instrument interactions. However, some visual features, which provide essential perceptual information in microscopic surgery, are not present in 4D OCT. Such a feature is the shadow of the surgical instruments cast onto the retina by the endo-illumination probe, which is among the most important cognitive cues for perceptual distance estimation. In this work, we propose Semantic Virtual Shadows (SVS), a novel concept to artificially generate instrument-specific shadows in OCT volumes, enabling naturally non-existent but important perceptual cues that are present in microscopic surgery. Semantic scene information is leveraged by considering only voxels associated with shadow-casting and shadow-receiving objects, identified using a learning-based approach and efficient volume processing, respectively. Real-time performance is achieved by a precomputed semantic shadow volume texture that assigns a shadowing factor to each voxel associated with a shadow-receiving object. The novelty of the method includes not only instrument-specific shadowing on the surface anatomy but also exclusively on deep-seated subsurface structures, providing advantages for various vitreoretinal procedures. Our user study indicates the benefits of the method for 4D OCT-guided surgery in several cognitive and performance-specific aspects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Navigating surgical instruments in vitreoretinal procedures requires extreme manual dexterity. Surgeons need to manipulate fine structures, such as an Epiretinal Membrane (ERM), which can measure up to only 60µm <ref type="bibr" target="#b15">[16]</ref>, or carefully placing a microsurgical cannula within the on average 250µm <ref type="bibr" target="#b5">[6]</ref> thick retina for intra or subretinal injections.</p><p>Accurate distance perception regarding the surgical instruments and the retina is of utmost importance for punctuated surgical action. Any unintentional contact with the retina could cause severe damage to important retinal cells. In conventional procedures, surgeons rely only on visual guidance from an operating microscope that only allows a top view. An endo-illumination probe is inserted into the eye to visualize the operating area. When surgical instruments are introduced, they cast a shadow onto the retina, relative to the position of the illumination probe. Such shadows provide one of the most essential cognitive cues to perceive the distance between the instrument and the retina and therefore are important for performing surgical action. Figure <ref type="figure" target="#fig_0">1</ref> shows frames of a video sequence during retinal membrane peeling. While the instrument approaches the retina, the distance to its shadow gradually decreases until the instrument tip coincides with the tip of its shadow at the retina.</p><p>Besides performing a procedure through this conventional stereoscopic view, recently, other imaging modalities have also become available. Technical advances have led to the integration of Swept-Source Optical Coherence Tomography (SS-OCT) into surgical microscopes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7]</ref>, enabling near video-rate, depth-resolved imaging. Such 4D OCT systems have the potential to generate advanced visualizations of surgical interactions and improved treatments. While previous studies have demonstrated the feasibility of 4D OCT-guided surgery <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>, some important perceptual cues that are available in the microscopic images are not present in OCT, such as the instrument shadow generated by the illumination probe. The absence of such perceptual cues increases the burden for surgeons to adapt to 4D OCT. While instrument shadows still exist in OCT, they are generated by the OCT laser and occur when the laser beam is blocked by the instrument such that structures below the instrument are fully obscured in the imaging modality. This OCT shadow is fixed in perspective and always occurs directly beneath the instrument. Hence, it does not provide the same intuitive cognitive cues, especially when viewing the volume from a top view, which is the usual view during ophthalmic surgery. To enable perceptually similar features as generated by the illumination probe, advanced visualization techniques need to be explored. Such visualizations could provide cues that improve the general usability of 4D-OCT without deviating from the standard workflow and steepen the learning curve for surgeons.</p><p>In this paper, we propose Semantic Virtual Shadows (SVS), a concept that integrates such visual cues for perceptual distance estimation in 4D OCT. By identifying shadow-casting and shadow-receiving objects in the OCT volume, we augment the shadow of surgical instruments on the retina. In particular, instrument-specific shadows can be generated in one case on the retinal surface, and in other cases, exclusively on deep-seated layers below the surface. Precomputing a semantic shadow volume texture prior to direct volume rendering (DVR) enables real-time performance and more flexibility in the rendering approach. We demonstrate the versatility of SVS by proposing two visualization approaches for different scenarios in retinal surgery, including retinal membrane peeling and subretinal injection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>So far, only a few works have addressed the integration of perceptual cues into OCT volume rendering algorithms for interactive surgical visualizations in 4D OCT. Among these, approaches to color voxels based on distance information have been explored as a means of conveying spatial perceptual understanding. In an early study <ref type="bibr" target="#b0">[1]</ref>, a color transfer function was applied to each voxel, based on the distance to the center of mass of the volume. In a later work, <ref type="bibr" target="#b13">[14]</ref> proposed a visualization that leverages a perceptually linear color map, which is anchored by a retinal layer segmentation, thus encoding distance information to an anatomical reference depth in a color map. They furthermore employ shadow rays <ref type="bibr" target="#b9">[10]</ref> to enhance surface structures. However, their method is limited to local shadow rays to achieve real-time performance. There are only a few works that deal with the generation of a shadow for distance estimation in 4D OCT. Only in <ref type="bibr" target="#b4">[5]</ref> do the authors propose to define a virtual light direction orthogonal to the OCT A-scans and employ shadow rays to create a shadow projection on an external plane positioned next to the volume. Such visualizations, however, are perceptually similar to 2D lateral volume projections, do not convey correct distance cues in presence of curved structures, and require removing the gaze from the surgical area to perceive instrument distance.</p><p>As opposed to previous works, we integrate an instrument-specific shadow augmented directly in the rendered OCT volume. Hence, our method aims to provide surgeons with familiar cognitive cues for perceptual distance estimation, as described in Sect. 1. Additionally, identifying shadow-casting and shadowreceiving objects allows the generation of instrument shadows on a specific retinal layer below the surface, which is not possible with previous approaches due to the self-shadowing of the retina.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We define a Semantic Virtual Shadow (SVS) as a shadow that is only generated by identified shadow-casting objects and is explicitly cast onto identified shadow-receiving objects. In our application, the primary use case of SVS is to generate visual cues for spatial distance perception. Taking into consideration shadow-casting and shadow-receiving objects as well as a virtual light source, a semantic shadow volume V s can be constructed as a 3D texture that assigns a shadowing factor to each voxel associated with a shadow-receiving object in the OCT volume. This precomputed V s is directly consumed by the direct volume rendering (DVR) to generate the instrument shadow augmentation. An overview of the proposed method is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>While in theory any segmentation method could be employed to identify, we combine efficient volume processing with a learning based approach to approximate the segmentations and achieve the required processing rates for 4D OCT. For our interventional scenarios, we define the instrument as the shadow-casting object. Inspired by <ref type="bibr" target="#b14">[15]</ref>, to segment the instrument we first generate a 2D average projection image along the OCT A-scan direction (Z-axis) using</p><formula xml:id="formula_0">P avg (x, y) = 1 N z I(x, y, z)<label>(1)</label></formula><p>from which A-scans containing parts of the instrument signal can be identified. We train a U-net style <ref type="bibr" target="#b7">[8]</ref> architecture with a ResNet34 <ref type="bibr" target="#b3">[4]</ref> backbone to generate a 2D binary instrument segmentation map M sc . Since the OCT signal is blocked at the instrument surface, the A-scans corresponding to identified pixels in M sc contain only instrument information. Therefore the instrument can be identified by thresholding the voxel intensities of A-scans obtained by M sc . We further define the retinal layer, on which the SVS is generated as the shadow-receiving object. Due to their terrain-like surface properties, the retinal layer surface can be defined by a 2D depth map M sr , indicating the relative surface position for each A-scan in the volume. Further, a distance parameter d r is introduced, defining the extent of the shadow-receiving object. To generate V s , a shadowing factor is calculated for each voxel p associated with a shadow-receiving object.</p><p>Given the position of the virtual light source p l and L(p) = p l -p |p l -p| defining the light direction, V s (p) is calculated with:</p><formula xml:id="formula_1">V s (p) = i (1 -(I(p + i • L(p)) α ) • M sc (p + i • L(p)) if |M sr (p x , p y ) -p z | &lt; d r 1.0 otherwise.</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>sampling volume intensity values I(p) along the light direction through the entire volume. Note that, in our equations, the z axis corresponds to the axial Ascan direction. We precompute V s prior to volume raymarching using a compute shader program, enabling high update rates and flexibility in the design of the DVR algorithm that directly consumes V s . In the following, we propose two specific visualization approaches that integrate SVS according to equation ( <ref type="formula" target="#formula_2">2</ref>) with different shadow-receiving objects, demonstrating the versatility and advantages of the method for different vitreoretinal procedures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shadow Augmentation on Surface Structures</head><p>During surgical phases, in which the instrument is located above the retinal surface, unintentional contact needs to be avoided. By defining the retinal surface as the shadow-receiving object, the SVS generates cognitive cues for surface distance perception. We use the following 2D projection for M sr :</p><formula xml:id="formula_3">P surf (x, y) = min z I(x, y, z) &gt; t s (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where t s = 0.17 is an empirically chosen threshold. We render the OCT volume using classic Phong shading to visualize surface structure details while integrating V s to augment the instrument-specific shadow C(p) = C P hong (p) • V s (p). The top row of Fig. <ref type="figure" target="#fig_2">3</ref> shows frames of a volume sequence with Phong shading, while in the bottom the same volumes are rendered with both Phong shading and SVS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shadow Augmentation on Subsurface Structures</head><p>In intra-and subretinal injection procedures, once reached the retinal surface, surgeons need to carefully guide the cannula to the injection target without damaging the underlying Retinal Pigment Epithelium (RPE) which is a comparably deep-seated layer in OCT. We propose to augment an instrument shadow explicitly on the RPE and visualize the retinal surface semi-transparent. Compared to the previous section, we define the RPE as the shadow-receiving object. Since in OCT imaging, the RPE typically is a hyper-reflective layer <ref type="bibr" target="#b16">[17]</ref>, M sr can be efficiently approximated by:</p><formula xml:id="formula_5">P subsurf (x, y) = arg max z I(x, y, z)<label>(4)</label></formula><p>During volume raymarching, we apply Phong shading models to visualize the instrument and render the retinal surface semi-transparent while preserving surface highlights, as presented in <ref type="bibr" target="#b11">[12]</ref>. As previously, Eq. ( <ref type="formula" target="#formula_3">3</ref>) is used to obtain the surface depth. For the RPE and underlying structures, we integrate V s and apply an RGB color map C RGB (p). During volume raymarching, this leads to the following convention, integrated via a piece-wise linear opacity function:</p><formula xml:id="formula_6">C(p) = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ C P hongT ool (p) if M sc (p) C P hongILM (p) if |P surf (p x , p y ) -p z | &lt; d surf C RGB (p) • V s (p) if P subsurf (p x , p y ) -p z &lt; 0 0 otherwise.<label>(5)</label></formula><p>Here, C P hongT ool (p) and C P hongILM (p) are the Phong shading models for the instrument and the retinal surface, respectively. We refer to Fig. <ref type="figure" target="#fig_3">4</ref> and in particular to our supplementary video demonstrating visualization examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation and Comparative Time Profiling</head><p>Our instrument segmentation network was trained on 2D projection images generated from OCT volumes using Eq. 1. In total 330 OCT volumes of resolution 391 × 391 × 720 (spiral scanning) acquired from a model eye and 160 volumes of resolution 128 × 512 × 1024 (linear scanning) acquired from ex-vivo porcine eyes were used to generate a data set of 3928 images, including data augmentation strategies. The axial projection images were manually labeled by two biomedical engineers. We used Pytorch 1.13 for model training and TensorRT 8.4 for optimization. We implemented our method using C++ and OpenGL 4.6 (Windows 10 with Intel Core i7-8700K @3.7 GHz, Nvidia RTX 3090Ti), employing a compute shader to generate the 2D projection images and V s , and a fragment shader for DVR. Table <ref type="table" target="#tab_0">1</ref> shows the inference times of SVS compared to baseline shadow rays (SR), which were previously integrated for OCT visualization <ref type="bibr" target="#b13">[14]</ref>. For ablation, we also compare SVS to only using a shadow volume buffer without semantic information (SB). The visualization parameters were adjusted in all methods to achieve a similar visual outcome. The volumes were rendered at a resolution of 3840 × 2160. To further demonstrate the potential of SVS to support surgical tasks, our method was integrated into the 4D SS-OCT system presented in <ref type="bibr" target="#b1">[2]</ref> with a volume acquisition rate of 10Hz. Surgical actions with a forceps in a phantom eye model are demonstrated in our supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">User Study</head><p>To evaluate the effect of SVS for distance perception compared to baseline DVR in 4D OCT guided surgery, we conducted a user study with 12 biomedical experts (10 male, 2 female) familiar with OCT. We conducted the study in a virtual environment simulating 4D OCT using the method proposed in <ref type="bibr" target="#b10">[11]</ref>, which was displayed using stereo rendering in a VR (HTC VIVE Pro) headset. A 3D input device (3D Systems Geomagic Touch) was used to control a calibrated virtual surgical tool, where a motion scaling of 4 : 1 was applied to not affect the results by the limitations of manual dexterity. Prior to the study, participants were asked to perform vision tests to ensure normal visual acuity, including stereo vision, contrast vision, and normal visual field. The study data was anonymized and performed in accordance with the declaration of Helsinki.</p><p>Users were asked to familiarize themselves with the interaction in the virtual environment before the study, reducing the impact of a learning curve on the study results. For each trial, we randomly positioned a small target point close to the retina, simulating anatomical structures that could be targeted in a surgical scenario. Participants were asked to move the instrument tip to the target and press a button to confirm the positioning. Each uses performed 10 trials, each in the following three variants: (i) baseline DVR with Phong shading as in <ref type="bibr" target="#b12">[13]</ref>, (ii) (SV S T ool ) treating the instrument as a shadow-casting object and the retinal surface as a shadow-receiving object, and (iii) SV S T oolT arget with both the instrument and the target as shadow-casting objects, as the target represents an anatomical structure that could theoretically be segmented in OCT. We provide examples of the study variants in our supplementary material. During the study, the order of the trial variants and the position of the virtual light source was randomized, while the view onto the retina was fixed at 10 • C from the axial  OCT direction. Over the progression of each trial, we extracted the distance error to the target and to the retina. With baseline DVR, a final mean targeting error of 0.29 mm (±0.18) was achieved, while with SV S T ool and SV S T oolT arget , mean errors of 0.19 mm (±0.15) and 0.12 mm (±0.07) where achieved respectively. After identifying unequal variances in the distributions, a Kruskal-Wallis test obtained significant differences between the methods (p &lt; 0.001). Figure <ref type="figure" target="#fig_4">5</ref> shows the distance error to the target in axial OCT direction measured over the trial progression, along with a 95% confidence interval of the error values.</p><p>With SVS, users approached with faster convergence to the target and less error variance toward the end of the trial. In addition, Fig. <ref type="figure" target="#fig_5">6</ref> shows the results of the NASA-TLX survey. Statistically significant differences (p &lt; 0.001) in all categories could be obtained based on the ANOVA test, while equal variances were found in all categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>The results of our user study indicate that the proposed method is able to generate effective perceptual cues for distance estimation in 4D OCT. This is reflected in improved targeting performance, as well as a high acceptance of the generated perceptual cues. Figure <ref type="figure" target="#fig_4">5</ref> suggests more robust targeting in both SV S T ool and SV S T oolT arget variants, implying higher confidence in the approach. Preliminary qualitative feedback from four clinicians confirmed the intuitiveness of SVS and its strong integration potential without changing the existing clinical workflow. An essential component of SVS is the identification of shadow-casting and shadow-receiving objects in the volume. Compared to previous methods, SVS is able to generate perceptual cues exclusively on subsurface structures. Precomputing a semantic shadow volume texture was shown to achieve processing rates suitable for 4D OCT systems with volume acquisition rates of 24.2Hz <ref type="bibr" target="#b6">[7]</ref>. In the future, prior knowledge of the instrument's geometrical model could be integrated to enable more accurate shadow representations. We also plan to investigate optimal virtual light source positions and envision employing the illumination probe to control the virtual light for more intuitive interactions.</p><p>In conclusion, the proposed SVS augments visual cues that are naturally not present in OCT, but essential in microscopic vitreoretinal surgery. The flexibility of our approach enables object-specific shadow generation not only on surface structures but also exclusively on subsurface structures, supporting various surgical procedures. We provided a general definition of object-specific shadow generation and demonstrated our method on a 4D SS-OCT system for live display. In our user study, SVS was shown to provide intuitive and effective visual cues for targeted instrument maneuvers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A surgical forceps is carefully guided towards the retina (a-b), then grasps (c) and peels (d) a thin membrane from the surface. The instrument shadow cast onto the retina acts as a main visual cue for perceptual distance estimation.</figDesc><graphic coords="2,62,46,54,47,328,00,73,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Method overview: given an OCT volume and a virtual light source position, first shadow-casting and shadow-receiving objects are identified. These allow the generation of a semantic shadow volume texture, which is finally consumed by the DVR.</figDesc><graphic coords="4,70,47,62,36,311,44,145,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of temporal OCT volumes rendered with Phong shading (top) and integrated SVS generating shadows on the retina surface (bottom).</figDesc><graphic coords="5,48,27,257,45,206,92,90,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. SVS on the RPE layer. The retina above the RPE does not generate shadow.</figDesc><graphic coords="5,284,82,258,05,97,00,89,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Depth-specific error over the progression of the trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Task load rating comparing baseline, SV S T ool and SV S T oolT arget .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average inference time in milliseconds comparing SVS, a 3D shadow volume buffer without semantic information (SB) and shadow rays (SR) as baseline.</figDesc><table><row><cell cols="2">Method Filter</cell><cell>Enface</cell><cell cols="2">Depth Map Seg</cell><cell>Shadow</cell><cell>Render</cell><cell>Total</cell></row><row><cell>SVS</cell><cell cols="3">7.7 ± 0.7 0.7 ± 0.13 0.7 ± 0.1</cell><cell cols="3">3.8 ± 0.1 2.3 ± 0.2 13.5 ± 0.2 28.8 ± 0.8</cell></row><row><cell>SB</cell><cell cols="2">7.7 ± 0.7 -</cell><cell>-</cell><cell>-</cell><cell cols="2">51.9 ± 6.8 14.6 ± 0.3 74.3 ± 6.8</cell></row><row><cell>SR</cell><cell cols="2">7.7 ± 0.7 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>331.3 ± 4.2 339.1 ± 4.6</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is partially supported and the data is provided by <rs type="funder">Carl Zeiss Meditec</rs>. The authors wish to thank SynthesEyes (https://syntheseyes.de) for providing the excellent simulation setup for the user study.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_39.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth-based, motion-stabilized colorization of microscope-integrated optical coherence tomography volumes for microscope-independent microsurgery</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Bleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jackson-Atogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Viehland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gabr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Izatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Toth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transl. Vis. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surgical microscope integrated MHz SS-OCT with live volumetric visualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Britten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Optics Express</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="846" to="865" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constant linear velocity spiral scanning for near video rate 4D OCT ophthalmic and surgical imaging with isotropic transverse sampling</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Carrasco-Zevallos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Viehland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Mcnabb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Izatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Optics Express</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">5052</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shadow extension for ray casting enhances volumetric visualization in real-time 4D-OCT</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Hyeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics Commun</title>
		<imprint>
			<biblScope unit="volume">460</biblScope>
			<biblScope unit="page">125237</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diurnal variation of retina thickness measured with time domain and spectral domain optical coherence tomography in healthy subjects</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6497" to="6500" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Live video rate volumetric OCT imaging of the retina with multi-MHz A-scan rates</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Draxinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">213144</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A surgical guidance system for big-bubble deep anterior lamellar keratoplasty</title>
		<author>
			<persName><forename type="first">H</forename><surname>Roodaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Di San Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46720-7_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46720-7_44" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9900</biblScope>
			<biblScope unit="page" from="378" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient shadows for GPU-based</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinrichs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">raycasting</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Surgical scene generation and adversarial networks for physics-based iOCT synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sommersperger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Optics Express</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2414" to="2430" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Methods for real-time feature-guided image fusion of intrasurgical volumetric optical coherence tomography with digital microscopy</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Trout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Optics Express</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3308" to="3326" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhanced volumetric visualization for real time 4d intraoperative ophthalmic swept-source oct</title>
		<author>
			<persName><forename type="first">C</forename><surname>Viehland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Optics Express</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1815" to="1829" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Layer-aware iOCT volume rendering for retinal surgery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nasseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on Visual Computing for Biology and Medicine. The Eurographics Association</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Kozlíková</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Linsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Vázquez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Lawonn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Raidou</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Processing-aware real-time rendering for optimized tissue visualization in intraoperative 4D OCT</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sommersperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nasseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-1_26" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Characterization of epiretinal membranes using optical coherence tomography</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wilkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2142" to="2151" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visible light optical coherence tomography (OCT) quantifies subcellular contributions to outer retinal band 4</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Kho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transl. Vis. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="30" to="30" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
