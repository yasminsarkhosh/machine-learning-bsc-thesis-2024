<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation</title>
				<funder ref="#_Xe7jNVT">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder ref="#_5s8g7Fd">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lennart</forename><surname>Bastian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Derkacz-Bogner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tony</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="57" to="67"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">96F109C3DDE472336F116F8380E3EB2D</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical Scene Understanding</term>
					<term>Context-aware Systems</term>
					<term>Surgical Phase Recognition</term>
					<term>Surgical Data Science</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The digitization of surgical operating rooms (OR) has gained significant traction in the scientific and medical communities. However, existing deep-learning methods for operating room recognition tasks still require substantial quantities of annotated data. In this paper, we introduce a method for weakly-supervised semantic segmentation for surgical operating rooms. Our method operates directly on 4D point cloud sequences from multiple ceiling-mounted RGB-D sensors and requires less than 0.01% of annotated data. This is achieved by incorporating a self-supervised temporal prior, enforcing semantic consistency in 4D point cloud video recordings. We show how refining these priors with learned semantic features can increase segmentation mIoU to 10% above existing works, achieving higher segmentation scores than baselines that use four times the number of labels. Furthermore, the 3D semantic predictions from our method can be projected back into 2D images; we establish that these 2D predictions can be used to improve the performance of existing surgical phase recognition methods. Our method shows promise in automating 3D OR segmentation with a 20 times lower annotation cost than existing methods, demonstrating the potential to improve surgical scene understanding systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automating systems to interpret complex behaviors in surgical operating rooms (OR) has seen a surge of interest in recent years <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b21">20]</ref>. Robot-assisted surgeries Lennart Bastian and Daniel Derkacz-Bogner contributed equally to this work.</p><p>have improved patient outcomes by reducing blood loss, recovery periods, and hospitalization times <ref type="bibr" target="#b29">[26,</ref><ref type="bibr" target="#b30">27]</ref>. For robotic systems to autonomously interact with hospital staff, surgical tools, or patients, they must attain a sophisticated and detailed understanding of a highly complex environment. To achieve this, robotic detection systems must comprehend high-level surgical phases and granular 3D object semantics and interactions <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b27">25]</ref>.</p><p>Recent works have established the necessity of combining data from multiple cameras to obtain better coverage of surgical procedures <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b29">26]</ref>, as frequent occlusions and obstructions due to personnel and medical equipment obscure important events for individual cameras. Furthermore, a metric 3D semantic understanding is crucial for robotic systems operating and interacting with objects in an environment <ref type="bibr" target="#b34">[31]</ref>. In surgical operating rooms, 3D segmentation has previously been approached by fusing semantic RGB predictions from multiple views in 3D under the full supervision of dense 2D labels <ref type="bibr" target="#b16">[16]</ref>.</p><p>However, to adequately represent the distribution of possible events in the surgical domain, large volumes of data must be acquired <ref type="bibr" target="#b30">[27]</ref>. Training deeplearning models for automated recognition tasks thus induces an enormous annotation burden, particularly as the privacy-sensitive nature of such materials can prevent annotation outsourcing. Therefore, the surgical data science community actively seeks methods to alleviate this burden, particularly through means of domain-adaptation <ref type="bibr" target="#b25">[24]</ref>, as well as unsupervised and self-supervised learning <ref type="bibr" target="#b12">[12]</ref>. While progress has been made in surgical workflow recognition, methods for 3D surgical scene understanding still require fine-grained labels <ref type="bibr" target="#b27">[25]</ref>, particularly for semantic segmentation <ref type="bibr" target="#b16">[16]</ref>.</p><p>To this end, we propose SegmentOR, a weakly-supervised indoor semantic segmentation method for 4D multi-view OR datasets. By leveraging the innate temporal consistency of 4D point cloud sequences, we reduce the annotation burden to only a single click per class (about 0.005% of points), decreasing average annotation time per surgical phase from 3 h to 9.6 min while achieving a higher segmentation mIoU than existing methods that use four times the amount of labels. Furthermore, we establish the soundness of semantic predictions from our model for surgical scene understanding by showing that surgical phase recognition performance can be improved using our segmentation predictions as input. Our main contributions can thus be summarized as follows:</p><p>-We propose the first 3D weakly-supervised semantic segmentation method for operating room environments and validate it on a manually annotated dataset of 3D point clouds from real surgical acquisitions. -We demonstrate that various temporal priors can be used to exploit consistency in weakly-supervised semantic segmentation, improving performance to 10% mIoU above baseline methods. -We show that the semantic outputs from our model can improve the performance of downstream surgical phase recognition methods, formally establishing the link between these two previously disjoint tasks.</p><p>-Finally, we release all code and tools, as well as 2577 anonymized and annotated point clouds from the dataset, to advance progress in surgical scene understanding. https://bastianlb.github.io/segmentOR/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Surgical Scene Understanding. Workflow recognition is pivotal for contextual awareness in operating room (OR) intelligent systems. Activity recognition has been achieved for single-frame <ref type="bibr" target="#b30">[27,</ref><ref type="bibr" target="#b32">29]</ref> and multi-view acquisitions <ref type="bibr" target="#b29">[26]</ref>, including laparoscopic views and ceiling-mounted cameras <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.  Modeling OR activities also requires semantic understanding <ref type="bibr" target="#b16">[16]</ref>. Semantic scene graphs offer a detailed approach to surgical procedure modeling <ref type="bibr" target="#b27">[25]</ref>. While future intelligent OR systems will employ semantics, manual labeling is time-consuming. However, recent progress in weakly-supervised semantic segmentation, such as one-thing-one-click (OTOC) <ref type="bibr" target="#b20">[19]</ref>, can decrease this burden, requiring only a single annotation per semantic class <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b33">30]</ref>.</p><p>The effectiveness of such weakly-supervised segmentation methods in dynamic OR environments remains unclear. Unlike static indoor datasets like ScanNet <ref type="bibr" target="#b7">[8]</ref>, dynamic ORs blur the geometric class separation due to humanobject interaction. Moreover, 3D surgical acquisitions, unlike static indoor reconstructions, are fragmented due to static sensor positions and severe occlusions. Imprecise 3D registration or temporal synchronization creates artifacts that further complicate 3D modeling, worsened by dynamic non-rigid movements (see suppl. for examples).</p><p>Temporal Modeling. Optical flow effectively extracts movement from image sequences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>. Self-supervised methods have recently offered scene flow extraction from LiDAR point clouds <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b24">23]</ref>, but few ground-truth flow annotated datasets exist, leading to potential generalization issues <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b23">22]</ref>.</p><p>Few have combined temporal consistency with 3D semantic segmentation for dynamic point cloud sequences outside autonomous driving settings <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b31">28]</ref>. These methods typically rely on dense ground truth labels or require multi-stage label propagation methods and pre-training. Our approach guides temporal label propagation with unsupervised priors, resolving this cold-start problem costeffectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Problem Setting. Given a point cloud</p><formula xml:id="formula_0">X t ∈ R N ×3 in a temporal sequence t ∈ [T ], we seek to predict a semantic label ŷi ∈ Y t for each point p i ∈ X t .</formula><p>In contrast to the supervised setting where dense ground truth labels y i ∈ Y t are available for every point p i , we infer dense semantic labels in an unseen test sequence by training on sparsely annotated point clouds. We refer to this setting as "weakly-supervised", meaning ground truth train annotations consist of a randomly chosen point per class (see Fig. <ref type="figure" target="#fig_1">1b</ref>). This results in an average of 0.005% of ground truth labels compared to full supervision.</p><p>Inspired by recent works, we assume semantic instances in a point cloud X t adhere to geometric boundaries and partition the point cloud into a set of supervoxels S t <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b20">19]</ref>. For S j , S j ∈ S t we have S j ∩ S j = ∅, and ∪ N j=1 S j = X t . This allows us to represent a group of points with a single feature, increasing the level of supervision obtained from a single "click", and enabling efficient label propagation. Due to the sparse nature of the click annotations, most supervoxels in each point cloud are unlabeled (see Fig. <ref type="figure" target="#fig_1">1c</ref>). By propagating learned information into unlabeled supervoxels, the level of supervision can be drastically increased through pseudo-labeling. For clarity, we use indices i for points and j for supervoxels <ref type="bibr" target="#b20">[19]</ref>. Indices t are used to indicate timesteps. Proposed Method. Following the approach of OTOC <ref type="bibr" target="#b20">[19]</ref>, we train a sparse 3D-UNet <ref type="bibr" target="#b4">[5]</ref> F Θ to model the semantic class of each point p i , Y t = F Θ (X t ) with a cross-entropy loss L CE , and an identically structured relation network R Ψ to predict a category specific embedding R t = R Ψ (X t ) with a contrastive loss L Cont . The point-wise embeddings obtained from both networks are accumulated using mean-pooling over each supervoxel S j . The pooled feature embeddings f j , r j can then be used to construct a fully-connected graph G i to propagate labels to unlabeled supervoxels. This is achieved by maximizing the expectation E of unlabeled supervoxels given the complete supervoxel set S:</p><formula xml:id="formula_1">E(Y |S) = j ψ u (y j |S, F Θ ) + j ψ p (y j , y j |S, R Ψ , F Θ ) (1)</formula><p>where ψ u represents the pooled class predictions, and ψ p is a pairwise similarity between supervoxels S j , S j <ref type="bibr" target="#b20">[19]</ref>. By measuring the similarity between supervoxels in this manner, semantic information in the two networks F Θ and R Ψ can be propagated to unlabeled supervoxels iteratively through pseudo-labeling, using a likelihood threshold of, e.g., E(Y |S j ) ≥ 0.90. Setting a high confidence threshold reduces incorrect pseudo-labels, which would negatively impact subsequent training iterations. OTOC <ref type="bibr" target="#b20">[19]</ref> considers all supervoxel pairs in a single static acquisition as candidates during this expansion. An intuitive way to extend the graph propagation in the temporal dimension would be to pool all supervoxels over a pair of frames X t and X t+1 , creating a fully connected graph over both supervoxel sets. We refer to this method as OTOC+T, as the original method uses only spatial context. Aside from being computationally expensive, this naive approach does not consider that the nearest supervoxel in an adjacent timestep is highly likely to describe a similar region in the point cloud.</p><p>To further improve upon this idea, we propose to enforce temporal consistency through the use of a supervoxel matching matrix M [t,t+1] ∈ R m×n where |S t | = m and |S t+1 | = n are the dimensions of the respective supervoxel sets, reducing computational complexity to an additional comparison per supervoxel instead of n as with the OTOC+T. An entry m j,j ∈ M [t,t+1] indicates the probability that supervoxels S j and S j describe a similar region across time steps. Intuitively, this can establish consistency between the pair of point clouds by considering matched supervoxels from a different timestep X t as pseudo-label candidates. To initialize the matching matrix, we explore how nearest neighbor, unsupervised optical <ref type="bibr" target="#b8">[9]</ref> and scene flow <ref type="bibr" target="#b24">[23]</ref> priors can improve temporal pseudo-label propagation (see suppl. sec. 1 for mathematical details).</p><p>After initialization through any of these priors, we propose to update the matching iteratively during training, establishing a link between temporal consistency and semantic understanding. To strengthen this dynamic and account for potentially incorrect matches, we additionally incorporate relation net R Ψ features to refine the supervoxel matching matrix M . Formally, we can define the matching update for a single entry m j,j ∈ M [t,t+1] as follows:</p><formula xml:id="formula_2">p(S j |S j ) = λ p(S j |S j , M [t,t+1] ) + (1 -λ) p(S j |S j , R Ψ )<label>(2)</label></formula><p>The updated matching is the supervoxel with the highest matching probability, i.e., mj,j = arg max Sj ∈Yt+1 p(S j |S j , M [t,t+1] ). We can then additionally regularize the graph propagation (Eq. 1) using the updated matching matrix for the merged supervoxel sets Ŝ = S t ∪ S t+1 :</p><formula xml:id="formula_3">E(Y | Ŝ) = j ψ u (y j | Ŝ, F Θ ) + j ψ p (y j , y j | Ŝ, M [t,t+1] )<label>(3)</label></formula><p>where ψ u describes the probability of S j being assigned label y j based on the prediction f j = F Θ (S j ). ψ p describes the pairwise similarity between two supervoxels additionally based on r j = R Ψ (S j ), mean supervoxel color c j , and mean coordinate p j .</p><formula xml:id="formula_4">ψ u (y j |S j , F Θ ) = -log P (y j |S j , F Θ ) (4) ψ p (y j , y j |S j , M [t,t+1] ) = m j,j • e {-cj -c j 2 2 -pj -p j 2 2 -fj -f j 2 2 -rj -r j 2 2 } (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Dataset Description. All experiments are carried out on an existing dataset of 18 laparoscopic surgeries <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. The full dataset consists of RGB-D video acquisitions from four co-registered ceiling-mounted Azure Kinect cameras, containing 582,000 images per camera, with video annotations for 8 surgical workflow phases. We uniformly split a subset of the fused point cloud data into training and validation sets, ensuring similar distributions of non-overlapping surgeries, camera calibrations, and class distributions. We use 1500 point clouds for training and 1077 for validation. We additionally manually annotate these point clouds with segmentation labels covering 12 medically relevant classes.</p><p>Each training split contains around 500 sparsely annotated point clouds, with 5436 click annotations on average per split. The densely labeled validation annotations comprise approximately 93% of the on average one million points per point cloud. To reduce the bias from a subjective annotation, we employed four different data annotators for a total annotation time of ∼115 h. To measure the robustness of our method, we split the train and validation sets into 3 nonoverlapping splits, referring to this as "cross-validation" despite the train and validation splits having different types of labels (sparse click and dense, respectively). For more details on data annotation and splits, as well as qualitative examples, please refer to the supplementary materials.</p><p>Experimental Setup. In contrast to OTOC <ref type="bibr" target="#b20">[19]</ref>, which relies on additional ground truth instance segmentation as input <ref type="bibr" target="#b7">[8]</ref>, we use a heuristic oversegmentation approach to generate supervoxels <ref type="bibr" target="#b17">[17]</ref>. All experiments use the same hyperparameters unless otherwise noted. The relation and feature networks were pre-trained on ScanNet <ref type="bibr" target="#b7">[8]</ref>. We then fine-tune on our dataset for four iterations, with pseudo-label propagation occurring after each iteration. We report standardized segmentation metrics. The average training time on an NVIDIA A40 GPU was 7.45 h, using PyTorch 1.13.1 with CUDA 11.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Experiment 1: Baseline Comparisons and Color Ablation. To quantify the overall impact of temporal guidance in the weakly-supervised setting, we compare SegmentOR with the baseline OTOC <ref type="bibr" target="#b20">[19]</ref> over three-fold crossvalidation. As color information may be unavailable in OR datasets due to privacy concerns, we perform additional experiments without RGB features to contextualize the performance in such a setting. Furthermore, we explore how different unsupervised priors can improve semantic predictions, namely (i) nearest neighbor matching, (ii) optical flow matching, and (iii) scene flow matching. For each prior, the matching matrix M [t,t+1] is then initialized based on obtained  flow. Interestingly, initializing the matching matrix with the nearest neighbor prior outperformed optical and scene flow initialization. Using any single prior resulted in improvements over OTOC+T. We refer to the supplementary materials for a detailed description and ablation of the flow priors.</p><p>Results: Table <ref type="table" target="#tab_0">1</ref> shows that SegmentOR consistently outperforms the baseline both with and without RGB features when initialized with a nearest neighbor temporal prior and a learned update according to Eq. 2. The addition of RGB features marginally impacts the baseline, with a difference of only 0.9% mIoU. Without color, SegmentOR outperforms OTOC by ∼6% mIoU, making it more suitable for privacy-constrained setups. This improves to ∼10% for colored point clouds, indicating that the proposed learned temporal matching can leverage color similarities across time steps more effectively. The most significantly moving entities in ORs are surgical staff, who tend to wear gowns with specific and consistent colors. The presence of color features could influence the ability to distinguish humans from other objects more consistently (Fig. <ref type="figure" target="#fig_2">2</ref>). This is supported by a segmentation mIoU increase of over 15% concerning the human class for SegmentOR (see supplementary for all class distributions).</p><p>Experiment 2: Number of Clicks. A model's performance should theoretically increase with the level of supervision. We thus quantify the impact of adding up to three additional clicks on OTOC's performance. This experiment is performed on the first training and validation split, using a varying number of click annotations per class. Results: Increasing annotations by three or four times lead to an improvement of approximately 8% mIoU (see Table <ref type="table" target="#tab_2">2</ref>). The performance saturates with three clicks. Notably, the baseline does not achieve the 73% mIoU of the proposed method, even with increased supervision. This could suggest that temporal consistency not only increases the supervision signal but enables a more robust overall feature representation.</p><p>Experiment 3: Application to Surgical Phase Recognition. To further assess the quality of our semantic predictions, we evaluate their impact on surgical workflow analysis (see Table <ref type="table" target="#tab_3">3</ref>). We use a ResNet50 <ref type="bibr" target="#b11">[11]</ref> backbone and perform four-fold cross-validation using random splits over different surgeries of the complete, larger RGB-D video dataset. We use our best-performing segmentation model to infer the semantic predictions for each of the 582k fused point clouds (inference @12.5 fps), projecting them back into the two cameras. We then compare the performance of raw RGB, depth, and semantic labels inputs against fusing the latter two via late fusion <ref type="bibr" target="#b32">[29]</ref>. Results: Consistent with previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">29]</ref>, RGB achieves the best performance across both cameras. Semantic predictions alone yield a performance well below RGB or depth. However, the fact that noisy semantic predictions from our network (achieving 73% mIoU) can be used for this challenging task demonstrates the benefits of our segmentation outputs for surgical scene understanding. Furthermore, when combined with depth through late fusion, results are improved by nearly 6% and 0.6% accuracy over depth for the surgical and workflow cameras, respectively. This suggests that segmentation maps could substitute RGB features when unavailable due to privacy reasons <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>This work presents a novel semantic segmentation method for surgical scene understanding that significantly reduces the annotation burden by leveraging the temporal consistency of point cloud sequences. We demonstrate the effectiveness of our approach on point clouds from a surgical phase recognition dataset, which we enrich with manual 3D annotations. By incorporating self-supervised temporal priors, our method achieves a high segmentation mIoU of 73.10% using only 0.005% of annotated points. Furthermore, we establish a formal link between semantic segmentation and workflow analysis by demonstrating that our semantic predictions benefit downstream surgical phase recognition methods. Finally, we release all anonymized point clouds, annotations, and code used to ease the deployment of context-aware systems in surgical environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Proposed architecture. SegmentOR extracts semantic and class-specific features from a sequence of point clouds (a). Sparse labels (in red) are expanded to their nearest supervoxel cluster (b) and used as supervision for learning the segmentation task. In contrast to previous works, we propose to incorporate a prior to establish a temporal consistency between the pooled semantic features in a point cloud sequence (c), enabling spatiotemporal pseudo-label propagation across timesteps. (Color figure online)</figDesc><graphic coords="3,66,21,196,34,325,42,130,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative Results. Comparison between the ground truth annotation (GT), the OTOC baseline, and SegmentOR. SegmentOR demonstrates improved segmentation mIOU for moving classes such as Person (red arrow) but also for static classes such as Patient Table (orange arrow). Best viewed digitally.</figDesc><graphic coords="7,57,48,171,02,337,54,63,58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Main Results. Comparison of our best proposed model, using a nearest neighbor temporal prior with a learned update, with OTOC [19] in a three-fold crossvalidation. OTOC 63.29 ± 3.49 75.51 ± 3.76 75.88 ± 4.07 76.80 ± 3.88 75.88 ± 4.07 62.40 ± 0.01 74.59 ± 0.01 75.02 ± 0.54 76.19 ± 0.20 75.02 ± 0.54 SegmentOR 69.32 ± 2.05 70.57 ± 1.06 79.88 ± 2.06 79.96 ± 2.49 79.88 ± 2.05 72.</figDesc><table><row><cell>Method</cell><cell>Color mIoU (%)</cell><cell>F1 (%)</cell><cell>Recall (%)</cell><cell>Precision (%) Accuracy (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>99 ± 0.19 82.77 ± 0.13 83.25 ± 0.71 83.31 ± 1.08 83.25 ± 0.71</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>GT</cell><cell></cell><cell>Baseline</cell><cell>SegmentOR</cell></row><row><cell>Floor</cell><cell>Tool Table</cell><cell>Person</cell><cell>OR Lights</cell><cell>Monitor Tower</cell><cell>Radiation Protection Gates</cell></row><row><cell>Wall</cell><cell>Patient Table</cell><cell>C-Arm</cell><cell>Rubbish Bin</cell><cell>Patient</cell><cell>Anesthesia Machine</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>To assess the trade-off between annotation % and performance, we train the baseline OTOC method with up to four times the amount of "click" annotations.</figDesc><table><row><cell cols="7"># clicks annotated (%) mIoU (%) F1 (%) Recall (%) Precision (%) Accuracy (%)</cell></row><row><cell>1</cell><cell>0.0054</cell><cell>62.60</cell><cell>74.91</cell><cell>74.45</cell><cell>77.86</cell><cell>74.40</cell></row><row><cell>2</cell><cell>0.0108</cell><cell>69.01</cell><cell>79.92</cell><cell>79.86</cell><cell>81.13</cell><cell>79.82</cell></row><row><cell>3</cell><cell>0.0163</cell><cell>70.83</cell><cell cols="2">81.45 81.57</cell><cell>82.12</cell><cell>81.56</cell></row><row><cell>4</cell><cell>0.0217</cell><cell>70.56</cell><cell>80.01</cell><cell>79.13</cell><cell>81.75</cell><cell>79.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 . Downstream Surgical Phase Recognition.</head><label>3</label><figDesc>We evaluate the capability of a ResNet50<ref type="bibr" target="#b11">[11]</ref> as a surgical phase recognition backbone based on 3 modalities. Accuracy and mAP are reported for two differently placed surgical workflow cameras ± 7.89 70.<ref type="bibr" target="#b20">19</ref> ± 8.44 52.48 ± 5.19 58.34 ± 6.74 Depth 69.19 ± 1.46 77.95 ± 2.82 63.62 ± 6.45 67.56 ± 5.98 RGB 76.04 ± 4.27 88.71 ± 3.61 73.74 ± 1.85 85.44 ± 3.84 Depth + Semantic 74.98 ± 5.62 83.34 ± 4.08 64.29 ± 5.35 71.76 ± 4.76</figDesc><table><row><cell>Input</cell><cell>Camera 01</cell><cell></cell><cell>Camera 02</cell></row><row><cell></cell><cell>Accuracy</cell><cell>mAP</cell><cell>Accuracy</cell><cell>mAP</cell></row><row><cell>Semantic</cell><cell>61.98</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was funded by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs>, No.: <rs type="grantNumber">16SV8088</rs> and <rs type="grantNumber">13GW0236B</rs>. We additionally thank the <rs type="institution">J&amp;J Robotics &amp; Digital Solutions team</rs> for their support. Furthermore, we thank <rs type="person">Ruiyang Li</rs> for supporting the point cloud annotation. Code and data can be found at: https://bastianlb.github.io/segmentOR/.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Xe7jNVT">
					<idno type="grant-number">16SV8088</idno>
				</org>
				<org type="funding" xml:id="_5s8g7Fd">
					<idno type="grant-number">13GW0236B</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 6.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lucas-Kanade 20 years on: a unifying framework</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="221" to="255" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Know your sensors-a modality study for surgical action classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bastian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Biomech. Biomed. Eng.: Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DisguisOR: holistic face anonymization for the operating room</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">4D spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://github.com/traveller59/spconv" />
		<title level="m">Spconv Contributors: Spconv: spatially sparse convolution library</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TeCNO: surgical phase recognition with multi-stage temporal convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Surgical workflow recognition: from analysis of challenges to architectural study</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mohareri</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-832" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part III</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13803</biblScope>
			<biblScope unit="page" from="556" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Scan-Net: richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA 2003</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Gustavsson</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2749</biblScope>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45103-x_50</idno>
		<ptr target="https://doi.org/10.1007/3-540-45103-x50" />
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning spatial and temporal variations for 4D point cloud segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiacheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fayao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guosheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.04673</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Less: Label-efficient semantic segmentation for lidar point clouds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19842-7_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19842-75" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part VII</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13699</biblScope>
			<biblScope unit="page" from="70" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computer vision in the operating room: opportunities and caveats</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Kennedy-Metz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Robot. Bionics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="10" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scene flow propagation for semantic mapping and object discovery in dynamic street scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kochanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1785" to="1792" />
		</imprint>
		<respStmt>
			<orgName>IROS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RigidFlow: self-supervised scene flow learning on point clouds by local rigidity prior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16959" to="16968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rabindran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mohareri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09487</idno>
		<title level="m">A robotic 3D perception system for operating room environment awareness</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward better boundary preserved supervoxel segmentation for 3D point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0924271618301370.iSPRS" />
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogram. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Journal of Photogrammetry and Remote Sensing Theme Issue. Point Cloud Processing</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LESS: label-efficient semantic segmentation for lidar point clouds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13699</biblScope>
			<biblScope unit="page" from="70" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19842-7_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19842-75" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">One thing one click: a self-training approach for weakly supervised 3d semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1726" to="1736" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Surgical data science-from concepts toward clinical translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">102306</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Just go with the flow: self-supervised scene flow estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Okorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11177" to="11185" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptation of surgical activity recognition models across operating rooms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mohareri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="530" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-151" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">4D-OR: semantic scene graphs for or domain modeling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Özsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Örnek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="475" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-145" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-view surgical video action detection via mixed global view attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haugerud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mohareri</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-160" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic operating room surgical activity recognition for robot-assisted surgery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haugerud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mohareri</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-037" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="385" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Weakly supervised segmentation on outdoor 4D point clouds with temporal matching and spatial graph propagation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11840" to="11849" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-stream deep architecture for surgical phase recognition on multi-view RGBD videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the M2CAI Workshop MICCAI</title>
		<meeting>the M2CAI Workshop MICCAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An mil-derived transformer for weakly supervised point cloud segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11830" to="11839" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An overview to visual odometry and visual SLAM: applications to mobile robotics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yousif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bab-Hadiashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoseinnezhad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Industr. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="289" to="311" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
