<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights</title>
				<funder ref="#_BB6Y9jx">
					<orgName type="full">JSPS KAKENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuna</forename><surname>Kato</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mariko</forename><surname>Isogawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shohei</forename><surname>Mori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hideo</forename><surname>Saito</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hiroki</forename><surname>Kajita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoshifumi</forename><surname>Takatsume</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Keio University</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="271" to="280"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B49FABB44BEDA5141A3A9E2DE24F1EDB</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical Video Synthesis</term>
					<term>Multi-view Camera Calibration</term>
					<term>Event Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Occlusion-free video generation is challenging due to surgeons' obstructions in the camera field of view. Prior work has addressed this issue by installing multiple cameras on a surgical light, hoping some cameras will observe the surgical field with less occlusion. However, this special camera setup poses a new imaging challenge since camera configurations can change every time surgeons move the light, and manual image alignment is required. This paper proposes an algorithm to automate this alignment task. The proposed method detects frames where the lighting system moves, realigns them, and selects the camera with the least occlusion. This algorithm results in a stabilized video with less occlusion. Quantitative results show that our method outperforms conventional approaches. A user study involving medical doctors also confirmed the superiority of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Surgical videos can provide objective records in addition to medical records. Such videos are used in various applications, including education, research, and information sharing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>. In endoscopic surgery and robotic surgery, the surgical field can be easily captured because the system is designed to place a camera close to it to monitor operations directly within the camera field of view. Conversely, in open surgery, surgeons need to observe the surgical field; therefore, the room for additional cameras can be limited and disturbed <ref type="bibr" target="#b5">[6]</ref>.</p><p>To overcome this issue, Kumar and Pal <ref type="bibr" target="#b6">[7]</ref> installed a stationary camera arm to record surgery. However, their camera system had difficulty recording details (i.e., close-up views) since the camera had to be placed far from the surgical field so as not to disturb the surgeons. Instead, Nair et al. <ref type="bibr" target="#b8">[9]</ref> used a camera mounted on a surgeon's head, which moved frequently and flexibly.</p><p>For solid and stable recordings, previous studies have installed cameras on surgical lights. Byrd et al. <ref type="bibr" target="#b0">[1]</ref> mounted a camera on a surgical light, which could easily be blocked by the surgeon's head and body. To address this issue, Shimizu et al. <ref type="bibr" target="#b11">[12]</ref> developed a surgical light with multiple cameras to ensure that at least one camera would observe the surgical field (Fig. <ref type="figure" target="#fig_0">1</ref>). In such a multicamera system, automatically switching cameras can ensure that the surgical field is visible in the generated video <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. However, the cameras move every time surgeons move the lighting system, and thus, the image alignment becomes challenging. Obayashi et al. <ref type="bibr" target="#b9">[10]</ref> relied on a video player to manually seek and segment a video clip with no camera movement. This unique camera setup and view-switching approaches create a new task to be fulfilled, which we address in this paper: automatic occlusionfree video generation by automated change detection in camera configuration and multi-camera alignment to smoothly switch to the camera with the least occlusion. <ref type="foot" target="#foot_0">1</ref> In summary, our contributions are as follows:</p><p>-We are the first to fulfill the task of automatic generation of stable virtual single-view video with reduced occlusion for a multi-camera system installed in a surgical light. -We propose an algorithm that detects camera movement timing by measuring the degree of misalignment between the cameras. -We propose an algorithm that finds frames with less occluded surgical fields.</p><p>-We present experiments showing greater effectiveness of our algorithm than conventional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Given a sequence of image frames captured by five cameras installed in a surgical light x i = [x 1 , x 2 , ..., x T ] (Fig. <ref type="figure" target="#fig_0">1</ref>), our goal is to generate a stable single-view video sequence with less occlusion z = [z 1 , z 2 , ..., z T ](Fig. <ref type="figure" target="#fig_1">2</ref>). Here, x i represents a sequence captured with i-th camera c i , and T indicates the number of frames.</p><p>We perform an initial alignment using the method by Obayashi et al. <ref type="bibr" target="#b9">[10]</ref> (Fig. <ref type="figure" target="#fig_1">2a</ref>), which cumulatively collects point correspondences over none-feature rich frames and calculates homography matrices, M i , via a common planar scene proxy. Then, we iteratively find a frame ID, t c , where the cameras started moving (Sect. 2.1, Fig. <ref type="figure" target="#fig_1">2b</ref>) and a subsequent frame ID, t h , to update homography matrices under no moving cameras and the least occlusion (Sect. 2.2, Fig. <ref type="figure" target="#fig_1">2c</ref>). Updated homography warping can provide a newly aligned camera view sequence y i = [y 1 , y 2 , ..., y T ] after the cameras moved. Finally, using the learning-based object detection method by Shimizu et al. <ref type="bibr" target="#b11">[12]</ref>, we select camera views with the least occlusion from y i (Fig. <ref type="figure" target="#fig_1">2d</ref>). Collecting such frames results in a stable single-view video sequence with the least occlusion z = [z 1 , z 2 , ..., z T ]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Camera Movement Detection</head><p>To find the t c , we use the "degree of misalignment" obtained from the sequence y i of the five aligned cameras. Since the misalignment should be zero if the geometric calibration between the cameras works well and each view overlaps perfectly, it can be used as an indication of camera movement.</p><p>First, the proposed method performs feature point detection in each frame of y i . Specifically, we use the SIFT algorithm <ref type="bibr" target="#b7">[8]</ref>. Then, feature point matching is performed for each of the 10 combinations of the five frames. The degree of misalignment D t at frame t is represented as</p><formula xml:id="formula_0">D t = 1 10n 5 i,j=1 i =j n k p t,i,k -p t,j,l ,<label>(1)</label></formula><p>where p and k denote a keypoint position and its index in the i-th camera's coordinates respectively, l represents the corresponding index of k in the j-th camera's coordinates, and n represents the total number of corresponding points.</p><p>If D t exceeds a certain threshold, our method detects camera movement. However, the calculated misalignment is too noisy to be used as is. To eliminate the noise, the outliers are removed, and smoothing is performed by calculating the movement average. Moreover, to determine the threshold, sample clustering is performed according to the degree of misalignment. Assuming that the multicamera surgical light never moves more than twice in 10 min, the detected degree of misalignment is divided into two classes, one for every 10 min.</p><p>The camera movement detection threshold is expressed by Eq. ( <ref type="formula" target="#formula_1">2</ref>), where t represents the frames classified as the frames before the camera movement. The frame at the moment when the degree of misalignment exceeds the threshold is considered as the frame when the camera moved.</p><formula xml:id="formula_1">threshold = min (max t (D t ) + 1, 2 T T t=1 D t )<label>(2)</label></formula><p>To make the estimation of t c more robust, this process is performed multiple times on the same group of frames, and the median value is used as t c . This is expected to minimize false detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detecting the Timing for Obtaining a Homography Matrix</head><p>t h represents the timing when to obtain homography matrix. Although it would be ideal to generate always-aligned camera sequences by performing homography transformation on every frame, this would incur high computational costs if the homography is constantly calculated. Therefore, the proposed method calculates the homography matrix only after the cameras have stopped moving. Unlike a previous work that determined the timing for performing homography transformation manually <ref type="bibr" target="#b9">[10]</ref>, our method automatically detects t h by using the area of surgical field appearing in surgical videos as an indication. This region indicates the extent of occlusion. Since the five cameras capture the same surgical field, if there is no occluded camera, the area of surgical field will have the same extent in all five camera frames. Eq. ( <ref type="formula" target="#formula_2">3</ref>) is used to calculate the degree to which the area of surgical field is the same in all five camera frames, where s i is the area of surgical field detected in each camera.</p><formula xml:id="formula_2">S = {max i (s i ) -min i (s i )}/ si , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where S is calculated every 30 frames, and if it is continuously below a given threshold (0.5 in this method), the corresponding timing is selected as the t h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We conducted two experiments to quantitatively and qualitatively investigate our method's efficacy. Dataset. We used a multi-camera system attached to a surgical light to capture videos of surgical procedures. We captured three types of actual surgical procedures: polysyndactyly, anterior thoracic keloid skin graft, and posttraumatic facial trauma rib cartilage graft. From these videos, we prepared five videos which were trimmed to one minute each. Videos 1 and 2 show the surgery of polysyndactyly, videos 3 and 4 show the anterior thoracic keloid skin graft scene, and video 5 shows the surgery of posttraumatic facial trauma rib cartilage graft.</p><p>Implementation Details. We used Ubuntu 20.04 LTS OS, an Intel Core i9-12900 for the CPU, and 62GiB of RAM. We defined the area of surgical field as hue ranging from 0 to 30 or from 150 to 179 in HSV color space.</p><p>Virtual Single-View Video Generation. Figure <ref type="figure" target="#fig_2">3</ref> shows a representative frame from the automatic positioning of the surgical video of the polysyndactyly operation using the proposed method. The figure also includes frames with detected camera movement. Once all five viewpoints were aligned, they were fed into the camera-switching algorithm to generate a virtual single-viewpoint video. The method requires about 40 min every time the cameras move. Please note that it is fully automated and needs no human labor, unlike the existing method, i.e., manual-alignment.</p><p>Comparison with Conventional Methods. We compared our auto alignment method (auto-alignment) with two conventional methods. In one of these methods, which is used in a hospital camera switching is performed after manual alignment (manual-alignment). The other method switches between camera views with no alignment (no-alignment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Qualitative Evaluation</head><p>To qualitatively compare our method against baseline methods, we conducted a subjective evaluation. 11 physicians involved in surgical procedures regularly who were expected to actually use the surgical videos were selected as subjects. Figure <ref type="figure" target="#fig_3">4</ref> shows single-view video frames generated by our method and the two conventional methods. The red lines indicate the position and orientation of the instep of patient's foot. In video generated with no-alignment, the position and orientation of the insteps changed every time camera switching was performed, making it difficult to observe the surgical region with comfort. Manualalignment showed better results than no-alignment. However, it was characterized by greater misalignment than the proposed method. It should also be noted that manual alignment requires time and effort. In contrast, our method effectively reduced misalignment between viewpoints even when camera switching was performed.</p><p>To perform a qualitative comparison between the three methods, following a previous work <ref type="bibr" target="#b12">[13]</ref>, we recruited eleven experienced surgeons who were expected to actually use surgical videos and asked them to conduct a subjective evaluation of the captured videos. The subjects were asked to score five statements, 1 ("disagree") to 5 ("agree").  The results are shown in Fig. <ref type="figure" target="#fig_5">5</ref>. For almost all videos and statements, the Wilcoxon signed-rank test showed that the proposed method (auto-alignment) scored significantly higher than the two conventional methods. Significant differences are indicated by asterisks in the graphs in Fig. <ref type="figure" target="#fig_5">5</ref>. The specific p-values are provided in the supplemental material. The results showing that the proposed method outperformed the conventional methods in statements 1 and 2 suggest that our method generates a stable video with little misalignment between camera viewpoints. Additionally, the results indicating that the proposed method outperformed the conventional method in statements 3 and 4 suggest that the video generated by our method makes it easier to confirm the surgical area. Furthermore, as shown in statement 5, the proposed method received the highest score in terms of the subjects' willingness to use the system in actual medical practice. We believe that the proposed method can contribute to improving the quality of medical care by facilitating stable observation of the surgical field.</p><p>Although we observed statistically significant differences between the proposed method and the baselines for almost all the test videos, significant differences were not observed only in Video 3, Statement 3 and Video 4, Statements 3 and 4. There may be two reasons for this. One is the small number of participants. Since we limited the participants to experienced surgeons, it was quite difficult to obtain a larger sample size. The second reason is differences in the geometry of the surgical fields. Our method is more effective for scenes with a three-dimensional geometry. If the surgical field is flat with fewer feature points, as in the case of the anterior thoracic keloid skin graft procedure, differences between our method and the manual alignment mehod, which does not take into account three-dimensional structures, are less likely to be observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative Evaluation</head><p>Our method aims to reduce the misalignment between viewpoints that occurs when swıtching between multiple cameras and generate single-view surgical videos with less occlusion. To investigate the method's effectiveness, we conducted a quantitative evaluation to assess the degree of misalignment between video frames. Following a previous work that calculated degree of misalignment between consecutive time-series frames <ref type="bibr" target="#b2">[3]</ref>, we used two metrics, the interframe transformation fidelity (ITF) and the average speed (AvSpeed). ITF represents the average peak signal-to-noise ratio (PSNR) between frames as</p><formula xml:id="formula_4">ITF = 1 N f -1 N f -1 i=1 PSNR(t),<label>(4)</label></formula><p>where N f is the total number of frames. ITF is higher for videos with less motion blur. AvSpeed expresses the average speed of feature points. With the total number of frames N f and the number of all feature points in a frame N p , AvSpeed is calculated as</p><formula xml:id="formula_5">AvSpeed = 1 N p (N f -1) Np i=1 N f -1 t=1 żi (t) ,<label>(5)</label></formula><p>where z i (t) denotes the image coordinates of the feature point and is calculated as żi (t) = z i (t + 1)z i (t).</p><p>The results are shown in Table <ref type="table" target="#tab_0">1</ref>. The ITF of the videos generated using the proposed method was 20%-50% higher than that of the videos with manual alignment. The AvSpeed of the videos generated using the proposed method was 40%-70% lower than that of the videos with manual alignment, indicating that the shake was substantially corrected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>In this work, we propose a method for generating high-quality virtual singleviewpoint surgical videos captured by multiple cameras attached to a surgical light without occlusion or misalignment through automatic geometric calibration. In evaluation experiments, we compared our auto-alignment method with manual-alignment and no-alignment. The results verified the superiority of the proposed method both qualitatively and quantitatively. The ability to easily confirm the surgical field with the automatically generated virtual single-viewpoint surgical video will contribute to medical treatment.</p><p>Limitations. Our method relies on visual information to detect the timing of homography calculations (i.e., t h ). However, we may use prior knowledge of a geometric constraint such that cameras are at the pentagon corners (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>We assume that the multi-camera surgical light does not move more than twice in 10 min for a robust calculation of D t . Although surgeons rarely moved the light more often, fine-tuning the parameter may result in further performance improvement. The current implementation shows misaligned images if the cameras move more frequently.</p><p>In the user-involved study, several participants reported noticeable black regions where no camera views were projected. (e.g., Fig. <ref type="figure" target="#fig_3">4</ref>). One possible complement is to project pixels from other views.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Surgical light with multiple cameras. The unit consists of five cameras (left), each of which is surrounded by multiple light sources (right).</figDesc><graphic coords="2,42,81,220,82,338,68,81,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed method.</figDesc><graphic coords="3,56,97,278,96,338,68,205,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Video frames after auto-alignment.</figDesc><graphic coords="5,58,47,154,40,335,08,147,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Video frames obtained using the three methods. The red lines indicate the position and orientation of the instep of patient's foot. (Color figure online)</figDesc><graphic coords="6,46,29,210,20,331,60,153,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 .</head><label>1</label><figDesc>No discomfort when the cameras switched. 2. No fatigue, even after long hours of viewing. 3. Easy to check the operation status. 4. Easy to see important parts of the frame. 5. I would like to use this system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results of the subjective evaluation experiment.</figDesc><graphic coords="7,58,98,141,05,334,84,189,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of the quantitative evaluation.</figDesc><table><row><cell></cell><cell cols="2">ITF [dB] (↑)</cell><cell></cell><cell cols="2">AvSpeed [pixel/frame] (↓)</cell></row><row><cell cols="3">Video ID alignment</cell><cell></cell><cell cols="2">alignment</cell></row><row><cell></cell><cell>no</cell><cell cols="3">manual auto(Ours) no</cell><cell>manual auto(Ours)</cell></row><row><cell>1</cell><cell cols="2">11.97 11.87</cell><cell>17.54</cell><cell cols="2">406.3 416.1</cell><cell>166.1</cell></row><row><cell>2</cell><cell cols="2">11.30 11.93</cell><cell>15.77</cell><cell cols="2">339.4 328.7</cell><cell>195.6</cell></row><row><cell>3</cell><cell cols="2">16.17 17.85</cell><cell>22.26</cell><cell cols="2">448.6 230.9</cell><cell>92.2</cell></row><row><cell>4</cell><cell cols="2">14.43 16.01</cell><cell>19.26</cell><cell cols="2">379.0 240.7</cell><cell>77.5</cell></row><row><cell>5</cell><cell cols="2">15.19 17.42</cell><cell>21.66</cell><cell cols="2">551.6 383.2</cell><cell>169.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>project page: https://github.com/isogawalab/SingleViewSurgicalVideo.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partially supported by <rs type="funder">JSPS KAKENHI</rs> Grant Number <rs type="grantNumber">22H03617</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BB6Y9jx">
					<idno type="grant-number">22H03617</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 26.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Surgical lighting system with integrated digital video camera</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Ujjin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kongchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US</title>
		<imprint>
			<biblScope unit="volume">6633328</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">NS NOW Updated No.9 Thorough Knowledge and Application of Device and Information Technology (IT) for Neurosurgical Opperation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Date</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenichiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Medical View Co., Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A performance evaluation framework for video stabilization methods</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guilluy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oudre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 7th European Workshop on Visual Information Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep selection: a fully supervised camera selection network for surgery recordings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hachiuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kajita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takatsume</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-040" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">[special talk] video recording, storing, distributing and editing system for surgical operation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hanada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITE Technical Report</title>
		<imprint>
			<biblScope unit="page" from="77" to="80" />
			<date type="published" when="2017">2017</date>
			<publisher>The Institute of Image Information and Television Engineers</publisher>
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Surgical video recording and application of deep learning for open surgery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kajita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Japan Soc. Comput. Aided Surg</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="64" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Digital video recording of cardiac surgical procedures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Thorac. Surg</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1063" to="1065" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Surgeon point-of-view recording: using a high-definition headmounted video camera in the operating room</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Nair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Indian J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="771" to="774" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view surgical camera calibration with none-feature-rich video frames: toward 3D surgery playback</title>
		<author>
			<persName><forename type="first">M</forename><surname>Obayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kajita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takatsume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2447</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Camera selection for occlusion-less surgery recording via training with an egocentric camera</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hachiuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kajita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takatsume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="138307" to="138322" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Surgery recording without occlusions by multi-view surgical videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hachiuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kajita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Takatsume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VISIGRAPP (5: VISAPP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="837" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatiotemporal video highlight by neural network considering gaze and hands of surgeon in egocentric surgical videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2141001</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
