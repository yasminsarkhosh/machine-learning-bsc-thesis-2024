<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence</title>
				<funder ref="#_3dKAYTb">
					<orgName type="full">JSPS Bilateral International Collaboration Grants; JST CREST</orgName>
				</funder>
				<funder ref="#_TzFbgNP #_3QvFnyZ #_7umBg69">
					<orgName type="full">MEXT/JSPS KAKENHI Grants</orgName>
				</funder>
				<funder ref="#_4th3cY5">
					<orgName type="full">CIBoG initiative of Nagoya University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenda</forename><surname>Li</surname></persName>
							<email>wdli@mori.m.is.nagoya-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<postCode>464-8601</postCode>
									<settlement>Aichi</settlement>
									<region>Nagoya</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuichiro</forename><surname>Hayashi</surname></persName>
							<idno type="ORCID">0000-0003-4486-7207</idno>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<postCode>464-8601</postCode>
									<settlement>Aichi</settlement>
									<region>Nagoya</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Masahiro</forename><surname>Oda</surname></persName>
							<idno type="ORCID">0000-0001-7714-422X</idno>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<postCode>464-8601</postCode>
									<settlement>Aichi</settlement>
									<region>Nagoya</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Information and Communications</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<postCode>464-8601</postCode>
									<settlement>Aichi</settlement>
									<region>Nagoya</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takayuki</forename><surname>Kitasaka</surname></persName>
							<idno type="ORCID">0000-0003-1648-1999</idno>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Information Science</orgName>
								<orgName type="institution">Aichi Institute of Technology</orgName>
								<address>
									<postCode>470-0392</postCode>
									<settlement>Yakusacho, Aichi, Toyota</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kazunari</forename><surname>Misawa</surname></persName>
							<idno type="ORCID">0000-0002-2047-3919</idno>
							<affiliation key="aff3">
								<orgName type="institution">Aichi Cancer Center Hospital</orgName>
								<address>
									<postCode>464-8681</postCode>
									<settlement>Aichi</settlement>
									<region>Nagoya</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kensaku</forename><surname>Mori</surname></persName>
							<email>kensaku@is.nagoya-u.ac.jp</email>
							<idno type="ORCID">0000-0002-0100-4797</idno>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<postCode>464-8601</postCode>
									<settlement>Aichi</settlement>
									<region>Nagoya</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Information Technology Center</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<postCode>464-8601</postCode>
									<settlement>Aichi</settlement>
									<region>Nagoya</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Research Center of Medical Bigdata</orgName>
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<postCode>101-8430</postCode>
									<settlement>Tokyo</settlement>
									<region>Hitotsubashi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-view Guidance for Self-supervised Monocular Depth Estimation on Laparoscopic Images via Spatio-Temporal Correspondence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A8DAC54C7482482FFF6A843FB77C493D</idno>
					<idno type="DOI">10.1007/978-3-031-43996-441.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Monocular depth estimation</term>
					<term>Self-supervised learning</term>
					<term>Laparoscopic images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes an innovative self-supervised approach to monocular depth estimation in laparoscopic scenarios. Previous methods independently predicted depth maps ignoring spatial coherence in local regions and temporal correlation between adjacent images. The proposed approach leverages spatio-temporal coherence to address the challenges of textureless areas and homogeneous colors in such scenes. This approach utilizes a multi-view depth estimation model to guide monocular depth estimation when predicting depth maps. Moreover, the minimum reprojection error is extended to construct a cost volume for the multi-view model using adjacent images. Additionally, a 3D consistency of the point cloud back-projected from predicted depth maps is optimized for the monocular depth estimation model. To benefit from spatial coherence, deformable patch-matching is introduced to the monocular and multi-view models to smooth depth maps in local regions. Finally, a cycled prediction learning for view synthesis and relative poses is designed to exploit the temporal correlation between adjacent images fully. Experimental results show that the proposed method outperforms existing methods in both qualitative and quantitative evaluations. Our code is available at https://github.com/MoriLabNU/MGMDepthL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The significance of depth information is undeniable in computer-assisted surgical systems <ref type="bibr" target="#b20">[20]</ref>. In robotic-assisted surgery, the depth value is used to accurately map the surgical field and track the movement of surgical instruments <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b22">22]</ref>. Additionally, the depth value is essential to virtual and augmented reality to create 3D models and realize surgical technique training <ref type="bibr" target="#b18">[18]</ref>.</p><p>Learning-based approaches have significantly improved monocular depth estimation (MDE) in recent years. As the pioneering work, Eigen et al. <ref type="bibr" target="#b1">[2]</ref> proposed the first end-to-end deep learning framework using multi-scale CNN under supervised learning for MDE. Following this work, ResNet-based and Hourglass-based models were introduced as the variants of CNN-based methods <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b21">21]</ref>. However, supervised learning requires large amounts of annotated data. Collecting depth value from hardware or synthesis scenes is time-consuming and expensive <ref type="bibr" target="#b14">[14]</ref>. To address this issue, researchers have explored self-supervised methods for MDE. Zhou et al. <ref type="bibr" target="#b30">[30]</ref> proposed a novel depth-pose self-supervised monocular depth estimation from a video sequence. They generate synthesis views through the estimated depth maps and relative poses. Gordon et al. <ref type="bibr" target="#b3">[3]</ref> optimized this work by introducing a minimum reprojection error between adjacent images and made a notable baseline named Monodepth2. Subsequently, researchers have proposed a variety of self-supervised methods for MDE, including those based on semantic segmentation <ref type="bibr" target="#b4">[4]</ref>, adversarial learning <ref type="bibr" target="#b28">[28]</ref> and uncertainty <ref type="bibr" target="#b17">[17]</ref>. These days, MDE has been applied to laparoscopic images. Ye et al. <ref type="bibr" target="#b27">[27]</ref> and Max et al. <ref type="bibr" target="#b0">[1]</ref> have provided exceptional laparoscopic scene datasets. Huang et al. <ref type="bibr" target="#b6">[6]</ref> used generative adversarial networks to derive depth maps on laparoscopic images. Li et al. <ref type="bibr" target="#b12">[12]</ref> combined depth estimation with scene coordinate prediction to improve network performance.</p><p>This study presents a novel approach to predict depth values in laparoscopic images using spatio-temporal correspondence. Current self-supervised models for monocular depth estimation face two significant challenges in laparoscopic settings. First, monocular models individually predicted depth maps, ignoring the temporal correlation between adjacent images. Second, accurate point matching is difficult to achieve due to the misleading of large textureless regions caused by the smooth surface of organs. And the homogenous color misleads that the local areas of the edge are regarded with the same depth value. To overcome these obstacles, We introduce multi-view depth estimation (MVDE) with the optimized cost volume to guide the self-supervised monocular depth estimation model. Moreover, we exploit more informative values in a spatio-temporal manner to address the limitation of existing multi-view and monocular models.</p><p>Our main contributions are summarized as follows. (i) A novel self-supervised monocular depth estimation guided by a multi-view depth model to leverage adjacent images when estimating depth value. (ii) Cost volume construction for multi-view depth estimation under minimum reprojection error and an optimized point cloud consistency for the monocular depth estimation. (iii) An extended deformable patch matching based on the spatial coherence in local regions and a cycled prediction learning for view synthesis and relative poses to exploit the temporal correlation between adjacent images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Self-supervised Monocular Depth Estimation</head><p>Following <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b12">12]</ref>, we train a self-supervised monocular depth network using a short clip from a video sequence. The short clip consists of a current frame I t as reference image I r and adjacent images I s ∈ {I t-1 , I t+1 } regarded as source images. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, a monocular depth network f m D (I r , I s ; θ m D ) respectively predicts pixel-level depth maps D r and D s corresponding to I r and I s . A pose network f T (I r , I s ; θ T ) estimates a transformation matrix T s r as a relative pose of the laparoscope from view I r to I s . We use D r and T s r to match the pixels between I r and I s by</p><formula xml:id="formula_0">p s = KT s r D (p r ) K -1 p r ,<label>(1)</label></formula><p>where p r and p s are the 2D pixel coordinates in I r and I s . K is the laparoscope's intrinsic parameter matrix. This allows for the generation of a synthetic image I s→r through I s→r (p r ) = I s (p s ). To implement the self-supervised learning strategy, the reprojection error is calculated based on I r and I s→r by</p><formula xml:id="formula_1">E (I r , I s→r ) = α 2 L SSIM (I r , I s→r ) + (1 -α) L L1 (I r , I s→r ) ,<label>(2)</label></formula><p>with</p><formula xml:id="formula_2">L SSIM (I r , I s→r ) = 1 -SSIM (I r , I s→r ))<label>(3)</label></formula><p>and</p><formula xml:id="formula_3">L L1 (I r , I s→r ) = I r -I s→r 1 ,<label>(4)</label></formula><p>where structured similarity (SSIM) <ref type="bibr" target="#b24">[24]</ref> and L1-norm operator both adopt α at 0.85 followed as <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b13">13]</ref>. Instead of adding the auxiliary task proposed in prior work <ref type="bibr" target="#b12">[12]</ref>, we back-project the 2D pixel coordinates to the 3D coordinates P (p) = K -1 D(p)p. All the 3D coordinates from I r and I s gather as point cloud S r and S s . Then synthesized point cloud is warped as S s→r (p r ) = S s (p s ). We construct the point cloud consistency by</p><formula xml:id="formula_4">L p = L L1 (S r , S s→r ) , (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where S r and S s→r are based on depth maps from the monocular depth network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Improving Depth with Multi-view Guidance</head><p>Unlike </p><formula xml:id="formula_6">f p d s = KT s r Z d k ( f p r ) K -1 f p r , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where f p r is the pixel coordinates in F r and f p d s is the pixel coordinates in F s based on the depth value d. Then F s is warped to the synthesis feature map by F d s→r ( f p r ) = F s f p d s . Feature volumes V r and V s→r are aggregations of feature maps F r and F s→r . We construct a cost volume C by</p><formula xml:id="formula_8">C = L L1 (V r , V s→r ) = V r -V s→r 1 . (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>Fig. <ref type="figure">3</ref>. Illustration of deformable patch matching process with pixel coordinates offset and depth propagation.</p><p>Previous approaches average the difference between V r and all V s→r from adjacent views to generate cost volumes without considering the occlusion problem and uniform differences between the reference and adjacent feature maps <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>. Motivated by these challenges, we introduce the minimum reprojection loss to C, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We construct the cost volume Ĉ via the minor difference value on the corresponding coordinates of the feature volumes as Ĉ = min</p><formula xml:id="formula_10">s L L1 (V r , V s→r ) . (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>We construct a consistency between MVDE and MDE by </p><formula xml:id="formula_12">L c = L L1 (D m r , D mv r ) = D m r -D mv r 1 ,<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deformable Patch Matching and Cycled Prediction Learning</head><p>Since large areas of textureless areas and reflective parts will cause brightnessbased reprojection errors to become unreliable. Furthermore, the homogeneous color on the edge of organs causes local regions to be regarded in the same depth plane. We introduced deformable patch-matching-based local spatial propagation to MDE. As shown in Fig. <ref type="figure">3</ref>, an offset map O r (p r ) is adopted to obtain a local region for each pixel by transforming the pixel coordinates in the reference image I r . Inspired by <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b23">23]</ref>, to avoid marginal areas affecting the spatial coherence of the local region, an offset network f O (I r ; θ O ) generates a pixel-level add additional offset map ΔO r (p r ). The deformable local region for each pixel can be obtained by</p><formula xml:id="formula_13">R r = p r + O r (p r ) + ΔO r (p r ) , (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where R r is the deformable local regions for pixel p r . After sharing the same depth value by depth propagation in the local region, we implement the deformable local regions on Eq. 1 to complete patch matching by</p><formula xml:id="formula_15">R s = KT s r D r (R r ) K -1 R r , (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where R s is the matched local regions in source views I s . Based on R r and R s , I s (R s ) is warped to the synthesised regions I s→r (R r ). The patch-matching-based reprojection error is calculated by</p><formula xml:id="formula_17">L r = E (I r (R r ) , I s→r (R r )) . (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>To better use the temporal correlation, we considered each image as a reference to construct a cycled prediction learning for depth and pose. The total loss on the final computation is averaged from the error of each combination as</p><formula xml:id="formula_19">L total = 1 3 3 i=1 L m r (i) + γL mv r (i) + μL p (i) + λL c (i) + δL s (i) , (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>where L m r is the reprojection error term for MDE, and L mv r is for MVDE. i is the index number of views in the short clip. L s is the smoothness term <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b12">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Metrics</head><p>SCARED <ref type="bibr" target="#b0">[1]</ref> datasets were adopted for all the experiments. This dataset contained 35 laparoscopic stereo videos with nine different scenes. And the corresponding depth values obtained through coded structured light images served as ground-truth. We divided the SCARED datasets into a 10:1:1 ratio for each scene based on the video sequence to conduct our experiments. For training, validation, and testing, there were 23,687, 2,405, and 2,405 frames, respectively. Because of limitations in computational resources, we resized the images to 320 × 256 pixels, a quarter of their original dimensions. Following the previous methods <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b25">25]</ref>, we adopted seven classical 2D metrics to evaluate the predicted depth maps. Additionally, we only used the monocular depth model to predict depth values with a single RGB image as input during testing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We utilized PyTorch <ref type="bibr" target="#b16">[16]</ref> for model training, employing the Adam optimizer <ref type="bibr" target="#b8">[8]</ref> across 25 training epochs. The learning rate started at 1 × 10 -4 and dropped by a scale factor of 10 for the final 10 epochs. A batch size is 6, and the total loss function's parameters γ, μ, λ, and δ were set to 0.5, 0.5, 0.2, and 1 × 10 -3 , respectively. Additionally, we capped the predicted depth values at 150mm. To construct the cost volume, we adopted the adaptive depth range method <ref type="bibr" target="#b25">[25]</ref> and set the number of hypothesized depth values k to 96. Following <ref type="bibr" target="#b25">[25]</ref>, we used ResNet-18 <ref type="bibr" target="#b5">[5]</ref> with pretrained weights on the Ima-geNet dataset <ref type="bibr" target="#b19">[19]</ref> as encoder module. The feature extractor comprised the first five layers of ResNet-18 <ref type="bibr" target="#b5">[5]</ref>. The offset network was two 2D convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison Experiments</head><p>We conducted a comprehensive evaluation of our proposed method by comparing it with several classical and state-of-the-art techniques <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29]</ref> retrained on SCARED datasets <ref type="bibr" target="#b0">[1]</ref>. Table <ref type="table" target="#tab_1">1</ref> presents the quantitative results. We also assessed the baseline performance of our proposed method. We compared the depth maps and generated error maps on various laparoscopic scenes based on absolute relative error <ref type="bibr" target="#b25">[25]</ref>, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>We conducted an ablation study to evaluate the influence of different components in our proposed approach. Table <ref type="table" target="#tab_2">2</ref> shows the results of our method with four different components, namely, point cloud consistency (PCC), minimum reprojection error (MRE), deformable patch matching (DPM), and cycled prediction learning (CPL). We adopted GCDepthL <ref type="bibr" target="#b12">[12]</ref> and Manydepth <ref type="bibr" target="#b25">[25]</ref> as baselines for our method's monocular and multi-view depth models. We proposed PCC and MRE as two optimized modules for these baseline models and evaluated their impact on each baseline individually. Furthermore, we trained the monocular and multi-view depth models separately in our proposed method without consistency to demonstrate the contribution of combining these two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusions</head><p>The laparoscopic scenes typically feature large, smooth regions with organ surfaces and homogeneous colors along the edges of organs. This can cause issues while matching pixels, as the points in the boundary area can be mistaken to be in the same depth plane. Our proposed method's depth maps, as shown in Fig <ref type="figure" target="#fig_3">4</ref>, exhibit a smoother performance in the large regions of input images when compared to the existing methods <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29]</ref>. Additionally, the error maps reveal that our proposed method performs better even when the depth maps look similar qualitatively. At the marginal area of organs, our proposed method generates better depth predictions with smoother depth maps and lower errors, despite the color changes being barely perceptible with the depth value changes. Our proposed method outperforms current approaches on the seven metrics we used, as demonstrated in Table <ref type="table" target="#tab_1">1</ref>. The ablation study reveals that the proposed method improves significantly when combining each component, and each component contributes to the proposed method. Specifically, the optimized modules PCC and MRE designed for monocular and multi-view depth models enhance the performance of the baselines <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b25">25]</ref>. The combination of monocular and multi-view depth models yields better results than the single model trained independently, as seen in the last three rows of Table <ref type="table" target="#tab_2">2</ref>.</p><p>In conclusion, we incorporate more temporal information in the monocular depth model by leveraging the guidance of the multi-view depth model when predicting depth values. We introduce the minimum reprojection error to construct the multi-view depth model's cost volume and optimize the monocular depth model's point cloud consistency module. Moreover, we propose a novel method that matches deformable patches in spatially coherent local regions instead of point matching. Finally, cycled prediction learning is designed to exploit temporal information. The outcomes of the experiments indicate an improved depth estimation performance using our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our self-supervised monocular depth estimation framework. The proposed method consists of a monocular depth network, a pose network, an offset network, and a multi-view depth network.</figDesc><graphic coords="3,55,98,136,40,340,18,199,39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization depicting cost volume construction. It consists of a plane-sweep process, reprojection in three views as inputs and adopted minimum reprojection error.</figDesc><graphic coords="4,41,79,54,02,340,21,121,69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where D m r and D mv r are the depth map predicted by our networks f m D (I r , I s ; θ m D ) and f mv D Ĉ, F r , F s ; θ mv D .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Depth estimation qualitative results: input images (first column), predicted depth maps and error maps calculated via the absolute relative error metric</figDesc><graphic coords="7,55,98,232,37,340,30,239,83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MDE, MVDE fully leverages the temporal information in the short clip when estimating depth maps. MVDE first samples hypothesized depths value d k in a depth range from d min to d max . k is the number of sampled depth values.</figDesc><table /><note><p>Then, a feature extractor f F (I r , I s ; θ F ) obtains the deep feature map F r and F s from input images I r and I s . Similar to the pixel-coordinate matching between I r and I s as Eq. 1, 2D pixel coordinates of F s is back-projected to the each plane Z d k of hypothesised depth value. Z d k shares the same depth value d for each pixels. Then the pixel coordinates are matched by</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Depth estimation quantitative results. * denotes the method need multiframe at test. † denotes that the input images are five frames instead of three during training time.Abs Rel↓ Sq Rel↓ RMSE↓ RMSE log↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</figDesc><table><row><cell>Baseline</cell><cell>0.079</cell><cell>0.999</cell><cell>7.868</cell><cell>0.103</cell><cell>0.918</cell><cell>0.995</cell><cell>1.000</cell></row><row><cell cols="2">Monodepth2 [3] 0.083</cell><cell>0.994</cell><cell>8.167</cell><cell>0.107</cell><cell>0.937</cell><cell>0.995</cell><cell>1.000</cell></row><row><cell>HR-Depth [13]</cell><cell>0.080</cell><cell>0.938</cell><cell>7.943</cell><cell>0.104</cell><cell>0.940</cell><cell>0.996</cell><cell>1.000</cell></row><row><cell cols="2">Manydepth [25]  *  0.075</cell><cell>0.830</cell><cell>7.403</cell><cell>0.099</cell><cell>0.945</cell><cell>0.996</cell><cell>1.000</cell></row><row><cell>MonoVit [29]</cell><cell>0.074</cell><cell>0.865</cell><cell>7.517</cell><cell>0.097</cell><cell>0.949</cell><cell>0.996</cell><cell>1.000</cell></row><row><cell cols="2">SC-Depth [11]  † 0.070</cell><cell>0.744</cell><cell>6.932</cell><cell>0.092</cell><cell>0.951</cell><cell>0.998</cell><cell>0.999</cell></row><row><cell>AJ-Depth [10]</cell><cell>0.078</cell><cell>0.896</cell><cell>7.578</cell><cell>0.101</cell><cell>0.937</cell><cell>0.996</cell><cell>1.000</cell></row><row><cell cols="2">GCDepthL [12] 0.071</cell><cell>0.801</cell><cell>7.105</cell><cell>0.094</cell><cell>0.938</cell><cell>0.996</cell><cell>1.000</cell></row><row><cell>Ours</cell><cell>0.066</cell><cell cols="3">0.655 6.441 0.086</cell><cell>0.955</cell><cell>0.999</cell><cell>1.000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation results for each component contribution in the proposed method. * denotes the method need multi-frame at test. PCC: Point Cloud Consistency; MRE: Minimum Reprojection Error; DPM: Deformable Patch Matching; CPL: Cycled Prediction Learning. M represents monocular depth estimation; MV represents multi-view depth estimation. Abs Rel↓ Sq Rel↓ RMSE↓ RMSE log↓ δ &lt; 1.25↑ δ &lt; 1.25 2 ↑ δ &lt; 1.25 3 ↑</figDesc><table><row><cell>w/o PCC</cell><cell>0.084</cell><cell>1.364</cell><cell>9.217</cell><cell>0.116</cell><cell>0.925</cell><cell>0.990</cell><cell>0.997</cell></row><row><cell>w/o MRE</cell><cell>0.068</cell><cell>0.711</cell><cell>6.576</cell><cell>0.088</cell><cell>0.953</cell><cell>0.998</cell><cell>0.999</cell></row><row><cell>w/o DPM</cell><cell>0.069</cell><cell>0.753</cell><cell>6.901</cell><cell>0.091</cell><cell>0.945</cell><cell>0.997</cell><cell>1.000</cell></row><row><cell>w/o CPL</cell><cell>0.071</cell><cell>0.702</cell><cell>6.868</cell><cell>0.091</cell><cell>0.956</cell><cell>0.998</cell><cell>1.000</cell></row><row><cell>Baseline (M) [12]</cell><cell>0.071</cell><cell>0.801</cell><cell>7.105</cell><cell>0.094</cell><cell>0.938</cell><cell>0.996</cell><cell>1.000</cell></row><row><cell>w/ PCC</cell><cell>0.070</cell><cell>0.763</cell><cell>6.959</cell><cell>0.092</cell><cell>0.948</cell><cell>0.997</cell><cell>1.000</cell></row><row><cell cols="2">Baseline (MV) [25]  *  0.075</cell><cell>0.830</cell><cell>7.403</cell><cell>0.099</cell><cell>0.945</cell><cell>0.997</cell><cell>1.000</cell></row><row><cell>w/ MRE  *</cell><cell>0.074</cell><cell>0.797</cell><cell>7.241</cell><cell>0.095</cell><cell>0.953</cell><cell>0.997</cell><cell>1.000</cell></row><row><cell>Ours (M)</cell><cell>0.068</cell><cell>0.708</cell><cell>6.788</cell><cell>0.089</cell><cell>0.955</cell><cell>0.998</cell><cell>1.000</cell></row><row><cell>Ours (MV)  *</cell><cell>0.070</cell><cell>0.743</cell><cell>6.842</cell><cell>0.091</cell><cell>0.948</cell><cell>0.997</cell><cell>1.000</cell></row><row><cell>Ours</cell><cell>0.066</cell><cell cols="3">0.655 6.441 0.086</cell><cell>0.955</cell><cell>0.999</cell><cell>1.000</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. We extend our appreciation for the assistance provided through the <rs type="funder">JSPS Bilateral International Collaboration Grants; JST CREST</rs> Grant (<rs type="grantNumber">JPMJCR20D5</rs>); <rs type="funder">MEXT/JSPS KAKENHI Grants</rs> (<rs type="grantNumber">17H00867</rs>, <rs type="grantNumber">26108006</rs>, <rs type="grantNumber">21K19898</rs>); and the <rs type="funder">CIBoG initiative of Nagoya University</rs>, part of the <rs type="programName">MEXT WISE program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3dKAYTb">
					<idno type="grant-number">JPMJCR20D5</idno>
				</org>
				<org type="funding" xml:id="_TzFbgNP">
					<idno type="grant-number">17H00867</idno>
				</org>
				<org type="funding" xml:id="_3QvFnyZ">
					<idno type="grant-number">26108006</idno>
				</org>
				<org type="funding" xml:id="_7umBg69">
					<idno type="grant-number">21K19898</idno>
				</org>
				<org type="funding" xml:id="_4th3cY5">
					<orgName type="program" subtype="full">MEXT WISE program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01133</idno>
		<title level="m">Stereo correspondence and reconstruction of endoscopic data challenge</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Digging into selfsupervised monocular depth estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName><forename type="first">V</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12319</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognitionm</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognitionm</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-supervised generative adversarial network for depth estimation in laparoscopic images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-122" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Essert</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="227" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Applying depth-sensing to automated surgical manipulation with a da Vinci robot</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Symposium on Medical Robotics (ISMR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention guided self-supervised monocular depth estimation based on joint depth-pose loss for laparoscopic images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kensaku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Assist. Radiol. Surg</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatially variant biases considered self-supervised depth estimation based on laparoscopic videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Biomech. Biomed. Eng.: Imaging Vis</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric constraints for self-supervised monocular depth estimation on laparoscopic images with dual-task consistency</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-845" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="467" to="477" />
		</imprint>
	</monogr>
	<note>Proceedings, LNCS, Part IV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HR-Depth: high resolution self-supervised monocular depth estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2294" to="2301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning for monocular depth estimation: a review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">438</biblScope>
			<biblScope unit="page" from="14" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-local spatial propagation network for depth completion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58601-0_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58601-08" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12358</biblScope>
			<biblScope unit="page" from="120" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Workshop on Autodiff</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the uncertainty of self-supervised monocular depth estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3227" to="3237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ARAMIS: augmented reality assistance for minimally invasive surgery using a head-mounted display</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kazanzides</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32254-0_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32254-09" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11768</biblScope>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Laparoscopic video analysis for training and imageguided surgery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sánchez-González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minim. Invasive Therapy Allied Technol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="311" to="320" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation infusing traditional stereo knowledge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9799" to="9809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">History of laparoscopic surgery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vecchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Macfayden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Palazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Panminerva Med</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="90" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Patchmatchnet: learned multi-view patchmatch stereo</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Speciale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14194" to="14203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The temporal opportunist: self-supervised multi-frame monocular depth</title>
		<author>
			<persName><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1164" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MVSNet: depth inference for unstructured multi-view stereo</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08260</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Masked GAN for unsupervised depth and pose prediction with scale consistency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5392" to="5403" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">MonoViT: self-supervised monocular depth estimation with a vision transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03543</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
