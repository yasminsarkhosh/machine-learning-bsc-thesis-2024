<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure</title>
				<funder ref="#_hU6JHYh">
					<orgName type="full">Science, Technology and Innovation Commission of Shenzhen Municipality</orgName>
				</funder>
				<funder ref="#_8tgmCe7">
					<orgName type="full">Hong Kong Research Grants Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Shenzhen Portion of Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone</orgName>
				</funder>
				<funder ref="#_tUEBuwN">
					<orgName type="full">Hong Kong Innovation and Technology Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueming</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering and Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueyao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hon-Chi</forename><surname>Yip</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Surgery</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Markus</forename><surname>Scheppach</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Internal Medicine III -Gastroenterology</orgName>
								<orgName type="institution">University Hospital of Augsburg</orgName>
								<address>
									<settlement>Augsburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip Wai-Yan</forename><surname>Chiu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Multi-scale Medical Robotics Center</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yeung</forename><surname>Yam</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Department of Mechanical and Automation Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Helen Mei-Ling</forename><surname>Meng</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qi</forename><surname>Dou</surname></persName>
							<email>qidou@cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="494" to="504"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E20C0FFA70F8FBB6D834BFAA726D6435</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_47</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Imitation Learning</term>
					<term>Surgical Trajectory Prediction</term>
					<term>Endoscopic Submucosal Dissection</term>
					<term>Surgical Data Science</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-level cognitive assistance, such as predicting dissection trajectories in Endoscopic Submucosal Dissection (ESD), can potentially support and facilitate surgical skills training. However, it has rarely been explored in existing studies. Imitation learning has shown its efficacy in learning skills from expert demonstrations, but it faces challenges in predicting uncertain future movements and generalizing to various surgical scenes. In this paper, we introduce imitation learning to the formulated task of learning how to suggest dissection trajectories from expert video demonstrations. We propose a novel method with implicit diffusion policy imitation learning (iDiff-IL) to address this problem. Specifically, our approach models the expert behaviors using a joint state-action distribution in an implicit way. It can capture the inherent stochasticity of future dissection trajectories, therefore allows robust visual representations for various endoscopic views. By leveraging the diffusion model in policy learning, our implicit policy can be trained and sampled efficiently for accurate predictions and good generalizability. To achieve conditional sampling from the implicit policy, we devise a forward-process guided action inference strategy that corrects the state mismatch. We collected a private ESD video dataset with 1032 short clips to validate our method. Experimental results demonstrate that our solution outperforms SOTA imitation learning methods on our formulated task. To the best of our knowledge, this is the first work applying imitation learning for surgical skill learning with respect to dissection trajectory prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite that deep learning models have shown success in surgical data science to improve the quality of surgical intervention <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, such as intelligent workflow analysis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> and scene understanding <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>, research on higher-level cognitive assistance for surgery still remains underexplored. One essential task is supporting decision-making on dissection trajectories <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref>, which is challenging yet crucial for ensuring surgical safety. Endoscopic Submucosal Dissection (ESD), a surgical procedure for treating early gastrointestinal cancers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>, involves multiple dissection actions that require considerable experience to determine the optimal dissection trajectory. Informative suggestions for dissection trajectories can provide helpful cognitive assistance to endoscopists, for mitigation of intraoperative errors, reducing risks of complications <ref type="bibr" target="#b14">[15]</ref>, and facilitating surgical skill training <ref type="bibr" target="#b16">[17]</ref>. However, predicting the desired trajectory for future time frames based on the current endoscopic view is challenging. First, the decision of dissection trajectories is complicated and depends on numerous factors such as safety margins surrounding the tumor. Second, dynamic scenes and poor visual conditions may further hamper scene recognition <ref type="bibr" target="#b26">[27]</ref>. To date, there is still no work on data-driven solutions to predict such dissection trajectories, but we argue that it is possible to reasonably learn this skill from expert demonstrations based on video data.</p><p>Imitation learning has been widely studied in various domains <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> with its good ability to learn complex skills, but it still needs adaptation and improvement when being applied to learn dissection trajectory from surgical data. One challenge arises from the inherent uncertainty of future trajectories. Supervised learning such as Behavior Cloning (BC) <ref type="bibr" target="#b2">[3]</ref> tends to average all possible prediction paths, which leads to inaccurate predictions. While advanced probabilistic models are employed to capture the complexity and variability of dissection trajectories <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>, how to ensure reliable predictions across various surgical scenes still remains a great challenge. To overcome these issues, implicit models are emerging for policy learning, inspiring us to rely on implicit Behavior Cloning (iBC) <ref type="bibr" target="#b4">[5]</ref>, which can learn robust representations by capturing the shared features of both visual inputs and trajectory predictions with a unified implicit function, yielding superior expressivity and visual generalizability. However, these methods still bear their limitations. For instance, approaches leveraging energy-based models (EBMs) <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b11">12]</ref> suffer from intensive computations due to reliance on the Langevin dynamics, which leads to a slow training process. In addition, the model performance can be sensitive to data distribution and the noise in training data would result in unstable trajectory predictions.</p><p>In this paper, we explore an interesting task of predicting dissection trajectories in ESD surgery via imitation learning on expert video data. We propose Implicit Diffusion Policy Imitation Learning (iDiff-IL), a novel imitation learning approach for dissection trajectory prediction. To effectively model the surgeon's behaviors and handle the large variation of surgical scenes, we leverage implicit modeling to express expert dissection skills. To address the limitations of inefficient training and unstable performance associated with EBM-based implicit policies, we formulate the implicit policy using an unconditional diffusion model, which demonstrates remarkable ability in representing complex high-dimensional data distribution for videos. Subsequently, to obtain predictions from the implicit policy, we devise a conditional action inference strategy with the guidance of forward-diffusion, which further improves the prediction accuracy. For experimental evaluation, we collected a surgical video dataset of ESD procedures, and preprocessed 1032 short clips with dissection trajectories labelled. Results show that our method achieves superior performances in different contexts of surgical scenarios compared with representative popular imitation learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we describe our approach iDiff-IL, which learns to predict the dissection trajectory from expert video data using the implicit diffusion policy. An overview of our method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We first present the formulation of the task and the solution with implicit policy for dissection trajectory learning. Next, we present how to train the implicit policy as an unconditional generative diffusion model. Finally, we show the action inference strategy with forward-diffusion guidance which produces accurate trajectory predictions with our implicit diffusion policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Implicit Modeling for Surgical Dissection Decision-Making</head><p>In our approach, we formulate the dissection trajectory prediction to an imitation learning from expert demonstrations problem, which defines a Markov Decision Process (MDP) M = (S, A, T , D), comprising of state space S, action set A, state transition distribution T , and expert demonstrations D. The goal is to learn a prediction policy π * (a|s) from a set of expert demonstrations D. The input state of the policy is a clip of video frames s = {I t-L+1 , I t-L+2 , . . . , I t }, I t ∈ R H×W ×3 and the output is an action distribution of a sequence of 2D coordinates a = {y t+1 , y t+2 , ..., y t+N }, y t ∈ R 2 indicating the future dissection trajectory projected to the image space.</p><p>In order to obtain the demonstrated dissection trajectories from the expert video data, we first manually annotate the dissection trajectories on the video frame according to the moving trend of the instruments observed from future frames, then create a dataset D = {(s, a) i } M i=0 containing M pairs of video clip (state) and dissection trajectory (action).</p><p>To precisely predict the expert dissection behaviors and effectively learn generalizable features from the expert demonstrations, we use the implicit model as our imitation policy. Extending the formulation in <ref type="bibr" target="#b4">[5]</ref>, we model the dissection trajectory prediction policy to a maximization of the joint state-action probability density function arg max a∈A p θ (s, a) instead of an explicit mapping F θ (s). The optimal action is derived from the policy distribution conditioned on the state s, and p θ (s, a) represents the joint state-action distribution.</p><p>To learn the implicit policy from the demonstrations, we adopt the Behavior Cloning objective which is to essentially minimize the Kullback-Leibler (KL) divergence between the learning policy π θ (a|s) and the demonstration distribution D, also equivalent to maximize the expected log-likelihood of the joint state-action distribution, as shown:</p><formula xml:id="formula_0">max θ E (s,a)∼D [log π θ (a|s)] = max θ E (s,a)∼D [log p θ (s, a)].</formula><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>In this regard, the imitation of surgical dissection decision-making is converted to a distribution approximation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Implicit Policy as Diffusion Models</head><p>Approximating the joint state-action distribution in Eq. 1 from the video demonstration data is challenging for previous EBM-based methods. To address the learning of implicit policy, we rely on recent advances in diffusion models. By representing the data using a continuous thermodynamics diffusion process, which can be discretized into a series of Gaussian transitions, the diffusion model is able to express complex high-dimensional distribution with simple parameterized functions. In addition, the diffusion process also serves as a form of data augmentation by adding a range of levels of noise to the data, which guarantees a better generalization in high-dimensional state space.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref> (a), the diffusion model comprises a predefined forward diffusion process and a learnable reverse denoising process. The forward process gradually diffuses the original data x 0 = (s, a), to a series of noised data {x 0 , x 1 , • • • , x T } with a Gaussian kernel q(x t |x t-1 ), where T denotes the diffusion step. In the reverse process, the data is recovered via a parameterized Gaussian p θ (x t-1 |x t ) iteratively. With the reverse process, the joint state-action distribution in the implicit policy can be expressed as:</p><formula xml:id="formula_2">p θ (x 0 ) = x1:T p θ (x 0:T ) = x1:T p(x T ) p θ (x t-1 |x t ) = E p θ (x1:T ) p θ (x 0 |x 1 ). (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>The probability of the noised data x t in forward diffusion process is a Gaussian distribution expressed as q(x t |x 0 ) = N (x t , √ α t x 0 , (1α t )I ), where α t is a scheduled variance parameter, which can be referred from <ref type="bibr" target="#b9">[10]</ref>, and I is an identity matrix. The trainable reverse transition is a Gaussian distribution as well, whose posterior is</p><formula xml:id="formula_4">p θ (x t-1 |x t ) = N x t-1 ; μ θ (x t , t), Σ θ (x t , t) , in which μ θ (x t , t)</formula><p>and Σ θ (x t , t) are the means and the variances parameterized by a neural network.</p><p>To train the implicit diffusion policy, we maximize the log-likelihood of the state-action distribution in Eq. 1. Using the Evidence Lower Bound (ELBO) as the proxy, the likelihood maximization can be simplified to a noise prediction problem, more details can be referred to <ref type="bibr" target="#b9">[10]</ref>. Noise prediction errors for the state and the action are combined using a weight γ ∈ [0, 1] as the following:</p><formula xml:id="formula_5">L noise (θ) = E ,t,x0 [(1 -γ) a θ (x t , t) -a + γ s θ (x t , t) -s ],<label>(3)</label></formula><p>where s and a are sampled from N (0, I s ), N (0, I a ) respectively. To better process features from video frames and trajectories of coordinates, we employ a variant of the UNet as the implicit diffusion policy network, where the trajectory information is fused into feature channels via MLP embedding layers. Then the trajectory noise is predicted by an MLP branch at the bottleneck layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Conditional Sampling with Forward-Diffusion Guidance</head><p>Since the training process introduced in Sect. 2.2 is for unconditional generation, the conventional sampling strategy through the reverse process will predict random trajectories in expert data. An intuitive way to introduce the condition into the inference is to input the video clip as the condition state s * to the implicit diffusion policy directly, then only sample the action part. But there is a mismatch between the distribution of the state s * and the s t in the training process, which may lead to inaccurate predictions. Hence, we propose a sampling strategy to correct such distribution mismatch by introducing the forward-process guidance into the reverse sampling procedure.</p><p>Considering the reverse process of the diffusion model, the transition probability conditioned by s * can be decomposed as:</p><formula xml:id="formula_6">p θ (x t-1 |x t , s * ) = p θ (x t-1 |s t , a t , s * ) = p θ (x t-1 |x t )q(s t |s * ),<label>(4)</label></formula><p>where x t = (s t , a t ), p θ (x t-1 |x t ) denotes the learned denoising function of the implicit diffusion model, and q(s t |s * ) represents a forward diffusion process from the condition state to the t-th diffused state. Therefore, we can attain conditional sampling via the incorporation of forward-process guidance into the reverse sampling process of the diffusion model. The schematic illustration of our sampling approach is shown in Fig. <ref type="figure" target="#fig_0">1 (b</ref>). At the initial step t = T , action a T is sampled from a pure Gaussian noise, whereas the input state s T is diffused from the input video clip s * through a forward-diffusion process. At the t-th step of the denoising loop, the action input a t comes from the denoised action from the last time step, while the visual inputs s t are still obtained from s * via the forward diffusion process. The above forward diffusion process and the denoising step are repeated till t = 0. The final action â0 is the prediction from the implicit diffusion policy. The deterministic action can be obtained by taking the most probable samples during the reverse process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Dataset and Evaluation Metrics</head><p>Dataset. We evaluated the proposed approach on a dataset assembled from 22 videos of ESD surgery cases, which are collected from the Endoscopy Centre of the Prince of Wales Hospital in Hong Kong. All videos were recorded via Olympus microscopes operated by an expert surgeon with over 15 years of experience in ESD. Considering the inference speed, we downsampled the original videos to 2FPS frames which are resized to 128 × 128 in resolution. The input state is a 1.5-s length video clip containing 3 consecutive frames, and the expert dissection trajectory is represented by a 6-point polyline indicating the tool's movements in future 3 s. We totally annotated 1032 video clips, which contain 3 frames for each clip. We randomly selected 742 clips from 20 cases for training, consisting of 2226 frames, where 10% of these are for validation. The remaining 290 clips (consisting of 970 frames) were used for testing.</p><p>Experiment Setup. First, to study how the model performs on data within the same surgical context as the training data, we define a subset, referred as to the "in-the-context" testing set, which consists of consecutive frames selected from the same cases as included in the training data. Second, to assess the model's ability to generalize to visually distinct scenes, we created an "out-ofthe-context" testing set that is composed of video clips sampled from 2 unseen surgical cases. The sizes of these two subsets are 224 and 66 clips, respectively. Evaluation Metrics. To evaluate the performance of the proposed approach, we adopt several metrics, including commonly used evaluation metrics for trajectory prediction as used in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>, including Average Displacement Error (ADE), which respectively reports the overall deviations between the predictions and the ground truths, and Final Displacement Error (FDE) describing the difference from the moving target by computing the L2 distance between the last trajectory points. Besides, we also use the Fréchet Distance (FD) metric, to indicate the geometrical similarity between two temporal sequences. Pixel errors are used as units for all metrics, while the input images are in 128 × 128 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with State-of-the-Art Methods</head><p>To evaluate the proposed approach, we have selected popular baselines and stateof-the-art methods for comparison. We have chosen the fully supervised method, Behavior Cloning, as the baseline, which is implemented using a CNN-MLP network. In addition, we have included iBC <ref type="bibr" target="#b4">[5]</ref>, an EBM-based implicit policy learning method and MID <ref type="bibr" target="#b7">[8]</ref>, a diffusion-based trajectory prediction approach, as comparison state-of-the-art approaches.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, our method outperforms the comparison approaches in both "in-the-context" and "out-of-the-context" scenarios on all metrics. Compared with the diffusion-based method MID <ref type="bibr" target="#b7">[8]</ref>, our iDiff-IL is more effective in predicting long-term goals, particularly in the "out-of-the-context" scenes, with the evidence of 2.18 error reduction on FDE. For iBC <ref type="bibr" target="#b4">[5]</ref>, the performance did not meet our expectations and was even surpassed by the baseline. This exhibits the limitations of EBM-based methods in learning visual representations from complex endoscopic scenes. The superior results achieved by our method demonstrate the effectiveness of the diffusion model in learning the implicit policy from the expert video data. In addition, our method can learn generalizable dissection skills by exhibiting a lower standard deviation of the prediction errors compared to the BC, which severely suffers from over-fitting to the training data. The qualitative results are presented in Fig. <ref type="figure" target="#fig_1">2</ref>. We selected three typical scenes in ESD surgery (i.e., submucosa dissection, mucosa dissection and mucosa incision), and showed the predictions of iDiff-IL accompanying the ground truth trajectories. From the results, our method can generate reasonable visual guidance aligning with the expert demonstrations on both evaluation sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>Implicit Modeling. First, we examined the importance of using implicit modeling as the policy representation. We simulated the explicit form of the imitation policy by training a conditional diffusion model whose conditional input is a video clip. According to the bar charts in Fig. <ref type="figure" target="#fig_2">3</ref>, the explicit diffusion policy shows a performance drop for both evaluation sets on ADE compared with the implicit form. The implicit modeling makes a more significant contribution in predicting within the "in-the-context" scenes, suggesting that the implicit model excels at capturing subtle changes in surgical scenes. While our method improves marginally compared with the explicit form on the "out-of-the-context" data, exhibiting a slighter over-fitting with a lower standard deviation.</p><p>Forward-Diffusion Guidance. We also investigated the necessity of the forward-diffusion guidance in conditional sampling for prediction accuracy. We remove the forward-diffusion guidance during the action sampling procedure so that the condition state is directly fed into the policy while sampling actions through the reverse process. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the implicit diffusion policy benefits more from the forward-diffusion guidance in the "in-the-context" scenes, achieving an improvement of 0.33 on ADE. When encountered with the unseen scenarios in "out-of-the-context" data, the performance improvement of such inference strategy is marginal.</p><p>Value of Synthetic Data. Since the learned implicit diffusion policy is capable of generating synthetic expert dissection trajectory data, which can potentially reduce the expensive annotation cost. To better explore the value of such synthetic expert data for downstream tasks, we train the baseline model with the generated expert demonstrations. We randomly generated 9K video-trajectory pairs by unconditional sampling from the implicit diffusion policy. Then, we train the BC model with different data, the pure expert data (real), synthetic data only (synt) and the mixed data with the real and the synthetic (mix).</p><p>The table in Fig. <ref type="figure" target="#fig_2">3</ref> shows the synthetic data is useful as the augmented data for downstream task learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents a novel approach on imitation learning from expert video data, in order to achieve dissection trajectory prediction in endoscopic surgical procedure. Our iDiff-IL method utilizes a diffusion model to represent the implicit policy, which enhances the expressivity and visual generalizability of the model. Experimental results show that our method outperforms state-of-the-art approaches on the evaluation dataset, demonstrating the effectiveness of our approach for learning dissection skills in various surgical scenarios. We hope that our work can pave the way for introducing the concept of learning from expert demonstrations into surgical skill modelling, and motivate future exploration on higher-level cognitive assistance in computer-assisted intervention.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our imitation learning method for surgical dissection trajectory prediction. (a) illustrates the modeling of the implicit policy, and its training process.We train a diffusion model to approximate the joint state-action distribution, and (b) depicts the inference loop for trajectory prediction with the forward-diffusion guidance.</figDesc><graphic coords="3,54,36,70,10,323,68,182,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Typical results of our imitation learning method under settings of in-the-context and out-of-the-context evaluations. Green and yellow arrows respectively denote ground truths and predictions of dissection trajectory. (Color figure online)</figDesc><graphic coords="8,72,66,72,26,317,68,152,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Left: ablation study of key method components; Middle: visualization of reverse processes of unconditional/conditional sampling from implicit policy; Right: performance of BC trained with synthetic data v.s. our method on ADE.</figDesc><graphic coords="9,151,11,153,74,113,20,64,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on the in-the-context and the out-of-the-context data in metrics of ADE/FDE/FD. Values in parentheses denote video-clip wise standard deviation. The lower is the better.</figDesc><table><row><cell cols="2">Method In-the-context</cell><cell></cell><cell></cell><cell cols="2">Out-of-the-context</cell></row><row><cell></cell><cell>ADE</cell><cell>FDE</cell><cell>FD</cell><cell>ADE</cell><cell>FDE</cell><cell>FD</cell></row><row><cell>BC</cell><cell cols="6">10.43 (±5.52) 16.68 (±10.59) 24.92 (±14.59) 13.67 (±6.43) 17.03 (±12.50) 29.23 (±16.56)</cell></row><row><cell cols="7">iBC[5] 15.54 (±4.79) 22.66 (±8.06) 35.26 (±11.78) 15.81 (±4.66) 19.66 (±7.56) 31.66 (±9.14)</cell></row><row><cell cols="7">MID[8] 9.90 (±0.66) 15.26 (±1.35) 23.78 (±1.73) 12.42 (±1.89) 16.32 (±3.11) 27.04 (±4.79)</cell></row><row><cell>Ours</cell><cell cols="6">9.47 (±1.66) 13.85 (±2.01) 21.43 (±3.89) 10.21 (±3.17) 14.14 (±3.63) 21.56 (±5.97)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by <rs type="funder">Shenzhen Portion of Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone</rs> under HZQB-KCZYB-20200089, in part by <rs type="funder">Hong Kong Innovation and Technology Commission</rs> Project No. <rs type="grantNumber">ITS/237/21FP</rs>, in part by <rs type="funder">Hong Kong Research Grants Council</rs> Project No. <rs type="grantNumber">T45-401/22</rs>-N, in part by <rs type="funder">Science, Technology and Innovation Commission of Shenzhen Municipality</rs> Project No. <rs type="grantNumber">SGDX20220530111201008</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tUEBuwN">
					<idno type="grant-number">ITS/237/21FP</idno>
				</org>
				<org type="funding" xml:id="_8tgmCe7">
					<idno type="grant-number">T45-401/22</idno>
				</org>
				<org type="funding" xml:id="_hU6JHYh">
					<idno type="grant-number">SGDX20220530111201008</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11190</idno>
		<title level="m">robotic scene segmentation challenge</title>
		<imprint>
			<date type="published" when="2018">2018. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endoscopic submucosal dissection (ESD) compared with gastrectomy for treatment of early gastric neoplasia: a retrospective cohort study</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W Y</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surg. Endosc</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3584" to="3591" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring the limitations of behavior cloning for autonomous driving</title>
		<author>
			<persName><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9329" to="9338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Implicit behavioral cloning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="158" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Implicit kinematic policies: unifying joint and cartesian action spaces in end-to-end robot learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2656" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine learning for surgical phase recognition: a systematic review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Garrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="684" to="693" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic trajectory prediction via motion indeterminacy diffusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="17113" to="17122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel trajectory predicting method of catheter for the vascular interventional surgical robot</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Mechatronics and Automation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1304" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imitation learning: a survey of learning methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Strictly batch imitation learning by energybased distribution matching</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7354" to="7365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trans-svnet: hybrid embedding aggregation transformer for surgical workflow analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2193" to="2202" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imitation learning as f-divergence minimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Foundations of Robotics XIV: Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="313" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factors predictive of perforation during endoscopic submucosal dissection for the treatment of colorectal tumors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page" from="573" to="578" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imitation learning for improved 3D pet/MR attenuation correction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kläser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102079</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Laparoscopic or open cholecystectomy in cirrhosis: a systematic review of outcomes and meta-analysis of randomized trials</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Laurence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Pleass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HPB</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on imitation learning techniques for end-to-end autonomous vehicles</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Mero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dianati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mouzakitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="14128" to="14147" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Infogail: interpretable imitation learning from visual demonstrations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Artificial intelligence and surgical decision-making</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Loftus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Surg</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="158" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Surgical data science-from concepts toward clinical translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">102306</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Surgical data science for next-generation interventions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="691" to="696" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social-STGCNN: a social spatio-temporal graph convolutional neural network for human trajectory prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Claudel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14424" to="14432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Davincinet: joint prediction of motion and surgical state in robot-assisted surgery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feyzabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Azizian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalization guarantees for imitation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Veer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1426" to="1442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive social behavior graph for trajectory prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="660" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time landmark detection for precise endoscopic submucosal dissection via shape-aware relation network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102291</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural rendering for stereo 3D reconstruction of deformable tissues in robotic surgery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="431" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time trajectory prediction of laparoscopic instrument tip based on long short-term memory neural network in laparoscopic surgery training</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Med. Robot. Comput. Assist. Surg</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2441</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Symmetric dilated convolution for surgical gesture recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-039" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="409" to="418" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
