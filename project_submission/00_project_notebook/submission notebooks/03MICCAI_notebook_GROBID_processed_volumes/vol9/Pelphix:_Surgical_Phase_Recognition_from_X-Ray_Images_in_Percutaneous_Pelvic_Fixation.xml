<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medical Image Computing and Computer Assisted Intervention – MICCAI 2023</title>
				<funder ref="#_fAmGnJy">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">Johns Hopkins University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benjamin</forename><forename type="middle">D</forename><surname>Killeen</surname></persName>
							<email>killeen@jhu.edu</email>
							<idno type="ORCID">0000-0003-2511-7929</idno>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Mangulabnan</surname></persName>
							<idno type="ORCID">0009-0005-5941-0535</idno>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehran</forename><surname>Armand</surname></persName>
							<email>marmand2@jhmi.edu</email>
							<idno type="ORCID">0000-0003-1028-8303</idno>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Russell</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
							<idno type="ORCID">0000-0001-6272-1100</idno>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><surname>Osgood</surname></persName>
							<email>gosgood2@jhmi.edu</email>
							<idno type="ORCID">0000-0001-6271-4971</idno>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><surname>Unberath</surname></persName>
							<email>unberath@jhu.edu</email>
							<idno type="ORCID">0000-0002-0055-9950</idno>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">87286C313097C9DF0A958536CF92365C</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Activity recognition</term>
					<term>fluoroscopy</term>
					<term>orthopedic surgery</term>
					<term>surgical data science</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surgical phase recognition (SPR) is a crucial element in the digital transformation of the modern operating theater. While SPR based on video sources is well-established, incorporation of interventional X-ray sequences has not yet been explored. This paper presents Pelphix, a first approach to SPR for X-ray-guided percutaneous pelvic fracture fixation, which models the procedure at four levels of granularity -corridor, activity, view, and frame value -simulating the pelvic fracture fixation workflow as a Markov process to provide fully annotated training data. Using added supervision from detection of bony corridors, tools, and anatomy, we learn image representations that are fed into a transformer model to regress surgical phases at the four granularity levels. Our approach demonstrates the feasibility of X-ray-based SPR, achieving an average accuracy of 99.2% on simulated sequences and 71.7% in cadaver across all granularity levels, with up to 84% accuracy for the target corridor in real data. This work constitutes the first step toward SPR for the X-ray domain, establishing an approach to categorizing phases in X-ray-guided surgery, simulating realistic image sequences to enable machine learning model development, and demonstrating that this approach is feasible for the analysis of real procedures. As X-ray-based SPR continues to mature, it will benefit procedures in orthopedic surgery, angiography, and interventional radiology by equipping intelligent surgical systems with situational awareness in the operating room. Code and data available at https://github.com/benjamindkilleen/pelphix.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In some ways, surgical data is like the expanding universe: 95% of it is dark and unobservable <ref type="bibr" target="#b1">[2]</ref>. The vast majority of intra-operative X-ray images, for example, are "dark", in that they are not further analyzed to gain quantitative insights into routine practice, simply because the human-hours required would drastically outweigh the benefits. As a consequence, much of this data not only goes un-analyzed but is discarded directly from the imaging modality after inspection. Fortunately, machine learning algorithms for automated intra-operative image analysis are emerging as an opportunity to leverage these data streams. A popular application is surgical phase recognition (SPR), a way to obtain quantitative analysis of surgical workflows and equip automated systems with situational awareness in the operating room (OR). SPR can inform estimates of surgery duration to maximize OR throughput <ref type="bibr" target="#b6">[7]</ref> and augment intelligent surgical systems, e.g. for suturing <ref type="bibr" target="#b20">[20]</ref> or image acquisition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, enabling smooth transitions from one specialized subsystem to the next. Finally, SPR provides the backbone for automated skill analysis to produce immediate, granular feedback based on a specific surgeon's performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">21]</ref>. The possibilities described above have motivated the development of algorithms for surgical phase recognition based on the various video sources in the OR <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref>. However, surgical phase recognition based on interventional X-ray sequences remains largely unexplored. Although X-ray guidance informs more than 17 million procedures across the United States (as of 2006) <ref type="bibr" target="#b12">[13]</ref>, the unique challenges of processing X-ray sequences compared to visible or structured light imaging have so far hindered research in this area. Video cameras collect many images per second from relatively stationary viewpoints. By contrast, C-arm Xray imaging often features consecutive images from vastly different viewpoints, resulting in highly varied object appearance due to the transmissive nature of X-rays. X-ray images are also acquired irregularly, usually amounting to several hundred frames in a procedure of several hours, limiting the availability of training data for machine learning algorithms.</p><p>Following recent work that enables sim-to-real transfer in the X-ray domain <ref type="bibr" target="#b5">[6]</ref>, we now have the capability to train generalizable deep neural networks (DNNs) using simulated images, where rich annotations are freely available. This paper represents the first step in breaking open SPR for the X-ray domain, establishing an approach to categorizing phases, simulating realistic image sequences, and analyzing real procedures. We focus our efforts on percutaneous pelvic fracture fixation, which involves the acquisition of standard views and the alignment of Kirschner wires (K-wires) and orthopedic screws with bony corridors <ref type="bibr" target="#b16">[17]</ref>. We model the procedure at four levels, the current target corridor, activity (position-wire, insert-wire, and insert-screw), C-arm view (AP, lateral, etc.), and frame-level clinical value. Because of radiation exposure for both patients and clinicians, it is relevant to determine which X-ray images are acquired in the process of "fluoro-hunting" (hunting) versus those used for clinical assessment. Each of these levels is modeled as a Markov process in a stochastic simulation, which provides fully annotated training data for a transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>SPR from video sources is a popular topic, and has benefited from the advent of transformer architectures for analyzing image sequences. The use of convolutional layers as an image encoder has proven effective for recognizing surgical phases in endoscopic video <ref type="bibr" target="#b22">[22]</ref> and laparoscopic video <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">19]</ref>. These works especially demonstrate the effectiveness of transformers for dealing with long image sequences <ref type="bibr" target="#b2">[3]</ref>, while added spatial annotations improve both the precision and information provided by phase recognition <ref type="bibr" target="#b19">[19]</ref>. Although some work explores activity recognition in orthopedic procedures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> they rely on head-mounted cameras with no way to assess tool-to-tissue relationships in percutaneous procedures. The inclusion of X-ray image data in this space recenters phase recognition on patient-centric data and makes possible the recognition of surgical phases which are otherwise invisible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The Pelphix pipeline consists of stochastic simulation of X-ray image sequences, based on a large database of annotated CT images, and a transformer architecture for phase recognition with additional task-aware supervision. A statistical shape model is used to propagate landmark and corridor annotations over 337 CTs, as shown in Fig. <ref type="figure" target="#fig_2">2a</ref>. The simulation proceeds by randomly aligning virtual K-wires and screws with the annotated corridors (Sect. 3.1). In Sect. 3.2, we describe a transformer architecture with a U-Net style encoder-decoder structure enables sim-to-real transfer for SPR in X-ray.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Sequence Simulation for Percutaneous Fixation</head><p>Unlike sequences collected from real surgery <ref type="bibr" target="#b14">[15]</ref> or human-driven simulation <ref type="bibr" target="#b13">[14]</ref>, our workflow simulator must capture the procedural workflow while also maintaining enough variation to allow algorithms to generalize. We accomplish this by modeling the procedural state as a Markov process, in which the transitions depend on evaluations of the projected state, as well as an adjustment factor λ adj ∈ [0, 1] that affects the number of images required for a given task. A low adjustment factor decreases the probability of excess acquisitions for the simulated procedure. In our experiments, we sample λ adj ∈ U(0.6, 0.8) at the beginning of each sequence. Figure <ref type="figure" target="#fig_3">3</ref> provides an overview of this process. Given a CT image with annotated corridors, we first sample a target corridor with start and endpoints a, b ∈ R 3 . For the ramus corridors, we randomly swap the start and endpoints to simulate the retrograde and antegrade approaches. We then uniformly sample the initial wire tip position within 5 mm of a and the direction within 15 • of ba. Sample Desired View. The desired view is sampled from views appropriate for the current target corridor. For example, appropriate views for evaluating wire placement in the superior ramus corridor are typically the inlet and obturator oblique views, and other views are sampled with a smaller probability. We refer to the "oblique left" and "oblique right" view independent of the affected patient side, so that for the right pubic ramus, the obturator oblique is the "oblique left" view, and the iliac oblique is "oblique right." We define the "ideal" principle ray direction r * for each standard view in the anterior pelvic plane (APP) coordinate system (see supplement) and the ideal viewing point p * as the midpoint of the target corridor. At the beginning of each sequence, we sample the intrinsic camera matrix of the virtual C-arm with sensor width w s ∼ U(300, 400) mm, d sd ∼ U(900, 1200), and an image size of 384 × 384. Given a viewing point and direction (p, r), the camera projection matrix P is computed with the X-ray source (or camera center) at p -d sp r and principle ray r, where d sp U(0.65 d sd , 0.75 d sd ) is the source-to-viewpoint distance, and d sd is the source-to-detector distance (or focal length) of the virtual C-arm.</p><p>Evaluate View. Given a current view (p, r) and desired view (p * , r * ), we first evaluate whether the current view is acceptable and, if it is not, make a random adjustment. View evaluation considers the principle ray alignment and whether the viewing point is reasonably centered in the image, computing,</p><formula xml:id="formula_0">r • r * &lt; cos(θ t ) AND Pp * -H 2 W 2 1 T &lt; 2 5 min(H, W )<label>(1)</label></formula><p>where the angular tolerance θ t ∈ [3 • , 10 • ] depends on the desired view, ranging from teardrop views (low) to lateral (high tolerance).</p><p>Sample View. If Eq. 1 is not satisfied, then we sample a new view (p, r) uniformly within a uniform window that shrinks every iteration by the adjustment factor, according to</p><formula xml:id="formula_1">p ∼ U • (p * , clip(λ adj ||p * -p|| , 5 mm, 100 mm)) (2) r ∼ U (r * , clip(λ adj arccos(r * • r), 1 • , 45 • )) , (<label>3</label></formula><formula xml:id="formula_2">)</formula><p>where U • (c, r) is the uniform distribution in the sphere with center c and radius r, and U (r, θ) is the uniform distribution on the solid angle centered on r with colatitude angle θ. This formula emulates observed fluoro-hunting by converging on the desired view until a point, when further adjustments are within the same random window <ref type="bibr" target="#b11">[12]</ref>. We proceed by alternating view evaluation and sampling until evaluation is satisfied, at which point the simulation resumes with the current activity: wire positioning, wire insertion, or screw insertion.</p><p>Evaluate Wire Placement. During wire positioning, we evaluate the current wire position and make adjustments from the current view, iterating until evaluation succeeds. Given the current wire tip x, direction v, and projection matrix P, the wire placement is considered "aligned" if it appears to be aligned with the projected target corridor in the image, modeled as a cylinder. In addition, we include a small likelihood of a false positive evaluation, which diminishes as the wire is inserted.</p><p>Sample Wire Placement. If the wire evaluation determines the current placement is unsuitable, then a new wire placement is sampled. For the down-thebarrel views, this is done similarly to Eq. 2, by bringing the wire closer to the corridor in 3D. For orthogonal views, repositioning consists of a small random adjustment to x, a rotation about the principle ray (the in-plane component), and a minor perturbation orthogonal to the ray (out-of-plane). This strategy emulates real repositioning by only adjusting the degree of freedom visible in the image, i.e. the projection onto the image plane:</p><formula xml:id="formula_3">x ∼ U • (x, clip(λ adj ||x -a||, 5 mm, 10 mm)) (4) v ← Rot (v × r, θ ⊥ ) Rot r, θ * + θ , where θ ⊥ ∼ U(-0.1 θ * , 0.1 θ * ), (<label>5</label></formula><formula xml:id="formula_4">) θ ∼ U(-clip(λ adj θ * , 3 • , 10 • ) , clip(λ adj θ * , 3 • , 10 • )), (<label>6</label></formula><formula xml:id="formula_5">)</formula><p>and θ * is the angle between the wire and the target corridor in the image plane.</p><p>If the algorithm returns "Good," the sequence either selects a new view to acquire (and stays in the position-wire activity) or proceeds to insert-wire or insert-screw, according to random transitions.</p><p>In our experiments, we used 337 CT images: 10 for validation, and 327 for generating the training set. (Training images were collected continuously during development, after setting aside a validation set.) A DRR was acquired at every decision point in the simulation, with a maximum of 1000 images per sequence, and stored along with segmentations and anatomical landmarks. We modeled a K-wire with 2 mm diameter and orthopedic screws with lengths from 30 to 130 mm and a 16 mm thread, with up to eight instances of each in a given sequence. Using a customized version of DeepDRR <ref type="bibr" target="#b17">[18]</ref>, we parallelized image generation across 4 RTX 3090 GPUs with an observed GPU memory footprint of ∼ 13 GB per worker, including segmentation projections. Over approximately five days, this resulted in a training set of 677 sequences totaling 279,709 images and 22 validation sequences with 8,515 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer Architecture for X-ray-based SPR</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the transformer architecture used to predict surgical phases based on embedding tokens for each frame. To encourage local temporal features in each embedding token, we cross-pollinate adjacent frames in the channel dimension, so that each (3, H, W ) encoder input contains the previous, current, and next frame. The image encoder is a U-Net <ref type="bibr" target="#b15">[16]</ref> encoder-decoder variant with 5 Down and Up blocks and 33 spatial output channels, consisting of (a) 7 segmentation masks of the left hip, right hip, left femur, right femur, sacrum, L5 vertebra, and pelvis; (b) 8 segmentation masks of bony corridors, including the ramus (2), teardrop (2) and sacrum corridors (4), as in Fig. <ref type="figure" target="#fig_2">2a</ref>; (c) 2 segmentation masks for wires and screws; and (d) 16 heatmaps corresponding to the anatomical landmarks in Fig. <ref type="figure" target="#fig_2">2a</ref>. These spatial annotations provide additional supervision, trained with DICE loss L DICE for segmentation channels and normalized cross correlation L NCC for heatmap channels as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. To compute tokens for input to the transformer, we apply a 1 × 1 Conv + BatchNorm + ReLU block with kernel size 512 to the encoder output, followed by global average pooling. The transformer has 6 layers with 8 attention heads and a feedforward dimension of 2048. During training and inference, we apply forward masking so that only previous frames are considered. The output of the transformer are vectors in R 21  Training Details. Following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>, we use a pipeline of heavy domain randomization techniques to enable sim-to-real transfer. In our experiments, we trained the transformer for 200 epochs on 2 RTX 3090 GPUs with 24 GB of memory each, with a sequence length of 48 and a batch size of 4. The initial learning rate was 0.0001, reduced by a factor of 10 at epoch 150 and 180.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Simulation. We report the results of our approach first on simulated image sequences, generated from the withheld set of CT images, which serves as an upper bound on real X-ray performance. In this context our approach achieves an accuracy of 99.7%, 98.2%, 99.8%, and 99.0% with respect to the corridor, activity, view, and frame level, respectively. Moreover, we achieve an average DICE score of 0.72 and landmark detection error of 1.01 ± 0.153 pixels in simulation, indicating that these features provide a meaningful signal. That the model generalizes so well to the validation set reflects the fact that these sequences are sampled using the same Markov-based simulation as the training data. Cadaver Study. We evaluate our approach on cadaveric image sequences with five screw insertions. An attending orthopedic surgeon performed percutaneous fixation on a lower torso specimen, taking the antegrade approach for the left and right pubic ramus corridors, followed by the left and right teardrop and S1 screws. An investigator acted as the radiological technician, positioning a mobile C-arm according to the surgeon's direction. A total of 257 images were acquired during these fixations, with phase labels based on the surgeon's narration.</p><p>Our results, shown in Fig. <ref type="figure" target="#fig_4">4</ref> demonstrate the potential for Pelphix as a viable approach to SPR in X-ray. We achieve an overall accuracy of 84%, 60%, 65%, and 77% with respect to the corridor, activity, view, and frame levels, respectively. Figure <ref type="figure" target="#fig_6">5</ref> shows exemplary success and failure modes for our approach, which struggles with ambiguities that may arise due to the similarity of certain views. For instance, image 98 was acquired during fluoro-hunting for the teardrop left view, but our approach associates this sequence with verification of the left ramus screw, which was just finished. Similarly, after the right teardrop wire was inserted, our approach anticipated the insertion of an S2 wire. This was a valid transition in our simulation, so surgeon preferences may be needed to resolve the ambiguity. At the same time, we observe significantly higher accuracy for the pubic ramus corridors (97.7, 76.9, 98.3, and 84.4% respectively) than the teardrop (60.2, 56.9, 64.2, 73.7%) and S1 corridors (100%, 40.6%, 23%, 71%), which may reflect the challenges of interpreting associated images or simply the accumulation of orthopedic hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>As our results show, Pelphix is a potentially viable approach to robust SPR based on X-ray images. We showed that stochastic simulation of percutaneous fracture fixation, despite having no access to real image sequences, is a sufficiently realistic data source to enable sim-to-real transfer. While we expect adjustments to the simulation approach will close the gap even further, truly performative SPR algorithms for X-ray may rely on Pelphix-style simulation for pretraining, before fine-tuning on real image sequences to account for human-like behavior. Extending this approach to other procedures in orthopedic surgery, angiography, and interventional radiology will require task-specific simulation capable of modeling possibly more complex tool-tissue interactions and human-in-the-loop workflows. Nevertheless, Pelphix provides a viable first route toward X-ray-based surgical phase recognition, which we hope will motivate routine collection and interpretation of these data, in order to enable advances in surgical data science that ultimately improve the standard of care for patients.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our model architecture incorporates frame-level spatial annotations using a U-Net encoder-decoder variant. Anatomical landmarks and segmentation maps provide added supervision to the image encoder for a transformer, which predicts the surgical phase. The images shown here are the result of Markov-based simulation of percutaneous fixation, used for training.</figDesc><graphic coords="2,42,30,291,86,339,16,150,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Corridors and Landmarks (b) Anterior Pelvic Plane</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The ramus, teardrop and S2 bony corridors, as well as 16 anatomical landmarks with added supervision for phase recognition. (b) The anterior pelvic plane (APP) coordinate system is used to define principle ray directions for standard views of the pelvis, enabling realistic simulation of image sequences for percutaneous fixation.</figDesc><graphic coords="4,70,80,123,11,282,94,112,87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The image sequence simulation pipeline for Pelphix. We model the procedure as a Markov random process, where transition probabilities depend on realistic evaluation of the current frame.</figDesc><graphic coords="5,56,46,65,75,328,45,105,97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results of surgical phase recognition for a cadaveric procedure. We observe varying performance based on the target corridor, either because of the associated views or due to the accumulated orthopedic hardware.</figDesc><graphic coords="7,57,96,54,14,336,28,116,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>with phase predictions for each frame, corresponding to (a) the 8 target corridors; (b) 3 activities (position-wire, insert-wire, or insert-screw); (c) 8 standard views (see Sect. 3.1); and (d) 2 frame values (hunting or assessment).We compute the cross entropy loss separately for the corridor L cor , activity L act , view L view , and frame L fr phases, and take the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Exemplary cases from the cadaver study. Image 55 is an ideal prediction, whereas image 56 merely confirms the final placement. Image 98 illustrates an ambiguity that can arise between the teardrop and oblique views during fluoro-hunting, while image 164 is interpreted as belonging to an S2 insertion.</figDesc><graphic coords="8,42,30,205,79,339,40,81,40" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">NIH</rs> Grant No. <rs type="grantNumber">R21EB028505</rs> and <rs type="funder">Johns Hopkins University</rs> internal funds. Thank you to <rs type="person">Demetries Boston</rs>, <rs type="person">Henry Phalen</rs>, and <rs type="person">Justin Ma</rs> for assistance with cadaver experiments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fAmGnJy">
					<idno type="grant-number">R21EB028505</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">X-ray-transform invariant anatomical landmark detection for pelvic trauma surgery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bier</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-37" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dark matter and dark energy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamionkowski</surname></persName>
		</author>
		<idno type="DOI">10.1038/458587a</idno>
		<ptr target="https://doi.org/10.1038/458587a" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">458</biblScope>
			<biblScope unit="issue">7238</biblScope>
			<biblScope unit="page" from="587" to="589" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OperA: attention-regularized transformers for surgical phase recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ostler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-158" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part IV</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="604" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SCAN: system for camera autonomous navigation in robotic-assisted surgery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Da Col</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menciassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kazanzides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Momi</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS45743.2020.9341548</idno>
		<ptr target="https://doi.org/10.1109/IROS45743.2020.9341548" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2996" to="3002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmenting and classifying activities in robot-assisted surgery with recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dipietro</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-019-01953-x</idno>
		<ptr target="https://doi.org/10.1007/s11548-019-01953-x" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2005">2005-2020 (2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SyntheX: scaling up learning-based X-ray image analysis through in silico experiments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.06127</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2206.06127" />
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for surgical phase recognition using endoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C P</forename><surname>Guédon</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00464-020-08110-5</idno>
		<idno>00464- 020-08110-5</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Surg. Endosc</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6150" to="6157" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time surgical tools recognition in total knee arthroplasty using deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nishio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hiranaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kobashi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIEV.2018.8641074</idno>
		<ptr target="https://doi.org/10.1109/ICIEV.2018.8641074" />
	</analytic>
	<monogr>
		<title level="m">Joint 7th International Conference on Informatics, Electronics &amp; Vision (ICIEV) and 2018 2nd International Conference on Imaging, Vision &amp; Pattern Recognition (icIVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="470" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards video-based surgical workflow understanding in open orthopaedic surgery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<idno type="DOI">10.1080/21681163.2020.1835552</idno>
		<ptr target="https://doi.org/10.1080/21681163.2020.1835552" />
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Biomech. Biomed. Eng. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="293" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">C-Arm positioning for spinal standard projections in different intra-operative settings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kausch</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-134" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="352" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An autonomous X-ray image acquisition and interpretation system for assisting percutaneous pelvic fracture fixation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Killeen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-023-02941-y</idno>
		<ptr target="https://doi.org/10.1007/s11548-023-02941-y" />
	</analytic>
	<monogr>
		<title level="j">Int. J. CARS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mixed reality interfaces for achieving desired views with robotic X-ray systems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Killeen</surname></persName>
		</author>
		<idno type="DOI">10.1080/21681163.2022.2154272</idno>
		<ptr target="https://doi.org/10.1080/21681163.2022.2154272" />
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Biomech. Biomed. Eng. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Occupational radiation doses to operators performing fluoroscopically-guided procedures</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1097/HP.0b013e31824dae76</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="j">Health Phys</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Virtual reality for synergistic surgical training and data generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Munawar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Biomech. Biomed. Eng. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine and deep learning for workflow recognition during surgery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="DOI">10.1080/13645706.2019.1584116</idno>
		<ptr target="https://doi.org/10.1080/13645706" />
	</analytic>
	<monogr>
		<title level="j">Minim. Invasive Therapy Allied Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2019">2019. 2019.1584116</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Internal fixation of the unstable anterior pelvic ring a biomechanical comparison of standard plating techniques and the retrograde medullary superior pubic ramus screw</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Simonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L C</forename><surname>Routt</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Harrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Tencer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Orthop. Trauma</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">476</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepDRR -a catalyst for machine learning in fluoroscopyguided procedures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-312" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards holistic surgical scene understanding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Valderrama</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-142" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="442" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collaborative suturing: a reinforcement learning approach to automate hand-off task in suturing for surgical robots</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Varier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tavakkolmoghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="DOI">10.1109/RO-MAN47096.2020.9223543</idno>
		<ptr target="https://doi.org/10.1109/RO-MAN47096" />
	</analytic>
	<monogr>
		<title level="m">2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">9223543</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-modal self-supervised representation learning for gesture and skill recognition in robotic surgery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kazanzides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-021-02343-y</idno>
		<ptr target="https://doi.org/10.1007/s11548-021-02343-y" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="779" to="787" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards accurate surgical workflow recognition with convolutional networks and transformers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1080/21681163.2021.2002191</idno>
		<ptr target="https://doi.org/10.1080/21681163.2021.2002191" />
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Biomech. Biomed. Eng. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="349" to="356" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepPhase: surgical phase recognition in CATARACTS videos</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zisimopoulos</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-331" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
