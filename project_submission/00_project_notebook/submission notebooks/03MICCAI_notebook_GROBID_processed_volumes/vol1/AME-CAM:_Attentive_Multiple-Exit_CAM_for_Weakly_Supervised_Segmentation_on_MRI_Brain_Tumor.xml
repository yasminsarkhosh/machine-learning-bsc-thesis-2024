<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu-Jen</forename><surname>Chen</surname></persName>
							<email>yujenchen@gapp.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinrong</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiyu</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tsung-Yi</forename><surname>Ho</surname></persName>
							<email>tyho@cse.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="173" to="182"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B20A50D4823BE1B70147586CDF817117</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_17</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Tumor segmentation • Weakly-supervised semantic segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Magnetic resonance imaging (MRI) is commonly used for brain tumor segmentation, which is critical for patient evaluation and treatment planning. To reduce the labor and expertise required for labeling, weakly-supervised semantic segmentation (WSSS) methods with class activation mapping (CAM) have been proposed. However, existing CAM methods suffer from low resolution due to strided convolution and pooling layers, resulting in inaccurate predictions. In this study, we propose a novel CAM method, Attentive Multiple-Exit CAM (AME-CAM), that extracts activation maps from multiple resolutions to hierarchically aggregate and improve prediction accuracy. We evaluate our method on the BraTS 2021 dataset and show that it outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning techniques have greatly improved medical image segmentation by automatically extracting specific tissue or substance location information, which facilitates accurate disease diagnosis and assessment. However, most deep learning approaches for segmentation require fully or partially labeled training datasets, which can be time-consuming and expensive to annotate. To address this issue, recent research has focused on developing segmentation frameworks that require little or no segmentation labels.</p><p>To meet this need, many researchers have devoted their efforts to Weakly-Supervised Semantic Segmentation (WSSS) <ref type="bibr" target="#b20">[21]</ref>, which utilizes weak supervision, such as image-level classification labels. Recent WSSS methods can be broadly categorized into two types <ref type="bibr" target="#b3">[4]</ref>: Class-Activation-Mapping-based (CAM-based) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>, and Multiple-Instance-Learning-based (MIL-based) <ref type="bibr" target="#b14">[15]</ref> methods.</p><p>The literature has not adequately addressed the issue of low-resolution Class-Activation Maps (CAMs), especially for medical images. Some existing methods, such as dilated residual networks <ref type="bibr" target="#b23">[24]</ref> and U-Net segmentation architecture <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>, have attempted to tackle this issue, but still require many upsampling operations, which the results become blurry. Meanwhile, LayerCAM <ref type="bibr" target="#b8">[9]</ref> has proposed a hierarchical solution that extracts activation maps from multiple convolution layers using Grad-CAM <ref type="bibr" target="#b15">[16]</ref> and aggregates them with equal weights. Although this approach successfully enhances the resolution of the segmentation mask, it lacks flexibility and may not be optimal.</p><p>In this paper, we propose an Attentive Multiple-Exit CAM (AME-CAM) for brain tumor segmentation in magnetic resonance imaging (MRI). Different from recent CAM methods, AME-CAM uses a classification model with multipleexit training strategy applied to optimize the internal outputs. Activation maps from the outputs of internal classifiers, which have different resolutions, are then aggregated using an attention model. The model learns the pixel-wise weighted sum of the activation maps by a novel contrastive learning method.</p><p>Our proposed method has the following contributions:</p><p>-To tackle the issues in existing CAMs, we propose to use multiple-exit classification networks to accurately capture all the internal activation maps of different resolutions. -We propose an attentive feature aggregation to learn the pixel-wise weighted sum of the internal activation maps. -We demonstrate the superiority of AME-CAM over state-of-the-art CAM methods in extracting segmentation results from classification networks on the 2021 Brain Tumor Segmentation Challenge (BraTS 2021) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14]</ref>. -For reproducibility, we have released our code at https://github.com/windstormer/AME-CAM Overall, our proposed method can help overcome the challenges of expensive and time-consuming segmentation labeling in medical imaging, and has the potential to improve the accuracy of disease diagnosis and assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attentive Multiple-Exit CAM (AME-CAM)</head><p>The proposed AME-CAM method consists of two training phases: activation extraction and activation aggregation, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In the activation extraction phase, we use a binary classification network, e.g., ResNet-18, to obtain the class probability y = f (I) of the input image I. To enable multipleexit training, we add one internal classifier after each residual block, which generates the activation map M i of different resolutions. We use a cross-entropy loss to train the multiple-exit classifier, which is defined as where GAP (•) is the global-average-pooling operation, CE(•) is the cross-entropy loss, and L is the image-wise ground-truth label.</p><formula xml:id="formula_0">loss = 4 i=1 CE(GAP (M i ), L)<label>(1)</label></formula><p>In the activation aggregation phase, we create an efficient hierarchical aggregation method to generate the aggregated activation map M f by calculating the pixel-wise weighted sum of the activation maps M i . We use an attention network A(•) to estimate the importance of each pixel from each activation map. The attention network takes in the input image I masked by the activation map and outputs the pixel-wised importance score S xyi of each activation map. We formulate the operation as follows:</p><formula xml:id="formula_1">S xyi = A([I ⊗ n(M i )] 4 i=1 )<label>(2)</label></formula><p>where [•] is the concatenate operation, n(•) is the min-max normalization to map the range to [0,1], and ⊗ is the pixel-wise multiplication, which is known as image masking. The aggregated activation map M f is then obtained by the pixel-wise weighted sum of M i , which is</p><formula xml:id="formula_2">M f = 4 i=1 (S xyi ⊗ M i ).</formula><p>We train the attention network with unsupervised contrastive learning, which forces the network to disentangle the foreground and the background of the aggregated activation map M f . We mask the input image by the aggregated activation map M f and its opposite (1 -M f ) to obtain the foreground feature and the background feature, respectively. The loss function is defined as follows:</p><formula xml:id="formula_3">loss = SimM in(v f i , v b j ) + SimM ax(v f i , v f j ) + SimM ax(v b i , v b j )<label>(3)</label></formula><p>where v f i and v b i denote the foreground and the background feature of the i-th sample, respectively. SimM in and SimM ax are the losses that minimize and maximize the similarity between two features (see C 2 AM <ref type="bibr" target="#b21">[22]</ref> for details).</p><p>Finally, we average the activation maps M 1 to M 4 and the aggregated map M f to obtain the final CAM results for each image. We apply the Dense Conditional Random Field (DenseCRF) <ref type="bibr" target="#b11">[12]</ref> algorithm to generate the final segmentation mask. It is worth noting that the proposed method is flexible and can be applied to any classification network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluate our method on the Brain Tumor Segmentation challenge (BraTS) dataset <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14]</ref>, which contains 2,000 cases, each of which includes four 3D volumes from four different MRI modalities: T1, post-contrast enhanced T1 (T1-CE), T2, and T2 Fluid Attenuated Inversion Recovery (T2-FLAIR), as well as a corresponding segmentation ground-truth mask. The official data split divides these cases by the ratio of 8:1:1 for training, validation, and testing (5,802 positive and 1,073 negative images). In order to evaluate the performance, we use the validation set as our test set and report statistics on it. We preprocess the data by slicing each volume along the z-axis to form a total of 193,905 2D images, following the approach of Kang et al. <ref type="bibr" target="#b9">[10]</ref> and Dey and Hong <ref type="bibr" target="#b5">[6]</ref>. We use the ground-truth segmentation masks only in the final evaluation, not in the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details and Evaluation Protocol</head><p>We implement our method in PyTorch using ResNet-18 as the backbone classifier. We pretrain the classifier using SupCon <ref type="bibr" target="#b10">[11]</ref> and then fine-tune it in our experiments. We use the entire training set for both pretraining and fine-tuning. We set the initial learning rate to 1e-4 for both phases, and use the cosine annealing scheduler to decrease it until the minimum learning rate is 5e-6. We set the weight decay in both phases to 1e-5 for model regularization. We use Adam optimizer in the multiple-exit phase and SGD optimizer in the aggregation phase. We train all classifiers until they converge with a test accuracy of over 0.9 for all image modalities. Note that only class labels are available in the training set.</p><p>We use the Dice score and Intersection over Union (IoU) to evaluate the quality of the semantic segmentation, following the approach of Xu et al. <ref type="bibr" target="#b22">[23]</ref>, Tang et al. <ref type="bibr" target="#b17">[18]</ref>, and Qian et al. <ref type="bibr" target="#b14">[15]</ref>. In addition, we report the 95% Hausdorff Distance (HD95) to evaluate the boundary of the prediction mask.</p><p>Interested readers can refer to the supplementary material for results on other network architectures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative and Qualitative Comparison with State-of-the-Art</head><p>In this section, we compare the segmentation performance of the proposed AME-CAM with five state-of-the-art weakly-supervised segmentation methods, namely Grad-CAM <ref type="bibr" target="#b15">[16]</ref>, ScoreCAM <ref type="bibr" target="#b18">[19]</ref>, LFI-CAM <ref type="bibr" target="#b12">[13]</ref>, LayerCAM <ref type="bibr" target="#b8">[9]</ref>, and Swin-MIL <ref type="bibr" target="#b14">[15]</ref>. We also compare with an unsupervised approach C&amp;F <ref type="bibr" target="#b4">[5]</ref>, the supervised version of C&amp;F, and the supervised Optimized U-net <ref type="bibr" target="#b7">[8]</ref> to show the comparison with non-CAM-based methods. We acknowledge that the results from fully supervised and unsupervised methods are not directly comparable to the weakly supervised CAM methods. Nonetheless, these methods serve as interesting references for the potential performance ceiling and floor of all the CAM methods. Quantitatively, Grad-CAM and ScoreCAM result in low dice scores, demonstrating that they have difficulty extracting the activation of medical images. LFI-CAM and LayerCAM improve the dice score in all modalities, except LFI-CAM in T1-CE and T2-FLAIR. Finally, the proposed AME-CAM achieves optimal performance in all modalities of the BraTS dataset.</p><p>Compared to the unsupervised baseline (UL), C&amp;F is unable to separate the tumor and the surrounding tissue due to low contrast, resulting in low dice scores in all experiments. With pixel-wise labels, the dice of supervised C&amp;F improves significantly. Without any pixel-wise label, the proposed AME-CAM outperforms supervised C&amp;F in all modalities.</p><p>The fully supervised (FSL) Optimized U-net achieves the highest dice score and IoU score in all experiments. However, even under different levels of supervision, there is still a performance gap between the weakly supervised CAM methods and the fully supervised state-of-the-art. This indicates that there is still potential room for WSSS methods to improve in the future.</p><p>Qualitatively, Fig. <ref type="figure" target="#fig_1">2</ref> shows the visualization of the CAM and segmentation results from all six CAM-based approaches under four different modalities from the BraTS dataset. Grad-CAM (Fig. <ref type="figure" target="#fig_1">2(c</ref>)) results in large false activation region, where the segmentation mask is totally meaningless. ScoreCAM eliminates false activation corresponding to air. LFI-CAM focus on the exact tumor area only in the T1 and T2 MRI (row 1 and 3). Swin-MIL can hardly capture the tumor region of the MRI image, where the activation is noisy. Among all, only LayerCAM and the proposed AME-CAM successfully focus on the exact tumor area, but AME-CAM reduces the under-estimation of the tumor area. This is attributed to the benefit provided by aggregating activation maps from different resolutions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Different Aggregation Approaches:</head><p>In Table <ref type="table" target="#tab_2">2</ref>, we conducted an ablation study to investigate the impact of using different aggregation approaches after extracting activations from the multiple-exit network. We aim to demonstrate the superiority of the proposed attention-based aggregation approach for segmenting tumor regions in T1 MRI of the BraTS dataset. Note that we only report the results for T1 MRI in the BraTS dataset. Please refer to the supplementary material for the full set of experiments.</p><p>As a baseline, we first conducted the average of four activation maps generated by the multiple-level activation extraction (Avg. ME). We then applied C 2 AM <ref type="bibr" target="#b21">[22]</ref>, a state-of-the-art CAM-based refinement approach, to refine the result of the baseline, which we call "Avg. ME+C 2 AM". However, we observed that C 2 AM tended to segment the brain region instead of the tumor region due to the larger contrast between the brain tissue and the air than that between the tumor region and its surrounding tissue. Any incorrect activation of C 2 AM also led to inferior results, resulting in a degradation of the average dice score from 0.617 to 0.484. In contrast, the proposed attention-based approach provided a significant weighting solution that led to optimal performance in all cases.</p><p>Table <ref type="table">3</ref>. Ablation study for using single-exit from M1, M2, M3 or M4 of Fig. <ref type="figure" target="#fig_0">1</ref> and the multiple-exit using results from M2 and M3 and using all exits (AME-CAM). The experiments are done on the T1-CE MRI of BraTS dataset. The dice score, IoU, and the HD95 are reported in the form of mean ± std.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected Exit</head><p>Dice Effect of Single-Exit and Multiple-Exit: Table <ref type="table">3</ref> summarizes the performance of using single-exit from M 1 , M 2 , M 3 , or M 4 of Fig. <ref type="figure" target="#fig_0">1</ref> and the multipleexit using results from M 2 and M 3 , and using all exits (AME-CAM) on T1-CE MRI in the BraTS dataset.</p><p>The comparisons show that the activation map obtained from the shallow layer M 1 and the deepest layer M 4 result in low dice scores, around 0.15. This is because the network is not deep enough to learn the tumor region in the shallow layer, and the resolution of the activation map obtained from the deepest layer is too low to contain sufficient information to make a clear boundary for the tumor. Results of the internal classifiers from the middle of the network (M 2 and M 3 ) achieve the highest dice score and IoU, both of which are around 0.5.</p><p>To evaluate whether using results from all internal classifiers leads to the highest performance, we further apply the proposed method to the two internal classifiers with the highest dice scores, i.e., M 2 and M 3 , called M 2 + M 3 . Compared with using all internal classifiers (M 1 to M 4 ), M 2 + M 3 results in 18.6% and 22.1% lower dice and IoU, respectively. In conclusion, our AME-CAM still achieves the optimal performance among all the experiments of single-exit and multiple-exit.</p><p>Other ablation studies are presented in the supplementary material due to space limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a brain tumor segmentation method for MRI images using only class labels, based on an Attentive Multiple-Exit Class Activation Mapping (AME-CAM). Our approach extracts activation maps from different exits of the network to capture information from multiple resolutions. We then use an attention model to hierarchically aggregate these activation maps, learning pixel-wise weighted sums.</p><p>Experimental results on the four modalities of the 2021 BraTS dataset demonstrate the superiority of our approach compared with other CAM-based weakly-supervised segmentation methods. Specifically, AME-CAM achieves the highest dice score for all patients in all datasets and modalities. These results indicate the effectiveness of our proposed approach in accurately segmenting brain tumors from MRI images using only class labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the proposed AME-CAM method, which contains multiple-exit network based activation extraction phase and attention based activation aggregation phase. The operator and ⊗ denote the pixel-wise weighted sum and the pixel-wise multiplication, respectively.</figDesc><graphic coords="3,70,98,54,29,310,15,221,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative results of all methods. (a) Input Image. (b) Ground Truth. (c) Grad-CAM [16] (d) ScoreCAM [19]. (e) LFI-CAM [13]. (f) LayerCAM [9]. (g) Swin-MIL [15]. (h) AME-CAM (ours). The image modalities of rows 1-4 are T1, T1-CE, T2, T2-FLAIR, respectively from the BraTS dataset.</figDesc><graphic coords="6,47,79,97,22,328,69,175,09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with weakly supervised methods (WSSS), unsupervised method (UL), and fully supervised methods (FSL) on BraTS dataset with T1, T1-CE, T2, and T2-FLAIR MRI images. Results are reported in the form of mean ± std. We mark the highest score among WSSS methods with bold text.</figDesc><table><row><cell cols="2">BraTS T1</cell><cell></cell><cell></cell></row><row><cell cols="2">Type Method</cell><cell>Dice ↑</cell><cell>IoU ↑</cell><cell>HD95 ↓</cell></row><row><cell cols="5">WSSS Grad-CAM (2016) 0.107 ± 0.090 0.059 ± 0.055 121.816 ± 22.963</cell></row><row><cell></cell><cell cols="4">ScoreCAM (2020) 0.296 ± 0.128 0.181 ± 0.089 60.302 ± 14.110</cell></row><row><cell></cell><cell cols="4">LFI-CAM (2021) 0.568 ± 0.167 0.414 ± 0.152 23.939 ± 25.609</cell></row><row><cell></cell><cell cols="4">LayerCAM (2021) 0.571 ± 0.170 0.419 ± 0.161 23.335 ± 27.369</cell></row><row><cell></cell><cell cols="4">Swin-MIL (2022) 0.477 ± 0.170 0.330 ± 0.147 46.468 ± 30.408</cell></row><row><cell></cell><cell cols="4">AME-CAM (ours) 0.631 ± 0.119 0.471 ± 0.119 21.813 ± 18.219</cell></row><row><cell>UL</cell><cell>C&amp;F (2020)</cell><cell cols="3">0.200 ± 0.082 0.113 ± 0.051 79.187 ± 14.304</cell></row><row><cell cols="2">FSL C&amp;F (2020)</cell><cell cols="3">0.572 ± 0.196 0.426 ± 0.187 29.027 ± 20.881</cell></row><row><cell></cell><cell cols="4">Opt. U-net (2021) 0.836 ± 0.062 0.723 ± 0.090 11.730 ± 10.345</cell></row><row><cell cols="2">BraTS T1-CE</cell><cell></cell><cell></cell></row><row><cell cols="2">Type Method</cell><cell>Dice ↑</cell><cell>IoU ↑</cell><cell>HD95 ↓</cell></row><row><cell cols="5">WSSS Grad-CAM (2016) 0.127 ± 0.088 0.071 ± 0.054 129.890 ± 27.854</cell></row><row><cell></cell><cell cols="4">ScoreCAM (2020) 0.397 ± 0.189 0.267 ± 0.163 46.834 ± 22.093</cell></row><row><cell>UL</cell><cell>C&amp;F (2020)</cell><cell cols="3">0.179 ± 0.080 0.101 ± 0.050 77.982 ± 14.042</cell></row><row><cell cols="2">FSL C&amp;F (2020)</cell><cell cols="3">0.246 ± 0.104 0.144 ± 0.070 130.616 ± 9.879</cell></row><row><cell></cell><cell cols="4">Opt. U-net (2021) 0.845 ± 0.058 0.736 ± 0.085 11.593 ± 11.120</cell></row><row><cell cols="2">BraTS T2</cell><cell></cell><cell></cell></row><row><cell cols="2">Type Method</cell><cell>Dice ↑</cell><cell>IoU ↑</cell><cell>HD95 ↓</cell></row><row><cell cols="5">WSSS Grad-CAM (2016) 0.049 ± 0.058 0.026 ± 0.034 141.025 ± 23.107</cell></row><row><cell></cell><cell cols="4">ScoreCAM (2020) 0.530 ± 0.184 0.382 ± 0.174 28.611 ± 11.596</cell></row><row><cell></cell><cell cols="4">LFI-CAM (2021) 0.673 ± 0.173 0.531 ± 0.186 18.165 ± 10.475</cell></row><row><cell></cell><cell cols="4">LayerCAM (2021) 0.624 ± 0.178 0.476 ± 0.173 23.978 ± 44.323</cell></row><row><cell></cell><cell cols="4">Swin-MIL (2022) 0.437 ± 0.149 0.290 ± 0.117 38.006 ± 30.000</cell></row><row><cell></cell><cell cols="2">AME-CAM (ours) 0</cell><cell></cell></row></table><note><p>LFI-CAM (2021) 0.121 ± 0.120 0.069 ± 0.076 136.246 ± 38.619 LayerCAM (2021) 0.510 ± 0.209 0.367 ± 0.180 29.850 ± 45.877 Swin-MIL (2022) 0.460 ± 0.169 0.314 ± 0.140 46.996 ± 22.821 AME-CAM (ours) 0.695 ± 0.095 0.540 ± 0.108 18.129 ± 12.335</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.721 ± 0.086 0.571 ± 0.101 14.940 ± 8.736</head><label></label><figDesc></figDesc><table><row><cell>UL</cell><cell>C&amp;F (2020)</cell><cell cols="3">0.230 ± 0.089 0.133 ± 0.058 76.256 ± 13.192</cell></row><row><cell cols="2">FSL C&amp;F (2020)</cell><cell cols="3">0.611 ± 0.221 0.474 ± 0.217 109.817 ± 27.735</cell></row><row><cell></cell><cell cols="4">Opt. U-net (2021) 0.884 ± 0.064 0.798 ± 0.098 8.349 ± 9.125</cell></row><row><cell cols="2">BraTS T2-FLAIR</cell><cell></cell><cell></cell></row><row><cell cols="2">Type Method</cell><cell>Dice ↑</cell><cell>IoU ↑</cell><cell>HD95 ↓</cell></row><row><cell cols="5">WSSS Grad-CAM (2016) 0.150 ± 0.077 0.083 ± 0.050 110.031 ± 23.307</cell></row><row><cell></cell><cell cols="4">ScoreCAM (2020) 0.432 ± 0.209 0.299 ± 0.178 39.385 ± 17.182</cell></row><row><cell></cell><cell cols="4">LFI-CAM (2021) 0.161 ± 0.192 0.102 ± 0.140 125.749 ± 45.582</cell></row><row><cell></cell><cell cols="4">LayerCAM (2021) 0.652 ± 0.206 0.515 ± 0.210 22.055 ± 33.959</cell></row><row><cell></cell><cell cols="4">Swin-MIL (2022) 0.272 ± 0.115 0.163 ± 0.079 41.870 ± 19.231</cell></row><row><cell></cell><cell cols="4">AME-CAM (ours) 0.862 ± 0.088 0.767 ± 0.122 8.664 ± 6.440</cell></row><row><cell>UL</cell><cell>C&amp;F (2020)</cell><cell cols="3">0.306 ± 0.190 0.199 ± 0.167 75.651 ± 14.214</cell></row><row><cell cols="2">FSL C&amp;F (2020)</cell><cell cols="3">0.578 ± 0.137 0.419 ± 0.130 138.138 ± 14.283</cell></row></table><note><p>Opt. U-net (2021) 0.914 ± 0.058 0.847 ± 0.093 8.093 ± 11.879 4 Results</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study for aggregation phase using T1 MRI images from the BraTS dataset. Avg. ME denotes that we directly average four activation maps generated by the multiple-exit phase. The dice score, IoU, and the HD95 are reported in the form of mean ± std.</figDesc><table><row><cell>Method</cell><cell>Dice ↑</cell><cell>IoU ↑</cell><cell>HD95 ↓</cell></row><row><cell>Avg. ME</cell><cell cols="3">0.617 ± 0.121 0.457 ± 0.121 23.603 ± 20.572</cell></row><row><cell cols="4">Avg. ME+C 2 AM [22] 0.484 ± 0.256 0.354 ± 0.207 69.242 ± 121.163</cell></row><row><cell>AME-CAM (ours)</cell><cell>0.631 ± 0</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>.119 0.471 ± 0.119 21.813 ± 18.219</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.095 0.540 ± 0.108 18.129 ± 12.335</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>↑</cell><cell>IoU ↑</cell><cell>HD95 ↓</cell></row><row><cell>Single-exit</cell><cell>M 1</cell><cell cols="2">0.144 ± 0.184 0.090 ± 0.130 74.249 ± 62.669</cell></row><row><cell></cell><cell>M 2</cell><cell cols="2">0.500 ± 0.231 0.363 ± 0.196 43.762 ± 85.703</cell></row><row><cell></cell><cell>M 3</cell><cell cols="2">0.520 ± 0.163 0.367 ± 0.141 43.749 ± 54.907</cell></row><row><cell></cell><cell>M 4</cell><cell cols="2">0.154 ± 0.101 0.087 ± 0.065 120.779 ± 44.548</cell></row><row><cell cols="2">Multiple-exit M 2 + M 3</cell><cell cols="2">0.566 ± 0.207 0.421 ± 0.186 27.972 ± 56.591</cell></row><row><cell></cell><cell cols="2">AME-CAM (ours) 0.695 ± 0</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 17.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">F-cam: Ffull resolution class activation maps via guided parametric upscaling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mccaffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3490" to="3499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive analysis of weaklysupervised semantic segmentation in different image domains</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Plataniotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="361" to="384" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Medical image segmentation via unsupervised convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10155</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ASC-Net: adversarial-based selective network for unsupervised anomaly segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-323" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="236" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Poly-cam: high resolution class activation map for convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Englebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cornu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Vleeschouwer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13359</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Futrega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ribalta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03352</idno>
		<title level="m">Optimized u-net for brain tumor segmentation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Layercam: exploring hierarchical class activation maps for localization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5875" to="5888" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards a quantitative analysis of class activation mapping for deep learning-based computer-aided diagnosis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Messem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>De Neve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Observer Performance, and Technology Assessment</title>
		<imprint>
			<biblScope unit="volume">11599</biblScope>
			<biblScope unit="page">115990</biblScope>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note>Image Perception</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<title level="m">Supervised contrastive learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lfi-cam: learning feature importance for better visual explanation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1355" to="1363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transformer based multiple instance learning for weakly supervised histopathology image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-716" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="160" to="170" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-resolution class activation mapping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tagaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sdraka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stafylopatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference On Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4514" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">M-SEAM-NAM: multi-instance self-supervised equivalent attention mechanism with neighborhood affinity module for double weakly supervised segmentation of COVID-19</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-225" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Score-cam: score-weighted visual explanations for convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="24" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12275" to="12284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffusion models for medical anomaly detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-14" />
	</analytic>
	<monogr>
		<title level="m">th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">C2am: contrastive learning of class-agnostic activation map for weakly supervised object localization and semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="989" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Whole heart and great vessel segmentation in congenital heart disease using deep neural networks and graph matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-853" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
