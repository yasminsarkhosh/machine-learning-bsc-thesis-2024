<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation</title>
				<funder ref="#_B67R9ME">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_7rWfPtM #_uwg3ENH">
					<orgName type="full">Beijing Municipal Science and Technology Planning Project</orgName>
				</funder>
				<funder ref="#_uqF2A7C">
					<orgName type="full">Scientific</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shengbo</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Deepwise AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziji</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiechao</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Deepwise AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Deepwise AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shu</forename><surname>Zhang</surname></persName>
							<email>zhangshu@deepwise.com</email>
							<affiliation key="aff0">
								<orgName type="department">Deepwise AI Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="98" to="108"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2FFBC1F472DA87B967B604B71A408D8D</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semi-supervised learning</term>
					<term>Medical Image Segmentation.</term>
					<term>Mutual learning</term>
					<term>Cross-sample correlation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning has become increasingly popular in medical image segmentation due to its ability to leverage large amounts of unlabeled data to extract additional information. However, most existing semi-supervised segmentation methods only focus on extracting information from unlabeled data, disregarding the potential of labeled data to further improve the performance of the model. In this paper, we propose a novel Correlation Aware Mutual Learning (CAML) framework that leverages labeled data to guide the extraction of information from unlabeled data. Our approach is based on a mutual learning strategy that incorporates two modules: the Cross-sample Mutual Attention Module (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module establishes dense cross-sample correlations among a group of samples, enabling the transfer of label prior knowledge to unlabeled data. The OCC module constructs omni-correlations between the unlabeled and labeled datasets and regularizes dual models by constraining the omni-correlation matrix of each sub-model to be consistent. Experiments on the Atrial Segmentation Challenge dataset demonstrate that our proposed approach outperforms state-of-the-art methods, highlighting the effectiveness of our framework in medical image segmentation tasks. The codes, pre-trained weights, and data are publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite the remarkable advancements achieved through the use of deep learning for automatic medical image segmentation, the scarcity of precisely annotated training data remains a significant obstacle to the widespread adoption of such techniques in clinical settings. As a solution, the concept of semi-supervised segmentation has been proposed to enable models to be trained using less annotated but abundant unlabeled data.</p><p>Recently, methods that adopt the co-teaching <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">19]</ref> or mutual learning <ref type="bibr" target="#b25">[25]</ref> paradigm have emerged as a promising approach for semi-supervised learning. Those methods adopt two simultaneously updated models, each trained to predict the prediction results of its counterpart, which can be seen as a combination of the notions of consistency regularization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> and entropy minimization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b24">24]</ref>. In the domain of semi-supervised medical image segmentation, MC-Net <ref type="bibr" target="#b19">[19]</ref> has shown significant improvements in segmentation performance.</p><p>With the rapid advancement of semi-supervised learning, the importance of unlabeled data has garnered increased attention across various disciplines in recent years. However, the role of labeled data has been largely overlooked, with the majority of semi-supervised learning techniques treating labeled data supervision as merely an initial step of the training pipeline or as a means to ensure training convergence <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">26]</ref>. Recently, methods that can leverage labeled data to directly guide information extraction from unlabeled data have attracted the attention of the community <ref type="bibr" target="#b15">[16]</ref>. In the domain of semi-supervised medical image segmentation, there exist shared characteristics between labeled and unlabeled data that possess greater intuitiveness and instructiveness for the algorithm. Typically, partially labeled clinical datasets exhibit similar foreground features, including comparable texture, shape, and appearance among different samples. As such, it can be hypothesized that constructing a bridge across the entire training dataset to connect labeled and unlabeled data can effectively transfer prior knowledge from labeled data to unlabeled data and facilitate the extraction of information from unlabeled data, ultimately overcoming the performance bottleneck of semi-supervised learning methods.</p><p>Based on the aforementioned conception, we propose a novel Correlation Aware Mutual Learning (CAML) framework to explicitly model the relationship between labeled and unlabeled data to effectively utilize the labeled data. Our proposed method incorporates two essential components, namely the Crosssample Mutual Attention module (CMA) and the Omni-Correlation Consistency module (OCC), to enable the effective transfer of labeled data information to unlabeled data. The CMA module establishes mutual attention among a group of samples, leading to a mutually reinforced representation of co-salient features between labeled and unlabeled data. Unlike conventional methods, where supervised signals from labeled and unlabeled samples are separately back-propagated, the proposed CMA module creates a new information propagation path among each pixel in a group of samples, which synchronously enhances the feature representation ability of each intra-group sample.</p><p>In addition to the CMA module, we introduce the OCC module to regularize the segmentation model by explicitly modeling the omni-correlation between unlabeled features and a group of labeled features. This is achieved by constructing a memory bank to store the labeled features as a reference set of features or basis vectors. In each iteration, a portion of features from the memory bank is utilized to calculate the omni-correlation with unlabeled features, reflecting the similarity relationship of an unlabeled pixel with respect to a set of basis vectors of the labeled data. Finally, we constrain the omni-correlation matrix of each sub-model to be consistent to regularize the entire framework. With the proposed omni-correlation consistency, the labeled data features serve as anchor groups to guide the representation learning of the unlabeled data feature and explicitly encourage the model to learn a more unified feature distribution among unlabeled data.</p><p>In summary, our contributions are threefold: <ref type="bibr" target="#b0">(1)</ref> We propose a novel Correlation Aware Mutual Learning (CAML) framework that focuses on the efficient utilization of labeled data to address the challenge of semi-supervised medical image segmentation. (2) We introduce the Cross-sample Mutual Attention module (CMA) and the Omni-Correlation Consistency module (OCC) to establish cross-sample relationships directly. (3) Experimental results on a benchmark dataset demonstrate significant improvements over previous SOTAs, especially when only a small number of labeled images are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Figure <ref type="figure" target="#fig_0">1</ref> gives an overview of CAML. We adopt a co-teaching paradigm like MC-Net <ref type="bibr" target="#b19">[19]</ref> to enforce two parallel networks to predict the prediction results of its counterpart. To achieve efficient cross-sample relationship modeling and enable information propagation among labeled and unlabeled data in a mini-batch, we incorporate a Cross-sample Mutual Attention module to the auxiliary segmentation network f a , whereas the vanilla segmentation network f v remains the original V-Net structure. In addition, we employ an Omni-Correlation Consistency regularization to further regularize the representation learning of the unlabeled data. Details about those two modules will be elaborated on in the following sections. The total loss of CAML can be formulated as:</p><formula xml:id="formula_0">L = L s + λ c l c + λ o l o (1)</formula><p>where l o represents the proposed omni-correlation consistency loss, while L s and l c are the supervised loss and the cross-supervised loss implemented in the Cross Pseudo Supervision(CPS) module. λ c and λ o are the weights to control l c and l o separately. During the training procedure, a batch of mixed labeled and unlabeled samples are fed into the network. The supervised loss is only applied to labeled data, while all samples are utilized to construct cross-supervised learning. Please refer to <ref type="bibr" target="#b2">[3]</ref> for a detailed description of the CPS module and loss design of L s and l c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Sample Mutual Attention Module</head><p>To enable information propagation through any positions of any samples in a mini-batch, one can simply treat each pixel's feature vector as a token and perform self-attentions for all tokens in a mini-batch. However, this will make the computation cost prohibitively large as the computation complexity of selfattention is O(n 2 ) with respect to the number of tokens. We on the other hand adopt two sequentially mounted self-attention modules along different dimensions to enable computation efficient mutual attention among all pixels.</p><p>As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the proposed CMA module consists of two sequential transformer encoder layers, termed as E 1 and E 2 , each including a multi-head attention and a MLP block with a layer normalization after each block. For an input feature map a in ∈ R b×c×k , where k = h × w × d , b represents batch size and c is the dimension of a in , E 1 performs intra-sample self-attention on the spatial dimension of each sample. This is used to model the information propagation paths between every pixel position within each sample. Then, to further enable information propagation among different samples, we perform an inter-sample self-attention along the batch dimension. In other words, along the b dimension, the pixels located in the same spatial position from samples are fed into a self-attention module to construct cross-sample relationships.</p><p>In CAML, we employ the proposed CMA module in the auxiliary segmentation network f a , whereas the vanilla segmentation network f v remains the original V-Net structure. The reasons can be summarized into two folds. From deployment perspective, the insertion of the CMA module requires a batch size of large than 1 to model the attention among samples within a mini-batch, which is not applicable for model inference(batchsize=1). From the perspective of model design, we model the vanilla and the auxiliary branch with different architectures to increase the architecture heterogeneous for better performance in a mutual learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Omni-Correlation Consistency Regularization</head><p>In this chapter, we introduce Omni-Correlation Consistency (OCC) to formulate additional model regularization. The core of the OCC module is omnicorrelation, which is a kind of similarity matrix that is calculated between the feature of an unlabeled pixel and a group of prototype features sampled from labeled instances features. It reflects the similar relationship of an unlabeled pixel with respect to a set of labeled reference pixels. During the training procedure, we explicitly constrain the omni-correlation calculated using heterogeneous unlabeled features from those two separate branches to remain the same. In practice, we use an Omni-correlation matrix to formulate the similarity distribution between unlabeled features and the prototype features.</p><p>Let g v and g a denote two projection heads attached to the backbones of f v and f a separately, and z v ∈ R m×c and z a ∈ R m×c represent two sets of embeddings sampled from their projected features extracted from unlabeled samples, where m is the number of sampled features and c is the dimension of the projected features. It should be noted that z v and z a are sampled from the embeddings corresponding to the same set of positions on unlabeled samples. Suppose z p ∈ R n×c represents a set of prototype embeddings sampled from labeled instances, where n represents the number of sampled prototype features, the omni-correlation matrix calculation between z v and z p can be formulated as:</p><formula xml:id="formula_1">sim vpi = exp(cos(z v , z pi ) * t) n j=1 exp(cos(z v , z pj ) * t) , i ∈ {1, ..., n}<label>(2)</label></formula><p>where cos means the cosine similarity and t is the temperature hyperparameter. sim vp ∈ R m×n is the calculated omni-correlation matrix. Similarly, the similarity distribution sim ap between z a and z p can be calculated by replacing z v with z a .</p><p>To constrain the consistency of omni-correlation between dual branches, the omni-correlation consistency regularization can be conducted with the crossentropy loss l ce as follows:</p><formula xml:id="formula_2">l o = 1 m l ce (sim vp , sim ap )<label>(3)</label></formula><p>Memory Bank Construction. We utilize a memory bank T to iteratively update prototype embeddings for OCC computation. Specifically, T initializes N slots for each labeled training sample and updates prototype embeddings with filtered labeled features projected by g v and g a . To ensure the reliability of the features stored in T , we select embeddings on the positions where both f v and f a have the correct predictions and update T with the mean fusion of the projected features projected by g v and g a . For each training sample, following <ref type="bibr" target="#b4">[5]</ref>, T updates slots corresponding to the labeled samples in the current mini-batch in a query-like manner.</p><p>Embeddings Sampling. For computation efficiency, omni-correlation is not calculated on all labeled and unlabeled pixels. Specifically, we have developed a confidence-based mechanism to sample the pixel features from the unlabeled data. Practically, to sample z v and z a from unlabeled features, we first select the pixels where f v and f a have the same prediction. For each class, we sort the confidence scores of these pixels, and then select features of the top i pixels as the sampled unlabeled features. Thus, m = i × C, where C represents the number of classes. With regards to the prototype embeddings, we randomly sample j embeddings from each class among all the embeddings contained in T and n = j × C to increase its diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Dataset. Our method is evaluated on the Left Atrium (LA) dataset <ref type="bibr" target="#b21">[21]</ref> from the 2018 Atrial Segmentation Challenge. The dataset comprises 100 gadoliniumenhanced MR imaging scans (GE-MRIs) and their ground truth masks, with an isotropic resolution of 0.625 3 mm 3 . Following <ref type="bibr" target="#b23">[23]</ref>, we use 80 scans for training and 20 scans for testing. All scans are centered at the heart region and cropped accordingly, and then normalized to zero mean and unit variance.</p><p>Implementation Details. We implement our CAML using PyTorch 1.8.1 and CUDA 10.2 on an NVIDIA TITAN RTX GPU. For training data augmentation, we randomly crop sub-volumes of size 112 × 112 × 80 following <ref type="bibr" target="#b23">[23]</ref>. To ensure a fair comparison with existing methods, we use the V-Net <ref type="bibr" target="#b12">[13]</ref> as the backbone for all our models. During training, we use a batch size of 4, with half of the images annotated and the other half unannotated. We train the entire framework using the SGD optimizer, with a learning rate of 0.01, momentum of 0.9, and weight decay of 1e-4 for 15000 iterations. To balance the loss terms in the training process, we use a time-dependent Gaussian warming up function for λ U and λ C , where λ(t) = β * e -5(1-t/tmax) 2 , and set β to 1 and 0.1 for λ U and λ C , respectively. For the OCC module, we set c to 64, j to 256, and i to 12800. During inference, prediction results from the vanilla V-Net are used with a general sliding window strategy without any post-processing.</p><p>Quantitative Evaluation and Comparison. Our CAML is evaluated on four metrics: Dice, Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD). It is worth noting that the previous researchers reported results (Reported Metrics in Table <ref type="table" target="#tab_0">1</ref>) on LA can be confusing, with some studies reporting results from the final training iteration, while others report the best performance obtained during training. However, the latter approach can lead to overfitting of the test dataset and unreliable model selection. To ensure a fair comparison, we perform all experiments three times with a fixed set of randomly selected seeds on the same machine, and report the mean and standard deviation of the results from the final iteration.</p><p>The results on LA are presented in Table <ref type="table" target="#tab_0">1</ref>. The results of the full-supervised V-Net model trained on different ratios serve as the lower and upper bounds  of each ratio setting. We report the reproduced results of state-of-the-art semisupervised methods and corresponding reported results if available. By comparing the reproduced and reported results, we observe that although the performance of current methods generally shows an increasing trend with the development of algorithms, the performance of individual experiments can be unstable. and the reported results may not fully reflect the true performance.</p><p>It is evident from Table <ref type="table" target="#tab_0">1</ref> that CAML outperforms other methods by a significant margin across all settings without incurring any additional inference or post-processing costs. With only 5% labeled data, CAML achieves 87.34% Dice score with an absolute improvement of 4.01% over the state-of-the-art. CAML also achieves 89.62% Dice score with only 10% labeled data. When the amount of labeled data is increased to 20%, the model obtains comparable results with the results of V-Net trained in 100% labeled data), achieving a Dice score of 90.78% compared to the upper-bound model's score of 90.98%. As presented in Table <ref type="table" target="#tab_0">1</ref>, through the effective transfer of knowledge between labeled and unlabeled data, CAML achieves impressive improvements. Table 1 also demonstrated that as the labeled data ratio declines, the model maintains a low standard deviation of results, which is significantly lower than other state-of-the-art methods. This finding suggests that CAML is highly stable and robust. Furthermore, the margin between our method and the state-of-theart semi-supervised methods increases with the decline of the labeled data ratio, indicating that our method rather effectively transfers knowledge from labeled data to unlabeled data, thus enabling the model to extract more universal features from unlabeled data. Figure <ref type="figure" target="#fig_1">2</ref> shows the qualitative comparison results. The figure presents 2D and 3D visualizations of all the compared methods and the corresponding ground truth. As respectively indicated by the orange rectangle and circle in the 2D and 3D visualizations Our CAML achieves the best segmentation results compared to all other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study.</head><p>In this section, we analyze the effectiveness of the proposed CMA module and OCC module. We implement the MC-Net as our baseline, which uses different up-sampling operations to introduce architecture heterogeneity. Table <ref type="table" target="#tab_1">2</ref> presents the results of our ablation study. The results demonstrate that under 5% ratio, both CMA and OCC significantly improve the performance of the baseline. By combining these two modules, CAML achieves an absolute improvement of 6.42% in the Dice coefficient. Similar improvements can be observed for a data ratio of 10%. Under a labeled data ratio of 20%, the baseline performance is improved to 90.43% in the Dice coefficient, which is approximately comparable to the upper bound of a fully-supervised model. In this setting, adding the CMA and OCC separately may not achieve a significant improvement. Nonetheless, by combining these two modules in our proposed CAML framework, we still achieve the best performance in this setting, which further approaches the performance of a fully-supervised model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a novel framework named CAML for semi-supervised medical image segmentation. Our key idea is that cross-sample correlation should be taken into consideration for semi-supervised learning. To this end, two novel modules: Cross-sample Mutual Attention(CMA) and Omni-Correlation Consistency(OCC) are proposed to encourage efficient and direct transfer of the prior knowledge from labeled data to unlabeled data. Extensive experimental results on the LA dataset demonstrate that we outperform previous state-of-the-art results by a large margin without extra computational consumption in inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our proposed CAML. CAML adopts a co-teaching scheme with cross-pseudo supervision. The CMA module incorporated into the auxiliary network and the OCC module are introduced for advanced cross-sample relationship modeling.</figDesc><graphic coords="3,47,79,54,20,328,84,201,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of the segmentations results from different methods.</figDesc><graphic coords="7,45,81,341,93,332,20,131,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art methods on the LA database. Metrics reported the mean±standard results with three random seeds, Reported Metrics are the results reported in the original paper.</figDesc><table><row><cell>Method</cell><cell cols="2">Scans used</cell><cell>Metrics</cell><cell></cell><cell></cell><cell cols="2">Reported Metrics</cell><cell></cell></row><row><cell></cell><cell cols="3">Labeled Unlabeled Dice(%)</cell><cell cols="6">Jaccard(%) 95HD(voxel) ASD(voxel) Dice(%) Jaccard(%) 95HD(voxel) ASD(voxel)</cell></row><row><cell>V-Net</cell><cell>4</cell><cell>0</cell><cell cols="2">43.32±8.62 31.43±6.90 40.19±1.11</cell><cell cols="2">12.13±0.57 52.55</cell><cell>39.60</cell><cell>47.05</cell><cell>9.87</cell></row><row><cell>V-Net</cell><cell>8</cell><cell>0</cell><cell cols="2">79.87±1.23 67.60±1.88 26.65±6.36</cell><cell>7.94±2.22</cell><cell>78.57</cell><cell>66.96</cell><cell>21.20</cell><cell>6.07</cell></row><row><cell>V-Net</cell><cell>16</cell><cell>0</cell><cell cols="2">85.94±0.48 75.99±0.57 16.70±1.82</cell><cell>4.80±0.62</cell><cell>86.96</cell><cell>77.31</cell><cell>11.85</cell><cell>3.22</cell></row><row><cell>V-Net</cell><cell>80</cell><cell>0</cell><cell cols="2">90.98±0.67 83.61±1.06 8.58±2.34</cell><cell>2.10±0.59</cell><cell>91.62</cell><cell>84.60</cell><cell>5.40</cell><cell>1.64</cell></row><row><cell cols="2">UA-MT [23] (MICCAI'19) 4(5%)</cell><cell>76(95%)</cell><cell cols="2">78.07±0.90 65.03±0.96 29.17±3.82</cell><cell>8.63±0.98</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SASSNet [8] (MICCAI'20)</cell><cell></cell><cell></cell><cell cols="2">79.61±0.54 67.00±0.59 25.54±4.60</cell><cell>7.20±1.21</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DTC [10] (AAAI'21)</cell><cell></cell><cell></cell><cell cols="2">80.14±1.22 67.88±1.82 24.08±2.63</cell><cell>7.18±0.62</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MC-Net [19] (MedIA'21)</cell><cell></cell><cell></cell><cell cols="2">80.92±3.88 68.90±5.09 17.25±6.08</cell><cell>2.76±0.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>URPC [12] (MedIA'22)</cell><cell></cell><cell></cell><cell cols="2">80.75±0.21 68.54±0.34 19.81±0.67</cell><cell>4.98±0.25</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SS-Net [18] (MICCAI'22)</cell><cell></cell><cell></cell><cell cols="2">83.33±1.66 71.79±2.36 15.70±0.80</cell><cell>4.33±0.36</cell><cell>86.33</cell><cell>76.15</cell><cell>9.97</cell><cell>2.31</cell></row><row><cell>MC-Net+ [17] (MedIA'22)</cell><cell></cell><cell></cell><cell cols="2">83.23±1.41 71.70±1.99 14.92±2.56</cell><cell>3.43±0.64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ours</cell><cell></cell><cell></cell><cell cols="2">87.34±0.05 77.65±0.08 9.76±0.92</cell><cell>2.49±0.22</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">UA-MT [23] (MICCAI'19) 8(10%) 72(90%)</cell><cell cols="2">85.81±0.17 75.41±0.22 18.25±1.04</cell><cell>5.04±0.24</cell><cell>84.25</cell><cell>73.48</cell><cell>3.36</cell><cell>13.84</cell></row><row><cell>SASSNet [8] (MICCAI'20)</cell><cell></cell><cell></cell><cell cols="2">85.71±0.87 75.35±1.28 14.74±3.14</cell><cell>4.00±0.86</cell><cell>86.81</cell><cell>76.92</cell><cell>3.94</cell><cell>12.54</cell></row><row><cell>DTC [10] (AAAI'21)</cell><cell></cell><cell></cell><cell cols="2">84.55±1.72 73.91±2.36 13.80±0.16</cell><cell>3.69±0.25</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MC-Net [19] (MedIA'21)</cell><cell></cell><cell></cell><cell cols="2">86.87±1.74 78.49±1.06 11.17±1.40</cell><cell>2.18±0.14</cell><cell>87.71</cell><cell>78.31</cell><cell>9.36</cell><cell>2.18</cell></row><row><cell>URPC [12] (MedIA'22)</cell><cell></cell><cell></cell><cell cols="2">83.37±0.21 71.99±0.31 17.91±0.73</cell><cell>4.41±0.17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SS-Net [18] (MICCAI'22)</cell><cell></cell><cell></cell><cell cols="2">86.56±0.69 76.61±1.03 12.76±0.58</cell><cell>3.02±0.19</cell><cell>88.55</cell><cell>79.63</cell><cell>7.49</cell><cell>1.90</cell></row><row><cell>MC-Net+ [17] (MedIA'22)</cell><cell></cell><cell></cell><cell cols="2">87.68±0.56 78.27±0.83 10.35±0.77</cell><cell>1.85±0.01</cell><cell>88.96</cell><cell>80.25</cell><cell>7.93</cell><cell>1.86</cell></row><row><cell>ours</cell><cell></cell><cell></cell><cell cols="2">89.62±0.20 81.28±0.32 8.76±1.39</cell><cell>2.02±0.17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">UA-MT [23] (MICCAI'19) 16(20%) 64(80%)</cell><cell cols="2">88.18±0.69 79.09±1.05 9.66±2.99</cell><cell>2.62±0.59</cell><cell>88.88</cell><cell>80.21</cell><cell>2.26</cell><cell>7.32</cell></row><row><cell>SASSNet [8] (MICCAI'20)</cell><cell></cell><cell></cell><cell cols="2">88.11±0.34 79.08±0.48 12.31±4.14</cell><cell>3.27±0.96</cell><cell>89.27</cell><cell>80.82</cell><cell>3.13</cell><cell>8.83</cell></row><row><cell>DTC [10] (AAAI'21)</cell><cell></cell><cell></cell><cell cols="2">87.79±0.50 78.52±0.73 10.29±1.52</cell><cell>2.50±0.65</cell><cell>89.42</cell><cell>80.98</cell><cell>2.10</cell><cell>7.32</cell></row><row><cell>MC-Net [19] (MedIA'21)</cell><cell></cell><cell></cell><cell cols="2">90.43±0.52 82.69±0.75 6.52±0.66</cell><cell>1.66±0.14</cell><cell>90.34</cell><cell>82.48</cell><cell>6.00</cell><cell>1.77</cell></row><row><cell>URPC [12] (MedIA'22)</cell><cell></cell><cell></cell><cell cols="2">87.68±0.36 78.36±0.53 14.39±0.54</cell><cell>3.52±0.17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SS-Net [18] (MICCAI'22)</cell><cell></cell><cell></cell><cell cols="2">88.19±0.42 79.21±0.63 8.12±0.34</cell><cell>2.20±0.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MC-Net+ [17] (MedIA'22)</cell><cell></cell><cell></cell><cell cols="2">90.60±0.39 82.93±0.64 6.27±0.25</cell><cell>1.58±0.07</cell><cell>91.07</cell><cell>83.67</cell><cell>5.84</cell><cell>1.67</cell></row><row><cell>ours</cell><cell></cell><cell></cell><cell cols="2">90.78±0.11 83.19±0.18 6.11±0.39</cell><cell>1.68±0.15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of our proposed CAML on the LA database.</figDesc><table><row><cell>Scans used</cell><cell>Components</cell><cell>Metrics</cell></row><row><cell cols="3">Labeled Unlabeled Baseline OCC CMA Dice(%) 4(5%) 76(95%) √ 80.92±3.88 68.90±5.09 17.25±6.08 Jaccard(%) 95HD(voxel) ASD(voxel) 2.76±0.49 √ √ 83.12±2.12 71.73±3.04 16.94±7.25 4.51±2.16 √ √ 86.35±0.26 76.16±0.40 12.36±0.20 2.94±0.21 √ √ √ 87.34±0.05 77.65±0.08 9.76±0.92 2.49±0.22 8(10%) 72(90%) √ 86.87±1.74 78.49±1.06 11.17±1.40 2.18±0.14 √ √ 88.50±3.25 79.53±0.51 9.89±0.83 2.35±0.21 √ √ 88.84±0.55 80.05±0.85 8.50±0.66 1.97±0.02 √ √ √ 89.62±0.20 81.28±0.32 8.76±1.39 2.02±0.17 16(20%) 64(80%) √ 90.43±0.52 82.69±0.75 6.52±0.66 1.66±0.14 √ √ 90.27±0.22 82.42±0.39 6.96±1.03 1.91±0.24 √ √ 90.25±0.28 82.34±0.43 6.95±0.09 1.79±0.18 √ √ √ 90.78±0.11 83.19±0.18 6.11±0.39 1.68±0.15</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work is funded by the <rs type="funder">Scientific</rs> and <rs type="projectName">Technological Innovation 2030 New Generation Artificial Intelligence</rs> Project of the <rs type="funder">National Key Research and Development Program of China</rs> (No. <rs type="grantNumber">2021ZD0113302</rs>), <rs type="funder">Beijing Municipal Science and Technology Planning Project</rs> (No. <rs type="grantNumber">Z201100005620008</rs>, <rs type="grantNumber">Z211100003521009</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_uqF2A7C">
					<orgName type="project" subtype="full">Technological Innovation 2030 New Generation Artificial Intelligence</orgName>
				</org>
				<org type="funding" xml:id="_B67R9ME">
					<idno type="grant-number">2021ZD0113302</idno>
				</org>
				<org type="funding" xml:id="_7rWfPtM">
					<idno type="grant-number">Z201100005620008</idno>
				</org>
				<org type="funding" xml:id="_uwg3ENH">
					<idno type="grant-number">Z211100003521009</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixmatch: a holistic approach to semi-supervised learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum labeling: revisiting pseudo-labeling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cascante-Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6912" to="6920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01916</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with error localization network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9957" to="9967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shape-aware semi-supervised 3D semantic segmentation for medical images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Racoceanu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perturbed and strict mean teachers for semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4258" to="4267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation through dual-task consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation via cross teaching between CNN and transformer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="820" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation via uncertainty rectified pyramid consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102517</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and low-level consistency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1369" to="1379" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12674" to="12684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Querying labeled for unlabeled: Cross-image semantic consistency guided semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mutual consistency learning for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102530</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring smoothness and class-separation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part V</title>
		<imprint>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-94" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised left atrium segmentation with mutual consistency training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-328" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Essert</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A global benchmark of algorithms for segmenting late gadoliniumenhanced cardiac magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">St++: make self-training work better for semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4268" to="4277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3d left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple baseline for semi-supervised semantic segmentation with strong data augmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8229" to="8238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Robust mutual learning for semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00609</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pseudoseg: designing pseudo labels for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09713</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
