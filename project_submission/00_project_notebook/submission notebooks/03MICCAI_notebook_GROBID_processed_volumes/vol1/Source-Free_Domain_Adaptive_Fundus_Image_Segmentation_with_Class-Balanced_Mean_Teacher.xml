<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher</title>
				<funder ref="#_HEPaug7">
					<orgName type="full">Shenzhen Key Laboratory of</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Longxiang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America</orgName>
								<address>
									<settlement>Princeton</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
							<email>yulun100@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>ZÃ¼rich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
							<email>li.xiu@sz.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="684" to="694"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A09A671CD9B9CC99BE228D99EE3BD58B</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_65</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Source-free domain adaptation</term>
					<term>Fundus image</term>
					<term>Mean teacher</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies source-free domain adaptive fundus image segmentation which aims to adapt a pretrained fundus segmentation model to a target domain using unlabeled images. This is a challenging task because it is highly risky to adapt a model only using unlabeled data. Most existing methods tackle this task mainly by designing techniques to carefully generate pseudo labels from the model's predictions and use the pseudo labels to train the model. While often obtaining positive adaption effects, these methods suffer from two major issues. First, they tend to be fairly unstable -incorrect pseudo labels abruptly emerged may cause a catastrophic impact on the model. Second, they fail to consider the severe class imbalance of fundus images where the foreground (e.g., cup) region is usually very small. This paper aims to address these two issues by proposing the Class-Balanced Mean Teacher (CBMT) model. CBMT addresses the unstable issue by proposing a weak-strong augmented mean teacher learning scheme where only the teacher model generates pseudo labels from weakly augmented images to train a student model that takes strongly augmented images as input. The teacher is updated as the moving average of the instantly trained student, which could be noisy. This prevents the teacher model from being abruptly impacted by incorrect pseudo-labels. For the class imbalance issue, CBMT proposes a novel loss calibration approach to highlight foreground classes according to global statistics. Experiments show that CBMT well addresses these two issues and outperforms existing methods on multiple benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation plays an essential role in computer-aided diagnosis systems in different applications and has been tremendously advanced in the past few years <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>. While the segmentation model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref> always requires sufficient labeled data, unsupervised domain adaptation (UDA) approaches have been proposed, learning an adaptive model jointly with unlabeled target domain images and labeled source domain images <ref type="bibr" target="#b8">[9]</ref>, for example, the adversarial training paradigm <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Although impressive performance has been achieved, these UDA methods may be limited for some real-world medical image segmentation tasks where labeled source images are not available for adaptation. This is not a rare scenario because medical images are usually highly sensitive in privacy and copyright protection such that labeled source images may not be allowed to be distributed. This motivates the investigation of source-free domain adaptation (SFDA) where adapts a source segmentation model trained on labeled source data (in a privateprotected way) to the target domain only using unlabeled data.</p><p>A few recent SFDA works have been proposed. OSUDA <ref type="bibr" target="#b16">[17]</ref> utilizes the domain-specific low-order batch statistics and domain-shareable high-order batch statistics, trying to adapt the former and keep the consistency of the latter. SRDA <ref type="bibr" target="#b0">[1]</ref> minimizes a label-free entropy loss guided with a domaininvariant class-ratio prior. DPL <ref type="bibr" target="#b3">[4]</ref> introduces pixel-level and class-level pseudolabel denoising schemes to reduce noisy pseudo-labels and select reliable ones. U-D4R <ref type="bibr" target="#b26">[27]</ref> applies an adaptive class-dependent threshold with the uncertaintyrectified correction to realize better denoising.</p><p>Although these methods have achieved some success in model adaptation, they still suffer from two major issues. First, they tend to be fairly unstable. Without any supervision signal from labeled data, the model heavily relies on the predictions generated by itself, which are always noisy and could easily make the training process unstable, causing catastrophic error accumulation after several training epochs as shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Some works avoid this problem by only training the model for very limited iterations (only 2 epochs in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>) and selecting the best-performing model during the whole training process for testing. However, this does not fully utilize the data and it is non-trivial to select the best-performing model for this unsupervised learning task. Second, they failed to consider the severe foreground and background imbalance of fundus images where the foreground (e.g., cup) region is usually very small (as shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>). This oversight could also lead to a model degradation due to the dominate background learning signal.</p><p>In this paper, we propose the Class-Balanced Mean Teacher (CBMT) method to address the limitations of existing methods. To mitigate the negative impacts of incorrect pseudo labels, we propose a weak-strong augmented mean teacher learning scheme which involves a teacher model and a student model that are both initialized from the source model. We use the teacher to generate pseudo label from a weakly augmented image, and train the student that takes strongly augmented version of the same image as input. We do not train the teacher model directly by back-propagation but update its weights as the moving average of the student model. This prevents the teacher model from being abruptly impacted by incorrect pseudo labels and meanwhile accumulates new knowledge learned by the student model. To address the imbalance between foreground and background, we propose to calibrate the segmentation loss and highlight the foreground class, based on the prediction statistics derived from the global information. We maintain a prediction bank to capture global information, which is considered more reliable than that inside one image.</p><p>Our contributions can be summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose the weakstrong augmented mean teacher learning scheme to address the stable issue of existing methods. <ref type="bibr" target="#b1">(2)</ref> We propose the novel global knowledge-guided loss calibration technique to address the foreground and background imbalance problem.</p><p>(3) Our proposed CBMT reaches state-of-the-art performance on two popular benchmarks for adaptive fundus image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Source-Free Domain Adaptive (SFDA) fundus image segmentation aims to adapt a source model h, trained with N S labeled source images S = {(X i , Y i )} NS i=1 , to the target domain using only N T unlabeled target images T = {X i } NT i=1 . Y i â {0, 1} HÃW ÃC is the ground truth, and H, W , and C denote the image height, width, and class number, respectively. A vanilla pseudo-labeling-based method generates pseudo labels Å· â R C from the sigmoided model prediction p = h(x) for each pixel x â X i with source model h:</p><formula xml:id="formula_0">Å·k = 1 [p k &gt; Î³] , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where 1 is the indicator function and Î³ â [0, 1] is the probability threshold for transferring soft probability to hard label. p k and y k is the k-th dimension of p and y, respectively, denoting the prediction and pseudo label for class k. Then (x, Å·) is utilized to train the source model h with binary cross entropy loss:</p><formula xml:id="formula_2">L bce = E xâ¼Xi [Å· log(p) + (1 -Å·) log(1 -p)]<label>(2)</label></formula><p>Most existing SFDA works refine this vanilla method by proposing techniques to calibrate p and get better pseudo label Å·, or measure the uncertainty of p and apply a weight when using Å· for computing the loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. While achieving improved performance, these methods still suffer from the unstable issue because noisy Å· will directly impact h, and the error will accumulate since then the predictions of h will be used for pseudo labeling. Another problem with this method is that they neglect the imbalance of the foreground and background pixels in fungus images, where the foreground region is small. Consequently, the second term in Eq. ( <ref type="formula" target="#formula_2">2</ref>) will dominate the loss, which is undesirable.</p><p>Our proposed CBMT model addresses the two problems by proposing the weak-strong augmented mean teacher learning scheme and the global knowledgeguided loss calibration technique. Figure <ref type="figure" target="#fig_0">1(c)</ref> shows the framework of CBMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Weak-Strong Augmented Mean Teacher</head><p>To avoid error accumulation and achieve a robust training process, we introduce the weak-strong augmented mean teacher learning scheme where there is a teacher model h t and a student model h s both initialized from the source model h. We generate pseudo labels with h t and use the pseudo labels to train h s . To enhance generalization performance, we further introduce a weak-strong augmentation mechanism that feeds weakly and strongly augmented images to the teacher model and the student model, respectively.</p><p>Concretely, for each image X i , we generate a weakly-augmented version X w i by using image flipping and resizing. Meanwhile, we generate a stronglyaugmented version X s i . The strong augmentations we used include a random eraser, contrast adjustment, and impulse noises. For each pixel x w â X w i , we generate pseudo label Å·w = h t (x) by the teacher model h t with Eq. ( <ref type="formula" target="#formula_0">1</ref>). Then, we train the student model h s with</p><formula xml:id="formula_3">L = E x s â¼X s i ,Å· w [ Lbce ],<label>(3)</label></formula><p>where Lbce is the refined binary cross entropy loss which we will introduce later. It is based on Eq. ( <ref type="formula" target="#formula_2">2</ref>) but addresses the fore-and back-ground imbalance problem. The weakly-strong augmentation mechanism has two main benefits. First, since fundus image datasets are always on a small scale, the model could easily get overfitted due to the insufficient training data issue. To alleviate it, we enhance the diversity of the training set by introducing image augmentation techniques. Second, learning with different random augmentations performs as a consistency regularizer constraining images with similar semantics to the same class, which forms a more distinguishable feature representation.</p><p>We update the student model by back-propagating the loss defined in Eq. <ref type="bibr" target="#b2">(3)</ref>. But for the teacher model, we update it as the exponential moving average (EMA) of the student model as,</p><formula xml:id="formula_4">Î¸ â Î» Î¸ + (1 -Î»)Î¸, (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where Î¸, Î¸ are the teacher and student model weights separately. Instead of updating the model with gradient directly, we define the teacher model as the exponential moving average of students, which makes the teacher model more consistent along the adaptation process. With this, we could train a model for a relatively long process and safely choose the final model without accuracy validation. From another perspective, the teacher model can be interpreted as a temporal ensemble of students in different time steps <ref type="bibr" target="#b17">[18]</ref>, which enhances the robustness of the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global Knowledge Guided Loss Calibration</head><p>For a fundas image, the foreground object (e.g., cup) is usually quite small and most pixel will the background. If we update the student model with Eq. ( <ref type="formula" target="#formula_2">2</ref>), the background class will dominate the loss, which dilutes the supervision signals for the foreground class. The proposed global knowledge guided loss calibration technique aims to address this problem.</p><p>A naive way to address the foreground and background imbalance is to calculate the numbers of pixels falling into the two categories, respectively, within each individual image and devise a loss weighting function based on the numbers. This strategy may work well for the standard supervised learning tasks, where the labels are reliable. But with pseudo labels, it is too risky to conduct the statistical analysis based on a single image. To remedy this, we analyze the class imbalance across the whole dataset, and use this global knowledge to calibrate our loss for each individual image.</p><p>Specifically, we store the predictions of pixels from all images and maintain the mean loss for foreground and background as,</p><formula xml:id="formula_6">Î· fg k = i L i,k â¢ 1[Å· i,k = 1] i 1[Å· i,k = 1] ; Î· bg k = i L i,k â¢ 1[Å· i,k = 0] i 1[Å· i,k = 0]<label>(5)</label></formula><p>where L is the segmentation loss mentioned above, and "fg" and "bg" represent foreground/background. The reason we use the mean of the loss, rather than the number of pixels, is that the loss of each pixel indicates the "hardness" of each pixel according to the pseudo ground truth. This gives more weight to those more informative pixels, thus more global knowledge is considered.</p><p>With each average loss, the corresponding learning scheme could be further calibrated. We utilize the ratio of Î· fg k to Î· bg k to weight background loss L bg k :</p><formula xml:id="formula_7">Lbce = E xâ¼Xi,kâ¼C [Å· k log(p k ) + Î· fg k /Î· bg k (1 -Å·k ) log(1 -p k )]<label>(6)</label></formula><p>The calibrated loss ensures fair learning among different classes, therefore alleviating model degradation issues caused by class imbalance.</p><p>Since most predictions are usually highly confident (very close to 0 or 1), they are thus less informative. We need to only include pixels with relatively large loss scales to compute mean loss. We realize this by adopting constraint threshold Î± to select pixels: |f (xi)-Î³| | Å·i-Î³| &gt; Î±, where Î± is set by default to 0.2. Î± represents the lower bound threshold of normalized prediction, which can filter well-segmented uninformative pixels out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Implementation Details<ref type="foot" target="#foot_0">1</ref> . We apply the Deeplabv3+ <ref type="bibr" target="#b4">[5]</ref> with MobileNetV2 <ref type="bibr" target="#b22">[23]</ref> backbone as our segmentation model, following the previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> for a fair comparison. For model optimization, we use Adam optimizer with 0.9 and 0.99 momentum coefficients. During the source model training stage, the initial learning rate is set to 1e-3 and decayed by 0.98 every epoch, and the training lasts 200 epochs. At the source-free domain adaptation stage, the teacher and student model are first initialized by the source model, and the EMA update scheme is applied between them for a total of 20 epochs with a learning rate of 5e-4. Loss calibration parameter Î· is computed every epoch and implemented on the class cup. The output probability threshold Î³ is set as 0.75 according to previous study <ref type="bibr" target="#b25">[26]</ref> and model EMA update rate Î» is 0.98 by default. We implement our method with PyTorch on one NVIDIA 3090 GPU and set batch size as 8 when adaptation.</p><p>Datasets and Metrics. We evaluate our method on widely-used fundus optic disc and cup segmentation datasets from different clinical centers. Following previous works, We choose the REFUGE challenge training set <ref type="bibr" target="#b19">[20]</ref> as the source domain and adapt the model to two target domains: RIM-ONE-r3 <ref type="bibr" target="#b6">[7]</ref> and Drishti-GS <ref type="bibr" target="#b23">[24]</ref> datasets for evaluation. Quantitatively, the source domain consists of 320/80 fundus images for training/testing with pixel-wise optic disc and cup segmentation annotation, while the target domains have 99/60 and 50/51 images. Same as <ref type="bibr" target="#b25">[26]</ref>, the fundus images are cropped to 512 Ã 512 as ROI regions.</p><p>We compare our CBMT model with several state-of-the-art domain adaptation methods, including UDA methods BEAL <ref type="bibr" target="#b25">[26]</ref> and AdvEnt <ref type="bibr" target="#b24">[25]</ref> and SFDA methods: SRDA <ref type="bibr" target="#b0">[1]</ref>, DAE <ref type="bibr" target="#b14">[15]</ref> and DPL <ref type="bibr" target="#b3">[4]</ref>. More comparisons with U-D4R <ref type="bibr" target="#b26">[27]</ref> under other adaptation settings could be found in supplementary materials. General metrics for segmentation tasks are used for model performance evaluation, including the Dice coefficient and Average Symmetric Surface Distance (ASSD). The dice coefficient (the higher the better) gives pixel-level overlap results, and ASSD (the lower the better) indicates prediction boundary accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Results</head><p>The quantitative evaluation results are shown in Table <ref type="table">1</ref>. We include the without adaptation results from <ref type="bibr" target="#b3">[4]</ref> as a lower bound, and the supervised learning results Table <ref type="table">1</ref>. Quantitative results of comparison with different methods on two datasets, and the best score for each column is highlighted. -means the results are not reported by that method, Â± refers to the standard deviation across samples in the dataset. S-F means source-free. from <ref type="bibr" target="#b25">[26]</ref> as an upper bound, same as <ref type="bibr" target="#b3">[4]</ref>. As shown in the table, both two quantitative metric results perform better than previous state-of-the-art SFDA methods and even show an improvement against traditional UDA methods on  <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>), and safely select the last checkpoint as our final result without concerning about model degradation issue, which is crucial in real-world clinical source-free domain adaptation application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-F Optic Disc Segmentation Optic Cup Segmentation Dice</head><formula xml:id="formula_8">[%] â ASSD[pixel] â Dice[%] â ASSD[pixel] â RIM-ONE-r3 W/o</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Further Analyses</head><p>Ablation Study. In order to assess the contribution of each component to the final performance, we conducted an ablation study on the main modules of CBMT, as summarized in Table <ref type="table" target="#tab_0">2</ref>. Note that we reduced the learning rates by a factor of 20 for the experiments of the vanilla pseudo-labeling method to get comparable performance because models are prone to degradation without EMA updating. As observed in quantitative results, the EMA update strategy avoids the model from degradation, which the vanilla pseudo-labeling paradigm suffers from. Image augmentation and loss calibration also boost the model accuracy, and the highest performance is achieved with both. The loss calibration module achieves more improvement in its solution to class imbalance, while image augmentation is easy to implement and plug-and-play under various circumstances.</p><p>Hyper-parameter Sensitivity Analysis. We further investigate the impact of different hyper-parameter. Figure <ref type="figure" target="#fig_1">2</ref>(a) presents the accuracy with different EMA update rate parameters Î». It demonstrates that both too low and too high update rates would cause a drop in performance, which is quite intuitive: a higher Î» leads to inconsistency between the teacher and student, and thus teacher can hardly learn knowledge from the student; On the other hand, a lower Î» will always keep teacher and student close, making it degenerated to vanilla pseudolabeling. But within a reasonable range, the model is not sensitive to update rate Î».</p><p>To evaluate the variation of the loss calibration weight Î· fg k /Î· bg k with different constraint thresholds Î±, we present the results in Table <ref type="table" target="#tab_1">3</ref>. As we discussed in Sect. 2.2, most pixels in an image are well-classified, and if we simply calculate with all pixels (i.e. Î± = 0), as shown in the first column, the mean loss of background will be severely underestimated due to the large quantity of zeroloss pixel. Besides, as Î± changes, the calibration weight varies little, indicating the robustness of our calibration technique to threshold Î±. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose a class-balanced mean teacher framework to realize robust SFDA learning for more realistic clinical application. Based on the observation that model suffers from degradation issues during adaptation training, we introduce a mean teacher strategy to update the model via an exponential moving average way, which alleviates error accumulation. Meanwhile, by investigating the foreground and background imbalance problem, we present a global knowledge guided loss calibration module. Experiments on two fundus image segmentation datasets show that CBMT outperforms previous SFDA methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Training curve of vanilla pseudo-labeling, DPL [4] and our approach. (b) Fundus image and its label with class proportion from the RIM-ONE-r3 dataset. (c) Illustrated framework of our proposed CBMT method.</figDesc><graphic coords="3,45,09,53,87,334,48,137,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Model performance with different EMA update rate Î» setting. (b) Training curves with and without our proposed loss calibration scheme.</figDesc><graphic coords="9,50,31,53,75,323,92,127,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Ablation study results of our proposed modules on the RIM-ONE-r3 dataset. P-L means vanilla pseudo-labeling method. * represents the accuracy is manually selected from the best epoch. The best results are highlighted.</figDesc><table><row><cell>DA [4]</cell><cell cols="4">83.18Â±6.46 24.15Â±15.58 74.51Â±16.40 14.44Â±11.27</cell></row><row><cell>Oracle [26]</cell><cell>96.80</cell><cell>-</cell><cell>85.60</cell><cell>-</cell></row><row><cell>BEAL [26]</cell><cell>Ã 89.80</cell><cell>-</cell><cell>81.00</cell><cell>-</cell></row><row><cell>AdvEnt [25]</cell><cell cols="2">Ã 89.73Â±3.66 9.84Â±3.86</cell><cell cols="2">77.99Â±21.08 7.57Â±4.24</cell></row><row><cell>SRDA [1]</cell><cell cols="2">89.37Â±2.70 9.91Â±2.45</cell><cell cols="2">77.61Â±13.58 10.15Â±5.75</cell></row><row><cell>DAE [15]</cell><cell cols="2">89.08Â±3.32 11.63Â±6.84</cell><cell cols="2">79.01Â±12.82 10.31Â±8.45</cell></row><row><cell>DPL [4]</cell><cell cols="2">90.13Â±3.06 9.43Â±3.46</cell><cell cols="2">79.78Â±11.05 9.01Â±5.59</cell></row><row><cell>CBMT(Ours)</cell><cell cols="2">93.36Â±4.07 6.20Â±4.79</cell><cell cols="2">81.16Â±14.71 8.37Â±6.99</cell></row><row><cell>Drishti-GS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W/o DA [4]</cell><cell cols="2">93.84Â±2.91 9.05Â±7.50</cell><cell cols="2">83.36Â±11.95 11.39Â±6.30</cell></row><row><cell>Oracle [26]</cell><cell>97.40</cell><cell>-</cell><cell>90.10</cell><cell>-</cell></row><row><cell>BEAL [26]</cell><cell>Ã 96.10</cell><cell>-</cell><cell>86.20</cell><cell>-</cell></row><row><cell>AdvEnt [25]</cell><cell cols="2">Ã 96.16Â±1.65 4.36Â±1.83</cell><cell cols="2">82.75Â±11.08 11.36Â±7.22</cell></row><row><cell>SRDA [1]</cell><cell cols="2">96.22Â±1.30 4.88Â±3.47</cell><cell cols="2">80.67Â±11.78 13.12Â±6.48</cell></row><row><cell>DAE [15]</cell><cell cols="2">94.04Â±2.85 8.79Â±7.45</cell><cell cols="2">83.11Â±11.89 11.56Â±6.32</cell></row><row><cell>DPL [4]</cell><cell cols="2">96.39Â±1.33 4.08Â±1.49</cell><cell cols="2">83.53Â±17.80 11.39Â±10.18</cell></row><row><cell>CBMT(Ours)</cell><cell cols="2">96.61Â±1.45 3.85Â±1.63</cell><cell cols="2">84.33Â±11.70 10.30Â±5.88</cell></row><row><cell cols="5">P-L EMA Aug. Calib. Avg. Dice â Avg. ASSD â</cell></row><row><cell></cell><cell></cell><cell>64.19</cell><cell>15.11</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(84.68*)</cell><cell>(9.67*)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>83.63</cell><cell>8.51</cell><cell></cell></row><row><cell></cell><cell></cell><cell>84.36</cell><cell>8.48</cell><cell></cell></row><row><cell></cell><cell></cell><cell>86.04</cell><cell>8.26</cell><cell></cell></row><row><cell></cell><cell></cell><cell>87.26</cell><cell>7.29</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Loss calibration weight with different thresholds Î± on RIM-ONE-r3 dataset. Our method is robust to the hyper-parameter setting.</figDesc><table><row><cell>Î±</cell><cell>0</cell><cell>0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</cell></row><row><cell>Î· fg k /Î· bg k</cell><cell cols="2">2.99 0.24 0.24 0.24 0.24 0.24 0.23 0.23 0.22 0.22</cell></row><row><cell cols="3">some metrics. Especially in the RIM-ONE-r3 dataset, our CBMT gains a great</cell></row><row><cell cols="3">performance increase than previous works (dice gains by 3.23 on disc), because</cell></row><row><cell cols="3">the domain shift issue is severer here and has big potential for improvement.</cell></row><row><cell cols="3">Moreover, CBMT alleviates the need for precise tuning of hyper-parameters.</cell></row><row><cell cols="3">Here we could set a relatively long training procedure (our epoch number is</cell></row><row><cell>10 times that of</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code can be found in https://github.com/lloongx/SFDA-CBMT.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was partly supported by <rs type="funder">Shenzhen Key Laboratory of</rs> next generation interactive media innovative technology (No: <rs type="grantNumber">ZDSYS202 10623092001004</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HEPaug7">
					<idno type="grant-number">ZDSYS202 10623092001004</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Source-Relaxed domain adaptation for image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bateson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_48" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="490" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards cross-modal organ translation and segmentation: a cycle-and shape-consistent generative adversarial network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="174" to="184" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-8_13" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_22" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="225" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46976-8_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46976-8_19" />
	</analytic>
	<monogr>
		<title level="m">LABELS/DLMIA -2016</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">10008</biblScope>
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rimone: an open retinal image database for optic nerve evaluation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fumero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>AlayÃ³n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sigut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzalez-Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 24th International Symposium on Computer-based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for facilitating stain-independent supervised and unsupervised segmentation: a study on kidney histology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gadermayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Klinkhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Merhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2293" to="2302" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Camouflaged object detection with feature decomposition and edge reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22046" to="22055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Weakly-supervised concealed object segmentation with SAM-based pseudo labeling and multi-scale feature grouping</title>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11003</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MultiResUNet: rethinking the u-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for biomedical image segmentation using adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="554" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation in brain lesion segmentation with adversarial networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-59050-9_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-59050-9_47" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10265</biblScope>
			<biblScope unit="page" from="597" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Test-time adaptable neural networks for robust medical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Karani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101907</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial feature hallucination networks for fewshot learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13470" to="13479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adapting off-the-shelf source Segmenter for target medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>El Fakhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_51" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="549" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09480</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">V-Net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Refuge challenge: a unified framework for evaluating automated methods for glaucoma assessment from fundus photographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Breda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Keer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Bathula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diaz-Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101570</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mobilenetv 2: inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comprehensive retinal image dataset for the assessment of glaucoma from the optic nerve head analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Tabish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSM Biomed. Imaging Data Pap</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1004</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ADVENT: adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>PÃ©rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boundary and entropydriven adversarial learning for fundus image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-7_12" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="102" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Denoising for relaxing: unsupervised domain adaptive fundus image segmentation without source data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_21" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
