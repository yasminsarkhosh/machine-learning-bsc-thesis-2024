<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification</title>
				<funder ref="#_uxhHhg2 #_ad9UPCy #_DJe8atH">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_V83vKsA">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_faTxJus">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yingjie</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Collaborative Innovation Center of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University Affiliated Sir Run Run Shaw Hospital</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianfeng</forename><surname>Gu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyin</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<orgName type="department" key="dep3">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>minzhang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Collaborative Innovation Center of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="130" to="140"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">488B743BE052986CD83292233D4B8F78</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_13</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semi-supervised learning</term>
					<term>Deep evidential regression</term>
					<term>EfficientNet-V2</term>
					<term>Alzheimer&apos;s disease</term>
					<term>Multi-modality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Alzheimer's disease (AD) is an irreversible neurodegenerative disease, so early identification of Alzheimer's disease and its early stage disorder, mild cognitive impairment (MCI), is of great significance. However, currently available labeled datasets are still small, so the development of semi-supervised classification algorithms will be beneficial for clinical applications. We propose a novel uncertainty-aware semisupervised learning framework based on the improved evidential regression. Our framework uses the aleatoric uncertainty (AU) from the data itself and the epistemic uncertainty (EU) from the model to optimize the evidential classifier and feature extractor step by step to achieve the best performance close to supervised learning with small labeled data counts. We conducted various experiments on the ADNI-2 dataset, demonstrating the effectiveness and advancement of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Alzheimer's disease (AD) is an irreversible neurodegenerative disease that leaves patients with impairments in memory, language and cognition <ref type="bibr" target="#b6">[7]</ref>. Previous work of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">22]</ref> show that the combination of image data and other related data is beneficial to the improvement of model performance, but how to efficiently combine statistical non-imaging data and medical image data is still an open question. Second, although it is not too difficult to obtain and collect patient data, subjective bias in the AD diagnosis process and the time-consuming and complicated process of labeling the diagnostic results lead to the scarcity of labeled data <ref type="bibr" target="#b11">[11]</ref>. Therefore, research and development of models that require only a small amount of labeled data to achieve higher accuracy has attracted great attention <ref type="bibr" target="#b14">[14]</ref>.</p><p>Semi-supervised learning (SSL) methods are commonly used in medical image analysis to address the lack of manually annotated data <ref type="bibr" target="#b24">[24]</ref>. Hang et al. <ref type="bibr" target="#b10">[10]</ref> proposed a contrastive self-ensembling framework by introducing the weight formula and reliability-awareness for semi-supervised medical image classification. In <ref type="bibr" target="#b2">[3]</ref>, Aviles et al., based on the diffusion model and hypergraph learning, proposed a multi-modal hypergraph diffusion network to implement semi-supervised learning for AD classification. In <ref type="bibr" target="#b3">[4]</ref>, researchers introduced CSEAL, a semi-supervised learning framework that combines consistency-based SSL with uncertainty-based active learning, for multi-label chest X-ray classification tasks. Other studies using evidential learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">17]</ref> have demonstrated the great potential of this theory in fitting low-dimensional manifolds in high-dimensional spaces for classification with uncertainty estimates. This feature makes the model based on evidential learning promising in the SSL field of medical images.</p><p>The proposal of evidential deep learning (EDL) <ref type="bibr" target="#b21">[21]</ref> allows the model to better estimate the uncertainty in multi-classification tasks. On the binary classification task, controlling evidential regression to obtain a continuous probability value before 0 and 1 can often achieve more accurate results than using the Dirichlet distribution to obtain a discrete distribution of EDL <ref type="bibr" target="#b19">[19]</ref>. The residual between the prediction results of the imperfect model and the true distribution of the data can be decomposed into aleatoric uncertainty (AU) and epistemic uncertainty (EU). Theoretically, the former comes from the noise of the data, which usually does not depend on the sample size. Therefore, by iteratively reducing this part of uncertainty, the best classification results can be obtained under a given amount of data. The latter is proportional to the sample size. As sample size increases, the reduction of this part of uncertainty can make the model closer to the observed distribution or fit with more complex conditions, thereby improving the performance of the model itself. Based on this understanding, we exploit the ability of evidential regression of handling uncertainty to decompose the two parts of uncertainty, AU and EU, and proposed a method by adjusting the two parts of uncertainty to achieve semi-supervised classification which shows in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Our main contributions include: 1) Adjusting the loss function of evidential regression so it can obtain more accurate results and better separate AU and EU; 2) Building a multi-layer and multi-step network to implement evidential regression and a semi-supervised learning method of step-by-step training is proposed; 3) A new SOTA of semi-supervised learning is achieved on the ADNI dataset, and performance close to supervised learning can be achieved with only a small amount of labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Original Deep Evidential Regression (DER)</head><p>DER <ref type="bibr" target="#b1">[2]</ref> adopts the simplest setting: y i ∼ N (0, σ 2 i ). In a Bayesian framework, this corresponds to taking the normal inverse Gamma distribution NIG(μ, σ 2 |m), m = (γ, ν, α, β), as a conjugate prior of a normal contribution with unknown mean μ and variance σ 2 . Combining the disturbance parameter with Bayesian inference, the likelihood of an observation y for a given m follows a t-distribution with 2α i degrees of freedom: βi(1+γi)   γiαi . For known m, Animi et al. <ref type="bibr" target="#b1">[2]</ref> defined the prediction of y i as E[μ i ] = γ i , and defined AU and EU as u a and u e :</p><formula xml:id="formula_0">L NIG i = St 2αi y i |γ i ,</formula><formula xml:id="formula_1">u 2 a = E[σ 2 i ] = β i /(α i -1), u 2 e = var[μ i ] = E[σ 2 i ]/ν i .</formula><p>And it follows that:</p><formula xml:id="formula_2">L i (w) = -log L NIG i (w) + λL R i (w), L R i (w) = |y i -γ i | • Φ</formula><p>where m = NN(w) is specified by a neural network (NN), λ is a hyperparameter, and Φ = 2γ i + α i represents the total evidence gained from training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evidential Regression Beyond DER</head><p>Although DER has achieved some success in both theoretical and practical applications <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">15]</ref>, as pointed out by Meinert et al. <ref type="bibr" target="#b16">[16]</ref>, this theory has some major flaws. First, although the regularization part of loss function L R is added, the constrain on parameter β i is not enough. Second, although the two parts of AU and EU are defined separately, the correlation between them is too high. In practice, disentangling and effectively using uncertainty information for training remains challenging.</p><p>After practice and theoretical proof, Meinert et al. <ref type="bibr" target="#b16">[16]</ref> states that the width of the t-distribution projected by the NIG distribution, that is, w St , can better reflect the noise in data.</p><p>And, correspondingly, we use the residual 1/ √ ν i part of u a and u e in the original definition to represent EU:</p><formula xml:id="formula_3">u A = w St = β i (1 + ν i ) α i ν i , u E = u e u a = 1 √ ν i</formula><p>where ν i , α i , and β i are part of the parameters of the evidence distribution m = (γ, ν, α, β), and we verify the performance of this new uncertainty estimation method through experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model and Workflow</head><p>With efficient estimation of AU and EU, our model has the basis for implementation. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our model is divided into three parts: multimodal feature extractor, evidential predictor, and recycle classifier. The multimodal feature extractor form a high-dimensional feature space and the evidential predictor generates the evidential distribution in the feature space. After calculating the classification result and the uncertainty (AU and EU) based on the evidential distribution, the recycle classifier controls the training process and reaches the best performances through a step-by-step recurrent training workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AU for Training Classifier.</head><p>Based on the manifold assumption, the real data is gathered on the low-dimensional manifold of the high-dimensional space, and the noise of the data is located on the edge of the manifold for the corresponding category. When using ER to fit the manifold, these noise data will make marginal data with high AU. By optimizing the classifier to iteratively reduce the AU, optimal classification result under the current conditions can be obtained. We use L a to optimize AU:</p><formula xml:id="formula_4">L a i (w) = -log L NIG i (w) + λ a γ i -γ 2 i w 2 St Φ .</formula><p>In the above formula λ a = [0.005, 0.01] is a parameter that controls the degree of deviations of the regularization part and w St uses the previous definition, and Φ is the total amount of evidence learned by the model. In order to better motivate the learning of the model, we adopted the work of Liu et al. <ref type="bibr" target="#b15">[15]</ref> and used the form of Φ = γ i + 2α i . We used the expanded form of L NIG with minor adjustments according to the optimization objective, specifically,log</p><formula xml:id="formula_5">L NIG i = 1 2 log( π ν ) -α log(2β +2βν)+(α +0.5) log(ν(γ i -γ 2 i )+2β +2βν) + log Γ (α) Γ (α+0.5) .</formula><p>EU for Training Extractor. If only the AU part is optimized, there will always be this gap between the model prediction and the real data. EU is mainly used to optimize the feature extractor since EU mainly reflects the bias of the model in the prediction. For data D l , given groundtruth labels, we use which can make the model more conservative about making predictions in the next iteration. This reduces our models being affected by misleading evidence and obtains better performance by retaining higher uncertainty to allow the model to have more room to optimize. In order to effectively combine labeled and unlabeled data we adjust the weights of different data:</p><formula xml:id="formula_6">L l i (w) = -log L NIG i (w) + λ l yi-γi wSt</formula><formula xml:id="formula_7">L e i (w) = μ l L l i + μuL u i = -log L NIG i (w) + μuλu y i -γi wSt 2 Φ + μ l λ l yi -γi wSt 2 Φ</formula><p>where μ l + μ u = 1, μ l , μ u ∈ [0, 1], are two weight factors.</p><p>Model. In terms of the feature extractor, we use the latest EfficientNetV2, which, in Feng et al. <ref type="bibr" target="#b7">[8]</ref>, has achieved good results in combination with EDL.</p><p>In order to avoid overfitting, we used the minimum model in this network and added Dropout to the output end. At the same time, in order to fill the differences between multi-modality data and model input, we have added the fully connected (FC) layer and convolutional layer (Conv) to adaptive adjust input channels. We employed three evidential FC layers proposed by Amini et al. <ref type="bibr" target="#b1">[2]</ref> to form our evidential predictor. At the same time, in order to achieve the optimization of AU and EU separately, we froze some parameters in the first two layers and limited the range of the last layer of parameter adjustment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Workflow of Recycle</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Data Description. In this paper, we assess the effectiveness of our multimodal semi-supervised evidential recycle framework on the ADNI-2 dataset 1 , which comprises multi-center data consisting of various modalities, including imaging and multiple phenotype data. Specifically, the dataset consists of four categories: normal control (NC), early mild cognitive impairment (EMCI), late mild cognitive impairment (LMCI), and Alzheimer's disease (AD). To ensure the effectiveness of our training and balance the number of categories, we used a sample of 515 patients, utilizing their MRI, PET, demographics, and APOE as inputs. On MRI images, we used 3T T1-weighted and FLAIR MR images, and the preprocessing process used CAT12 and SPM tools. All MRI data were processed using standard pipeline, including anterior commissure (AC)-posterior commissure (PC) correction, intensity correction, and skull stripping. Affine registration is performed to linearly align each MRI to the Colin27 template and resample to 224 × 224 × 91 for subsequent processing. For PET images, we used the official pre-processed AV-45 PET image and resampled them in the same way as the MRIs. We chose to include APOE in our analysis, as it is a well-established genetic risk factor for developing AD.</p><p>Evaluation. We evaluated our model from three aspects. First, for the sake of comparison, we followed the technical conventions of most similar studies and selected three comparison tasks: AD vs NC, LMCI vs NC, and EMCI vs LMCI.</p><p>Second, we compared and demonstrated the results of our model under different numbers of ground truth labels to verify that its performance improves as the label data volume increases. Third, we conducted different ablation experiments, which shows in Fig. <ref type="figure" target="#fig_3">3</ref>, to prove the validity and rationality of each part of the proposed model framework. Among them, CNNs represents the performance when using only the EfficientNetV2-S model and its original supervised learning classifier without using unlabeled data for training, which is the baseline model.</p><p>AU and EU represent the training process using only the corresponding parts. DER uses our proposed complete training process but does not use our improved u a and u e estimations, instead continuing to use the estimation method u A and u E which proposed in the original DER paper <ref type="bibr" target="#b1">[2]</ref>. To compare performance fairly, we ran all techniques under the same conditions. The results were evaluated on accuracy (ACC), specificity (SPE), and sensitivity (SEN).</p><p>Implementation Details. The upper bound in performance is the result obtained when the model is trained with all the input data are labeled. In the current supervised learning algorithms, the performance of each algorithm on  each task is not consistent, so we selected three papers in supervised learning, each representing the SOTA performance of the three tasks <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23]</ref> for comparison. Our implementation employs PyTorch v1.4.0 and utilizes the Adam optimizer with a learning rate of 1 × 10 -4 and a weight decay of 1 × 10 -5 . We utilize a linear decay scheduler of 0.1 based on the loss functions above. The optimizer is set with β values of [0.9, 0.999] and value of 1 × 10 -8 . In terms of data, since the SSL method needs to learn from unlabeled data, 100% of the data is put into training, and some of the data have ground truth labels. In the test, only the result index of the unlabeled data is calculated, so the training set and the test set are not divided. But in order to determine the threshold of each uncertainty, we randomly selected 10% of the data as the validation set, and calculated the uncertainty independently outside the training process.</p><p>Results. We compared our model with the semi-supervised learning methods currently achieving the best performance on the ADNI-2 dataset, as well as other top models in the semi-supervised learning field. As shown in Table <ref type="table" target="#tab_0">1</ref>, our model achieved SOTA performance in all three tasks of the semi-supervised learning category. At the same time, compared with other semi-supervised learning algorithms, our results are unprecedentedly close to the best supervised learning methods, indicating the performance of our model under less labeled data and the feasibility of applying this algorithm in clinical settings.</p><p>Our ablation experiment results are shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Firstly, compared with the baseline, our semi-supervised learning algorithm effectively learns classification information from unlabeled data. Secondly, compared with DER, our uncertainty estimation surpasses the original DER method. The AU and EU items demonstrate the importance of optimizing both the AU and EU components in our framework. From Table <ref type="table" target="#tab_2">2</ref>, we can observe that we have outperformed the currently representative advanced semi-supervised learning algorithm DS 3 L <ref type="bibr" target="#b9">[9]</ref> in each labeled data count. At the same time, the superiority of our model compared to the baseline method also proves the learning efficiency of our framework. The performance of our model at 20% labeled data count is already very close to the upper bound, which is the result obtained using 100% labeled data. This indicates the strong learning ability of our model in the case of a small labeled data amount.</p><p>In addition, we have plotted the error rate of our framework under different labeled data counts in Fig. <ref type="figure" target="#fig_4">4</ref>. It is apparent that the performance of our model improves as the labeled data amount increases from 5% to 10%, 15%, and 20%. Combining with Table <ref type="table" target="#tab_2">2</ref>, we can observe the well-known transductive effect in the field of semi-supervised learning, which means that beyond a certain data amount, increasing the size of the dataset can only bring marginal performance improvement. This is evident when comparing the model performance under 20%, 40%, 80%, and 100% labeled data counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We proposed an evidential regression-based semi-supervised learning framework, using the characteristics of AU and EU to train classifiers and extractors, respectively. Our model achieves SOTA performance on the ADNI-2 dataset. And due to the characteristics of semi-supervised learning, our model has unique advantages in adding private data, fine-tuning downstream tasks, and avoiding overfitting, which makes our model have great potential in clinical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An iteration of optimization of our model. The AU optimization process does not rely on any label and obtains the best classification of the current model by reducing AU. Then, on the basis of this classification, EU optimization relies on ground truth and pseudo labels to optimize the model to get better prediction results.</figDesc><graphic coords="2,122,49,327,95,207,55,145,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of our network. The recycle classifier part judges the decrease of the uncertainty of the predicted value, and controls the framework to train the other two parts in a loop.</figDesc><graphic coords="4,57,48,54,38,337,36,122,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Φ(w) + λ u y i -γi wSt 2 Φ</head><label>22</label><figDesc>. In order to enhance the certainty of the data with ground truth, we set a smaller λ l = [0.005, 0.015]. For dataset D u without real labels, we use the prediction results y obtained in the last iterative training to replace the real labels to get L u i (w) =log L NIG i . In order for the model to utilize the results of the previous round of learning, we set a larger λ u = [0.015, 0.025],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Ablation experiment results.</figDesc><graphic coords="7,45,30,495,50,333,88,79,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Error rate of different percentages of label counts, the well-known conduction effect in the field of semi-supervised learning can be observed.</figDesc><graphic coords="9,80,31,54,02,263,68,128,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Training. First, we do not fix any weight and we use a small part of the data for warm-up training. Second, we freeze the weight update of the extractor and P 2 and P 3 in the evidential predictor and use L a to optimize the classifier. We calculate and record the AU score after each update. When the difference between the update |ΔAU | is smaller than the threshold value T a = [0.0005, 0.001] we set, the cycle of AU optimization is over. Then, we fix the weight of P1 and P 3 in the evidential predictor and use L e to optimize the extractor. Similarly, when the change |ΔEU | brought by the update is less than the threshold T e = [0.0025, 0.005], end the cycle and output y . Finally, we fix all network parameters except P 3 to fine-tune until |ΔU | = |ΔAU |+|ΔEU | brought by the update loss function L = L e +L a is less than threshold T u = [0.002, 0.005].All thresholds are adjusted according to the proportion of labels and unlabeled data during training. Classification results of all comparison methods on ADNI-2 dataset (%). The top section of the table shows the results of supervised learning (SL), while the bottom section shows the performance of the current SSL SOTA methods.</figDesc><table><row><cell>Method</cell><cell>AD vs. NC</cell><cell>EMCI vs. LMCI</cell><cell>LMCI vs. NC</cell></row><row><cell></cell><cell cols="3">ACC SPE SEN ACC SPE SEN ACC SPE SEN</cell></row><row><cell>Baseline [12]</cell><cell cols="3">80.53 80.10 80.32 74.10 73.18 75.85 72.05 70.80 71.26</cell></row><row><cell>SL SOTA [18, 20, 23]</cell><cell cols="3">96.84 98.23 95.76 92.40 93.70 89.50 92.49 91.08 93.48</cell></row><row><cell>Upper bound (UB)</cell><cell cols="3">94.45 93.80 94.07 89.95 90.29 90.81 88.56 88.81 86.32</cell></row><row><cell>Π model [13]</cell><cell cols="3">90.45 85.82 90.05 81.58 80.15 84.50 80.65 83.48 78.75</cell></row><row><cell>DS 3 L [9]</cell><cell cols="3">90.86 89.03 89.72 81.07 83.25 82.81 80.79 81.55 81.16</cell></row><row><cell>RFS-LDA [1]</cell><cell cols="3">92.11 89.50 88.40 80.90 81.05 83.63 81.90 84.72 80.05</cell></row><row><cell cols="4">Hypergraph diffusion [3] 92.11 92.80 91.33 85.22 86.40 84.02 82.01 84.01 81.80</cell></row><row><cell>Ours</cell><cell>93.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>90 92.95 93.01 89.45 88.50 89.47 87.27 86.94 85.83</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Classification results(%) with different percentages of labeled and unlabelled data in the training process.</figDesc><table><row><cell>Method</cell><cell cols="2">Labeled AD vs. NC</cell><cell>EMCI vs. LMCI</cell><cell>LMCI vs. NC</cell></row><row><cell></cell><cell></cell><cell cols="3">ACC SPE SEN ACC SPE SEN ACC SPE SEN</cell></row><row><cell cols="2">Upper Bound 100%</cell><cell cols="3">94.45 93.80 94.07 89.95 90.29 90.81 88.56 88.81 86.32</cell></row><row><cell>Baseline</cell><cell>5%</cell><cell cols="3">75.11 73.92 75.79 73.17 69.47 75.14 68.95 67.81 68.53</cell></row><row><cell>DS 3 L</cell><cell></cell><cell cols="3">78.92 76.75 78.14 74.68 71.65 72.39 74.27 70.58 73.71</cell></row><row><cell>Ours</cell><cell></cell><cell cols="3">82.45 81.63 84.97 73.05 70.84 74.22 73.86 74.15 72.28</cell></row><row><cell>Baseline</cell><cell>10%</cell><cell cols="3">78.50 78.03 77.48 73.67 70.29 74.11 69.13 68.57 68.38</cell></row><row><cell>DS 3 L</cell><cell></cell><cell cols="3">84.73 79.67 82.69 80.93 80.13 79.54 74.57 75.39 72.88</cell></row><row><cell>Ours</cell><cell></cell><cell cols="3">90.18 89.73 87.42 81.67 83.45 82.29 80.01 78.25 80.89</cell></row><row><cell>Baseline</cell><cell>20%</cell><cell cols="3">80.53 80.10 80.32 74.10 73.18 75.85 72.05 70.80 71.26</cell></row><row><cell>DS 3 L</cell><cell></cell><cell cols="3">90.86 89.03 89.72 81.07 83.25 82.81 80.79 81.55 81.16</cell></row><row><cell>Ours</cell><cell></cell><cell>93.90</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>92.95 93.01 89.45 88.50 89.47 87.27 86.94 85.83</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>*Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu).</p></note>
		</body>
		<back>

			<div type="funding">
<div><p><rs type="person">M. Zhang</rs> was partially supported by <rs type="grantNumber">NSFC62202426</rs>. X. Gu was partially supported by <rs type="funder">NIH</rs> <rs type="grantNumber">3R01LM012434-05S1</rs>, <rs type="grantNumber">1R21EB029733-01A1</rs>, <rs type="funder">NSF</rs> <rs type="grantNumber">FAIN-2115095</rs>, <rs type="funder">NSF</rs> <rs type="grantNumber">CMMI-1762287</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_V83vKsA">
					<idno type="grant-number">NSFC62202426</idno>
				</org>
				<org type="funding" xml:id="_faTxJus">
					<idno type="grant-number">3R01LM012434-05S1</idno>
				</org>
				<org type="funding" xml:id="_uxhHhg2">
					<idno type="grant-number">1R21EB029733-01A1</idno>
				</org>
				<org type="funding" xml:id="_ad9UPCy">
					<idno type="grant-number">FAIN-2115095</idno>
				</org>
				<org type="funding" xml:id="_DJe8atH">
					<idno type="grant-number">CMMI-1762287</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised discriminative classification robust to sampleoutliers and feature-noises</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="515" to="522" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep evidential regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schwarting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soleimany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14927" to="14937" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal hypergraph diffusion network with dual prior for Alzheimer classification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Aviles-Rivero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Runkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kourtzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Schönlieb</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_69</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-869" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part III</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="717" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised evidential active learning for diagnostic radiograph classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-664" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part I</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="675" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pitfalls of epistemic uncertainty quantification through loss minimisation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bengs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Waegeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing variations in multi-center Alzheimer&apos;s disease classification with convolutional adversarial autoencoder</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Cobbinah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102585</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cellular phase of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Strooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Karran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="615" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">End-to-end evidentialefficient net for radiomics analysis of brain MRI to predict oncogene expression and overall survival</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<editor>Wang, L., Dou, Q., Fletcher, P.T., Speidel, S., Li, S.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<idno type="DOI">10.1007/978-3-031-16437-8_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-827" />
		<title level="m">MICCAI 2022, Part III</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="282" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Safe deep semi-supervised learning for unseen-class unlabeled data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reliability-aware contrastive self-ensembling for semi-supervised medical image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_71</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-671" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-scale graph-based grading for Alzheimer&apos;s disease prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Manjón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coupé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101850</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Cision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Cision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chronic gastritis classification using gastric x-ray images with a semi-supervised learning method based on tri-training</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haseyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Biol. Eng. Comput</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1239" to="1250" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient and robust lidar-based end-to-end navigation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13247" to="13254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Meinert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gawlikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<title level="m">The unreasonable effectiveness of deep evidential regression</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2205</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MetaEDL: Meta evidential learning for uncertainty-aware cold-start recommendations</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Neupane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1258" to="1263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation-induced multi-modal shared representation learning for Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1632" to="1645" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving evidential deep learning via multi-task learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="7895" to="7903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-scale attentionbased pseudo-3D convolution neural network for Alzheimer&apos;s disease diagnosis using structural MRI. Pattern Recogn</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">108825</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph convolution network with similarity awareness and adaptive calibration for disease-induced deterioration prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101947</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-center and multi-channel pooling GCN for early AD diagnosis based on dual-modality fused brain network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on deep semi-supervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
