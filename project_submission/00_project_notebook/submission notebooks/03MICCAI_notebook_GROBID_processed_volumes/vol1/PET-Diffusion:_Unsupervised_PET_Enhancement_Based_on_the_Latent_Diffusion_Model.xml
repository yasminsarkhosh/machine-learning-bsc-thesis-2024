<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model</title>
				<funder ref="#_8BJHNrC">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_2dePNRJ">
					<orgName type="full">Key R&amp;D Program of Guangdong Province, China</orgName>
				</funder>
				<funder ref="#_B598h5V">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_5x6N4rq">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_9cp4V6V">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
					<orgName type="abbreviated">STCSM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Caiwen</forename><surname>Jiang</surname></persName>
							<email>jiangcw@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongsheng</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mianxin</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>200232</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiameng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaosong</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<postCode>200230</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Shanghai Clinical Research and Trial Center</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="3" to="12"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7B8AAFD015B560C2102D73D23FEB04E2</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Positron emission tomography</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Positron emission tomography (PET) is an advanced nuclear imaging technique with an irreplaceable role in neurology and oncology studies, but its accessibility is often limited by the radiation hazards inherent in imaging. To address this dilemma, PET enhancement methods have been developed by improving the quality of low-dose PET (LPET) images to standard-dose PET (SPET) images. However, previous PET enhancement methods rely heavily on the paired LPET and SPET data which are rare in clinic. Thus, in this paper, we propose an unsupervised PET enhancement (uPETe) framework based on the latent diffusion model, which can be trained only on SPET data. Specifically, our SPET-only uPETe consists of an encoder to compress the input SPET/LPET images into latent representations, a latent diffusion model to learn/estimate the distribution of SPET latent representations, and a decoder to recover the latent representations into SPET images. Moreover, from the theory of actual PET imaging, we improve the latent diffusion model of uPETe by 1) adopting PET image compression for reducing the computational cost of diffusion model, 2) using Poisson diffusion to replace Gaussian diffusion for making the perturbed samples closer to the actual noisy PET, and 3) designing CT-guided cross-attention for incorporating additional CT images into the inverse process to aid the recovery of structural details in PET. With extensive experimental validation, our uPETe can achieve superior performance over state-of-the-art methods, and shows stronger generalizability to the dose changes of PET imaging. The code of our implementation is available at https://github.com/jiang-cw/PET-diffusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Positron emission tomography (PET) is a sensitive nuclear imaging technique, and plays an essential role in early disease diagnosis, such as cancers and Alzheimer's disease <ref type="bibr" target="#b7">[8]</ref>. However, acquiring high-quality PET images requires injecting a sufficient dose (standard dose) of radionuclides into the human body, which poses unacceptable radiation hazards for pregnant women and infants even following the As Low As Reasonably Achievable (ALARA) principle <ref type="bibr" target="#b18">[19]</ref>. To reduce the radiation hazards, besides upgrading imaging hardware, designing advanced PET enhancement algorithms for improving the quality of low-dose PET (LPET) images to standard-dose PET (SPET) images is a promising alternative.</p><p>In recent years, many enhancement algorithms have been proposed to improve PET image quality. Among the earliest are filtering-based methods such as non-local mean (NLM) filter <ref type="bibr" target="#b0">[1]</ref>, block-matching 3D filter <ref type="bibr" target="#b3">[4]</ref>, bilateral filter <ref type="bibr" target="#b6">[7]</ref>, and guided filter <ref type="bibr" target="#b21">[22]</ref>, which are quite robust but tend to over-smooth images and suppress the high-frequency details. Subsequently, with the development of deep learning, the end-to-end PET enhancement networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref> were proposed and achieved significant performance improvement. But these supervised methods relied heavily on the paired LPET and SPET data that are rare in actual clinic due to radiation exposure and involuntary motions (e.g., respiratory and muscle relaxation). Consequently, unsupervised PET enhancement methods such as deep image prior <ref type="bibr" target="#b2">[3]</ref>, Noise2Noise <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref>, and their variants <ref type="bibr" target="#b16">[17]</ref> were developed to overcome this limitation. However, these methods still require LPET to train models, which contradicts with the fact that only SPET scans are conducted in clinic.</p><p>Fortunately, the recent glowing diffusion model <ref type="bibr" target="#b5">[6]</ref> provides us with the idea for proposing a clinically-applicable PET enhancement approach, whose training only relies on SPET data. Generally, the diffusion model consists of two reversible processes, where the forward diffusion adds noise to a clean image until it becomes pure noise, while the reverse process removes noise from pure noise until the clean image is recovered. By combining the mechanics of diffusion model with the observation that the main differences between LPET and SPET are manifested as levels of noises in the image <ref type="bibr" target="#b10">[11]</ref>, we can view LPET and SPET as results at different stages in an integrated diffusion process. Therefore, when a diffusion model (trained only on SPET) can recover noisy samples to SPET, this model can also recover LPET to SPET. However, extending the diffusion model developed for 2D photographic images to PET enhancement still faces two problems: a) three-dimensionsal (3D) PET images will dramatically increase the computational cost of diffusion model; b) PET is the detail-sensitive images and may be introduced/lost some details during the procedure of adding/removing noise, which will affect the downstream diagnosis.</p><p>Taking all into consideration, we propose the SPET-only unsupervised PET enhancement (uPETe) framework based on the latent diffusion model. Specifically, uPETe has an encoder-&lt;diffusion model&gt;-decoder structure that first uses the encoder to compress input the LPET/SPET images into latent representations, then uses the latent diffusion model to learn/estimate the distribution of SPET latent representations, and finally uses the decoder to recover SPET images from the estimated SPET latent representations. The keys of our uPETe include 1) compressing the 3D PET images into a lower dimensional space for reducing the computational cost of diffusion model, 2) adopting the Poisson noise, which is the dominant noise in PET imaging <ref type="bibr" target="#b19">[20]</ref>, to replace the Gaussian noise in the diffusion process for avoiding the introduction of details that are not existing in PET images, and 3) designing CT-guided cross-attention to incorporate additional CT images into the inverse process for helping the recovery of structural details in PET.</p><p>Our work had three main features/contributions: i) proposing a clinicallyapplicable unsupervised PET enhancement framework, ii) designing three targeted strategies for improving the diffusion model, including PET image compression, Poisson diffusion, and CT-guided cross-attention, and iii) achieving better performance than state-of-the-art methods on the collected PET datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The framework of uPETe is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. When given an input PET image x (i.e., SPET for training and LPET for testing), x is first compressed into the latent representation z 0 by the encoder E. Subsequently, z 0 is fed into a latent diffusion model followed by the decoder D to output the expected SPET image x. In addition, a specialized encoder E CT is used to compress the CT image corresponding to the input PET image into the latent representation z CT , which is fed into each denoising network for CT-guided cross-attention. In the following, we introduce the details of image compression, latent diffusion model, and implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Compression</head><p>The conventional diffusion model is computationally-demanding due to its numerous inverse denoising steps, which severely restricts its application to 3D PET enhancement. To overcome this limitation, we adopt two strategies including 1) compressing the input image and 2) reducing the diffusion steps (as described in Sect. 2.3).</p><p>Similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, we adopt an autoencoder (E and D) to compress the 3D PET images into a lower dimensional but more compact space. The crucial aspects of this process is to ensure that the latent representation contains the necessary and representative information for the input image. To achieve this, we train the autoencoder by a combination of perceptual loss <ref type="bibr" target="#b23">[24]</ref> and patch-based adversarial loss <ref type="bibr" target="#b4">[5]</ref>, instead of simple voxel-level loss such as L 2 or L 1 loss. Among them, the perceptual loss, designed on a pre-trained 3D ResNet <ref type="bibr" target="#b1">[2]</ref>, constrains higher-level information such as texture and semantic content, and the patchbased adversarial loss ensures globally coherent while remaining locally realistic. Let x ∈ R H,W,Z denote the input image and z 0 ∈ R h,w,z,c denote the latent representation. The compression process can be formulated as x = D(z 0 ) = D(E(x)). In this way, we compress the input image by a factor of f = H/h = W/w = Z/z. The results of SPET estimation under different compression rates f are provided in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Latent Diffusion Model</head><p>After compressing the input PET image, its latent representation is fed into the latent diffusion model, which is the key to achieving the SPET-only unsupervised PET enhancement. As described above, the LPET can be viewed as noisy SPET (even in the compressed space), so the diffusion process from SPET to pure noise actually covers the situations of LPET. That is, the diffusion model trained with SPET is capable of estimating SPET from the noisy sample (diffused from LPET). But the diffusion model is developed from photographic images, which have significant difference with the detail-sensitive PET images. To improve its applicability for PET images, we design several targeted strategies for the diffusion process and inverse process, namely Poisson diffusion and CT-guided cross-attention, respectively.</p><p>Poisson Diffusion. In conventional diffusion models, the forward process typically employs Gaussian noise to gradually perturb input samples. However, in PET images, the dominant source of noise is Poisson noise, rather than Gaussian noise. Considering this, in our uPETe we choose to adopt Poisson diffusion to perturb the input samples, which facilitates the diffusion model for achieving better performance on the PET enhancement task.</p><p>Let z t be the perturbation sample in Poisson diffusion, where t = 0, 1, ..., T . Then the Poisson diffusion can be formulate as follows:</p><formula xml:id="formula_0">z t = perturb(z t-1 , λ t ), λ 1 &lt; λ 2 &lt; ... &lt; λ T . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>At each diffusion step, we apply the perturb function to the previous perturbed sample z t-1 by imposing a Poisson noise with an expectation of λ t , which is linearly interpolated from [0, 1] and incremented with t. In our implementation, we apply the same Poisson noise imposition operation as in <ref type="bibr" target="#b19">[20]</ref>, i.e., applying Poisson deviates on the projected sinograms, to generate a sequence of perturbed samples with increasing Poisson noise intensity as the step number t increases.</p><p>CT-Guided Cross-Attention. The attenuation correction of PET typically relies on the corresponding anatomical image (CT or MR), resulting in a PET scan usually accompanied by a CT or MR scan. To fully utilize the extramodality images (i.e., CT in our work) as well as improve the applicability of diffusion models, we design a CT-guided cross-attention to incorporate the CT images into the reverse process for assisting the recovery of structural details.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, to achieve a particular SPET estimation, the corresponding CT image is first compressed into the latent representation z CT by encoder E CT . Then z CT is fed into a denoising attention U-Net <ref type="bibr" target="#b15">[16]</ref> at each step for calculation of cross-attention, where the query Q and key K are calculated from z CT while the value V is still calculated from the output of the previous layer because our final goal is SPET estimation. Denoting the output of previous layer as z P ET , the CT-guided cross-attention can be formulated as follows:</p><formula xml:id="formula_2">Output = sof tmax( Q CT K T CT √ d + B) • V P ET , Q CT = Conv Q (z CT ), K CT = Conv K (z CT ), V P ET = Conv V (z P ET ),<label>(2)</label></formula><p>where d is the number of channels, B is the position bias, and Conv(•) denotes the 1 × 1 × 1 convolution with stride of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation Details</head><p>Typically, the trained diffusion model generates target images from random noise, requiring a large number of steps T to make the final perturbed sample (z T ) close to pure noise. However, in our task, the target SPET image is generated from a given LPET image during testing, and making z T as close to pure noise as possible is not necessary since the remaining PET-related information can also benefit the image recovery. Therefore, we can considerably reduce the number of diffusion steps T to accelerate the model training, and T is set to 400 in our implementation. We evaluate the quantitative results using two metrics, including Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM).  3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Our dataset consists of 100 SPET images for training and 30 paired LPET and SPET images for testing. Among them, 50 chest-abdomen SPET images are collected from (total-body) uEXPLORER PET/CT scanner <ref type="bibr" target="#b24">[25]</ref>, and 20 paired chest-abdomen images are collected by list mode of the scanner with 256 MBq of [ 18 F]-FDG injection. Specifically, the SPET images are reconstructed by using the 1200 s data between 60-80 min after tracer injection, while the corresponding LPET images are simultaneously reconstructed by 120 s data uniformly sampled from 1200 s data. As a basic data preprocessing, all images are resampled to voxel spacing of 2 × 2 × 2 mm 3 and resolution of 256 × 256 × 160, while their intensity range is normalized to [0, 1] by min-max normalization. For increasing the training samples and reducing the dependence on GPU memory, we extract the overlapped patches of size 96 × 96 × 96 from every whole PET image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Analysis</head><p>To verify the effectiveness of our proposed strategies, i.e. Poisson diffusion process and CT-guided cross-attention, we design another four variant latent diffusion models (LDMs) with the same compression model, including: 1) LDM: standard LDM; 2) LDM-P: LDM with Poisson diffusion process; 3) LDM-CT: LDM with CT-guided cross-attention; 4) LDM-P-CT: LDM with Poisson diffusion process and CT-guided cross-attention. All methods use the same experimental settings, and their quantitative results are given in Table <ref type="table" target="#tab_0">1</ref>.</p><p>From Table <ref type="table" target="#tab_0">1</ref>, we can have the following observations. (1) LDM-P achieves better performance than LDM. This proves that the Poisson diffusion is more appropriate than the Gaussian diffusion for PET enhancement. (2) LDM-CT with the corresponding CT image for assisting denoising achieves better results than LDM. This can be reasonable as the CT image can provide anatomical information, thus benefiting the recovery of structural details (e.g., organ boundaries) in SPET images. (3) LDM-P-CT achieves better results than all other variants on both PSNR and SSIM, which shows both of our proposed strategies contribute to the final performance. These three comparisons conjointly verify the effective design of our proposed uPETe, where the Poisson diffusion process and CT-guided cross-attention both benefit the PET enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-Art Methods</head><p>We further compare our uPETe with several state-of-the-art PET enhancement methods, which can be divided into two classes: 1) fully-supervised methods, including LA-GAN <ref type="bibr" target="#b20">[21]</ref>, Transformer-GAN (Trans-GAN) <ref type="bibr" target="#b12">[13]</ref>, Dual-frequency GAN (DF-GAN) <ref type="bibr" target="#b8">[9]</ref>, and AR-GAN <ref type="bibr" target="#b13">[14]</ref>; 2) unsupervised methods, including deep image prior (DIP) <ref type="bibr" target="#b2">[3]</ref>, Noisier2Noise <ref type="bibr" target="#b22">[23]</ref>, magnetic resonance guided deep decoder (MR-GDD) <ref type="bibr" target="#b16">[17]</ref>, and Noise2Void <ref type="bibr" target="#b19">[20]</ref>. The quantitative and qualitative results are provided in Table <ref type="table" target="#tab_1">2</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>, respectively.</p><p>Quantitative Comparison: Table <ref type="table" target="#tab_1">2</ref> shows that our uPETe outperforms all competing methods. Compared to the fully-supervised method AR-GAN which achieves sub-optimal performance, our uPETe does not require paired LPET and SPET, yet still achieves improvement. Additionally, uPETe also achieves noticeable performance improvement to Noise2Void (which is a supervised method). Specifically, the average improvement in PSNR and SSIM on SPET estimation are 1.554 dB and 0.005, respectively. This suggests that our uPETe can generate promising results without relying on paired data, demonstrating its potential for clinical applications.</p><p>Qualitative Comparison: In Fig. <ref type="figure" target="#fig_2">3</ref>, we provide a visual comparison of SPET estimation for two typical cases. First, compared to unsupervised methods such as DIP and Noise2Void, the SPET images estimated by our uPETe have less noise but clearer boundaries. Second, our uPETe performs better on the structural details compared to the fully-supervised methods, i.e., missing unclear tissue (Trans-GAN) or introducing non-existing artifacts in PET image (DF-GAN). Overall, these pieces of evidence demonstrate the superiority of our uPETe over state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generalization Evaluation</head><p>We further evaluate the generalizability of our uPETe to tracer dose changes by simulating Poisson noise on SPET to produce different doses for LPET, which is a common way to generate noisy PET data <ref type="bibr" target="#b19">[20]</ref>. Notably, we do not need to retrain the models since they have been trained in Sect. 3.3. The quantitative results of our uPETe and five state-of-the-art methods are provided in Fig. <ref type="figure" target="#fig_1">2</ref>. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our uPETe outperforms the other five methods at all doses and exhibits a lower PSNR descent slope as dose decreases (i.e., λ increases), demonstrating its superior generalizability to dose changes. This is because uPETe is based on diffusion model, which simplifies the complex distribution prediction task into a series of simple denoising tasks and thus has strong generalizability. Moreover, we also find that the unsupervised methods (i.e., uPETe, Noise2Void, and DIP) have stronger generalizability than fully-supervised methods (i.e., AR-GAN, DF-GAN, and Trans-GAN) as they have a smoother descent slope. The main reason is that the unsupervised learning has the ability to extract patterns and features from the data based on the inherent structure and distribution of the data itself <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Limitations</head><p>In this paper, we have developed a clinically-applicable unsupervised PET enhancement framework based on the latent diffusion model, which uses only the clinically-available SPET data for training. Meanwhile, we adopt three strategies to improve the applicability of diffusion models developed from photographic images to PET enhancement, including 1) compressing the size of the input image, 2) using Poisson diffusion, instead of Gaussian diffusion, and 3) designing CT-guided cross-attention to enable additional anatomical images (e.g., CT) to aid the recovery of structural details in PET. Validated by extensive experiments, our uPETe achieved better performance than both state-of-the-art unsupervised and fully-supervised PET enhancement methods, and showed stronger generalizability to the tracer dose changes.</p><p>Despite the advance of uPETe, our current work still suffers from a few limitations such as (1) lacking theoretical support for our Poisson diffusion, which is just an engineering attempt, and 2) only validating the generalizability of uPETe on a simulated dataset. In our future work, we will complete the design of Poisson diffusion from theoretical perspective, and collect more real PET datasets (e.g., head datasets) to comprehensively validate the generalizability of our uPETe.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of proposed uPETe. (a) and (b) provide the framework of uPETe as well as depict its implementation during both the training and testing phases, and (c) illustrates the details of CT-guided cross-attention.</figDesc><graphic coords="3,56,46,54,59,339,43,149,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Generalizability to dose changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparison of estimated SPET images on two typical cases. In each case, the first and second rows show the axial and coronal views, respectively, and from left to right are the input (LPET), ground truth (SPET), results by five other methods (3rd-7th columns), and the result by our uPETe (last column). Red boxes and arrows show areas for detailed comparison. (Color figure online)</figDesc><graphic coords="8,42,30,53,93,339,55,163,21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results of ablation analysis, in terms of PSNR and SSIM.</figDesc><table><row><cell>Method</cell><cell>PSNR [dB]↑</cell><cell>SSIM ↑</cell></row><row><cell>LDM</cell><cell>23.732 ± 1.264</cell><cell>0.986 ± 0.010</cell></row><row><cell>LDM-P</cell><cell>24.125 ± 1.072</cell><cell>0.987 ± 0.009</cell></row><row><cell>LDM-CT</cell><cell>25.348 ± 0.822</cell><cell>0.990 ± 0.006</cell></row><row><cell cols="3">LDM-P-CT 25.817 ± 0.675 0.992 ± 0.004</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of our uPETe with several state-of-the-art PET enhancement methods, in terms of PSNR and SSIM, where * denotes unsupervised method and † denotes fully-supervised method.</figDesc><table><row><cell>Method</cell><cell>PSNR [dB]↑</cell><cell>SSIM ↑</cell></row><row><cell>DIP  *  [3]</cell><cell>22.538 ± 2.136</cell><cell>0.981 ± 0.015</cell></row><row><cell cols="2">Noisier2Noise  *  [23] 22.932 ± 1.983</cell><cell>0.983 ± 0.014</cell></row><row><cell>LA-GAN  † [21]</cell><cell>23.351 ± 1.725</cell><cell>0.984 ± 0.012</cell></row><row><cell>MR-GDD  *  [17]</cell><cell>23.628 ± 1.655</cell><cell>0.985 ± 0.011</cell></row><row><cell>Trans-GAN  † [13]</cell><cell>23.852 ± 1.522</cell><cell>0.985 ± 0.009</cell></row><row><cell>Noise2Void  *  [20]</cell><cell>24.263 ± 1.351</cell><cell>0.987 ± 0.009</cell></row><row><cell>DF-GAN  † [9]</cell><cell>24.821 ± 0.975</cell><cell>0.989 ± 0.007</cell></row><row><cell>AR-GAN  † [14]</cell><cell>25.217 ± 0.853</cell><cell>0.990 ± 0.006</cell></row><row><cell>uPETe  *</cell><cell cols="2">25.817 ± 0.675 0.992 ± 0.004</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62131015</rs>), <rs type="funder">Science and Technology Commission of Shanghai Municipality (STCSM)</rs> (No. <rs type="grantNumber">21010502600</rs>), The <rs type="funder">Key R&amp;D Program of Guangdong Province, China</rs> (No. <rs type="grantNumber">2021B0101420006</rs>), and the <rs type="funder">China Postdoctoral Science Foundation</rs> (Nos. <rs type="grantNumber">BX2021333</rs>, <rs type="grantNumber">2021M703340</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8BJHNrC">
					<idno type="grant-number">62131015</idno>
				</org>
				<org type="funding" xml:id="_9cp4V6V">
					<idno type="grant-number">21010502600</idno>
				</org>
				<org type="funding" xml:id="_2dePNRJ">
					<idno type="grant-number">2021B0101420006</idno>
				</org>
				<org type="funding" xml:id="_5x6N4rq">
					<idno type="grant-number">BX2021333</idno>
				</org>
				<org type="funding" xml:id="_B598h5V">
					<idno type="grant-number">2021M703340</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00625</idno>
		<title level="m">Med3D: transfer learning for 3D medical image analysis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PET image denoising using unsupervised deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2780" to="2789" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image denoising with blockmatching and 3D filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Process. Algorithms Syst. Neural Netw. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">6064</biblScope>
			<biblScope unit="page" from="354" to="365" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Suitability of bilateral filtering for edge-preserving noise reduction in PET</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hofheinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EJNMMI Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised standard-dose PET image generation via region-adaptive normalization and structural consistency constraint</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reconstruction of standard-dose PET from low-dose PET via dual-frequency supervision and global aggregation module</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Medical diffusion-denoising diffusion probabilistic models for 3D medical image generation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khader</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.03364</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An investigation of quantitative accuracy for deep learning based denoising in oncological PET</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">165019</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Two-stage self-supervised cycle-consistency network for reconstruction of thin-slice MR images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15395</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D transformer-GAN for high-quality PET reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_27" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive rectification based adversarial network with spectrum constraint for high-quality PET image synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102335</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46466-4_5" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9910</biblScope>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention U-Net: learning where to look for the pancreas</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anatomical-guided attention enhances unsupervised PET image denoising performance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Onishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">102226</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The ALARA concept in pediatric CT: myth or reality?</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Slovis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">223</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="6" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise2Void: unsupervised denoising of PET images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">214002</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">auto-context-based locality adaptive multi-modality GANs for PET synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3D</biblScope>
			<biblScope unit="page" from="1328" to="1339" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MRI-guided brain PET image filtering and partial volume correction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Townsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">961</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised PET denoising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucl. Med. Mol. Imaging</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="299" to="304" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Total-body dynamic reconstruction and parametric imaging on the uEXPLORER</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nucl. Med</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="291" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
