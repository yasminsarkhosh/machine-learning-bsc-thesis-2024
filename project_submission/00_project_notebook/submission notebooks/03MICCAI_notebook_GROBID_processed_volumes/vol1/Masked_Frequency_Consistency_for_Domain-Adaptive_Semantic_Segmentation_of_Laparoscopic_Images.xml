<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images</title>
				<funder>
					<orgName type="full">MEXT</orgName>
				</funder>
				<funder ref="#_XEmNyxu">
					<orgName type="full">JST CREST</orgName>
				</funder>
				<funder ref="#_x4yeSsS #_vZGTfjU #_8PtWzkF">
					<orgName type="full">JSPS KAKENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinkai</forename><surname>Zhao</surname></persName>
							<email>xkzhao@mori.m.is.nagoya-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuichiro</forename><surname>Hayashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Masahiro</forename><surname>Oda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Information Strategy Office, Information and Communications</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Takayuki</forename><surname>Kitasaka</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">Aichi Institute of Technology</orgName>
								<address>
									<settlement>Toyota</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kensaku</forename><surname>Mori</surname></persName>
							<email>kensaku@is.nagoya-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Information Technology Center</orgName>
								<orgName type="institution">Nagoya University</orgName>
								<address>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Research Center for Medical Bigdata</orgName>
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="663" to="673"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3939107305261B25818CF8AC1DC3BD28</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_63</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Domain Adaptation</term>
					<term>Laparoscopic Image</term>
					<term>Semantic Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation of laparoscopic images is an important issue for intraoperative guidance in laparoscopic surgery. However, acquiring and annotating laparoscopic datasets is labor-intensive, which limits the research on this topic. In this paper, we tackle the Domain-Adaptive Semantic Segmentation (DASS) task, which aims to train a segmentation network using only computer-generated simulated images and unlabeled real images. To bridge the large domain gap between generated and real images, we propose a Masked Frequency Consistency (MFC) module that encourages the network to learn frequency-related information of the target domain as additional cues for robust recognition. Specifically, MFC randomly masks some high-frequency information of the image to improve the consistency of the network's predictions for low-frequency images and real images. We conduct extensive experiments on existing DASS frameworks with our MFC module and show performance improvements. Our approach achieves comparable results to fully supervised learning method on the CholecSeg8K dataset without using any manual annotation. The code is available at github.com/MoriLabNU/MFC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Laparoscopic surgery is a minimally invasive surgical technique in which a camera and surgical instruments are inserted through a series of small skin punctures. During this procedure, the surgeon relies heavily on a screen to visualize the surgical site, which can be a serious challenge. An inaccurate interpretation of abdominal anatomy can result in serious injury to the patient's bile ducts <ref type="bibr" target="#b24">[25]</ref>. Therefore, deploying neural networks to accurately identify anatomical structures during laparoscopic surgery can markedly enhance both the quality and safety of the procedure <ref type="bibr" target="#b14">[15]</ref>. Despite the marked achievements of deep neural networks in various medical computer vision tasks, the training of supervised models necessitates a substantial volume of precisely annotated images. Because the acquisition of large, high-quality datasets of laparoscopic images is laborintensive and requires expert knowledge, the size and quality of publicly available datasets limit current research on semantic segmentation of laparoscopic images <ref type="bibr" target="#b20">[21]</ref>. To overcome these limitations, several active learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref> and domain generalization <ref type="bibr" target="#b13">[14]</ref> methods have been developed to minimize the manual annotation required for network training. We take a step further and employ unsupervised domain adaptation (UDA) to eliminate the dependence on manual annotation.</p><p>The aim of UDA is to train a model on a labeled source and an unlabeled target domain for enhanced target domain performance. Various UDA methods exist, but we concentrate on two types that are relevant to our approach: self-training-based and Fourier transform-based. Self-training-based approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref> apply different data augmentations, multiple models or domain mixtures to the images and gauge the consistency regularization between them. On the other hand, Fourier transform-based approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref> exchange the low frequency components across domains to transform source domain images into target domain ones. While UDA has been extensively investigated in the medical field, the majority of existing research has focused only on domain migration between datasets from different sources <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> or segmentation of some distinct categories (e.g. instrument <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>). In contrast, we utilize computersimulated images and unlabeled laparoscopic images to train a semantic segmentation network for laparoscopic images, which is more demanding and practical, as it deals with a more severe domain shift.</p><p>We propose a novel module for the UDA task in laparoscopic semantic segmentation, aiming to promote the network's exploration of consistency regularization between high-frequency and low-frequency images. Our approach is motivated by the observation in Fig. <ref type="figure" target="#fig_0">1</ref>(a) that computer-generated images lack the high-frequency details present in real images. For example, the computergenerated abdominal wall appears smooth, whereas the real abdominal wall has rich textural information. Inspired by the effectiveness of masked image models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>, we randomly mask the high frequency information of the image in the frequency domain and train the network to predict the semantic segmentation result of the image lacking high frequency information. We use the pseudo-label generated by the exponential moving average (EMA) teacher network for supervision. During training, the masking of high frequency regions allows the network to discover a shared latent space for both high and low frequency images. Consequently, real laparoscopic images with high-frequency information and computergenerated images with low-frequency information can share the same feature space, facilitating the transfer of knowledge learned in the generated images to the real images, as illustrated in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. This paper's primary contributions are as follows: (1) We creatively address the domain-adaptive semantic segmentation task for laparoscopic images, which involves training the model with both unlabeled real images and computergenerated simulated images. (2) To bridge the severe domain shift between generated and real images, we propose a novel masking frequency consistency (MFC) module to reduce the domain gap. MFC encourages the network to learn shared features between high-frequency and low-frequency images. To our knowledge, MFC is the first UDA method that applies masking strategy on frequency domain. <ref type="bibr" target="#b2">(3)</ref> We collect a vast number of image frames from public datasets and train them with computer-generated images. We evaluate our method, demonstrating that our MFC approach not only outperforms existing state-of-the-art UDA methods but also achieves performance comparable to fully supervised methods without the need for any manual annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>This paper addresses the task of domain-adaptive semantic segmentation of laparoscopic images. Suppose we have a source domain of computer-generated simulated laparoscopic images, consisting of N images</p><formula xml:id="formula_0">X S = {X S i | i = 1, 2, . . . , N} with corresponding pixel-level annotations Y S = {Y S i | i = 1, 2, . . . ,</formula><p>N} , and a target domain of M real laparoscopic images X T = {X T j | j = 1, 2, . . . , M} without annotations. Our objective is to train a network f with robust semantic segmentation capability on the unlabeled target domain.</p><p>To achieve this, we introduce a masked frequency consistency module for selflearning on the unlabeled target domain images X T i , while supervised loss is used for training on the labeled source domain images X S i . Our approach can integrate with different networks, effectively bridging the domain gap that occurs when applying networks to the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2.</head><p>The overview of our proposed Masked Frequency Consistency (MFC) module, which can be seamlessly integrated with different UDA methods and backbone networks. The MFC module works by augmenting the input image in the frequency domain using a mask, and then using a teacher-student structure to take both the original and the augmented image as inputs. A consistency loss is applied to facilitate the bridging of domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Frequency Representation</head><p>Considering an RGB image, X ∈ R H×W ×3 , we can generate its frequency representation map by applying the 2D Discrete Fourier Transform F for each channel c ∈ {0, 1, 2}, independently:</p><formula xml:id="formula_1">F (X) (u,v,c) = H-1 h=0 W -1 w=0 X (h,w,c) e -i2π( uh H + vw W ) , with i 2 = -1 (1)</formula><p>where (u, v) and (h, w) donate the coordinates in frequency map and image.</p><p>To facilitate subsequent operations, we rearrange the FFT data so that negative frequency terms precede positive ones, thereby centering the low frequency information. Furthermore, the inverse Fourier transform (iFFT) F -1 is utilized to transform the spectral signals back into the original image space:</p><formula xml:id="formula_2">X (h,w,c) = 1 HW H-1 u=0 W -1 v=0 F(X) (u,v,c) e i2π( uh H + vw W ) , with i 2 = -1 (2)</formula><p>Computation of both the Fourier transform and its corresponding inverse is achieved through the Fast Fourier Transform (FFT) algorithm <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Masking Strategy</head><p>As illustrated in Fig. <ref type="figure">2</ref>, MFC module perturbs the frequency information for target domain images. To do this, we define a mask M ∈ {0, 1}</p><p>H×W that randomly erases parts of the frequency map, thereby reducing the frequency data. Specifically, a patch mask M is randomly sampled as follows:</p><formula xml:id="formula_3">M mb+1:(m+1)b, nb+1:(n+1)b = [v &gt; r], with v ∼ U(0, 1)<label>(3)</label></formula><p>where [•] is the Iverson bracket, U(0, 1) the uniform distribution, b is the patch size, r represents the mask ratio, m and n are the patch indices. After this procedure, the patches in the mask are randomly masked. However, using the random patch mask alone may result in the loss of all low frequency information, which would exacerbate the domain gap and lead to training instability. To avoid this, we set the central elements to 1, thus preserving the low frequency information from the images as:</p><formula xml:id="formula_4">M H/2-h:H/2+h, W/2-w:W/2+w = 1,<label>(4)</label></formula><p>where h and w denote the size of the low-frequency information to be preserved. We utilize the mask M to apply masking in the frequency domain and use the iFFT F -1 to transform the image back into the original spatial domain as the network input. The enhanced image can then be obtained as:</p><formula xml:id="formula_5">X m = F -1 (F (X) M) , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where is the Hadamard product between the matrices. Moreover, with conjugate symmetry's disruption inhibiting the imaginary component's cancellation, we employ complex number magnitudes as outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Consistency Regularization</head><p>Consistency regularization is employed to extract common representations between high and low frequency images, thereby enhancing the generality of the network. Specifically, during training, the student segmentation network f S takes the enhanced image X m as input, whereas the original image X serves as the input for the teacher network f T . The weight θ T of the teacher network undergoes updates using the exponential moving average (EMA) <ref type="bibr" target="#b21">[22]</ref> of the weight θ S belonging to the student network:</p><formula xml:id="formula_7">θ T = αθ S + (1 -α)θ T ,<label>(6)</label></formula><p>where θ T representing the weight from the previous training step. The EMA teacher generates a series of stable pseudo-labels over time, a tactic frequently utilized in both semi-supervised learning <ref type="bibr" target="#b21">[22]</ref> and UDA <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>To evaluate the prediction results, we employ the mean squared error (MSE) as as our loss function, which quantifies the divergence between the predictions: <ref type="bibr" target="#b6">(7)</ref> where q T denotes the quality weight. Due to potential inaccuracies in pseudolabeling, like prior works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref>, we only use confident pixels surpassing the maximum probability threshold τ , defined as:</p><formula xml:id="formula_8">L C = q T MSE(fT (X ), fS(X m)),</formula><formula xml:id="formula_9">q T = H-1 h=0 W -1 w=0 [max c f T (X) (hwc) &gt; τ] H • W . (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Implementation</head><p>Datasets. Our experiments were conducted on three laparoscopic datasets, described as follows: (1) Simulation dataset <ref type="bibr" target="#b16">[17]</ref> consists of 20,000 labeled images of 3D scenes assembled from CT data, with 6 categories. In addition, I2I <ref type="bibr" target="#b16">[17]</ref> used generative adversarial networks (GAN) to translate these images into five different styles of realistic laparoscopic images, resulting in a total of 100,000 images. We used these two types of images (simulated and translated) as the source domain datasets with annotations. (2) CholecSeg8k <ref type="bibr" target="#b5">[6]</ref> is a semantic segmentation dataset containing laparoscopic cholecystectomy images from 17 video clips of the Cholec80 dataset <ref type="bibr" target="#b23">[24]</ref>, labeled with 13 categories. We used these 17 video clips as a test set. (3) The Cholec80 dataset <ref type="bibr" target="#b23">[24]</ref> consists of 80 videos of cholecystectomy procedures with annotations for phase and instrument presence, but no annotations related to segmentation. To train the UDA model, we selected 6819 images from 63 surgical videos, excluding the 17 videos used in CholecSeg8k <ref type="bibr" target="#b5">[6]</ref>. Specifically, for the video of the preparation phase of surgery, we extracted one frame per second as an unlabeled target domain image. A more detailed description is available in supplementary material.</p><p>Dataset Partitioning. Our experiments were performed with two different settings: (1) simulated images to real images and (2) translated images to real images. Considering the 6 categories present in the simulated dataset and the 13 categories in the CholecSeg8k dataset, we performed semantic segmentation on the following 6 categories: Background (BG), Abdominal Wall (AW), Liver, Fat, Gallbladder (GB), and Instruments (INST).</p><p>Implementation. We used the mmsegmentation <ref type="bibr" target="#b3">[4]</ref> codebase and trained each model on a single NVIDIA Tesla V100 GPU. We evaluated the Segformer <ref type="bibr" target="#b25">[26]</ref> and DeepLabV2 <ref type="bibr" target="#b2">[3]</ref> backbone networks, based on HRDA strategy <ref type="bibr" target="#b7">[8]</ref>, and initialized all backbone networks with pre-training on ImageNet. Training was performed using AdamW <ref type="bibr" target="#b12">[13]</ref>, with hyper-parameters taken from previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>. In all experiments, we trained the models on a batch of randomly cropped 256px × 256px images for 40k iterations, and the batch size is set to 4. As indicated in the ablation studies presented in the supplementary material, the optimal hyper-parameters for the MFC method vary according to the type of input datasets. Therefore, in all subsequent experiments, we set r to 0.7, b to 32, and h and w to 8, without further optimization of these hyper-parameters.</p><p>State-of-the-Art Methods. We benchmarked our method against contemporary leading approaches, which include UDA methods (DAFormer <ref type="bibr" target="#b6">[7]</ref>, HRDA <ref type="bibr" target="#b7">[8]</ref>, and MIC <ref type="bibr" target="#b8">[9]</ref>), image translation method <ref type="bibr" target="#b16">[17]</ref>, and fully supervised network on the CholecSeg8k dataset using cross-validation. For equitable comparison, all methods employed the same segmentation network, initialization, and optimizer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Qualitative Evaluation</head><p>In Fig. <ref type="figure" target="#fig_1">3</ref>, we present visualizations of the proposed MFC method and other compared methods on the CholecSeg8k dataset. All methods used only the simulated dataset for training, without any manual annotation. Our proposed method has two major advantages: (1) it effectively performs surgical instrument segmentation with minimal interference from reflections and shadows, and (2) it achieves a more accurate distinction between the boundaries of gallbladder and fat. However, we have found that our method exhibits imprecision in distinguishing the liver from the abdominal wall in certain cases.  <ref type="table" target="#tab_0">1</ref>. MFC, trained on the simulated data as the source domain, outperform the existing SOTA methods in all categories except for the abdominal wall and liver. Notably, our method significantly improves surgical instrument segmentation. This improvement can be attributed to the fact that our method excludes the disturbing high frequency noise such as reflections and shadows, which are absent in the source domain dataset. Such results indicate that our approach effectively bridges the domain gap between the generated and real images by randomly masking high frequency information. Furthermore, DeepLabV2-based methods underperform SegFormer-based methods in this setting.</p><p>To verify the effectiveness of the patch mask outlined in Eq. ( <ref type="formula" target="#formula_3">3</ref>) and the low frequency mask in Eq. ( <ref type="formula" target="#formula_4">4</ref>), we also conduct ablation experiments. The results of variants of our method are presented in Table <ref type="table" target="#tab_0">1</ref>. It is observed that the adoption of either of the two masking strategies enhances the segmentation performance.</p><p>Translated Images → Real Images. Furthermore, we assessed the performance of various methods using translated images as the source domain, with results summarized in Table <ref type="table" target="#tab_1">2</ref>. These translated images reduced the domain disparity with real images, thereby boosting the performance of UDA methods on the target domain. To gauge the efficacy of our proposed module, we incorporated MFC into two different network backbones. The results show that our approach efficiently improves the mIoU performance of segmentation across different source domain datasets and network backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper tackles the crucial issue of laparoscopic image segmentation, which is essential for surgical guidance and navigation. We propose a novel UDA module, called MFC, that leverages the consistency between high and low-frequency information in latent space. This consistency facilitates knowledge transfer from computer-simulated to real laparoscopic datasets for segmentation. Experimentally, MFC not only bolsters existing UDA models' performance but also outperforms leading methods, including fully supervised models that rely on annotated data. Our work unveils the potential of using computer-generated image data and UDA techniques for laparoscopic image segmentation. However, a limitation of our approach is that it does not account for the long-tail category distribution prevalent in real-world scenarios, such as venous vessels. Therefore, a future direction of our research is to extend our MFC module to handle rare category segmentation, thereby improving UDA models' generalization capabilities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Motivation for this work. (a) real images tend to contain more high frequency information than simulated images, and (b) our method aims to mitigate domain gaps by minimizing the discrepancy between high and low frequency information.</figDesc><graphic coords="2,80,25,53,93,293,23,85,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative results of different methods applied to the CholecSeg8k [6] dataset.</figDesc><graphic coords="7,83,10,275,72,250,15,152,86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The results, alongside a comparison with other SOTA UDA methods utilizing simulated images as the source domain. The highest scores are emphasized in bold black text.</figDesc><table><row><cell>Network</cell><cell>UDA Method</cell><cell>BG</cell><cell>AW</cell><cell>Liver Fat</cell><cell>GB</cell><cell cols="2">INST mIoU</cell></row><row><cell cols="2">SegFormer [26] Baseline</cell><cell cols="2">60.78 1.38</cell><cell cols="3">36.56 38.86 69.55 28.9</cell><cell>39.34</cell></row><row><cell></cell><cell>Supervised</cell><cell cols="6">98.01 81.44 84.24 84.63 74.78 79.62 83.78</cell></row><row><cell></cell><cell>I2I [17]</cell><cell cols="6">79.11 46.40 57.31 59.55 51.62 52.92 57.82</cell></row><row><cell></cell><cell>DAFormer [7]</cell><cell cols="6">96.55 25.35 55.45 78.59 52.83 45.27 59.01</cell></row><row><cell></cell><cell>HRDA [8]</cell><cell cols="6">97.80 44.04 63.23 78.90 55.39 55.28 65.77</cell></row><row><cell></cell><cell>MIC [9]</cell><cell cols="6">98.19 56.54 66.56 85.09 54.49 59.26 70.02</cell></row><row><cell></cell><cell>MFC(Ours)</cell><cell cols="6">98.30 43.48 60.40 85.87 62.21 82.12 72.06</cell></row><row><cell></cell><cell cols="7">MFC w/o Eq.(3) 97.83 38.02 58.29 83.48 54.90 72.81 67.55</cell></row><row><cell></cell><cell cols="7">MFC w/o Eq.(4) 98.11 34.35 56.53 85.81 61.10 83.37 69.88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The results, alongside a comparison with other SOTA UDA methods utilizing translated images as the source domain. The highest scores are emphasized in bold black text.</figDesc><table><row><cell>Network</cell><cell cols="2">UDA Method BG</cell><cell>AW</cell><cell>Liver Fat</cell><cell>GB</cell><cell>INST mIoU</cell></row><row><cell cols="7">DeepLabV2 [3] Baseline [17] 62.25 38.20 52.29 57.48 51.08 43.61 50.82</cell></row><row><cell></cell><cell>Supervised</cell><cell cols="5">97.04 72.45 85.64 68.79 58.72 82.42 77.51</cell></row><row><cell></cell><cell>HRDA [8]</cell><cell cols="5">96.71 76.12 75.70 74.99 65.97 69.78 76.54</cell></row><row><cell></cell><cell>MIC [9]</cell><cell cols="5">97.63 78.09 78.06 75.36 67.91 66.18 77.21</cell></row><row><cell></cell><cell>MFC(Ours)</cell><cell cols="5">97.02 80.46 78.51 75.13 67.11 70.98 78.20</cell></row><row><cell cols="7">SegFormer [26] Baseline [17] 79.11 46.40 57.31 59.55 51.62 52.92 57.82</cell></row><row><cell></cell><cell>Supervised</cell><cell cols="5">98.01 81.44 84.24 84.63 74.78 79.62 83.78</cell></row><row><cell></cell><cell>HRDA [8]</cell><cell cols="5">97.96 80.49 82.59 79.25 71.99 64.29 79.43</cell></row><row><cell></cell><cell>MIC [9]</cell><cell cols="5">98.66 84.47 84.85 78.08 75.94 69.54 81.92</cell></row><row><cell></cell><cell>MFC(Ours)</cell><cell cols="2">98.32 86</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.77 87.21 80.10 72.74 70.71 82.64 3.3 Quantitative Evaluation Simulated Images → Real Images.</head><label></label><figDesc>For quantitative evaluation, we employed the intersection over union (IoU) and its overall mean of 6 categories. A performance comparison of our method with other SOTA method is presented in Table</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported in part by the <rs type="funder">JSPS KAKENHI</rs> Grant Numbers <rs type="grantNumber">17H00867</rs>, <rs type="grantNumber">21K19898</rs>, <rs type="grantNumber">26108006</rs>; in part by the <rs type="funder">JST CREST</rs> Grant Number <rs type="grantNumber">JPMJCR20D5</rs>; and in part by the fellowship of the <rs type="programName">Nagoya University TMI WISE program</rs> from <rs type="funder">MEXT</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_x4yeSsS">
					<idno type="grant-number">17H00867</idno>
				</org>
				<org type="funding" xml:id="_vZGTfjU">
					<idno type="grant-number">21K19898</idno>
				</org>
				<org type="funding" xml:id="_8PtWzkF">
					<idno type="grant-number">26108006</idno>
				</org>
				<org type="funding" xml:id="_XEmNyxu">
					<idno type="grant-number">JPMJCR20D5</idno>
					<orgName type="program" subtype="full">Nagoya University TMI WISE program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 63.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ALGES: active learning with gradient embeddings for semantic segmentation of laparoscopic surgical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aklilu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning for Healthcare</title>
		<meeting>Machine Learning for Healthcare</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="892" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15384" to="15394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MMSegmentation: openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Shih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12453</idno>
		<title level="m">Cholec-Seg8k: a semantic segmentation dataset for laparoscopic cholecystectomy based on Cholec80</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DaFormer: improving network architectures and training strategies for domain-adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9924" to="9935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HRDA: context-aware high-resolution domainadaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20056-4_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20056-422" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13690</biblScope>
			<biblScope unit="page" from="372" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MIC: masked image consistency for context-enhanced domain adaptation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11721" to="11732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain specific convolution and high frequency reconstruction based unsupervised domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-162" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="650" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prototypical interaction graph for unsupervised domain adaptation in surgical instrument segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-426" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="272" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FedDG: federated domain generalization on medical image segmentation via episodic learning in continuous frequency space</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1013" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AADG: automatic augmentation for domain generalization on retinal image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3699" to="3711" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Artificial intelligence for intraoperative guidance: using semantic segmentation to identify surgical anatomy during laparoscopic cholecystectomy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">276</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="363" to="369" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The fast fourier transform. Fast Fourier Transform and Convolution Algorithms</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Nussbaumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Nussbaumer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="80" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating large labeled data sets for laparoscopic image processing tasks using unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32254-0_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32254-014" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11768</biblScope>
			<biblScope unit="page" from="119" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Class-wise confidence-aware active learning for laparoscopic images segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kitasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inter. J. Comput. Assisted Radiol. Surgery</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simulation-to-real domain adaptation with teacher-student learning for endoscopic instrument segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="849" to="859" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Endo-Sim2Real: consistency learning-based domain adaptation for instrument segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Strömsdörfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_75</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-075" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="784" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysis of current deep learning networks for semantic segmentation of anatomical structures in laparoscopic surgery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3502" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DACS: domain adaptation via cross-domain mixed sampling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causes and prevention of laparoscopic bile duct injuries: analysis of 252 cases from a human factors and cognitive psychology perspective</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="460" to="469" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SegFormer: simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12077" to="12090" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FDA: fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feather-light fourier domain adaptation in magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">I</forename><surname>Zakazov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shaposhnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bespalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16852-9_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16852-99" />
	</analytic>
	<monogr>
		<title level="m">DART 2022</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13542</biblScope>
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Context-aware mixup for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="804" to="817" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
