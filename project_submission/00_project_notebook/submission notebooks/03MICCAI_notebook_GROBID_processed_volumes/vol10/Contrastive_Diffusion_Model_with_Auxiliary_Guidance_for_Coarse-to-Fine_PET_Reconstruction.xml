<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction</title>
				<funder ref="#_JQ4tHrE">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_QFJh8De #_DzR6c8U #_672c4sw #_bvRgfuC">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zeyu</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luping</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Binyu</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiliu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Chengdu University of Information Technology</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<email>wangyanscu@hotmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dinggang.shen@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Research and Development</orgName>
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="239" to="249"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">AED7A458C7D6A3D9EFD2EDF862D2C03B</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_23</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Positron emission tomography (PET)</term>
					<term>PET reconstruction</term>
					<term>Diffusion probabilistic models</term>
					<term>Contrastive learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To obtain high-quality positron emission tomography (PET) scans while reducing radiation exposure to the human body, various approaches have been proposed to reconstruct standard-dose PET (SPET) images from low-dose PET (LPET) images. One widely adopted technique is the generative adversarial networks (GANs), yet recently, diffusion probabilistic models (DPMs) have emerged as a compelling alternative due to their improved sample quality and higher loglikelihood scores compared to GANs. Despite this, DPMs suffer from two major drawbacks in real clinical settings, i.e., the computationally expensive sampling process and the insufficient preservation of correspondence between the conditioning LPET image and the reconstructed PET (RPET) image. To address the above limitations, this paper presents a coarse-to-fine PET reconstruction framework that consists of a coarse prediction module (CPM) and an iterative refinement module (IRM). The CPM generates a coarse PET image via a deterministic process, and the IRM samples the residual iteratively. By delegating most of the computational overhead to the CPM, the overall sampling speed of our method can be significantly improved. Furthermore, two additional strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, are proposed and integrated into the reconstruction process, which can enhance the correspondence between the LPET image and the RPET image, further improving clinical reliability. Extensive experiments on two human brain PET datasets demonstrate that our Z. Han and Y. Wang-These authors contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Positron emission tomography (PET) is a widely-used molecular imaging technique that can help reveal the metabolic and biochemical functioning of body tissues. According to the dose level of injected radioactive tracer, PET images can be roughly classified as standard-(SPET) and low-dose PET (LPET) images. SPET images offer better image quality and more information in diagnosis compared to LPET images containing more noise and artifacts. However, the higher radiation exposure associated with SPET scanning poses potential health risks to the patient. Consequently, it is crucial to reconstruct SPET images from corresponding LPET images to produce clinically acceptable PET images.</p><p>In recent years, deep learning-based PET reconstruction approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13</ref>] have shown better performance than traditional methods. Particularly, generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8]</ref> have been widely adopted <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref> due to their capability to synthesize PET images with higher fidelity than regression-based models <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. For example, Kand et al. <ref type="bibr" target="#b10">[11]</ref> applied a Cycle-GAN model to transform amyloid PET images obtained with diverse radiotracers. Fei et al. <ref type="bibr" target="#b5">[6]</ref> made use of GANs to present a bidirectional contrastive framework for obtaining high-quality SPET images. Despite the promising achievement of GAN, its adversarial training is notoriously unstable <ref type="bibr" target="#b22">[22]</ref> and can lead to mode collapse <ref type="bibr" target="#b17">[17]</ref>, which may result in a low discriminability of the generated samples, reducing their confidence in clinical diagnosis.</p><p>Fortunately, likelihood-based generative models offer a new approach to address the limitations of GANs. These models learn the distribution's probability density function via maximum likelihood and could potentially cover broader data distributions of generated samples while being more stable to train. As an example, Cui et al. <ref type="bibr" target="#b2">[3]</ref> proposed a model based on Nouveau variational autoencoder for PET image denoising. Among likelihood-based generative models, diffusion probabilistic models (DPMs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">23]</ref> are noteworthy for their capacity to outperform GANs in various tasks <ref type="bibr" target="#b4">[5]</ref>, such as medical imaging <ref type="bibr" target="#b24">[24]</ref> and textto-image generation <ref type="bibr" target="#b20">[20]</ref>. DPMs consist of two stages: a forward process that gradually corrupts the given data and a reverse process that iteratively samples the original data from the noise. However, sampling from a diffusion model is computationally expensive and time-consuming <ref type="bibr" target="#b25">[25]</ref>, making it inconvenient for real clinical applications. Besides, existing conditional DPMs learn the inputoutput correspondence implicitly by adding a prior to the training objective, while this learned correspondence is prone to be lost in the reverse process <ref type="bibr" target="#b33">[33]</ref>, resulting in the RPET image missing crucial clinical information from the LPET image. Hence, the clinical reliability of the RPET image may be compromised. Motivated to address the above limitations, in this paper, we propose a coarse-to-fine PET reconstruction framework, including a coarse prediction module (CPM) and an iterative refinement module (IRM). The CPM generates a coarse prediction by invoking a deterministic prediction network only once, while the IRM, which is the reverse process of the DPMs, iteratively samples the residual between this coarse prediction and the corresponding SPET image. By combining the coarse prediction and the predicted residual, we can obtain RPET images much closer to the SPET images. To accelerate the sampling speed of IRM, we manage to delegate most of the computational overhead to the CPM <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">28]</ref>, hoping to narrow the gap between the coarse prediction and the SPET initially. Additionally, to enhance the correspondence between the LPET image and the generated RPET image, we propose an auxiliary guidance strategy at the input level based on the finding that auxiliary guidance can help to facilitate the reverse process of DPMs, and reinforce the consistency between the LPET image and RPET image by providing more LPET-relevant information to the model. Furthermore, at the output level, we suggest a contrastive diffusion strategy inspired by <ref type="bibr" target="#b33">[33]</ref> to explicitly distinguish between positive and negative PET slices. To conclude, the contributions of our method can be described as follows:</p><p>-We introduce a novel PET reconstruction framework based on DPMs, which, to the best of our knowledge, is the first work that applies DPMs to PET reconstruction. -To mitigate the computational overhead of DPMs, we employ a coarse-to-fine design that enhances the suitability of our framework for real-world clinical applications. -We propose two novel strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, to improve the correspondence between the LPET and RPET images and ensure that RPET images contain reliable clinical information.</p><p>2 Background: Diffusion Probabilistic Models Diffusion Probabilistic Models (DPMs): DPMs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">23]</ref> define a forward process, which corrupts a given image data x 0 ∼ q(x 0 ) step by step via a fixed Markov chain q(x t |x t-1 ) that gradually adds Gaussian noise to the data:</p><formula xml:id="formula_0">q(x t |x t-1 ) = N (x t ; √ α t x t-1 , (1 -α t )I), t = 1, 2, • • • , T,<label>(1)</label></formula><p>where α 1:T is the constant variance schedule that controls the amount of noise added at each time step, and q(x T ) ∼ N (x T ; 0, I) is the stationary distribution.</p><p>Owing to the Markov property, a data x t at an arbitrary time step t can be sampled in closed form:</p><formula xml:id="formula_1">q(x t |x 0 ) = N (x t ; √ γ t x 0 , (1 -γ t )I); x t = √ γ t x 0 + 1 -γ t , ∼ N (0, I), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where γ t = t i=1 α i . Furthermore, we can derive the posterior distribution of</p><formula xml:id="formula_3">x t-1 given (x 0 , x t ) as q(x t-1 |x 0 , x t ) = N (x t-1 ; μ(x 0 , x t ), σ 2 t I), where μ(x 0 , x t )</formula><p>and σ 2 t are subject to x 0 , x t and α 1:T . Based on this, we can leverage the reverse process from x T to x 0 to gradually denoise the latent variables by sampling from the posterior distribution q(x t-1 |x 0 , x t ). However, since x 0 is unknown during inference, we use a transition distribution p θ (x t-1 |x t ) := q(x t-1 |H θ (x t , t), x t ) to approximate q(x t-1 |x 0 , x t ), where H θ (x t , t) manages to reconstruct x 0 from x t and t, and it is trained by optimizing a variational lower bound of logp θ (x).</p><p>Conditional DPMs: Given an image x 0 with its corresponding condition c, conditional DPMs try to estimate p(x 0 |c). To achieve that, condition c is concatenated with x t <ref type="bibr" target="#b21">[21]</ref> as the input of H θ , denoted as H θ (c, x t , t).</p><p>Simplified Training Objective: Instead of training H θ to reconstruct the x 0 directly, we use an alternative parametrization D θ named denoising network <ref type="bibr" target="#b9">[10]</ref> trying to predict the noise vector ∼ N (0, I) added to x 0 in Eq. 2, and derive the following training objective:</p><formula xml:id="formula_4">L DP M = E (c,x0)∼ptrain E ∼N (0,I) E γ∼pγ D θ (c, √ γx 0 + 1 -γ , γ) -1 , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where the distribution p γ is the one used in WaveGrad <ref type="bibr" target="#b0">[1]</ref>. Note that we also leverage techniques from WaveGrad to let the denoising network D θ conditioned directly on the noise schedule γ rather than time step t, and this gives us more flexibility to control the inference steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our proposed framework (Fig. <ref type="figure" target="#fig_0">1</ref>(a)) has two modules, i.e., a coarse prediction module (CPM) and an iterative refinement module (IRM). The CPM predicts a coarse-denoised PET image from the LPET image, while the IRM models the residual between the coarse prediction and the SPET image iteratively. By combining the coarse prediction and residual, our framework can effectively generate high-quality RPET images. To improve the correspondence between the LPET image and the RPET image, we adopt an auxiliary guidance strategy (Fig. <ref type="figure" target="#fig_0">1(b)</ref>) at the input level and a contrastive diffusion strategy (Fig. <ref type="figure" target="#fig_0">1(c</ref>)) at the output level. The details of our method are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Coarse-to-Fine Framework</head><p>To simplify notation, we use a single conditioning variable c to represent the input required by both CPM and IRM, which includes the LPET image x lpet and the auxiliary guidance x aux . During inference, CPM first generates a coarse prediction x cp = P θ (c), where P θ is the deterministic prediction network in CPM. The IRM, which is the reverse process of DPM, then tries to sample the residual r 0 (i.e., x 0 in Sect. 2) between the coarse prediction x cp and the SPET image y via the following iterative process:</p><formula xml:id="formula_6">r t-1 ∼ p θ (r t-1 |r t , c), t = T, T -1, • • • , 1.<label>(4)</label></formula><p>Herein, the prime symbol above the variable indicates that it is sampled from the reverse process instead of the forward process. When t = 1, we can obtain the final sampled residual r 0 , and the RPET image y can be derived by r 0 + x cp .</p><p>In practice, both CPM and IRM use the same network architecture shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>). CPM generates the coarse prediction x cp by using P θ only once, but the denoising network D θ in IRM will be invoked multiple times during inference. Therefore, it is rational to delegate more computation overhead to P θ to obtain better initial results while keeping D θ small, since the reduction in computation cost in D θ will be accumulated by multiple times. To this end, we set the channel number in P θ much larger than that in the denoising network D θ . This leads to a larger network size for P θ compared to D θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Auxiliary Guidance Strategy</head><p>In this section, we will describe our auxiliary guidance strategy in depth which is proposed to enhance the reconstruction process at the input level by incorporating two auxiliary guidance, i.e., neighboring axial slices (NAS) and the spectrum. Our findings indicate that incorporating NAS provides insight into the spatial relationship between the current slice and its adjacent slices, while incorporating the spectrum imposes consistency in the frequency domain.</p><p>To effectively incorporate these two auxiliary guidances, as illustrated in Fig. <ref type="figure" target="#fig_0">1(c</ref>), we replace the ResBlock in the encoder with a Guided ResBlock as done in <ref type="bibr" target="#b19">[19]</ref>. During inference, the auxiliary guidance x aux is first downsampled by a factor of 2 k as x k aux , where k = 1, • • • , M, and M is the number of downsampling operations in the U-net encoder. Then x k aux is fed into a feature extractor F θ to generate its corresponding feature map f k aux = F θ (x k aux ), which is next injected into the Guided ResBlock matching its resolution through 1 × 1 convolution.</p><p>To empower the feature extractor to contain information of its high-quality counterpart y aux , we constrain it with L 1 loss through a convolution layer C θ (•):</p><formula xml:id="formula_7">L opt G = M k=1 C θ (F θ (x k aux )) -y k aux 1 ,<label>(5)</label></formula><p>where opt ∈{NAS, spectrum} denotes the kind of auxiliary guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contrastive Diffusion Strategy</head><p>In addition to the auxiliary guidance at the input level, we also develop a contrastive diffusion strategy at the output level to amplify the correspondence between the condition LPET image and the corresponding RPET image. In detail, we introduce a set of negative samples Neg = y 1 , y 2 , ..., y N , which consists of N SPET slices, each from a randomly selected subject that is not in the current batch for training. Then, for the noisy latent residual r t at time step t, we obtain its corresponding intermediate RPET y, and draw it close to the corresponding SPET y while pushing it far from the negative sample y i ∈ Neg.</p><p>Before this, we need to estimate the intermediate residual corresponding to r t firstly, denoted as r 0 . According to Sect. 2, the denoising network D θ manages to predict the Gaussian noise added to r 0 , enabling us to calculate r 0 directly from r t :</p><formula xml:id="formula_8">r 0 = r t -( √ 1 -γ t )D θ (c, r t , γ t ) √ γ t . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>Then r 0 is added to the coarse prediction x cp to obtain the intermediate RPET y = x cp + r 0 . Note that y is a one-step estimated result rather than the final RPET y . Herein, we define a generator p θ (y|r t , c) to represent the above process. Subsequently, the contrastive learning loss L CL is formulated as:</p><formula xml:id="formula_10">L CL = E q(y) [-log p θ (y|r t , c)] - y i ∈Neg E q(y i ) [-log p θ (y i |r t , c)]. (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>Intuitively, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>(b), the L CL aims to minimize the discrepancy between the training label y and the intermediate RPET y at each time step (first term), while simultaneously ensuring that y is distinguishable from the negative samples, i.e., the SPET images of other subjects (second term). The contrastive diffusion strategy extends contrastive learning to each time step, which allows LPET images to establish better associations with their corresponding RPET images at different denoising stages, thereby enhancing the mutual information between the LPET and RPET images as done in <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Loss</head><p>Following <ref type="bibr" target="#b28">[28]</ref>, we modify the objective L DP M in Eq. 3, and train CPM and IRM jointly by minimizing the following loss function:</p><formula xml:id="formula_12">L main = E (c,y)∼ptrain E ∼N (0,I) E γ∼pγ D θ (c, √ γ(y -P θ (c)) + 1 -γ , γ) -1 . (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>In summary, the final loss function is:</p><formula xml:id="formula_14">L total = L main + mL NAS G + nL spectrum G + kL CL ,<label>(9)</label></formula><p>where m, n and k are the hyper-parameters controlling the weights of each loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>The proposed method is implemented by the Pytorch framework using an NVIDIA GeForce RTX 3090 GPU with 24GB memory. The IRM in our framework is built upon the architecture of SR3 <ref type="bibr" target="#b21">[21]</ref>, a standard conditional DPM.</p><p>The number of downsampling operations M is 3, and the negative sample set number N is 10. 4 neighboring slices are used as the NAS guidance and the spectrums are obtained through discrete Fourier transform. As for the weights of each loss, we set m = n = 1, and k = 5e-5 following <ref type="bibr" target="#b33">[33]</ref>. We train our model for 500,000 iterations with a batch size of 4, using an Adam optimizer with a learning rate of 1e-4. The total diffusion steps T are 2,000 during training and 10 during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Datasets and Evaluation: We conducted most of our low-dose brain PET image reconstruction experiments on a public brain dataset, which is obtained from the Ultra-low Dose PET Imaging Challenge 2022 <ref type="bibr" target="#b3">[4]</ref>. Out of the 206 18F-FDG brain PET subjects acquired using a Siemens Biograph Vision Quadra, 170 were utilized for training and 36 for evaluation. Each subject has a resolution of 128 × 128 × 128, and 2D slices along the z-coordinate were used for training and evaluation. To simulate LPET images, we applied a dose reduction factor of 100 to each SPET image. To quantify the effectiveness of our method, we utilized three common evaluation metrics: the peak signal-to-noise (PSNR), structural similarity index (SSIM), and normalized mean squared error (NMSE). Additionally, we also used an in-house dataset, which was acquired on a Siemens Biograph mMR PET-MR system. This dataset contains PET brain images collected from 16 subjects, where 8 subjects are normal control (NC) and 8 subjects are mild cognitive impairment (MCI). To evaluate the generalizability of our method, all the experiments on this in-house dataset are conducted in a cross-dataset manner, i.e., training exclusively on the public dataset and inferring on the in-house dataset. Furthermore, we perform NC/MCI classification on this dataset as the clinical diagnosis experiment. Please refer to the supplementary materials for the experimental results on the in-house dataset.</p><p>Comparison with SOTA Methods: We compare the performance of our method with 6 SOTA methods, including DeepPET <ref type="bibr" target="#b8">[9]</ref> (regression-based method), Stack-GAN <ref type="bibr" target="#b27">[27]</ref>, Ea-GAN <ref type="bibr" target="#b31">[31]</ref>, AR-GAN <ref type="bibr" target="#b16">[16]</ref>, 3D CVT-GAN <ref type="bibr" target="#b32">[32]</ref> (GAN-based method) and NVAE <ref type="bibr" target="#b2">[3]</ref> (likelihood-based method) on the public  dataset. Since the IRM contains a stochastic process, we can also average multiple sampled (AMS) results to obtain a more stable reconstruction, which is denoted as Ours-AMS. Results are provided in Table <ref type="table" target="#tab_0">1</ref>. As can be seen, our method significantly outperforms all other methods in terms of PSNR, SSIM, and NMSE, and the performance can be further amplified by averaging multiple samples. Specifically, compared with the current SOTA method 3D CVT-GAN, our method (or ours-AMS) significantly boosts the performance by 0.558 dB (or 0.796 dB) in terms PSNR, 0.003 (or 0.004) in terms of SSIM, and 0.006 (or 0.007) in terms of NMSE. Moreover, 3D CVT-GAN uses 3D PET images as input. Since 3D PET images contain much more information than 2D PET images, our method has greater potential for improvement when using 3D PET images as input. Visualization results are illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. Columns from left to right show the SPET, LPET, and RPET results output by different methods.</p><p>Rows from top to bottom display the reconstructed results, zoom-in details, and error maps. As can be seen, our method generates the lowest error map while the details are well-preserved, consistent with the quantitative results.</p><p>Ablation Study: To thoroughly evaluate the impact of each component in our method, we perform an ablation study on the public dataset by breaking down our model into several submodels. We begin by training the SR3 model as our baseline (a). Then, we train a single CPM with an L2 loss (b), followed by the incorporation of the IRM to calculate the residual (c), and the addition of the auxiliary NAS guidance (d), the spectrum guidance (e), and the L CL loss term (f). Quantitative results are presented in Table <ref type="table" target="#tab_1">2</ref>. By comparing the results of (a) and (c), we observe that our coarse-to-fine design can significantly reduce the computational overhead of DPMs by decreasing MParam from 128.740 to 31.020 and BFLOPs from 5973 to 132, while achieving better results. The residual generated in (c) also helps to improve the result of the CPM in (b), leading to more accurate PET images. Moreover, our proposed auxiliary guidance strategy and contrastive learning strategy further improve the reconstruction quality, as seen by the increase in PSNR, SSIM, and NMSE scores from (d) to (f). Additionally, we calculate the standard deviation (SD) of the averaged multiple sampling results to measure the input-output correspondence. The standard deviation (SD) of (c) (6.16e-03) is smaller compared to (a) (3.78e-03). This is because a coarse RPET has been generated by the deterministic process. As such, the stochastic process IRM only needs to generate the residual, resulting in less output variability. Then, the SD continues to decrease (3.78e-03 to 2.49e-03) as we incorporate more components into the model, demonstrating the improved input-output correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a DPM-based PET reconstruction framework to reconstruct high-quality SPET images from LPET images. The coarse-to-fine design of our framework can significantly reduce the computational overhead of DPMs while achieving improved reconstruction results. Additionally, two strategies, i.e., the auxiliary guidance strategy and the contrastive diffusion strategy, are proposed to enhance the correspondence between the input and output, further improving clinical reliability. Extensive experiments on both public and private datasets demonstrate the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall architecture of our proposed framework.</figDesc><graphic coords="4,47,79,312,98,328,60,158,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparison with SOTA methods.</figDesc><graphic coords="8,56,31,227,87,311,32,101,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison results on the public dataset. *: We implemented this method ourselves as no official implementation was provided.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PSNR↑ SSIM↑ NMSE↓ MParam.</cell></row><row><cell cols="2">regression-based method DeepPET [9]</cell><cell>23.078 0.937 0.087</cell><cell>11.03</cell></row><row><cell>GAN-based method</cell><cell>Stack-GAN [27]</cell><cell>23.856 0.959 0.071</cell><cell>83.65</cell></row><row><cell></cell><cell>Ea-GAN [31]</cell><cell>24.096 0.962 0.064</cell><cell>41.83</cell></row><row><cell></cell><cell>AR-GAN [16]</cell><cell>24.313 0.961 0.055</cell><cell>43.27</cell></row><row><cell></cell><cell cols="2">3D CVT-GAN [32] 25.080 0.971 0.039</cell><cell>28.72</cell></row><row><cell cols="2">likelihood-based method *NVAE [3]</cell><cell>23.629 0.956 0.064</cell><cell>58.24</cell></row><row><cell></cell><cell>Ours</cell><cell>25.638 0.974 0.033</cell><cell>34.10</cell></row><row><cell></cell><cell>Ours-AMS</cell><cell>25.876 0.975 0.032</cell><cell>34.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of the ablation study on the public dataset.</figDesc><table><row><cell></cell><cell>Single Sampling</cell><cell></cell><cell></cell><cell cols="4">Averaged Multiple Sampling</cell></row><row><cell></cell><cell cols="7">PSNR↑ SSIM↑ NMSE↓ MParam. BFLOPs PSNR↑ SSIM↑ NMSE↓ SD</cell></row><row><cell cols="2">(a) baseline 23.302 0.962 0.058</cell><cell>128.740</cell><cell>5973</cell><cell cols="3">23.850 0.968 0.052</cell><cell>6.16e-3</cell></row><row><cell>(b) CPM</cell><cell>24.354 0.963 0.049</cell><cell>24.740</cell><cell>38</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">(c) +IRM 24.015 0.966 0.044</cell><cell>31.020</cell><cell>132</cell><cell cols="3">24.339 0.967 0.041</cell><cell>3.78e-3</cell></row><row><cell cols="2">(d) +NAS 24.668 0.969 0.046</cell><cell>33.040</cell><cell>140</cell><cell cols="3">24.752 0.970 0.044</cell><cell>3.41e-3</cell></row><row><cell>(e) +spec</cell><cell>25.208 0.972 0.044</cell><cell>34.100</cell><cell>145</cell><cell cols="3">25.376 0.973 0.043</cell><cell>3.30e-3</cell></row><row><cell>(f)+LCL</cell><cell>25.638 0.974 0.033</cell><cell>34.100</cell><cell>145</cell><cell cols="3">25.876 0.975 0.032</cell><cell>2.49e-3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">NSFC 62371325</rs>, <rs type="grantNumber">62071314</rs>), <rs type="programName">Sichuan Science and Technology Program</rs> <rs type="grantNumber">2023YFG0263</rs>, <rs type="grantNumber">2023YFG0025</rs>, <rs type="grantNumber">2023NSFSC0497</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JQ4tHrE">
					<idno type="grant-number">NSFC 62371325</idno>
				</org>
				<org type="funding" xml:id="_QFJh8De">
					<idno type="grant-number">62071314</idno>
					<orgName type="program" subtype="full">Sichuan Science and Technology Program</orgName>
				</org>
				<org type="funding" xml:id="_DzR6c8U">
					<idno type="grant-number">2023YFG0263</idno>
				</org>
				<org type="funding" xml:id="_672c4sw">
					<idno type="grant-number">2023YFG0025</idno>
				</org>
				<org type="funding" xml:id="_bvRgfuC">
					<idno type="grant-number">2023NSFSC0497</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_23.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<title level="m">WaveGrad: estimating gradients for waveform generation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Come-closer-diffuse-faster: accelerating conditional diffusion models for inverse problems through stochastic contraction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12413" to="12422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pet denoising and uncertainty estimation based on NVAE model using quantile regression loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_17" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part IV</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="173" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MICCAI challenges: Ultra-low dose pet imaging challenge</title>
		<idno type="DOI">10.5281/zenodo.6361846</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6361846" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classification-aided high-quality pet image synthesis via bidirectional contrastive GAN with shared information maximization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_50" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VI</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="527" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pet image denoising using a deep neural network through fine tuning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeepPET: a deep encoder-decoder network for directly solving the pet image reconstruction inverse problem</title>
		<author>
			<persName><forename type="first">I</forename><surname>Häggström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Schmidtlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Campanella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="253" to="262" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Translating amyloid pet of different radiotracers by a deep generative model for interchangeability</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="page">117890</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Full-dose pet image estimation from low-dose pet image using deep learning: a pilot study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="773" to="778" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Penalized pet reconstruction using deep learning prior and local linear fitting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1478" to="1487" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Whole-body pet estimation from low count statistics using cycleconsistent generative adversarial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">215017</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D transformer-GAN for high-quality PET reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part VI</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_27" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive rectification based adversarial network with spectrum constraint for high-quality pet image synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102335</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ultra-low-dose pet reconstruction using generative adversarial network with feature matching and task-specific perceptual loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3555" to="3564" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Image deblurring with domain generalizable diffusion models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.01789</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image superresolution via iterative refinement</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="4713" to="4726" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Solving inverse problems in medical imaging with score-based generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08005</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient diffusion models for vision: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ulhaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pogrebna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09292</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3D conditional generative adversarial networks for high-quality pet image estimation at low dose</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="550" to="562" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">auto-context-based locality adaptive multi-modality GANs for pet synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3D</biblScope>
			<biblScope unit="page" from="1328" to="1339" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deblurring via stochastic refinement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16293" to="16303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep auto-context convolutional neural networks for standarddose pet image estimation from low-dose PET/MRI</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="406" to="416" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">200x low-dose pet reconstruction using deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04119</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">EA-GANs: edge-aware generative adversarial networks for cross-modality mr image synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bourgeat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1750" to="1762" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">D CVT-GAN: a 3D convolutional vision transformer-GAN for pet reconstruction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_49" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VI</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="516" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Discrete contrastive diffusion for cross-modal and conditional generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07771</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
