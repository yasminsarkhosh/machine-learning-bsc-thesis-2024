<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform</title>
				<funder ref="#_N993ug4">
					<orgName type="full">MSIT in South Korea</orgName>
				</funder>
				<funder ref="#_kKjJH5y">
					<orgName type="full">ITRC</orgName>
				</funder>
				<funder ref="#_vemMwRV #_y7HFT5w #_8rqRdcz #_XRAM8Dg #_t5VkhK3 #_f3tdfaW">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ellen</forename><forename type="middle">Jieun</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yechan</forename><surname>Hwang</surname></persName>
							<email>yechan99@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yubin</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taegeun</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<settlement>Pohang</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Mediwhale, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Geunyoung</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Mediwhale, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Won</forename><forename type="middle">Hwa</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="470" to="480"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3C1D4999D6A023ACC293F71422E798A7</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retina images are non-invasive and highly effective in the diagnosis of various diseases such as cardiovascular and ophthalmological diseases. Accurate diagnosis depends on the quality of the retina images, however, obtaining high-quality images can be challenging due to various factors, such as noise, artifacts, and eye movement. Methods for enhancing retina images are therefore in high demand for clinical purposes, yet the problem remains challenging as there is a natural trade-off between preserving anatomical details (e.g., vessels) and increasing overall image quality other than the content in it. Moreover, training an enhancement model often requires paired images that map low-quality images to high-quality images, which may not be available in practice. In this regime, we propose a novel Retina image Enhancement framework using Scattering Transform (REST). REST uses unpaired retina image sets and does not require prior knowledge of the degraded factors. The generator in REST enhances retina images by utilizing the Anatomy Preserving Branch (APB) and the Tone Transferring Branch (TTB) with different roles. Our model successfully enhances low-quality retina images demonstrating commendable results on two independent datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Microvasculature and neural tissue in the retina can be directly and noninvasively visualized in vivo <ref type="bibr" target="#b18">[19]</ref>, and retina images are used for the diagnosis of various diseases including cardiovascular diseases <ref type="bibr" target="#b17">[18]</ref>, Alzheimer's disease <ref type="bibr" target="#b22">[23]</ref>, and ophthalmological diseases such as glaucoma and cataract <ref type="bibr" target="#b8">[9]</ref>. Therefore, screening diseases using retina images has significant advantages over painful invasive methods such as blood tests or biopsies. However, the screening accuracy highly depends on the quality of the image <ref type="bibr" target="#b20">[21]</ref> which can be compromised by various factors such as noise, low contrast, uneven illumination, blurriness, camera model, and experience level of the clinician taking the image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>These issues make it difficult to discriminate important indicators and biomarkers, leading to the failure of early detection of diseases and proper intervention. Therefore, image enhancement of low-quality retina images is highly necessary. However, retina image enhancement is a challenging task due to several reasons. Intuitively, it requires paired low and high-quality retina images to learn to map low-quality images to their high-quality ones and such data are difficult to acquire in practice <ref type="bibr" target="#b0">[1]</ref>. Moreover, enhancing images according to signal-to-noise ratio might discard important anatomical structures, e.g., the shape of the optic disc, vessels, and disease-related features, that are critical for disease screening.</p><p>To deal with the issues above, a structure-preserving guided retina image filter (SGRIF) was proposed to enhance unpaired retina images with prior knowledge of clouding effect <ref type="bibr" target="#b3">[4]</ref>. While dealing with unpaired data, it is biased with de-clouding the artifact caused by the cataract. Several parametric methods were recently proposed including <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b23">[24]</ref> with simple CycleGAN <ref type="bibr" target="#b25">[26]</ref> structure which require authentic unpaired data only. Although they can map the tone of the input image such as color, contrast, and illuminance, to those of high-quality images, their ability to preserve crucial anatomical structures that are essential for accurate image examination <ref type="bibr" target="#b5">[6]</ref> remains limited. Later, ISE-CRET <ref type="bibr" target="#b4">[5]</ref> and PCENet (Pyramid Constraint Enhancement Network) <ref type="bibr" target="#b12">[13]</ref> were proposed, which utilize supervised learning with synthesized low-quality images and authentic high-quality image pairs. These models preserve detailed features effectively, however, generating degraded images with priors remains a laborintensive process, and trained models are specific to the dataset in question.</p><p>In this context, we propose an unpaired Retina image Enhancement with Scattering Transform (REST) which preserves the anatomical structure (e.g., vessels, optic disc, and cup) and maps the tone (e.g., color and illuminance) effectively by utilizing an unpaired dataset. Our model contains a genuine generator that includes two branches: the Anatomy Preserving Branch (APB) and the Tone Transferring Branch (TTB). The APB incorporates scattering transform, which effectively captures anatomic structures and the TTB employs a multilayer convolution to refine the tone of the image as that of high-quality images. Constructing a cyclic architecture <ref type="bibr" target="#b25">[26]</ref> with the proposed generators and discriminators for consistency regularization, the REST successfully learns how to enhance the low-quality retina images with the following contributions: 1) REST only uses authentic unpaired data and does not require any prior knowledge, 2) Successful preservation of anatomic structures is achieved with scattering transform, 3) REST is extensively evaluated qualitatively and quantitatively on two independent datasets. Experiments on UK Biobank and EyeQ <ref type="bibr" target="#b5">[6]</ref> datasets demonstrate that the REST can adequately enhance retina images by restoring dark and uncertain regions without compromising anatomical structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>The SGRIF proposed in <ref type="bibr" target="#b3">[4]</ref> is a non-parametric method that utilizes a filter with degradation equations describing clouding effects caused by cataracts on the retina image. SGRIF pose and tackles the enhancement problem as a dehazing problem in computer vision. Using the degradation equation, global structure transfer filter, and global edge-preserving smoothing filter, SGRIF de-clouds the retina image. Although this method does not require image pairs, its performance depends on prior functions for degradation which causes a lack of generalizability. Parametric methods, especially those based on deep learning, perform better at generalization by identifying and improving low-quality factors in input images with the parameters. Simple CycleGAN-based models, such as <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b23">[24]</ref>, use only the unpaired images and do not require prior knowledge but face challenges in preserving detailed information. To address this limitation, recent approaches synthesize low-quality images paired with authentic highquality images for supervised learning, as proposed in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. Although such methods preserve structural information, it is prone to being dataset-specific by the degrading model, and synthesizing low-quality images is time-consuming due to the difficulty in designing an accurate degradation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We propose a generative framework for unpaired retina image enhancement, which translates low-quality retina images X to high-quality images Y through two separate branches: APB and TTB. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, an input image is simultaneously fed into both branches with encoder-decoder structures, and their outputs are combined with a kernel to obtain an enhanced image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Anatomy Preserving Branch</head><p>In order to preserve fine anatomical details, such as the shape of an optic disc, an optic cup, and vessels, the model must accurately capture high-frequency components, i.e., edges. To extract these high-frequency components, we designed the APB, which employs a wavelet scattering transform over multiple encoding layers. Wavelet scattering transform extracts high-frequency factors and invariants with respect to translation and rotation <ref type="bibr" target="#b2">[3]</ref> by comprising wavelet filters Ψ with the set of frequency and phase indices Λ and J number of scales, a low pass filter Φ, and a modulus operator <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>. Given an image x as an input, the scattering transform consisting of one layer, results in 0-th and first-order scattering coefficients, S 0 J and S 1 J , as</p><formula xml:id="formula_0">SJ x = [S 0 J x, S 1 J x] = [x * Φ, |x * Ψ λ | * Φ], λ ∈ Λ (1)</formula><p>The 0-th order scattering coefficient S 0 J x is computed by convolving ( * ) x with Φ. The first-order coefficients set S 1 J x is obtained by convolving x with a set of wavelet filters Ψ λ and modulus operation followed by a low pass filter Φ. Since the wavelet filter sets Ψ λ have diverse frequencies and angles (i.e., Λ) within 2D image space as well as scales (i.e., J), the filters ensure the scattering transform to capture the structural information with respect to the frequency and direction <ref type="bibr" target="#b2">[3]</ref>. The design of APB with scattering transform can be seen in Fig. <ref type="figure" target="#fig_0">1 (top)</ref>. First, the input image, denoted as x, is processed through a sequence of N encoding operations E APB i , where i denotes the layer index as</p><formula xml:id="formula_1">e AP B i = ⎧ ⎨ ⎩ x, i = 0 E APB i (e APB i-1 ) = Ci SCi, 0 &lt; i &lt; N E APB i (e APB i-1 ) = Ci, i= N<label>(2)</label></formula><p>where,</p><formula xml:id="formula_2">Ci = e APB i-1 * k APB Ei , SCi = (S1e APB i-1 ) * k1×1.</formula><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>The input of each layer e APB i-1 undergoes both the scattering transform S 1 and convolution with a trainable kernel k 1×1 and k APB Ei , except for the last layer in the encoding process. The C i and SC i are concatenated ( ) to yield an input for a subsequent layer. In the final layer, the input undergoes only convolution with a kernel k APB Ei . After the completing N encoding processes, the decoding process, denoted as D APB i , commences. The final encoded feature map of the encoder, e APB N , undergoes N + 1 decoding processes. The decoding process is composed of a resizing R(•) and convolution with trainable kernel k APB Di as</p><formula xml:id="formula_4">d APB i = ⎧ ⎨ ⎩ D APB i (e APB i ) = R(e APB i ) * k APB Di , i = N D APB i (d APB i+1 , e APB i ) = R(d APB i+1 e APB i ) * k APB Di , 0 &lt; i &lt; N D APB i (d APB i+1 ) = d APB i+1 * k APB Di , i = 0.<label>(4)</label></formula><p>In the upscaling phase (0 &lt; i ≤ N ), a combination of resizing and convolution techniques is implemented instead of transpose convolution. This approach prevents the occurrence of checkerboard artifacts, which can arise from uneven overlap during the transpose convolution <ref type="bibr" target="#b13">[14]</ref>. The artifacts are particularly problematic in the APB since the APB deals with high-frequency features which are easily affected them. The use of a combination of resizing and convolution techniques prevents the occurrence of these artifacts and ultimately produces clearer images. More specifically, we utilized interpolation with a factor of 2 and convolution with the k APB Di kernel. Additionally, to preserve anatomical structures such as vessels and an optic disc of the input image, we introduce skip connections <ref type="bibr" target="#b19">[20]</ref> between the encoder and decoder. These connections involve the concatenation of the output of each encoder layer, e i (0 &lt; i &lt; N), with the corresponding decoder layer's input, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> (top).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tone Transferring Branch</head><p>As the quality of a retina image depends on the tone of the image such as color, illumination, and contrast in addition to anatomical structures <ref type="bibr" target="#b5">[6]</ref>, it is crucial to generate synthetic images that have a similar tone to high-quality images. To ensure that the synthetic images resemble the tone of high-quality images, we designed TTB, which consists of multiple convolutional layers with a U-Net <ref type="bibr" target="#b19">[20]</ref> architecture as in Fig. <ref type="figure" target="#fig_0">1</ref> (bottom). Similar to the APB, the input x is subject to a sequence of N encoding and decoding operations, denoted as E TTB i and D TTB i , respectively. For the encoding layers E TTB i , the input e TTB i-1 is convolved with the trainable kernel k TTB i as</p><formula xml:id="formula_5">e TTB i = x, i = 0 E TTB i (e TTB i-1 ) = e TTB i-1 * k TTB i , 0 &lt; i ≤ N.</formula><p>(</p><p>After the encoding process, the N sequence of decoding operations D TTB i commences with the layer i = N . In the first decoding layer, the output of the encoder e TTB N is up-scaled using the transposed convolution operator ( ). For subsequent layers (1 ≤ i &lt; N), the output of the previous decoding layer d TTB i+1 and the corresponding output of the encoding layer e TTB i are concatenated processed through the transposed convolution operator as</p><formula xml:id="formula_7">d TTB i = D TTB i (e TTB i ) = e TTB i k TTB i , i = N D TTB i (d TTB i+1 , e TTB i ) = (d TTB i+1 e TTB i ) k TTB i , 1 ≤ i &lt; N. (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>Along with our objective function containing GAN Loss <ref type="bibr" target="#b6">[7]</ref>, which will be illustrated in the next section, Sect. 3.3, the sequence of convolution and transposed convolution layers encourage the distribution of the generated image, including the tone of the image, to match that of the high-quality images <ref type="bibr" target="#b25">[26]</ref>. Moreover, skip connections that provide direct information from the encoder to the decoder constrain the branch from modifying the input image beyond recognition <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function for Unpaired Image Enhancement</head><p>The popular adversarial training architecture for unpaired image translation <ref type="bibr" target="#b25">[26]</ref> is devised to train an enhancement model G comprised of APB and TTB, from Sect. 3.1 and Sect. 3.2, for synthesizing high-quality images Y from low-quality and identity losses L G identity , L F identity <ref type="bibr" target="#b25">[26]</ref> as follows:</p><formula xml:id="formula_9">L(G, F, DX , DY ) = L G GAN + L F GAN + L cycle + L identity . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>To ensure that generated images look like genuine images, the GAN losses</p><formula xml:id="formula_11">L G GAN , L F GAN are defined as [7] L G GAN = Ey∈Y [||DY (y)|| 2 2 ] + Ex∈X [||1 -DY (G(x))|| 2 2 ], L F GAN = Ex∈X [||DX (x)|| 2 2 ] + Ey∈Y [||(1 -DX (F (y))|| 2 2 ].<label>(8)</label></formula><p>To preserve the contents of the input image, we aim to make F (G(x)) ≈ x and G(F (y)) ≈ y with the cycle consistency losses as <ref type="bibr" target="#b25">[26]</ref> L</p><formula xml:id="formula_12">G cycle = λc1Ex∈X [||F (G(x)) -x||1], L F cycle = λc2Ey∈Y [||G(F (y)) -y||1] (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where λ c1 and λ c2 are hyperparameters. To make sure that the generators avoid translation when there is no need, i.e., a high-quality image as an input to G should lead to a high-quality image without changes, identity losses are defined as <ref type="bibr" target="#b25">[26]</ref> L</p><formula xml:id="formula_14">G identity = λi1Ey∈Y [||G(y) -y||1], L F identity = λi2Ex∈X [||F (x) -x||1] (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>with the hyperparameters λ i1 and λ i2 . With the aforementioned losses, we aim to obtain generators G * , F * that minimize the loss function while discriminators D * X , D * Y that maximize it as</p><formula xml:id="formula_16">G * , F * D * X , D * Y = arg max D X ,D Y min G,F L(G, F, DX , DY ).<label>(11)</label></formula><p>After training, G * is utilized at the inference stage to enhance retina images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Experimental Setup</head><p>Datsets. From UKB, 2000 images, i.e., 1000 high-quality and 1000 low-quality images respectively, were randomly sampled and split into train and test sets by a ratio of 2:1. For the EyeQ, we utilized 23252 labeled images, i.e., 6434 lowquality images labeled 'usable' and 16818 high-quality images labeled 'good' <ref type="bibr" target="#b5">[6]</ref> and followed the data splitting protocol into the train, validation, and test sets as in <ref type="bibr" target="#b4">[5]</ref>. As ISECRET and PCENet require low-quality pairs and a mask for every image during training, degraded pairs were generated between high-quality ones and masks with the given degrading and masking methods in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>. Evaluation Setup. To quantitatively evaluate the quality of the generated retina images, Fundus Image Quality Assessment (FIQA) <ref type="bibr" target="#b5">[6]</ref> and Weighted FIQA (WFQA) <ref type="bibr" target="#b12">[13]</ref> scores were adopted. For FIQA, a Multiple Color-space Fusion Network (MCF-Net) <ref type="bibr" target="#b5">[6]</ref> which labels each input image of the retina as 'Good', 'Usable', or 'Reject' is utilized. With the output of the MCF-Net, the FIQA score is calculated. The FIQA is calculated as the ratio of the number of images labeled as 'Good' to the total number of inputs, while the WFQA is determined by the ratio of the weighted sum of the number of images labeled as 'Good' and 'Usable' to the total number of inputs, with weights of 2 and 1 assigned to 'Good' and 'Usable' respectively. For evaluation, the FIQA and WFQA scores were calculated by inputting output images from enhancement models to pretrained MCF-Net. For UKB data, both quantitative and qualitative evaluations were done for our model (REST) and four baseline models, i.e., two traditional unpaired image translation models, CycleGANs with ResNet and U-Net <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref> and two latest models designed for unpaired retina image enhancement, ISECRET <ref type="bibr" target="#b4">[5]</ref> and PCENet <ref type="bibr" target="#b12">[13]</ref>. For EyeQ data, experiments were done for REST and PCENet following the settings in <ref type="bibr" target="#b4">[5]</ref>, and results are compared with traditional paired image translation model cGAN <ref type="bibr" target="#b14">[15]</ref> and unpaired image translation models, CycleGAN <ref type="bibr" target="#b25">[26]</ref>, CutGAN <ref type="bibr" target="#b14">[15]</ref>, and ISECRET <ref type="bibr" target="#b4">[5]</ref>, reported in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Implmenemtation. The batch size was set to 4 and the Adam optimizer was adopted with the initial learning rate of 0.0002 linearly decaying. For scattering transform, a Morlet wavelet was used with eight different angles within 2D image space and J was set to 1. A 2D Gabor filter was used as a low-pass filter. Parameters in losses, i.e., λ c1 , λ c2 , λ i1 and λ i2 , were set to 10, 10, 5 and 5 respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Regarding the experiments on UKB, the result of the qualitative evaluation is illustrated in  <ref type="figure" target="#fig_2">3</ref>. In Fig. <ref type="figure" target="#fig_2">3</ref>, the low-quality input image and synthesized high-quality images by each model are shown sequentially. The second and third rows in Fig. <ref type="figure" target="#fig_2">3</ref> show the details of the image marked with a box in the first row.</p><p>As can be seen in Fig. <ref type="figure" target="#fig_2">3</ref>, REST preserved the anatomical structures such as vessels both in and out of the optic disc and the shape of the optic disc and cup better than the baselines. The overall tone of the image was also successfully transferred to make the contrast and the illuminance become even.</p><p>For the experiment on the EyeQ dataset, the result of the qualitative evaluation is illustrated in Table <ref type="table" target="#tab_0">1</ref> (right). REST achieved the second-best FIQA score slightly behind by only 0.001p from ISECRET. However, notice that ISECRET requires synthetic paired images while REST uses only unpaired images. As the low-quality images utilized in the EyeQ experiment, i.e., labeled as 'Usable', are already deemed to be of relatively high quality, the capability of REST that shows superior preservation of anatomical structure in contaminated images was not sufficiently manifested as in the UKB experiment. Still, the results are highly competitive and demonstrate potentials to be used for real applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In the study, we proposed a novel approach to unpaired retina image enhancement, i.e., REST. While using only the authentic unpaired images, the proposed method effectively preserves anatomical structures during the enhancement process. This is done by utilizing APB for preserving the details by the scattering transform and TTB for transferring the tone of the images. Notably, REST has demonstrated commendable performance on both two different datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of the generator in REST with APB (top) and TTB (bottom). Input images are fed into two branches and the two outputs are concatenated and convolved by 1 × 1 kernel to generate the output images. The symbols on the legend will be explained in Sect. 3.1 and Sect. 3.2.</figDesc><graphic coords="4,79,98,53,90,292,27,143,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall architecture of the REST is composed of cycles of two generators: G, F , and two discriminators: DX , DY . images X. As seen in Fig. 2, the overall architecture is composed of two generators G:X → Y and F :Y → X, and two discriminators D X and D Y which discriminate authentic images and synthetic images of low-quality and high-quality respectively. The two generators share the same architecture as Fig. 1 and the two discriminators are also identically modeled with multiple convolution layers. To successfully train the enhancement model G:X → Y , we composed our loss function L with GAN losses L G GAN , L F GAN , cycle consistency losses L G cycle , L F cycle</figDesc><graphic coords="6,77,46,54,08,297,85,80,62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of the low-quality input image and the outputs of different models. The zoomed-in figures from yellow and blue boxes (2nd and 3rd rows) show that our model REST better preserves the anatomical structures (vessels, optic disc, and cup) and transfers the tone. (Color figure online)</figDesc><graphic coords="8,55,98,165,68,340,30,166,69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>FIQA, WFQA, the number (#) of parameters, usage of paired data, and training samples (# of data) for UKB (left) and EyeQ (right) are compared.</figDesc><table><row><cell></cell><cell cols="3">Parameters Pair Samples</cell><cell>FIQA</cell><cell>WFQA</cell><cell></cell><cell cols="2">Pair FIQA</cell></row><row><cell>Real high</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.989</cell><cell>1.978</cell><cell>Real high</cell><cell>-</cell><cell>0.980</cell></row><row><cell>Real low</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.051</cell><cell>0.141</cell><cell>Real low</cell><cell>-</cell><cell>0.198</cell></row><row><cell>CycleGAN (ResNet) [7, 8] CycleGAN (U-Net) [7, 20] ISECRET [5]</cell><cell>28.29 M 114.35 M 15.36 M</cell><cell>--</cell><cell>1332 1332 1998</cell><cell>0.926 ± 0.04 0.893 ± 0.04 0.898 ± 0.02</cell><cell>1.853 ± 0.07 1.765 ± 0.09 1.765 ± 0.09</cell><cell>cGAN [10] CycleGAN [7]</cell><cell>--</cell><cell>0.304 0.890</cell></row><row><cell>PCENet [13]</cell><cell>26.65 M</cell><cell></cell><cell>1332</cell><cell>0.389 ± 0.06</cell><cell>0.782 ± 0.12</cell><cell>CutGAN [15]</cell><cell>-</cell><cell>0.919</cell></row><row><cell>REST (ours)</cell><cell>16.35 M</cell><cell>-</cell><cell>1332</cell><cell cols="2">0.940 ± 0.01 1.881 ± 0.03</cell><cell>ISECRET [5]</cell><cell></cell><cell>0.937</cell></row><row><cell>REST w/o APB (ours)</cell><cell>13.70 M</cell><cell>-</cell><cell>1332</cell><cell>0.920 ± 0.02</cell><cell>1.843 ± 0.54</cell><cell>PCENet [13]</cell><cell></cell><cell>0.894</cell></row><row><cell>REST w/o TTB (ours)</cell><cell>8.18 M</cell><cell>-</cell><cell>1332</cell><cell>0.223 ± 0.05</cell><cell>0.454± 0.09</cell><cell>REST (ours)</cell><cell>-</cell><cell>0.936</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 (</head><label>1</label><figDesc>left). In the Table1, (1) the number of parameters, (2) the requirement for the paired data while training, (3) the total number of images used for training, and (4) the FIQA score and WFQA score for each model are illustrated with the best score denoted in bold, and the second-best in underlined. REST achieved the best FIQA with the smallest standard deviation.</figDesc><table><row><cell>Compared with the second-best model, excluding the ablation study, cycleGAN</cell></row><row><cell>with Resnet backbone, the FIQA score of REST is 0.014 (1.4%p) higher with a</cell></row><row><cell>lot smaller number of total parameters. Qualitative evaluation on a UKB sample</cell></row><row><cell>is illustrated in Fig.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This research was supported by <rs type="grantNumber">NRF-2022R1A2C2092336</rs> (<rs type="grantNumber">50%</rs>), <rs type="grantNumber">IITP-2022-0-00290</rs> (<rs type="grantNumber">30%</rs>), <rs type="grantNumber">IITP-2019-0-01906</rs> (<rs type="programName">AI Graduate Program</rs> at <rs type="institution">POSTECH</rs>, <rs type="grantNumber">10%</rs>), and <rs type="grantNumber">IITP-2022-2020-0-01461</rs> (<rs type="funder">ITRC</rs>, <rs type="grantNumber">10%</rs>) funded by <rs type="funder">MSIT in South Korea</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vemMwRV">
					<idno type="grant-number">NRF-2022R1A2C2092336</idno>
				</org>
				<org type="funding" xml:id="_y7HFT5w">
					<idno type="grant-number">50%</idno>
				</org>
				<org type="funding" xml:id="_8rqRdcz">
					<idno type="grant-number">IITP-2022-0-00290</idno>
				</org>
				<org type="funding" xml:id="_XRAM8Dg">
					<idno type="grant-number">30%</idno>
				</org>
				<org type="funding" xml:id="_t5VkhK3">
					<idno type="grant-number">IITP-2019-0-01906</idno>
					<orgName type="program" subtype="full">AI Graduate Program</orgName>
				</org>
				<org type="funding" xml:id="_f3tdfaW">
					<idno type="grant-number">10%</idno>
				</org>
				<org type="funding" xml:id="_kKjJH5y">
					<idno type="grant-number">IITP-2022-2020-0-01461</idno>
				</org>
				<org type="funding" xml:id="_N993ug4">
					<idno type="grant-number">10%</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 45.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ntire 2021 nonhomogeneous dehazing challenge report</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Vasluianu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="627" to="646" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Kymatio: scattering transforms in python</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andreux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Angles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Exarchakisgeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2256" to="2261" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structure-preserving guided retinal image filtering and its application for optic disk analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2536" to="2546" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">I-SECRET: importance-guided fundus image enhancement via semi-supervised contrastive constraining</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-39" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation of retinal image quality assessment networks in different color-spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-76" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automated hemorrhage detection from coarsely annotated fundus images in diabetic retinopathy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1369" to="1372" />
		</imprint>
		<respStmt>
			<orgName>ISBI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On statistical analysis of neuroimages with imperfect registration</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="666" to="674" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structure-consistent restoration network for cataract fundus image enhancement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-747" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Degradation-invariant enhancement of fundus images via pyramid constraint network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-749" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-719" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A conditional generative adversarial network-based method for eye fundus image quality enhancement</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perdomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-63419-3_19</idno>
		<idno>978-3-030-63419-3 19</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">OMIA 2020</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Garvin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Macgillivray</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12069</biblScope>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fundus image quality assessment: survey, challenges, and future scope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Martini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1211" to="1224" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep-learning-based cardiovascular risk stratification using coronary artery calcium scores predicted from retinal photographs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Tham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Dig. Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="306" to="e316" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prediction of systemic biomarkers from retinal photographs: development and validation of deep-learning algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Tham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Dig. Health</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="526" to="e536" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identification of suitable fundus images using automated quality assessment methods</title>
		<author>
			<persName><forename type="first">U</forename><surname>Köse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erdöl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Opt</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="46006" to="046006" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling and enhancing low-quality retinal fundus images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="996" to="1006" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Retinal imaging in Alzheimer&apos;s and neurodegenerative diseases</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s Dementia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="111" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fundus image enhancement method based on cyclegan</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="4500" to="4503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data-driven enhancement of blurry retinal images via generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_9</idno>
		<idno>978-3-030-32239-7 9</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
