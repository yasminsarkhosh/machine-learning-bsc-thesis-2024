<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X-Ray to CT Rigid Registration Using Scene Coordinate Regression</title>
				<funder ref="#_FwyfbjK">
					<orgName type="full">JSPS KAK-ENHI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pragyan</forename><surname>Shrestha</surname></persName>
							<email>shrestha.pragyan@image.iit.tsukuba.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chun</forename><surname>Xie</surname></persName>
							<email>xiechun@ccs.tsukuba.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hidehiko</forename><surname>Shishido</surname></persName>
							<email>shishid@ccs.tsukuba.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuichi</forename><surname>Yoshii</surname></persName>
							<email>yyoshii@tokyo-med.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Tokyo Medical University</orgName>
								<address>
									<settlement>Ami</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Itaru</forename><surname>Kitahara</surname></persName>
							<email>kitahara@ccs.tsukuba.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tsukuba</orgName>
								<address>
									<settlement>Tsukuba</settlement>
									<region>Ibaraki</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">X-Ray to CT Rigid Registration Using Scene Coordinate Regression</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="781" to="790"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CE0B192803633170CEE78644FED4E669</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_74</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Registration</term>
					<term>X-Ray Image</term>
					<term>Scene Coordinates</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Intraoperative fluoroscopy is a frequently used modality in minimally invasive orthopedic surgeries. Aligning the intraoperatively acquired X-ray image with the preoperatively acquired 3D model of a computed tomography (CT) scan reduces the mental burden on surgeons induced by the overlapping anatomical structures in the acquired images. This paper proposes a fully automatic registration method that is robust to extreme viewpoints and does not require manual annotation of landmark points during training. It is based on a fully convolutional neural network (CNN) that regresses the scene coordinates for a given X-ray image. The scene coordinates are defined as the intersection of the back-projected rays from a pixel toward the 3D model. Training data for a patient-specific model were generated through a realistic simulation of a C-arm device using preoperative CT scans. In contrast, intraoperative registration was achieved by solving the perspective-n-point (PnP) problem with a random sample and consensus (RANSAC) algorithm. Experiments were conducted using a pelvic CT dataset that included several real fluoroscopic (X-ray) images with ground truth annotations. The proposed method achieved an average mean target registration error (mTRE) of 3.79+/1.67 mm in the 50 th percentile of the simulated test dataset and projected mTRE of 9.65+/-4.07 mm in the 50 th percentile of real fluoroscopic images for pelvis registration. The code is available at https://github.com/Pragyanstha/SCR-Registration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image-guided navigation plays a crucial role in modern surgical procedures. In the field of orthopedics, many surgical procedures such as total hip arthro-plasty, total knee arthroplasty, and pedicle screw injections utilize intraoperative fluoroscopy for surgical navigation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11]</ref>. Due to overlapping anatomical structures in X-ray images, it is often difficult to correctly identify and reason the 3D structure from solely the image. Therefore, registering an intraoperatively acquired X-ray image to the preoperatively acquired CT scan is crucial in performing such procedures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>. The standard procedure for acquiring highly accurate registration involves embedding fiducial markers into the patient and acquiring a preoperative CT scan to obtain 2D-3D correspondences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>. Inserting fiducial markers onto the body involves extra surgical costs and might not be viable for minimally invasive surgeries. To circumvent such issues with the feature-based method, an intensity-based optimization scheme for registration has been extensively studied <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>. Since the objective function is highly nonlinear for optimizing pose parameters, a good initialization is necessary for the method to converge in a global minimum. Therefore, it is usually accompanied by initial coarse registration using manual alignment of the 3D model to the image, interrupting the surgical flow. On the other hand, learningbased methods have proved to be efficient in solving the registration task. Existing learning-based methods can be broadly categorized into landmark estimation and direct pose regression. Landmark estimation methods aim to solve for pose using correspondences between 3D landmark annotations and its estimated 2D projection points <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>, while methods based on pose regression estimate the global camera pose in a single inference <ref type="bibr" target="#b11">[12]</ref>. Pose regressors are known to overfit training data and generalize poorly to unseen images <ref type="bibr" target="#b13">[14]</ref>. This makes the landmark estimation methods stand out in terms of registration quality and generalization. However, there exist two main issues with landmark estimation methods: 1) Annotation cost of a sufficiently large number of landmarks in the CT image. 2) Failure to solve for the pose in extreme views where projected landmarks are not visible or the number of visible landmarks is small. This paper addresses these issues by introducing scene coordinates <ref type="bibr" target="#b15">[16]</ref> to establish dense 2D-3D correspondences. Specifically, the proposed method regresses the scene coordinates of the CT-scan model from corresponding Xray images. A rigid transformation that aligns the CT-scan model to the image is then calculated by solving the Perspective-n-point (PnP) problem with the Random sample and consensus (RANSAC) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>The problem of 2D-3D registration can be formulated a finding the rigid transformation that transforms the 3D model defined in the anatomical or world coordinate system into the camera coordinate system. Specifically, given a CTscan volume V CT (x w ) where x w is defined in the world coordinate system, the registration problem is concerned with finding T c w = [R|t] such that the following holds. </p><formula xml:id="formula_0">I = {V CT (T c -1 w x c ); K}<label>(1)</label></formula><p>where {•} is the X-ray transform that can be applied to volumes in the camera coordinate system given an intrinsic matrix K and I, the target X-ray image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Registration</head><p>The proposed registration pipeline overview is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The proposed method comprises the following four parts: first, the scene coordinates were regressed using a single-view X-ray image as input to the U-Net model; second, the PnP + RANSAC algorithm is used to solve for the pose of the captured X-ray system; third, the CT-scan volume was segmented to obtain a 3D model of the bone regions; and fourth, the computed rigid transformation from world coordinates to camera coordinates is used to generate projection overlay images.</p><p>Scene Coordinates. The scene coordinates are defined as the points of intersection between the camera's back-projected rays and the 3D model in a world coordinate system (i.e., only the first intersection and the last intersections are considered). The same concept was adapted for X-ray images and their underlying 3D models obtained from the CT-scans. Specifically, given an arbitrary point x ij in the image plane, the scene coordinates X ij satisfy the following conditions.</p><formula xml:id="formula_1">X ij = [R T | -t](dK T x ij )<label>(2)</label></formula><p>where R and t are the rotation matrix and translation vector that maps points in the world coordinate system to the camera coordinate system, K is the intrinsic matrix, d is the depth, as seen from the camera, of the point X on the 3D model.</p><p>Uncertainty Estimation. The task of scene coordinate regression is to estimate these X ij for every pixel ij, given an X-ray image I. However, the existence of X ij is not guaranteed for all pixels because back-projected rays may not intersect the 3D model. One of the many ways to address such a case is to prepare a mask (i.e., 1 if the bone area, 0 otherwise) in advance so that only the pixels that lie inside the mask are estimated. As this approach requires an explicit method for estimating the mask image, an alternative approach was adopted in this study. Instead of estimating a single X ij , the mean and variance of the scene coordinates are estimated. The non-intersecting scene coordinates were identified by applying thresholding to the estimated variance (i.e., points with high variance were considered non-existent scene coordinates and were filtered out). This approach assumes that the observed scene coordinates are corrupted with a zero mean, non-zero and non-constant variance, and isotropic Gaussian noise.</p><formula xml:id="formula_2">X ij ∼ N (u(I, x ij ), σ(I, x ij ))<label>(3)</label></formula><p>where u(I, x ij ) and σ(I, x ij ) are the functions that produce the mean and standard deviation of the scene coordinates, respectively. This work represents these functions using a fully convolutional neural network.</p><p>Loss Function. A U-Net architecture was used to estimate the mean and standard deviation of the scene coordinates at every pixel in a given image. The loss function for the intersecting scene coordinates is derived from the maximum likelihood estimates using the likelihood X ij . This can be expressed as follows:</p><formula xml:id="formula_3">Loss intersecting = ( (X ij -u(I, x ij )) σ(I, x ij ) ) 2 + 2 log(σ(I, x ij )) (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>Because it is desirable to have a high variance for non-existent scene coordinates, the loss function for non-existent coordinates is designed as follows:</p><formula xml:id="formula_5">Loss non-existent = 1 σ(I, x ij )<label>(5)</label></formula><p>2D-3D Registration. An iterative PnP implementation from OpenCV was run using RANSAC with maximum iteration of 1000 and reprojection error of 10px and 20px for the simulated and real X-ray images respectively. An example of a successful registration is shown in the left part of Fig. <ref type="figure" target="#fig_1">2</ref> below. 3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>A dataset containing six annotated CT scans, each with several registered real X-ray images from <ref type="bibr" target="#b6">[7]</ref> was used to properly evaluate the proposed method. The annotations included 14 landmarks and 7 segmentation labels. The CT scans were of the pelvic bones of cadaveric specimens. Because there were only a few real X-ray images, simulated X-ray images were generated from each CT-scans to train and test the model. In particular, DeepDRR <ref type="bibr" target="#b2">[3]</ref> was used to simulate a Siemens Cios Fusion Mobile C-arm imaging device. Similar to <ref type="bibr" target="#b2">[3]</ref>, the left anterior oblique/right anterior oblique (LAO/RAO) views were sampled at angles of <ref type="bibr">[45,</ref><ref type="bibr">45]</ref> degrees with 1-degree intervals. A random offset was applied in each direction.</p><p>The offset vector was sampled from a normal distribution with a mean of zero and standard deviations of 90 mm in the lateral direction, and 30 mm in the other two directions. Images intentionally included partially visible structures.</p><p>A selection of randomly sampled images is displayed on the right side of Fig. <ref type="figure" target="#fig_1">2</ref>. Ground-truth scene coordinates for each image were obtained by rendering the depth map of the 3D models and converting them to 3D coordinates using Eq. 2.</p><p>In total, 8100 simulated X-rays were generated for each specimen. Among these, 5184 images were randomly assigned to the training set, 1296 to the validation set, and the remaining 1620 to the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The input image and output scene coordinates had a size of 512 × 512 pixels.</p><p>The U-Net model had eight output channels, which consisted of three channels for scene coordinates and one channel for standard deviation, multiplied by two for the entry and exit points. Patient-specific models were trained individually for each dataset. Adam optimizer with a constant learning rate of 0.0001 and a batch size of 16 was used. Online data augmentation which includes random inversion, color jitter, and random erasing was applied. The scene coordinates were filtered using a log variance threshold of 0.0 for simulated images and -2.0 for real X-ray images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines and Evaluation Metrics</head><p>The proposed method was compared with two other baseline methods: PoseNet <ref type="bibr" target="#b7">[8]</ref> and DFLNet <ref type="bibr" target="#b6">[7]</ref>. PoseNet was implemented using ResNet-50 as the backbone for the feature extractor and was trained using geometric loss. DFLNet uses the same architecture as the proposed method however the last layer regresses 14 heatmaps of the landmarks instead of scene coordinates. Note that the original study's segmentation layer and the gradient-based optimization phase were omitted for architectural comparison. Each baseline was trained in a patientspecific manner following the proposed method. The mean target registration error (mTRE) and Gross Failure Rate (GFR), were used as the evaluation metrics for comparison with the baselines. mTRE is defined in 6, where X k is the position of the ground truth landmark Xk after applying the predicted transformation. The GFR is the ratio of failed cases, defined as the registration results with an mTRE greater than 10 mm. Because we could only obtain the projection of the ground truth landmarks and not the ground truth transformation matrix for the real X-ray images, the projected mTRE (proj. mTRE) was used for the evaluation. It is similar to mTRE except the X k and Xk represent the projected coordinates of the landmarks, in the detector plane (i.e., the pixel coordinates are scaled according to the detector size to match the units).</p><formula xml:id="formula_6">mTRE = 1 N k=N k=1 X k -Xk 2<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Registration Results</head><p>Simulated X-Ray Images. Table <ref type="table" target="#tab_0">1</ref> shows the mTRE for the 25 th , 50 th , and 95 th percentiles of the total test sample size and the GFR. The proposed method could retain a GFR below 20% for most of the specimens, whereas PoseNet and DFLNet failed to register with more than 20% GFR in most cases. This is because the network in PoseNet cannot reason about the spatial structure or its local relation to the image patches. For DFLNet, this is inevitable because of the visibility issue of the landmark points, mostly located in the pubic region of the pelvis. Comparing the mTRE of each specimen with that of each method, the proposed method achieved an mTRE of 7.98 mm even in the 95 th percentile of Specimen 2. DFLNet achieved the lowest mTRE of 0.98 mm in the 25 th percentile of Specimen 4. This illustrates the highly accurate registration of landmark estimation methods. However, with extreme or partial views such as the one shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the method cannot estimate the correct pose parameter because of incorrect landmark localization or insufficiently visible landmarks. Please refer to the supplemental material for the registration overlay results of the different specimens using the proposed method.</p><p>Real X-Ray Images. Table <ref type="table" target="#tab_1">2</ref> lists the mTRE values calculated for the projected image points (abbreviated as proj. mTRE) for PoseNet and the proposed method, respectively. DFLNet did not adapt to real X-ray images, therefore, it was omitted from the table. Because our dataset consisted mostly of images with partially visible hips, only a few landmarks were visible in each image. This causes the DFLNet to overfit to the partially visible landmark distribution, whereas our proposed model mitigates this issue by learning the general structure (i.e., every surface point that is visible). The proposed method estimates good transformations (that is proj. mTRE approximately 10 mm in the 50 th percentile). In contrast, the proj. mTRE for PoseNet is significantly higher. This suggests that PoseNet overfitted the training data despite applying domain randomization. This result agrees with previous reports <ref type="bibr" target="#b13">[14]</ref> addressing this issue.</p><p>A visualization of the overlays is presented in the Supplemental Material.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Limitations</head><p>As the proposed method was designed to give initial estimates of the pose parameters, a further refinement step using an intensity-based optimization method would be required to obtain clinically relevant registration accuracy. Although the proposed method provided a good initial estimate, the average runtime for the entire pipeline was 1.75 s which was approximately two orders of magnitude greater than that of PoseNet, which had an average runtime of 0.06 s. This is because RANSAC must determine a good pose from a dense set of correspondences. This issue can be addressed by heuristically selecting a good variance threshold per image that filters out bad correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presented a scene coordinate regression-based approach for the Xray to CT-scan model registration problem. Experiments with simulated and real X-ray images showed that the proposed method performed well even under partially visible structures and extreme view angles, compared with direct pose regression and landmark estimation methods. Testing the model trained solely on simulated X-ray images, on real X-ray images did not result in catastrophic failure. Instead, the results were positive for instantiating further refinement steps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. An overview of the proposed method. Scene coordinates are regressed using a U-Net architecture given an X-ray image. With the obtained dense correspondences, PnP with RANSAC is run to get the transformation matrix that aligns the projection of the 3D model with the X-ray image in the camera coordinate system.</figDesc><graphic coords="3,87,96,54,26,276,67,139,45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of successful registration with the proposed method (left two images) and Randomly picked data samples in the test set (right). The X-ray image and model's gradient projection overlay (middle) and the model's pose in the camera coordinates system (left). The origin of the view frustum is the X-ray source position and the simulated X-ray images are placed in the detector plane for visualization (right).</figDesc><graphic coords="5,80,46,54,20,291,52,77,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example case illustrating an extreme partial viewpoint. The proposed method successfully registers the image with 1.70 mm mTRE, while PoseNet struggles with 16.95 mm mTRE. Since there is an insufficient number (less than 4) of visible landmarks, the DFLNet hallucinates landmarks providing incorrect 2D-3D correspondences, leading to a large mTRE.</figDesc><graphic coords="8,42,30,54,56,339,52,80,14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The mean target registration errors each in 25 th , 50 th and 95 th percentile of the simulated test dataset. All models are trained individually on the 6 specimens shown below. The proposed method outperforms other methods regarding 50 th percentile mTRE and GFR in most specimens.</figDesc><table><row><cell cols="3">Specimen PoseNet</cell><cell></cell><cell cols="2">DFLNet</cell><cell></cell><cell>Ours</cell></row><row><cell></cell><cell cols="2">mTRE[mm]↓</cell><cell cols="3">GFR[%]↓ mTRE[mm]↓</cell><cell cols="2">GFR[%]↓ mTRE[mm]↓</cell><cell>GFR[%]↓</cell></row><row><cell></cell><cell cols="2">25 th 50 th</cell><cell>95 th</cell><cell cols="2">25 th 50 th</cell><cell>95 th</cell><cell cols="2">25 th 50 th 95 th</cell></row><row><cell>#1</cell><cell>5.75</cell><cell cols="2">8.37 24.81 38.53</cell><cell cols="2">3.36 212.59</cell><cell>680.58 62.27</cell><cell cols="2">1.37 2.50 9.80 4.87</cell></row><row><cell>#2</cell><cell cols="3">6.97 10.23 25.95 51.63</cell><cell>1.98</cell><cell>7.04</cell><cell>656.41 46.15</cell><cell cols="2">1.15 2.14 7.98 2.54</cell></row><row><cell>#3</cell><cell>5.42</cell><cell cols="2">7.86 23.34 35.35</cell><cell>1.03</cell><cell>2.51</cell><cell>583.63 28.65</cell><cell cols="2">1.67 3.05 12.25 8.56</cell></row><row><cell>#4</cell><cell>4.67</cell><cell cols="2">6.46 16.91 18.77</cell><cell>0.98</cell><cell>2.30</cell><cell>558.20 23.59</cell><cell cols="2">1.76 3.38 19.19 12.54</cell></row><row><cell>#5</cell><cell>4.81</cell><cell cols="2">6.52 18.98 22.43</cell><cell>1.51</cell><cell>4.28</cell><cell>767.56 37.47</cell><cell cols="2">3.09 5.30 17.32 18.85</cell></row><row><cell>#6</cell><cell>4.06</cell><cell cols="2">5.85 18.42 22.69</cell><cell cols="3">2.26 139.96 15321.19 58.72</cell><cell cols="2">3.80 6.37 18.25 23.18</cell></row><row><cell>mean</cell><cell>5.28</cell><cell cols="2">7.55 21.40 31.57</cell><cell>1.85</cell><cell>61.45</cell><cell>3094.60 42.81</cell><cell cols="2">2.14 3.79 14.13 11.76</cell></row><row><cell>std</cell><cell>1.02</cell><cell cols="2">1.62 3.77 12.58</cell><cell>0.90</cell><cell>91.88</cell><cell>5990.24 15.76</cell><cell>1.06 1.67</cell><cell>4.75 8.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The projected mean target registration errors for real X-ray images. The proposed method achieved significantly low registration errors compared to PoseNet, implying that it generalizes well to unseen data and domains.</figDesc><table><row><cell cols="3">Specimen Number of Images PoseNet</cell><cell>Ours</cell></row><row><cell></cell><cell></cell><cell cols="3">proj. mTRE [mm]↓ proj. mTRE [mm]↓</cell></row><row><cell></cell><cell></cell><cell cols="2">25 th 50 th 95 th 25 th 50 th</cell><cell>95 th</cell></row><row><cell>#1</cell><cell>111</cell><cell cols="3">43.64 49.11 64.23 5.45 8.02 55.87</cell></row><row><cell>#2</cell><cell>24</cell><cell cols="2">19.42 27.18 43.68 2.74 3.32</cell><cell>6.48</cell></row><row><cell>#3</cell><cell>104</cell><cell cols="3">31.18 38.97 66.06 7.60 11.85 162.83</cell></row><row><cell>#4</cell><cell>24</cell><cell cols="3">35.52 38.37 57.07 11.12 15.52 92.34</cell></row><row><cell>#5</cell><cell>48</cell><cell cols="3">38.97 46.60 69.06 6.09 9.07 21.91</cell></row><row><cell>#6</cell><cell>55</cell><cell cols="3">34.51 37.13 47.72 7.18 10.14 20.78</cell></row><row><cell>mean</cell><cell></cell><cell cols="3">33.87 39.56 57.97 6.70 9.65 60.04</cell></row><row><cell>std</cell><cell></cell><cell cols="2">8.25 7.77 10.37 2.77 4.07</cell><cell>59.15</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was partially supported by a grant from <rs type="funder">JSPS KAK-ENHI</rs> grant number <rs type="grantNumber">JP23K08618</rs>. This study (in part) used the computational resources for Cygnus provided by the <rs type="programName">Multidisciplinary Cooperative Research Program</rs> at the <rs type="institution">Center for Computational Sciences, University of Tsukuba, Japan</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FwyfbjK">
					<idno type="grant-number">JP23K08618</idno>
					<orgName type="program" subtype="full">Multidisciplinary Cooperative Research Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 74.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Accurate and precise 2D-3D registration based on X-ray intensity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aouadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sarry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="151" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fluoroscopic navigation system for hip surface replacement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Aided Surg</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="160" to="167" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">X-ray-transform invariant anatomical landmark detection for pelvic trauma surgery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accuracy of acetabular component positioning using computer-assisted navigation in direct anterior total hip arthroplasty</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Muir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cureus</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4478</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards fully automatic X-ray to CT registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zahnd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_70</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32226-770" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="631" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust automatic rigid registration of MRI and X-ray using external fiducial markers for XFM-guided interventional procedures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Lederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Faranesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic annotation of hip anatomy in fluoroscopy for robust and efficient 2D/3D registration</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grupp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="759" to="769" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PoseNet: a convolutional network for realtime 6-DOF camera relocalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient-based 2-D/3-D rigid registration of fluoroscopic X-ray to CT</title>
		<author>
			<persName><forename type="first">H</forename><surname>Livyatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1395" to="1406" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Registration of head volume images using implantable fiducial markers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Galloway</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Maciunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="462" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fluoroscopy-based navigation system in spine surgery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Merloz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Inst. Mech. Eng. H</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="813" to="820" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<title level="m">Real-time 2D/3D registration via CNN regression</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intraoperative fluoroscopy allows the reliable assessment of deformity correction during periacetabular osteotomy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Matziolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Wassilew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Med. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the limitations of CNN-Based absolute camera pose regression</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3297" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">EF3X study group: the value of intraoperative 3-dimensional fluoroscopy in the treatment of distal radius fractures: a randomized clinical trial</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Selles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S H</forename><surname>Beerekamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Leenhouts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J M</forename><surname>Segers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Goslings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W L</forename><surname>Schep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Hand Surg. Am</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="195" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fiducial registration from a single X-ray image: a new technique for fluoroscopic guidance and radiotherapy</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2000</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Delp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Digoia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Jaramaz</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1935</biblScope>
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-40899-4_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-40899-451" />
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual intraoperative estimation of cup and stem position is not reliable in minimally invasive hip arthroplasty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Woerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Orthop</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="230" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Operative fluoroscopic correction is reliable and correlates with postoperative radiographic correction in periacetabular osteotomy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wylie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Orthop. Relat. Res</title>
		<imprint>
			<biblScope unit="volume">475</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1100" to="1106" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
