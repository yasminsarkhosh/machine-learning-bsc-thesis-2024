<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions</title>
				<funder ref="#_jVX3C2S">
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Health &amp; Welfare</orgName>
				</funder>
				<funder ref="#_d9NEUuu">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder ref="#_wj8JYn9">
					<orgName type="full">Ministry of Food and Drug Safety</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Science and ICT (MSIT)</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea Medical Device Development Fund</orgName>
				</funder>
				<funder ref="#_MMGtqfY">
					<orgName type="full">MSIT, Korea</orgName>
				</funder>
				<funder ref="#_dsPTSmE">
					<orgName type="full">Korea Government</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Trade, Industry and Energy</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sihwa</forename><surname>Park</surname></persName>
							<email>sihwapark@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seongjun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">In-Seok</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Korea University Anam Hospital</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seung</forename><forename type="middle">Jun</forename><surname>Baek</surname></persName>
							<email>sjbaek@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="376" to="386"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6381DDF35747D5D2B32E235A99295313</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Panoramic radiographs</term>
					<term>3D reconstruction</term>
					<term>Teeth segmentation</term>
					<term>Neural implicit function</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Panoramic radiography is a widely used imaging modality in dental practice and research. However, it only provides flattened 2D images, which limits the detailed assessment of dental structures. In this paper, we propose Occudent, a framework for 3D teeth reconstruction from panoramic radiographs using neural implicit functions, which, to the best of our knowledge, is the first work to do so. For a given point in 3D space, the implicit function estimates whether the point is occupied by a tooth, and thus implicitly determines the boundaries of 3D tooth shapes. Firstly, Occudent applies multi-label segmentation to the input panoramic radiograph. Next, tooth shape embeddings as well as tooth class embeddings are generated from the segmentation outputs, which are fed to the reconstruction network. A novel module called Conditional eXcitation (CX) is proposed in order to effectively incorporate the combined shape and class embeddings into the implicit function. The performance of Occudent is evaluated using both quantitative and qualitative measures. Importantly, Occudent is trained and validated with actual panoramic radiographs as input, distinct from recent works which used synthesized images. Experiments demonstrate the superiority of Occudent over state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Panoramic radiography (panoramic X-ray, or PX) is a commonly used technique for dental examination and diagnosis. While PX produces 2D images from panoramic scanning, Cone-Beam Computed Tomography (CBCT) is an alternative imaging modality which provides 3D information on dental, oral, and maxillofacial structures. Despite providing more comprehensive information than PX, CBCT is more expensive and exposes patients to a greater dose of radiation <ref type="bibr" target="#b2">[3]</ref>. Thus, 3D teeth reconstruction from PX is of significant value, e.g., 3D visualization can aid clinicians with dental diagnosis and treatment planning. Other applications include treatment simulation and interactive virtual reality for dental education <ref type="bibr" target="#b11">[12]</ref>.</p><p>Previous 3D teeth reconstruction methods from 2D PX have relied on additional information such as tooth landmarks or tooth crown photographs. For example, <ref type="bibr" target="#b13">[14]</ref> developed a model which uses landmarks on PX images to estimate 3D parametric models for tooth shapes, while <ref type="bibr" target="#b0">[1]</ref> reconstructed a single tooth using a shape prior and reflectance model based on the corresponding crown photograph. Recent advances in deep neural networks have significantly impacted research on 3D teeth reconstruction. X2Teeth <ref type="bibr" target="#b12">[13]</ref> performs 3D reconstruction of the entire set of teeth from PX based on 2D segmentation using convolutional neural networks. Oral-3D <ref type="bibr" target="#b21">[22]</ref> generated 3D oral structures without supervised segmentation from PX using a GAN model <ref type="bibr" target="#b7">[8]</ref>. Yet, those methods relied on synthesized images as input instead of real-world PX images, where the synthesized images are obtained from 2D projections of CBCT <ref type="bibr" target="#b26">[27]</ref>. The 2D segmentation of teeth from PX is useful for 3D reconstruction in order to identify and isolate teeth individually. Prior studies on 2D teeth segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> focused on binary segmentation determining the presence of teeth. However, this information alone is insufficient for the construction of individual teeth. Instead, we leverage recent frameworks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref> on multi-label segmentation of PX into 32 classes including wisdom teeth.</p><p>In this paper, we propose Occudent, an end-to-end model to reconstruct 3D teeth from 2D PX images. Occudent consists of a multi-label 2D segmentation followed by 3D teeth reconstruction using neural implicit functions <ref type="bibr" target="#b14">[15]</ref>. The function aims to learn the occupancy of dental structures, i.e., whether a point in space lies within the boundaries of 3D tooth shapes. Learning implicit functions is computationally advantageous over conventional encoder-decoder models outputting explicit 3D representations such as voxels, e.g., implicit models do not require large memory footprints to store and process voxels. Considering that 3D tooth shapes are characterized by tooth classes, we generate embeddings for tooth classes as well as segmented 2D tooth shapes. The combined class and shape embeddings are infused into the reconstruction network by a novel module called Conditional eXcitation (CX). CX performs learnable scaling of occupancy features conditioned on tooth class and shape embeddings. The performance of Occudent is evaluated with actual PX as input images, which differs from recent works using synthesized PX images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. Experiments show Occudent outperforms state-of-the-art baselines both quantitatively and qualitatively. The main contributions are summarized as follows: (1) the first use of a neural implicit function for 3D teeth reconstruction, (2) novel strategies to inject tooth class and shape information into implicit functions, (3) the superiority over existing baselines which is demonstrated with real-world PX images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The proposed model, Occudent, consists of two main components: 2D teeth segmentation and 3D teeth reconstruction. The former performs the segmentation of 32 teeth from PX using UNet++ model <ref type="bibr" target="#b28">[29]</ref>. The individually segmented tooth and the tooth class are subsequently passed to the latter for the reconstruction. The reconstruction process estimates the 3D representation of the tooth based on a neural implicit function. The overall architecture of Occudent is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. CX uses a trainable weight matrix to encode the condition vector into an excitation vector via a gating function. The input feature is scaled using the excitation vector through component-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">2D Teeth Segmentation</head><p>The teeth in input PX are segmented into 32 teeth classes. The 32 classes correspond to the traditional numbering of teeth, which includes incisors, canines, premolars and molars in both upper and lower jaws. We pose 2D teeth segmentation as a multi-label segmentation problem <ref type="bibr" target="#b16">[17]</ref>, since nearby teeth can overlap with each other in the PX image, i.e., a single pixel of the input image can be classified into two or more classes.</p><p>The input of the model is H ×W size PX image. The segmentation output has dimension C × H × W , where channel dimension C = 33 represents the number of tooth classes: one class for the background and 32 classes for teeth similar to <ref type="bibr" target="#b16">[17]</ref>. Hence, the H × W output at each channel is a segmentation output for each tooth class. The segmentation outputs are used to generate tooth patches for reconstruction, which is explained later in detail. For the segmentation, we adopt pre-trained UNet++ <ref type="bibr" target="#b28">[29]</ref> as the base model. UNet++ is advantageous for medical image segmentation due to its modified skip pathways, which results in better performance compared to the vanilla UNet <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">3D Teeth Reconstruction</head><p>Neural Implicit Representation. Typical representations of 3D shapes are point-based <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>, voxel-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, or mesh-based methods <ref type="bibr" target="#b24">[25]</ref>. These methods represent 3D shapes explicitly through a set of discrete points, vertices, and faces. Recently, implicit representation methods based on a continuous function which defines the boundary of 3D shapes have become increasingly popular <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. Occupancy Networks <ref type="bibr" target="#b14">[15]</ref> is a pioneering work which utilizes neural networks to approximate the implicit function of an object's occupancy. The term occupancy refers to whether a point in space lies in the interior or exterior of object boundaries. The occupancy function maps a 3D point to either 0 or 1, indicating the occupancy of the point. Let o A denote the occupancy function for an object A as follows:</p><formula xml:id="formula_0">o A : IR 3 → {0, 1}<label>(1)</label></formula><p>In practice, o A can be estimated only by a set of observations of object A, denoted by X A . Examples of observations are projected images or point cloud data obtained from the object. Our objective is to estimate the occupancy function conditioned on X A . Specifically, we would like to find function f θ which estimates the occupancy probability of a point in 3D space based on X A <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_1">f θ : IR 3 × X A → [0, 1]<label>(2)</label></formula><p>Inspired by the aforementioned framework, we leverage segmented tooth patch and tooth class as observations denoted by condition vector c. Specifically, the input to the function is a set of T randomly sampled locations within a unit cube, and the function outputs the occupancy probability of the input. Thus, the function is given by f</p><formula xml:id="formula_2">θ : (x, y, z, c) → [0, 1].</formula><p>The model for f θ is depicted in Fig. <ref type="figure" target="#fig_0">1 (b</ref>). The sampled locations are projected to 128 dimensional feature vectors using 1D convolution. Next, the features are processed by a sequence of ResNet blocks followed by FC (fully connected) layers. Conditional vector c is used for each block through Conditional eXcitation (CX) which we will explain later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-Specific Conditional Features.</head><p>A distinctive feature of the tooth reconstruction task is that teeth with the same number share properties such as surface and root shapes. Hence, we propose to use tooth class information in combination with a segmented tooth patch from PX. The tooth class is processed by a learnable embedding layer which outputs a class embedding vector.</p><p>Next, we create a square patch of the tooth using the segmentation output as follows. A binary mask of the segmented tooth is generated by applying thresholding to the segmentation output. A tooth patch is created by cropping out the tooth region from the input PX, i.e., the binary mask is applied (bitwise AND) to the input PX to obtain the patch. The segmented tooth patch is subsequently encoded using a pre-trained ResNet18 model <ref type="bibr" target="#b8">[9]</ref>, which outputs a patch embedding vector. The patch and class embeddings are added to yield the condition vector for the reconstruction model. This process is depicted in Fig. <ref type="figure" target="#fig_0">1 (a)</ref>.</p><p>Our approach differs from previous approaches, such as Occupancy Networks <ref type="bibr" target="#b14">[15]</ref> which uses only single-view images for 3D reconstruction. X2Teeth <ref type="bibr" target="#b12">[13]</ref> also addresses the task of 3D teeth reconstruction from 2D PX. However, X2Teeth only uses segmented image features for the reconstruction. By contrast, Occudent leverages a class-specific encoding method to boost the reconstruction performance, as demonstrated in ablation analysis in Supplementary Materials.</p><p>Conditional eXcitation. To effectively inject 2D observations into the reconstruction network, we propose Conditional eXcitation (CX) inspired by Squeezeand-Excitation Network (SENet) <ref type="bibr" target="#b9">[10]</ref>. In SENet, excitation refers to scaling input features according to their importance. In Occudent, the concept of excitation is extended to incorporating conditional features into the network. Firstly, the condition vector is encoded into excitation vector e. Next, the excitation is applied to input feature by scaling the feature components by e. The CX procedure can be expressed as:</p><formula xml:id="formula_3">e = α • σ(W c),<label>(3)</label></formula><formula xml:id="formula_4">y = F ext (e, x)<label>(4)</label></formula><p>where c is the condition vector, σ is a gating function, W is a learnable weight matrix, α is a hyperparameter for the excitation result, and F ext is the excitation function. We use sigmoid function for σ, and component-wise multiplication for the excitation, F ext (e, x) = e ⊗ x. The CX module is depicted in Fig. <ref type="figure" target="#fig_0">1 (d)</ref>. CX differs from SENet in that CX derives the excitation from the condition vector, whereas SENet derives it from input features. Our approach also differs from Occupancy Networks which used Conditional Batch Normalization (CBN) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> which combines conditioning with batch normalization. However, the conditioning process should be independent of input batches because those components serve different purposes in deep learning models. Thus, we propose to separate conditioning from batch normalization, as is done by CX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset. Implementation Details. For the pre-training of the segmentation model, we utilized a combination of cross-entropy and dice loss. For the main segmentation training, we used only dice loss. The segmentation and reconstruction models were trained separately. Following the completion of the segmentation model training, we fixed this model to predict its output for the reconstruction model.</p><p>Each 3D tooth label was fit in 144 × 80 × 80 size tensor which was then regarded as [-0.5, 0.5] 3 normalized cube in 3D space. For the training of the neural implicit function, a set of T = 100, 000 points was sampled from the unit cube. The preprocessing was consistent with that used in <ref type="bibr" target="#b22">[23]</ref>. We trained all the other baseline models with these normalized cubes. For example, for 3D-R2N2 <ref type="bibr" target="#b3">[4]</ref>, we voxelized the cube to 128 3 size. For a fair comparison, the final meshes produced by each model were extracted and compared using four different metrics. The detailed configuration of our model is provided in Supplementary Materials.</p><p>Baselines. We considered several state-of-the-art models as baselines, including 3D-R2N2 <ref type="bibr" target="#b3">[4]</ref>, DeepRetrieval <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>, Pix2Vox <ref type="bibr" target="#b25">[26]</ref>, PSGN <ref type="bibr" target="#b6">[7]</ref>, Occupancy Networks (OccNet) <ref type="bibr" target="#b14">[15]</ref>, and X2Teeth <ref type="bibr" target="#b12">[13]</ref>. To adapt the 3D-R2N2 model to singleview reconstruction, we removed its LSTM component, following the approach in <ref type="bibr" target="#b14">[15]</ref>. As for the DeepRetrieval method, we employed the same encoder architecture as 3D-R2N2, and utilized the encoded feature vector of the test image Evaluation Metrics. The evaluation of the proposed method was conducted using the following metrics: volumetric Intersection over Union (IoU), Chamfer-L 1 distance, and Normal Consistency (NC), as outlined in prior work <ref type="bibr" target="#b14">[15]</ref>. In addition, we used volumetric precision <ref type="bibr" target="#b12">[13]</ref> as a metric given by |D ∩ G|/|D| where G denotes the ground-truth set of points occupied by the object, and D denotes the set of points predicted as the object.</p><p>Quantitative Comparison. Table <ref type="table" target="#tab_1">1</ref> presents a quantitative comparison of the proposed model with several baseline models. The results demonstrate that Occudent surpasses the other methods across all the metrics, and the methods based on neural implicit functions (Occudent and OccNet) perform better compared to conventional encoder-decoder approaches, such as Pix2Vox and 3D-R2N2. The performance gap between Occudent and X2Teeth is presumably because real PX images are used as input data. X2Teeth used synthesized images generated from the 2D projections of CBCT in <ref type="bibr" target="#b26">[27]</ref>. Thus, both the input 2D shape and the target 3D shape come from the same modality (CBCT). However, the distribution of real PX images may differ significantly from that of 2D-projected CBCT. Explicit methods can be more sensitive to such differences than implicit methods, because typically in explicit methods, input features are directly encoded and subsequently decoded to predict the target shapes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. Overall, the differences in the IoU performances among the baselines are somewhat small. This is because all the baselines are moderately successful in generating coarse tooth shapes. However, Occudent is significantly better at generating details such as root shapes, which will be shown in the subsequent section.</p><p>Qualitative Comparison. Figure <ref type="figure" target="#fig_1">2A</ref> illustrates the qualitative results of the proposed method in generating 3D teeth mesh outputs. From our model, each tooth is generated, and generated teeth are combined along with an arch curve based on a beta function <ref type="bibr" target="#b1">[2]</ref>. Figure <ref type="figure" target="#fig_1">2A</ref> demonstrates that our proposed method generates the most similar-looking outputs compared to the ground truth. For instance, our model can reconstruct a plausible shape for all tooth types including detailed shapes of molar roots. 3D-R2N2 produces larger and less detailed tooth shapes. PSGN and OccNet are better at generating rough shapes than 3D-R2N2, however, lack in detailed root shapes. As illustrated in Fig. <ref type="figure" target="#fig_1">2B</ref>, Occudent produces a more refined mesh of tooth shape representation than voxel-based methods like 3D-R2N2 or X2Teeth. One of the limitations of voxel-based methods is that they heavily depend on the resolution of the output. For example, increasing the output size leads to an exponential increase in model size. By contrast, Occudent employs continuous neural implicit functions to represent shape boundaries, which enables us to generate smoother output and to be robust to the target size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present a framework for 3D teeth reconstruction from a single PX. To the best of our knowledge, our method is the first to utilize a neural implicit function for 3D teeth reconstruction. The performance of our proposed framework is evaluated quantitatively and qualitatively, demonstrating its superiority over state-of-the-art techniques. Importantly, our framework is capable of accommodating two distinct modalities, PX, and CBCT. Our framework has the potential to be valuable in clinical practice and also can support virtual simulation or educational tools. In the future, further improvements can be made, such as incorporating additional imaging modalities or exploring neural architectures for more robust reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) The PX image is segmented into 32 teeth classes using UNet++ model. For each tooth, a segmented patch is generated by cropping input PX with the predicted segmentation mask, which is subsequently encoded via an image encoder. The tooth class is encoded by an embedding layer. The patch and class embeddings are added together to produce the condition vector. (b) The reconstruction process consists of N ResBlocks to compute occupancy features of points sampled from 3D space. The condition vector from PX is processed by a Conditional eXcitation (CX) module incorporated in the ResBlocks. (c) The Conv with CX sub-module is composed of batch normalization, CX, ReLU, and a convolutional layer. The FC with CX is similar to the Conv with CX where Conv1D layer is replaced by fully connected layer. (d) CX injects condition information into the reconstruction network using excitation values.CX uses a trainable weight matrix to encode the condition vector into an excitation vector via a gating function. The input feature is scaled using the excitation vector through component-wise multiplication.</figDesc><graphic coords="3,43,80,184,13,336,49,259,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual representation of sample outputs. Boxes are used to highlight the incisors and molars in the upper jaw.</figDesc><graphic coords="8,58,98,184,88,334,54,253,06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The pre-training of the segmentation model was done with a dataset of 4000 PX images, sourced from 'The Open AI Dataset Project (AI-Hub, S. Korea)'. All data information can be accessed through 'AI-Hub (www.aihub.or.kr)'. The dataset consisted of two image sizes, 1976 × 976 and 2988 × 1468, which were resized to 256 × 768 to train the UNet++ model.For the main experiments for reconstruction, we used a set of 39 PX images and matched CBCT images, obtained from Korea University Anam Hospital. This study was approved by the Institutional Review Board at Korea University (IRB number: 2020AN0410). The panoramic radiographs were of dimensions 1536 × 2860 and were resized to 600 × 1200 and randomly cropped of 592 × 1184 size for the segmentation training. The CBCT images were of size 768 × 768 × 576, capturing cranial bones. The teeth labels for 2D PX and CBCT were manually annotated by two experienced annotators and subsequently verified by a board-certified dentist. To train and evaluate the model, the dataset was partitioned into training (30 cases), validation (2 cases), and testing (7 cases) subsets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with baseline methods. The format of results is mean±std obtained from 10 repetitions of experiments. Subsequently, we compared each encoded vector from the test image to the encoded vectors from the training set, and retrieved the tooth with the minimum Euclidean distance of encoded vectors from the training set.</figDesc><table><row><cell>Method</cell><cell>IoU</cell><cell>Chamfer-L 1</cell><cell>NC</cell><cell>Precision</cell></row><row><cell>3D-R2N2</cell><cell cols="4">0.585 ± 0.005 0.382 ± 0.008 0.617 ± 0.009 0.634 ± 0.010</cell></row><row><cell>PSGN</cell><cell cols="4">0.606 ± 0.015 0.342 ± 0.016 0.829 ± 0.012 0.737 ± 0.018</cell></row><row><cell>Pix2Vox</cell><cell cols="4">0.562 ± 0.005 0.388 ± 0.008 0.599 ± 0.007 0.664 ± 0.009</cell></row><row><cell>DeepRetrieval</cell><cell cols="4">0.564 ± 0.005 0.394 ± 0.006 0.824 ± 0.003 0.696 ± 0.005</cell></row><row><cell>X2Teeth</cell><cell cols="4">0.592 ± 0.006 0.361 ± 0.017 0.618 ± 0.002 0.670 ± 0.009</cell></row><row><cell>OccNet</cell><cell cols="4">0.611 ± 0.006 0.353 ± 0.008 0.872 ± 0.003 0.691 ± 0.011</cell></row><row><cell cols="5">Occudent (Ours) 0.651 ± 0.004 0.298 ± 0.006 0.890 ± 0.001 0.739 ± 0.008</cell></row><row><cell>query.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was supported by the <rs type="funder">Korea Medical Device Development Fund</rs> grant funded by the <rs type="funder">Korea Government</rs> (the <rs type="institution">Ministry of Science and ICT</rs>, the <rs type="funder">Ministry of Trade, Industry and Energy</rs>, the <rs type="funder">Ministry of Health &amp; Welfare</rs>, the <rs type="funder">Ministry of Food and Drug Safety</rs>) (Project Number: <rs type="grantNumber">1711195279</rs>, <rs type="grantNumber">RS-2021-KD000009</rs>); the <rs type="funder">National Research Foundation of Korea (NRF)</rs> Grant through the <rs type="funder">Ministry of Science and ICT (MSIT)</rs>, <rs type="funder">Korea Government</rs>, under Grant <rs type="grantNumber">2022R1A5A1027646</rs>; the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Korea government (MSIT)</rs> (No. <rs type="grantNumber">2021R1A2C1007215</rs>); the <rs type="funder">MSIT, Korea</rs>, under the <rs type="programName">ICT Creative Consilience program</rs> (<rs type="grantNumber">IITP-2023-2020-0-01819</rs>) supervised by the <rs type="institution">IITP (Institute for Information &amp; communications Technology Planning &amp; Evaluation</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wj8JYn9">
					<idno type="grant-number">1711195279</idno>
				</org>
				<org type="funding" xml:id="_jVX3C2S">
					<idno type="grant-number">RS-2021-KD000009</idno>
				</org>
				<org type="funding" xml:id="_dsPTSmE">
					<idno type="grant-number">2022R1A5A1027646</idno>
				</org>
				<org type="funding" xml:id="_d9NEUuu">
					<idno type="grant-number">2021R1A2C1007215</idno>
				</org>
				<org type="funding" xml:id="_MMGtqfY">
					<idno type="grant-number">IITP-2023-2020-0-01819</idno>
					<orgName type="program" subtype="full">ICT Creative Consilience program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 36.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">D-PCA shape models: application to 3D reconstruction of the human teeth from a single image</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Abdelrehim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Shalaby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>El-Melegy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-05530-5_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-05530-55" />
	</analytic>
	<monogr>
		<title level="m">MCV 2013</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Montillo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kelm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8331</biblScope>
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The form of the human dental arch</title>
		<author>
			<persName><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Hnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Fender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Legan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Angle Orthod</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CBCT dosimetry: orthodontic considerations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Seminars in Orthodontics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="14" to="18" />
			<date type="published" when="2009">2009</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D-R2N2: a unified approach for single and multi-view 3D object reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46484-8_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46484-838" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate segmentation of dental panoramic radiographs with U-Nets</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perslev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="15" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The current situation and future prospects of simulators in dental education</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">23635</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">X2Teeth: 3D Teeth reconstruction from a single panoramic radiograph</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59713-939" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="400" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From 2D to 3D: construction of a 3D parametric model for detection of dental roots shape and position from a panoramic radiograph-a preliminary report</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mazzotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cozzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razionale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mutinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silvestrini-Biavati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Dent</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Occupancy networks: learning 3D reconstruction in function space</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">NeRF: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-824" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic teeth segmentation on panoramic X-rays using deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smorodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De La Fourniere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Amouriq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Autrusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 26th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4299" to="4305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepSDF: learning continuous signed distance functions for shape representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PointNet: deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study on tooth segmentation and numbering using end-to-end deep neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pithon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Oral-3D: reconstructing the 3D structure of oral cavity from panoramic x-ray</title>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/16135" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-09">2-9 February 2021. 2021</date>
			<biblScope unit="page" from="566" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning 3D shape completion under weak supervision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>CoRR abs/1805.07290</idno>
		<ptr target="http://arxiv.org/abs/1805.07290" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What do single-view 3D reconstruction networks learn?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3405" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: generating 3D mesh models from single RGB images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01252-6_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01252-64" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11215</biblScope>
			<biblScope unit="page" from="55" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pix2Vox: context-aware 3D reconstruction from single and multi-view images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2690" to="2698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic reconstruction method for high-contrast panoramic image from dental cone-beam CT data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="205" to="214" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TSASNet: tooth segmentation on dental panoramic X-ray images by two-stage attention segmentation network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page">106338</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-51" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
