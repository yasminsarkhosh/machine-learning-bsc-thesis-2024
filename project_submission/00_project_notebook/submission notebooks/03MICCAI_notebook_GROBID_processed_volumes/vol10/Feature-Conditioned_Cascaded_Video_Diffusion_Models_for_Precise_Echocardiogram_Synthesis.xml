<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis</title>
				<funder ref="#_EEgW4Xk">
					<orgName type="full">Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
					<orgName type="abbreviated">FAU</orgName>
				</funder>
				<funder ref="#_F9SvEPk">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_AeksSZ2">
					<orgName type="full">Ultromics Ltd.</orgName>
				</funder>
				<funder>
					<orgName type="full">German Research Foundation (DFG) -440719683</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hadrien</forename><surname>Reynaud</surname></persName>
							<email>hadrien.reynaud19@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UKRI CDT in AI for Healthcare</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengyun</forename><surname>Qiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Brain Sciences and DSI</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mischa</forename><surname>Dombrowski</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Day</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of BMEIS</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Guy&apos;s and St Thomas&apos; NHS Foundation Trust</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Razavi</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of BMEIS</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Guy&apos;s and St Thomas&apos; NHS Foundation Trust</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><surname>Gomez</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of BMEIS</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Ultromics Ltd</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Leeson</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Ultromics Ltd</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Cardiovascular Clinical Research Facility</orgName>
								<orgName type="institution">John Radcliffe Hospital</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="142" to="152"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3FFBC97BB0F85432DC83F209C979FC30</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative</term>
					<term>Diffusion</term>
					<term>Video</term>
					<term>Cardiac</term>
					<term>Ultrasound</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image synthesis is expected to provide value for the translation of machine learning methods into clinical practice. Fundamental problems like model robustness, domain transfer, causal modelling, and operator training become approachable through synthetic data. Especially, heavily operator-dependant modalities like Ultrasound imaging require robust frameworks for image and video generation. So far, video generation has only been possible by providing input data that is as rich as the output data, e.g., image sequence plus conditioning in → video out. However, clinical documentation is usually scarce and only single images are reported and stored, thus retrospective patient-specific analysis or the generation of rich training data becomes impossible with current approaches. In this paper, we extend elucidated diffusion models for video modelling to generate plausible video sequences from single images and arbitrary conditioning with clinical parameters. We explore this idea within the context of echocardiograms by looking into the variation of the Left Ventricle Ejection Fraction, the most essential clinical metric gained from these examinations. We use the publicly available EchoNet-Dynamic dataset for all our experiments. Our image to sequence approach achieves an R 2 score of 93%, which is 38 points higher than recently proposed sequence to sequence generation methods. Code and weights are available at https://github.com/HReynaud/EchoDiffusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ultrasound (US) is widely used in clinical practice because of its availability, realtime imaging capabilities, lack of side effects for the patient and flexibility. US is a dynamic modality that heavily relies on operator experience and on-the-fly interpretation, which requires many years of training and/or Machine Learning (ML) support that can handle image sequences. However, clinical reporting is conventionally done via single, selected images that rarely suffice for clinical audit or as training data for ML. Simulating US from anatomical information, e.g. Computed Tomography (CT) <ref type="bibr" target="#b27">[28]</ref>, Magnetic Resonance Imaging (MRI) <ref type="bibr" target="#b24">[25]</ref> or computational phantoms <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>, has been considered as a possible avenue to provide more US data for both operator and ML training. However, simulations are usually very computationally expensive due to complex scattering, reflection and refraction of sound waves at tissue boundaries during image generation. Therefore, the image quality of US simulations has not yet met the necessary quality to support tasks such as cross-modality registration, multi-modal learning, and robust decision support for image analysis during US examinations. More recently, generative deep learning methods have been proposed to address this issue. While early approaches show promising results, they either focus on generating individual images <ref type="bibr" target="#b15">[16]</ref> or require video input data and further conditioning to provide useful results <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. Research in the field of imageconditioned video generation is very scarce <ref type="bibr" target="#b32">[33]</ref> and, to the best of our knowledge, we are the first to apply it to medical imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution:</head><p>In this paper, we propose a new method for video diffusion <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> based on the Elucidated Diffusion Model (EDM) <ref type="bibr" target="#b12">[13]</ref> that allows to synthesise plausible video data from single frames together with precise conditioning on interpretable clinical parameters, e.g., Left Ventricular Ejection Fraction (LVEF) in echocardiography. This is the first time diffusion models have been extended for US image and video synthesis. Our contributions are three-fold: <ref type="bibr" target="#b0">(1)</ref> We show that discarding the conventional text-embeddings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref> to control the reverse diffusion process is desirable for medical use cases where very specific elements must be precisely controlled; <ref type="bibr" target="#b1">(2)</ref> We quantitatively improve upon existing methods <ref type="bibr" target="#b20">[21]</ref> for counterfactual modelling, e.g., when doctors try to answer questions like "how would the scan of this patient look like if we would change a given clinical parameter?"; (3) We show that fine-grained control of the conditioning leads to precise data generation with specific properties and outperforms the state-of-the-art when using such data, for example, for the estimation of LVEF in patients that are not commonly represented in training databases.</p><p>Related Work: Video Generation has been a research area within computer vision for many years now. Prior works can be organized in three categories: (1) pixel-level autoregressive models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>, (2) latent-level autoregressive model coupled with generators or up-samplers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> and (3) latent-variable transformer-based models with up-samplers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref>. Diffusion models have shown reasonable performance on low temporal and spatial resolutions <ref type="bibr" target="#b9">[10]</ref> as well as on longer samples with high definition image quality <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> conditioned on text inputs. Recently, <ref type="bibr" target="#b37">[38]</ref> combined an autoregressive pixel-level model with a diffusion-based pipeline that predicts a correction of the frame, while <ref type="bibr" target="#b2">[3]</ref> presents an autoregressive latent diffusion model.</p><p>Ultrasound simulation has been attempted with three major approaches: (1) physics-based simulators <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>, (2) cross-modality registration-based methods <ref type="bibr" target="#b14">[15]</ref> and (3) deep-learning based methods, usually conditioned on US, MRI or CT image priors <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> to condition the anatomy of the generated US images. Cine-ultrasound has also attracted some interest. <ref type="bibr" target="#b16">[17]</ref> presents a motion-transferbased method for pelvic US video generation, while <ref type="bibr" target="#b20">[21]</ref> proposes a causal model for generating echocardiograms conditioned on arbitrary LVEF.</p><p>LVEF is a major metric in the assessment of cardiac function and diagnosis of cardiomyopathy. The EchoNet-dynamic dataset <ref type="bibr" target="#b18">[19]</ref> is used as the go-to benchmark for LVEF-regression methods. Various works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref> have attempted to improve on <ref type="bibr" target="#b18">[19]</ref> but the most reproducible method remains the use of an R2+1D model trained over fixed-length videos. The R2+1D_18 trained for this work achieves an R 2 score of 0.81 on samples of 64 frames spanning 2 s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Diffusion probabilistic models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> are the most recent family of generative models. In this work, we follow the definition of the EDM from <ref type="bibr" target="#b12">[13]</ref>. Let q(x) represent the real distribution of our data, with a standard deviation of σ q . A family of distributions p(x; σ) can be obtained by adding i.i.d Gaussian noise with a standard deviation of σ to the data. When σ max σ q , the distribution p(x; σ max ) is essentially the same as pure Gaussian noise. The core idea of diffusion models is to sample a pure noise data point x 0 ∼ N (0, σ 2 max I) and then progressively remove the noise, generating images x i with standard deviation σ i such that σ max = σ 0 &gt; σ 1 &gt; ... &gt; σ N = 0, and x i ∼ p(x; σ i ). The final image x N produced by this process is thus distributed according to q(x), the true distribution of the data. To perform the reverse diffusion process, we define a denoising function D(x, σ) trained to minimize the L 2 denoising error for all samples drawn from q for every σ such that:</p><formula xml:id="formula_0">L = E y ∼q E n ∼N (0,σ 2 I) ||D(y + n; σ) -y|| 2 2 (1)</formula><p>where y is a training data point and n is noise. By following the definition of ordinary differential equations (ODE) we can continuously increase or decrease the noise level of our data point by moving it forward or backward in the diffusion process, respectively. To define the ODE we need a schedule σ(t) that sets the noise level given the time step t, which we set to σ(t) = t. The probability flow ODE's characteristic property is that moving a sample</p><formula xml:id="formula_1">x a ∼ p(x a ; σ(t a )) from the diffusion step t a to t b with t a &gt; t b or t a &lt; t b should result in a sample x b ∼ p(x b ; σ(t b )</formula><p>) and this requirement is satisfied by setting dx = -σ(t)σ(t)∇ x log p(x; σ(t))dt where σ denotes the time derivative and ∇ x log p(x; σ) is the score function. From the score function, we can thus write ∇ x log p(x; σ) = (D(x; σ)-x)/σ 2 in the case of our denoising function, such that the score function isolates the noise from the signal x and can either amplify it or diminish it depending on the direction we take in the diffusion process. We define D(x; σ) to transform a neural network F , which can be trained inside D by following the loss described in Eq. ( <ref type="formula">1</ref>). The EDM also defines a list of four important preconditionings which are defined as</p><formula xml:id="formula_2">c skip (σ) = (σ 2 q )/(σ 2 + σ 2 q ), c out (σ) = σ * σ q * 1/(σ 2 q * σ 2 ) 0.5 , c in (σ) = 1/(σ 2 q * σ 2 ) 0.</formula><p>5 and c noise (σ) = log(σ t )/4 where σ q is the standard deviation of the real data distribution. In this paper, we focus on generating temporally coherent and realistic-looking echocardiograms. We start by generating a low resolution, low-frame rate video v 0 from noise and condition on arbitrary clinical parameters and an anatomy instead of the commonly used text-prompt embeddings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>. Then, the video is used as conditioning for the following diffusion model, which generates a temporally and/or spatially upsampled video v 1 resembling v 0 , following the Cascaded Diffusion Model (CDM) <ref type="bibr" target="#b8">[9]</ref> idea. Compared to image diffusion models, the major change to the Unet-based architecture is to add time-aware layers, through attention, at various levels as well as 3D convolutions (see Fig. <ref type="figure" target="#fig_0">1</ref> and Appendix Fig. <ref type="figure" target="#fig_0">1</ref>). For the purpose of this research, we extend <ref type="bibr" target="#b6">[7]</ref> to handle our own set of conditioning inputs, which are a single image I c and a scalar value λ c , while following the EDM setup, which we apply to video generation. We formally define the denoising models in the cascade as D θs where s defines the rank (stage) of the model in the cascade, and where D θ0 is the base model. The base model is defined as:</p><formula xml:id="formula_3">D θ0 (x; σ, I c , λ c ) = c skip (σ)x + c out (σ)F θ0 (c in (σ)x; c noise (σ), I c , λ c )),</formula><p>where F θ0 is the neural network transformed by D θ0 and D θ0 outputs v 0 . For all subsequent models in the cascade, the conditioning remains similar, but the models also receive the output from the preceding model, such that:</p><formula xml:id="formula_4">D θs (x; σ, I c , λ c , v s-1 ) = c skip (σ)x + c out (σ)F θs (c in (σ)x; c noise (σ), I c , λ c , v s-1 )).</formula><p>This holds ∀s &gt; 0 and inputs I c , v s-1 are rescaled to the spatial and temporal resolutions expected by the neural network F θs as a pre-processing. We apply the robustness trick from <ref type="bibr" target="#b8">[9]</ref>, i.e, we add a small amount of noise to real videos v s-1 during training, when using them as conditioning, in order to mitigate domain gaps with the generated samples v s-1 during inference.</p><p>Sampling from the EDM is done through a stochastic sampling method. We start by sampling a noise sample x 0 ∼ N (0, t 2 0 I), where t comes from our previously defined σ(t i ) = t i and sets the noise level. We follow <ref type="bibr" target="#b12">[13]</ref> and set constants S noise = 1.003, S tmin = 0.05, S tmax = 50 and one constant S churn dependent on the model. These are used to compute γ i (t i ) = min(S churn /N, √ 2 -1) ∀t i ∈ [S tmin , S tmax ] and 0 otherwise, where N is the number of sampling steps. Then ∀i ∈ {0, ..., N -1}, we sample i ∼ N (0, S noise I) and compute a slightly increased noise level ti = (γ i (t i ) + 1)t i , which is added to the previous sample xi = x i + ( t2 i -t 2 i ) 0.5 i . We then execute the denoising model D θ on that sample and compute the local slope d i = (x i -D θ ( xi ; ti ))/ ti which is used to predict the next sample x i+1 = xi + ( ti+1 -ti )d i . At every step but the last (i.e: ∀i = N -1), we apply a correction to x i+1 such that:</p><formula xml:id="formula_5">d i = (x i+1 -D θ (x i+1 ; t i+1 ))/t i+1 and x i+1 = xi + (t i+1 -ti )(d i + d i )/2.</formula><p>The correction step doubles the number of executions of the model, and thus the sampling time per step, compared to DDPM <ref type="bibr" target="#b7">[8]</ref> or DDIM <ref type="bibr" target="#b31">[32]</ref>. The whole sampling process is repeated sequentially for all models in the cascaded EDM. Models are conditioned on the previous output video v s-1 inputted at each step of the sampling process, with the frame conditioning I c as well as the scalar value λ c .</p><p>Conditioning: Our diffusion models are conditioned on two components. First, an anatomy, which is represented by a randomly sampled frame I c . It defines the patient's anatomy, but also all the information regarding the visual style and quality of the target video. These parameters cannot be explicitly disentangled, and we therefore limit ourselves to this approach. Second, we condition the model on clinical parameters λ c . This is done by discarding the text-encoders that are used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref> and directly inputting normalized clinical parameters into the conditional inputs of the Unets. By doing so, we give the model fine-grained control over the generated videos, which we evaluate using task-specific metrics.</p><p>Parameters: As video diffusion models are still in their early stage, there is no consensus on which are the best methods to train them. In our case, we define, depending on our experiment, 1-, 2-or 4-stages CDMs. We also experiment with various schedulers and parametrizations of the model. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref> show relatively fast sampling techniques which work fine for image sampling. However, in the case of video, we reach larger sampling times as we sample 64 frames at once. We therefore settled for the EDM <ref type="bibr" target="#b12">[13]</ref>, which presents a method to sample from the model in much fewer steps, largely reducing sampling times. We do not observe any particular speed-up in training and would argue, from our experience, that the v-parametrization <ref type="bibr" target="#b31">[32]</ref> converges faster. We experimentally find our models to behave well with parameters close to those suggested in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Data: We use the EchoNet-Dynamic <ref type="bibr" target="#b18">[19]</ref> dataset, a publicly available dataset that consists of 10,030 4-chamber cardiac ultrasound sequences, with a spatial resolution of 112 × 112 pixels. Videos range from 0.7 to 20.0 s long, with frame rates between 18 and 138 frames per second (fps). Each video has 3 channels, although most of them are greyscale. We keep the original data split of EchoNet-Dynamic which has 7465 training, 1288 validation and 1277 testing videos. We only train on the training data, and validate on the validation data. In terms of labels, each video comes with an LVEF score λ ∈ [0, 100], estimated by a trained clinician. At every step of our training process, we pull a batch of videos, which are resampled to 32 fps. For each video, we retrieve its corresponding ground truth LVEF as well as a random frame. After that, the video is truncated or padded to 64 frames, in order to last 2 s, which is enough to cover any human heartbeat. The randomly sampled frame is sampled from the same original video as the 64-frames sample, but may not be contained in those 64 frames, as it may come from before or after that sub-sample.</p><p>Architectural Variants: We define three sets of models, and present them in details in Table <ref type="table">1</ref> of the Appendix. We call the models X -Stage Cascaded Models (X SCM) and present the models' parameters at every stage. Every CDM starts with a Base diffusion model that is conditioned on the LVEF and one conditional frame. The subsequent models perform either temporal super resolution (TSR), spatial super resolution (SSR) or temporal and spatial super resolution (TSSR). TSR, SSR and TSSR models receive the same conditioning inputs as the Base model, along with the output of the previous-stage model. Note that <ref type="bibr" target="#b6">[7]</ref> does not mention TSSR models and <ref type="bibr" target="#b29">[30]</ref> states that extending an SSR model to perform simultaneous temporal and spatial up-sampling is too challenging.</p><p>Training: All our models are trained from scratch on individual cluster nodes, each with 8 × NVIDIA A100. We use a per-GPU batch size of 4 to 8, resulting in batches of 32 to 64 elements after gradient accumulation. The distributed training is handled by the accelerate library from HuggingFace. We did not see any speed-up or memory usage reduction when enabling mixed precision and thus used full precision. As pointed out by <ref type="bibr" target="#b8">[9]</ref> all models in a CDM can be trained in parallel which significantly speeds up experimentation. We empirically find that training with a learning rate up to 5 * 10 -4 is stable and reaches good image quality. We use an Exponential Moving Average (EMA) copy of our model to smooth out the training. We train all our models' stages for 48h, i.e., the 2SCM and 4SCM CDMs are proportionally more costly to train than the 1SCM. As noted by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> training on images and videos improves the overall image quality. As our dataset only consists of videos, we simply deactivate the time attention layers in the Unet with a 25% chance during training, for all models.</p><p>Results: We evaluate our models' video synthesis capabilities on two objectives: LVEF accuracy (R 2 , MAE, RMSE) and image quality (SSIM, LPIPS, FID, FVD). We formulate the task as counterfactual modelling, where we set (1) a random conditioning frame as confounder, (2) the ground-truth LVEF as a factual conditioning, and (3) a random LVEF in the physiologically plausible range from 15% to 85% as counterfactual conditioning. For each ground truth video, we sample three random starting noise samples and conditioning frames. We use the LVEF regression model to create a feedback loop, following what <ref type="bibr" target="#b20">[21]</ref> did, even though their model was run 100× per sample instead of 3×. For each ground truth video, we keep the sample with the best LVEF accuracy to compute all our scores over 1288 videos for each model.</p><p>The results in Table <ref type="table">1</ref> show that increasing the frame rate improves model fidelity to the given LVEF, while adding more models to the cascade decreases image quality. This is due to a distribution gap between true low-resolution samples and sequentially generated samples during inference. This issue is partially addressed by adding noise to real low-resolution samples during training, but the 1SCM model with only one stage still achieves better image quality metrics. However, the 2SCM and 4SCM models perform equally well on LVEF metrics and outperform the 1SCM model thanks to their higher temporal resolution that precisely captures key frames of the heartbeat. The TSSR model, used in the 2SCM, yields the best compromise between image quality, LVEF accuracy, and sampling times, and is compared to previous literature.</p><p>We outperform previous work for LVEF regression: counterfactual video generation improves with our method by a large margin of 38 points for the R 2 score as shown in Table <ref type="table" target="#tab_1">2</ref>. The similarity between our factual and counterfactual results show that our time-agnostic confounding factor (i.e. an image instead of a video) prevents entanglement, as opposed to the approach taken in <ref type="bibr" target="#b20">[21]</ref>. Our method does not score as high for SSIM as global image similarity metric, which is expected because of the stochasticity of the speckle noise. In <ref type="bibr" target="#b20">[21]</ref> this was mitigated by their data-rich confounder. Our results also match other video diffusion models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref> as structure is excellent, while texture tends to be more noisy as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Table <ref type="table">1</ref>. Metrics for all CDMs. The Gen. task is the counterfactual generation comparable to <ref type="bibr" target="#b20">[21]</ref>, the Rec. task is the factual reconstruction task. Frames is the number of frames generated by the model, always spanning 2 s. ‡ Videos are temporally upsampled to 64 frames for metric computation. S. time is the sampling time for one video on an RTX A5000. R 2 , MAE and RMSE are computed between the conditional LVEF λc and the regressed LVEF using the model described in Sect. 1. SSIM, LPIPS, FID and FVD are used to quantify the image quality. LPIPS is computed with VGG <ref type="bibr" target="#b28">[29]</ref>, FID <ref type="bibr" target="#b5">[6]</ref> and FVD <ref type="bibr" target="#b35">[36]</ref>    Qualitative Study: We asked three trained clinicians (Consultant cardiologist &gt; 10 years experience, Senior trainee in cardiology &gt; 5 years experience, Chief cardiac physiologist &gt; 15 years experience) to classify 100 samples, each, as real or fake. Experts were not given feedback on their performance during the evaluation process and were not shown fake samples beforehand. All samples were generated with the 1SCM model or were true samples from the EchoNet-Dynamic dataset, resampled to 32fps and 2 s. The samples were picked by alphabetical order from the validation set. Among the 300 samples evaluated, 130 (43.33%) were real videos, 89 (29.67%) were factual generated videos, and 81 (27.0%) were counterfactual generated videos. The average expert accuracy was 54.33%, with an inter-expert agreement of 50.0%. More precisely, experts detected real samples with an accuracy of 63.85%, 50.56% for factual samples and 43.21% for the counterfactual samples. The average time taken to evaluate each sample was 16.2 s. We believe that these numbers show the video quality that our model reaches, and can put in perspective the SSIM scores from Table <ref type="table">1</ref>.</p><p>Downstream Task: We train our LVEF regression model on rebalanced datasets and resampled datasets. We rebalance the datasets by using our 4SCM model to generate samples for LVEF values that have insufficient data. The resampled datasets are smaller datasets randomly sampled from the real training set. We show that, in small data regimes, using generated data to rebalance the dataset improves the overall performance. Training on 790 real data samples yields an R 2 score of 56% while the rebalanced datasets with 790 samples, ∼50% of which are real, reaches a better 59% on a balanced validation set. This observation is mitigated when more data is available. See Appendix Table <ref type="table" target="#tab_1">2</ref> for all our results.</p><p>Discussion: Generating echocardiograms is a challenging task that differs from traditional computer vision due to the noisy nature of US images and videos. However, restricting the training domain simplifies certain aspects, such as not requiring a long series of CDMs to reach the target resolution of 112 × 112 pixels and limiting samples to 2 s, which covers any human heartbeat. The limited pixel-intensity space of the data also allows for models with fewer parameters.</p><p>In the future, we plan to explore other organs and views within the US domain, with different clinical conditionings and segmentation maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our application of EDMs to US video generation achieves state-of-the-art performance on a counterfactual generation task, a data augmentation task, and a qualitative study by experts. This significant advancement provides a valuable solution for downstream tasks that could benefit from representative foundation models for medical imaging and precise medical video generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Summarized view of our Model. Inputs (blue): a noised sample xi, a diffusion step ti, one anatomy image Ic, and one LVEF λc. Output (red): a slightly denoised version of xi named xi+1. See Appendix Fig. 1 for more details. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Top: Ground truth frames with 29.3% LVEF. Middle: Generated factual frames, with estimated 27.9% LVEF. Bottom: Generated counterfactual frames, with estimated 64.0% LVEF. (Counter-)Factual frames are generated with the 1SCM, conditioned on the ground-truth anatomy.</figDesc><graphic coords="8,125,04,202,22,270,40,106,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>with I3D. FID and FVD are computed over padded frames of 128 × 128 pixels.</figDesc><table><row><cell cols="5">Model Task Res. Frames S. time R 2 ↑ MAE↓ RMSE↓ SSIM↑ LPIPS↓ FID↓ FVD↓</cell></row><row><cell>1SCM Gen. 112 16  ‡</cell><cell>62 s  ‡</cell><cell>0.64 9.65 12.2</cell><cell>0.53 0.21</cell><cell>12.3 60.5</cell></row><row><cell>2SCM Gen. 112 64</cell><cell cols="2">146 s 0.89 4.81 6.69</cell><cell>0.53 0.24</cell><cell>31.7 141</cell></row><row><cell>4SCM Gen. 112 64</cell><cell cols="2">279 s 0.93 3.77 5.26</cell><cell>0.48 0.25</cell><cell>24.6 230</cell></row><row><cell>1SCM Rec. 112 16  ‡</cell><cell cols="2">62 s  ‡ 0.76 4.51 6.07</cell><cell>0.53 0.21</cell><cell>13.6 89.7</cell></row><row><cell>2SCM Rec. 112 64</cell><cell cols="2">146 s 0.93 2.22 3.35</cell><cell>0.54 0.24</cell><cell>31.4 147</cell></row><row><cell>4SCM Rec. 112 64</cell><cell cols="2">279 s 0.90 2.42 3.87</cell><cell>0.48 0.25</cell><cell>24.0 228</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of our 2SCM model to previous work. We try to reconstruct a ground truth video or to generate a new one. Our model is conditioned on a single frame and an LVEF, while<ref type="bibr" target="#b20">[21]</ref> conditions on the entire video and an LVEF. In both cases the LVEF is either the ground truth LVEF (Rec.) or a randomly sampled LVEF (Gen.).</figDesc><table><row><cell cols="2">Dartagnan [21] Video+EF</cell><cell>Gen. ∼1 s</cell><cell>0.51 15.7</cell><cell>18.4</cell><cell>0.79</cell></row><row><cell>2SCM</cell><cell>Image+EF</cell><cell>Gen. 146 s</cell><cell>0.89 4.81</cell><cell>6.69</cell><cell>0.53</cell></row><row><cell cols="2">Dartagnan [21] Video+EF</cell><cell>Rec. ∼1 s</cell><cell>0.87 2.79</cell><cell>4.45</cell><cell>0.82</cell></row><row><cell>2SCM</cell><cell>Image+EF</cell><cell>Rec. 146 s</cell><cell>0.93 2.22</cell><cell>3.35</cell><cell>0.54</cell></row></table><note><p><p><p>Method</p>Conditioning Task S. time</p>R 2 ↑ MAE ↓ RMSE ↓ SSIM ↑</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">Ultromics Ltd.</rs>, the <rs type="affiliation">UKRI Centre for Doctoral Training in Artificial Intelligence for Healthcare</rs> (<rs type="grantNumber">EP/S023283/1</rs>) and HPC resources provided by the <rs type="funder">Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)</rs> under the <rs type="projectName">NHR</rs> project <rs type="grantNumber">b143dc</rs>. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the <rs type="funder">German Research Foundation (DFG) -440719683</rs>. We thank <rs type="person">Phil Wang</rs> (https://github.com/lucidrains) for his open source implementation of [7]. Support was also received from the <rs type="funder">ERC</rs> -project <rs type="projectName">MIA-NORMAL 101083647</rs> and <rs type="grantNumber">DFG KA 5801/2-1, INST 90/1351-1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AeksSZ2">
					<idno type="grant-number">EP/S023283/1</idno>
				</org>
				<org type="funded-project" xml:id="_EEgW4Xk">
					<idno type="grant-number">b143dc</idno>
					<orgName type="project" subtype="full">NHR</orgName>
				</org>
				<org type="funded-project" xml:id="_F9SvEPk">
					<idno type="grant-number">DFG KA 5801/2-1, INST 90/1351-1</idno>
					<orgName type="project" subtype="full">MIA-NORMAL 101083647</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_14.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Stochastic variational video prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11252</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13195</idno>
		<title level="m">FitVid: overfitting in pixel-level video prediction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Structure and content-guided video synthesis with diffusion models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Atighehchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Granskog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Germanidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03011</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MaskViT: masked visual pre-training for video prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11894</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Imagen video: high definition video generation with diffusion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02303</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03458</idno>
		<title level="m">Video diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simulation of advanced ultrasound systems using Field II</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="636" to="639" />
		</imprint>
	</monogr>
	<note>IEEE Cat No. 04EX821</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Elucidating the design space of diffusionbased generative models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00364</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">VideoFlow: a conditional flow-based model for stochastic video generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01434</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatio-temporal nonrigid registration for ultrasound cardiac motion estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ledesma-Carbayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1113" to="1126" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102461</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly-supervised high-fidelity ultrasound video synthesis with feature decoupling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_30" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="310" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">EchoGNN: explainable ejection fraction estimation with graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_35" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video-based AI for beat-to-beat assessment of cardiac function</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">580</biblScope>
			<biblScope unit="page" from="252" to="256" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">D&apos;ARTAGNAN: counterfactual video generation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Reynaud</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_57" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="599" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ultrasound video transformers for cardiac ejection fraction estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Reynaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlontzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beqiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leeson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_48" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="495" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10752</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Patient-specific 3D ultrasound simulation based on convolutional ray-tracing and appearance optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24571-3_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24571-3_61" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9350</biblScope>
			<biblScope unit="page" from="510" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00512</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">4D XCAT phantom for multimodality imaging research</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Segars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sturgeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M W</forename><surname>Tsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4902" to="4915" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time simulation of medical ultrasound from CT images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-85990-1_88</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-85990-1_88" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2008</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Axel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Székely</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5242</biblScope>
			<biblScope unit="page" from="734" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Make-a-video: text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">U</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<title level="m">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Talking face generation by conditional recurrent adversarial network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04786</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interactive translation in echocardiography training system with enhanced cycle-GAN</title>
		<author>
			<persName><forename type="first">L</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="106147" to="106156" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Content-preserving unpaired translation from simulated to realistic ultrasound images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_63" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="659" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FVD: a new metric for video generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 Workshop: Deep Generative Models for Highly Structured Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Phenaki: variable length video generation from open domain textual description</title>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02399</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Diffusion probabilistic modeling for video generation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09481</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
