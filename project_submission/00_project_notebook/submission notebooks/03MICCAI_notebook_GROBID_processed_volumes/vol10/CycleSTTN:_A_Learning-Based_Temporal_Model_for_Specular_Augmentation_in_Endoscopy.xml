<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy</title>
				<funder ref="#_WXJmhEP #_bW9zREr #_PEUqjJt">
					<orgName type="full">Engineering and Physical Sciences Research Council</orgName>
					<orgName type="abbreviated">EPSRC</orgName>
				</funder>
				<funder ref="#_ACAq5nU">
					<orgName type="full">Royal Academy of Engineering Chair in Emerging Technologies Scheme</orgName>
				</funder>
				<funder ref="#_Mu6TW4E">
					<orgName type="full">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
					<orgName type="abbreviated">WEISS</orgName>
				</funder>
				<funder ref="#_fmxg8Cs">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rema</forename><surname>Daher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">O</forename><surname>León Barbed</surname></persName>
							<idno type="ORCID">0000-0001-8191-6261</idno>
							<affiliation key="aff1">
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
							<idno type="ORCID">0000-0002-7580-9037</idno>
							<affiliation key="aff1">
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francisco</forename><surname>Vasconcelos</surname></persName>
							<idno type="ORCID">0000-0002-4609-1177</idno>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
							<email>danail.stoyanov@ucl.ac.uk</email>
							<idno type="ORCID">0000-0002-0980-3227</idno>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ai</forename><forename type="middle">•</forename><surname>Surgical</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Generative</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Endoscopy</forename><forename type="middle">•</forename><surname>Specularity</surname></persName>
						</author>
						<title level="a" type="main">CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4EC4BB2A46AD1902A0F70BCE9B8C30E0</idno>
					<idno type="DOI">10.1007/978-3-031-43999-554.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Surgical Data Science</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature detection and matching is a computer vision problem that underpins different computer assisted techniques in endoscopy, including anatomy and lesion recognition, camera motion estimation, and 3D reconstruction. This problem is made extremely challenging due to the abundant presence of specular reflections. Most of the solutions proposed in the literature are based on filtering or masking out these regions as an additional processing step. There has been little investigation into explicitly learning robustness to such artefacts with single-step end-to-end training. In this paper, we propose an augmentation technique (CycleSTTN) that adds temporally consistent and realistic specularities to endoscopic videos. Such videos can act as ground truth data with known texture occluded behind the added specularities. We demonstrate that our image generation technique produces better results than a standard CycleGAN model. Additionally, we leverage this data augmentation to re-train a deep-learning based feature extractor (SuperPoint) and show that it improves. CycleSTTN code is made available here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During endoscopic procedures, such as colonoscopy, the camera light source produces abundant specular highlight reflections on the visualised anatomy. This is due to its very close proximity to the scene coupled with the presence of wet tissue. These reflections can occlude texture and produce salient artifacts, which may reduce the accuracy of surgical vision algorithms aiming at scene understanding, including depth estimation and 3D reconstruction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>. To resolve these challenges, a simple solution is to detect and mask out these regions before performing any other downstream visual task. However, this approach comes with limitations. In the context of sparse feature point detection, simply discarding points falling on specularities results in excessive filtering, often leading to an insufficient number of detected features. As for dense estimation problems, such as pixel depth regression and optical flow, masking out specular regions makes interpolation necessary.</p><p>Alternatively, videos can be pre-processed to inpaint specular highlights with its hidden texture inferred from neighbouring frames <ref type="bibr" target="#b6">[7]</ref>. This allows running other algorithms with reflection-free data. However, this adds a significant computational overhead and requires the processing of temporal frame windows that restrict online inference applications. Given that state-of-the-art feature detection methods are deep learning models, an appealing approach would be to learn robustness to reflections during training, as this would result in an single-step end-to-end inference model without any pre or post-processing overhead.</p><p>In this paper, we propose to learn robustness to specular reflections via data augmentation. We use a CycleGAN <ref type="bibr" target="#b23">[24]</ref> methodology that takes advantage of a pre-trained specular highlight removal network in adversarial training. Our proposed generator network, based on STTN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>, performs video-to-video translation. In doing so, we create a cycle structure for STTN, that we call CycleSTTN. To demonstrate the effectiveness of our approach, we use it to improve the performance of the SuperPoint feature detector/descriptor. We combine the proposed method with that of <ref type="bibr" target="#b6">[7]</ref> to add and remove specularities as data augmentation. The contributions of this paper can be summarised as:</p><p>-We propose the CycleSTTN training pipeline as an extension of STTN to a cyclic structure. -We use CycleSTTN to train a model for synthetic generation of temporally consistent and realistic specularities in endoscopy videos. We compare results of our method against CycleGAN. -We demonstrate CycleSTTN as a data augmentation technique that improves the performance of SuperPoint feature detector in endoscopy videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Barbed et al. investigated the challenge of specular reflections when performing feature detection in endoscopy images <ref type="bibr" target="#b2">[3]</ref>. As a solution, they design a loss function to explicitly avoid specular regions and focus on detecting points in the remaining parts of the image. However, this ignores the fact that features extracted from these points will still be contaminated by specularity pixels in their neighborhood. Following a different strategy, we propose data augmentation as a way to induce specularity robustness in both point detection and feature extraction. Data augmentation for lighting conditions in surgical data has been explored extensively before. Classical techniques for modelling specular highlights include the use of a parametric model to add specularities and illumination to real images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>. Another technique for augmentation uses synthetic data or phantoms, where different light sources and environment variables can be captured. However, synthetic data lacks real textures and artifacts. Thus many image-toimage translation techniques have been developed to add more realistic textures to synthetic data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. These methods rely on CycleGAN <ref type="bibr" target="#b23">[24]</ref> for unsupervised learning and thus, are able to map from real to synthetic domain and vice-versa. This allows for the generation of real images with the same structure but different lighting, texture, and blurriness. To have more control over the augmentations, some methods add a controllable noise vector to the network input that modifies the image lighting, specular reflections, and texture. However, this vector does not have a physical meaning, and it is thus challenging to independently control the different environment variables by directly manipulating its values. To address this, <ref type="bibr" target="#b13">[14]</ref> uses two separate noise inputs, one controlling texture and specular reflection and another controlling color and light. However, all these approaches still use multiple steps to finally create new real data, which might lead to loss of important information in the process.</p><p>Single-step approaches have also been developed that augment real data directly, but the generated images have different structures and thus, do not create paired data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. A CycleGAN model has been proposed to map from images with specularities to images without specularities <ref type="bibr" target="#b10">[11]</ref> using manually labelled patches cropped from frames, however, this work focuses on specular highlight removal, and does not test the data augmentation capabilities of generating synthetic specularities. In <ref type="bibr" target="#b11">[12]</ref> a classification model is used to categorize data for unpaired training of CycleGAN. From the output of CycleGAN, they generate a paired dataset, however, only quality metrics are used to filter out images in the generated paired dataset. Furthermore, most of these approaches are applicable to single frames and are not able to generate synthetic videos with temporally consistent specularities. While some other methods use a temporal component for endoscopic video augmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>, they do not have a single-step structure and have not been applied to generate/remove specular highlights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We use the STTN model as our video-to-video translation architecture and T-PatchGAN <ref type="bibr" target="#b5">[6]</ref> as the discriminator. STTN contains an encoder followed by a spatio-temporal transformer and a decoder <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>. While STTN was originally proposed to remove occlusions in videos, in this paper, we use two instances of this model, ST T N R and ST T N A , to respectively remove and add specular occlusions. We start by pre-training these models separately using their respective discriminators D R and D A . Then, we continue training them simultaneously in an adversarial manner with a CycleGAN methodology. We denote the complete training pipeline as CycleSTTN (Fig. <ref type="figure" target="#fig_1">1</ref>), which is divided into 3 sections:</p><p>1. Paired Dataset Generation We generate a dataset of paired videos with and without specularities. We train a generator for specularity removal fol-lowing the same methodology as in <ref type="bibr" target="#b6">[7]</ref>. This model is denoted as (ST T N R0 , D R0 ), where ST T N is the generator and D is the discriminator. For a set of real endoscopic videos with specularities V A and specularity masks M , we run ST T N R0 to generate their inpainted counterparts without specularities V R (Fig. <ref type="figure" target="#fig_1">1</ref> -Step 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ST T N A Pre-training</head><p>Using the paired dataset (V A , V R ) we train a new model to add specularities. We denote it as ST T N A0 with D A0 as its discriminator. This is shown in Fig. <ref type="figure" target="#fig_1">1</ref> as step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ST T N R , ST T N A Joint Training By initializing with the models from</head><p>Step  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Inputs</head><p>The STTN architecture receives as input a paired sequence of frames and masks. Originally masks are meant to represent occluded image regions that should be inpainted, however, in this work, we do not always use them in this way.</p><p>When training ST T N R we define the mask inputs as regions to be inpainted (to remove specularities). However, when training ST T N A , input masks are set to 1 for all pixels, since we do not want to enforce specific locations for specularity generation; we want to encourage the model to learn these patterns from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Losses</head><p>The loss function of the T-PatchGAN discriminators are shown below, such that E is the expected value of the data distributions as done in <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_0">L DR = E VR∼p(VR) [ReLU (1 -D R (V R ))] + E F ake R ∼p(F ake R ) [ReLU (1 + D R (F ake R ))]</formula><p>(1)</p><formula xml:id="formula_1">L DA = E VA∼p(VA) [ReLU (1 -D A (V A ))] + E F akeA∼p(F akeA) [ReLU (1 + D A (F ake A ))]<label>(2)</label></formula><p>where F ake A = ST T N A (V R ) represents fake videos with added specularities, and analogously F ake R = ST T N R (V A ) represents fake videos with removed specularities. Further, we also define F ake R = M.F ake R + V A (1 -M ) for the discriminator loss, where inpainted occluded regions from F ake R are overlaid over V A . M denotes masks with 1 values in specular regions of V A and 0 otherwise. For the generators, an adversarial loss was used as done in <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_2">L advR = -E F ake R ∼p(F ake R ) [D R (F ake R )]; L advA = -E F akeA∼p(F akeA) [D A (F ake A )] (3)</formula><p>The identity loss was the only loss modified from the original STTN model <ref type="bibr" target="#b6">[7]</ref>. The identity loss for ST T N R and ST T N A are:</p><formula xml:id="formula_3">L idtR = V R -ST T N R (V R ) 1 ; L idtA = V A -F ake A 1<label>(4)</label></formula><p>Here L idtR ensures that if a video does not have specularities, it would stay the same when fed into the model that removes specularities. Whereas, L idtA ensures predicted videos with specularities, F ake A , resemble real specular videos V A .</p><p>Finally, we added cycle loss terms:</p><formula xml:id="formula_4">L cR = E VA∼p(VA) [ V A -ST T N A (F ake R ) 1 ]; L cA = E VR∼p(VR) [ V R -ST T N R (F ake A ) 1 ] (5)</formula><p>The total generator losses L R and L A for removing and adding specularities are shown below, such that the loss weights λ are all set to 1 except for λ adv , which is set to 0.01 as advised by <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_5">L R = λ adv L advR + λ idt L idtR + λ c L cR ; L A = λ adv L advA + λ idt L idtA + λ c L cA (6)</formula><p>In summary, we adopted the original STTN model <ref type="bibr" target="#b6">[7]</ref> and changed the training pipeline, model inputs, and losses as shown in Fig. <ref type="figure" target="#fig_1">1</ref>. In particular, (a) the training pipeline was transformed into a multi-task one of adding and removing specularities, where ST T N R0 from <ref type="bibr" target="#b6">[7]</ref> was used as an initialization model. (b) For ST T N A , specularity masks were removed from model inputs. (c) Identity losses and cycle losses were also added while masked based losses were removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Parameters</head><p>To evaluate our pipeline, we use 373 videos from the Hyper Kvasir dataset <ref type="bibr" target="#b3">[4]</ref> to generate our paired dataset (V R , V A ) as described in Sect. 3 and Fig. <ref type="figure" target="#fig_1">1</ref>. 343 video pairs were used for training and 30 for testing with an upper limit of 927 frames per video. Models were trained on NVIDIA A100-SXM4-40GB GPUs. We use the same training parameters as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref> with the exception of batch size, which we changed from 8 to 3 for training (ST T N A1 , ST T N R1 ). CycleGAN models were trained with suggested parameters in CycleGAN's public repository <ref type="foot" target="#foot_0">1</ref> , with the exception of batch size, which was changed from 1 to 3.</p><p>In our experimental analysis, we use our proposed models shown in Fig. <ref type="figure" target="#fig_1">1</ref> along with CycleGAN models, which use ResNet with 9 residual blocks as the generator. All these models are listed in Table <ref type="table" target="#tab_0">1</ref>. We note that even though ST T N R1 was trained with masks, it seems it was affected by the cycle loss and was only able to give decent results without a mask as input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pseudo Evaluation Experiments</head><p>We input the pseudo ground truth V R to our models, ST T N A0 and ST T N A1 . We compare the output F ake A to real videos V A . We conduct non-temporal testing by using single frame inputs, as opposed to video inputs, to demonstrate the temporal effect. We report the Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Square Error (MSE) metrics. We show visual results for different models in Fig. <ref type="figure" target="#fig_2">2</ref>. When V R (videos with no specularities) are used as input, our CycleSTTN model ST T N A1 shows the highest similarity to the ground truth (V A ). With V A (videos with specularities) as input, ST T N A1 is able to add more realistic specularities that flow smoothly from one frame to another. CycleGAN based models were not able to add new specularities with V A as input. For ResN et A0 , this is expected, due to the original CycleGAN identity loss. When changing the identity loss, ResN et A1 only intensifies specularities and darkens the background texture. This was further validated through results shown in Table <ref type="table" target="#tab_1">2</ref>, where ST T N A1 has the best SSIM, PSNR and MSE values. We can also see that ST T N A1 results are only slightly  worse without temporal testing according to PSNR and MSE, yet slightly better according to SSIM. Thus, the temporal component only benefits training as a regularizer (i.e. our models outperforms CycleGAN based models). However, during inference, temporal testing is not necessary since it yields similar results to non-temporal testing.</p><p>The pseudo evaluation shows that generated specularities closely resemble real ones in appearance and location. While not fully enforcing physical realism, this augmentation improves upon traditional warping methods (Sect. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relative Pose Estimation Experiments</head><p>We use our models as data augmentation to re-train the feature detector proposed in <ref type="bibr" target="#b2">[3]</ref>, an adaptation of SuperPoint <ref type="bibr" target="#b7">[8]</ref> to endoscopy. While <ref type="bibr" target="#b2">[3]</ref> is originally trained with a specularity loss term that encourages the network to ignore specularity regions, we omit this term in our training. We want our models to be robust to specularities, rather than just avoiding them. We use the pre-trained model in <ref type="bibr" target="#b2">[3]</ref> to generate data labels and initialize our models. SuperPoint is trained by generating a sparse set of point matches in an image and its warped version via homography. Augmentations already used by SuperPoint include traditional random brightness, contrast, Gaussian noise, speckle noise, blur, and shade. We use our models as augmentations to original and warped images separately by randomly choosing between (no augmentation, specularity addition, and specularity removal). SuperPoint models with various augmentations are listed in Table <ref type="table" target="#tab_2">3</ref>. These models are trained using 131 randomly chosen videos from V A (with cropped boundaries) and 14 for validation for 25,000 iterations with the same training parameters as <ref type="bibr" target="#b2">[3]</ref>. Temporal (non-temporal) refers to feeding an image and its warped version together (separately) to the augmentation model. We evaluate the quality of point detections by using them to estimate relative camera motion in endoscopic sequences. We first apply brute-force matching of detected points in image pairs and then estimate motion via RANSAC <ref type="bibr" target="#b9">[10]</ref>. The test data for this experiment is the same as in <ref type="bibr" target="#b2">[3]</ref>. It includes 6 sequences (14191 frames) from the EndoMapper dataset <ref type="bibr" target="#b1">[2]</ref> with a relative camera motion pseudo ground truth based on structure-from-motion (SFM: COLMAP <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>). Reported metrics include the precision of inlier points from RANSAC-estimated (threshold = 10 px) essential matrices as compared to inlier points using pseudo ground truth essential matrix (from COLMAP). To generate the pseudo ground truth inliers, the same distance metric used in RANSAC was applied to the pseudo ground truth essential matrix (from COLMAP). We also report the Rota-tion error, which is the geodesic angle in degrees between the RANSAC-based pose estimation and the pseudo ground truth pose (from COLMAP).</p><p>In Table <ref type="table" target="#tab_2">3</ref>, all specularity augmentations (models 2-15) improve Super-Point relative to not using them (model 1), which demonstrates their usefulness. Most STTN-based augmentations (models 2-9) produce better results than CycleGAN-based ones (models 10-15). Overall, the best performing augmentation is (ST T N A1 , ST T N R0 ), showing the effectiveness of our system. This makes sense since the best removal and addition models are ST T N R0 and ST T N A1 according to the rotation error. However, it appears that non-temporal testing sometimes gives lower rotation errors than temporal testing. This could be due to the unrealistic nature of warped images used as consecutive frames. As also discussed in Sect. 4.2, temporal testing does not improve results, only temporal training does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In conclusion, we introduce CycleSTTN, a temporal CycleGAN applied to generate temporally consistent and realistic specularities in endoscopy. Our model outperforms CycleGAN, as demonstrated by mean PSNR, MSE, and SSIM metrics using a pseudo ground truth dataset. We also observe a positive effect of our model as augmentation for training a feature extractor, resulting in improved inlier precision and rotation errors. However, our evaluation relies on SFM generated ground truth, different testing and training datasets, and indirect metrics, which may introduce some uncertainty. Nevertheless, augmentation shows great promise as an addition for training various endoscopic computer vision tasks to enhance performance and provide insights into the impact of specific artifacts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1 and 2 ((ST T N R0 , D R0 ) and (ST T N A0 , D A0 )), we continue training ST T N R and ST T N A simultaneously in an adversarial manner with a CycleGAN methodology. We denote the final removal and addition models as (ST T N R1 , D R1 ) and (ST T N A1 , D A1 ), respectively. This is also shown in Fig. 1 as step 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. CycleSTTN training pipeline with 3 main steps: 1 Paired Dataset Generation, 2 ST T NA Pre-training, and 3 ST T NR, ST T NA Joint Training.</figDesc><graphic coords="4,55,98,254,03,340,18,133,63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Sample consecutive video frames from the model output using pseudo ground truth VR as input in columns 1-4 and real videos VA as input in columns 5-8.</figDesc><graphic coords="7,41,79,53,99,340,21,191,71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Trained models used in our analysis input type and training iterations.</figDesc><table><row><cell>Proposed</cell><cell>Input</cell><cell cols="2">Iterations CycleGAN</cell><cell>Identity Loss Input Iterations</cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell>Models</cell><cell></cell></row><row><cell>ST T NR0</cell><cell>videos +</cell><cell>90,000</cell><cell>(ResN etA0,</cell><cell>Original [24] videos 285,600</cell></row><row><cell></cell><cell>masks</cell><cell></cell><cell>ResN etR0)</cell><cell></cell></row><row><cell>ST T NA0</cell><cell>videos</cell><cell>30,000</cell><cell>(ResN etA1,</cell><cell>L idt A -Eq. 4 videos 285,600</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResN etR1)</cell><cell></cell></row><row><cell>(ST T NA1,</cell><cell>videos</cell><cell>20,000</cell><cell></cell><cell></cell></row><row><cell>ST T NR1)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Mean SSIM, PSNR, and MSE values for model output videos using pseudo ground truth VR as input. The output is compared to real videos VA.</figDesc><table><row><cell></cell><cell cols="3">Non-Temporal Testing</cell><cell></cell><cell cols="3">Temporal Testing</cell></row><row><cell>Method</cell><cell>SSIM</cell><cell>PSNR</cell><cell>MSE</cell><cell cols="2">SSIM</cell><cell cols="2">PSNR</cell><cell>MSE</cell></row><row><cell></cell><cell cols="7">Mean Std Mean Std Mean Std Mean Std Mean Std Mean Std</cell></row><row><cell cols="8">ST T NA0 0.802 0.040 26.20 2.31 231 98.0 0.808 0.039 26.33 2.37 226 91.9</cell></row><row><cell cols="8">ST T NA1 0.826 0.036 26.38 2.43 224 102.5 0.824 0.036 26.49 2.45 219 91.4</cell></row><row><cell cols="4">ResN etA0 0.792 0.041 21.29 1.92 675 311</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">ResN etA1 0.780 0.044 21.48 2.03 666 329</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Pose estimation analysis for 12 SuperPoint models each trained with different specular augmentations. Metrics are described in Sect. 4.3.</figDesc><table><row><cell></cell><cell cols="2">Specularity Augmentation Models</cell><cell></cell><cell cols="3">Non-Temporal Rot. error</cell><cell></cell><cell cols="3">Temporal Rot. error</cell></row><row><cell></cell><cell>Addition</cell><cell>Removal</cell><cell cols="8">Matches Precision Mean Median Matches Precision Mean Median</cell></row><row><cell>1</cell><cell>-</cell><cell>-</cell><cell>368.6</cell><cell>25.0</cell><cell>24.8</cell><cell>12.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>2</cell><cell>-</cell><cell>ST T N R0</cell><cell>537.4</cell><cell>29.2</cell><cell>21.4</cell><cell>10.1</cell><cell>538.2</cell><cell>29.2</cell><cell>21.2</cell><cell>9.8</cell></row><row><cell>3</cell><cell>ST T N A0</cell><cell>ST T N R0</cell><cell>402.1</cell><cell>27.8</cell><cell>21.7</cell><cell>10.7</cell><cell>404.2</cell><cell>27.5</cell><cell>22.0</cell><cell>10.6</cell></row><row><cell>4</cell><cell>ST T N A0</cell><cell>-</cell><cell>398.3</cell><cell>26.7</cell><cell>23.7</cell><cell>11.4</cell><cell>390.1</cell><cell>26.5</cell><cell>22.8</cell><cell>11.1</cell></row><row><cell>5</cell><cell>-</cell><cell>ST T N R1</cell><cell>373.3</cell><cell>26.0</cell><cell>22.6</cell><cell>10.3</cell><cell>542.8</cell><cell>28.6</cell><cell>23.4</cell><cell>11.1</cell></row><row><cell>6</cell><cell>ST T N A1</cell><cell>ST T N R1</cell><cell>360.3</cell><cell>27.3</cell><cell>21.3</cell><cell>9.6</cell><cell>548.5</cell><cell>28.7</cell><cell>23.1</cell><cell>11.0</cell></row><row><cell>7</cell><cell>ST T N A1</cell><cell>-</cell><cell>386.3</cell><cell>27.2</cell><cell>21.5</cell><cell>10.1</cell><cell>542.5</cell><cell>28.3</cell><cell>23.5</cell><cell>11.2</cell></row><row><cell>8</cell><cell>ST T N A1</cell><cell>ST T N R0</cell><cell>541.5</cell><cell>29.9</cell><cell>20.4</cell><cell>9.5</cell><cell>543.6</cell><cell>29.8</cell><cell>20.2</cell><cell>9.6</cell></row><row><cell>9</cell><cell>ST T N A0</cell><cell>ST T N R1</cell><cell>536.6</cell><cell>29.2</cell><cell>21.4</cell><cell>10.0</cell><cell>526.7</cell><cell>29.0</cell><cell>21.1</cell><cell>9.7</cell></row><row><cell>10</cell><cell>-</cell><cell>ResN et R0</cell><cell>571.1</cell><cell>27.6</cell><cell>22.0</cell><cell>10.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">11 ResN et A0</cell><cell>ResN et R0</cell><cell>531.0</cell><cell>26.8</cell><cell>22.5</cell><cell>10.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">12 ResN et A0</cell><cell>-</cell><cell>529.4</cell><cell>27.3</cell><cell>22.6</cell><cell>10.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>13</cell><cell>-</cell><cell>ResN et R1</cell><cell>571.6</cell><cell>28.2</cell><cell>22.7</cell><cell>10.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">14 ResN et A1</cell><cell>ResN et R1</cell><cell>548.4</cell><cell>29.0</cell><cell>22.2</cell><cell>10.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">15 ResN et A1</cell><cell>-</cell><cell>527.3</cell><cell>29.0</cell><cell>22.4</cell><cell>10.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/pytorch0.3.1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>. This research was funded in part, by the <rs type="funder">Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)</rs> [<rs type="grantNumber">203145/Z/16/Z</rs>]; the <rs type="funder">Engineering and Physical Sciences Research Council (EPSRC)</rs> [<rs type="grantNumber">EP/P027938/1</rs>, <rs type="grantNumber">EP/R004080/1</rs>, <rs type="grantNumber">EP/P012841/1</rs>]; the <rs type="funder">Royal Academy of Engineering Chair in Emerging Technologies Scheme</rs>; <rs type="grantNumber">H2020 FET (GA863146</rs>); and the <rs type="institution">UCL Centre for Digital Innovation</rs> through the <rs type="institution">Amazon Web Services (AWS)</rs> <rs type="grantName">Doctoral Scholarship in Digital Innovation</rs> <rs type="grantNumber">2022/2023</rs>. For the purpose of open access, the author has applied a CC BY public copyright licence to any author accepted manuscript version arising from this submission.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Mu6TW4E">
					<idno type="grant-number">203145/Z/16/Z</idno>
				</org>
				<org type="funding" xml:id="_WXJmhEP">
					<idno type="grant-number">EP/P027938/1</idno>
				</org>
				<org type="funding" xml:id="_bW9zREr">
					<idno type="grant-number">EP/R004080/1</idno>
				</org>
				<org type="funding" xml:id="_PEUqjJt">
					<idno type="grant-number">EP/P012841/1</idno>
				</org>
				<org type="funding" xml:id="_ACAq5nU">
					<idno type="grant-number">H2020 FET (GA863146</idno>
					<orgName type="grant-name">Doctoral Scholarship in Digital Innovation</orgName>
				</org>
				<org type="funding" xml:id="_fmxg8Cs">
					<idno type="grant-number">2022/2023</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An automatic framework for endoscopic image restoration and enhancement</title>
		<author>
			<persName><forename type="first">M</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1959" to="1971" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Azagra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14240</idno>
		<title level="m">Endomapper dataset of complete calibrated endoscopy procedures</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Superpoint features in endoscopy</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Barbed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chadebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morlana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-21083-9_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-21083-95" />
	</analytic>
	<monogr>
		<title level="m">GRAIL 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Manfredi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13754</biblScope>
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperkvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Borgli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Artificial intelligence and automation in endoscopy and surgery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chadebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Lovat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Gastroenterol. Hepatol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="182" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Free-form video inpainting with 3D gated convolution and temporal PatchGAN</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9066" to="9075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A temporal learning approach to inpainting endoscopic specularities and its effect on image correspondence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Daher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.17013</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Superpoint: self-supervised interest point detection and description</title>
		<author>
			<persName><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="224" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Endovae: generating endoscopic images with a variational autoencoder</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Diamantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gatoula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Iakovidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for specular highlight removal in endoscopic images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Riediger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image-Guided Procedures, Robotic Interventions, and Modeling</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">10576</biblScope>
			<biblScope unit="page" from="8" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A novel hybrid endoscopic dataset for evaluating machine learning-based photometric image enhancement models</title>
		<author>
			<persName><forename type="first">A</forename><surname>García-Vega</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19493-1_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19493-122" />
	</analytic>
	<monogr>
		<title level="m">MICAI 2022</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Pichardo Lagunas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Martínez-Miranda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Martínez Seis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13612</biblScope>
			<biblScope unit="page" from="267" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Impact of endoscopic image degradations on LBP based features using one-class SVM for classification of celiac disease</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hegenbart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vécsei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 7th International Symposium on Image and Signal Processing and Analysis (ISPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="715" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CLTS-GAN: color-lighting-texture-specular reflection augmentation for colonoscopy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-149" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="519" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Augmenting colonoscopy using extended and directional cyclegan for lossy image translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4696" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Endoslam dataset and an unsupervised monocular visual odometry and depth estimation approach for endoscopic videos</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Ozyoruk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102058</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long-term temporally consistent unpaired video translation from simulated surgical 3D data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rivoir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3343" to="3353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-9_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46487-931" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Assisting barrett&apos;s esophagus identification using endoscopic data augmentation based on generative adversarial networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>De Souza Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page">104029</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">OfGAN: realistic rendition of synthetic colonoscopy videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_70</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-070" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="732" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic generation of polyp image using depth map for endoscope dataset</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yamane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="page" from="2355" to="2364" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58517-4_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58517-4" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12361</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
