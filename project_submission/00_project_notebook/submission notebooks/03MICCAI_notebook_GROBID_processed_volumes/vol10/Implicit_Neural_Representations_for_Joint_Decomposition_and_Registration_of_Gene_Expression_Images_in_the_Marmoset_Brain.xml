<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain</title>
				<funder ref="#_KVHz875">
					<orgName type="full">Japan Agency for Medical Research and Development AMED</orgName>
				</funder>
				<funder ref="#_hXmN5MF">
					<orgName type="full">Japan Society for the Promotion of Science</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michal</forename><surname>Byra</surname></persName>
							<email>michal.byra@riken.jp</email>
							<affiliation key="aff0">
								<orgName type="department">RIKEN Center for Brain Science</orgName>
								<orgName type="laboratory">Brain Image Analysis Unit</orgName>
								<address>
									<settlement>Wako</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Fundamental Technological Research</orgName>
								<orgName type="institution">Polish Academy of Sciences</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charissa</forename><surname>Poon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">RIKEN Center for Brain Science</orgName>
								<orgName type="laboratory">Brain Image Analysis Unit</orgName>
								<address>
									<settlement>Wako</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomomi</forename><surname>Shimogori</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">RIKEN Center for Brain Science</orgName>
								<orgName type="laboratory">Laboratory for Molecular Mechanisms of Brain Development</orgName>
								<address>
									<settlement>Wako</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Henrik</forename><surname>Skibbe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">RIKEN Center for Brain Science</orgName>
								<orgName type="laboratory">Brain Image Analysis Unit</orgName>
								<address>
									<settlement>Wako</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="645" to="654"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2A08CBF7A67072B06AE5B21EA6E4E0A8</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>brain</term>
					<term>deep learning</term>
					<term>gene expression</term>
					<term>implicit neural representations</term>
					<term>registration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel image registration method based on implicit neural representations that addresses the challenging problem of registering a pair of brain images with similar anatomical structures, but where one image contains additional features or artifacts that are not present in the other image. To demonstrate its effectiveness, we use 2D microscopy in situ hybridization gene expression images of the marmoset brain. Accurately quantifying gene expression requires image registration to a brain template, which is difficult due to the diversity of patterns causing variations in visible anatomical brain structures. Our approach uses implicit networks in combination with an image exclusion loss to jointly perform the registration and decompose the image into a support and residual image. The support image aligns well with the template, while the residual image captures individual image characteristics that diverge from the template. In experiments, our method provided excellent results and outperformed other registration techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image registration is a crucial prerequisite for image comparison, data integration, and group studies in contemporary medical and neuroscience research. In research and clinical settings, pairs of images often show similar anatomical structures but may contain additional features or artifacts, such as specific staining, electrodes, or lesions, that are not present in the other image. This difficulty of finding corresponding structures for automatically aligning images complicates image registration. In this work, we address the challenging problem of the gene expression image registration in the marmoset brain. Brain atlases of gene expression, created using images of brain tissue processed through in situ hybridization (ISH), offer single-cell resolution of spatial gene expression patterns across the entire brain <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. However, accurately quantifying gene expression requires brain image registration to spatially align ISH images to a common atlas space. The diversity of gene expression patterns in ISH images causes variations in visible anatomical brain structures with respect to the template image. ISH microscopy images are also susceptible to tissue processing artifacts, resulting in non-specific staining and tissue deformations.</p><p>Traditional pair-wise image registration methods use optimization algorithms to find the deformation field that maximizes the similarity between a pair of images. While several deep learning methods based on convolutional neural networks (CNNs) have been proposed for calculating the deformation field between two images <ref type="bibr" target="#b2">[3]</ref>, such models typically require large training sets and may suffer from generalization issues when applied to images presenting texture patterns that diverge from the training data. Therefore, classic algorithms, such as Advanced Normalization Tools (ANTs) <ref type="bibr" target="#b0">[1]</ref>, are still preferred as off-the-shelf tools for image registration in neuroscience due to scarce experimental data and the diversity of data acquisition protocols and registration tasks. Recently, implicit neural representations (INRs) have been utilized for image registration in MRI and CT <ref type="bibr">[14,</ref><ref type="bibr">16]</ref>, offering a hybrid approach that connects modern deep learning techniques with per-case optimization as used in classical approaches. INRs are defined on continuous coordinate spaces, making them suitable for registration of images that differ in geometry.</p><p>In this work, we propose a novel INR-based framework well-suited to address the challenging problem of gene expression brain image registration. We associate the registration problem with an image decomposition task. We utilize implicit neural networks to decompose the ISH image into two separate images: a support image and a residual image. The support image corresponds to the part of the ISH image that is well-aligned with the registration template image in respect to the texture. On the contrary, the residual image presents features of the ISH image, such as artifacts or texture patterns (e.g. gene expression), which presumably undermine the registration procedure. The support image is used to improve the deformation field calculations. We also introduce an exclusion loss to encourage clearer separation of the support and residual images. The usefulness of the proposed method is demonstrated using 2D ISH gene expression images of the marmoset brain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Registration with Implicit Networks</head><p>The goal of the pairwise image registration is to determine a spatial transformation that maximizes the similarity between the moving image M and the target fixed template image F . INRs serve as a continuous, coordinate based approximation of the deformation field obtained through a fully connected neural network. In this study, as the backbone for our method, we utilized the standard approach to registration with INRs, as described in <ref type="bibr">[14,</ref><ref type="bibr">16]</ref>. We used a single implicit deformation network D to map 2D spatial coordinates x ∈ [-1, 1] 2 of the moving image M to a displacement vector Δx ∈ R 2 . Next, the transformation field was determined as Φ(x) = x + Δx and the bilinear interpolation algorithm was applied to obtain the corresponding moved image T Φ (M ).</p><p>To train the deformation network, the following loss function based on correlation coefficients was applied to assess the similarity between the moved image T Φ (M ) and the fixed template image F :</p><formula xml:id="formula_0">L cc (F, T Φ (M )) = 1 2N x NCC(F, T Φ (M )) + LNCC(F, T Φ (M )) ,<label>(1)</label></formula><p>where NCC and LNCC stand for the normalized cross-correlation and local normalized cross-correlation based loss functions averaged over the entire image domain consisting of N elements. NCC was used to stabilize the training of the network, while LNCC ensured good local registration results. Additionally, following the standard approach to INR based registration, we regularized the deformation field based on the Jacobian matrix determinant |J Φ(x) | using following equation [16]:</p><formula xml:id="formula_1">L reg (Φ(x)) = 1 N x |1 -|J Φ(x) ||. (<label>2</label></formula><formula xml:id="formula_2">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Registration Guided Image Decomposition</head><p>Our aim is to improve the registration performance associated with the implicit deformation network D. The proposed framework is presented in Fig. <ref type="figure" target="#fig_0">1</ref>. We assume that the moving image M can be decomposed with separate implicit networks, S and R, into two images: the support image M S and the residual image M R . Ideally, the support image should correspond to the part of the moving image that contributes to the registration performance. On the contrary, we expect the residual image to include image artifacts and texture patterns (e.g. ISH gene expression patterns) that diverge from the fixed template image and undermine the registration procedure. We impose the following condition based on the mean squared error loss function for the decomposition of the moving image:</p><formula xml:id="formula_3">L rec (M, M S + M R ) = 1 N x (M -M S -M R ) 2 , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>stating that the support M S and residual M R images should sum up to the moving image M . To ensure that the support image M S contributes to the registration with respect to the fixed image F , we utilize the cross-correlation based loss function L cc (F, T Φ (M S )) (Eq. 1), where T Φ (M S ) stands for the transformed support image M S . Therefore, the deformation network is trained to provide the transformation field Φ(x) both for the moving image and the support image For this, we utilize the following exclusion loss to encourage the gradient structure of the implicit networks S and R to be decorrelated <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_5">L excl (M S , M R ) = 1 N x i,j |Γ (J S (x), J R (x))| (4) Γ (J S (x), J R (x)) = tanh(J S (x)) ⊗ tanh(J R (x)</formula><p>), ⊗ indicates element-wise multiplication and indices i, j go over all elements of the matrix Γ .</p><p>In our framework, we jointly optimize all three implicit networks (D, S and R) using the following composite loss function:</p><formula xml:id="formula_6">Loss = α 1 L cc (F, T Φ (M )) + α 2 L cc (F, T Φ (M S )) + α 3 L reg (Φ(x)) + α 4 L rec (M, M S + M R ) + α 5 L excl (M S , M R ). (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>The first row of Eq. 5 can be perceived as a standard registration loss, while the second row stands for a regularized image reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation</head><p>We designed the proposed method with the aim to address the problem of ISH gene expression image registration. For the evaluation, we used neonate marmoset brain ISH images collected at the Laboratory for Molecular Mechanisms of Brain Development, RIKEN Center for Brain Science, Wako, Japan (geneatlas.brainminds.jp) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">12]</ref>. We prepared manual annotations for 2D images from 50 gene expression datasets. Atlas template images were created using ANTs <ref type="bibr" target="#b0">[1]</ref>, based on semi-automatically aligned sets of 2D ISH images from 1942 gene expression datasets. ISH images used to generate the template were converted to gray-scale to meet ANTs requirements and better highlight brain tissue interfaces.</p><p>Performance of the proposed approach was compared to the SynthMorph network and the ANTs SyN registration algorithm based on mutual information metric, as these two methods do not require pre-training and can serve as off-theshelf registration tools for neuroscience <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. We conducted an ablation study to assess the effectiveness of the proposed representation decomposition approach with and without the exclusion loss. Registration methods were evaluated quantitatively based on Dice scores using manual 2D segmentations prepared for the following five brain structures ranging in size and shape complexity: aqueduct (AQ, 95 masks), hippocampus area (HA, 570 masks), dorsal lateral geniculate (DLG, 370 masks), inferior colliculus (IC, 70 masks) and visual cortex area (VCA, 68 masks). Segmentations were outlined both for the template and ISH 2D images, resulting in 1114 image pairs corresponding to the same brain regions. We also calculated the percentage of the non-positive Jacobian determinant values to assess the deformation field folding. Moreover, we determined the structural similarity index (SSIM) between the moved images and the template fixed images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Implementation</head><p>We utilized sinusoidal representation networks to determine the implicit representations <ref type="bibr">[13]</ref>. Each network contained five fully connected hidden layers with 256 neurons. We used the Fourier mapping with six frequencies to encode the input coordinates <ref type="bibr">[15]</ref>. The coordinates and the encoded coordinates were additionally concatenated within the middle layer of the network. Weights of the networks were initialized following the original paper except for the last linear layer of the deformation network D, for which we uniformly sampled the weights from [-0.0001, 0.0001] interval to ensure small deformations at initial epochs. Additional details about the network architecture can be found in the supplementary materials. Networks were trained for 1000 epochs using AdamW optimizer with learning rate of 0.0001 on a server equipped with several NVIDIA A100 GPUs <ref type="bibr" target="#b7">[8]</ref>. ISH images of size 360 × 420 were downsampled to 256 × 256. Each epoch corresponded to a batch of all image pixel coordinates [13]. After some initial experiments, we set the composite loss function weights (Eq. 5) to  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Qualitative Results</head><p>Support and residual images generated with the proposed method are shown in Fig. <ref type="figure" target="#fig_2">2</ref>. The support images retain the main style and content of the fixed template image, while the residual images include the remaining image contents, along with gene expression patterns not present in the template image. Utilization of the exclusion loss resulted in a clearer and more visually plausible separation between the support and residual images, particularly for gene expression patterns. Figure <ref type="figure" target="#fig_3">3</ref> further highlights the usefulness of the proposed registration guided image decomposition technique. First, our method can be applied to extract microscopy image artifacts, and therefore mitigate their impact on the registration. Second, the proposed method is general and can also be applied to register an ISH gene expression image to a Nissl image. In this case, the color distribution of the support image corresponds to that of a Nissl image, while the residual image presents the local contents of the gene expression image. We also used the proposed method to register an ISH brain image to another ISH image with a different gene expression. For this example, the residual image highlighted the gene expression patterns of the moving image, while the support image showed the gene expression patterns of the fixed image.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> visually compares the registration performance of the proposed technique, equipped with the exclusion loss, to ANTs. We found that the proposed method provided good results both in respect to the image registration and the transformation of the manual segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantitative Results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows Dice scores obtained for the selected marmoset brain regions. Registration techniques based on INRs outperformed the other methods on four out of five brain regions. ANTs achieved better registration results for only one structure, the VCA, which was the largest among the annotated brain regions    <ref type="table" target="#tab_1">2</ref> show that the registration based on implicit networks provided the most structurally similar results to the template images. With respect to the SSIM metric, our method significantly outperformed other approaches (t-test's p-values &lt; 0.05). ANTs and SynthMorph provided smoother deformation fields compared to the implicit networks, with significantly lower percentage of folding (t-test's p-values &lt; 0.05). However, the percentage of the folding obtained for the implicit networks was small and acceptable, as defined by folds in 0.5% of all pixels <ref type="bibr">[11]</ref>. The main disadvantage of the proposed approach was the relatively long optimization time of about 90 s for a single pairwise registration, resulting from the requirement to jointly train three implicit networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our approach based on implicit networks and registration-guided image decomposition has demonstrated excellent performance for the challenging task of registering ISH gene expression images of the marmoset brain. The results show that our approach outperformed pairwise registration methods based on ANTs and SynthMorph CNN, highlighting the potential of INRs as versatile off-the-shelf tools for image registration. Moreover, the proposed registration-guided image decomposition mechanism not only improved the registration performance, but also could be used to effectively separate the patterns that diverge from the target fixed image. In the future, we plan to investigate the possibility of using image decomposition for simultaneous registration and pattern segmentation, and methods to speed up the training <ref type="bibr" target="#b8">[9]</ref>. We also plan to extend our technique to 3D and test it on medical images that include pathologies. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. We use implicit networks S and R to decompose the moving image into the support and residual images. The moving and support images are jointly registered to the fixed template image, which guides the image decomposition procedure to generate a support image that is well-aligned to the fixed image with respect to the texture. The residual image includes the remaining moving image contents that do not contribute to the registration, such as local gene expression patterns or image artifacts.</figDesc><graphic coords="4,59,79,54,20,304,21,141,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>α 1 =</head><label>1</label><figDesc>α 2 = α 3 = α 5 = 1 and α 4 = 100, partially following the previous studies on INRs [10,14,16]. The window size for the LNCC loss was set to [32, 32]. Our PyTorch implementation of the proposed INR based registration method is available at https://github.com/BrainImageAnalysis/ImpRegDec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the moving image decomposition obtained with the proposed method (dec). Incorporation of the exclusion loss (excl) resulted in clearer separation of the gene expression texture patterns in the residual images.</figDesc><graphic coords="6,75,81,123,50,272,26,129,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Proposed technique can be useful for the extraction of microscope image artifacts (e.g. diagonal lines in the first row of images). It can also be applied to register ISH brain images to Nissl images or other ISH images. For such cases the support image presents image style of the fixed image, while the residual image includes local image patterns of the moving image.</figDesc><graphic coords="7,87,96,398,60,276,55,151,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of the proposed registration technique based on implicit networks and ANTs. AQ, HA, DLG, IC and VCA indicate the aqueduct, hippocampus area, dorsal lateral geniculate, inferior colliculus and visual cortex area, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dice scores (mean ± std) determined for the aqueduct (AQ), hippocampus area (HA), dorsal lateral geniculate (DLG), inferior colliculus (IC) and visual cortex are (VCA). Best results are shown in bold. dec and excl stand for the proposed image decomposition technique and the exclusion loss.</figDesc><table><row><cell>Method</cell><cell>AQ ↑</cell><cell>HA ↑</cell><cell>DLG ↑</cell><cell>IC ↑</cell><cell>VCA ↑</cell></row><row><cell>None</cell><cell cols="5">0.497 ± 0.194 0.311 ± 0.132 0.612 ± 0.141 0.742 ± 0.169 0.848 ± 0.051</cell></row><row><cell>ANTs SyN</cell><cell cols="5">0.673 ± 0.102 0.644 ± 0.141 0.757 ± 0.130 0.831 ± 0.133 0.941 ± 0.015</cell></row><row><cell>SynthMorph</cell><cell cols="5">0.625 ± 0.129 0.503 ± 0.190 0.719 ± 0.146 0.798 ± 0.157 0.922 ± 0.034</cell></row><row><cell>INRs</cell><cell cols="5">0.734 ± 0.071 0.657 ± 0.127 0.756 ± 0.130 0.804 ± 0.191 0.922 ± 0.022</cell></row><row><cell>INRs, dec</cell><cell cols="5">0.748 ± 0.067 0.662 ± 0.138 0.767 ± 0.125 0.839 ± 0.142 0.916 ± 0.033</cell></row><row><cell cols="6">INRs, dec+excl 0.749 ± 0.063 0.665 ± 0.134 0.766 ± 0.128 0.845 ± 0.143 0.920 ± 0.017</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Structural similarity index (SSIM) and the percentage of the non-positive Jacobian determinant values (mean ± std) calculated for the investigated registration methods. Best results are shown in bold. dec and excl indicate the proposed image decomposition technique and the exclusion loss, respectively.</figDesc><table><row><cell>Method</cell><cell>SSIM ↑</cell><cell>|JΦ| ≤ 0 [%] ↓</cell></row><row><cell>None</cell><cell cols="2">0.619 ± 0.046 -</cell></row><row><cell>ANTs</cell><cell cols="2">0.656 ± 0.059 &lt;0.001</cell></row><row><cell>SynthMorph</cell><cell cols="2">0.683 ± 0.039 &lt;0.001</cell></row><row><cell>INRs</cell><cell cols="2">0.713 ± 0.052 0.353 ± 0.459</cell></row><row><cell>INRs, dec</cell><cell cols="2">0.725 ± 0.054 0.359 ± 0.415</cell></row><row><cell cols="3">INRs, dec+excl 0.727 ± 0.054 0.429 ± 0.460</cell></row><row><cell cols="3">and already similar in unregistered images with an initial Dice score of 0.848.</cell></row><row><cell cols="3">Additionally, the Dice score for the VCA was high and comparable across all</cell></row><row><cell cols="3">investigated registration methods. Our approach achieved significantly better</cell></row><row><cell cols="3">Dice scores compared to the standard INRs for AQ, HA, DLG and IC (t-test's</cell></row><row><cell cols="3">p-values &lt; 0.05). Furthermore, incorporating the exclusion loss slightly improved</cell></row><row><cell cols="2">the Dice scores for three structures.</cell><cell></cell></row><row><cell>SSIM values in Table</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>10. Nam, S., Brubaker, M.A., Brown, M.S.: Neural image representations for multiimage fusion and layer separation. In: Avidan, S., Brostow, G., Cissé, M., Farinella, G.M., Hassner, T. (eds.) ECCV 2022, Part VII. LNCS, vol. 13667, pp. 216-232. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-20071-7 13 11. Qiu, H., Qin, C., Schuh, A., Hammernik, K., Rueckert, D.: Learning diffeomorphic and modality-invariant registration using B-splines. In: Medical Imaging with Deep Learning (2021) 12. Shimogori, T., et al.: Digital gene atlas of neonate common marmoset brain. Neurosci. Res. 128, 1-13 (2018) 13. Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein, G.: Implicit neural representations with periodic activation functions. Adv. Neural. Inf. Process. Syst. 33, 7462-7473 (2020) 14. Sun, S., Han, K., Kong, D., You, C., Xie, X.: MIRNF: medical image registration via neural fields. arXiv preprint arXiv:2206.03111 (2022) 15. Tancik, M., et al.: Fourier features let networks learn high frequency functions in low dimensional domains. Adv. Neural. Inf. Process. Syst. 33, 7537-7547 (2020) 16. Wolterink, J.M., Zwienenberg, J.C., Brune, C.: Implicit neural representations for deformable image registration. In: International Conference on Medical Imaging with Deep Learning, pp. 1349-1359. PMLR (2022)</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. The authors do not have any conflicts of interest. This work was supported by the program for Brain Mapping by <rs type="institution">Integrated Neurotechnologies for Disease Studies (Brain/MINDS)</rs> from the <rs type="funder">Japan Agency for Medical Research and Development AMED</rs> (<rs type="grantNumber">JP15dm0207001</rs>) and the <rs type="funder">Japan Society for the Promotion of Science</rs> (<rs type="grantName">JSPS, Fellowship</rs> <rs type="grantNumber">PE21032</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KVHz875">
					<idno type="grant-number">JP15dm0207001</idno>
				</org>
				<org type="funding" xml:id="_hXmN5MF">
					<idno type="grant-number">PE21032</idno>
					<orgName type="grant-name">JSPS, Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 61.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A reproducible evaluation of ants similarity metric performance in brain image registration</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2033" to="2044" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A single-cell transcriptomic atlas of complete insect nervous systems across multiple life stages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Corrales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Dev</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning in medical image registration: a review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno>20TR01</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">20</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Double-dip&quot;: unsupervised image decomposition via coupled deep-image-priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gandelsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11026" to="11035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Syn-thMorph: learning contrast-invariant registration without acquired images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Billot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Greve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="543" to="558" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cellular-resolution gene expression profiling in the neonatal marmoset brain reveals dynamic species-and region-specific differences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">2020125118</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Genome-wide atlas of gene expression in the adult mouse brain</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Lein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">445</biblScope>
			<biblScope unit="page" from="168" to="176" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modulated periodic activations for generalizable local functional representations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14214" to="14223" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
