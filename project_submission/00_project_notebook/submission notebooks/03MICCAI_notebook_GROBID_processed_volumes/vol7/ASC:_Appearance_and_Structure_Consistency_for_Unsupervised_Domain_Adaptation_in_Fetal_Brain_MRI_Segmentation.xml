<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation</title>
				<funder ref="#_zybqTdF">
					<orgName type="full">Chinese Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
				<funder ref="#_GcKAc6Q">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder>
					<orgName type="full">Guangdong Provincial Key Lab-</orgName>
				</funder>
				<funder ref="#_gS44aFM">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_TGUEKxx">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zihang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haifan</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haofeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">oratory of Big Data Computing</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="325" to="335"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">06EDE81F2BAC81E78CDF6FE281DAA400</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised domain adaptation</term>
					<term>Magnetic Resonance Imaging</term>
					<term>Semantic segmentation</term>
					<term>Fetal Brain</term>
					<term>Consistency learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic tissue segmentation of fetal brain images is essential for the quantitative analysis of prenatal neurodevelopment. However, producing voxel-level annotations of fetal brain imaging is timeconsuming and expensive. To reduce labeling costs, we propose a practical unsupervised domain adaptation (UDA) setting that adapts the segmentation labels of high-quality fetal brain atlases to unlabeled fetal brain MRI data from another domain. To address the task, we propose a new UDA framework based on Appearance and Structure Consistency, named ASC. We adapt the segmentation model to the appearances of different domains by constraining the consistency before and after a frequency-based image transformation, which is to swap the appearance between brain MRI data and atlases. Consider that even in the same domain, the fetal brain images of different gestational ages could have significant variations in the anatomical structures. To make the model adapt to the structural variations in the target domain, we further encourage prediction consistency under different structural perturbations. Extensive experiments on FeTA 2021 benchmark demonstrate the effectiveness of our ASC in comparison to registration-based, semisupervised learning-based, and existing UDA-based methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic resonance imaging (MRI) has emerged as an important tool for assessing brain development in utero <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>. Since manual segmentation is timeconsuming <ref type="bibr" target="#b29">[30]</ref> and suffers from high inter-rater variability in quantitative assessment, automatically segmenting brain tissue from MRI data becomes an urgent need <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>. However, the available annotated fetal brain datasets are limited in number and heterogeneity, hindering the development of automatic strategy. To achieve the unsupervised fetal brain tissue segmentation, registrationbased methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref> use image registration and label fusion to obtain the segmentation result from a set of templates <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>. Still, the accuracy of these methods is not sufficient due to the complexity of registration, and they usually underperform on the abnormal fetal brain image. Recently, deep learning (DL) based unsupervised domain adaptation (UDA) methods have shown their advance on medical image segmentation tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>. UDA methods usually narrow the distribution discrepancy between source and target domains by enforcing image-/feature-level alignment <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. In addition to inter-domain knowledge transfer, some works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref> explored the knowledge from both intermediate domains.</p><p>However, existing UDA methods in medical imaging mainly focus on the gap from different modalities (e.g., CT and MR), and pay less attention to the domain gap from different centres. Due to motion artifacts <ref type="bibr" target="#b16">[17]</ref>, it is difficult to collect high-quality fetal brain MR images and is expensive to label the newly collected data voxel-wisely. The above observations motivate us to establish a new UDA problem setting that aims to transfer the segmentation knowledge from the publicly available atlases to unlabeled fetal brain MRIs from new centres.</p><p>To solve the above UDA task, we propose an Appearance and Structure Consistency (ASC) framework. Consider the fact <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> that swapping the lowlevel spectrum between images can exchange their style/color/brightness without changing semantic content, while swapping the higher spectrum introduces unwanted artifacts. Thus, we propose to align appearance by only swapping the low-level spectrum. We develop an appearance consistency regularization based on a frequency-based appearance transformation, which is performed between labeled source data and unlabeled target data. Specifically, the source domain and the source data under the target appearance, are supervised with the same source labels. Then, the target data under the target appearance and source appearance are forced to maintain the same segmentation via dual unsupervised appearance consistency. Considering that significant variances in the shape of abnormal fetal brain tissue can cause difficulties in segmentation, we further constrain structure consistency under different perturbations in the target domain, besides aligning the inter-domain appearance gap. All the above consistency constraints are integrated with a teacher-student framework.</p><p>The contributions of this work are three-fold: <ref type="bibr" target="#b0">(1)</ref> we propose a novel Appearance and Structure Consistency framework for UDA in fetal brain tissue segmentation; <ref type="bibr" target="#b1">(2)</ref> we propose to address a practical UDA task adapting publicly available brain atlases to unlabeled fetal brain MR images; (3) experimental results on FeTA2021 benchmark <ref type="bibr" target="#b16">[17]</ref> show that the proposed framework outperforms representative state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In the UDA setting, D s = {X s , Y s } M s=1 denotes a set of source domain images (e.g., fetal brain atlases) and corresponding labels, respectively. D t = {X t } N t=1 denotes a set of target domain images (e.g., images from the FeTA benchmark). We aim to learn a semantic segmentation model for target domain data based on the labeled source and unlabeled target domain data. Usually, this goal is achieved by minimizing the domain gap between source domain samples D s and target domain samples D t . Figure <ref type="figure" target="#fig_1">2</ref> depicts the proposed Appearance and Structure Consistency (ASC) framework based on a teacher-student model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Frequency-Based Appearance Transformation</head><p>Atlases are magnetic resonance fetal images with "average shape". Domain shifts between the atlases and fetal images are mainly due to the texture, different hospital sensors, illumination or other low-level sources of variability. However, traditional UDA employing GAN to synthetic style-transfer images hardly capture such domain shift. Thus, we align the low-level statistics based on Fourier transformation to narrow the distribution of the two domains. This process is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Taking source data as an example, we compute the Fast Fourier transform (FFT) of each input image to obtain an amplitude spectrum F A and a phase component F P , where the low-frequency part of the amplitude of the source image F A (X s ) is swapped with the amplitude of the target image F A (X t ). Then, the transformed spectral representation of X s and the original phase F P (X s ) are mapped back to the image X sf t by inverse FFT (iFFT). X sf t has the same content as X s and similar appearance to X t . The above process can be formally defined as:</p><formula xml:id="formula_0">F A (X sf t ) = M • F A (X t ) + (1 -M ) • F A (X s ),<label>(1)</label></formula><formula xml:id="formula_1">X sf t = F -1 ([F A (X sf t ), F P (X s )]),<label>(2)</label></formula><p>where the mask M = I (h,w,d)∈[-βH:βH,-βW :βW,-βD:βD] controls the proportion of the swapped part over the whole amplitude by a parameter β ∈ (0, 1). Here we assume the center of the image is (0, 0, 0). Then we can train a student network with domain alignment images X sf t , the original images X s and the labels Y s by minimizing the dice loss:</p><formula xml:id="formula_2">L seg = L dice (P s , Y s ) + L dice (P sf t , Y s ),<label>(3)</label></formula><p>where P s and P sf t are the prediction of X s and X sf t , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Appearance Consistency</head><p>The above loss function imposes an implicit regularization before and after frequency-based transformation. In other words, source domain image X s and its transformation image X sf t should predict the same segmentation. However, the label of images from the target domain is not available X t in UDA settings.</p><p>As a replacement, we propose a teacher model for keeping semantic consistency across domain transformation. Specifically, the target domain image X t and its aligned image X tf s are regarded as representations of an object under different domains. Given the inputs of X t and X tf s of teacher and student models, we expect their predictions to be consistent. Further, considering that appearance transformation may break certain semantic information and make the model learn the wrong mapping relationship, we employ a form of dual consistency, which directs the model to focus on invariant information between the two views. f (•) and f (•) represent the outputs of the student model and the teacher model, respectively. Following the conventional consistency learning methods <ref type="bibr" target="#b19">[20]</ref>, we calculate the appearance consistency loss L app con between the teacher and student networks as:</p><formula xml:id="formula_3">L app con = 1 N N i=1 ||f (X t,i ) -f (X tf s,i )|| 2 + 1 N N i=1 ||f (X tf s,i ) -f (X t,i )|| 2 . (<label>4</label></formula><formula xml:id="formula_4">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Structure Consistency</head><p>Although frequency-based transformation and appearance consistency align the two domains' styles, the variance of tissue structure in pathological subjects still brings difficulty to domain alignment, which limits the model's gener alization ability. To this end, we utilise the teacher-student model <ref type="bibr" target="#b19">[20]</ref> keeping prediction consistency L str con under structure perturbation <ref type="bibr" target="#b27">[28]</ref> to alleviate the above problem. Here structure perturbation is sp for short. To achieve the structure perturbation, we first use a 3D cuboid mask consisting of a single box that randomly covers 25-50% of the image area at a random position, to blend two input images, which are sampled from the same batch. Then we blend the teacher predictions for the input images to produce a pseudo label for the student prediction of the blended image. Such an operation changes the original structural information, reduces the overfitting risk, and increases the robustness of the model to adapt to different structural variations. As appearance transformation doesn't affect the structure information, we add sp to both X t and X tf s to obtain X t,sp and X tf s,sp , which are fed into the teacher-student model and expected their predictions to be consistent. Then, L str con and L app con are combined as L asc :</p><formula xml:id="formula_5">L asc = 1 N N i=1 ||f (X t,i,sp ) -f (X tf s,i )|| 2 + 1 N N i=1 ||f (X tf s,i,sp ) -f (X t,i )|| 2 . (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Overall Training Strategy</head><p>We calculate appearance and structure consistency using the same teacher model. Its model weight θ is updated with the exponential moving average (EMA) of the student model f (θ), i.e., θ t = αθ t-1 + (1 -α)θ t , where α is the EMA decay rate that reflects the influence level of the current student model parameters.</p><p>Let λ control the trade-off between the supervised loss and the unsupervised regularization loss, the overall loss is:</p><formula xml:id="formula_6">L total = L seg + λL asc . (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>3 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Pre-processing</head><p>We evaluated our method on the Fetal Brain Tissue Annotation and Segmentation Challenge (FeTA) 2021 benchmark dataset <ref type="bibr" target="#b16">[17]</ref>, which contains 80 3D T2 MRI volumes with manual segmentations annotation of external cerebrospinal fluid (eCSF), grey matter (GM), white matter (WM), lateral ventricles (LV), cerebellum (CBM), deep grey matter (dGM) and brainstem (BS). The dataset cohort consisted of two subgroups: 31 neurological fetuses and 49 fetuses with abnormal development. Following the general UDA setting <ref type="bibr" target="#b10">[11]</ref>, the target set was randomly divided into 40 scans for training and 40 scans for testing. A collection from three atlases was used as the source set, including 32 neurotypical fetal brain atlases <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> and 15 spina bifida fetal brain atlases <ref type="bibr" target="#b4">[5]</ref>. Segmentations for all tissue types are available for all the atlas data. Some examples of slices, segmentations and histogram distribution are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We cropped the foreground region of fetal volumes and reshaped them to 144 × 144 × 144. Before being fed into the network, the input scans were normalized to a zero mean and unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>All models were implemented in PyTorch 1.12 and trained with NVIDIA A100 GPU with CUDA 11.3. Following the top-ranked method in the FeTA2021 competition <ref type="bibr" target="#b16">[17]</ref>, we use SegResNet <ref type="bibr" target="#b15">[16]</ref> as the backbone for the teacher/student model. The network parameters were optimized with Adam with the initial learning rate of 1 × 10 -4 . "Poly" learning rate policy is applied, where lr = lr init × (1 -epoch epoch total ) 0.9 . The batch size and training epoch were set to 4 (2 from each domain) and 100, respectively. The EMA decay rate α of the teacher model was set to 0.99, and hyperparameters λ were ramped up individually with function λ(t) = γ × e (-5(1-t tmax ) 2 ) , where t, t max and γ were the current step, the last step and weight, respectively. β was set to 0.1. Cutmix <ref type="bibr" target="#b27">[28]</ref> was used for target images as the structure perturbation for consistency regularization. We employed the student model prediction as the final result and used the Dice Similarity Coefficient (DSC) scores to evaluate the accuracy of the results. The average results of three runs were reported in all experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with the State-of-the-arts</head><p>We implemented several state-of-the-art label-limited segmentation methods for comparison, including Registration-based (SCALE <ref type="bibr" target="#b18">[19]</ref>), Unsupervised Domain Adaptatopm (UDA) (FDA <ref type="bibr" target="#b25">[26]</ref>, OLVA <ref type="bibr" target="#b0">[1]</ref> and DSA <ref type="bibr" target="#b7">[8]</ref>) and Semi-supervised Learning (CUTMIX <ref type="bibr" target="#b27">[28]</ref>, ASE-NET <ref type="bibr" target="#b11">[12]</ref>) in Table <ref type="table" target="#tab_0">1</ref>. It reports the segmentation performance of UDA of adapting atlas to the fetal brain on FeTA2021, including average DSC for the full set, normal set and abnormal set. The upper bound is given by supervised training which uses fully-labeled target data for model training.</p><p>It is worth noting that registration-based <ref type="bibr" target="#b18">[19]</ref> only successfully segments on the normal set, and GAN-based UDA approach <ref type="bibr" target="#b7">[8]</ref> performs worse than the frequency-based approach <ref type="bibr" target="#b25">[26]</ref> on the abnormal set. The proposed method is superior to the existing state-of-the-art methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> and achieves mean Dice of 78.5% over the seven tissue structures, reducing the Dice gap to supervised training to 3.5%. Compared with the baseline model (W/o adaptation), our proposed learning strategy further improves the performance by an average of 3.2% Dice. Visual results in Fig. <ref type="figure" target="#fig_3">4</ref> show our method can perform better in the junction areas of brain tissue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study and Sensitivity Analysis</head><p>The ablation study is shown in Table <ref type="table" target="#tab_2">2</ref>, and all the results boost our method's performance. "M1" represents the lower bound that only trains on the source domain data D s . "M2" uses the aligned source images X sf t for training. The following component are based on L asc , which is decoupled as L app con(Xt) , L app con(X tf s )</p><p>and L str con . "M3" denotes the appearance consistency loss L app con(Xt) to align distribution from source to target. "M4" indicates the dual-view appearance consistency loss to constrain semantic invariance. "M5" denotes the structure consistency L str con . We can see that appearance consistency can boost performance on normal and abnormal fetal MRIs, showing that minimizing the appearance gap between the source domain and target domain is effective. Further, we can obtain better results on the abnormal samples by applying the structure consistency loss. Table <ref type="table" target="#tab_4">3</ref> shows that the performance of our method grows with the increase of the hyper-parameter γ of consistency loss, and achieves the best when γ = 200. Besides, efficiency analysis is shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present a novel UDA framework and an atlas-based UDA setting for fetal brain tissue segmentation. Our method integrates appearance consistency encouraging the model to adapt different domain styles to narrow the domain gap and structure consistency making the model robust against the anatomical variations in the target domain. Experiments on the FeTA2021 benchmark demonstrate that our method outperforms the state-of-the-art methods. The proposed novel setting of atlas-based UDA could provide accurate segmentation for the fetal brain MRI data without pixel-wise annotations, greatly reducing the labeling costs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of MR images from brain atlases (a) and FeTA dataset (c). (b) and (d) show the ground truth. (e) shows the gray-scale histograms of the samples of 30-week gestational age from the FeTA dataset and brain atlases. Horizontal and vertical coordinates denote intensity values and voxel numbers, respectively. (Color figure online)</figDesc><graphic coords="2,43,80,169,16,336,94,70,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed Appearance and Structure Consistency framework. The student model learns from source data Xs and frequency-based transformed source data X sf t via the supervised loss Lseg. The appearance and structure consistency is achieved by the loss Lasc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the frequency-based appearance transformation. The transformation exchanges the domain-specific appearance between two images by swapping their low-frequency components of the spectrum. FFT is the Fast Fourier Transform. Phase component and amplitude spectrum are denoted by F P and F A in Eq. (2), respectively.</figDesc><graphic coords="5,77,46,66,32,297,22,118,39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual comparisons with existing methods. Due to space limitations, we only present those with the best average DSC in Registration-based, UDA and SSL methods. It can be seen that our predictions are visually closer to the ground truth than the other methods.</figDesc><graphic coords="7,59,46,54,11,333,52,154,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with the state-of-the-art methods. DSCA / DSCN stands for the average DSC calculated in the Abnormal / Normal subset. The best results are in bold. The DSC of each category is in the supplementary material.</figDesc><table><row><cell>Strategy</cell><cell>Method</cell><cell>Venue</cell><cell>DSC A</cell><cell>DSC N</cell><cell>Avg. DSC</cell></row><row><cell>Upper</cell><cell cols="2">Supervised (D t ) -</cell><cell cols="3">81.4 ±0.2 83.1 ±0.3 82.0 ±0.1</cell></row><row><cell>Lower</cell><cell cols="2">W/o Adaptation -</cell><cell cols="3">73.1 ±0.6 79.0 ±0.3 75.3 ±0.5</cell></row><row><cell cols="2">Registration-based SCALE [19]</cell><cell>MIA'18</cell><cell cols="3">63.6 ±1.5 77.3 ±1.7 68.7 ±1.7</cell></row><row><cell>UDA</cell><cell>FDA [26]</cell><cell>CVPR'20</cell><cell cols="3">74.4 ±0.4 80.4 ±0.4 76.7 ±0.4</cell></row><row><cell></cell><cell>OLVA [1]</cell><cell cols="4">MICCAI'21 73.3 ±0.6 79.1 ±0.3 75.6 ±0.3</cell></row><row><cell></cell><cell>DSA [8]</cell><cell>TMI'22</cell><cell cols="3">73.4 ±0.6 79.9 ±0.4 75.9 ±0.5</cell></row><row><cell>SSL</cell><cell>CUTMIX [28]</cell><cell>ICCV'19</cell><cell cols="3">74.1 ±0.1 79.7 ±0.5 76.2 ±0.2</cell></row><row><cell></cell><cell>ASE-NET [12]</cell><cell>TMI'22</cell><cell cols="3">73.7 ±0.3 79.8 ±0.3 76.2 ±0.2</cell></row><row><cell>UDA</cell><cell>ASC (ours)</cell><cell>-</cell><cell>76.6</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>±0.2 81.7 ±0.1 78.5 ±0.1</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of the contribution of each component in the proposed framework. The highest evaluation score is marked in bold.</figDesc><table><row><cell cols="2">Method L seg(Xs) L seg(X sf t ) Lasc</cell><cell></cell><cell></cell><cell>DSCA</cell><cell>DSCN</cell><cell>Avg DSC</cell></row><row><cell></cell><cell cols="3">L app con(Xt) L app con(X tf s ) L str</cell><cell></cell></row><row><cell>√</cell><cell>√</cell><cell>√</cell><cell>√</cell><cell>76.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>±0.2 81.7 ±0.1 78.5 ±0.1</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance of our method with different values of hyperparameters γ, which are used to balance consistency loss and supervisory loss. Avg. DSC 78.1±0.1 78.4±0.1 78.5 ±0.1 78.4±0.2 78.2±0.4</figDesc><table><row><cell>γ value</cell><cell>10</cell><cell>100</cell><cell>200</cell><cell>500</cell><cell>1000</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work is supported by <rs type="funder">Chinese Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101350001</rs>), and the <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62102267</rs>), and the <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (<rs type="grantNumber">2023A1515011464</rs>), and the <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">JCYJ20220818103001002</rs>), and the <rs type="funder">Guangdong Provincial Key Lab-</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zybqTdF">
					<idno type="grant-number">2020B0101350001</idno>
				</org>
				<org type="funding" xml:id="_gS44aFM">
					<idno type="grant-number">62102267</idno>
				</org>
				<org type="funding" xml:id="_TGUEKxx">
					<idno type="grant-number">2023A1515011464</idno>
				</org>
				<org type="funding" xml:id="_GcKAc6Q">
					<idno type="grant-number">JCYJ20220818103001002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 31.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OLVA: Optimal latent vector alignment for unsupervised domain adaptation in medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Al Chanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-425" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021: 24th International Conference</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Strasbourg, France; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27-October 1, 2021. 2021</date>
			<biblScope unit="page" from="261" to="271" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toward the automatic quantification of in utero brain development in 3D structural MRI: a review: quantification of fetal brain development</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Benkarim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2772" to="2787" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised bidirectional crossmodality adaptation via deeply synergistic image and feature alignment for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2494" to="2505" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adverse prenatal exposures and fetal brain development: insights from advanced fetal magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">De</forename><surname>Asis-Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andescavage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Limperopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Psychiatry: Cogn. Neurosci. Neuroimaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="480" to="490" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A spatio-temporal atlas of the developing fetal brain with spina bifida aperta</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Open Res. Eur</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">123</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A normative spatiotemporal MRI atlas of the fetal brain for automatic segmentation and analysis of early brain growth</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">476</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Magnetic resonance imaging of the newborn brain: manual segmentation of labelled atlases in term-born and preterm infants</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Gousias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1499" to="1509" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep symmetric adaptation network for cross-modality medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accuracy of in-utero MRI to detect fetal brain abnormalities and prognosticate developmental outcome: postnatal follow-up of the meridian cohort</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Child Adolesc. Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="140" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attentive symmetric autoencoder for brain MRI segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-920" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SynSeg-Net: synthetic segmentation without target modality ground truth</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1016" to="1025" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-Supervised medical image segmentation using adversarial consistency learning and dynamic convolution Network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1265" to="1277" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">View-disentangled transformer for brain lesion detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-Teacher: integrating Intra-domain and Inter-domain Teachers for Annotation-Efficient Cardiac Segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Racoceanu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-841" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2020: 23rd International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<meeting><address><addrLine>Lima, Peru; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">October 4-8, 2020. 2020</date>
			<biblScope unit="page" from="418" to="427" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review on automatic fetal and neonatal brain MRI segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Counsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="231" to="248" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D MRI brain tumor segmentation using autoencoder regularization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Keyvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11726-9_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11726-928" />
	</analytic>
	<monogr>
		<title level="m">Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018</title>
		<title level="s">Revised Selected Papers</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Walsum</surname></persName>
		</editor>
		<meeting><address><addrLine>Granada, Spain; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-16">September 16, 2018. 2019</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An automatic multi-tissue human fetal brain segmentation benchmark using the fetal tissue annotation dataset</title>
		<author>
			<persName><forename type="first">K</forename><surname>Payette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">167</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A generative model for image segmentation based on label fusion</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Leemput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1714" to="1729" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning non-linear patch embeddings with neural networks for label fusion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sanroma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selfattentive spatial adaptive normalization for cross-modality domain adaptation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lortkipanidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2926" to="2938" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiatlas segmentation with joint label fusion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pluta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Craige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="623" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Age-specific structural fetal brain atlases construction and cortical development quantification for Chinese population</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="page">118412</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep label fusion: a generalizable hybrid multi-atlas and deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102683</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for medical image segmentation by disentanglement learning and self-training</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2022.3192303</idno>
		<ptr target="https://doi.org/10.1109/TMI.2022.3192303" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FDA: fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust real-world image super-resolution against adversarial attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5148" to="5157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CutMix: regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MT-UDA: towards unsupervised crossmodality medical image segmentation with limited source labels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-228" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021: 24th International Conference</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Strasbourg, France; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27-October 1, 2021. 2021</date>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SSMD: Semi-supervised medical image detection with adaptive consistency and heterogeneous perturbation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102117</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
