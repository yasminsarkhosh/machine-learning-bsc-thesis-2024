<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siyuan</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AIM for Health Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Monash Medical AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Inception Institute of AI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AIM for Health Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Monash Medical AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Inception Institute of AI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Yu</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Inception Institute of AI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lie</forename><surname>Ju</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AIM for Health Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Monash Medical AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Inception Institute of AI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dwarikanath</forename><surname>Mahapatra</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AIM for Health Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Monash Medical AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Dermatology Research Centre</orgName>
								<orgName type="institution">The University of Queensland Diamantina Institute</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Inception Institute of AI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Victoria</forename><surname>Mar</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Monash Medical AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Inception Institute of AI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Monika</forename><surname>Janda</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Victorian Melanoma Service</orgName>
								<orgName type="institution">Alfred Health</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Inception Institute of AI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Soyer</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Victorian Melanoma Service</orgName>
								<orgName type="institution">Alfred Health</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Inception Institute of AI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
							<email>zongyuan.ge@monash.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">AIM for Health Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Victoria</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Inception Institute of AI</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="249" to="259"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B5598811F239971FC869FD5D9772DA24</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_24</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Skin lesions</term>
					<term>Prompt</term>
					<term>Domain generalization</term>
					<term>Debiasing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on diseaseirrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A domain mixup strategy is additionally devised to reduce the co-occurring artifacts in each domain, which allows for more flexible decision margins and mitigates the issue of incorrectly assigned domain labels. Experiments on four out-of-distribution datasets and six different biased ISIC datasets demonstrate the superior generalization ability of EPVT in skin lesion recognition across various environments. Code is available at https://github.com/SiyuanYan1/EPVT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Skin cancer is a serious and widespread form of cancer that requires early detection for successful treatment. Computer-aided diagnosis systems (CAD) using deep learning models have shown promise in accurate and efficient skin lesion diagnosis. However, recent research has revealed that the success of these models may be a result of overly relying on "spurious cues" in dermoscopic images, such as rulers, gel bubbles, dark corners, and hairs <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b30">29]</ref>, which leads to unreliable diagnoses. When a deep learning model overfits specific artifacts instead of learning the correct dermoscopic patterns, it may fail to identify skin lesions in real-world environments where the artifacts are absent or inconsistent.</p><p>To alleviate the artifact bias and enhance the model's generalization ability, we rethink the problem from the domain generalization (DG) perspective, where a model trained within multiple different but related domains are expected to perform well in unseen test domains. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, we define the domain labels based on the types of artifacts present in the training images, which can provide environment-aware prior knowledge reflecting a range of noisy contexts. By doing this, we can develop a DG algorithm to learn the generalized and robust features from diverse domains.</p><p>Previous DG algorithms learning domain-invariant features from source domains have succeeded in natural image tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19]</ref>, but cannot directly apply to medical images, in particular skin images, due to the vast cross-domain diversity of skin lesions in terms of shapes, colors, textures, etc. As each domain contains ad hoc intrinsic knowledge, learning domain-invariant features is highly challenging. One promising way is, as suggested in some recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b33">32]</ref>, exploiting multiple learnable domain experts (e.g., batch norm statistic, auxiliary classifiers, etc.) to capture domain-specific knowledge from different source domains individually. Still, two significant challenges remain. First, previous work only exploits some weak experts, like the batch norm, to capture knowledge, which naturally hampers the capability of capturing essential domain-specific knowledge. Second, previous methods such as <ref type="bibr" target="#b31">[30]</ref> focused on learning domain knowledge independently while overlooking the rich cross-domain information that all domain experts can contribute collectively for the target domain prediction.</p><p>To overcome the above problems, we propose an environment-aware prompt vision transformer (EPVT) for domain generalization of skin lesion recognition. On the one hand, inspired by the emerging prompt learning techniques that embed prompts into a model for adaptation to diverse downstream tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b32">31]</ref>, we construct different prompt vectors to strengthen the learning of domainspecific knowledge for adaptation to diverse domains. Then, the self-attention mechanism of the vision transformer (ViT) <ref type="bibr" target="#b7">[8]</ref> is adopted to fully model the relationship between image tokens and prompt vectors. On the other hand, to encourage cross-domain information sharing while preserving the domain-specific knowledge of each domain prompt, we propose a domain prompt generator based on low-rank weights updating. The prompt generator enables multiple domain prompts to work collaboratively and benefit from each other for generalization to unknown domains. Additionally, we devise a domain mixup strategy to resolve the problem of co-occurring artifacts in dermoscopic images and mitigate the resulting noisy domain label assignments.</p><p>Our contributions can be summarized as: <ref type="bibr" target="#b0">(1)</ref> We resolve an artifacts-derived biasing problem in skin cancer diagnosis using a novel environment-aware prompt learning-based DG algorithm, EPVT; (2) EPVT takes advantage of a ViTbased domain-aware prompt learning and a novel domain prompt generator to improve domain-specific and cross-domain knowledge learning simultaneously;</p><p>(3) A domain mixup strategy is devised to reduce the co-artifacts specific to dermoscopic images; (4) Extensive experiments on four out-of-distribution skin datasets and six biased ISIC datasets demonstrate the outperforming generalization ability and robustness of EPVT under heterogeneous distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In domain generalization (DG), the training dataset D train consists of M source domains, denoted as D train = {D k |k = 1, ..., M }. Here, each source domain D k is represented by n labeled instances {(x k j , y k j )} n j=1 . The goal of DG is to learn a model G : X → Y from the M source domains so that it can generalize well in unseen target domains D test . The overall architecture of our proposed model, EPVT, is shown in Fig. <ref type="figure" target="#fig_1">2a</ref>. We will illustrate its details in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain-Specific Prompt Learning with Vision Transformer</head><p>To enable the pre-trained vision transformer (ViT) to capture knowledge from different domains, as shown in Fig. <ref type="figure" target="#fig_1">2a</ref>, we define a set of M learnable domain prompts produced by a domain prompt generator (introduced in Sect. 2.2), denoted as</p><formula xml:id="formula_0">P D = {P m ∈ R d } M m=1</formula><p>, where d is the same size as the feature embedding of the ViT and each prompt P m corresponds to one domain (i.e. dark corners). To incorporate these prompts into the model, we follow the conventional practice of visual prompt tuning <ref type="bibr" target="#b11">[12]</ref>, which prepends the prompts P D into the first layer of the transformer. Particularly, for each prompt P m in P D , we extract the domain-specific features as:</p><formula xml:id="formula_1">F m (x) = F ([ X 0 , P m , E 0 ])<label>(1)</label></formula><p>where F is the feature encoder of the ViT, X 0 denotes the class token, E 0 is the image patch embedding, F m is the feature extracted by ViT with the m-th prompt, and 0 is the index of the first layer. Domain prompts P D are a set of learnable tokens, with each prompt P m being fed into the vision transformer along with the image and corresponding class tokens from a specific domain.</p><p>Through optimizing, each prompt becomes a domain expert only responsible for the images from its own domain. By the self-attention mechanism of ViT, the model can effectively capture domain-specific knowledge from the domain prompt tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Domain Knowledge Learning</head><p>To facilitate effective knowledge sharing across different domains while maintaining its own parameters of each domain prompt, we propose a domain prompt generator, as depicted in Fig. <ref type="figure" target="#fig_1">2b</ref>. Our approach is inspired by model adaptation and multi-task learning techniques used in natural language processing <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b27">26]</ref>. Aghajanyan et al. <ref type="bibr" target="#b0">[1]</ref> have shown that when adapting a model to a specific task, the updates to weights possess a low intrinsic rank. Similarly, each domain prompt P m should also have a unique low intrinsic rank when learning knowledge from its own domain. To this end, we decompose each P m into a Hadamard product between a randomly initialized shared prompt P * and a rank-one matrix P k obtained from two randomly initialized learnable vectors u k and v k , which is:</p><formula xml:id="formula_2">P m = P * P k where P k = u k • v T k (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where P m represents the domain-specific prompt, computed by Hadamard product of P * and P k . Here, P * ∈ R s×d is utilized to learn general knowledge, with s and d representing the dimensions of the prompt vector and feature embedding respectively. On the other hand, P k is computed using domain-specific trainable vectors: u k ∈ R s and v k ∈ R d . These vectors capture domain-specific information in a low-rank space. The decomposition of domain prompts into rank-one subspaces ensures that the model effectively encodes domain-specific information. By using the Hadamard product, the model can efficiently leverage cross-domain knowledge for target domain prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mitigating the Co-artifacts Issue</head><p>The artifacts-based domain labels can provide domain information for dermoscopic images. However, a non-trivial issue arises due to the possible cooccurrence of different artifacts from other domains within each domain. To address this issue, we employ a domain mixup strategy <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref>. Instead of assigning a hard prediction label ("0" or "1") to each image, in each batch, we mix every image using two randomly selected images from two different domains. This allows us to learn a flexible margin relative to both domains. We then apply the cross-entropy loss to the corresponding labels of bot images, as shown in Fig. <ref type="figure" target="#fig_1">2c</ref> and can be represented by the following equation:</p><formula xml:id="formula_4">L mixup = λL CE (G(x mix ), y i ) + (1 -λ)L CE (G(x mix ), y j )<label>(3)</label></formula><p>where x mix = λx k i +(1-λ)x q j ; x k i and x q j are samples from two different domains k and q, and y k i and y q j are the corresponding labels. This strategy can overcome the challenge of ambiguous domain labels in dermoscopic images and improve the performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimization</head><p>So far, we have introduced L mixup in Eq. 3 for optimizing our model. However, since our goal is to generalize the model to unseen environments, we also need to take advantage of each domain prompt. Instead of assigning equal weights to each domain prompt, we employ an adapter <ref type="bibr" target="#b31">[30]</ref> that learns the linear correlation between the domain prompts and the target image prediction. To obtain the adapted prompt for inference in the target domain, we define it as a linear combination of the source domain prompts:</p><formula xml:id="formula_5">P adapted = A(F (x)) = M m=1 w m • P m , s.t. M m=1 w m = 1<label>(4)</label></formula><p>where A represents an adapter containing a two-layer MLP with a softmax layer, and w m denotes the learned weights.</p><p>To train the adapter A, we simulate the inference process for each image in the source domain by treating it as an image from the pseudo-target domain.</p><p>Specifically, we first extract features from the ViT: Fm (x) = F ([X 0 , E 0 ]). Then we calculated the adapted prompt P adapted for the pseudo-target environment image x using the adapter A: P adapted = A( Fm (x)). Next, we extract features from ViT using the adapted prompt: Fm (x) = F ([ Fm (x), P adapted , E 0 ]). Finally, the classification head H is applied to predict the label y: y = H( Fm (x)). Additionally, the inferece process is the same as the simulated inference process and our final prediction will be conditioned on the adapted prompt P adapted .</p><p>To ensure that the adapter learns the correct linear correlation between the domain prompts and the target image, we use the domain label from source domains to directly supervise the weights w m . We also use the cross-entropy loss to maintain the model performance with the adapted prompt:</p><formula xml:id="formula_6">L adapted = L CE (H( Fm (x)), y) + λ( 1 M M m=1 1 M (L CE (w m m , 1) + t =m L CE (w m t , 0))</formula><p>(5) where Fm (x) is the obtained feature map conditioned on the adapted prompt P adapted , and H is the classification head. The total loss is then defined as L total = L mixup + L adapted .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Experimental Setup: We consider two challenging melanoma-benign classification settings that can effectively evaluate the generalization ability of our model in different environments and closely mimic real-world scenarios. (1) Outof-distribution evaluation: The task is to evaluate the model on test sets that contain different artifacts or attributes compared to the training set. We train and validate all algorithms on ISIC2019 <ref type="bibr" target="#b5">[6]</ref> dataset, following the split of <ref type="bibr" target="#b2">[3]</ref>. We use the artifacts annotations from <ref type="bibr" target="#b2">[3]</ref> and divide the training set of ISIC2019 into five groups: dark corner, hair, gel bubble, ruler, and clean, with 2351, 4884, 1640, 672, and 2796 images, respectively. We evaluate models on four out-of-distribution (OOD) datasets, including Derm7pt-Dermoscopic <ref type="bibr" target="#b14">[14]</ref>, Derm7pt-Clinical <ref type="bibr" target="#b14">[14]</ref>, PH2 <ref type="bibr" target="#b18">[18]</ref>, and PAD-UFES-20 <ref type="bibr" target="#b21">[21]</ref>. It's worth noting that ISIC2019, Derm7pt-Dermoscopic, and PH2 are dermoscopic images, while Derm7pt-Clinical and PAD are clinical images. (2) Trap set debiasing: We train and test our EPVT with its baseline on six trap sets <ref type="bibr" target="#b2">[3]</ref> with increasing bias levels, ranging from 0 (randomly split training and testing sets from the ISIC2019 dataset) to 1 (the highest bias level where the correlation between artifacts and class label is in the opposite direction in the dataset splits). More details about these datasets and splits are provided in the complementary material.</p><p>Implementation Details: For a fair comparison, we train all models using ViT-Base/16 <ref type="bibr" target="#b7">[8]</ref> backbone pre-trained on Imagenet and report the ROC-AUC with five random seeds. Hyperparameter and model selection methods are crucial for domain generalization algorithms. We conduct a grid search over learning rate (from 3e -4 to 5e -6 ), weight decay (from 1e -2 to 1e -5 ), and the length of the prompt (from 4 to 16, when available) and report the best performance of all models. We employ the training-domain validation set method <ref type="bibr" target="#b10">[11]</ref> for model selection. After the grid search, we use the AdamW optimizer with a learning rate of 5e -6 and a weight decay of 1e -2 . The batch size is 130, and the length of the prompt is 10. We resize the input image to a size of 224 × 224 and adopt the standard data augmentation like random flip, crop, rotation, and color jitter. An early stopping with the patience of 22 is set and with a total of 60 epochs for OOD evaluation and 100 epochs for trap set debiasing. All experiments are conducted on a single NVIDIA RTX 3090 GPU.</p><p>Out-of-Distribution Evaluation: Table <ref type="table" target="#tab_0">1</ref> presents a comprehensive comparison of our EPVT algorithm with existing domain generalization methods. The results clearly demonstrate the superiority of our approach, with the best performance on three out of four OOD datasets and remarkable improvements over the ERM algorithm, especially achieving 4.1% and 8.9% improvement on the PAD and PH2 datasets, respectively. Although some algorithms may perform similarly to our model on one of the four datasets, none can consistently match the performance of our method across all four datasets. Particularly, our approach showcases the highest average performance, with a 2.05% improvement over the second-best algorithm across all four datasets. These findings highlight the effectiveness of our algorithm in learning robust features and its strong generalization abilities across diverse environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study:</head><p>We perform ablation studies to analyze each component of our model, as shown in Table <ref type="table" target="#tab_1">2</ref>. We set our baseline as the Empirical Risk Minimization (ERM) algorithm, and we gradually add P (prompt <ref type="bibr" target="#b11">[12]</ref>), A (Adapter), M (Mixup), and G (domain prompt generator) into the model. Firstly, we observe that the baseline model with prompt only improves the average performance by  0.1%, showing that simply combining prompt does not very helpful for domain generalization. When we combine the adapter, the model's average performance improves by 1.37%, but it performs worse than ERM on PAD dataset. Subsequently, we added domain mixup and domain prompt generator to the model, resulting in significant further improvements in the model's average performance by 1.32% and 1.69%, respectively. The consistently better performance than the baseline on all four datasets also highlights the importance of addressing coartifacts and cross-domain learning for DG in skin lesion recognition.</p><p>Trap Set Debiasing: In Fig. <ref type="figure" target="#fig_2">3a</ref>, we present the performance of the ERM baseline and our EPVT on six biased ISIC2019 datasets. Each point on the graph represents an algorithm that is trained and tested on a specific bias degree split.</p><p>The graph shows that the ERM baseline performs better than our EPVT when the bias is low (0 and 0.3). However, this is because ERM relies heavily on spurious correlations between artifacts and class labels, leading to overfitting on the training set. As the bias degree increases, the correlation between artifacts and class labels decreases, and overfitting the train set causes the performance of ERM to drop dramatically on the test set with a significant distribution difference. In contrast, our EPVT exhibits greater robustness to different bias levels. Notably, our EPVT outperforms the ERM baseline by 9.4% on the bias 1 dataset.</p><p>Prompt Weights Analysis: To verify whether our model has learned the correct domain prompts for target domain prediction, we analyze and plot the results in Fig. <ref type="figure" target="#fig_2">3b</ref> and<ref type="figure" target="#fig_2">3c</ref>. Firstly, we extract the features of each domain from our training set and extract the feature from one target dataset, Derm7pt-Clin.</p><p>We then calculate the Frechet distance <ref type="bibr" target="#b8">[9]</ref> between each domain and the target dataset using the extracted feature, representing the domain distance between them. The results are recorded in Fig. <ref type="figure" target="#fig_2">3b</ref>. Next, we record the learned weights of each domain prompt in Fig. <ref type="figure" target="#fig_2">3c</ref>; it shows that our model assigns the highest weight to the "dark corner" group, as the domain distance between "dark corner" and Derm7pt-Clin is the closest, as shown in Fig. <ref type="figure" target="#fig_2">3b</ref>. This suggests that they share the most similar domain information. Further, the "clean" group is assigned the smallest weight as the domain distance between them is the largest, indicating that their domains are significantly different and contain less useful information for target domain prediction. In summary, we observe a negative correlation between domain distance and the prompt's weights, indicating that our model can learn the correct knowledge from different domains precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel DG algorithm called EPVT for robust skin lesion recognition. Our approach addresses the co-artifacts problem using a domain mixup strategy and cross-domain learning problems using a domain prompt generator. Compared to other competitive domain generalization algorithms, our method achieves outstanding results on three out of four OOD datasets and the second-best on the remaining one. Additionally, we conducted a debiasing experiment that highlights the shortcomings of conventional training using empirical risk minimization, which leads to overfitting in dermoscopic images due to artifacts. In contrast, our EPVT model effectively reduces overfitting and consistently performs better in different biased environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The training data is split into five domains: clean, rulers, hairs, air pockets, and dark corners. Domain generalization aims to train the model to learn from these domains to generalize well in unseen domains.</figDesc><graphic coords="2,59,79,53,84,304,81,76,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overview of our environment-aware prompt vision transformer (EPVT).</figDesc><graphic coords="3,84,33,57,77,227,56,164,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Deibiasing evaluation (b) domain distance (c) domain weights</figDesc><graphic coords="8,43,29,171,92,337,48,71,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The comparison on out-of-distribution datasets DoPrompt<ref type="bibr" target="#b31">[30]</ref> 82.38 ± 1.0 71.61 ± 1.7 83.81 ± 1.4 91.33 ± 1.8 82.06 ± 1.6 SelfReg [15] 81.83 ± 1.9 73.29 ± 1.4 85.27 ± 1.3 85.16 ± 3.3 81.12 ± 1.0 EPVT (Ours) 83.69 ± 1.4 73.96 ± 1.6 86.67 ± 1.5 91.91 ± 1.5 84.11 ± 1.4</figDesc><table><row><cell>Method</cell><cell cols="2">derm7pt_d derm7pt_c pad</cell><cell>ph2</cell><cell>Average</cell></row><row><cell>ERM</cell><cell cols="4">81.24 ± 1.6 71.61 ± 1.9 82.62 ± 1.6 83.06 ± 1.9 79.63 ± 1.5</cell></row><row><cell>DRO [23]</cell><cell cols="4">82.46 ± 1.7 72.88 ± 1.9 81.52 ± 1.2 84.64 ± 1.8 81.27 ± 1.6</cell></row><row><cell>CORAL [25]</cell><cell cols="3">81.42 ± 1.9 71.45 ± 1.3 88.13 ± 1.2 85.2 ± 2.2</cell><cell>81.55 ± 1.5</cell></row><row><cell>MMD [17]</cell><cell>82.08 ± 1.7 71.8 ± 1.5</cell><cell cols="3">85.89 ± 1.9 87.17 ± 1.4 81.73 ± 1.5</cell></row><row><cell>DANN [10]</cell><cell cols="4">81.79 ± 1.1 73.12 ± 1.6 84.12 ± 1.6 85.18 ± 1.9 81.87 ± 1.7</cell></row><row><cell>IRM [2]</cell><cell>79.07 ± 1.7 71.3 ± 1.8</cell><cell cols="3">77.82 ± 3.4 79.37 ± 1.2 76.64 ± 1.7</cell></row><row><cell>SagNet [20]</cell><cell cols="4">82.28 ± 1.8 73.19 ± 1.6 78.89 ± 4.5 88.79 ± 1.9 81.79 ± 1.8</cell></row><row><cell>MLDG [16]</cell><cell cols="4">81.06 ± 1.6 71.79 ± 1.6 83.41 ± 1.0 84.22 ± 1.8 79.87 ± 1.2</cell></row><row><cell>CAD [22]</cell><cell cols="3">82.72 ± 1.5 69.57 ± 1.6 81.36 ± 1.9 88.4 ± 1.5</cell><cell>81.51 ± 1.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on out-of-distribution datasets ± 1.6 71.61 ± 1.9 82.62 ± 1.6 83.06 ± 1.9 79.63 ± 1.5 +P 82.13 ± 1.1 71.41 ± 1.3 82.15 ± 1.6 84.21 ± 1.4 79.73 ± 1.3 +P+A 82.55 ± 1.6 72.86 ± 1.1 81.02 ± 1.5 84.97 ± 1.8 81.10 ± 1.6 +P+A+M 81.43 ± 1.4 73.18 ± 1.5 85.78 ± 1.9 89.28 ± 1.3 82.42 ± 1.7 +P+A+M+G 83.69 ± 1.4 73.96 ± 1.6 86.67 ± 1.5 91.91 ± 1.5 84.11 ± 1.4</figDesc><table><row><cell>Method</cell><cell>derm7pt_d derm7pt_c pad</cell><cell>ph2</cell><cell>Average</cell></row><row><cell>Baseline</cell><cell>81.24</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_24.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>arXiv abs/1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Artifact-based domain generalization of skin lesion models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bissoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ECCV Workshops</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">(de) constructing bias on skin lesion datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bissoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fornaciali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2766" to="2774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Debiasing skin lesion datasets and models? Not so fast</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bissoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="3192" to="3201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Validation of artificial intelligence prediction models for skin cancer diagnosis using dermoscopy images: the 2019 international skin imaging collaboration grand challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="330" to="e339" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalizable person re-identification with relevance-aware mixture of experts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16140" to="16149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The fréchet distance between multivariate normal distributions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dowson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multivar. Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="450" to="455" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">In search of lost domain generalization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=lQdXeXDoWtI" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual prompt tuning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13693</biblScope>
			<biblScope unit="page" from="709" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19827-4_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19827-4_41" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compacter: efficient low-rank hypercomplex adapter layers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1022" to="1035" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Seven-point checklist and skin lesion classification using multitask multimodal neural nets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daneshvar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Argenziano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="538" to="546" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selfreg: self-supervised contrastive regularization for domain generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9619" to="9628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to generalize: meta-learning for domain generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PH2: a public database for the analysis of dermoscopic images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dermoscopy Image Anal</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5716" to="5726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reducing domain gap by reducing style bias</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8690" to="8699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PAD-UFES-20: a skin lesion dataset composed of patient data and clinical images collected from smartphones</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Pacheco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">106221</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimal representations for covariate shift</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Rf58LPCwJj0" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributionally robust neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryxGuJrFvS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to optimize domain specific normalization for domain generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58542-6_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58542-6_5" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12367</biblScope>
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep CORAL: correlation alignment for deep domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">9915</biblScope>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-49409-8_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-49409-8_35" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multitask prompt tuning enables parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Nk2pDtuhTq" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6502" to="6509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improve unsupervised domain adaptation with mixup training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00677</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards trustable skin cancer diagnosis via rewriting model&apos;s decision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11568" to="11577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Prompt vision transformer for domain generalization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<idno>arXiv abs/2208.08914</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain adaptive ensemble learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="8008" to="8018" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
