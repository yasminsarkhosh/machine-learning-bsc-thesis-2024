<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Feature Propagation and Aggregation in Polyp Segmentation</title>
				<funder ref="#_aXTmzK2">
					<orgName type="full">Sichuan Science and Technology Program</orgName>
				</funder>
				<funder ref="#_v4w5fRC">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_WF6GsfQ #_4neVaex">
					<orgName type="full">NSFC&amp;CAAC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanzhou</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiqing</forename><surname>Shen</surname></persName>
							<idno type="ORCID">0000-0001-7866-3339</idno>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Ye</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
								<address>
									<postCode>200232</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junjun</forename><surname>He</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<email>chenjian@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
								<address>
									<postCode>200232</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Feature Propagation and Aggregation in Polyp Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="632" to="641"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">455F6BEED3195816BDEECE86FF474EEB</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Polyp Segmentation</term>
					<term>Feature Propagation</term>
					<term>Feature Aggregation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate segmentation of polyps is a crucial step in the efficient diagnosis of colorectal cancer during screening procedures. The prevalent UNet-like encoder-decoder frameworks are commonly employed, due to their capability of capturing multi-scale contextual information efficiently. However, two major limitations hinder the network from achieving effective feature propagation and aggregation. Firstly, the skip connection only transmits a single scale feature to the decoder, which can result in limited feature representation. Secondly, the features are transmitted without any information filter, which is inefficient for performing feature fusion at the decoder. To address these limitations, we propose a novel feature enhancement network that leverages feature propagation enhancement and feature aggregation enhancement modules for more efficient feature fusion and multi-scale feature propagation. Specifically, the feature propagation enhancement module transmits all encoder-extracted feature maps from the encoder to the decoder, while the feature aggregation enhancement module performs feature fusion with gate mechanisms, allowing for more effective information filtering. The multi-scale feature aggregation module provides rich multi-scale semantic information to the decoder, further enhancing the network's performance. Extensive evaluations on five datasets demonstrate the effectiveness of our method, particularly on challenging datasets such as CVC-ColonDB and ETIS, where it can outperform the previous state-of-the-art models by a significant margin (3%) in terms of mIoU and mDice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colorectal cancer is a life-threatening disease that results in the loss of millions of lives each year. In order to improve survival rates, it is essential to identify colorectal polyps early. Hence, regular bowel screenings are recommended, where endoscopy is the gold standard. However, the accuracy of endoscopic screening can heavily rely on the individual skill and expertise of the domain experts involved, which are prone to incorrect diagnoses and missed cases. To reduce the workload on physicians and enhance diagnostic accuracy, computer vision technologies, such as deep neural networks, are involved to assist in the presegmentation of endoscopic images. The UNet-like model uses skip connections that transmit only single-stage features. In contrast, our approach utilizes FPE to propagate features from all stages, incorporating a gate mechanism to regulate the flow of valuable information.</p><p>Deep learning-based image segmentation methods have gained popularity in recent years, dominated by UNet <ref type="bibr" target="#b10">[11]</ref> in the field of medical image segmentation. UNet's success has led to the development of several other methods that use a similar encoder-decoder architecture to tackle polyp segmentation, including ResUNet++ <ref type="bibr" target="#b6">[7]</ref>, PraNet <ref type="bibr" target="#b2">[3]</ref>, CaraNet <ref type="bibr" target="#b9">[10]</ref> and UACANet <ref type="bibr" target="#b7">[8]</ref>. However, these methods are prone to inefficient feature fusion at the decoder due to the transmission of multi-stage features without filtering out irrelevant information.</p><p>To address these limitations, we propose a novel feature enhancement network for polyp segmentation that employs Feature Propagation Enhancement (FPE) modules to transmit multi-scale features from all stages to the decoder. Figure <ref type="figure" target="#fig_0">1</ref> illustrates a semantic comparison of our feature propagation scheme with the UNet-like model. While the existing UNet-like models use skip connections to propagate a single-scale feature, our method utilizes FPE to propagate multi-scale features from all stages in encoder. More importantly, this research highlights the usage of FPE can effectively replace skip connections by providing more comprehensive multi-scale characteristics from full stages in encoder. To further address the issue of high-level semantics being overwhelmed in the progressive feature fusion process, we also integrate a Feature Aggregation Enhancement (FAE) module that aggregates the outputs of FPE from previous stages at decoder. Moreover, we introduce gate mechanisms in both FPE and FAE to filter out redundant information, prioritizing informative features for efficient feature fusion. Finally, we propose a Multi-Scale Aggregation (MSA) module appended to the output of the encoder to capture multi-scale features and provide the decoder with rich multi-scale semantic information. The MSA incorporates a cross-stage multi-scale feature aggregation scheme to facilitate the aggregation of multi-scale features. Overall, our proposed method improves upon existing UNet-like encoder-decoder architectures by addressing the limitations in feature propagation and feature aggregation, leading to improved polyp segmentation performance.</p><p>Our major contributions to accurate polyp segmentation are summarized as follows.</p><p>(1) The method addresses the limitations of the UNet-like encoder-decoder architecture by introducing three modules: Feature Propagation Enhancement (FPE), Feature Aggregation Enhancement (FAE), and Multi-Scale Aggregation (MSA). ( <ref type="formula" target="#formula_1">2</ref>) FPE transmits all encoder-extracted feature maps to the decoder, and FAE combines the output of the last stage at the decoder and multiple outputs from FPE. MSA aggregates multi-scale high-level features from FPEs to provide rich multi-scale information. (3) The proposed method achieves state-of-the-art results in five polyp segmentation datasets and outperforms the previous cutting-edge approach by a large margin (3%) on CVC-ColonDB and ETIS datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Overview. Our proposed feature enhancement network illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>(a), is also a standard encoder-decoder architecture. Following Polyp-PVT <ref type="bibr" target="#b1">[2]</ref>, we adopt PVT <ref type="bibr" target="#b15">[16]</ref> pretrained on ImageNet as the encoder. The decoder consists of three feature aggregation enhancement modules (FAE) and a multi-scale aggregation module (MSA). Given an input image I, we first extract the pyramidal features using the encoder, which is defined as follows,</p><formula xml:id="formula_0">P 1 , P 2 , P 3 , P 4 = PVT(I)<label>(1)</label></formula><p>where, {P 1 , P 2 , P 3 , P 4 } is the set of pyramidal features from four stages with the spatial size of 1/4, 1/8, 1/16, 1/32 of the input respectively. Features with lower spatial resolution usually contain richer high-level semantics. Then, these features are transmitted by the feature propagation enhancement module (FPE) to yield the feature set {C 1 , C 2 , C 3 , C 4 }, which provides multi-scale information from all the stages. This is different from the skip connection which only transmits the single-scale features at the present stage. Referring to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>, three highest-level features generated by FPEs are subsequently fed into a MSA module for aggregating rich multi-scale information. Afterwards, feature fusion is performed by FAE in the decoder, whereby it progressively integrates the outputs from FPE and previous stages. The higher-level semantic features of the FPE output are capable of effectively compensating for the semantics that may have been overwhelmed during the upsampling process. This process is formulated as,</p><formula xml:id="formula_1">O 4 = MSA({C 2 , C 3 , C 4 }) O i = FAE({C i , C i+1 , ..., C 3 }, O i+1 ) i = 1, 2, 3.<label>(2)</label></formula><p>A noteworthy observation is that the gating mechanism has been widely utilized in both FPE and FAE to modulate the transmission and integration of features. By selectively controlling the flow of relevant information, this technique has shown promise in enhancing the overall quality of feature representations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. The final features O 1 are passed through the classifier (i.e., a 1 × 1 convolutional layer) to get the final prediction result in O. Further details on FPE, FAE, and MSA will be provided in the following sections.</p><p>Feature Propagation Enhancement Module. In contrast to the traditional encoder-decoder architecture with skip connections, the FPE aims to transmit multi-scale information from full stage at the encoder to the decoder, rather than single-scale features at the current stage. The FPE architecture is illustrated in Fig. <ref type="figure" target="#fig_1">2(b</ref>). The input of the FPE includes the features from the other three stages, in addition to the feature of the current stage, which delivers richer spatial and semantic information to the decoder. However, these multi-stage inputs need to be downsampled or upsampled to match the spatial resolution of the features at the present stage. To achieve this, FPE employs a stepwise downsampling strategy, with each step only downsampling by a factor of 2, and each downsampling step is followed by a Convolutional Unit (CU) to perform feature transformation. The number of downsamplings is denoted as N = log T 2 , where T stands for the scale factor for downsampling. The CU consists of a 3 × 3 convolutional layer, a Batch Normalization layer, and an activation layer (i.e., ReLU). This strategy can be a highly effective means of mitigating the potential loss of intricate details during the process of large-scale interpolation. Similarly, this same strategy is employed in FAE.</p><p>The features from the other three stages, denoted as P 1 , P 2 , and P 3 , are downsampled or upsampled to generate P 1 , P 2 , and P 3 . Instead of directly combining the four inputs, FPE applies gate mechanisms to emphasize informative features. The gate mechanism takes the form of Y = G(X ) * Y, where G (in this work, Sigmoid is used) measures the importance of each feature vector in the reference feature X ∈ R H×W . By selectively enhancing useful information and filtering out irrelevant information, the reference features X assist in identifying optimal features Y at the current level. The output of G is in [0, 1] H×W , which controls the transmission of informative features from Y or helps filter useless information. Notably, X can be Y itself, serving as a reference feature. FPE leverages such gate mechanism to obtain informative features in P and passes them through a CU respectively. After that, FPE concatenates features from the four branches to accomplish feature aggregation. A CU is followed to boost the feature fusion.</p><p>Feature Aggregation Enhancement Module. The FAE is a novel approach that integrates the outputs of the last stages at the decoder with the FPE's outputs at both the current and deeper stages to compensate for the high-level semantics that may be lost in the process of progressive feature fusion. In contrast to the traditional encoder-decoder architecture with skip connections, the FAE assimilates the output of the present and higher-stage FPEs, delivering richer spatial and semantic information to the decoder.</p><p>The FAE, depicted in Fig. <ref type="figure" target="#fig_1">2</ref>(c), integrates the outputs of the current and deeper FPE stages with high-level semantics. As an example, the last FAE takes as inputs O 2 (output of the penultimate FAE), C 1 (output of the current FPE stage), and C 2 and C 3 (outputs of FPE from deeper stages). Multiple outputs from deeper FPE stages are introduced to compensate for high-level semantics. Furthermore, gate mechanisms are utilized to filter out valueless features for fusion, and the resulting enhanced feature is generated by a CU after concatenating the filtered features. Finally, O 2 is merged with the output feature through element-wise summation, followed by a CU to produce the final output feature O 1 .</p><p>Multi-Scale Aggregation Module. The MSA module in our proposed framework, inspired by the Parallel Partial Decoder in PraNet <ref type="bibr" target="#b2">[3]</ref>, integrates three highest-level features C 2 , C 3 , and C 4 from the FPE output. This provides rich multi-scale information for subsequent feature aggregation in the FAE and also helps to form a coarse localization of polyps under supervision. It benefits from an additional supervision signal, as observed in PraNet <ref type="bibr" target="#b2">[3]</ref>, CaraNet <ref type="bibr" target="#b7">[8]</ref>, and etc. by aiding in forming a coarse location of the polyp and contributing to improved accuracy and performance. As depicted in Fig. <ref type="figure" target="#fig_1">2(d</ref>), the MSA module first processes these three features separately. C 2 , which has the highest feature resolution, is processed with multiple dilated convolutions to capture its multiscale information while keeping its spatial resolution unchanged. C 3 is processed with only one dilated convolution due to its higher spatial resolution, while C 4 is not processed since it already contains the richest contextual information. The output features of the three branches are then upsampled to the size of C 2 . To better integrate these three multi-scale high-level features for subsequent fusion, additional cross-feature fusion operations (i.e., 2 CU layer) are performed in the MSA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. We conduct extensive experiments on five polyp segmentation datasets, including Kvasir <ref type="bibr" target="#b5">[6]</ref>, CVC-ClinicDB <ref type="bibr" target="#b0">[1]</ref>, CVC-ColonDB <ref type="bibr" target="#b12">[13]</ref>, ETIS <ref type="bibr" target="#b11">[12]</ref> and CVC-T <ref type="bibr" target="#b13">[14]</ref>. Following the setting in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>, the model is trained using a fraction of the images from CVC-ClinicDB and Kvasir, and its performance is evaluated by the remaining images as well as those from CVC-T, CVC-ColonDB, and ETIS. In particular, there are 1450 images in the training set, of which 900 are from Kvasir and 550 from CVC-ClinicDB. The test set contains all of the images from CVC-T, CVC-ColonDB, and ETIS, which have 60, 380, and 196 images, respectively, along with the remaining 100 images from Kvasir and the remaining 62 images from CVC-ClinicDB.</p><p>Implementations. We utilize PyTorch 1.10 to run experiments on an NVIDIA RTX3090 GPU. We set an initial learning rate to 1e-4 and halve it after 80 epochs. We train the model for 120 epochs. The same multi-scale input and gradient clip strategies used in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref> are employed in the training phase. The batch size is 16 by default. AdamW is selected as the optimizer with a weight decay of 1e-4. We adopt the same data augmentation techniques as UACANet <ref type="bibr" target="#b7">[8]</ref>, including random flip, random rotation, and color jittering. In evaluation phase, we mainly focus on mDice, mIoU, the two most common metrics in medical image segmentation, to evaluate the performance of the model. Referring to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>, we use a combination loss consisting of weighted Dice loss and weighted IoU loss to supervise network optimization. Comparison with State-of-the-Art Methods. We compared our proposed method with previous state-of-the-art methods. According to the experimental settings, the results on CVC-ClinicDB and Kvasir demonstrate the learning ability of the proposed model, while the results on CVC-T, CVC-ColonDB, and ETIS demonstrate the model's ability for cross-dataset generalization. The experimental results are listed in Table .1. It can be seen that our model is slightly inferior to Polyp-PVT on the CVC-ColonDB, but the gap is quite small, e.g., 0.6% in mDice and 0.4% in mIoU. On Kvasir, we are ahead of the previous best model by 1.1% in mDice and 1.6% in mIoU. This shows that our model is second to none in terms of learning ability, which demonstrates the effectiveness of our model. Furthermore, our proposed method demonstrates strong cross-dataset generalization capability on CVC-T, CVC-ColonDB, and ETIS datasets, with particularly good performance on the latter two due to their larger and more representative datasets. Our model outperforms state-of-the-art models by 2.9% mDice and 3.2% mIoU on CVC-ColonDB and 3.5% mDice and 4.0% mIoU on ETIS. These results validate the effectiveness of feature-level enhancement and highlight the superior performance of our method. We also provide visual results in Fig. <ref type="figure" target="#fig_2">3</ref>, where our predictions are shown to be closer to the ground truth.</p><p>Ablation Study. We carried out ablation experiments to verify the effectiveness of the proposed FPE, FAE, and MSA. For our baseline, we use the simple encoder-decoder structure with skip connections for feature fusion and perform element-wise summation at the decoder. Table <ref type="table" target="#tab_1">2</ref> presents the results of our ablation experiments. Following the ablation study conducted on our proposed approach, it is with confidence that we assert the significant contribution of each module to the overall performance enhancement compared to the baseline. Our results indicate that the impact of each module on the final performance is considerable, and their combination yields the optimal overall performance. Specifically, across the five datasets, our proposed model improves the mDice score by at least 1.4% and up to 3.4% on CVC-T, compared to the baseline. For mIoU, the improvements are 1.5% and 3.5% on the corresponding datasets. In summary, our ablation study underscores the crucial role played by each component of our approach, and establishes its potential as a promising framework for future research in this domain.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduce a new approach to polyp segmentation that addresses inefficient feature propagation in existing UNet-like encoder-decoder networks. Specifically, a feature propagation enhancement module is introduced to propagate multiscale information over full stages in the encoder, while a feature aggregation enhancement module is attended at the decoder side to prevent the loss of high-level semantics during progressive feature fusion. Furthermore, a multiscale aggregation module is used to aggregate multi-scale features to provide rich information for the decoder. Experimental results on five popular polyp datasets demonstrate the effectiveness and superiority of our proposed method. Specifically, it outperforms the previous cutting-edge approach by a large margin (3%) on CVC-ColonDB and ETIS datasets. To extend our work, our future direction focuses on exploring more effective approaches to feature utilization, such that efficient feature integration and propagation can be achieved even on lightweight networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Comparison of feature propagation methods. The UNet-like model uses skip connections that transmit only single-stage features. In contrast, our approach utilizes FPE to propagate features from all stages, incorporating a gate mechanism to regulate the flow of valuable information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An overview of the proposed framework. (a) Overall architecture; (b) FPE: Feature propagation enhancement module; (c) FAE: Feature aggregation enhancement module; (d) MSA: Multi-scale aggregation module</figDesc><graphic coords="4,56,46,54,41,339,37,283,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Exemplary images and results that are segmented by different approaches.</figDesc><graphic coords="8,56,46,362,84,339,43,130,87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on CVC-ClinicDB, Kvasir, CVC-T, CVC-ColonDB and ETIS.</figDesc><table><row><cell>Methods</cell><cell cols="3">Published Venue CVC-ClinicDB Kvasir</cell><cell cols="2">CVC-ColonDB ETIS</cell><cell>CVC-T</cell></row><row><cell></cell><cell></cell><cell>mDice mIoU</cell><cell cols="2">mDice mIoU mDice mIoU</cell><cell>mDice mIoU mDice mIoU</cell></row><row><cell>UNet [11]</cell><cell>MICCAI'15</cell><cell>0.823 0.755</cell><cell cols="2">0.818 0.746 0.512 0.444</cell><cell>0.398 0.335 0.710 0.627</cell></row><row><cell>PraNet [3]</cell><cell>MICCAI'19</cell><cell>0.899 0.849</cell><cell cols="2">0.898 0.840 0.709 0.640</cell><cell>0.628 0.567 0.871 0.797</cell></row><row><cell cols="2">ResUNet++ [7] JBHI'21</cell><cell>0.846 0.786</cell><cell cols="2">0.807 0.727 0.588 0.497</cell><cell>0.337 0.275 0.687 0.598</cell></row><row><cell>SANet [17]</cell><cell>MICCAI'21</cell><cell>0.916 0.859</cell><cell cols="2">0.904 0.847 0.752 0.669</cell><cell>0.750 0.654 0.888 0.815</cell></row><row><cell>MSNet [19]</cell><cell>MICCAI'21</cell><cell>0.915 0.866</cell><cell cols="2">0.902 0.847 0.747 0.668</cell><cell>0.720 0.650 0.862 0.796</cell></row><row><cell cols="2">UACANet-S [8] MM'21</cell><cell>0.916 0.870</cell><cell cols="2">0.905 0.852 0.783 0.704</cell><cell>0.694 0.615 0.902 0.837</cell></row><row><cell cols="2">UACANet-L [8] MM'21</cell><cell>0.926 0.880</cell><cell cols="2">0.912 0.859 0.751 0.678</cell><cell>0.766 0.689 0.910 0.849</cell></row><row><cell>Polyp-PVT [2]</cell><cell>arxiv'21</cell><cell>0.937 0.889</cell><cell cols="2">0.917 0.864 0.808 0.727</cell><cell>0.787 0.706 0.900 0.833</cell></row><row><cell>CaraNet [10]</cell><cell>SPIE MI'22</cell><cell>0.921 0.876</cell><cell cols="2">0.913 0.859 0.775 0.700</cell><cell>0.740 0.660 0.902 0.836</cell></row><row><cell>LDNet [18]</cell><cell>MICCAI'22</cell><cell>0.923 0.872</cell><cell cols="2">0.912 0.855 0.794 0.715</cell><cell>0.778 0.707 0.893 0.826</cell></row><row><cell cols="2">SSFormer-S [15] MICCAI'22</cell><cell>0.916 0.873</cell><cell cols="2">0.925 0.878 0.772 0.697</cell><cell>0.767 0.698 0.887 0.821</cell></row><row><cell cols="2">SSFormer-L [15] MICCAI'22</cell><cell>0.906 0.855</cell><cell cols="2">0.917 0.864 0.802 0.721</cell><cell>0.796 0.720 0.895 0.827</cell></row><row><cell>Ours</cell><cell>-</cell><cell>0.931 0.885</cell><cell cols="2">0.928 0.880 0.837 0.759</cell><cell>0.822 0.746 0.905 0.839</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on FPE, MSA, and FAE. Each item presents mDice/mIoU. /83.3 91.8/87.2 91.7/86.6 81.4/73.3 81.2/73.3 90.1/83.4 92.6/87.7 92.2/87.2 81.6/73.4 80.9/72.9</figDesc><table><row><cell>FPE MSA FAE CVC-T</cell><cell cols="2">CVC-ClinicDB Kvasir</cell><cell>CVC-ColonDB ETIS</cell></row><row><cell cols="2">87.1/80.4 91.6/87.0</cell><cell cols="2">91.4/86.3 81.0/72.8</cell><cell>79.9/71.5</cell></row><row><cell cols="2">90.2/83.7 92.7/87.8</cell><cell cols="2">91.5/86.4 81.0/73.1</cell><cell>80.2/71.9</cell></row><row><cell cols="2">89.9/83.2 91.6/86.8</cell><cell cols="2">91.9/87.0 82.1/73.9</cell><cell>81.3/73.1</cell></row><row><cell>90.0</cell><cell></cell><cell></cell></row></table><note><p>90.2/83.5 93.0/88.4 92.1/87.2 81.5/73.4 80.9/72.9 90.0/83.6 92.7/88.0 92.6/87.8 81.2/73.1 81.6/73.9 90.5/83.9 93.1/88.5 92.8/88.0 83.7/75.9 82.2/74.6</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was partly supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62071104</rs>), partly supported by the <rs type="funder">Sichuan Science and Technology Program</rs> (No. <rs type="grantNumber">2021YFG0328</rs>), partly supported by the <rs type="funder">NSFC&amp;CAAC</rs> (No. <rs type="grantNumber">U2233209</rs>, No.<rs type="grantNumber">U2133211</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_v4w5fRC">
					<idno type="grant-number">62071104</idno>
				</org>
				<org type="funding" xml:id="_aXTmzK2">
					<idno type="grant-number">2021YFG0328</idno>
				</org>
				<org type="funding" xml:id="_WF6GsfQ">
					<idno type="grant-number">U2233209</idno>
				</org>
				<org type="funding" xml:id="_4neVaex">
					<idno type="grant-number">U2133211</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CMIG</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wenhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jinpeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng-Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06932v3</idno>
		<title level="m">Polyp-pvt: Polyp segmentation with pyramid vision transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07172</idno>
		<title level="m">Hardnet-mseg: A simple encoder-decoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: a segmented polyp dataset</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-37734-2_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-37734-2_37" />
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling: 26th International Conference, MMM 2020</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</editor>
		<meeting><address><addrLine>Daejeon, South Korea; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">January 5-8, 2020. 2020</date>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
		<respStmt>
			<orgName>ISM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">UACANet: Uncertainty augmented context attention for polyp segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM MM</publisher>
			<biblScope unit="page" from="2167" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">GFF: Gated fully fusion for semantic segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01803</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">CaraNet: context axial reverse attention network for segmentation of small medical objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Loew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>SPIE</publisher>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in WCE images for early diagnosis of colorectal cancer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colonoscopy videos using shape and context information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="630" to="644" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JHE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stepwise feature fusion: local guides global</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_11" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: a versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shallow Attention Network for Polyp Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_66" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021: 24th International Conference</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Strasbourg, France; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27-October 1, 2021. 2021</date>
			<biblScope unit="page" from="699" to="708" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lesion-aware dynamic kernel for polyp segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_10" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="99" to="109" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic polyp segmentation via multi-scale subtraction network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_12" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021: 24th International Conference</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Strasbourg, France; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27-October 1, 2021. 2021</date>
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
