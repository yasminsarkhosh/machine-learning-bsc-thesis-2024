<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI</title>
				<funder ref="#_jy8vTRq">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Aq4TyYr">
					<orgName type="full">Startup Funds at Beijing University of Posts and Telecommunications</orgName>
					<orgName type="abbreviated">BUPT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanjie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Youhao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Liyun</forename><surname>Tu</surname></persName>
							<email>tuliyun@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="452" to="461"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">27FA5BD93B7FA7958BF75D8323CD209E</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_44</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain-knowledge encoding</term>
					<term>Patch-free</term>
					<term>Structural magnetic resonance imaging (sMRI)</term>
					<term>Alzheimer&apos;s disease</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Construct a generalizable model for the diagnosis of Alzheimer's disease (AD) is an important task in medical imaging. While deep neural networks have recently advanced classification performance for various diseases using structural magnetic resonance imaging (sMRI), existing methods often provide suboptimal and untrustworthy results because they do not incorporate domain-knowledge and global context information. Additionally, most state-of-the-art deep learning methods rely on multi-stage preprocessing pipelines, which are inefficient and prone to errors. In this paper, we propose a novel domainknowledge-constrained neural network for automatic diagnosis of AD using multi-center sMRI. Specifically, we incorporate domain-knowledge into a ResNet-like architecture. We explicitly enforce the network to learn domain invariant and domain specific features by jointly training multiple weighted classifiers, so that pixel-wise predictive performance generalizes to unseen images. In addition, the network directly takes segmentationfree and patch-free images in original resolution as input, which offers accurate inference with global context information and accurate individualized abnormalities to further refines reproducible predictions. The framework was evaluated on a set of sMRI collected from 7 independent centers. The proposed approach identifies important discriminative brain abnormalities associated with AD. Experimental results demonstrate superior performance of our method compared to state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Alzheimer's disease (AD) is one of the most pervasive neurodegenerative disorders, causing an increasing morbidity burden that may outstrip diagnosis and management capacity with the population ages. The assessment of AD usually involves the acquisition of structural magnetic resonance imaging (sMRI) images, since it offers accurate visualization of the anatomy and pathology of the brain. Brain abnormalities (e.g., atrophy, enlargement, malformation) are known to be the most discriminative and reliable biomarkers <ref type="bibr" target="#b0">[1]</ref> of AD that can be observed and analyzed through sMRI. However, automatic and reproducible identification of AD remains challenging due to heterogeneous of sMRI collected from different centers.</p><p>Recently, convolutional neural networks (CNN) have been used for automatic classification of AD from sMRI. Many methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> use a bag of patches selected from the skull-stripped brain region, which ignores global context information that can play a significant role in identifying lesions for accurate inference <ref type="bibr" target="#b3">[4]</ref>. Many studies <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b8">[8]</ref> proposed to characterize AD using segmented anatomies (e.g., gray matter or hippcampus). These methods rely on the accurate segmentation of the anatomies which is usually performed in a multi-stage data processing pipeline with the help of third-party softwares (e.g., FreeSurfer <ref type="bibr" target="#b9">[9]</ref>) driven by a prior template. However, template-driven methods depend on variable image registration accuracy and highly affected by the anatomical variability between subjects, introducing errors to the characterization of individualized abnormalities. Similarly, methods (e.g., <ref type="bibr" target="#b10">[10]</ref>) use detected landmarks also depend on a template-driven pipeline. Taking advantage of attention mechanism, some methods <ref type="bibr" target="#b4">[5]</ref> proposed to diagnose AD using sMRI images from multiple centers. However, the classification performance is either hardly reproducible or difficult to compare across studies. One of the major reasons is that existing methods are often trained with samples from the same training (source) domain, while testing samples come from an independent new (target) domain with a different feature distribution. In the literature, this situation relates to domain adaptation <ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref> or domain generalization <ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref>. A widely used solution for the problem is to learn a domain-invariant latent feature space <ref type="bibr" target="#b20">[20]</ref>. Unfortunately, there is no guarantee that the target samples' features will fall into the shared source domain-invariant representation, and in practice it is that new domains typically do not.</p><p>In this paper, we propose a novel domain-knowledge-constrained neural network for the diagnosis of AD using sMRI from multiple source domains. We designed a new domain-knowledge encoding module into a ResNet-like architecture for feature learning that yields a latent feature space with domain specific and domain shared information. In addition, we propose to use segmentationfree, resampling-free, patch-free 3D sub-images, which offers global context information and subject-level abnormalities to further refines generalizable and reproducible predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We propose to design and implement an end-to-end neural network (Fig. <ref type="figure" target="#fig_0">1</ref>) for automatic, robust, and reproducible diagnosis of AD using sMRI images, with the hope to identify and understand the most discriminative anatomical regions associate with AD. The model operates in 3 major steps: a) crop the input sMRI image to keep a sub-region (red rectangle), containing relevant anatomy structures (e.g., hippocampus, caudate, ventricles) associate with AD; b) extract features shared by all training sources based on ResNet <ref type="bibr" target="#b21">[21]</ref>; c) design a domainknowledge encoding module and a set of label predictors to constrain the feature learning process for better generalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Patch-Free 3D Feature Extractor</head><p>We first estimate a bounding box around relevant anatomical objects in the input sMRI. The objects are automatically identified by affine registration, which transforms the reference template to each image in the dataset to estimate label for the image. We note that, the estimated labels are only used to locate the bounding box, it has no effect on the individual's atrophy since we pad extra space to ensure the cropped image contain all interested objects with respect to registration errors. Then, we crop the input image using the located bounding box to obtain the sub-image as input to our network. It need to be clarified that the cropping size is a fixed tuple determined by the maximum bounding box containing informative anatomical objects associated with AD.</p><p>To encode global context information, we propose a patch-free 3D feature extractor for different source domains, which is expected to learn domaininvariant features while not eliminating domain-specific features. Each domain has a unique label classifier, allowing adjustments for domain differences. Based on ResNet, we design our feature extractor as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Each basic block consists of two convolutional layers. Each convolutional layer is followed by a batch normalization and a nonlinear activation function LeakyReLU. The basic block can be wrote as:</p><formula xml:id="formula_0">X l+1 = F (W i , X l ) + W s X l ,<label>(1)</label></formula><p>where X l and X l+1 are the input and output of the basic block and F (W i , X l ) denotes the nonlinear mapping in the basic block. Since the dimensions of X and F (W i , X) must be the same for summation, we use the linear mapping W s to adjust the dimensions of X in the shortcut connection.</p><p>In the proposed method, we use global average pooling function which is more suitable for disease classification, because the global average pooling operation reflects the information of gray matter volume in brain regions and preserves the relative position relationship between different channels of the feature map.</p><p>In the output layer, we use a softmax classifier based on cross-entropy loss to calculate the loss between the predicted and true labels.</p><formula xml:id="formula_1">L = cross-entropy( Y i (X i ∈ D s ; ω), Y i )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global Average Pooling</head><p>Global average pooling solves the problem of excessive image feature dimensions.</p><p>If the feature maps of 3D images are directly expanded for classification, it will significantly increase the number of classifier parameters and increase the time and space complexity of training. Global average pooling averages the 3D feature maps in the channel dimension, preserving the relative position relationship between channels and reducing the resources required for model training.</p><p>The dimension change in the global average pooling is</p><formula xml:id="formula_2">[B, C, D, H, W ] → [B, C, 1, 1, 1]</formula><p>, where B denotes the batch-size and C denotes the channel number.</p><formula xml:id="formula_3">GAP (δ) = 1 D × H × W D i=1 H j=1 W k=1 δ i,j,k<label>(3)</label></formula><p>where δ denotes the image feature extracted by ResNet, and D, H, W denote the three dimensions of the feature. Since global average pooling has fewer parameters, it can prevent over-fitting to some extent, further more, global average pooling sums out the spatial information, thus it is more robust to spatial translation of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Domain-Knowledge Encoding</head><p>The domain-knowledge encoding module is designed to give relative similarity weights to source domains from a new sample. The weights reflect the similarity between the testing sample and source domains, allowing the module to share strength only between similar domains.</p><p>Our model uses multiple classifiers for prediction from the features extracted by the feature extractor. The classifiers are independent from each other. We feed the image features to different classifiers and generate weights to each classifier, summing the predictions of each classifier according to the weights as the final output.</p><formula xml:id="formula_4">Y = c_num j=1 ω ij • classif ier j (δ(X ∈ D i ), θ j )<label>(4)</label></formula><p>where Y denotes the prediction result of X, c_num denotes the number of classifiers, D i denotes the center which X belongs, δ denotes the extracted feature from X, classif ier j denotes one classifier and θ j are the parameters in classif ier j . Multiple classifiers can capture the invariant and specific feature distributions between different domains, comparing the similarity of feature distributions between training source and unseen target domains by a joint training of the admixture classifiers, generating weights to integrate the feature distributions of known domains to fit the unknown domain feature distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Description</head><p>Structural T1-weighted brain MRI data of 809 subjects (468 male, 341 female, age 68.16 ± 8.12 years, range 42-89 year) were acquired from 7 in-house independent multiple centers as detailed in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">22]</ref>. In total, 552 subjects (295 of normal control (NC), 257 of AD) were used for leave-center-out training. The rest 257 subjects with mild cognitive impairment (MCI) were used as an independent dataset for evaluation and compared with clinical diagnosis metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We first evaluated the model using leave-center-out cross-validation, where one center was selected for testing at a time and all remaining centers were used for training. Then, we applied the trained model on an independent validation set of unseen images for subjects with MCI. All images were cropped to have the same size of <ref type="bibr">[80,</ref><ref type="bibr">128,</ref><ref type="bibr">72]</ref>. Image features were extracted with 3 × 3 × 3 convolution in the network and 2 × 2 × 2 convolution with a stride of 2 replacing the maximum pooling. The extracted features were passed through a global average pooling layer (Sect. 2.1). N = 6 independent classifiers were used.</p><p>During training, we sorted all training centers and feed the image features from site i to all classifiers, and set the weight of classif ier j(j=i) to 1 and the weight of the rest classifiers to 0. We used cross-entropy to calculate the prediction error and update the parameters of the feature extractor and classif ier j by backpropagation. In testing stage, we feed the image features from the test center to all classifiers, and the final prediction was used the weighted average of predicted probability over all classifiers as the final prediction.</p><p>We used SGD algorithm to optimize the model coefficients, and set the initial learning rate to 0.001 and reduce the learning rate to one-tenth of the previous value every 50 epochs. The method was implemented using PyTorch 1.1 with Python 3.7. The experiments were run on an Intel Xeon CPU with 16 cores, 43 GB. RAM and a NVIDIA A5000 GPU with 24 GB memory. The code and model are available at https://github.com/Yanjie-Z/DomainKnowledge4AD. Fig. <ref type="figure">2</ref>. First row: the left panel evaluates the AUC-ROC curve for each domain through leave-center-out cross validation, and the right panel investigates the association between the predicted probabilities and clinical measure (MMSE) in subjects with Alzheimer's disease (AD), mild cognitive impairment (MCI), and healthy controls (NC). Second row: attention map for an arbitrary example sMRI of a subject with AD, illustrating the most discriminative features learnt from the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Evaluation</head><p>To evaluate the proposed approach, we feed 2 different types of input to the conventional 3D-ResNet <ref type="bibr" target="#b21">[21]</ref> and each obtains a models: 1) ResNet, which use the original image as input, and 2) Baseline, which use the bounding box cropping strategy as proposed in Sect. 2.1. In addition, we incorporated the patch-free cropping strategy inspired by <ref type="bibr" target="#b3">[4]</ref> to crop the middle-half sub-region of the original input sMRI image of the brain, and feed to ResNet, which we denote as ResNet-PF. The prediction performance are compared in Table <ref type="table" target="#tab_0">1</ref>. Our model achieves an average classification accuracy of 89.25% on all test centers during cross-validation, compared to the average classification accuracy of 85.95% with baseline (without the use of domain knowledge encoding module).</p><p>We used AUC-ROC curves to evaluate the classification effectiveness <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b23">23]</ref> of the model on the test centers, and we counted the AUC-ROC curves for seven centers and compared them accordingly in Fig. <ref type="figure">2</ref>.</p><p>To evaluate the interpretability of the model, we used Grad-CAM <ref type="bibr" target="#b24">[24]</ref> to analyze the sensitive regions of the model in discriminating AD. We found that the model focused on the hippocampus in the images during prediction, which confirms that AD and the hippocampus have a significant correlation. We also find that the model pays more attention to the hippocampus in discriminating AD than healthy controls. Figure <ref type="figure" target="#fig_1">3</ref> compares the 3D attention map between a subject with AD and a healthy subject who never has AD, demonstrating obvious higher values in hippocampus region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We proposed a novel reproducible and generalizable neural network to assist the automatically diagnosis of AD that benefits from domain knowledge and global contextual information with the help of segmentation-free, resamplingfree, patch-free sub-image. The model was evaluated with leave-center-out crossvalidation and with an independent set of unseen images for subjects with MCI (Fig. <ref type="figure">2</ref>). It obtains an average accuracy of 89.25%, loss of 0.39 and AUC of 0.92 comparing with 85.95%, 0.58 and 0.91 using ResNet. We apply the proposed model to images from a new domain (never used during training), demonstrating promising results.</p><p>We did ablation studies to evaluate the proposed method (Table <ref type="table" target="#tab_0">1</ref>), unsurprisingly, the cropped images obtain the best performance. Figures <ref type="figure">2</ref> and<ref type="figure" target="#fig_1">3</ref> evaluated the explainability of the proposed neural network. The results suggest that the hippocampus and ventricles regions suffer the most in AD, which are consistent with multi-stage segmentation-based methods <ref type="bibr" target="#b4">[5]</ref>, and clinical measures (in terms of MMSE) on an independent dataset (Fig. <ref type="figure">2</ref>).</p><p>Our results and all comparative frameworks tend to predict worse for center 3, probably because it has some subjects with AD who have higher MMSE (Fig. <ref type="figure">2</ref>) making the diagnosis challenging. As opposite, all models provide the best accuracy for center 5. We will further explore possible reasons of this center imbalance in future work. Another limitation of the presented study is the empirical estimation of early stop strategy during leave-center-out cross validation based on the observed loss ranges. In future work, we will also explore a more automated mechanism to increase model robustness for images from more center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel end-to-end domain-knowledge constrained neural network for automatic and reproducible diagnosis of AD using sMRI images. We proposed a new domain-knowledge encoding module that learn simultaneously with a ResNet-like feature extractor for domain specific and domain shared representations. The network directly takes the segmentation-free, patch-free images in original resolution as input, which is able to learn with global contextual information for subject-level pathological brain dysmorphologies features to further refines reproducible predictions. Our experiments demonstrate superior performance and generalize well to completely unseen domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic of the proposed generalizable classification model. Feature extractor is a ResNet18-like 3D network that extracts high-dimensional features from MRI images for classification using 3D convolution and residual connection. Basic block is the basic component of the feature extractor and consists of two 3D convolutional layers, two BatchNorm layers, a ReLu layer and residual connection. Classifier is a multilayer perceptron (MLP), consisting of two linear layers and a ReLu layer. Domain-Knowledge Encoding captures domain invariant features and domain-specific features and generates weights for classifiers based on domain similarity. Label Predictor specifies that our model has multiple mutually independent classifiers, and the predictions of all classifiers are weighted and summed to obtain the final output. (Color figure online)</figDesc><graphic coords="3,41,79,112,28,340,27,171,91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. 3D attention maps for a healthy subject (first row) and a subject with AD (second row) in 4 different views (column). The bottom row shows a visual navigator.</figDesc><graphic coords="8,55,98,54,47,340,18,179,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,74,46,226,82,303,67,228,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons among different methods with leave-center-out cross-validation. Abbreviations: ACC = accuracy, AUC = area under the curve of the receiver operating characteristic, AVG = average performance over centers. ACC in percentage.</figDesc><table><row><cell></cell><cell>S0</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell><cell>S5</cell><cell>S6</cell><cell>AVG</cell></row><row><cell>ResNet</cell><cell>LOSS 0.90</cell><cell>0.53</cell><cell>0.35</cell><cell>1.34</cell><cell>0.53</cell><cell>0.38</cell><cell>0.56</cell><cell>0.66</cell></row><row><cell></cell><cell cols="8">ACC 87.05 88.31 90.83 72.14 79.39 87.57 87.71 84.71</cell></row><row><cell></cell><cell>AUC 0.91</cell><cell>0.91</cell><cell>0.96</cell><cell>0.83</cell><cell>0.86</cell><cell>0.94</cell><cell>0.94</cell><cell>0.91</cell></row><row><cell cols="2">ResNet-PF LOSS 0.79</cell><cell>0.53</cell><cell>0.39</cell><cell>1.76</cell><cell>0.64</cell><cell>0.38</cell><cell>0.45</cell><cell>0.71</cell></row><row><cell></cell><cell cols="8">ACC 84.00 86.67 88.89 72.63 82.86 95.56 89.78 85.77</cell></row><row><cell></cell><cell>AUC 0.91</cell><cell>0.90</cell><cell>0.94</cell><cell>0.83</cell><cell>0.86</cell><cell>0.95</cell><cell>0.94</cell><cell>0.90</cell></row><row><cell>Baseline</cell><cell>LOSS 0.47</cell><cell>0.50</cell><cell>0.37</cell><cell>1.37</cell><cell>0.72</cell><cell>0.24</cell><cell>0.42</cell><cell>0.58</cell></row><row><cell></cell><cell cols="8">ACC 87.66 87.03 88.36 71.40 82.85 95.40 89.00 85.95</cell></row><row><cell></cell><cell>AUC 0.93</cell><cell>0.86</cell><cell>0.95</cell><cell>0.83</cell><cell>0.92</cell><cell>0.95</cell><cell>0.93</cell><cell>0.91</cell></row><row><cell>Proposed</cell><cell>LOSS 0.32</cell><cell>0.39</cell><cell>0.34</cell><cell>0.85</cell><cell>0.34</cell><cell>0.20</cell><cell>0.33</cell><cell>0.39</cell></row><row><cell></cell><cell cols="8">ACC 90.79 88.88 88.33 74.28 91.42 97.77 93.33 89.25</cell></row><row><cell></cell><cell>AUC 0.94</cell><cell>0.92</cell><cell>0.94</cell><cell>0.84</cell><cell>0.93</cell><cell>0.94</cell><cell>0.93</cell><cell>0.92</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62201091</rs>, the <rs type="funder">Startup Funds at Beijing University of Posts and Telecommunications (BUPT)</rs>, and the <rs type="programName">BUPT innovation and entrepreneurship support program</rs> under <rs type="grantNumber">2023-YC-A208</rs>. We are grateful to the <rs type="institution">Multicenter Alzheimer Disease Imaging Consortium</rs> (PI: <rs type="person">Prof. Xi Zhang</rs>, <rs type="person">Prof. Yuying Zhou</rs>, <rs type="person">Prof. Ying Han</rs>, and <rs type="person">Prof. Qing Wang</rs>). The content is solely the responsibility of the authors and does not necessarily represent the official views of any of the funding agencies or sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jy8vTRq">
					<idno type="grant-number">62201091</idno>
				</org>
				<org type="funding" xml:id="_Aq4TyYr">
					<idno type="grant-number">2023-YC-A208</idno>
					<orgName type="program" subtype="full">BUPT innovation and entrepreneurship support program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Progressive lateral ventricular enlargement as a clue to Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Guptha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Holroyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0140-6736(02)08806-2</idno>
		<ptr target="https://doi.org/10.1016/S0140-6736(02)08806-2" />
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">9322</biblScope>
			<biblScope unit="page">2040</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dual attention multi-instance deep learning for Alzheimer&apos;s disease diagnosis with structural MRI</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for classification of Alzheimer&apos;s disease: overview and reproducible evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1361841520300591" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101694</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Super-resolution based patch-free 3D medical image segmentation with self-supervised guidance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.14645" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalizable, reproducible, and neuroscientifically interpretable imaging biomarkers for Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">2000675</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for Alzheimer prediction using brain biomarkers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Goenka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4827" to="4871" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep multi-structural shape analysis: application to neuroanatomy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gutiérrez-Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11072</biblScope>
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00931-1_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00931-1_60" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interpretable differential diagnosis for Alzheimer&apos;s disease and Frontotemporal dementia</title>
		<author>
			<persName><forename type="first">H.-D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mansencal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coupé</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-6_6" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022: 25th International Conference, Singapore</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reliability of structural MRI measurements: the effects of scan session, head tilt, inter-scan interval, acquisition sequence, freesurfer version and processing stream</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Hedges</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1053811921010235" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">246</biblScope>
			<biblScope unit="page">118751</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting anatomical landmarks for fast Alzheimer&apos;s disease diagnosis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Munsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2524" to="2533" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Design and analysis of closed-loop decoder adaptation algorithms for brain-machine interfaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Danig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Orsborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Moorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Carmena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical report</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On target shift in adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2019-03">March 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CyCADA: cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.03213" />
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3162" to="3174" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of multi-source domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="84" to="92" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating Nesterov momentum into Adam</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2013" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A literature survey on domain adaptation of statistical Classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-03">March 2008</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note type="report_type">UIUC Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MetaReg: towards domain generalization using meta-regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7378-metareg-towards-domain-generalization-using-meta-regularization" />
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="998" to="1008" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to generalize: metalearning for domain generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16067" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1902.00113.pdf" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Support and invertibility in domaininvariant representations</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<meeting>the Twenty-Second International Conference on Artificial Intelligence and Statistics, ser. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019-04-18">18 April 2019</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Independent and reproducible hippocampal radiomic biomarkers for multisite Alzheimer&apos;s disease: diagnosis, longitudinal progress and biological basis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S2095927320302140" />
	</analytic>
	<monogr>
		<title level="j">Sci. Bull</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1103" to="1113" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervising the decoder of variational autoencoders to improve scientific utility</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="5954" to="5966" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
