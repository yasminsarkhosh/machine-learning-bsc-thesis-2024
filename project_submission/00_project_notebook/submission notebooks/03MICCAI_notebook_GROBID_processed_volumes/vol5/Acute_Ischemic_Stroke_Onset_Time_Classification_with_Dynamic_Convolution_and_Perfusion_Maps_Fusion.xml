<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion</title>
				<funder ref="#_83pJjQp #_enAMNBd">
					<orgName type="full">National Natural Science Foundation of Guangdong Province</orgName>
				</funder>
				<funder>
					<orgName type="full">Beijing Key Specialists in Major Epidemic Prevention and Control</orgName>
				</funder>
				<funder ref="#_pq9d4AB">
					<orgName type="full">Shenzhen Key Basic Research Project</orgName>
				</funder>
				<funder ref="#_h8VAchY #_eWQMCGU #_uZW5uZ2 #_D6KQykA">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_7UTXeJb #_vK7WzCB #_tJadDgr #_QkdJgbj">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_dVAuSum #_Hm3q9kR">
					<orgName type="full">Beijing Hospitals Authority&apos;s Ascent Plan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory" key="lab1">Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging</orgName>
								<orgName type="laboratory" key="lab2">Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution" key="instit1">National-Regional Key Technology</orgName>
								<orgName type="institution" key="instit2">Shenzhen University Medical School</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science and Software Engineering</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Service Computing and Applications</orgName>
								<orgName type="laboratory" key="lab2">Guangdong Province Key Laboratory of Popular High Performance Computers</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haijun</forename><surname>Lei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science and Software Engineering</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Service Computing and Applications</orgName>
								<orgName type="laboratory" key="lab2">Guangdong Province Key Laboratory of Popular High Performance Computers</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueyan</forename><surname>Bian</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution" key="instit1">Beijing Chaoyang Hospital</orgName>
								<orgName type="institution" key="instit2">Capital Medical University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution" key="instit1">Beijing Chaoyang Hospital</orgName>
								<orgName type="institution" key="instit2">Capital Medical University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Laboratory for Clinical Medicine</orgName>
								<orgName type="institution">Capital Medical University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baiying</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory" key="lab1">Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging</orgName>
								<orgName type="laboratory" key="lab2">Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution" key="instit1">National-Regional Key Technology</orgName>
								<orgName type="institution" key="instit2">Shenzhen University Medical School</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="558" to="568"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3D1B0E100DF439842FE59348FB1C500D</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_54</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computed tomography perfusion</term>
					<term>Dynamic convolution</term>
					<term>Multi-map</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In treating acute ischemic stroke (AIS), determining the time since stroke onset (TSS) is crucial. Computed tomography perfusion (CTP) is vital for determining TSS by providing sufficient cerebral blood flow information. However, the CTP has small samples and high dimensions. In addition, the CTP is multi-map data, which has heterogeneity and complementarity. To address these issues, this paper demonstrates a classification model using CTP to classify the TSS of AIS patients. Firstly, we use dynamic convolution to improve model representation without increasing network complexity. Secondly, we use multi-scale feature fusion to fuse the local correlation of low-order features and use a transformer to fuse the global correlation of higher-order features. Finally, multi-head pooling attention is used to learn the feature information further and obtain as much important information as possible. We use a five-fold cross-validation strategy to verify the effectiveness of our method on the private dataset from a local hospital. The experimental results show that our proposed method achieves at least 5% higher accuracy than other methods in TTS classification task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Acute ischemic stroke (AIS) is a disease of ischemic necrosis or softening of localized brain tissue caused by cerebral blood circulation disturbance, ischemia, and hypoxia <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Intravenous thrombolysis can be used to open blood vessels within 4.5 h. For patients with large vessel occlusion, the internal blood vessels can be opened by removing the thrombus within 6 h. Therefore, determining the time since stroke onset (TSS) is crucial for creating a treatment plan for AIS patients, with a TSS of less than 6 h being critical. However, approximately 30% of AIS occurs at unknown time points due to wake-up stroke (WUS) and unknown onset stroke (UOS) <ref type="bibr" target="#b2">[3]</ref>. For such patients, determining the TSS accurately is challenging, and they may be excluded from appropriate treatment. Computed tomography perfusion (CTP) is processed with special software to generate perfusion maps: cerebral blood volume (CBF), cerebral blood volume (CBV), mean transit time (MTT), and peak response time (Tmax) <ref type="bibr" target="#b3">[4]</ref>. These perfusion maps can provide sufficient information on cerebral blood flow, ischemic penumbra, and infarction core area.</p><p>There are some machine learning methods to determine the TSS of AIS by automatic discrimination <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. For example, Ho et al. <ref type="bibr" target="#b4">[5]</ref> first used auto-encoder (AE) to learn magnetic resonance imaging (MRI) and then put the learned and original features into the classifier for TSS classification. Lee et al. <ref type="bibr" target="#b6">[7]</ref> analyzed diffusion-weighted imaging (DWI) and fluid-attenuated inversion recovery (FLAIR) using automatic image processing methods to obtain appropriate dimensional features and used machine learning to classify the TSS. The above machine learning-based TSS classification methods often use regions of interest (ROI) while ignoring the spatial correlation between neural images. Several researchers try to employ deep learning techniques to classify AIS TSS, considering the spatial correlation among neural images. Zhang et al. <ref type="bibr" target="#b8">[9]</ref> designed a new intra-domain task adaptive migration learning method to classify the TSS of AIS. Polson et al. <ref type="bibr" target="#b9">[10]</ref> designed the neighborhood and attention network with segmented weight sharing to learn DWI, apparent diffusion coefficient (ADC), and FLAIR, then used weighted softmax to aggregate sub-features and achieve TSS classification.</p><p>With the small samples and the high dimension of CTP, the convolution neural network (CNN) cannot effectively extract features, resulting in the problem of network non-convergence. Additionally, some existing TSS classification methods leverage multi-map mainly by simple linear connections, which do not thoroughly learn the supplementary information of CTP <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. Therefore, we design a classification model based on dynamic convolution and multi-map fusion to classify the TSS. Firstly, we replace the ordinary convolution in the feature extraction network with dynamic convolution (DConv) <ref type="bibr" target="#b10">[11]</ref> to improve the network performance without increasing the network complexity. Secondly, low-order multi-map features are fused to enhance the acquisition of local information by multi-scale feature fusion (MFF). Then high-order features are fused to obtain the global association information by transformer fusion (Transfus). Finally, multi-head pooling attention (MPA) is used to emphasize the high-order features, and the most discriminative features are selected to classify TSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The main framework of our proposed method is depicted in Fig. <ref type="figure" target="#fig_1">1</ref>. Specifically, the feature extraction of each map is performed independently using four feature extraction networks, each consisting of five stages. Dconv <ref type="bibr" target="#b10">[11]</ref> is used to improve network performance without increasing complexity. In the first three stages, MFF is used to capture and fuse the details of different scales of multi-map features. In the last two stages, Transfus is used to fuse the global correlation of the high-order features. The learned multi-map high-order features are put into the MPA to learn them further and merge the potential tensor sequence. Finally, the selected features are put into the full connection layer to achieve the classification of TSS.  The feature extraction network of a single map consists of five stages. The structure of each stage is shown in Fig. <ref type="figure" target="#fig_1">1 (a)</ref>. Considering the high data dimension and the small number of samples, the network layers are too deep to cause over-fitting. We replace traditional convolution with dynamic convolution <ref type="bibr" target="#b10">[11]</ref>. DConv improves the model expression ability by fusing multiple convolution cores, and its structure is shown in Fig. <ref type="figure" target="#fig_1">1 (b</ref>). DConv will can not increase the network complexity and thus improve the network performance. It will not increase too many parameters and calculation amount while increasing the capacity of the model. Inspired by static perceptron <ref type="bibr" target="#b11">[12]</ref>, Dconv has k convolution cores, which share the same core size and input/output dimensions. An attention block is used <ref type="bibr" target="#b12">[13]</ref> to generate the attention weight of k convolution cores, and finally aggregate the results through convolution. The formula is shown in Eq. ( <ref type="formula">1</ref>):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dynamic Convolution Feature Extraction Network</head><formula xml:id="formula_0">y = k i=1 π i (x)Conv(x), s.t 0 ≤ π i (x) ≤ 1, k i=1 π i (x) = 1 ( 1 )</formula><p>where π i (x) is the weight of the i-th convolution, which varies with each input x. . The attention block compresses the features of each channel through global average pooling and then uses two fully connected layers (with ReLU activation function between them) and a Softmax function to generate the attention weight of k convolution cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-map Fusion Module</head><p>For multi-map information fusion of low-order features, considering the small area of acute stroke focus, a multi-scale attention module is used to fuse multi-map features in the first three stages. Its structure is shown in Fig. <ref type="figure" target="#fig_1">1 (c</ref>). Record the output feature of each stage as x ij (i = 1, 2, 3; j = 1, 2, 3, 4), where i represents the output feature of the i-th stage, and j represents the j-th map. The feature x i of the input multi-scale channel attention module are denoted as:</p><formula xml:id="formula_1">x i = x i1 ⊕ x i2 ⊕ x i3 ⊕ x i4<label>(2)</label></formula><p>By setting different global average pooling (GAP) sizes, we can focus on the interactive feature information in channel dimensions at multiple scales, and aggregate local and global features. Through point-wise convolution (PWConv), point-wise channel interaction is used for each spatial location to realize the integration of local information. The local channel features are calculated as follows:</p><formula xml:id="formula_2">L(x i ) = PWConv(ReLu(PWConv(GAP(x i )))) (3) The core size of PWConv is C r × C × 1 × 1 × 1 and C × C r × 1 × 1 × 1.</formula><p>The global channel features are calculated as follows:</p><formula xml:id="formula_3">G(x i ) = PWConv(ReLu(PWConv(x i )))<label>(4)</label></formula><p>The final feature x i is calculated as follows:</p><formula xml:id="formula_4">x i = x i ⊗ σ (L(x i ) ⊕ G(x i )) (5)</formula><p>For the fusion of multi-map information of high-order features, considering the relevance of global information between different modes, the self-attention mechanism <ref type="bibr" target="#b13">[14]</ref> in the transformer is used to learn multi-map information, and its structure is shown in Fig. <ref type="figure" target="#fig_1">1</ref> (d). Specifically, the output characteristic of each stage is x ij (i = 4, 5; j = 1, 2, 3, 4), where i represents the output feature of the i-th stage, and j represents the j-th mode. Similar to the previous work of some scholars <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>, we think that the middle feature map of each mode is a set rather than a patch, and treat each element in the set as a token <ref type="bibr" target="#b18">[19]</ref>. At this time, each token takes into account all the token information of the four branches. Finally, the fused features are superimposed on the branches for the next stage. Let the characteristic x ∈ R N ×D of the input transformer block, where N is the number of tokens in the sequence and each token is represented by the feature vector of dimension D. It uses the scaling dot product between Query and Key to calculate the focus weight and aggregates the value of each Query. Finally, nonlinear transformation is used to calculate the fused output features x out . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-head Pooling Attention</head><p>With the deepening of the network layers, the semantic information contained in the output features becomes higher and higher. After the post-fusion of the branch network, we use an MPA to learn the high-order semantic details further. Here, a smaller number of tokens is used to increase the dimension of each token to facilitate the storage of more information. Unlike the original multi-head attention (MHA) operator <ref type="bibr" target="#b13">[14]</ref>, the multi-head pooling attention module gathers the potential tensor sequence to reduce the length of the input sequence, and its structure is shown in Fig. <ref type="figure" target="#fig_1">1</ref> (e). Like MHA <ref type="bibr" target="#b13">[14]</ref>, Query, Key, and Value are obtained through the linear operation. Add the corresponding pooling layer to Query, Key, and Value to further sample it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Configuration</head><p>Dataset and Data Preprocessing. The dataset of 200 AIS patients in this paper is from a local hospital. The patients are divided into two categories: positive (TSS &lt; 6 h) and negative (TSS ≥ 6 h). Finally, 133 in the positive subjects and 67 in the negative subjects are included. Each subject contains CBF, CBV, MTT, and Tmax. The size of all CTP images is set to 256 × 256 × 32.</p><p>Experimental Setup. The network structure is based on PyTorch 1.9.0 framework and CUDA 11.2 Titan × 2. We use a five-fold cross-validation method to verify the effectiveness of our method. 80% of the data is used as a training set and 20% as a test set. During the training process, the Adam optimizer optimizes the parameters, and the learning rate is set to 0.00001. The learning strategy of fixed step attenuation is adopted, in which the step size is set to 15, γ is 0.8. The number of iterations of training is 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results and Analysis</head><p>Comparative Study. We evaluate the effectiveness of our method by comparing it with other approaches on the same dataset <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. The results in Table <ref type="table" target="#tab_0">1</ref> demonstrate that our model achieves at least a 5% higher accuracy than the other methods and outperforms them in other evaluation indicators. Table <ref type="table" target="#tab_0">1</ref> also shows each method's area under the curve (AUC). Our method achieves an 81% AUC in the TSS classification task, indicating its superiority. To provide a more intuitive comparison of the model's performance under different indicators, we created radar charts to represent the evaluation results of the comparative experiment, as shown in Fig. <ref type="figure" target="#fig_3">2</ref>. These charts indicate that our method performs well in all evaluate indicators. To verify the reliability of our method, we conduct T-test verification on the comparison methods and find the p-value to be less than 0.05. Therefore, we believe that our method is valid.    Fusion Effectiveness. To effectively fuse the image features of CBV, CBF, MTT, and Tmax and realize the task of classifying TSS, This section verifies the fusion method in this paper. We mainly compare it with five different feature fusion methods. They are 1) Addition Fusion (AF); 2) Concatenation with Conv Fusion (CCF); 3) Addition with Conv Fusion (ACF); 4) Deep Concatenation Fusion (DCF); 5) Deep Addition Fusion (DAF). The details of these five fusion methods are shown in Fig. <ref type="figure" target="#fig_4">3</ref>. Based on the feature extraction network used in this paper, the comparison results are shown in Table <ref type="table" target="#tab_1">2</ref>. It can be seen from the results that the addition fusion method can better fuse features than the concatenation fusion method. The method we proposed is also to fuse features with the addition method. Our method is better than 1) and 4) because we have further learned the fused features. Therefore, our fusion method is the best.</p><p>Ablation Study. To assess the efficacy of each module in the proposed method, a series of ablation experiments are conducted by gradually incorporating the four main modules, namely Dconv, MFF, TransF, and MPA, into the backbone network. The results of these experiments are presented in Table <ref type="table" target="#tab_2">3</ref>. The first four lines in Table <ref type="table" target="#tab_2">3</ref> indicate that adding a single module is sufficient to enhance the performance of the backbone net-work. The CTP data is characterized by small size and high dimension, posing challenges to the deep learning model training. The network can extract more critical features without increasing the network depth by substituting the convolution with dynamic convolution on the backbone network. Map fusion on the backbone net-work improves the model accuracy by exploiting complementary information from multiple maps. Subsequently, MPA is added to extract more profound features from the learned features, ultimately improves the model's overall performance. In conclusion, the ablation experiments demonstrate that including the four modules in the proposed method positively impacts the model performance.</p><p>Map Combination Experiment. To investigate the impact of different modes on the time window of disease onset, a series of experiments are conducted on various mode combinations using the techniques proposed in this study. The results of different map combinations are presented in detail in Table <ref type="table" target="#tab_3">4</ref>. Comparing the results of the second to fourth rows in Table <ref type="table" target="#tab_3">4</ref> shows that the CBV mode fusion exhibits a higher classification rate, which is superior to the fusion of other groups of modes. Furthermore, the experimental outcomes of the single map are slightly lower than the experimental results after fusion, indicating that multi-map fusion can assist the feature extraction network in obtaining more crucial disease information, thereby enhancing the effectiveness of our network. These observations demonstrate that our approach can improve the TSS classification result by learning the multi-map relationship.</p><p>Comparison with SOTA Methods. In Table <ref type="table" target="#tab_4">5</ref>, we have chosen relevant works for comparison. These studies aim to classify TSS based on brain images, with a time threshold of 4.5 h. The results demonstrate that our method performs relatively well in comparison. Specifically, our method achieves the best AUC and ACC among all methods, as reported in <ref type="bibr" target="#b7">[8]</ref>. This may be attributed to the relatively large size of their dataset. Compared to studies such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>, our method achieves better results despite using the same amount of data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we propose a TSS classification model that integrates dynamic convolution and multi-map fusion to enable rapid and accurate diagnosis of unknown stroke cases.</p><p>Our approach leverages the dynamic convolution mechanism to enhance model representation without introducing additional network complexity. We also employ a multi-map fusion strategy, consisting of MFF and TransF, to incorporate local and global correlations across low-order and high-order features, respectively. Furthermore, we introduce an MPA module to extract and incorporate as much critical feature information as possible. Through a series of rigorous experiments, our proposed method outperforms several state-of-the-art models in accuracy and robustness. Our findings suggest that our approach holds immense promise in assisting medical practitioners in making effective diagnosis decisions for TSS classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The framework of our proposed method.</figDesc><graphic coords="3,46,80,246,50,331,00,104,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Radar chart of different model performances.</figDesc><graphic coords="6,104,49,234,08,243,85,182,17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The flow chart of different fusion methods.</figDesc><graphic coords="6,75,48,456,98,309,82,55,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of results of different methods (%). (p-value &lt; 0.05)</figDesc><table><row><cell>Methods</cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Precision</cell><cell>F1-score</cell><cell>Kappa</cell><cell>AUC</cell></row><row><cell>ResNet18</cell><cell>75.04 ± 4.39</cell><cell>92.42 ± 6.04</cell><cell>75.59 ± 3.22</cell><cell>83.07 ± 3.25</cell><cell>37.05 ± 10.17</cell><cell>69.57 ± 5.03</cell></row><row><cell>ResNet34</cell><cell>77.56 ± 4.57</cell><cell>91.74 ± 6.07</cell><cell>78.44 ± 4.07</cell><cell>84.43 ± 3.36</cell><cell>44.82 ± 11.65</cell><cell>77.07 ± 5.15</cell></row><row><cell>ResNet50</cell><cell>74.01 ± 2.63</cell><cell>91.62 ± 8.80</cell><cell>75.22 ± 3.46</cell><cell>82.31 ± 2.65</cell><cell>34.31 ± 6.70</cell><cell>67.60 ± 8.44</cell></row><row><cell>VGG11</cell><cell>73.52 ± 4.68</cell><cell>86.38 ± 7.62</cell><cell>76.92 ± 3.87</cell><cell>81.17 ± 3.90</cell><cell>36.71 ± 10.48</cell><cell>63.59 ± 5.28</cell></row><row><cell>AlexNet</cell><cell>74.02 ± 4.37</cell><cell>84.9 ± 7.28</cell><cell>78.31 ± 4.48</cell><cell>81.23 ± 3.51</cell><cell>38.93 ± 11.06</cell><cell>69.54 ± 6.24</cell></row><row><cell>CoAtNet0</cell><cell>76.00 ± 4.21</cell><cell>91.00 ± 3.25</cell><cell>77.38 ± 4.76</cell><cell>83.51 ± 2.30</cell><cell>40.17 ± 13.63</cell><cell>67.48 ± 8.29</cell></row><row><cell>[20]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C3D [21]</cell><cell>74.95 ± 2.98</cell><cell>87.09 ± 8.89</cell><cell>78.22 ± 3.54</cell><cell>82.11 ± 2.77</cell><cell>40.18 ± 7.28</cell><cell>67.78 ± 5.31</cell></row><row><cell>I3D [22]</cell><cell>73.09 ± 4.84</cell><cell>92.48 ± 7.08</cell><cell>74.07 ± 4.86</cell><cell>82.03 ± 3.17</cell><cell>30.28 ± 14.93</cell><cell>69.15 ± 7.35</cell></row><row><cell cols="2">MFNet [23] 74.55 ± 4.49</cell><cell>90.91 ± 6.97</cell><cell>75.99 ± 4.47</cell><cell>82.58 ± 3.23</cell><cell>36.11 ± 12.57</cell><cell>71.08 ± 4.78</cell></row><row><cell>SSFTTNet</cell><cell>75.95 ± 4.16</cell><cell>89.32 ± 10.00</cell><cell>78.09 ± 3.60</cell><cell>83.00 ± 3.82</cell><cell>41.70 ± 8.58</cell><cell>74.74 ± 6.05</cell></row><row><cell>[24]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Slowfast</cell><cell>76.05 ± 4.82</cell><cell>85.75 ± 4.70</cell><cell>79.79 ± 4.01</cell><cell>82.62 ± 3.73</cell><cell>44.16 ± 10.84</cell><cell>74.21 ± 5.58</cell></row><row><cell>[25]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>82.99 ± 4.14</cell><cell>89.40 ± 5.77</cell><cell>85.89 ± 4.61</cell><cell>87.45 ± 3.12</cell><cell>60.97 ± 9.65</cell><cell>81.48 ± 5.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The results of different fusion methods (%).</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Precision</cell><cell>F1-score</cell><cell>Specificity</cell><cell>Kappa</cell></row><row><cell>AF</cell><cell>79.44 ± 3.70</cell><cell>90.91 ± 5.19</cell><cell>80.68 ± 2.33</cell><cell>85.43 ± 2.80</cell><cell>56.59 ± 7.08</cell><cell>50.76 ± 8.54</cell></row><row><cell>CCF</cell><cell>79.41 ± 6.04</cell><cell>90.97 ± 7.77</cell><cell>81.05 ± 5.52</cell><cell>85.45 ± 4.06</cell><cell>56.37 ± 19.05</cell><cell>50.05 ± 17.31</cell></row><row><cell>ACF</cell><cell>79.45 ± 2.56</cell><cell>90.17 ± 4.44</cell><cell>81.19 ± 2.75</cell><cell>85.36 ± 1.91</cell><cell>58.02 ± 9.37</cell><cell>50.99 ± 6.59</cell></row><row><cell>DCF</cell><cell>77.99 ± 2.80</cell><cell>95.50 ± 3.17</cell><cell>77.19 ± 3.72</cell><cell>85.27 ± 1.32</cell><cell>43.19 ± 13.94</cell><cell>43.46 ± 10.32</cell></row><row><cell>DAF</cell><cell>80.48 ± 3.90</cell><cell>93.16 ± 5.08</cell><cell>80.62 ± 3.05</cell><cell>86.37 ± 2.90</cell><cell>55.27 ± 8.82</cell><cell>52.46 ± 9.26</cell></row><row><cell>Ours</cell><cell>82.99 ± 4.14</cell><cell>89.40 ± 5.77</cell><cell>85.89 ± 4.61</cell><cell>87.45 ± 3.12</cell><cell>70.33 ± 11.18</cell><cell>60.97 ± 9.65</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study (%) (D: DConv, M1: MFF, T: TransF, and M2: MPA).</figDesc><table><row><cell>Module</cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Precision</cell><cell>F1-score</cell><cell>Kappa</cell><cell>AUC</cell></row><row><cell>D</cell><cell>77.05 ± 2.70</cell><cell>90.23 ± 3.32</cell><cell>78.54 ± 2.58</cell><cell>83.94 ± 2.01</cell><cell>44.33 ± 6.48</cell><cell>66.9 + 7.91</cell></row><row><cell>M1</cell><cell>77.48 ± 2.06</cell><cell>89.34 ± 9.20</cell><cell>79.72 ± 3.14</cell><cell>83.92 ± 2.60</cell><cell>46.78 ± 14.24</cell><cell>66.15 ± 23.87</cell></row><row><cell>T</cell><cell>76.91 ± 4.62</cell><cell>85.67 ± 5.76</cell><cell>80.9 ± 3.37</cell><cell>83.12 ± 3.51</cell><cell>43.51 ± 14.76</cell><cell>73.10 ± 7.71</cell></row><row><cell>M1 + T</cell><cell>78.01 ± 1.81</cell><cell>86.44 ± 4.39</cell><cell>81.98 ± 4.65</cell><cell>83.96 ± 0.73</cell><cell>54.60 ± 15.90</cell><cell>72.96 ± 3.60</cell></row><row><cell>M1 + M2</cell><cell>78.48 ± 2.48</cell><cell>90.94 ± 5.75</cell><cell>79.66 ± 1.50</cell><cell>84.83 ± 2.25</cell><cell>51.78 ± 14.24</cell><cell>76.04 ± 5.45</cell></row><row><cell>T + M2</cell><cell>78.00 ± 2.74</cell><cell>91.60 ± 6.90</cell><cell>78.96 ± 2.95</cell><cell>84.65 ± 2.40</cell><cell>56.54 ± 10.46</cell><cell>74.69 ± 3.08</cell></row><row><cell>M1 + T + M2</cell><cell>80.04 ± 5.07</cell><cell>90.04 ± 4.23</cell><cell>81.82 ± 4.57</cell><cell>85.74 ± 3.51</cell><cell>58.26 ± 6.18</cell><cell>77.82 ± 12.48</cell></row><row><cell>Ours</cell><cell>82.99 ± 4.14</cell><cell>89.40 ± 5.77</cell><cell>85.89 ± 4.61</cell><cell>87.45 ± 3.12</cell><cell>60.97 ± 9.65</cell><cell>81.48 ± 5.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Map</figDesc><table><row><cell>Map</cell><cell></cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Precision</cell><cell>F1-score</cell><cell>Kappa</cell><cell>AUC</cell></row><row><cell>V F</cell><cell>M T</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>√</cell><cell></cell><cell cols="2">77.01 ± 4.68 85.67 ± 7.26</cell><cell cols="4">81.02 ± 3.66 83.11 ± 3.77 46.90 ± 10.30 76.58 ± 4.74</cell></row><row><cell>√</cell><cell>√</cell><cell cols="6">78.99 ± 5.18 83.50 ± 10.65 85.22 ± 5.46 83.85 ± 5.05 53.23 ± 10.13 71.61 ± 11.49</cell></row><row><cell cols="2">√ √</cell><cell cols="2">79.03 ± 3.94 90.14 ± 6.48</cell><cell cols="4">81.08 ± 5.16 85.11 ± 2.58 49.61 ± 11.83 75.46 ± 5.71</cell></row><row><cell>√</cell><cell>√</cell><cell cols="2">78.03 ± 3.43 90.91 ± 6.46</cell><cell cols="3">79.49 ± 4.38 84.59 ± 2.41 46.54 ± 9.99</cell><cell>75.22 ± 7.24</cell></row><row><cell cols="2">√ √ √</cell><cell cols="2">80.99 ± 2.93 91.68 ± 5.66</cell><cell cols="3">81.89 ± 0.90 86.44 ± 2.53 54.77 ± 5.19</cell><cell>75.18 ± 3.86</cell></row><row><cell>√</cell><cell>√ √</cell><cell cols="2">80.50 ± 5.66 89.40 ± 5.09</cell><cell cols="4">83.41 ± 7.98 86.00 ± 3.71 53.90 ± 15.11 75.93 ± 5.73</cell></row><row><cell cols="2">√ √ √</cell><cell cols="2">79.96 ± 3.79 95.41 ± 5.02</cell><cell cols="3">78.96 ± 3.04 86.34 ± 2.75 49.95 ± 8.97</cell><cell>76.94 ± 8.99</cell></row><row><cell cols="2">√ √ √ √</cell><cell cols="2">82.99 ± 4.14 89.40 ± 5.77</cell><cell cols="3">85.89 ± 4.61 87.45 ± 3.12 60.97 ± 9.65</cell><cell>81.48 ± 5.74</cell></row></table><note><p>combination study (%) (V: CBV, F: CBF, M: MTT, and T: Tmax).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison of results of SOTA methods.</figDesc><table><row><cell cols="2">Ref. Data</cell><cell cols="5">Subjects AUC sensitivity specificity Accuracy</cell></row><row><cell>[9]</cell><cell>DWI; FLAIR</cell><cell>422</cell><cell>0.74</cell><cell>0.70</cell><cell>0.81</cell><cell>0.758</cell></row><row><cell>[8]</cell><cell>DWI; FLAIR</cell><cell cols="3">342;245 0.896 0.823</cell><cell>0.827</cell><cell>0.878</cell></row><row><cell cols="2">[27] DWI; FLAIR</cell><cell cols="2">404;368 -</cell><cell>0.777</cell><cell>0.802</cell><cell>0.791</cell></row><row><cell cols="2">[28] DWI; MRI; ADC</cell><cell>25;26</cell><cell cols="2">0.754 0.952</cell><cell>0.500</cell><cell>0.788</cell></row><row><cell cols="2">[26] DWI; FLAIR</cell><cell>173;95</cell><cell>-</cell><cell>0.769</cell><cell>0.840</cell><cell>0.805</cell></row><row><cell>[6]</cell><cell cols="2">DWI; FLAIR; MR perfusion 85;46</cell><cell cols="2">0.765 0.788</cell><cell>-</cell><cell>-</cell></row><row><cell>[7]</cell><cell>DWI;FLAIR;ADC</cell><cell cols="2">149;173 -</cell><cell>0.48</cell><cell>0.91</cell><cell>-</cell></row><row><cell cols="2">Ours CTP</cell><cell>133;67</cell><cell cols="2">0.807 0.859</cell><cell>0.894</cell><cell>0.830</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62201360</rs>, <rs type="grantNumber">62101338</rs>, <rs type="grantNumber">61871274</rs>, and <rs type="grantNumber">U1902209</rs>), <rs type="funder">National Natural Science Foundation of Guangdong Province</rs> (<rs type="grantNumber">2019A1515111205</rs>), <rs type="person">Guangdong Basic</rs> and <rs type="person">Applied Basic Research</rs> (<rs type="grantNumber">2021A1515110746</rs>), <rs type="funder">Shenzhen Key Basic Research Project</rs> (<rs type="grantNumber">KCXFZ20201221173213036</rs>, <rs type="grantNumber">JCYJ20220818095809021</rs>, <rs type="grantNumber">SGDX202011030958020-07</rs>, <rs type="grantNumber">JCYJ201908081556188-06</rs>, and <rs type="grantNumber">JCYJ20190808145011259</rs>) Capital's Funds for Health Improvement and Research (No. <rs type="grantNumber">2022-1-2031</rs>), <rs type="funder">Beijing Hospitals Authority's Ascent Plan</rs> (No. <rs type="grantNumber">DFL20220303</rs>), and <rs type="funder">Beijing Key Specialists in Major Epidemic Prevention and Control</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7UTXeJb">
					<idno type="grant-number">62201360</idno>
				</org>
				<org type="funding" xml:id="_vK7WzCB">
					<idno type="grant-number">62101338</idno>
				</org>
				<org type="funding" xml:id="_tJadDgr">
					<idno type="grant-number">61871274</idno>
				</org>
				<org type="funding" xml:id="_QkdJgbj">
					<idno type="grant-number">U1902209</idno>
				</org>
				<org type="funding" xml:id="_83pJjQp">
					<idno type="grant-number">2019A1515111205</idno>
				</org>
				<org type="funding" xml:id="_enAMNBd">
					<idno type="grant-number">2021A1515110746</idno>
				</org>
				<org type="funding" xml:id="_pq9d4AB">
					<idno type="grant-number">KCXFZ20201221173213036</idno>
				</org>
				<org type="funding" xml:id="_h8VAchY">
					<idno type="grant-number">JCYJ20220818095809021</idno>
				</org>
				<org type="funding" xml:id="_eWQMCGU">
					<idno type="grant-number">SGDX202011030958020-07</idno>
				</org>
				<org type="funding" xml:id="_uZW5uZ2">
					<idno type="grant-number">JCYJ201908081556188-06</idno>
				</org>
				<org type="funding" xml:id="_D6KQykA">
					<idno type="grant-number">JCYJ20190808145011259</idno>
				</org>
				<org type="funding" xml:id="_dVAuSum">
					<idno type="grant-number">2022-1-2031</idno>
				</org>
				<org type="funding" xml:id="_Hm3q9kR">
					<idno type="grant-number">DFL20220303</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Management of acute ischemic stroke</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Phipps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Cronin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RMD Open</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="page">6983</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The concept of ischemic penumbra in acute stroke and therapeutic opportunities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paciaroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Caso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agnelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Neurol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="321" to="330" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wake-up stroke: from pathophysiology to management</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peter-Derex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Derex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sleep Med. Rev</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">101212</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theoretic basis and technical implementations of CT perfusion in acute ischemic stroke, part 1: theoretic basis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goldmakher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Neuroradiol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="662" to="668" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying acute ischemic stroke onset time using deep imaging features</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Speier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>El-Saden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="892" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A machine learning approach for classifying ischemic stroke onset time from imaging</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Speier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scalzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>El-Saden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1666" to="1676" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine learning approach to identify stroke within 4.5 hours</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stroke</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="860" to="866" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Development and external validation of a stability machine learning model to identify wake-up stroke onset time from MRI</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3661" to="3669" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intra-domain task-adaptive transfer learning to determine acute ischemic stroke onset time</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">101926</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying acute ischemic stroke patients within the thrombolytic treatment window using deep learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Polson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neuroimaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic convolution: attention over convolution kernels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11030" to="11039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The perceptron, a perceiving and recognizing automaton Project Para</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
		</imprint>
		<respStmt>
			<orgName>Cornell Aeronautical Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">ImageBERT: cross-modal pre-training with large-scale weak-supervised image-text data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sacheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07966</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">VideoBERT: a joint model for video and language representation learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TransFuser: Imitation with transformer-based sensor fusion for autonomous driving</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2022.3200245</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2022.3200245" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CoatNet: marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="352" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral-spatial feature tokenization transformer for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An automatic machine learning approach for ischemic stroke onset time identification based on DWI and FLAIR imaging</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage: Clin</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">102744</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep learning approaches to identify patients within the thrombolytic treatment window</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Polson</surname></persName>
		</author>
		<idno type="DOI">10.1111/jon.13043</idno>
		<ptr target="https://doi.org/10.1111/jon.13043" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MRI radiomic features-based machine learning approach to classify ischemic stroke onset time</title>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurol</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="350" to="360" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
