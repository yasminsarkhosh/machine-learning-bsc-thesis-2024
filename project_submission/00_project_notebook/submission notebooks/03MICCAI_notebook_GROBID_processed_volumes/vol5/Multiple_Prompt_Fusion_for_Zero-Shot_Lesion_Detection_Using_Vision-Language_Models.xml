<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Miaotian</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huahui</forename><surname>Yi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Biomedical Big Data Center</orgName>
								<orgName type="institution">West China Hospital</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziyuan</forename><surname>Qin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Biomedical Big Data Center</orgName>
								<orgName type="institution">West China Hospital</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiying</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aidong</forename><surname>Men</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qicheng</forename><surname>Lao</surname></persName>
							<email>qicheng.lao@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="283" to="292"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">4FC6109C7B5F5E5F2A731E068326C348</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_28</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vision-language models</term>
					<term>Lesion detection</term>
					<term>Multiple prompts</term>
					<term>Prompt fusion</term>
					<term>Ensemble learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of large-scale pre-trained vision-language models (VLM) has provided a promising direction of transferring natural image representations to the medical domain by providing a well-designed prompt with medical expert-level knowledge. However, one prompt has difficulty in describing the medical lesions thoroughly enough and containing all the attributes. Besides, the models pre-trained with natural images fail to detect lesions precisely. To solve this problem, fusing multiple prompts is vital to assist the VLM in learning a more comprehensive alignment between textual and visual modalities. In this paper, we propose an ensemble guided fusion approach to leverage multiple statements when tackling the phrase grounding task for zero-shot lesion detection. Extensive experiments are conducted on three public medical image datasets across different modalities and the detection accuracy improvement demonstrates the superiority of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical lesion detection plays an important role in assisting doctors with the interpretation of medical images for disease diagnosing, cancer staging, etc., which can improve efficiency and reduce human errors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref>. Current object detection approaches are mainly based on supervised learning with abundant well-paired image-level annotations, which heavily rely on expert-level knowledge. As such, these supervised approaches may not be suitable for medical lesion detection due to the laborious labeling.</p><p>Recently, large-scale pre-trained vision-language models (VLMs), by learning the visual concepts in the images through the weak labels from text, have prevailed in natural object detection or visual grounding and shown extraordinary performance. These models, such as GLIP <ref type="bibr" target="#b10">[11]</ref>, X-VLM <ref type="bibr" target="#b9">[10]</ref>, and VinVL <ref type="bibr" target="#b23">[24]</ref>, can perform well in detection tasks without supervised annotations. Therefore, substituting conventional object detection with VLMs is possible and necessary. The VLMs are first pre-trained to learn universal representations via large-scale unlabelled data and can be effectively transferred to downstream tasks. For example, a recent study <ref type="bibr" target="#b14">[15]</ref> has demonstrated that the pre-trained VLMs can be used for zero-shot medical lesion detection with the help of well-designed prompts.</p><p>However, current existing VLMs are mostly based on a single prompt to establish textual and visual alignment. This prompt needs refining to cover all the features of the target as much as possible. Apparently, even a well-designed prompt is not always able to combine all expressive attributes into one sentence without semantic and syntactic ambiguity, e.g., the prompt design for melanoma detection should include numerous kinds of information describing attributes complementing each other, such as shape, color, size, etc <ref type="bibr" target="#b7">[8]</ref>. In addition, each keyword in a single lengthy prompt cannot take effect equally as we expect, where the essential information can be ignored. This problem motivates us to study alternative approaches with multiple prompt fusion.</p><p>In this work, instead of striving to design a single satisfying prompt, we aim to take advantage of pre-trained VLMs in a more flexible way with the form of multiple prompts, where each prompt can elicit respective knowledge from the model which can then be fused for better lesion detection performance. To achieve this, we propose an ensemble guided fusion approach derived from clustering ensemble learning <ref type="bibr" target="#b2">[3]</ref>, where we design a step-wise clustering mechanism to gradually screen out the implausible intermediate candidates during the grounding process, and an integration module to obtain the final results by uniting the mutually independent candidates from each prompt. In addition, we also examine the language syntax based prompt fusion approach as a comparison, and explore several fusion strategies by first grouping the prompts either with described attributes or categories and then repeating the fusion process.</p><p>We evaluate the proposed approach on a broad range of public medical datasets across different modalities including photography images for skin lesion detection ISIC 2016 <ref type="bibr" target="#b1">[2]</ref>, endoscopy images for polyp detection CVC-300 <ref type="bibr" target="#b20">[21]</ref>, and cytology images for blood cell detection BCCD. The proposed approach exhibits extraordinary superiority compared to those with single prompt and other common ensemble learning based methods for zero-shot medical lesion detection. Considering the practical need of lesion detection, we further provide significantly improved fine-tuning results with a few labeled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object Detection and Vision-Language Models. In the vision-language field, phrase grounding can be regarded as another solution for object detection apart from conventional R-CNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref>. Recently, vision-language models have achieved exciting performance in the zero-shot and few-shot visual recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. GLIP <ref type="bibr" target="#b10">[11]</ref> unifies phrase grounding and object detection tasks, demonstrating outstanding transfer capability. In addition, ViLD <ref type="bibr" target="#b6">[7]</ref> is proposed for open-vocabulary object detection taking advantage of the rich knowledge learned from CLIP <ref type="bibr" target="#b3">[4]</ref> and text input.</p><p>Ensemble Learning. As pointed out by a review <ref type="bibr" target="#b2">[3]</ref>, ensemble learning methods achieve better performance by producing predictions based on extracted features and fusing via various voting mechanisms. For example, a selective ensemble of classifier chains <ref type="bibr" target="#b12">[13]</ref> is proposed to reduce the computational cost and the storage cost arose in multi-label learning <ref type="bibr" target="#b11">[12]</ref> by decreasing the ensemble size. UNDEED <ref type="bibr" target="#b22">[23]</ref>, a semi-supervised classification method, is presented to increase the classifier accuracy on labeled data and diversity on unlabeled data simultaneously. And a hybrid semi-supervised clustering ensemble algorithm <ref type="bibr" target="#b21">[22]</ref> is also proposed to generate basic clustering partitions with prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first briefly introduce the vision-language model for unifying object detection as phrase grounding, e.g., GLIP <ref type="bibr" target="#b10">[11]</ref> (Sect. 3.1). Then we present a simple language syntax based prompt fusion approach in Sect. 3.2. Finally, the proposed ensemble-guided fusion approach and several fusion strategies are detailed in Sect. 3.3 to improve the zero-shot lesion detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Phrase grounding is the task of identifying the fine-grained correspondence between phrases in a sentence and objects in an image. The GLIP model takes as input an image I and a text prompt p that describes all the M candidate categories for the target objects. Both inputs will go through specific encoders Enc I and Enc T to obtain unaligned representations. Then, GLIP uses a grounding module to align image boxes with corresponding phrases in the text prompt. The whole process can be formulated as follows:</p><formula xml:id="formula_0">O = Enc I (I), P = Enc T (p), S ground = OP , L cls = Loss(S ground ; T ),<label>(1)</label></formula><p>where O ∈ R N ×d , P ∈ R M ×d denote the image and text features respectively for N candidate region proposals and M target objects, S ground ∈ R N ×M represents the cross-modal alignment scores, and T ∈ {0, 1} N ×M is the target matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Syntax Based Prompt Fusion</head><p>As mentioned above, it is difficult for a single prompt input structure such as GLIP to cover all necessary descriptions even through careful designation of the prompt. Therefore, we propose to use multiple prompts instead of a single prompt for thorough and improved grounding. However, it is challenging to combine the grounding results from multiple prompts since manual integration is subjective, ineffective, and lacks uniform standards. Here, we take the first step to fuse the multiple prompts at the prompt level. We achieve this by extracting and fusing the prefixes and suffixes of each prompt based on language conventions and grammar rules. As shown in Fig. <ref type="figure" target="#fig_0">1</ref> (a), given serials of multiple prompts p 1 , p 2 , . . . , p k , the final fused prompt P fuse from k single prompts is given by:</p><formula xml:id="formula_1">P fuse = p 1 + p 2 + • • • + p k = (p pre 1 + p pre 2 + • • • + p pre k ) + body + (p suf 1 + p suf 2 + • • • + p suf k ) = P pre + body + P suf .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ensemble Learning Based Fusion</head><p>Although the syntax based fusion approach is simple and sufficient, it is restricted by the form of text descriptions which may cause ambiguity in the fused prompt during processing. Moreover, the fused prompts are normally too long that the model could lose proper attention to the key information, resulting in extremely unstable performance (results shown in Sect. 4.2).</p><p>Therefore, in this subsection, we further explore fusion approaches based on ensemble learning. More specifically, the VLM outputs a set of candidate region proposals C i for each prompt p i , and these candidates carry more multidimensional information than prompts. We find in our preliminary experiments that direct concatenation of the candidates is not satisfactory and effective, since simply integration hardly screens out the bad predictions. In addition, the candidate, e.g., c ij ∈ C i , carries richer information that can be further utilized, such as central coordinate x j and y j , region size w j and h j , category label, and prediction confidence score. Therefore, we consider step-wise clustering mechanisms using the above information to screen out the implausible candidates based on clustering ensemble learning <ref type="bibr" target="#b2">[3]</ref>.</p><p>Another observation in our preliminary experiments is that most of the candidates distribute near the target if the prompt description matches better with the object. Moreover, the candidate regions of inappropriate size containing too much background or only part of the object should be abandoned directly. As such, we consider clustering the center coordinate (x j , y j ) and region size (w j , h j ) respectively to filter out those candidates with the wrong location and size.</p><p>This step-wise clustering with the aid of different features embodies a typical ensemble learning idea. Therefore, we propose a method called Ensemble Guided Fusion based on semi-clustering ensemble, as detailed in Fig. <ref type="figure" target="#fig_0">1 (b</ref>). There are four sub-modules in our approach, where the location cluster f loc and size cluster f size discard the candidates with large deviations and abnormal sizes. Then, in the prediction corrector f correct , we utilize the voting mechanism to select the remaining candidates with appropriate category tags and relatively high prediction confidence. After the first three steps of processing, the remaining candidates C originated from each prompt can be written as:</p><formula xml:id="formula_2">C = C • f loc (C) • f size (C) • f correct (C).<label>(3)</label></formula><p>The remaining candidates are then transferred to the integration module for being integrated into the final fused result C fuse that is mutually independent:</p><formula xml:id="formula_3">C fuse = C . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>Besides, we also propose three fusion strategies to recluster candidates in different ways before executing ensemble guided fusion, i.e., fusing the multiple prompts equally, by category, and by attribute. Compared to the first strategy, fusing by category and by attribute both have an additional step of reorgnization. Candidates whose prompts belong to the same category or have identical attributes will share the similar distribution. Accordingly, we rearrange these candidates C i into a new set C for the subsequent fusion process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We collect three public medical image datasets across various modalities including skin lesion detection dataset ISIC 2016 <ref type="bibr" target="#b1">[2]</ref>, polyp detection dataset CVC-300 <ref type="bibr" target="#b20">[21]</ref>, and blood cell detection dataset BCCD to validate our proposed approach for zero-shot medical lesion detection. For the experiments, we use the GLIP-T variant <ref type="bibr" target="#b10">[11]</ref> as our base pre-trained model and adopt two metrics for the grounding evaluation, including Average Precision (AP) and AP50. More details on the dataset and implementation are described in the appendix. Multiple NMS <ref type="bibr" target="#b13">[14]</ref> 12.0 20.6 27.9 37.9 11.9 21.4 Soft-NMS <ref type="bibr" target="#b0">[1]</ref> 18.8 30. <ref type="bibr" target="#b2">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>This section demonstrates that our proposed ensemble guided fusion approach can effectively benefit the model's performance.</p><p>The Proposed Approach Achieves the Best Performance in Zero-Shot Lesion Detection Compared to Baselines. To confirm the validity of our method, we conduct extensive experiments under the zero-shot setting and include a series of fusion baselines: Concatenation, Non-Maximum Suppression (NMS) <ref type="bibr" target="#b13">[14]</ref>, Soft-NMS <ref type="bibr" target="#b0">[1]</ref> and Weighted Boxes Fusion (WBF) <ref type="bibr" target="#b19">[20]</ref> for comparisons. As illustrated in Table <ref type="table" target="#tab_0">1</ref>, our ensemble guided fusion rivals the GLIP <ref type="bibr" target="#b10">[11]</ref> with single prompt and other fusion baselines across all datasets. The first three rows in Table <ref type="table" target="#tab_0">1</ref> represent the results of single prompt by only providing shape, color, and location information, respectively. Furthermore, we conduct a comparison between YOLOv5 <ref type="bibr" target="#b16">[17]</ref> and our method on CVC-300 under 10-shot settings. Table <ref type="table" target="#tab_2">2</ref> shows that our method outperforms YOLOv5, which indicates fullysupervised models such as YOLO may not be suitable for medical scenarios where a large labeled dataset is often not available. In addition, we utilize the Automatic Prompt Engineering (APE) <ref type="bibr" target="#b24">[25]</ref> method to generate prompts. These prompts give comparable performance to our single prompt and can be still be improved by our fusion method. And the details are described in the appendix.    Here we present part of the single prompts used in the experiments for illustration. The misclassification problem in some of the single prompts is corrected (i.e., malignant to benign) on the first dataset. For all datasets, the candidate boxes are more precise and associated with higher confidence scores.</p><p>Fine-Tuned Models Can Further Improve the Detection Performance. We conduct 10-shot fine-tuning experiments as a complement, and find the performance greatly improved. As shown in Table <ref type="table" target="#tab_3">3</ref> and Fig. <ref type="figure" target="#fig_1">2</ref>, with the same group of multiple prompts, the accuracy of fine-tuned model has increased almost twice as much as that of zero-shot, further demonstrating the effectiveness of our method in both settings. Therefore, we can conclude that the pre-trained GLIP model has the ability to learn a reasonable alignment between textual and visual modalities in medical domains.</p><p>Visualizations. Figure <ref type="figure" target="#fig_2">3</ref> shows the visualization of the zero-shot results across three datasets. Syntax based fusion sometimes fails to filter out unreasonable predictions because these regions are generated directly by the VLM without further processing and eventually resulting in unstable detection performance. On the contrary, our approach consistently gives a better prediction that defeats all single prompts with a relatively proper description, yet syntax based fusion relies too much on the format and content of inputs, which results in great variance and uninterpretability. The step-wise clustering mechanism based on ensemble learning enables our method to exploit multi-dimensional information besides visual features. In addition, the key components in our proposed approach are unsupervised, which also enhances stability and generalization.</p><p>Comparison Among Fusion Strategies. In this work, we not only provide various solutions to fuse multiple prompts but also propose three fusion strategies to validate the generalization of ensemble-guided fusion. As shown in Table <ref type="table" target="#tab_4">4</ref>, we present the results obtained with three different fusion strategies: equally, by category, and by attribute. The first strategy is to process each prompt equally, which is the most convenient and suitable in any situation. Fusing prompts by category is specifically for multi-category datasets to first gather the prompts belonging to the same category and make further fusion. Similarly, Fusing by attribute is to fuse the candidates, whose prompts are describing the same attribute. The strategy of fusing by attribute, outperforms the other ones, due to the fact that candidates with the same attribute share a similar distribution, which is prone to obtain a more reasonable cluster. On the contrary, it is possible to neglect this distribution when fusing each prompt equally.</p><p>Ablation Study. As shown in Table <ref type="table" target="#tab_5">5</ref>, we perform ablation studies on three datasets. Our approach has three key components, i.e., location cluster, size cluster and prediction corrector. The location cluster filters out the candidates with severe deviation from the target. The size cluster removes those abnormal ones. Finally, the prediction corrector further eliminates the candidates that cause low accuracy. The results show that when combining the above three components, the proposed approach gives the best lesion detection performance, suggesting that all components are necessary and effective in the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose an ensemble guided fusion approach to leverage multiple text descriptions when tackling the zero-shot medical lesion detection based on vision-language models and conduct extensive experiments to demonstrate the effectiveness of our approach. Compared to a single prompt that typically requires exhaustive engineering and designation, the multiple medical prompts provide a flexible way of covering all key information that help with lesion detection. We also present several fusion strategies for better exploiting the relationship among multiple prompts. One limitation of our method is that it requires diverse prompts for effective clustering of the candidates. However, with the help of other prompt engineering methods, the limitation can be relatively alleviated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed approach: (a) Syntax Based Fusion, which fuses multiple prompts at the prompt level; (b) Ensemble Guided Fusion, which includes step-wise clustering mechanisms followed by voting and an integration module.</figDesc><graphic coords="4,44,79,54,02,334,51,155,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Fine-tuning v.s. zero-shot results on the ISIC 2016 dataset.</figDesc><graphic coords="7,58,98,53,72,334,51,164,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Comparisons of test results between before and after multi-prompt fusion under zero-shot settings. Here we present part of the single prompts used in the experiments for illustration. The misclassification problem in some of the single prompts is corrected (i.e., malignant to benign) on the first dataset. For all datasets, the candidate boxes are more precise and associated with higher confidence scores.</figDesc><graphic coords="7,58,98,254,81,334,48,195,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Our approaches v.s. single prompts and other fusion methods.</figDesc><table><row><cell cols="2">Prompt Method</cell><cell>ISIC 2016</cell><cell>CVC-300</cell><cell>BCCD</cell></row><row><cell></cell><cell></cell><cell cols="3">AP AP50 AP AP50 AP AP50</cell></row><row><cell>Single</cell><cell>GLIP [11]</cell><cell cols="3">10.5 20.0 29.8 37.9 8.9</cell><cell>18.4</cell></row><row><cell></cell><cell></cell><cell cols="3">11.3 22.7 16.8 21.7 12.2 23.1</cell></row><row><cell></cell><cell></cell><cell cols="3">13.6 25.5 20.2 30.3 9.6</cell><cell>18.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with YOLOv5.</figDesc><table><row><cell cols="3">Method Training mAP Test mAP</cell></row><row><cell cols="2">YOLOv5 16.2</cell><cell>6.4</cell></row><row><cell>Ours</cell><cell>60.8</cell><cell>50.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Zero-shot v.s. 10-shot results.</figDesc><table><row><cell cols="3">Dataset ISIC 2016 CVC-300</cell></row><row><cell cols="2">Zero-shot 19.8</cell><cell>36.1</cell></row><row><cell>10-shot</cell><cell>38.2</cell><cell>50.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results with different fusion strategies.</figDesc><table><row><cell>Dataset</cell><cell cols="2">ISIC 2016</cell><cell cols="2">CVC-300</cell><cell cols="2">BCCD</cell></row><row><cell>Strategy</cell><cell>AP</cell><cell>AP50</cell><cell>AP</cell><cell>AP50</cell><cell>AP</cell><cell>AP50</cell></row><row><cell>Equally</cell><cell>16.8</cell><cell>25.2</cell><cell>30.8</cell><cell>40.4</cell><cell>12.5</cell><cell>21.6</cell></row><row><cell>Category</cell><cell>13.2</cell><cell>20.4</cell><cell>30.8</cell><cell>40.4</cell><cell>15.3</cell><cell>24.9</cell></row><row><cell>Attribute</cell><cell>19.8</cell><cell>30.9</cell><cell>36.1</cell><cell>47.9</cell><cell>15.8</cell><cell>32.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation for key components in our proposed approach.</figDesc><table><row><cell cols="2">Components</cell><cell></cell><cell cols="2">ISIC 2016</cell><cell cols="2">CVC-300</cell><cell>BCCD</cell></row><row><cell>Location</cell><cell>Size</cell><cell>Prediction</cell><cell cols="4">AP (%) AP50 (%) AP (%) AP50 (%) AP (%) AP50 (%)</cell></row><row><cell>Cluster</cell><cell>Cluster</cell><cell>Corrector</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>13.9</cell><cell>26.4</cell><cell>24.2</cell><cell>30.1</cell><cell>10.9</cell><cell>20.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10.4</cell><cell>19.5</cell><cell>23.9</cell><cell>29.5</cell><cell>11.9</cell><cell>21.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>13.8</cell><cell>24.8</cell><cell>29.5</cell><cell>37.3</cell><cell>9.3</cell><cell>17.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>16.9</cell><cell>27.4</cell><cell>30.6</cell><cell>41.7</cell><cell>15.1</cell><cell>31.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>19.8</cell><cell>30.9</cell><cell>34.1</cell><cell>45.7</cell><cell>15.8</cell><cell>32.6</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_28.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th international symposium on biomedical imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey on ensemble learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="241" to="258" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04544</idno>
		<title level="m">Clip-adapter: better vision-language models with feature adapters</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13921</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ugly Duckling Sign&quot; in an effort to improve patient self-screening examinations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Elewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ABCDEF rule: combining the &quot;ABCDE Rule&quot; and the</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ElixirNet: relation-aware network architecture adaptation for medical lesion detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11093" to="11100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grounded language-image pre-training</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10965" to="10975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-label selective ensemble</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-20248-8_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-20248-8_7" />
	</analytic>
	<monogr>
		<title level="m">MCS 2015</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9132</biblScope>
			<biblScope unit="page" from="76" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selective ensemble of classifier chains</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-38067-9_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-38067-9_13" />
	</analytic>
	<monogr>
		<title level="m">MCS 2013</title>
		<editor>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7872</biblScope>
			<biblScope unit="page" from="146" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th international conference on pattern recognition (ICPR 2006)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Medical image understanding with pretrained vision language models: a comprehensive study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15517</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computer aided design and manufacturing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sarcar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narayan</surname></persName>
		</author>
		<ptr target="https://books.google.co.jp/books?id=zXdivq93WIUC" />
	</analytic>
	<monogr>
		<title level="j">PHI Learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weighted boxes fusion: Ensembling boxes from different object detection models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Solovyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gabruseva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">104117</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Healthcare Eng</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combined constraint-based with metric-based in semisupervised clustering ensemble</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1085" to="1100" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploiting unlabeled data to enhance ensemble diversity</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Disc</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="98" to="129" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VinVL: revisiting visual representations in vision-language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Large language models are human-level prompt engineers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
