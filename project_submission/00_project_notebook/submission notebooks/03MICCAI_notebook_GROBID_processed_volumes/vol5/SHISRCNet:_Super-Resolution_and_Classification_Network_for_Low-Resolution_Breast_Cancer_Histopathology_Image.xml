<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image</title>
				<funder ref="#_njHTkqn">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luyuan</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Cloud Media</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Cloud Media</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center for Software Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Cloud Media</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Cloud Media</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boyan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Cloud Media</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Qingni</forename><surname>Shen</surname></persName>
							<email>qingnishen@ss.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Cloud Media</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhonghai</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software and Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Cloud Media</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="23" to="32"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E0B2D1AFD6A18613FE2C58335BBD8A95</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>breast cancer</term>
					<term>histopathological image</term>
					<term>super-resolution</term>
					<term>classification</term>
					<term>joint training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rapid identification and accurate diagnosis of breast cancer, known as the killer of women, have become greatly significant for those patients. Numerous breast cancer histopathological image classification methods have been proposed. But they still suffer from two problems. (1) These methods can only hand high-resolution (HR) images. However, the low-resolution (LR) images are often collected by the digital slide scanner with limited hardware conditions. Compared with HR images, LR images often lose some key features like texture, which deeply affects the accuracy of diagnosis. (2) The existing methods have fixed receptive fields, so they can not extract and fuse multi-scale features well for images with different magnification factors. To fill these gaps, we present a Single Histopathological Image Super-Resolution Classification network (SHISRCNet), which consists of two modules: Super-Resolution (SR) and Classification (CF) modules. SR module reconstructs LR images into SR ones. CF module extracts and fuses the multi-scale features of SR images for classification. In the training stage, we introduce HR images into the CF module to enhance SHIS-RCNet's performance. Finally, through the joint training of these two modules, super-resolution and classified of LR images are integrated into our model. The experimental results demonstrate that the effects of our method are close to the SOTA methods with taking HR images as inputs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Breast cancer is one of the high-mortality cancers among women in the 21st century. Every year, 1.2 million women around the world suffer from breast cancer and about 0.5 million die of it <ref type="bibr" target="#b2">[3]</ref>. Accurate identification of cancer types will make a correct assessment of the patient's risk and improve the chances of survival. However, the traditional analysis method is time-consuming, as it mainly depends on the experience and skills of the doctors. Therefore, it is essential to develop computer-aided diagnosis (CADx) for assisting doctors to realize the rapid detection and classification.</p><p>Due to being collected by various devices, the resolution of histopathological images extracted may not always be high. Low-resolution (LR) images lack of lots of details, which will have an important impact on doctors' diagnosis. Considering the improvement of histopathological images' acquisition equipment will cost lots of money while significantly increasing patients' expense of detection. The super-resolution (SR) algorithms that improve the resolution of LR images at a small cost can be a practical solution to assist doctors in diagnosis. At present, most single super-resolution methods only have fixed receptive fields <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. These models cannot capture multi-scale features and do not solve the problems caused by LR in various magnification factors well. MRC-Net <ref type="bibr" target="#b5">[6]</ref> adopted LSTM <ref type="bibr" target="#b8">[9]</ref> and Multi-scale Refined Context to improve the effect of reconstructing histopathological images. It considered the problem of multi-scale, but only fused two scales features. This limits its performance in the scenarios with various magnification factors. Therefore, designing an appropriate feature extraction block for SR of the histopathological images is still a challenging task.</p><p>In recent years, a series of deep learning methods have been proposed to solve the breast cancer histopathological image classification issue by the highresolution (HR) histopathological images. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> improved the specific model structure to classify breast histopathology images, which showed a significant improvement in recognition accuracy compared with the previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>. SSCA <ref type="bibr" target="#b23">[24]</ref> considered the problem of multi-scale feature extraction which utilized feature pyramid network (FPN) <ref type="bibr" target="#b14">[15]</ref> and attention mechanism to extract discriminative features from complex backgrounds. However, it only concatenates multi-scale features and does not consider the problem of feature fusion. So it is still worth to explore the potential of extraction and fusion of multi-scale features for breast images classification.</p><p>To tackle the problem of LR breast cancer histopathological images reconstruction and diagnosis, we propose the Single Histopathological Image Super-Resolution Classification network (SHISRCNet) integrating Super-Resolution (SR) and Classification (CF) modules. The main contributions of this paper can be described as follows:</p><p>(1) In the SR module, we design a new block called Multi-Features Extraction block (MFEblock) as the backbone. MFEblock adopts multi-scale receptive fields to obtain multi-scale features. In order to better fuse multi-scale features, a new fusion method named multi-scale selective fusion (MSF) is used for multi-scale features. These make MFEblock reconstruct LR images into SR images well.</p><p>(2) The CF module completes the task of image classification by utilizing the SR images. Like SR module, it also needs to extract multi-scale features. The difference is that the CF module can use the method of downsampling to capture multi-scale features. So we combine the multi-scale receptive fields (SKNet) <ref type="bibr" target="#b12">[13]</ref>  with the feature pyramid network (FPN) to achieve the feature extraction of this module. In FPN, we design a cross-scale selective fusion block (CSFblock) to fuse features of different scales.</p><p>(3) Through the joint training of these two designed modules, the superresolution and classification of low-resolution histopathological images are integrated into our model. For improving the performance of CF module and reducing the error caused by the reconstructed SR images, we introduce HR images to CF module in the training stage. The experimental results demonstrate that the effects of our method are close to those of SOTA methods that take HR breast cancer histopathological images as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>This section describes the proposed SHISRCNet. The overall pipeline of the proposed network is shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. It composes two modules: SR module and CF module. The SR module reconstructs the LR image into the SR image. The CF module utilize the reconstructed SR images to diagnose histopathological images. In the training stage, we introduce HR images to improve the performance of CF module and alleviate the error caused by SR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Super-Resolution Module</head><p>To better extract and fuse multi-scale features for super-resolution, we propose a new SR network, called SRMFENet. Like SRResNet <ref type="bibr" target="#b10">[11]</ref>, SRMFENet takes a single low-resolution image as input and uses the pixelshuffle layer to get the restructured image. The difference between SRMFENet and SRResNet is that a Multi-Features Extraction block (MFEblock) is proposed to extract and fuse multi-scale histopathological images' features. The structure of the MFEblock is shown in Fig. <ref type="figure" target="#fig_0">1(b</ref>). The input features X capture multi-scale features Y i through four 3×3 atrous convolutions <ref type="bibr" target="#b3">[4]</ref> with different rates:</p><formula xml:id="formula_0">Y i = Cov3 × 3 rate=1 (X) Cov3 × 3 rate=2(i-1) (X + Y i-1 ) i = 1 1 &lt; i &lt;= n</formula><p>where n is the number of atrous convolutions and is set to 4 by the experiments. This design not only preserves the depth of the network, but also increases the width of the network. It is beneficial for the network to extract shallow local texture information and global semantic information. After the feature extraction phase, a new fusion method named MSF fuses all of different scale features Y i . In the end, the input features X are added with the fused features. The details of MSF show in the Fig. <ref type="figure" target="#fig_0">1(c</ref>). Firstly, we conduct Global Average Pooling (GAP) <ref type="bibr" target="#b13">[14]</ref> on the multi-scale features to obtain their average channel-wise weights. Then using Sigmoid activation function to map weight to 0 to 1. Next softmax operation normalizes the same position of the obtained multi-scale average channel-wise weights. Finally, the features are multiplied by the corresponding normalized weights and the processed features are added together to generate the new multi-scale features. MFEblock is very applicable to process histopathological images of different magnification factors, as it employs convolution and attention operations to capture local and global image context information and fuse them well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification Module</head><p>The task of the CF module is to classify the reconstructed SR images. It can use the method of downsampling to capture multi-scale features. So we combine the multi-scale receptive fields (SKNet as backbone network) with the FPN (a downsampling method) to achieve the feature extraction of this module. In Fig. <ref type="figure" target="#fig_0">1(a)</ref>, the multi-sacle features extracted by SKNet are the input of FPN. We propose a new fusion method, called cross-scale selective fusion block (CSFblock) to effectively fuse high-resolution and low-resolution features in FPN. After the fused features are processed by GAP, they are aggregated into a new multi-scale feature by concatenate operation. Finally, the aggregated multi-scale features are classified through the fully connected (FC) layer. The structure of CSFblock is shown in Fig. <ref type="figure" target="#fig_0">1(d)</ref>. The inputs of CSFblock are two-way inputs which are the high-resolution features X h ∈ W ×H×C and the low-resolution features X l ∈ W 1 ×H 1 ×C. In CSFblock, the upsampling operations are performed on the low-resolution features X l to realize consistency with X h dimension. X h and restructured X l are fused via an element-wise summation:</p><formula xml:id="formula_1">U = X h + Up(X l )</formula><p>Then, using GAP along the channel dimension to get the global information S. A FC layer generates a compact feature vector Z which guides the feature selection procedure. And Z is reconstructed into two weight vectors a, b of the same dimension as S through two FC layers, which can be defined as:</p><formula xml:id="formula_2">Z = δ(W c S), a = W a Z, b = W b Z</formula><p>where δ denotes ReLU and W a , W b , W c , means the weight of the FC layers. Specifically, a softmax operator is applied on a and b 's channel-wise digits:</p><formula xml:id="formula_3">a[i] = e a[i] e a[i] + e b[i] , b[i] = e b[i] e a[i] + e b[i] , i ∈ C</formula><p>The fused feature map F is obtained through the attention weights on multi-scale features:</p><formula xml:id="formula_4">F [i] = a[i] × X h [i] + b[i] × Up(X l )[i], i ∈ C 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Loss Function</head><p>The SR module and the CF module exploit different loss functions for training.</p><p>In the SR module, L1 Loss is used for super-resolution. In the CF module, we introduce HR images to CF module in the training stage for improving the performance of CF module and reducing the error caused by the reconstructed SR images. We use F ocal Loss <ref type="bibr" target="#b15">[16]</ref> to alleviate the class imbalanced data problem of the HR and SR images' classification. Inspired by the contrastive learning algorithm SimCLR <ref type="bibr" target="#b4">[5]</ref>, the HR and SR of the same images are similar to two different views. So the NT -Xent loss <ref type="bibr" target="#b18">[19]</ref> is adopted to calculate similarity between SR multi-scale features and HR multi-scale features for CF module's robustness. The total loss function can be expressed as:</p><formula xml:id="formula_5">L total = λ 1 L L1 + λ 2 L F L + λ 3 L NT -Xent , λ 1 + λ 2 + λ 3 = 1</formula><p>where λ 1 , λ 2 and λ 3 are the weights of L1 Loss, F ocal Loss and NT -Xent Loss, respectively. In the inference stage, only SR images are taken as inputs by CF module. In our experiment, λ 1 , λ 2 and λ 3 are set to 0.6, 0.3 and 0.1, respectively. And the temperature parameter in NT -Xent Loss is set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>Dataset: This work uses the breast cancer histopathological image database (BreaKHis)<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b19">[20]</ref>. The images in the dataset have four magnification factors The model is trained using the ADAM optimizer <ref type="bibr" target="#b24">[25]</ref> with the learning rate set to 1x10 -3 . The learning rate is multiplied by 0.9 for every two epochs. We use SKNet-26 <ref type="bibr" target="#b12">[13]</ref> as the backbone network in the CF module. The total training epochs are 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Results of Super-Resolution and Classification</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the results of the super-resolution phase. We adopt Peak Signal to Noise Ratio (PSNR) and structural similarity index (SSIM) <ref type="bibr" target="#b5">[6]</ref> to evaluate the performance of the SR model. MRC-Net and our proposed SRMFENet (SR module) achieves better metrics than the other algorithms. This proves the effectiveness of multi-scale features extraction. Compared with MRC-Net, our MFEblocks can extract and fuse multi-scale features well. And the joint training of SRMFENet and CF module improves the performance of super-resolution. Figure <ref type="figure">2</ref> demonstrates that our model can recover more details with less blurring. We compare our introduced CF module with five state-of-the-art breast cancer histopathological image models and Diagnosis Network with MRC-Net <ref type="bibr" target="#b5">[6]</ref>, as shown in Table <ref type="table" target="#tab_2">2</ref>. The results illustrate that the CF module reaches the best   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The structure of the SHISRCNet.</figDesc><graphic coords="3,62,46,54,50,327,64,230,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,72,48,53,90,307,36,194,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>For all experiments, we conduct 5-fold cross validation, and report the mean. We use LR histopathological images with size 48 × 48, 96 × 96, 192 × 192 as input for different single image SR tasks (x8, x4, x2) and set batch size to 8. For the corresponding LR and HR images in the training dataset, the same data augmentation is adopted, such as rotation, color jitter.</figDesc><table><row><cell>L. Xie et al.</cell></row><row><cell>(40x, 100x, 200x, 400x) and eight breast cancer classes. This dataset includes</cell></row><row><cell>four distinct histological types of benign breast tumors: adenosis (A), fibroade-</cell></row><row><cell>noma (F), phyllodes tumor (PT), and tubular adenona (TA); and four malignant</cell></row><row><cell>tumors (breast cancer): carcinoma (DC), lobular carcinoma (LC), mucinous car-</cell></row><row><cell>cinoma (MC) and papillary carcinoma (PC). The original dataset is randomly</cell></row><row><cell>divided into training set and testing set for each magnification at a ratio of 7:3</cell></row><row><cell>following previous work.</cell></row><row><cell>Implementation Details:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average PSNR/SSIM for x8, x4, x2 SR.</figDesc><table><row><cell>Methods</cell><cell>x8</cell><cell></cell><cell>x4</cell><cell></cell><cell>x2</cell></row><row><cell></cell><cell cols="2">PSNR(db) SSIM</cell><cell cols="2">PSNR(db) SSIM</cell><cell>PSNR(db) SSIM</cell></row><row><cell>Bicubic</cell><cell>20.75</cell><cell cols="2">0.4394 23.21</cell><cell cols="2">0.6305 26.60</cell><cell>0.9151</cell></row><row><cell>SRCNN</cell><cell>21.12</cell><cell cols="2">0.4872 24.04</cell><cell cols="2">0.6634 28.36</cell><cell>0.8631</cell></row><row><cell>WA-SRGAN</cell><cell>21.76</cell><cell cols="2">0.5141 26.20</cell><cell cols="2">0.7930 30.93</cell><cell>0.9351</cell></row><row><cell>EDSR</cell><cell>22.13</cell><cell cols="2">0.6063 26.22</cell><cell cols="2">0.8005 30.79</cell><cell>0.9325</cell></row><row><cell>MRC-Net</cell><cell>22.72</cell><cell cols="2">0.6213 26.86</cell><cell cols="2">0.8222 31.73</cell><cell>0.9433</cell></row><row><cell cols="2">SRMFENet (ours) 23.44</cell><cell cols="2">0.6325 27.21</cell><cell cols="2">0.8370 33.96</cell><cell>0.9566</cell></row><row><cell cols="2">SHISRCNet (ours) 24.21</cell><cell cols="2">0.6814 27.97</cell><cell cols="2">0.8413 35.61</cell><cell>0.9680</cell></row></table><note><p>Fig. 2. Qualitative Comparison with SR methods on breast cancer histopathological images x8 and x4.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Compare results with state-of-the-art on image level (* means that inputs are HR images, # means that inputs are down sample to half resolution from HR images.). And it gets results close to the CF module at all magnification factors. Meanwhile, compared with the Diagnosis Network that also uses LR images as input, SHISCNet has remarked performance advantages. Table3compares our results with the CF module using different resolution images. The performance of the CF module decreases significantly with the reduction of resolution. In contrast, SHISRCNet greatly improves the CF module performance of different scale low-resolution images.</figDesc><table><row><cell>Methods</cell><cell>Years</cell><cell></cell><cell cols="2">ACC(%)</cell></row><row><cell></cell><cell></cell><cell>40x</cell><cell cols="3">100x 200x 400x</cell></row><row><cell>AlexNet variant* [21]</cell><cell cols="2">2016 85.6</cell><cell>83.5</cell><cell>82.7</cell><cell>80.7</cell></row><row><cell>Inception V3* [2]</cell><cell cols="2">2018 90.2</cell><cell>85.6</cell><cell>86.1</cell><cell>82.5</cell></row><row><cell>DSoPN* [12]</cell><cell cols="2">2020 96</cell><cell cols="3">96.16 98.01 95.97</cell></row><row><cell>FE-BkCapsNet* [22]</cell><cell cols="5">2021 92.71 94.52 94.03 93.54</cell></row><row><cell>SSCA* [24]</cell><cell cols="5">2022 96.93 97.32 95.31 96.24</cell></row><row><cell>CF module only* (ours)</cell><cell>-</cell><cell cols="4">97.82 97.78 98.28 98.15</cell></row><row><cell cols="6">Diagnosis Network with MRC-Net # [6] 2021 94.43 94.45 94.73 93.92</cell></row><row><cell>SHISRCNet (ours) #</cell><cell>-</cell><cell cols="4">97.49 96.19 97.60 97.04</cell></row><row><cell cols="6">performance in four different magnification factors. This indicates the effective-</cell></row><row><cell cols="6">ness of our proposed combination of two multi-scale feature extraction methods.</cell></row><row><cell cols="6">SHISRCNet, which uses down sample to half resolution (x2) from HR images,</cell></row><row><cell cols="3">outperforms the SSCA at 40x, 200x and 400x.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of accuracy under different scales on the image level.To verify the effectiveness of the proposed components in SHISRCNet, a comparison between SHISRCNet and its five components on x2 images is given in Table3. (1) w/o MSF repalces MSF with concatenate operation and 1×1 convolution. (2) w/o FPN + CSFblock means that only SKNet is used for feature extraction in the CF module. (3) w/o CSFblock, w/o HR images and w/o NT-Xent loss remove the corresponding operation, respectively. As shown in table3, firstly, the performance of super-resolution in the SHISRCNet is significantly reduced when we remove MSF. It indicates the importance of MSF for multiscale feature fusion in the SR module. Secondly, only SKNet is used to extract multi-scale features in the CF module, and the accuracy decreased significantly. This again proves the effectiveness of our proposed combination of two multiscale feature extraction methods. Thirdly, compared with using FPN alone, the performance of SHISRCNet is further improved by adding CSFblock to FPN. Finally, the introduction of HR images further promotes the performance of SHISRCNet. Because the training method of HR and SR images proposed by us helps to improve the generalization of the SHISRCNet (Table4).</figDesc><table><row><cell cols="2">resolution Model</cell><cell>ACC(%)</cell></row><row><cell></cell><cell></cell><cell>40x</cell><cell>100x 200x 400x</cell></row><row><cell>HR</cell><cell cols="2">CF module only 97.82 97.78 98.28 98.15</cell></row><row><cell>x2 LR</cell><cell cols="2">CF module only 94.47 89.92 92.64 91.30</cell></row><row><cell></cell><cell>SHISRCNet</cell><cell>97.49 96.19 97.60 97.04</cell></row><row><cell>x4 LR</cell><cell cols="2">CF module only 90.32 87.71 88.61 86.89</cell></row><row><cell></cell><cell>SHISRCNet</cell><cell>94.15 94.22 95.14 95.26</cell></row><row><cell>x8 LR</cell><cell cols="2">CF module only 84.11 82.32 84.62 82.23</cell></row><row><cell></cell><cell>SHISRCNet</cell><cell>91.47 92.43 92.98 92.78</cell></row><row><cell cols="3">4.2 Ablation Study of the SHISRCNet</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of SHISRCNet on x2 images.This paper proposes SHISRCNet for the low-resolution breast cancer histopathological images' super-resolution and classification problem. The SR module employs MFEblock to extract and fuse multi-scale features for reconstructing low-resolution histopathological images into high-resolution ones. The CF module adopts two different multi-scale features extraction methods to capture features for the breast cancer diagnosis. We introduce high-resolution images into the CF module in the training stage to improve SHISRCNet's robustness. Through the joint training of the two modules, the super-resolution and classification of the low-resolution histopathological images are integrated in one model. Our method's results are close to the SOTA methods, which require using highresolution breast cancer histopathological images instead of low-resolution ones.</figDesc><table><row><cell></cell><cell cols="2">PSNR(db) SSIM</cell><cell>ACC(%)</cell></row><row><cell></cell><cell></cell><cell>40x</cell><cell>100x 200x 400x</cell></row><row><cell>SHISRCNet</cell><cell>35.61</cell><cell cols="2">0.9680 97.49 96.19 97.60 97.04</cell></row><row><cell>w/o MSF</cell><cell>33.13</cell><cell cols="2">0.9554 96.02 95.73 95.98 95.78</cell></row><row><cell cols="2">w/o FPN+CSFblock 34.41</cell><cell cols="2">0.9609 93.98 92.11 91.35 92.15</cell></row><row><cell>w/o CSFblock</cell><cell>34.57</cell><cell cols="2">0.9619 94.37 93.21 93.99 94.71</cell></row><row><cell>w/o HR images</cell><cell>34.43</cell><cell cols="2">0.9611 93.13 94.28 93.86 94.17</cell></row><row><cell>w/o NT-Xent loss</cell><cell>34.54</cell><cell cols="2">0.9623 95.53 95.20 95.01 95.36</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-databasebreakhis/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant No.<rs type="grantNumber">2022YFB2703301</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_njHTkqn">
					<idno type="grant-number">2022YFB2703301</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Support vector machines combined with feature selection for breast cancer diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Akay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3240" to="3247" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A first study exploring the performance of the state-of-the art CNN model in the problem of breast cancer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benhammou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Achchab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications</title>
		<meeting>the International Conference on Learning and Optimization Algorithms: Theory and Applications</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2018: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Soerjomataram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jemal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA: Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="394" to="424" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Super-resolution enhanced medical image diagnosis with sample affinity interaction</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1377" to="1389" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10593-2_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10593-2_13" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014: 13th European Conference</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Zurich, Switzerland; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mudern: multi-category classification of breast histopathological image using deep residual networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gandomkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mello-Thoms</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="14" to="24" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Breast cancer histopathological image classification based on deep second-order pooling network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Breast cancer histopathology image super-resolution using wideattention GAN with improved Wasserstein gradient penalty and perceptual loss</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shahidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="32795" to="32809" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A dataset for breast cancer histopathological image classification</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Spanhol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heutte</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBME.2015.2496264</idno>
		<ptr target="https://doi.org/10.1109/TBME.2015.2496264" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1455" to="1462" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Breast cancer histopathological image classification using convolutional neural networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Spanhol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heutte</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2016.7727519</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2016.7727519" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2560" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic classification of breast cancer histopathological images based on deep feature fusion and enhanced routing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Signal Process. Control</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">102341</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selective scale cascade attention network for breast cancer histopathology image classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1396" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved adam optimizer for deep neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 26th International Symposium on Quality of Service (IWQoS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
