<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection</title>
				<funder ref="#_HxUkMMr">
					<orgName type="full">Key Area R&amp;D Program of Guangdong Province</orgName>
				</funder>
				<funder>
					<orgName type="full">Guangdong Provincial Key Laboratory of Big Data Computing</orgName>
				</funder>
				<funder ref="#_mDypN5U #_EPTxZp9">
					<orgName type="full">Shenzhen Outstanding Talents Training Fund</orgName>
				</funder>
				<funder ref="#_ADPFeFx">
					<orgName type="full">Guangdong Provincial Key Laboratory of Future Networks of Intelligence</orgName>
				</funder>
				<funder>
					<orgName type="full">Tencent Open Fund</orgName>
				</funder>
				<funder ref="#_RKRuT9S #_9DfYZAT">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_jSUUgpk">
					<orgName type="full">Shenzhen-Hong Kong</orgName>
				</funder>
				<funder ref="#_JzG2G39">
					<orgName type="full">Shenzhen General</orgName>
				</funder>
				<funder ref="#_Q79z9WG">
					<orgName type="full">Shenzhen Key Laboratory of Big Data and Artificial Intelligence</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuncheng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen Research Insititute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zixun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen Research Insititute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">SDS</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SSE</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen Research Insititute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="44" to="54"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">579E929ED4F4E76CA3389B0DA03355E5</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Polyp Detection</term>
					<term>Colonoscopy</term>
					<term>Feature Alignment</term>
					<term>Contrastive Learning Y. Jiang and Z. Zhang-Equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate polyp detection is essential for assisting clinical rectal cancer diagnoses. Colonoscopy videos contain richer information than still images, making them a valuable resource for deep learning methods. However, unlike common fixed-camera video, the camera-moving scene in colonoscopy videos can cause rapid video jitters, leading to unstable training for existing video detection models. In this paper, we propose the YONA (You Only Need one Adjacent Reference-frame) method, an efficient end-to-end training framework for video polyp detection. YONA fully exploits the information of one previous adjacent frame and conducts polyp detection on the current frame without multi-frame collaborations. Specifically, for the foreground, YONA adaptively aligns the current frame's channel activation patterns with its adjacent reference frames according to their foreground similarity. For the background, YONA conducts background dynamic alignment guided by inter-frame difference to eliminate the invalid features produced by drastic spatial jitters. Moreover, YONA applies cross-frame contrastive learning during training, leveraging the ground truth bounding box to improve the model's perception of polyp and background. Quantitative and qualitative experiments on three public challenging benchmarks demonstrate that our proposed YONA outperforms previous state-of-the-art competitors by a large margin in both accuracy and speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colonoscopy plays a crucial role in identifying and removing early polyps and reducing mortality rates associated with rectal cancer. Over the past few years, the research community has devoted great effort to understanding colonoscopy videos using either optical flow <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> or temporal information aggregation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> between multiple frames.</p><p>However, those works are mainly designed based on the experience of previous natural video object detection studies, ignoring the inherent uniqueness of the colonoscopy motion patterns. Thus, we rethink the video polyp detection task and conclude three core challenges in colonoscopy videos. 1) Fast motion speed. In Fig. <ref type="figure" target="#fig_0">1</ref>(a), we show the target motion speed <ref type="bibr" target="#b25">[26]</ref> <ref type="foot" target="#foot_0">1</ref> on ImageNetVID <ref type="bibr" target="#b13">[14]</ref> (natural) and LDPolypVideo <ref type="bibr" target="#b8">[9]</ref> (colonoscopy) dataset. The motion speed in ImageNetVID evenly distributes in three intervals. In contrast, most targets in LDPolypVideo fall in the fast speed zone, leading to a large variance in the adjacent foreground features, like motion blur or occlusion, as shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>). Thus we conjecture that collaborating too many frames for polyp video detection will increase the misalignment between adjacent frames and leads to poor detection performance. Figure <ref type="figure" target="#fig_0">1(b)</ref> shows the performance of FGFA <ref type="bibr" target="#b25">[26]</ref> on two datasets with increasing reference frames. The different trends of the two lines confirm our hypothesis. 2) Complex background. Different from the common camera-fixed videos, the camera-moving of colonoscopy video will introduce large disturbances between adjacent frames (e.g., specular reflection, bubbles, water, etc.), as shown in Fig. <ref type="figure" target="#fig_0">1(d)</ref>. Those abnormalities disrupt the integrity of background structures and thus affect the effect of multi-frame fusion. 3) Concealed polyps. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(e), we noticed that some polyps could be seen as concealed objects in the colonoscopy video since such polyps have a very similar appearance to the intestine wall. The model will be confused by such frames in inference and result in high false-positive or false-negative predictions.</p><p>To address the above issues, we propose the YONA framework, which fully exploits the reference frame information and only needs one adjacent reference frame for accurate video polyp detection. Specifically, we propose the Foreground Temporal Alignment (FTA) module to explicitly align the foreground channel activation patterns between adjacent features according to their foreground similarity. In addition, we design the Background Dynamic Alignment (BDA) module after FTA that further learns the inter-frame background spatial dynamics to better eliminate the influence of motion speed and increase the training robustness. Finally, parallel to FTA and BDA, we introduce the Cross-frame Boxassisted Contrastive Learning (CBCL) that fully utilizes the box annotations to enlarge polyp and background discrimination in embedding space.</p><p>In summary, our contributions are in three-folds: (1) To the best of our knowledge, we are the first to investigate the obstacles to the development of existing video polyp detectors and conclude that two-frame collaboration is enough for video polyp detection. <ref type="bibr" target="#b1">(2)</ref> We propose the YONA, a novel framework for video polyp detection. It composes the foreground and background alignment modules to align the features under the fast-moving condition. It further introduces the cross-frame contrastive learning module to enhance the model's discrimination ability of polyps and intestine walls. <ref type="bibr" target="#b2">(3)</ref> Extensive experiments demonstrate that our YONA achieves new state-of-the-art performance on three large-scale public video polyp detection datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The whole pipeline is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We leverage the CenterNet <ref type="bibr" target="#b24">[25]</ref> as the base detector. Given a clip of a colonoscopy video, we take the current frame as anchor I a and its adjacent previous frame as reference I r . The binary maps M a , M r are generated using the bounding box of anchor and reference, where the foreground pixels are assigned with 1 while the background with 0. At each step, YONA first extracts multi-scale features from I a , I r using the backbone. Then, multi-scale features are fused and up-sampled to the resolution of the first stage as the intermediate features F a , F r . Then, we conduct foreground temporal alignment (Fig. <ref type="figure" target="#fig_1">2(a)</ref>) on intermediate features to align their channel activation pattern. Next, the enhanced anchor feature F is further refined by the background dynamic alignment module (Fig. <ref type="figure" target="#fig_1">2(b)</ref>) to mitigate the rapid dynamic changes in the spatial field. The BDA's output F * is used to compute the detection loss. Meanwhile, the intermediate features and binary maps are used to calculate the contrastive loss during training to improve the model's perception of polyp and background (Fig. <ref type="figure" target="#fig_1">2(c)</ref>).</p><p>Overall, the whole network is optimized with the combination loss function in an end-to-end manner. The final loss is composed of the same detection loss with CenterNet and our proposed contrastive loss, formulated as L = L detection + λ contrast L contrast . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Foreground Temporal Alignment</head><p>Since the camera moves at a high speed, the changes in the frame are very drastic for both foreground and background targets. As a result, multi-frame (reference&gt;3) fusion may easily incorporate more noise features into the aggregation features. On the other hand, the occluded or distorted foreground context may also influence the quality of aggregation. Thus we propose to conduct temporal alignment between adjacent features by leveraging the foreground context of only one adjacent reference frame. It is designed to align the certain channel's activation pattern of anchor feature to its preceding reference feature. Specifically, given the intermediate features F a , F r and reference binary map M r , we first pooling F r to 1D channel pattern f r by the binary map on the spatial dimension (R N ×C×H×W → R N ×C×1 ) and normalize it to [0, 1]:</p><formula xml:id="formula_0">f r = norm [Pooling(F r )] Pooling(F r ) = sum HW [F r (x, y)]/sum[M r (x, y)] if M r (x, y) = 1<label>(1)</label></formula><p>Then, the foreground temporal alignment is implemented by channel attention mechanism, where the attention maps are computed by weighted dot-product.</p><p>We obtain the enhanced anchor feature by adding the attention maps with the original anchor feature through skip connection to keep the gradient flow.</p><formula xml:id="formula_1">F = [αf r F a (x, y)] ⊕ F a if M r (x, y) = 1 (2)</formula><p>where α is the adaptive weight by similarity measuring. At the training stage, the ground truth boxes of the reference frame are used to generate the binary map M r . During the inference stage, we conduct FTA only if the validated bounding box of the reference frame exists, where "validated" denotes the confidence scores of detected boxes are greater than 0.6. Otherwise, we will skip this process and feed the original inputs to the next module.</p><p>Adaptive Re-weighting by Similarity Measuring. As discussed above, due to video jitters, adjacent frames may change rapidly at the temporal level, and directly fusing the reference feature will introduce noisy information and misguide the training. Thus we designed an adaptive re-weighting method by measuring the feature similarity, where the weight indicates the importance of the reference feature to the anchor feature. Specifically, if the foreground feature of the reference is close to the anchor, it is assigned a larger weight at all channels. Otherwise, a smaller weight is assigned. For efficiency, we use the cosine similarity metric <ref type="bibr" target="#b7">[8]</ref> to measure the similarity, where f a is the 1D channel pattern of F a computed with Eq. 1:</p><formula xml:id="formula_2">α = exp f r • f a |f r ||f a | (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Background Dynamic Alignment</head><p>The traditional convolutional-based object detector can detect objects well when the background is stable. However, once it receives obvious interference, such as light or shadow, the background changes may cause the degradation of spatial correlation and lead to many false-positive predictions. Motivated by the inter-frame difference method <ref type="bibr" target="#b19">[20]</ref>, we first mine the dynamic field of adjacent background contents, then consult to deformable convolution <ref type="bibr" target="#b2">[3]</ref> to learn the inherent geometric transformations according to the intensity of the dynamic field. In practice, given the enhanced anchor feature F from FTA and reference feature F r , the inter-frame difference is defined as the element-wise subtraction of enhanced anchor and reference feature. Then a 1 × 1 convolution is applied on the difference to generate dynamic field D, which encodes all spatial dynamic changes between adjacent frames.</p><formula xml:id="formula_3">D = Conv 1×1 ( F -F r )<label>(4)</label></formula><p>Finally, a 3 × 3 deformable convolution embeds the spatial dynamic changes of D on the enhanced anchor feature F .</p><formula xml:id="formula_4">F * = DeConv 3×3 ( F, D)<label>(5)</label></formula><p>where D works as the deformable offset and F * is the final aligned anchor feature.</p><p>Then the enhanced anchor feature is fed into three detection heads composed of a 3 × 3 Conv and a 1 × 1 Conv to produce center, size, and offset features for detection loss:</p><formula xml:id="formula_5">L detection = L center focal + λ size L size L1 + λ of f L offset L1<label>(6)</label></formula><p>where L focal is focal loss and L L1 is L1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-Frame Box-Assisted Contrastive Learning</head><p>Typically, in colonoscopy videos, some concealed polyps appear very similar to the intestine wall in color and texture. Thus, an advanced training strategy is required to distinguish such homogeneity. Inspired by recent studies on supervised contrastive learning <ref type="bibr" target="#b17">[18]</ref>, we select the foreground and background region on both two frames guided by ground truth boxes to conduct contrastive learning. In practice, Given a batch of intermediate feature maps F a , F r ∈ R N ×T ×C×H×W and corresponding binary maps M a , M r ∈ R N ×T ×H×W , we first concatenate the anchor and reference at the batch-wise level as F ∈ R NT ×C×H×W and M ∈ R NT ×H×W to exploit the cross-frame information. Then we extract the foreground and background channel patterns of cross-frame feature F using the Eq. 1 base on M (x, y) = 1 and M (x, y) = 0, respectively. After that, for each foreground channel pattern, which is the "query", we randomly select another different foreground feature as the "positive", while all the background features in the same batch are taken as the "negatives". Finally, we calculate the one-step contrastive loss by InfoNCE <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_6">L NCE j = -log exp(q j •i + /τ ) exp(q j •i + /τ )+ i -∈Nj exp(q j •i -/τ )<label>(7)</label></formula><p>where q j ∈ R C , j = 0, ..., N T is the query feature, i + ∈ R C and i -∈ R NT ×C are positives and negatives. N j denote embedding collections of the negatives. We repeat this process until every foreground channel pattern is selected and sum all steps as the final contrastive loss:</p><formula xml:id="formula_7">L contrast = 1 NT NT j=1 L NCE j (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>3 Experiments</p><p>We evaluate the proposed method on three public video polyp detection benchmarks: SUN Colonoscopy Video Database <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> (train set: 19,544 frames, test set: 12,522 frames), LDPolypVideo <ref type="bibr" target="#b8">[9]</ref> (train set: 20,942 frames, test set: 12,933 frames), and CVC-VideoClinicDB <ref type="bibr" target="#b0">[1]</ref> (train set: 7995 frames, test set: 2030 frames). For the fairness of the experiments, we keep the same dataset settings for YONA and all other methods. We use ResNet-50 <ref type="bibr" target="#b5">[6]</ref> as our backbone and CenterNet <ref type="bibr" target="#b24">[25]</ref> as our base detector. Following the same setting in CenterNet, we set λ size = 0.1 and λ of f = 1. We set λ contrast = 0.3 by ablation study. Detailed results are listed in the supplement. We randomly crop and resize the images to 512 × 512 and normalize them using ImageNet settings. Random rotation and flip with probability p = 0.5 are used for data augmentation. We set the batch size N = 32. Our model is trained using the Adam optimizer with a weight decay of 5 × 10 -4 for 64 epochs. The initial learning rate is set to 10 -4 and gradually decays to 10 -5 with cosine annealing. All models are trained with PyTorch <ref type="bibr" target="#b10">[11]</ref> framework. The training setting of other competitors follows the best settings given in their paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantitative and Qualitative Comparison</head><p>Quantitative Comparison. The comparison results are shown in Table <ref type="table" target="#tab_0">1</ref>. Following the standard of <ref type="bibr" target="#b0">[1]</ref>, the Precision, Recall, and F1-scores are used for evaluation. Firstly, compared with the CenterNet baseline, our YONA with three novel designs significantly improved the F1 score by 9.2%, 8.3%, and 7.4% on three benchmarks, demonstrating the effectiveness of the model design. Besides, YONA achieves the best trade-off between accuracy and speed compared with all other image-based SOTAs across all datasets. Second, for video-based competitors, previous video object detectors with multiple frame collaborations lack the ability for accurate detection on challenging datasets. Specifically, YONA surpasses the second-best STFT <ref type="bibr" target="#b18">[19]</ref> by 2.2%, 3.0%, and 1.3% on F1 score on three datasets and 33.8 on FPS. All the results confirm the superiority of our proposed framework for accurate and fast video polyp detection.</p><p>Qualitative Comparison. Figure <ref type="figure" target="#fig_2">3</ref> visualizes the qualitative results of YONA with other competitors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>. Thanks to this one-adjacent-frame framework, our YONA can not only prevent the false positive caused by part occlusion (1st and 2nd clips) but also capture useful information under severe image quality (2nd clip). Moreover, our YONA shows robust performance even for challenging scenarios like concealed polyps (3rd clip).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>We investigated the effectiveness of each component in YONA on the SUN database, as shown in Table <ref type="table">2</ref>. It can be observed that all the modules are necessary for precise detection compared with the baseline results. Due to the large variance of colonoscopy image content, the F1 score slightly decreases if directly adding FTA without the adaptive re-weighting strategy. Adding the adaptive weight greatly improves the F1 score by 5.4. Moreover, we use other two mainstream channel attention mechanisms to replace our proposed FTA for comparison. Compared with them, our FTA with adaptive weighting achieves the largest gain over the baseline and higher FPS. Overall, by combining all the proposed methods, our model can achieve new state-of-the-art performance. Table <ref type="table">2</ref>. Ablation studies of YONA under different settings. Ada means the adaptive re-weighting by similarity measuring; CW denotes the channel-wise attention <ref type="bibr" target="#b3">[4]</ref>; CA denotes the channel-aware attention <ref type="bibr" target="#b18">[19]</ref>.</p><p>FTA CW <ref type="bibr" target="#b3">[4]</ref> CA <ref type="bibr" target="#b18">[19]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Video polyp detection is a currently challenging task due to the fast-moving property of colonoscopy video. In this paper, We proposed the YONA framework that requires only one adjacent reference frame for accurate and fast video polyp detection. To address the problem of fast-moving polyps, we introduced the foreground temporal alignment module, which explicitly aligns the channel patterns of two frames according to their foreground similarity. For the complex background content, we designed the background dynamic alignment module to mitigate the large variances by exploiting the inter-frame difference. Meanwhile, we employed a cross-frame box-assisted contrastive learning module to enhance the polyp and background discrimination based on box annotations. Extensive experiment results confirmed the effectiveness of our method, demonstrating the potential for practical use in real clinical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) The histogram of the motion IoUs distribution on two datasets. Lower motion IoU denotes a faster target moving speed. The proportion of slow, medium and fast-moving targets is listed at the top of the figure. (b) The performance of FGFA [26] using multiple reference frames increases on ImageNetVID while decreasing on LDPolypVideo. (c) The typical challenges in colonoscopy videos. Yellow arrows point to the polyp, and red arrows point to distraction that causes false detection. (Color figure online)</figDesc><graphic coords="2,71,97,351,86,308,20,139,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our proposed video polyp detection framework, YONA. It first aligns the foreground channel patterns between the anchor and reference frame in (a). Then it extracts polyp context guided by dynamic field in (b). Meanwhile, YONA enhances the discrimination ability via contrastive learning in (c) during training. The final output of (b) is used to predict the bounding box of the current frame.</figDesc><graphic coords="4,70,98,123,38,310,60,153,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative results of polyp detection on some video clips. The yellow, green, and red denote the ground truth, true positive, and false positive, respectively (Color figure online)</figDesc><graphic coords="8,89,16,335,90,299,44,128,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison with other image/video-based detection models. P, R, and F1 denote the precision, recall, and F1-score. †: results from the original paper with the same data division. The best score is marked as red, while the second best score is marked as blue.</figDesc><table><row><cell>Methods</cell><cell cols="9">SUN Database LDPolypVideo CVC-VideoClinic FPS</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell cols="2">F1 P</cell><cell>R</cell><cell cols="2">F1 P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="10">Faster-RCNN [13] 77.2 69.6 73.2 68.8 46.7 55.6 84.6 98.2 90.9</cell><cell>44.7</cell></row><row><cell>FCOS [17]</cell><cell cols="9">75.7 64.1 69.4 65.1 46.0 53.9 92.1 74.1 82.1</cell><cell>42.0</cell></row><row><cell>CenterNet [25]</cell><cell cols="9">74.6 65.4 69.7 70.6 43.8 54.0 92.0 80.5 85.9</cell><cell>51.5</cell></row><row><cell cols="10">Sparse-RCNN [15] 75.5 73.7 74.6 71.6 47.9 57.4 85.1 96.4 90.4</cell><cell>40.0</cell></row><row><cell>DINO [21]</cell><cell cols="9">81.5 72.3 76.6 68.3 51.1 58.4 93.1 89.3 91.2</cell><cell>23.0</cell></row><row><cell>FGFA [26]</cell><cell cols="9">78.9 70.4 74.4 68.8 48.9 57.2 94.5 89.2 91.7</cell><cell>1.8</cell></row><row><cell>OptCNN † [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">84.6 97.3 90.5</cell><cell>-</cell></row><row><cell>AIDPT † [22]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">90.6 84.5 87.5</cell><cell>-</cell></row><row><cell>MEGA [2]</cell><cell cols="9">80.4 71.6 75.7 69.2 50.1 58.1 91.6 87.7 89.6</cell><cell>8.1</cell></row><row><cell>TransVOD [24]</cell><cell cols="9">79.3 69.6 74.1 69.2 49.2 57.5 92.1 91.4 91.7</cell><cell>8.4</cell></row><row><cell>STFT [19]</cell><cell cols="9">81.5 72.4 76.7 72.1 50.4 59.3 91.9 92.0 92.0</cell><cell>12.5</cell></row><row><cell>Ours-YONA</cell><cell cols="9">83.3 74.9 78.9 75.4 53.1 62.3 92.8 93.8 93.3</cell><cell>46.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>averaged intersection-over-union scores of target in the nearby frames (±10 frames).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by <rs type="funder">Shenzhen General</rs> Program No. <rs type="grantNumber">JCYJ20220530143600001</rs>, by the <rs type="programName">Basic Research Project No. HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S&amp;T Cooperation Zone</rs>, by <rs type="funder">Shenzhen-Hong Kong</rs> Joint Funding No. <rs type="grantNumber">SGDX20211123112401002</rs>, <rs type="funder">NSFC</rs> with Grant No. <rs type="grantNumber">62293482</rs>, by <rs type="funder">Shenzhen Outstanding Talents Training Fund</rs>, by Guangdong Research Project No. <rs type="grantNumber">2017ZT07X152</rs> and No. <rs type="grantNumber">2019CX01X104</rs>, by the <rs type="funder">Guangdong Provincial Key Laboratory of Future Networks of Intelligence</rs> (Grant No. <rs type="grantNumber">2022B1212010001</rs>), by the <rs type="funder">Guangdong Provincial Key Laboratory of Big Data Computing</rs>, <rs type="affiliation">The Chinese University of Hong Kong, Shenzhen</rs>, by the <rs type="funder">NSFC</rs> <rs type="grantNumber">61931024&amp;81922046</rs>, by the <rs type="funder">Shenzhen Key Laboratory of Big Data and Artificial Intelligence</rs> (Grant No. <rs type="grantNumber">ZDSYS201707251409055</rs>), and the <rs type="funder">Key Area R&amp;D Program of Guangdong Province</rs> with grant No. <rs type="grantNumber">2018B030338001</rs>, by zelixir biotechnology company Fund, by <rs type="funder">Tencent Open Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JzG2G39">
					<idno type="grant-number">JCYJ20220530143600001</idno>
					<orgName type="program" subtype="full">Basic Research Project No. HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S&amp;T Cooperation Zone</orgName>
				</org>
				<org type="funding" xml:id="_jSUUgpk">
					<idno type="grant-number">SGDX20211123112401002</idno>
				</org>
				<org type="funding" xml:id="_RKRuT9S">
					<idno type="grant-number">62293482</idno>
				</org>
				<org type="funding" xml:id="_mDypN5U">
					<idno type="grant-number">2017ZT07X152</idno>
				</org>
				<org type="funding" xml:id="_EPTxZp9">
					<idno type="grant-number">2019CX01X104</idno>
				</org>
				<org type="funding" xml:id="_ADPFeFx">
					<idno type="grant-number">2022B1212010001</idno>
				</org>
				<org type="funding" xml:id="_9DfYZAT">
					<idno type="grant-number">61931024&amp;81922046</idno>
				</org>
				<org type="funding" xml:id="_Q79z9WG">
					<idno type="grant-number">ZDSYS201707251409055</idno>
				</org>
				<org type="funding" xml:id="_HxUkMMr">
					<idno type="grant-number">2018B030338001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_5.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyp detection benchmark in colonoscopy videos using GTCreator: a novel fully configurable tool for easy and fast annotation of image databases</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 32nd CARS Conference</title>
		<meeting>32nd CARS Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory enhanced global-local aggregation for video object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10337" to="10346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Polyp detection on video colonoscopy using a hybrid 2d/3d CNN</title>
		<author>
			<persName><forename type="first">González-Bueno</forename><surname>Puyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102625</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<ptr target="https://amed8k.sundatabase.org/" />
		<title level="m">Sun colonoscopy video database</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cosine normalization: using cosine similarity instead of dot product in neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01418-6_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01418-6_38" />
	</analytic>
	<monogr>
		<title level="m">ICANN 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Kůrková</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Iliadis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Maglogiannis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11139</biblScope>
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LDPolypVideo benchmark: a largescale colonoscopy video dataset of diverse polyps</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_37" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Development of a computer-aided detection system for colonoscopy and a publicly accessible large colonoscopy video database (with video)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Misawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="960" to="967" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving automatic polyp detection using CNN by exploiting temporal dependency in colonoscopy video</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Solhusvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergsland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aabakken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="193" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis. (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse R-CNN: end-to-end object detection with learnable proposals</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14454" to="14463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic polyp detection in colonoscopy videos using an ensemble of convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="79" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring crossimage pixel contrast for semantic segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7303" to="7313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-frame collaboration for effective endoscopic video polyp detection via spatial-temporal feature transformation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_29" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An improved moving object detection algorithm based on frame difference and edge detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on Image and Graphics (ICIG 2007)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="519" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DINO: DETR with improved denoising anchor boxes for endto-end object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asynchronous in parallel detection and tracking (AIPDT): realtime robust polyp detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_69</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_69" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="722" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polyp tracking in video colonoscopy using optical flow with an on-the-fly trained CNN</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="79" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transvod: end-to-end video object detection with spatial-temporal transformers</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
