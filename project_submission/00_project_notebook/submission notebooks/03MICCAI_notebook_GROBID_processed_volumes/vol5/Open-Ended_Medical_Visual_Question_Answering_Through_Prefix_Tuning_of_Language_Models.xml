<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models</title>
				<funder>
					<orgName type="full">University of Amsterdam</orgName>
				</funder>
				<funder>
					<orgName type="full">Netherlands Ministry of Economic Affairs and Climate Policy</orgName>
				</funder>
				<funder>
					<orgName type="full">Inception Institute of Artificial Intelligence</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tom</forename><surname>Van Sonsbeek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Mahdi</forename><surname>Derakhshani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ivona</forename><surname>Najdenkoska</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cees</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Marcel</forename><surname>Worring</surname></persName>
							<email>m.worring@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="726" to="736"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E0F7EE67028F5543CBD24DBA526C6141</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Question Answering</term>
					<term>Language Models</term>
					<term>Prompting</term>
					<term>Prefix Tuning T. van Sonsbeek, M. M. Derakhshani and I. Najdenkoska-Equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical Visual Question Answering (VQA) is an important challenge, as it would lead to faster and more accurate diagnoses and treatment decisions. Most existing methods approach it as a multi-class classification problem, which restricts the outcome to a predefined closedset of curated answers. We focus on open-ended VQA and motivated by the recent advances in language models consider it as a generative task. Leveraging pre-trained language models, we introduce a novel method particularly suited for small, domain-specific, medical datasets. To properly communicate the medical images to the language model, we develop a network that maps the extracted visual features to a set of learnable tokens. Then, alongside the question, these learnable tokens directly prompt the language model. We explore recent parameter-efficient finetuning strategies for language models, which allow for resource-and data-efficient fine-tuning. We evaluate our approach on the prime medical VQA benchmarks, namely, Slake, OVQA and PathVQA. The results demonstrate that our approach outperforms existing methods across various training settings while also being computationally efficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Images and text are inherently intertwined in clinical diagnosis and treatment. Having an automated approach that is able to answer questions based on images, giving insight to clinicians and patients, can be a valuable asset. In such a medical Visual Question Answering (VQA) setting the common approach is to treat VQA as a multi-class classification problem solved by neural networks. Given a joint encoded representation of the image and question, the model classifies it into a predefined set of answers. Although these approaches yield good performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>, they deal with closed-set predictions, which is not an ideal solution for VQA. For instance, medical VQA datasets commonly contain hundreds to thousands of free-form answers <ref type="bibr" target="#b10">[11]</ref>, which is suboptimal to be treated as a classification task. Moreover, the severe class imbalance and out-of-vocabulary answers further hinder the generalizability of these classification methods.</p><p>We believe that a possible solution can be found in the generative capability of language models, since they are able to produce free text, instead of being limited to closed-set predictions. However, leveraging language models for solving open-ended medical VQA is limited due to several challenges, such as finding ways to properly communicate the visual features and letting such large-scale models be employed on small-sized medical VQA datasets.</p><p>Inspired by recent image captioning models <ref type="bibr" target="#b21">[22]</ref>, we propose to use the medical images by converting them into a set of learnable tokens through a small-scale mapping network. These tokens can then be interpreted as a visual prefix for the language model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23]</ref>. Afterward, the visual prefix is used together with the question as input to the language model, which generates the answer token by token <ref type="bibr" target="#b24">[25]</ref>.</p><p>Furthermore, large-scale language models can generalize across domains while keeping their weights frozen <ref type="bibr" target="#b29">[30]</ref>. This makes them very appealing for the medical domain, which inherently does not possess large quantities of labeled data required to train these models from scratch <ref type="bibr" target="#b28">[29]</ref>. Models like BioGPT <ref type="bibr" target="#b20">[21]</ref> and BioMedLM <ref type="bibr" target="#b30">[31]</ref> are based on the generic GPT2 language model <ref type="bibr" target="#b25">[26]</ref> and are trained on biomedical text corpora. They perform quite well compared to their general counterparts on specific biomedical language tasks, like question answering or relation extraction. We design our model in a flexible manner, which allows us to incorporate any of these pre-trained language models.</p><p>In summary, we contribute in three major aspects: (i) We propose the first large-scale language model-based method for open-ended medical VQA. (ii) We adopt parameter-efficient tuning strategies for the language backbone, which gives us the ability to fine-tune a large model with a small dataset without the danger of overfitting. (iii) We demonstrate through extensive experiments on relevant benchmarks that our model yields strong open-ended VQA performance without the need for extensive computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>To describe existing medical VQA methods, we make a distinction between classification methods and generative methods. The majority of methods are classification-based and make use of different types of encoders, such as CNNs or Transformers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref> followed by a classification layer.</p><p>Classification-Based VQA. We highlight a number of methods that showed good performance on current competitive medical VQA datasets. The Mixture Enhanced Visual Features (MEVF) <ref type="bibr" target="#b23">[24]</ref> is initialized based on pre-trained weights from the Model-Agnostic Meta-Learning (MAML) model <ref type="bibr" target="#b6">[7]</ref> in combination with image feature extraction from Conditional Denoising Auto-Encoders (CDAE) to generate a joint question-answer representation using Bilinear (BAN) or Stacked (SAN) Attention Networks. Do et al. <ref type="bibr" target="#b4">[5]</ref> create a similar embedding space by extracting annotations from multiple pre-trained meta-models, and learning meta-annotations by training each meta-model. Linear combinations <ref type="bibr" target="#b9">[10]</ref> or question-conditioned selections <ref type="bibr" target="#b33">[34]</ref> from this multi-modal embedding space can further enhance performance. The use of Transformer <ref type="bibr" target="#b13">[14]</ref> and CLIP <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref> encoders also results in strong VQA classification performance.</p><p>Open-Ended VQA. MedFuseNet <ref type="bibr" target="#b27">[28]</ref> is one of the few methods performing and reporting open-ended visual question answering on recent public datasets. They do so by creating a BERT-based multi-modal representation of image and question and subsequently passing it through an LSTM decoder. Ren et al. <ref type="bibr" target="#b26">[27]</ref> create open-ended answers by using the masked token prediction functionality of BERT. We aim to show that generative language models are more versatile and better suited for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Given an input image I and an input question in natural language Q, our method aims to sequentially generate an answer A = {A 0 , A 1 , ..., A N } composed of N tokens, by conditioning on both inputs. From a model definition perspective, we aim to find the optimal parameters θ * for a model by maximizing the conditional log-likelihood as follows:</p><formula xml:id="formula_0">θ * = arg max θ N i=1 log p θ (A i |Q, I, A i-1 ).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>Our VQA model is designed as an encoder-decoder architecture, with a twostream encoder and a language model (LM) as a decoder, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, the two streams encode the two input modalities, namely the image I and the question Q. The language model is defined as a causal language Transformer <ref type="bibr" target="#b25">[26]</ref>, and it generates the answer A in an autoregressive manner. It closely follows the prefix tuning technique for prompting a language model to produce an output of a particular style <ref type="bibr" target="#b15">[16]</ref>, such as in our case an answer given a question and an image<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision Encoding Stream.</head><p>For encoding the image, we employ a pre-trained vision encoder to extract visual features {x 1 , x 2 ...x x }. To use these features as input to the decoder, they should be mapped into the latent space of the language decoder. Following <ref type="bibr" target="#b21">[22]</ref>, we define a mapping network f M , implemented as a three-layer MLP. This network maps the visual features into a visual prefix {v 1 , v 2 , . . . v x } ∈ R x×e for the language model, where e is the embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Encoding Stream.</head><p>Regarding the encoding of the textual part, firstly we utilize a standard tokenization process to obtain a sequence of tokens, both for the question Q = {q 1 , q 2 ...q q } ∈ R q ×e and answer A = {a 1 , a 2 ...a a } ∈ R a ×e . This is followed by embedding the tokens using the embedding function of a pre-trained language model. Prompt Structure. To create a structured prompt, following existing QA methods using language models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>, we prepend the question, image, and answer tokens with tokenized descriptive strings, namely question:, context: and answer:. By placing the embeddings of the question before the visual tokens we mitigate the problem of fixation of the language model on the question <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. As an example this would yield the following prompt template: p =[question: What does the right side of the field show? context: v 1 , v 2 , . . . v x answer: ] which is fed as input to the language model. Language Model. Following standard language modeling systems, we treat VQA as a conditional generation of text, and we optimize the standard maximum likelihood objective during training. The language model receives the prompt sequence p as input and outputs the answer A, token by token. Specifically, at each time step i, the output of the model are the logits parametrizing a categorical distribution p θ (A) over the vocabulary tokens. This distribution is represented as follows:</p><formula xml:id="formula_1">log p θ (A) = la log p θ (a i |q 1 , ...q q , v 1 , ...v x , a 1 , ...a i-1 ). (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>The parameters of the language model are initialized from a pre-trained model, which has been previously pre-trained on huge web-collected datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter-Efficient Strategies for Fine-Tuning the Language Model</head><p>Standard fine-tuning of language models can hurt the generalization capabilities of the model, especially if small, domain-specific datasets are used as in our case. Therefore, we consider four different parameter-efficient strategies that adapt the attention blocks of language models, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> and outlined below:</p><p>Frozen Method: the parameters of the language model are kept entirely frozen during training, following <ref type="bibr" target="#b29">[30]</ref>. In this setting, only the mapping network is updated through backpropagation. Prompt Tuning: we prepend a set of m learnable tokens M ∈ R m×e to the input prompt sequence, which yields [M, p] <ref type="bibr" target="#b14">[15]</ref> as input to the frozen language model. Besides updating the mapping network, this approach also involves updating these learnable tokens through backpropagation. Prefix Tuning: we prepend a learnable prefix P j to the query Q j of each attention block j in the Transformer, such that Q ft j = [P j , Q j ] <ref type="bibr" target="#b15">[16]</ref>. Similar as in prompt tuning, we update both the mapping function and the learnable prefixes of the queries. Low-Rank Adaptation (LoRA): We add learnable weight matrices to the query Q and value V of the attention blocks in each layer of the frozen language model as W + ΔW following <ref type="bibr" target="#b11">[12]</ref>. Again, the mapping function is trained together with the learnable weight matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Datasets. The three datasets used for the evaluation of our method are Slake <ref type="bibr" target="#b19">[20]</ref>, PathVQA <ref type="bibr" target="#b10">[11]</ref>, and OVQA <ref type="bibr" target="#b12">[13]</ref>. These three datasets are the current most suitable VQA datasets given their large variety in answers and the manual curation of answers by domain experts. Each dataset is split 50/50 between 'yes/no' and open-set answers. See the datatset details in Table <ref type="table" target="#tab_0">1</ref>. We use the official train/validation/test splits across all three datasets. Evaluation Protocol. We evaluate our approach using the conventional metrics BLEU-1 and F1 Score. Additionally, we measure the contextual capturing of information with BERTScore <ref type="bibr" target="#b34">[35]</ref> this method can handle synonyms. Lastly to allow for comparison against existing classification-based methods we also report accuracy and F1 score. Implementation Details. We extract the visual features using a pre-trained CLIP model with ViT backbone <ref type="bibr" target="#b24">[25]</ref>, having a dimensionality of 512. The MLP layers of the mapping network f M have sizes {512, ( x • e)/2, x • e}. The length of x is set at 8. The lengths q and a are dataset dependent and defined by the mean number of tokens in the train set plus three times its standard deviation. Zero padding is added to the right side of the sequence for batch-wise learning. We use the following language models: GPT2-XL <ref type="bibr" target="#b25">[26]</ref>, a causal language model with 1.5B parameters trained on WebText <ref type="bibr" target="#b25">[26]</ref>. BioMedLM <ref type="bibr" target="#b30">[31]</ref> and BioGPT <ref type="bibr" target="#b20">[21]</ref> are both GPT2-based models, pre-trained on PubMed and biomedical data from The Pile <ref type="bibr" target="#b7">[8]</ref>, with a size of 1.5B and 2.7B parameters, respectively. All models are able to train on a single NVIDIA RTX 2080ti GPU (average training time ≈ 3 h). We use the AdamW optimizer with 600 warmup steps and a learning rate of 5e-3 and apply early stopping with a tolerance of 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Benefits of Parameter-Efficient Fine-Tuning. The evaluation of our method across various language models and fine-tuning settings in Table <ref type="table">2</ref> shows Table <ref type="table">2</ref>. Performance across different language models and fine-tuning strategies, measured in BLEU1 (BL1), BERTScore (BS), F1 and accuracy. Params% is the amount of trainable parameters in the language model. Our method using GPT2 in combination with LoRA yields the best performance across all datasets.  that language models can perform open-ended medical VQA. Specifically, we outperform the only existing method MedFuseNet <ref type="bibr" target="#b27">[28]</ref> that does open-ended VQA, due to the capability of pre-trained language models to capture long-term dependencies when generating free-form answers. Additionally, prefix <ref type="bibr" target="#b15">[16]</ref> and prompt tuning <ref type="bibr" target="#b14">[15]</ref> do not improve the performance of the model as much as using LoRA <ref type="bibr" target="#b11">[12]</ref> which directly adapts the Q and V weight matrices of the attention blocks. Moreover, larger datasets show the most consistent performance gain of parameter-efficient fine-tuning across all metrics.</p><p>Comparison Between Standard and Medical LMs. Using a language model pre-trained on a general text corpus, such as GPT2 <ref type="bibr" target="#b25">[26]</ref>, improves the overall performance compared to its medically-trained models (e.g. BioGPT or BioMedLM), as can be observed in Table <ref type="table">2</ref>. BioGPT and BioMedLM could be overoptimized to their medical text corpora, which leads to lack of generalization to different downstream domains. As mentioned in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>, these models require full fine-tuning on the respective downstream tasks, to achieve the desired performance. On the other hand, GPT2 benefits from observing diverse data during pre-training which also encompasses medically oriented text. This enables GPT2 models to generalize easily to other domains, which is relevant for our different VQA datasets.</p><p>Benefit of Open-Ended Answer Generation. Our method is performing significantly better on the open-set answering, in comparison to classificationbased methods, as shown in Table <ref type="table" target="#tab_2">3</ref>. We also confirm that CLIP based image embeddings perform well in the medical domain <ref type="bibr" target="#b5">[6]</ref> compared to the conventional use of CNNs. Since our approach is generative, it is not bounded by the class imbalance issue, which is considered a bottleneck of classification-based VQA Table <ref type="table">4</ref>. Effect of using different prompt structures. Note that Q and I denote the question and image respectively. The regular setting with the question embeddings followed by the visual prefix (Fig. <ref type="figure" target="#fig_0">1</ref>) leads to the best overall performance. models. Our method performs especially well compared to other method on PathVQA, which relatively has the largest class imbalance, accentuating this effect. Even on the simple 'yes/no' questions, the performance is better, showing that this simple yet effective method provides a more natural way of doing VQA. It worth noting that the comparison of accuracy as a metric for exact matches, between classification and generation methods is not in favor of generative methods. Despite that, we outperform existing methods on all datasets and metrics, which is a testament to the benefit of phrasing VQA as an open-ended generation problem.</p><p>In Fig. <ref type="figure">3(a-c</ref>), we show qualitative examples of capability of the language model to successfully predict the correct answer. However, in Fig. <ref type="figure">3 (d,</ref><ref type="figure">e</ref>) we show cases where our method predicts a factually correct answer which is not specific enough.</p><p>Effect of Using Different Prompt Structures. We also investigate the influence of the prompt structure on the overall performance, demonstrated in Table <ref type="table">4</ref>. It can be observed that the performance largely decreases when the question is removed, compared to when the visual information is removed. This suggests that the question plays a more important role in answer generation.</p><p>Interestingly, the model is sensitive the order of the elements in the prompt, as the swapping of the question embeddings and the visual prefix yields decreases the performance. The reason for this is that the language model conveys lower to no importance the visual information if it is located in front of the question. In this situation the language model basically generates blind answers. This highlights the importance of prompt structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a new perspective on medical VQA. We are using generative language models to generate answers in an open-ended manner, instead of performing a closed-set classification. Additionally, by using various parameterefficient fine-tuning strategies we are able to use language models with billions of parameters, even though dataset sizes in this domain are small. This leads to excellent performance compared to classification-based methods. In conclusion, our approach offers a more accurate and efficient solution for medical VQA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Model architecture of our proposed open-ended generative VQA method.</figDesc><graphic coords="4,56,46,53,87,340,00,109,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Parameter-efficient language model fine-tuning strategies used in our method.</figDesc><graphic coords="5,42,30,53,75,339,52,67,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>6 Fig. 3 .</head><label>63</label><figDesc>Fig. 3. Outputs of our open-ended VQA method, with GPT2 and LoRA fine-tuning, using data samples from PathVQA (a, b, d) and OVQA (c, e).</figDesc><graphic coords="8,56,46,194,09,339,31,116,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the medical VQA datasets used in this paper.</figDesc><table><row><cell></cell><cell cols="3">Slake OVQA PathVQA</cell></row><row><cell>Number of images</cell><cell>642</cell><cell cols="2">2,001 4,998</cell></row><row><cell>Number of questions</cell><cell cols="3">14,028 19,020 32,799</cell></row><row><cell cols="2">Mean length of questions 4.52</cell><cell>8.98</cell><cell>6.36</cell></row><row><cell>Mean length of answers</cell><cell>1.21</cell><cell>3.31</cell><cell>1.80</cell></row><row><cell cols="2">Number of unique answers 461</cell><cell>641</cell><cell>3,182</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.6 91.2 78.1 83.3 61.8 85.4 69.1 71.0 70.3 78.5 58.4 63.6</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="3">LM fine-tuning LM size Params%</cell><cell></cell><cell></cell><cell>Slake</cell><cell></cell><cell></cell><cell cols="2">OVQA</cell><cell></cell><cell></cell><cell>PathVQA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">BL1 BS F1 Acc. BL1 BS F1 Acc. BL1 BS F1 Acc.</cell></row><row><cell cols="2">MedFuseNet [28]</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.5</cell><cell>-</cell><cell>38.1</cell><cell>-</cell></row><row><cell></cell><cell>Frozen</cell><cell></cell><cell>0%</cell><cell cols="10">64.5 69.9 57.7 66.5 32.4 71.9 52.5 53.5 36.9 57.6 31.0 45.3</cell></row><row><cell>Ours w/ BioGPT</cell><cell>Prefix [16] Prompt [15]</cell><cell>1.5B</cell><cell cols="11">0.487% 58.1 74.1 54.1 67.4 37.9 65.0 46.1 53.2 53.6 61.8 34.8 46.7 0.001% 44.2 75.6 47.6 53.7 47.5 62.9 34.6 50.3 28.0 58.7 43.8 33.2</cell></row><row><cell></cell><cell>LoRA [12]</cell><cell></cell><cell cols="11">0.311% 59.2 72.2 63.1 71.9 41.0 68.5 57.7 57.3 57.8 62.9 40.4 47.9</cell></row><row><cell></cell><cell>Frozen</cell><cell></cell><cell>0%</cell><cell cols="10">70.2 77.8 47.8 66.0 55.2 72.9 54.2 61.1 61.2 66.1 52.4 53.0</cell></row><row><cell>Ours w/ BioMedLM</cell><cell>Prefix [16] Prompt [15]</cell><cell>2.7B</cell><cell cols="11">0.753% 64.3 79.4 60.9 63.3 49.1 76.9 51.5 60.1 59.7 60.7 48.9 52.3 0.009% 44.6 73.5 38.8 41.6 48.9 72.8 44.3 59.5 51.9 59.8 38.9 49.3</cell></row><row><cell></cell><cell>LoRA [12]</cell><cell></cell><cell cols="11">0.101% 72.3 80.6 62.4 71.7 59.0 76.2 62.6 67.8 67.9 76.0 54.4 57.2</cell></row><row><cell></cell><cell>Frozen</cell><cell></cell><cell>0%</cell><cell cols="10">65.1 83.3 57.7 71.2 60.2 79.8 59.4 66.1 64.2 74.6 47.5 58.1</cell></row><row><cell>Ours w/ GPT2</cell><cell>Prefix [16] Prompt [15]</cell><cell>1.5B</cell><cell cols="11">0.492% 70.0 86.5 66.3 74.1 61.2 83.9 65.5 68.9 67.5 76.2 52.5 60.5 0.003% 57.8 80.3 49.9 60.0 57.8 78.3 55.2 63.1 54.4 72.0 38.1 46.6</cell></row><row><cell></cell><cell>LoRA [12]</cell><cell></cell><cell cols="2">0.157% 78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of  the accuracy between open-ended VQA against classificationbased VQA methods, split between yes/no and open-set answers. Our method performs particularly well on both types of answers compared to the state-of-the-art methods.</figDesc><table><row><cell></cell><cell></cell><cell>Slake</cell><cell></cell><cell></cell><cell>OVQA</cell><cell></cell><cell cols="2">PathVQA</cell><cell></cell></row><row><cell></cell><cell>Open-set</cell><cell>Yes/no</cell><cell>All</cell><cell>Open-set</cell><cell>Yes/no</cell><cell>All</cell><cell>Open-set</cell><cell>Yes/no</cell><cell>All</cell></row><row><cell>MEVF-SAN [24]</cell><cell>75.3</cell><cell>78.4</cell><cell>76.5</cell><cell>36.9</cell><cell>72.8</cell><cell>58.5</cell><cell>6.0</cell><cell>81.0</cell><cell>43.6</cell></row><row><cell>MEVF-BAN [24]</cell><cell>77.8</cell><cell>79.8</cell><cell>78.6</cell><cell>36.3</cell><cell>76.3</cell><cell>60.4</cell><cell>8.1</cell><cell>81.4</cell><cell>44.8</cell></row><row><cell>MEVF-SAN+VQAMix [10]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>12.1</cell><cell>84.4</cell><cell>48.4</cell></row><row><cell>MEVF-BAN+VQAMix [10]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.4</cell><cell>83.5</cell><cell>48.6</cell></row><row><cell>MMQ-SAN [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.9</cell><cell>76.2</cell><cell>68.5</cell><cell>9.6</cell><cell>83.7</cell><cell>46.8</cell></row><row><cell>MMQ-BAN [5]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.2</cell><cell>76.2</cell><cell>65.0</cell><cell>11.8</cell><cell>82.1</cell><cell>47.1</cell></row><row><cell>QCR-BAN [34]</cell><cell>78.8</cell><cell>82.0</cell><cell>80.0</cell><cell>52.6</cell><cell>77.7</cell><cell>67.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CRPD-BAN [19]</cell><cell>81.2</cell><cell>84.4</cell><cell>82.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMBERT [14]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>37.9</cell><cell>80.2</cell><cell>63.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>QCR-CLIP [6]</cell><cell>78.4</cell><cell>82.5</cell><cell>80.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours w/ BioGPT (LoRA)</cell><cell>71.1</cell><cell>72.7</cell><cell>71.9</cell><cell>48.3</cell><cell>66.5</cell><cell>57.3</cell><cell>30.2</cell><cell>65.5</cell><cell>47.9</cell></row><row><cell>Ours w/ BioMedLM (LoRA)</cell><cell>72.1</cell><cell>71.4</cell><cell>71.7</cell><cell>55.3</cell><cell>80.3</cell><cell>67.8</cell><cell>34.1</cell><cell>80.4</cell><cell>57.2</cell></row><row><cell>Ours w/ GPT2 (LoRA)</cell><cell>84.3</cell><cell>82.1</cell><cell>83.3</cell><cell>62.6</cell><cell>84.7</cell><cell>71.0</cell><cell>40.0</cell><cell>87.0</cell><cell>63.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code available at: github.com/tjvsonsbeek/open-ended-medical-vqa.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is financially supported by the <rs type="funder">Inception Institute of Artificial Intelligence</rs>, the <rs type="funder">University of Amsterdam</rs> and the allowance <rs type="institution">Top consortia for Knowledge and Innovation (TKIs)</rs> from the <rs type="funder">Netherlands Ministry of Economic Affairs and Climate Policy</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of CLIP features for image captioning: an experimental analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barraco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cascianelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4662" to="4670" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Caption-aware medical VQA via semantic focusing and progressive cross-modality comprehension</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="3569" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Variational prompt tuning improves generalization of vision-language models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Derakhshani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02390</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiple meta-model quantifying for medical visual question answering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-37" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="64" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Does CLIP benefit visual question answering in the medical domain as much as it does in the general domain?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meinel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13906</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
		<respStmt>
			<orgName>ICLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: an 800 GB dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cross-modal self-attention with multitask pre-training for medical visual question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="456" to="460" />
		</imprint>
		<respStmt>
			<orgName>ICMR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vqamix: conditional triplet mixup for medical visual question answering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10286</idno>
		<title level="m">Pathvqa: 30000+ questions for medical visual question answering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">OVQA: A clinically generated visual question answering dataset</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2924" to="2938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">MMBERT: multimodal BERT pretraining for improved medical VQA</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">D</forename><surname>Priyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1033" to="1036" />
		</imprint>
		<respStmt>
			<orgName>ISBI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="3045" to="3059" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prefix-tuning: optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="4582" to="4597" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A bi-level representation learning model for medical visual question answering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Inf</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page">104183</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10056</idno>
		<title level="m">Medical visual question answering: a survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrastive pre-training and representation distillation for medical visual question answering based on radiology images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-320" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Slake: a semanticallylabeled knowledge-enhanced dataset for medical visual question answering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1650" to="1654" />
		</imprint>
		<respStmt>
			<orgName>ISBI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BioGPT: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings Bioinformat</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09734</idno>
		<title level="m">Clipcap: clip prefix for image captioning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta learning to bridge vision and language models for multimodal few-shot learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Najdenkoska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overcoming data limitation in medical visual question aswering</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-957" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cgmvqa: a new classification and generative model for medical visual question answering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="50626" to="50636" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MedFuseNet: an attention-based multimodal deep learning model for visual question answering in the medical domain</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19826</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nevado-Holgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kormilitzin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05535</idno>
		<title level="m">Clinical prompt learning with frozen language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">BioMedLM: a domain-specific large language model for biomedicine</title>
		<author>
			<persName><forename type="first">A</forename><surname>Venigalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="www.mosaicml.com/blog/introducing-pubmed-gpt" />
		<imprint>
			<date type="published" when="2022-03-06">2022. 06 Mar 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">MHKD-MVQA: multimodal hierarchical knowledge distillation for medical visual question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Medical VQA</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-19-0964-1_11</idno>
		<ptr target="https://doi.org/10.1007/978-981-19-0964-111" />
	</analytic>
	<monogr>
		<title level="m">Visual Question Answering: From Theory to Application</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Medical visual question answering via conditional reasoning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="2345" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bertscore: evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
