<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness</title>
				<funder ref="#_TW74kxn #_VaWApua">
					<orgName type="full">Johns Hopkins University Institute for Assured Autonomy</orgName>
					<orgName type="abbreviated">IAA</orgName>
				</funder>
				<funder ref="#_GeNsPME">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haolin</forename><surname>Yuan</surname></persName>
							<email>hyuan4@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Aucott</surname></persName>
							<email>jaucott2@jhmi.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University School of Medicine</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Armin</forename><surname>Hadzic</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins Applied Physics Laboratory</orgName>
								<address>
									<settlement>Laurel</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Paul</surname></persName>
							<email>william.paul@jhuapl.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins Applied Physics Laboratory</orgName>
								<address>
									<settlement>Laurel</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcia</forename><surname>Villegas De Flores</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><surname>Mathew</surname></persName>
							<email>philip.mathew@jhuapl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins Applied Physics Laboratory</orgName>
								<address>
									<settlement>Laurel</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philippe</forename><surname>Burlina</surname></persName>
							<email>philippe.burlina@jhuapl.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins Applied Physics Laboratory</orgName>
								<address>
									<settlement>Laurel</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinzhi</forename><surname>Cao</surname></persName>
							<email>yinzhi.cao@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="374" to="384"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1D266E1CDFBEE11AF72912FE3832B1EF</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lyme disease is a severe skin disease caused by tick bites, which affects hundreds of thousands of people. One task in diagnosing Lyme disease is lesion segmentation, i.e., separating benign skin from lesions, which can not only help clinicians to focus on lesions but also improve downstream tasks such as disease classification. However, it is challenging to segment Lyme disease lesions due to the lack of wellsegmented, labeled Lyme datasets and the nature of Lyme, e.g., the typical bull's eye lesion and its closeness to normal skin. In this paper, we design a simple yet novel data preprocessing and alteration method, called EdgeMixup, to help segment Lyme lesions on imbalanced training datasets. The key insight is to deploy a linear combination of lesion edge, either detected or computed, and the source image highlights the affected lesion area so that a learning model focuses more on the preserved lesion structure instead of skin tone, thus iteratively improving segmentation performance. Additionally, the improved edge from lesion segmentation can be further used for Lyme disease classification-e.g., in differentiating Lyme from other similar lesions including tinea corporis and herpes zoster-with improved model fairness on different subpopulations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical Image Analysis has greatly benefited from advances in AI <ref type="bibr" target="#b0">[1]</ref> yet some improvements still remain to be addressed, importantly in areas that allow both algorithmic performance and fairness <ref type="bibr" target="#b1">[2]</ref>, and in certain medical applications that promise to significantly lessen morbidity and mortality. Early detection of skin lesions is such an endeavor as it can aid in identifying infectious diseases with cutaneous manifestations. Lyme disease is an example of that with a potentially diagnostic skin lesion <ref type="bibr" target="#b2">[3]</ref>-which is caused by the bacterium Borrelia burgdorferi and leads to nearly 476,000 cases per annum during 2010-2018 <ref type="bibr" target="#b3">[4]</ref>. The earliest and most treatable phase of Lyme disease is manifested via a red concentric lesion at the site of a tick bite, called erythema migrans (EM) <ref type="bibr" target="#b4">[5]</ref>. While the EM pattern may appear simple to recognize, its diagnosis can be challenging for those with or without a medical background alike, as only 20% of United States patients have the stereotypical bull's eye lesion <ref type="bibr" target="#b5">[6]</ref>. When skin lesions are atypical they can be mistaken for other diseases such as tinea corporis (TC) or herpes zoster (HZ), two other diseases acting as confusers for Lyme, considered herein. This has increased interest in medical applications of deep learning (DL), and using deep convolutional neural networks (CNNs), to assist clinicians in timely and accurate diagnosis of conditions including Lyme disease, TC and HZ <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><p>One important diagnosis task is to segment Lyme lesion, particularly the EM pattern, from benign skins. Such DL-assisted segmentation not only helps clinicians in pre-screening patients but also improves downstream tasks such as lesion classification. However, while Lyme disease lesion segmentation is intuitively simple, it is challenging due to the following reasons. First, there lacks of a well-segmented dataset with manual labels on Lyme disease. On one hand, some datasets-such as HAM10000 <ref type="bibr" target="#b9">[10]</ref> and ISBI Challenges <ref type="bibr" target="#b10">[11]</ref>-have manual annotated segmentations for diseases like melanoma, but they do not have Lyme disease lesions. On the other hand, some datasets-such as Groh et al. <ref type="bibr" target="#b11">[12]</ref>-have Lyme disease and skin tone and classification labels, but not segmentation.</p><p>Second, the segmentation of Lyme lesion is itself challenging due to the nature of EM pattern. Specifically, a typical Lyme lesion exhibits a bull's eye pattern with one central redness and one outer circle, which is different from darkness lesion in cancer-related skin disease like melanoma. Furthermore, clinical data collected for training is usually imbalanced in some properties, e.g., more samples with light skins compared with dark skins. Therefore, existing skin disease segmentation <ref type="bibr" target="#b12">[13]</ref> as well as existing general segmentation works, such as U-Net <ref type="bibr" target="#b13">[14]</ref>, polar training <ref type="bibr" target="#b14">[15]</ref>, ViT-Adapter <ref type="bibr" target="#b15">[16]</ref>, and MFSNet <ref type="bibr" target="#b16">[17]</ref>, usually suffer from relatively low performance and reduced fairness <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>In this paper, we present the first Lyme disease dataset that contains labeled segmentation and skin tones. Our Lyme disease dataset contains two parts: (i) a classification dataset, composed of more than 3,000 diseased skin images that are either obtained from public resources or clinicians with patient-informed consent, and (ii) a segmentation dataset containing 185 samples that are manually annotated for three regions-i.e., background, skin (light vs. dark), and lesionconducted under clinician supervision and Institutional Review Boards (IRB) approval. Our dataset with manual labels is available at this URL <ref type="bibr" target="#b19">[20]</ref>.</p><p>Secondly, we design a simple yet novel data preprocessing and alternation method, called EdgeMixup, to improve Lyme disease segmentation and diagnosis fairness on samples with different skin-tones. The key insight is to alter a skin image with a linear combination of the source image and a detected lesion boundary so that the lesion structure is preserved while minimizing skin tone information. Such an improvement is an iterative process that gradually improves lesion edge detection and segmentation fairness until convergence. Then, the detected, converged edge in the first step also helps classification of Lyme diseases via mixup with improved fairness. Our source code is available at this URL <ref type="bibr" target="#b19">[20]</ref>.</p><p>We evaluate EdgeMixup for skin disease segmentation and classification tasks. Our results show that EdgeMixup is able to increase segmentation utility and improve fairness. We also show that the improved segmentation further improves classification fairness as well as joint fairness-utility metrics compared to existing debiasing methods, e.g., AD <ref type="bibr" target="#b20">[21]</ref> and ST-Debias <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>In this section, we motivate the design of EdgeMixup by showing that added lesion boundary helps a DL model focus more on the lesion part instead of other features such as skin or background. Note that not all skin disease datasets are carefully processed either due to the large amount of work required or the scarcity of data samples collected, e.g., SD-198 <ref type="bibr" target="#b22">[23]</ref> contains samples that are taken under variant environments. Specifically, we train two ResNet-34 models using the same dataset with and without EdgeMixup for a classification task of skin disease. We keep all hyper-parameters exactly the same for two models, and only augment the same image with and without mixing lesion boundary up with the original image. We generate initial lesion edges using EdgeMixup, which we will elaborate in following sections. Figure <ref type="figure" target="#fig_0">1</ref> shows the original image (Fig. <ref type="figure" target="#fig_0">1a</ref>) as well as two models' attention as heat-maps where red color represents the highest attention, yellow a higher attention, and purple the least attention. EdgeMixup helps the model to focus more on the lesion area comparing Fig. <ref type="figure" target="#fig_0">1b</ref> and<ref type="figure" target="#fig_0">1c</ref>. The reason is that a legacy diagnosis has no information about lesion and does not know where to locate its focus, thus easily gets distracted by fingers instead of the lesion pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first give the definition for model fairness, and we then describe the design of EdgeMixup for the purpose of de-biasing in Fig. <ref type="figure" target="#fig_1">2</ref>  EdgeMixup improves model fairness on light and dark skin samples in both segmentation and classification tasks, and it has two major components: (i) edge detection using mixup, and (ii) data preprocessing and alteration for downstream tasks. More specifically, our proposed edge detection has two parts: initial edge detection and iterative improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Edge Detection:</head><p>The purpose of initial detection, which is documented in the Initial_edge_detection function of Algorithm 1, is to provide a starting point, i.e., a rough boundary, for the next step of iterative improvement. The high-level idea is that EdgeMixup detects several edge candidates using the color range of ground-truth lesions in both Red-Green-Blue (RGB) and Hue-Saturation-Value (HSV) color space and then selects the target edge using a learning model based on the output confidence score. First, EdgeMixup trains a classification model based on a mixup of the ground-truth segmentation under clinician supervision and the original image (Line 7). Second, EdgeMixup generates many edge candidates. For example, EdgeMixup collects the mean range of lesion color from the training set and use the range as threshold to filter out any given sample for a candidate mask (Line 9). Lastly, EdgeMixup selects an edge candidate with the highest confidence score output by the learning model (Line 11) and returns it as the edge for this given sample. Note that the initial edge detection is irrelevant to the sample size of a particular subpopulation, thus improving the fairness. That is, even if the original dataset is imbalanced, as long as one sample from a subpopulation exists, the color range of the sample's lesion is considered in the initial detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Edge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Get all edge candidates {edge1, edge2, .., edgen} for each sample x 10:</p><p>Mixup each edge candidate with x 11:</p><p>Query mclass using all mixed-up {xedge 1 , ...xedge n } and choose the optimal edge edge opt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Generate edged sample xedge = Mixup(x, edge opt , α) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>while current_Jaccard &gt; best_Jaccard do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>best_Jaccard = current_Jaccard</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>23:</head><p>Predict lesion masks using miter, convert them to lesion edge edge</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>24:</head><p>Generate new training set for next model Mixup(Dtrain, edge, α)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25:</head><p>Train a model for next iteration miter+1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>26:</head><p>Evaluate miter+1 using edged D test edge and get current_Jaccard</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>27:</head><p>iter += 1  affected area, further detection will refine and constrain the detected boundary. Besides, EdgeMixup calculates a linear combination of original image and lesion boundary, i.e., by assigning the weight of original image as α and lesion boundary as 1α. Figure <ref type="figure">3</ref> shows the edge-mixed-up images for different iterations. EdgeMixup removes more skin areas after each iteration and gradually gets close to the real lesion at the third iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>We present two datasets: (i) a dataset collected and annotated by us (called Skin), and (ii) a subset of SD-198 <ref type="bibr" target="#b22">[23]</ref> with our annotation (called SD-sub). First, We collect and annotate a dataset with 3,027 images containing three types of disease/lesions, i.e., Tinea Corporis (TC), Herpes Zoster (HZ), and Erythema Migrans (EM). All skin images are either collected from publicly available sources or from clinicians with patient informed consent. Then, a medical technician and a clinician in our team manually annotate each image. For the segmentation task, we annotate skin images into three classes: background, skin, and lesion; then, for the classification task, we annotate skin images by classifying them into four classes: No Disease (NO), TC, HZ, and EM. We name it as Skin-class for later reference. Second, we select five classes from SD-198 <ref type="bibr" target="#b22">[23]</ref>, a benchmark dataset for skin disease classification, as another dataset for both segmentation and classification tasks. Note that due to the amount of manual work involved in annotation, we select those classes based on the number of samples in each class. The selected classes are Dermatofibroma (DF), Keratoacanthoma (KA), Pyogenic Granuloma (PG), Tinea Corporis (TC), and Tinea Faciale (TF). We choose 30 samples in each class for segmentation task, and we split them into 0.7, 0.1, and 0.2 ratio for training, validation, and testing, respectively.</p><p>Table <ref type="table" target="#tab_1">1</ref> show the characteristics of these two datasets for both classification and segmentation tasks broken down by the disease type and skin tone, as calculated by the Individual Typology Angle (ITA) <ref type="bibr" target="#b23">[24]</ref>. Specifically, we consider tan2, tan1, and dark as dark skin (ds) and others as light skin (ls). Compared to other skin tone classification schemas such as Fitzpartick scale <ref type="bibr" target="#b24">[25]</ref>, we divide ITA scores into more detailed categories (eight). One prominent observation is that ls images are more abundant than ds images due to a disparity in the availability of ds imagery found from either public sources or from clinicians with patient consent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We implement EdgeMixup using python 3.8 and Pytorch, and all experiments are performed using one GeForce RTX 3090 graphics card (NVIDIA).</p><p>Segmentation Evaluation. Our segmentation evaluation adopts four baselines, (i) a U-Net trained to segment skin lesions, (ii) a polar training <ref type="bibr" target="#b14">[15]</ref> transforming images from Cartesian coordinates to polar coordinates, (iii) ViT-Adapter <ref type="bibr" target="#b15">[16]</ref>, a state-of-the-art semantic segmentation using a fine-tuned ViT model, (iv) MFSNet <ref type="bibr" target="#b16">[17]</ref>, a segmentation model with differently scaled feature maps to compute the final segmentation mask. We follow the default setting from each paper for evaluation. Our evaluation metrics include (i) Jaccard index (IoU score), which measures the similarity between a predicted mask and the manually annotated ground truth, and (ii) the gap between Jaccard values (J gap ) to measure fairness. Table <ref type="table" target="#tab_2">2</ref> shows the performance and fairness of EdgeMixup and different baselines. We compare predicted masks with the manually-annotated ground truth by calculating the Jaccard index, and computing the gap for subpopulations with ls and ds (based on ITA). EdgeMixup, a data preprocessing method, improves the utility of lesion segmentation in terms of Jaccard index compared with all existing baselines. One reason is that EdgeMixup preserves skin lesion information, thus improving the segmentation quality, while attenuating markers for protected factors. Note that EdgeMixup iteratively improves the segmentation results. Take our Skin-seg dataset for example. We trained our baseline Unet model for three iterations, and the model utility is increased by 0.0468 on Jaccard index while the J gap between subpopulations is reduced by 0.0193. Classification Evaluation. Our classification evaluation involves: (i) Adversarial Debiasing (AD) <ref type="bibr" target="#b20">[21]</ref>, (ii) DexiNed-avg, the average version of DexiNed <ref type="bibr" target="#b25">[26]</ref> as an boundary detector used by EdgeMixup, and (iii) ST-Debias <ref type="bibr" target="#b21">[22]</ref>, a debiasing method augmenting data with conflicting shape and texture information. Our evaluation metrics include accuracy gap, the (Rawlsian) minimum accuracy across subpopulations, area under the receiver operating characteristic curve (AUC), and joint metrics (CAI α and CAUCI α ).</p><p>Table <ref type="table" target="#tab_3">3</ref> shows utility performance (acc and AUC) and fairness results (gaps of acc and AUC between ls and ds subpopulations). We here list two variants of EdgeMixup, and one of which, "Unet", uses the lesion edge generated by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Skin Disease Classification and Segmentation: Previous researches mainly work on improving model utility for both medical image <ref type="bibr" target="#b26">[27]</ref> and skin lesion <ref type="bibr" target="#b27">[28]</ref> classification. As for skin lesion segmentation tasks, few works has been proposed due to the lack of datasets with ground-truth segmentation masks. International Skin Imaging Collaboration (ISIC) hosts challenges of International Symposium on Biomedical Imaging (ISBI) <ref type="bibr" target="#b10">[11]</ref> to encourage researches studying lesion segmentation, feature detection, and image classification. However, official datasets released, e.g., HAM10000 <ref type="bibr" target="#b9">[10]</ref> only contains melanoma samples and all of the samples are with light skins according to our inspection using ITA scores.</p><p>Bias Mitigation: Researchers have addressed bias and heterogeneity in deep learning models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref>. First, masking sensitive factors in imagery is shown to improve fairness in object detection and action recognition <ref type="bibr" target="#b29">[30]</ref>. Second, adversarial debiasing operates on the principle of simultaneously training two networks with different objectives <ref type="bibr" target="#b30">[31]</ref>. The competing two-player optimization paradigm is applied to maximizing equality of opportunity <ref type="bibr" target="#b31">[32]</ref>. As a comparison, EdgeMixup is an effective preprocessing approach to debiasing when applied to skin disease particularly for Lyme-focused classification and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present a simple yet novel approach to segment Lyme disease lesion, which can be further used for disease classification. The key insight is a novel data preprocessing method that utilizes edge detection and mixup to isolate and highlight skin lesions and reduce bias. EdgeMixup outperforms SOTAs in terms of Jaccord index for segmentation and CAI α and CAUCI α for disease classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A motivating example to illustrate why EdgeMixup improves model performance and reduces biases via mixing up lesion boundary with original image (Heatmap is generated via Grad-CAM). (Color figure online)</figDesc><graphic coords="3,42,81,53,69,338,29,110,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. EdgeMixup Process</figDesc><graphic coords="4,63,36,64,52,156,28,65,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 : 3 : 6 : 7 : 8 :</head><label>23678</label><figDesc>Improvement: EdgeMixup includes iterative edge improvement in the training phase of our segmentation model to further improve model utility. The intuitive reason of utilizing such algorithm is that by applying the Algorithm 1. Pseudo-code of EdgeMixup Require: A labelled sample (x, y) ∈ D, mixup weights α, ground-truth edged training set D train edge_gt Ensure: dataset Dfinal_edge in which each sample has it lesion edge highlighted (xedge, y) 1: function main( ) Dinitial_edge = Initial_edge_detection(D, α) Dfinal_edge = Iterative_edge_improvement(Dinitial_edge, α) function Initial_edge_detection(D, α) Train classification model mclass using D train edge_gt for each sample x ∈ D do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>30 :Fig. 3 .</head><label>303</label><figDesc>Fig. 3. Illustration of iterative edge improvement on different iterations with train loss</figDesc><graphic coords="5,42,81,377,21,338,44,73,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Annotated segmentation and classification dataset characteristics, broken down by ITA-based skin tones (light skin/ dark skin) and disease types.</figDesc><table><row><cell cols="2">Split Skin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SD-sub</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>NO</cell><cell>EM</cell><cell>HZ</cell><cell>TC</cell><cell>Total</cell><cell>DF</cell><cell cols="2">KA PG</cell><cell>TC</cell><cell>TF</cell><cell>Total</cell></row><row><cell>seg</cell><cell>-</cell><cell>62</cell><cell>62</cell><cell>61</cell><cell>185</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>30</cell><cell>150</cell></row><row><cell></cell><cell>-</cell><cell cols="4">47/15 46/16 40/21 133/52</cell><cell cols="6">23/7 27/3 27/3 24/6 29/1 130/20</cell></row><row><cell cols="2">class 885</cell><cell>740</cell><cell>698</cell><cell>704</cell><cell>3027</cell><cell>40</cell><cell>40</cell><cell>40</cell><cell>40</cell><cell>40</cell><cell>200</cell></row><row><cell></cell><cell cols="11">822/63 682/58 608/90 609/95 2721/306 36/4 36/4 29/1 33/7 30/0 164/16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Segmentation: Performance and Fairness (margin of error reported in parenthesis)</figDesc><table><row><cell></cell><cell cols="2">Method Unet</cell><cell>Polar</cell><cell>MFSNet</cell><cell>ViT-Adapter EdgeMixup</cell></row><row><cell>Skin</cell><cell cols="4">Jaccard 0.7053(0.0035) 0.7126(0.0033) 0.5877(0.0080) 0.7027(0.0057) 0.7807(0.0031)</cell></row><row><cell></cell><cell>Jgap</cell><cell cols="3">0.0809(0.0001) 0.0813(0.0001) 0.1291(0.0076) 0.2346(0.0035) 0.0379(0.0001)</cell></row><row><cell cols="5">SD-seg Jaccard 0.7134(0.0031) 0.6527(0.0036) 0.6170(0.0052) 0.5088(0.0042) 0.7799(0.0031)</cell></row><row><cell></cell><cell>Jgap</cell><cell cols="3">0.0753(0.0001) 0.1210(0.0003) 0.0636(0.0033) 0.2530(0.0021) 0.0528(0.0001)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Skin disease classification and associated bias. Samples contain skin tones as a protected factor. (margin of error reported in parentheses, subpopulation reported in brackets) EdgeMixup outperforms SOTA approaches in balancing the model's performance and fairness, i.e., the CAI α and CAUCI α values of EdgeMixup are the highest compared with the vanilla ResNet34 and other baselines.</figDesc><table><row><cell></cell><cell>Metrics</cell><cell>ResNet34</cell><cell>Baselines</cell><cell></cell><cell></cell><cell cols="2">EdgeMixup (ours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AD</cell><cell cols="2">DexiNed-avg ST-Debias</cell><cell>U-Net</cell><cell>Mask-based</cell></row><row><cell>Skin</cell><cell>acc</cell><cell cols="6">88.08(3.66) 81.79(4.35) 69.87(5.17) 76.52(5.23) 86.75(3.82) 86.09(3.90)</cell></row><row><cell></cell><cell>accgap</cell><cell cols="4">16.38(12.21) 5.33(11.69) 19.79(13.52) 2.64(8.05)</cell><cell cols="2">8.280(9.66) 1.923(8.49)</cell></row><row><cell></cell><cell>accmin</cell><cell>73.33[ds]</cell><cell>76.92[ds]</cell><cell>51.85[ds]</cell><cell>71.12[ds]</cell><cell>79.41[ds]</cell><cell>84.38[ds]</cell></row><row><cell></cell><cell>CAI0.5</cell><cell>-</cell><cell>2.380</cell><cell>-10.81</cell><cell>1.090</cell><cell>3.385</cell><cell>6.233</cell></row><row><cell></cell><cell>CAI0.75</cell><cell>-</cell><cell>6.715</cell><cell>-7.110</cell><cell>7.415</cell><cell>5.743</cell><cell>10.35</cell></row><row><cell></cell><cell>AUC</cell><cell cols="6">0.977(0.02) 0.956(0.02) 0.889(0.04) 0.933(0.03) 0.974(0.02) 0.973(0.02)</cell></row><row><cell></cell><cell>AUCgap</cell><cell cols="5">0.039(0.07) 0.009(0.05) 0.090(0.11) 0.035(0.04) 0.011(0.02)</cell><cell>0.01 (0.05)</cell></row><row><cell></cell><cell>AUCmin</cell><cell>0.942[ds]</cell><cell>0.955[ds]</cell><cell>0.807[ds]</cell><cell>0.910[ds]</cell><cell>0.973[ds]</cell><cell>0.964[ds]</cell></row><row><cell></cell><cell cols="2">CAUCI0.5 -</cell><cell>0.004</cell><cell>-0.069</cell><cell>-0.024</cell><cell>0.012</cell><cell>0.013</cell></row><row><cell></cell><cell cols="2">CAUCI0.75 -</cell><cell>0.017</cell><cell>-0.060</cell><cell>-0.014</cell><cell>0.020</cell><cell>0.022</cell></row><row><cell cols="2">SD-sub acc</cell><cell cols="6">75.60(14.26) 73.53(14.83) 63.13(16.08) 71.73(13.01) 74.17(13.30) 76.47(11.26)</cell></row><row><cell></cell><cell>accgap</cell><cell cols="6">28.12(54.98) 25.00(54.30) 25.21(51.20) 18.66(13.21) 18.51(11.50) 15.00(9.62)</cell></row><row><cell></cell><cell>accmin</cell><cell>50.00[ds]</cell><cell>50.00[ds]</cell><cell>43.75 [ds]</cell><cell>70.59[ls]</cell><cell>72.11[ls]</cell><cell>75.00[ls]</cell></row><row><cell></cell><cell>CAI0.5</cell><cell>-</cell><cell>0.525</cell><cell>-4.780</cell><cell>2.795</cell><cell>4.090</cell><cell>6.995</cell></row><row><cell></cell><cell>CAI0.75</cell><cell>-</cell><cell>1.822</cell><cell>-0.934</cell><cell>6.127</cell><cell>6.850</cell><cell>10.06</cell></row><row><cell></cell><cell>AUC</cell><cell cols="6">0.922(0.10) 0.962(0.06) 0.824(0.13) 0.941(0.08) 0.953(0.11) 0.970(0.06)</cell></row><row><cell></cell><cell>AUCgap</cell><cell cols="6">0.429(0.35) 0.319(0.36) 0.175(0.29) 0.255(0.31) 0.178(0.29) 0.170(0.29)</cell></row><row><cell></cell><cell>AUCmin</cell><cell>0.500[ds]</cell><cell>0.650[ds]</cell><cell>0.650[ds]</cell><cell>0.711[ds]</cell><cell>0.784[ds]</cell><cell>0.800[ds]</cell></row><row><cell></cell><cell cols="2">CAUCI0.5 -</cell><cell>0.075</cell><cell>0.078</cell><cell>0.097</cell><cell>0.140</cell><cell>0.153</cell></row><row><cell></cell><cell cols="2">CAUCI0.75 -</cell><cell>0.092</cell><cell>0.166</cell><cell>0.135</cell><cell>0.196</cell><cell>0.206</cell></row><row><cell cols="8">the baseline Unet model while "mask-based" implements deep-learning model</cell></row><row><cell cols="8">involved methodology introduced in Sect. 3. By adding the "Unet" variant, we</cell></row><row><cell cols="8">demonstrate here that simply applying lesion edge predicetd by the baseline Unet</cell></row><row><cell cols="8">model, while not optimal, efficiently reduces model bias on different skin-tone</cell></row><row><cell cols="2">samples.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by <rs type="funder">Johns Hopkins University Institute for Assured Autonomy (IAA)</rs> with grants <rs type="grantNumber">80052272</rs> and <rs type="grantNumber">80052273</rs>, and <rs type="funder">National Science Foundation (NSF)</rs> under grants <rs type="grantNumber">CNS18-54000</rs>. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of <rs type="affiliation">NSF</rs> or <rs type="institution">JHU-IAA</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TW74kxn">
					<idno type="grant-number">80052272</idno>
				</org>
				<org type="funding" xml:id="_VaWApua">
					<idno type="grant-number">80052273</idno>
				</org>
				<org type="funding" xml:id="_GeNsPME">
					<idno type="grant-number">CNS18-54000</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AI for medical imaging goes deep</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="539" to="540" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Addressing artificial intelligence bias in retinal disease diagnostics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Bressler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transl. Vis. Sci. Technol</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lyme disease testing by large commercial laboratories in the United States</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Hinckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Infect. Diseases</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="676" to="681" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating the frequency of Lyme disease diagnoses</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kugeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Delorey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Mead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Hinckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emerg. Infect. Diseases</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">616</biblScope>
			<date type="published" when="2010">2010-2018. 2021</date>
			<pubPlace>United States</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Nadelman</surname></persName>
		</author>
		<title level="m">Erythema migrans. Infectious Disease Clinics of North America</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Does this patient have erythema migrans?</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Tibbles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Edlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">297</biblScope>
			<biblScope unit="page" from="2617" to="2627" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated detection of erythema migrans and other confounding skin lesions via deep learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rebman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Aucott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="151" to="156" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Progressive transfer learning and adversarial domain adaptation for cross-domain skin disease classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Bonnington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1379" to="1393" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AI-based detection of erythema migrans and disambiguation against other skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rebman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Aucott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page">103977</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC</title>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Groh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF CVPR</title>
		<meeting>the IEEE/CVF CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skin lesion segmentation and multiclass classification using deep learning features and improved moth flame optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Damaševičius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maskeliūnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">811</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training on polar image transformations improves biomedical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Benčević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Galić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Habijan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Babin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="133365" to="133375" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vision transformer adapter for dense predictions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MFSNet: a multi focus segmentation network for skin lesion segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">108673</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Caton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04053</idno>
		<title level="m">Fairness in machine learning: A survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lowshot deep learning of diabetic retinopathy with potential applications to address artificial intelligence bias in retinal diagnostics and rare ophthalmic diseases</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Bressler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1070" to="1077" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="https://github.com/Haolin-Yuan/EdgeMixup" />
		<title level="m">Edgemixup repository</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2018 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shape-texture debiased neural network training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A benchmark for automatic visual classification of clinical skin disease images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46466-4_13" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9910</biblScope>
			<biblScope unit="page" from="206" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Wilkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fitzpatrick skin type, individual typology angle, and melanin index in an African population: steps toward universally applicable skin photosensitivity assessments</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="902" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Soleil et peau</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<editor>J. Médecine Esthétique</editor>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
	<note>in French</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dense extreme inception network: towards a robust CNN model for edge detection</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Poma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale robust deep auc maximization: a new surrogate loss and empirical studies on medical image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Skin lesion classification of dermoscopic images using machine learning and convolutional neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chengoden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lakshmanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">18134</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Addressing heterogeneity in federated learning via distributional transformation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19839-7_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19839-7_11" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13698</biblScope>
			<biblScope unit="page" from="179" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Balanced datasets are not enough: estimating and mitigating gender bias in deep image representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial training for free!</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00075</idno>
		<title level="m">Data decisions and theoretical implications when adversarially learning fair representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
