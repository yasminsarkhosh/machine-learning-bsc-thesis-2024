<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziquan</forename><surname>Wei</surname></persName>
							<idno type="ORCID">0000-0001-6553-4482</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tingting</forename><surname>Dan</surname></persName>
							<idno type="ORCID">0000-0001-6936-2649</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Ding</surname></persName>
							<idno type="ORCID">0009-0005-0131-4348</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mustafa</forename><surname>Dere</surname></persName>
							<idno type="ORCID">0009-0009-1755-9672</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guorong</forename><surname>Wu</surname></persName>
							<email>guorong_wu@med.unc.edu</email>
							<idno type="ORCID">0000-0002-0550-6145</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<postCode>27599</postCode>
									<settlement>Chapel Hill</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="46" to="55"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">258653F95E14603495ABA551158BF368</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image stitching</term>
					<term>3D microscopy image</term>
					<term>Whole-brain nucleus instance segmentation</term>
					<term>Graph neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-throughput 3D nuclei instance segmentation (NIS) is critical to understanding the complex structure and function of individual cells and their interactions within the larger tissue environment in the brain. Despite the significant progress in achieving accurate NIS within small image stacks using cutting-edge machine learning techniques, there has been a lack of effort to extend this approach towards whole-brain NIS from light-sheet microscopy. This critical area of research has been largely overlooked, despite its importance in the neuroscience field. To address this challenge, we propose an efficient deep stitching neural network built upon a knowledge graph model characterizing 3D contextual relationships between nuclei. Our deep stitching model is designed to be agnostic, enabling existing limited methods (optimized for image stack only) to overcome the challenges of whole-brain NIS, particularly in addressing the issue of inter-and intra-slice gaps. We have evaluated the NIS accuracy on top of three state-of-the-art deep models with 128 × 128 × 64 image stacks, and visualized results in both inter-and intra-slice gaps of whole brain. With resolved gap issues, our deep stitching model enables the whole-brain NIS (gigapixel-level) on entry-level GPU servers within 27 h.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Light-sheet microscopy is a powerful imaging modality that allows for fast and high-resolution imaging of large samples, such as the whole brain of the Supported by NIH R01NS110791, NIH R01MH121433, NIH P50HD103573, and Foundation of Hope.</p><p>mouse <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. Tissue-clearing techniques enable the removal of light-scattering molecules, thus improving the penetration of light through biological samples and allowing for better visualization of internal structures, including nuclei <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. Together, light-sheet microscopy and tissue-clearing techniques have revolutionized the field of biomedical imaging and they have been widely used for studying the structure and function of tissues and organs at the cellular level.</p><p>Accurate 3D nuclei instance segmentation plays a crucial role in identifying and delineating individual nuclei within three-dimensional space, which is essential for understanding the complex structure and function of biological tissues in the brain. Previous <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b4">[5]</ref> have applied graph-based approaches that model links of voxel and neuron region, respectively, for 3D neuron segmentation from electron microscopy image stacks. However, accurate segmentation of nuclei from light-sheet microscopy images of cleared tissue can be a challenging task due to the presence of complex tissue structures, cell shapes, and variations in nuclei size and shape <ref type="bibr" target="#b0">[1]</ref>. Due to the high cost of 3D manual nuclei annotations and the complexity of learning, current end-to-end NIS models are typically limited to training and testing on small image stacks (e.g., 128×128×64). Considering these limitations, one approach for achieving whole-brain NIS is dividing the whole stack into smaller stacks, so the existing NIS methods can handle each piece individually. In such a scenario, constructing the whole-brain nuclei instance segmentation in 3D from these smaller image stacks arises a new challenge. The gaps between these smaller stacks (intra-slice) and the slices (inter-slice) require a robust stitching method for accurate NIS. We show these gaps in Fig. <ref type="figure" target="#fig_1">1</ref>. Note, the intra-slice gap, commonly referred to as the boundary gap, arises due to the existence of boundaries in the segmentation outcome of image stacks and poses a challenge in achieving smooth segmentation between neighboring image stacks. Current approaches may, however, undermine the overall quality of the wholebrain NIS results which leads to inaccurate reports of nuclei counts. Figure <ref type="figure" target="#fig_1">1</ref> (left) illustrates the typical examples of the boundary gap issues, where the red circle highlights the incidence of over-counting (both partial nuclei instances are recognized as a complete nucleus in the corresponding image stack), while the dashed blue box denotes the issue of under-counting (none of the partial nuclei instances has been detected in each image stack).</p><p>It is a common practice to use overlapped image stacks to stitch the intensity image (continuous values) by weighted averaging from multiple estimations <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. However, when nuclei are in close proximity and represent the same entity, it becomes crucial to accurately match the indexes of nuclei instances, which refer to the segmentation labels. We call this the nuclei stitching issue. This issue presents a significant challenge in the pursuit of achieving wholebrain NIS. To address this non-trivial challenge, we formulate this problem as a knowledge graph (KP) task that is built to characterize the nuclei-to-nuclei relationships based on the feature presentation of partial image appearance. By doing so, the primary objective of this learning problem is to determine whether to merge two partial nuclei instances that exist across different slices or stacks. Drawing inspiration from recent research on object tracking using graph models,  we construct a graph contextual model to assemble 3D nuclei in a graph neural network (GNN). In the overlapping area, the complete 2D nucleus instance is represented as a graph node, and the links between these nodes correspond to the nuclei-to-nuclei relationship. Conventional knowledge graph learning typically emphasizes the learning of the relationship function. In this context, it appears that the process of wholebrain NIS largely depends on the relationship function to link partial nuclei instances along the gap between slices. Nonetheless, a new challenge arises from the NIS backbone due to the anisotropic image resolution where inter-slice Z resolution (e.g., 2.5 µm) is often several times lower than the in-plane X-Y resolution (0.75 × 0.75 µm 2 ). That is, the (partial) nuclei instances located near the boundary across X-Y planes (along the inter-slice direction with poor image resolution) in the 3D image stack have a large chance of being misidentified, leading to the failure of NIS stitching due to the absence of nuclei instances that are represented as nodes in the contextual graph model. To alleviate this issue, we present a two-stage hierarchical whole-brain NIS framework that involves stitching 2D NIS results in the X-Y plane (stage 1) and then assembling these 2D instances into 3D nuclei using a graph contextual model (stage 2). The conjecture is that the high resolution in the X-Y plane minimizes the risk of missing 2D NIS instances, allowing the knowledge graph learning to be free of absent nodes in establishing correspondences between partial nuclei instances.</p><p>Stitching 2D nuclei instances in each image slice is considerably easier than stitching across image slices due to the finer image resolution in the X-Y plane. However, as shown in Fig. <ref type="figure" target="#fig_1">1</ref> (right), the poor resolution in the Z-axis makes it challenging to identify partial nuclei instances along this axis. Thus, prestitching in the X-Y plane of the first stage can reduce the probability of having 2D nuclei instances missing along the Z axis at the second stage. Among this, we train the graph contextual model to predict the nuclei-to-nuclei correspondence across image slices, where each node is the 2D nuclei instance without the intraslice gap issue. Since stage 2 is on top of the existing NIS methods in stage 1, our stitching framework is agnostic and can support any state-of-the-art NIS methods to expand from small image stacks to the entire brain.</p><p>In the experiments, we have comprehensively evaluated the segmentation and stitching accuracy (correspondence matching between 2D instances) and wholebrain NIS results (both visual inspection and quantitative counting results). Compared to no stitching, Our deep stitching model has shown a significant improvement in the whole-brain NIS results with different state-of-the-art models, indicating its potential for practical applications in the field of neuroscience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Current state-of-the-art NIS methods, such as Mask-RCNN <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, 3D Unet <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> and Cellpose <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> are designed to segment nuclei instances in a predefined small image stack only. To scale up to whole-brain NIS, we propose a graph-based contextual model to establish nuclei-to-nuclei correspondences across image stacks. On top of this backbone, we present a hierarchical wholebrain NIS stitching framework that is agnostic to existing NIS methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Contextual Model</head><p>Problem Formulation. Our graph contextual model takes a set of partial nuclei instances, sliced by the inter-slice gap, as input. These nuclei instances can be obtained using a 2D instance segmentation method, such as Mask-RCNN. The output of our model is a collection of nuclei-to-nuclei correspondences, which enable us to stitch together the NIS results from different image slices (by running NIS separately). We formulate this correspondence matching problem as a knowledge graph learning task where the links between nodes in the graph contextual model represent the probability of them belonging to the same nuclei instance. In this regard, the key component of NIS stitching becomes seeking for a relationship function that estimates the likelihood of correspondence based on the node features, i.e., image appearance of to-be-stitched 2D nuclei instances.</p><p>Machine Learning Components in Graph Contextual Model. First, we construct an initial contextual graph G = {V, E} for each 2D nucleus instance x (i.e., image appearance vector). The set of nodes V = {x i |D(x, x i ) &gt; δ} includes all neighboring 2D nuclei instances, where the distance between the centers of two instances is denoted by D, and δ is a predefined threshold. The matrix E ∈ R N ×N represents the edges between nodes, where N is the number of neighboring instances. Specifically, we compute the Intersection over Union (IoU) between the two instances and set the edge weight as e ij = IoU (x i , x j ).</p><p>Second, we train the model on a set of contextual graphs G to recursively (1) find the mapping function γ to describe the local image appearance on each graph node and (2) learn the triplet similarity function ψ.</p><p>-Graph feature representation learning. For the k th iteration, we enable two connected nodes to exchange their feature representations constrained by the current relationship topology e k ij by the k th layer of the deep stitching model. In this context, we define the message-passing function as:</p><formula xml:id="formula_0">x (k+1) i = γ (k+1) x (k) i , Σ j∈N (i) φ (k) (x (k) i , x (k) j , e (k) j,i ) . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Following the popular learning scheme in knowledge graphs <ref type="bibr" target="#b12">[13]</ref>, we employ Multilayer Perceptron (MLP) to act functions γ, φ. -Learning the link-wise similarity function to predict nuclei-to-nuclei correspondence. Given the updated node feature representations {x (k+1) i }, we train another MLP to learn the similarity function ψ in a layer-by-layer manner. In the k th layer, we update each 2D-to-3D contextual correspondence e (k+1) j,i for the next layer by</p><formula xml:id="formula_2">e (k+1) j,i = ψ (k+1) x (k+1) i , x (k+1) j , e (k) j,i .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Stitching Framework for Whole-Brain NIS</head><p>Our graph contextual model is able to stitch the NIS results across the intraslice gap areas in X/Y-Z plane. As demonstrated in Fig. <ref type="figure" target="#fig_1">1</ref>, the accuracy of no stitching is limited by the absence of 2D partial nuclei instances due to the poor inter-slice resolution. With the graph model, as shown in Fig. <ref type="figure" target="#fig_2">2</ref> and<ref type="figure" target="#fig_3">3</ref>, we propose a hierarchical stitching framework for whole-brain NIS in two stages.</p><p>1. Resolve intra-slice gap in X-Y plane. Suppose that each within-stack NIS result overlaps with its neighboring image stack in the X-Y plane. Then, we can resolve the intra-slice gap problem in X-Y plane in three steps: (i) identify the duplicated 2D nuclei instances from multiple overlapped image stacks, (ii) find the representative NIS result from the "gap-free" image stack, and (iii) unify multiple NIS estimations by using the "gap-free" NIS estimation as the appearance of the underlying 2D nuclei. The effect of this solution is shown in Fig. <ref type="figure" target="#fig_2">2</ref> right, where the gray areas are spatially overlapped. We use the arrows to indicate that the 2D nuclei instances (red dash circles) have been merged to the counterpart from the "gap-free" image stack (yellow circles). 2. Inter-slice stitching using graph contextual model. At each gap area along Z-axis, we deploy the graph contextual model to stitch the sliced nuclei instances. Specifically, we follow the partition of the whole-brain microscopy image in stage 1, that is a set of overlapped 3D image stacks, all the way from the top to the bottom as shown in the left of Fig. <ref type="figure" target="#fig_3">3</ref>. It is worth noting that each 2D nuclei instance in the X-Y plane is complete, as indicated by the red-highlighted portion extending beyond the image stack. Next, we assign a stack-specific local index to each 2D nuclei instance. After that, we apply the (trained) graph contextual model to each 2D nuclei instance. By tracking the correspondences among local indexes, we remove the duplicated inter-slice correspondence and assign the global index to the 3D nuclei instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation Details</head><p>We empirically use 18.75 µm for the overlap size between two neighboring image stacks in X-Y plane. The conventional NIS method is trained using 128 × 128 patches. For the graph contextual model, the MLPs consist of 12 fully-connected layers. Annotated imaging data has been split into training, validation, and testing sets in a ratio of 6:1:1. Adam is used with lr = 5e -4 as the optimizer, Dropout = 0.5, and focal loss as the loss function.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Stitching Methods Under Comparison. We mainly compare our stitching method with the conventional analytic approach by IoU (Intersection over Union) matching, which is widely used in object detection with no stitching. We perform the experiments using three popular NIS deep models, that are Mask-RCNN-R50 (with ResNet50 backbone), Mark-RCNN-R101 (with ResNet101 backbone) and CellPose, with two stitching methods, i.e., our hierarchical stitching framework and IoU-based matching scheme.</p><p>Data and Computing Environment. </p><formula xml:id="formula_3">In</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We use the common metrics precision, recall, and F1 score to evaluate the 3D NIS between annotated and predicted nuclei instances. Since the major challenge of NIS stitching is due to the large inter-slice gap, we also define the stitching accuracy for each 3D nuclei instance by counting the number of 2D NIS (in the X-Y plane) that both manual annotation and stitched nuclei share the same instance index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluating the Accuracy of NIS Stitching Results</head><p>Quantitative Evaluation. As shown in Fig. <ref type="figure">4</ref>, there is a clear sign that NIS models with our hierarchical stitching method outperform IoU-based counterparts on NIS metrics, regardless of the NIS backbone models. In average, our hierarchical stitching method has improved 14.0%, 5.1%, 10.2%, and 3.4% in precision, recall, F1 score, and stitching accuracy, respectively compared with IoU-based results.</p><p>Visual Inspection. We also show the visual improvement of whole-brain NIS results before and after stitching in Fig. <ref type="figure">5</ref>. Through the comparison, it is apparent that (1) the inconsistent NIS results along the intra-slice gap area have been </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Whole-Brain NIS in Neuroscience Applications</head><p>One of the important steps in neuroscience studies is the measurement of regional variations in terms of nuclei counts. In light of this, we evaluate the counting accuracy. Since we only have 16 image stacks with manual annotations, we sim-ulate the stack-to-stack gap in the middle of each image stack and compare counting results between whole-brain NIS with or without hierarchical stitching. Compared to whole-brain NIS without stitching, our hierarchical stitching method has reduced the nuclei counting error from 48.8% down to 10.1%. The whole-brain NIS improvement has vividly appeared in Fig. <ref type="figure">5</ref>. In addition, the running time of whole-brain NIS is around 26 h on average for the typical light-sheet microscopy images of the mouse brain. More visual results of the whole-brain NIS can be found in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we introduce a learning-based stitching approach to achieve 3D instance segmentation of nuclei in whole-brain microscopy images. Our stitching framework is flexible enough to incorporate existing NIS methods, which are typically trained on small image stacks and may not be able to scale up to the whole-brain level. Our method shows great improvement by addressing interand intra-slice gap issues. The promising results in simulated whole-brain NIS, particularly in terms of counting accuracy, also demonstrate the potential of our approach for neuroscience research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Top left: Nuclei over-counting (in red) and under-counting (in dashed blue) issues due to the boundary gaps across image stacks. Bottom left: Undetected nuclei instances make stitching difficult due to the absence of partial nuclei instances (indicated by red crosses). Top right: The limited inter-slice resolution presents a challenge for NIS in each image stack, particularly for partial nuclei instances near the stack boundary. Bottom right: Our hierarchal stitching framework leverages overlapping strategy in X-Y plane to reduce the chance of absent nuclei instances in intra-slice stitching. (Color figure online)</figDesc><graphic coords="3,41,79,54,20,340,33,198,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Stage one of the proposed hierarchical stitching framework for whole-brain NIS. Resolve the intra-slice gap in the X-Y plane by overlap. (Color figure online)</figDesc><graphic coords="5,41,79,54,50,340,33,135,22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Stage two of the proposed hierarchical stitching framework for whole-brain NIS. Graph contextual model for inter-slice gap.</figDesc><graphic coords="6,60,48,54,11,331,60,98,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. The NIS precision (a), recall (b), F1 score (c), and stitching accuracy (d) by stitching or not, where the NIS backbones include Mask-RCNN-R50 (A, blue), Mask-RCNN-R101 (B, orange), CellPose (C, green). (Color figure online)</figDesc><graphic coords="8,55,98,192,62,340,30,198,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the following experiments, we first train Mask-RCNN-R50, Mask-RCNN-R101, and CellPose on 16 image stacks (128 × 128 × 64), which include in total 6,847 manually labeled 3D nuclei. Then we make the methods comparison in two ways. For the stitching comparison based on 16 image stacks, We integrate each NIS model into two stitching methods respectively, which yields six NIS methods and corresponding results. For the stitching comparison based on whole-brain images with the size of 8, 729 × 9, 097 × 1, 116voxel 3 and the resolution of 0.75 µm × 0.75 µm × 2.5 µm, we firstly partition the one whole-brain image into 89,424 (69 × 72 × 18) image stacks with the size of 128 × 128 × 64, then employing the best NIS model to segment nuclei instances in each image stack in a parallel manner, finally we deploy our hierarchical stitching framework to stitch the image stacks together.</figDesc><table><row><cell>All experiments are run on a Linux server with 4 GPUs (24 GB) and 48 CPUs</cell></row><row><cell>with 125 GB RAM.</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_5.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review of nuclei detection and segmentation on microscopy images using deep learning with applications to unbiased stereology counting</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Alahmari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldgof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Mouton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhanced visualization of tissue microstructures using swept-source optical coherence tomography and edible oil as optical clearing agent</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poddar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optik</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page">169693</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advances in studying whole mouse brain vasculature using high-resolution 3D light microscopy imaging</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurophotonics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21902</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D U-net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_49" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016, Part II</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient automatic 3D-reconstruction of branching neurons from EM data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1004" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeNerD: high-throughput detection of neurons for brain-wide analysis with deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karayannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13828</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NucMM dataset: 3D neuronal nuclei instance segmentation at subcubic millimeter scale</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_16" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="164" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cellpose 2.0: how to train your own model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pachitariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Solving large multicut problems for connectomics via domain decomposition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kreshuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cellpose: a generalist algorithm for cellular segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michaelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pachitariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Handcrafted histological transformer (H2T): unsupervised representation of whole slide images</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102743</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relational message passing for knowledge graph completion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1697" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DaXi-high-resolution, large imaging volume and multi-view singleobjective light-sheet microscopy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="469" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning-based six-type classifier for lung cancer and mimics from histopathological whole slide images: a retrospective study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">High cell density and high-resolution 3D bioprinting for fabricating vascularized tissues</title>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Adv</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7923</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
