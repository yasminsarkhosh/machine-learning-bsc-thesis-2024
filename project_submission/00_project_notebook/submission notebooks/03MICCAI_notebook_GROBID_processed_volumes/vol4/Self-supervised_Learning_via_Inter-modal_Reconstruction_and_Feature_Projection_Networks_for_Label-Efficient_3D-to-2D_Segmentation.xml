<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation</title>
				<funder>
					<orgName type="full">Christian Doppler Research Association</orgName>
				</funder>
				<funder>
					<orgName type="full">Austrian Federal Ministry for Digital and Economic Affairs</orgName>
				</funder>
				<funder>
					<orgName type="full">National Foundation for Research, Technology and Development, and Heidelberg Engineering</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">José</forename><surname>Morano</surname></persName>
							<idno type="ORCID">0000-0003-3785-8185</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Christian Doppler Laboratory for Artificial Intelligence in Retina</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guilherme</forename><surname>Aresta</surname></persName>
							<idno type="ORCID">0000-0002-4225-2156</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Christian Doppler Laboratory for Artificial Intelligence in Retina</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dmitrii</forename><surname>Lachinov</surname></persName>
							<idno type="ORCID">0000-0002-2880-2887</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Christian Doppler Laboratory for Artificial Intelligence in Retina</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julia</forename><surname>Mai</surname></persName>
							<idno type="ORCID">0000-0002-0528-9742</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Lab for Ophthalmic Image Analysis</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ursula</forename><surname>Schmidt-Erfurth</surname></persName>
							<idno type="ORCID">0000-0002-7788-7311</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Lab for Ophthalmic Image Analysis</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hrvoje</forename><surname>Bogunović</surname></persName>
							<email>hrvoje.bogunovic@meduniwien.ac.at</email>
							<idno type="ORCID">0000-0002-9168-0894</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Christian Doppler Laboratory for Artificial Intelligence in Retina</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology and Optometry</orgName>
								<orgName type="laboratory">Lab for Ophthalmic Image Analysis</orgName>
								<orgName type="institution">Medical University of Vienna</orgName>
								<address>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="589" to="599"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2284551B4F7D96A28562CD905DF122AC</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image segmentation</term>
					<term>self-supervised learning</term>
					<term>OCT</term>
					<term>retina</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has become a valuable tool for the automation of certain medical image segmentation tasks, significantly relieving the workload of medical specialists. Some of these tasks require segmentation to be performed on a subset of the input dimensions, the most common case being 3D→2D. However, the performance of existing methods is strongly conditioned by the amount of labeled data available, as there is currently no data efficient method, e.g. transfer learning, that has been validated on these tasks. In this work, we propose a novel convolutional neural network (CNN) and self-supervised learning (SSL) method for label-efficient 3D→2D segmentation. The CNN is composed of a 3D encoder and a 2D decoder connected by novel 3D→2D blocks. The SSL method consists of reconstructing image pairs of modalities with different dimensionality. The approach has been validated in two tasks with clinical relevance: the en-face segmentation of geographic atrophy and reticular pseudodrusen in optical coherence tomography. Results on different datasets demonstrate that the proposed CNN significantly improves the state of the art in scenarios with limited labeled data by up to 8% in Dice score. Moreover, the proposed SSL method allows further improvement of this performance by up to 23%, and we show that the SSL is beneficial regardless of the network architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning can significantly reduce the workload of medical specialists during image segmentation tasks, which are essential for patient diagnosis and follow-up management <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>. For most tasks, segmentation masks have the same dimensionality as the input. However, there are some tasks for which segmentation has to be performed in a subset of the dimensions of the data, e.g. 3D→2D <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>. This occurs, for example, for the segmentation of geographic atrophy (GA) in optical coherence tomography (OCT), where the segmentation is performed on the OCT projection. In recent years, several methods have been proposed for this type of tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Li et al. <ref type="bibr" target="#b10">[11]</ref> proposed an image projection network (IPN) that reduces the features to the target dimensionality using unidirectional pooling layers in the encoder. However, IPN follows a patch-based approach with fixed patch size, which prevents its direct application to full 3D volumes of varying size. Also, it does not have skip connections, which have proven to be highly useful for accurate segmentation. Later, Lachinov et al. <ref type="bibr" target="#b8">[9]</ref> proposed a U-Net-like convolutional neural network (CNN) for 3D→2D segmentation that overcomes the limitations of IPN, which were also later overcome by the second version of IPN (IPNv2) <ref type="bibr" target="#b11">[12]</ref>. However, they still require a large amount of labeled data to provide adequate performance. In addition, there are works that explore the use of CNNs for 3D→2D regression, where Seeböck et al. <ref type="bibr" target="#b19">[20]</ref> proposed ReSensNet, a novel CNN based on Residual 3D U-Net <ref type="bibr" target="#b9">[10]</ref>, with a 3D encoder and a 2D decoder connected by 3D→2D blocks. However, ReSensNet only works at concrete input resolutions, and it is applied pixel-wise.</p><p>In general, one of the issues of these and other deep learning segmentation methods is that their performance strongly depends on the amount of annotated data <ref type="bibr" target="#b21">[22]</ref>, which hinders their deployment to real-world medical image analysis settings. Transfer learning from ImageNet is the standard approach to mitigate this issue <ref type="bibr" target="#b21">[22]</ref>. However, specifically for segmentation, ImageNet pre-training has shown minimal performance gains <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>, partially because it can only be performed on the encoder part of the very common encoder-decoder architectures.</p><p>A possible alternative is to pre-train the models using a self-supervised learning (SSL) paradigm <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref>. However, only some of these approaches have the potential to be applied for 3D→2D segmentation, as many of them, such as image denoising <ref type="bibr" target="#b1">[2]</ref>, require input and output images to have the same dimensionality. Among the suitable approaches, multi-modal reconstruction pretraining (MMRP) shows great potential in multi-modal scenarios <ref type="bibr" target="#b6">[7]</ref>. In this approach, models are trained to reconstruct pairs of images from different modalities, learning relevant patterns in the data without requiring manual annotations. MMRP, however, has only been proven useful for localizing non-pathological structures on 2D color fundus photography, using fluorescein angiography as the modality to reconstruct. Moreover, image pairs of these modalities have to be registered using a separate method.</p><p>Contributions. In this work, we propose a novel approach for label-efficient 3D→2D segmentation. In particular, our contributions are as follows: (1) As an alternative to state-of-the-art network architectures, we propose a 3D→2D segmentation CNN based on ReSensNet <ref type="bibr" target="#b19">[20]</ref> that has a 3D encoder and a 2D decoder connected by novel 3D→2D projective blocks. <ref type="bibr" target="#b1">(2)</ref> We propose a novel SSL strategy for 3D→2D models based on the reconstruction of modalities of different dimensionality, and show that it significantly improves the performance of the models in the target segmentation tasks. This is the first data efficient method proposed for 3D→2D models and the first work exploring 3D→2D reconstruction. (3) Lastly, the performed experiments deepen the understanding of the proposed SSL paradigm, by exploring different settings with different image modalities. The proposed approach was validated on two clinically-relevant tasks: the en-face segmentation of GA and reticular pseudodrusen (RPD) in retinal OCT. The results demonstrate that the proposed approach clearly outperforms the state of the art in scenarios with scarce labeled data. Our code is publicly available on GitHub<ref type="foot" target="#foot_0">1</ref> .</p><p>Clinical Background. Geographic atrophy (GA) is an advanced form of agerelated macular degeneration (AMD) that corresponds to a progressive loss of retinal photoreceptors and leads to irreversible visual impairment. GA is typically assessed with OCT and/or fundus autofluorescence (FAF) imaging modalities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>. In OCT, it is characterized by the loss of retinal pigment epithelium (RPE) tissue, accompanied by the contrast enhancement of the signal below the retina, and in FAF, by the loss of RPE autofluorescence <ref type="bibr" target="#b18">[19]</ref> (see Fig. <ref type="figure" target="#fig_0">1</ref>). In both cases, GA lesion is delineated as a 2D en-face area. Also, GA frequently appears brighter than the surrounding areas on scanning laser ophthalmoscopy (SLO) images due to its higher reflectance.</p><p>Reticular pseudodrusen (RPD) are accumulations of extracellular material that commonly occur in association with AMD. In OCT scans, these lesions are shown as granular hyperreflective deposits situated between the RPE layer and the ellipsoid zone. SLO visualizes RPD as a reticular pattern of iso-reflective round lesions surrounded by a hyporeflective border (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods and Experimental Setup</head><p>The proposed approach, illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, is as follows. Let x ∈ X ⊂ R n and y ∈ Y ⊂ R n-1 be two images from modalities X and Y, and z ∈ Z ⊂ R n-1 , their corresponding target segmentation mask. Let images and masks from Y and Z have related anatomical features. We optimize a reconstruction model y = f r (x; θ r ) and transfer its knowledge by initializing the weights of the segmentation model z = f s (x; θ s ) with the optimized weights of the reconstruction model f r . With this approach, modality Y serves as a free source of supervision, and images of this modality serve as soft segmentation targets. Thus, models can learn relevant patterns in a self-supervised way. In this work, we propose a new CNN for the special case of 3D→2D segmentation, and we evaluate the proposed SSL approach for this case. In particular, we pre-train the new CNN to reconstruct SLO/FAF images from OCT (3D→2D), and then fine-tune it for GA/RPD segmentation. The advantage of reconstructing SLO over FAF is that several modern OCT devices allow to obtain co-registered OCT and SLO scans, providing the coordinates of each OCT slice within the SLO; thus, there is no need to use a separate registration method.</p><p>Network Architecture. The proposed network architecture (Fig. <ref type="figure" target="#fig_2">3</ref>) is based on ReSensNet <ref type="bibr" target="#b19">[20]</ref>, and consists of a 3D encoder and a 2D decoder connected by novel 3D→2D feature projection blocks (FPBs). In the original work <ref type="bibr" target="#b19">[20]</ref>, training and inference are performed pixel-wise using fixed-size input patches. In contrast, we use full-size volumes of arbitrary resolution. To this end, we propose a novel type of FPB. In particular, all convolutions whose kernel size was equal to the expected feature size (calculated from the fixed size of the input patch) were replaced by 1×1×4 convolutions. Then, to project 3D features at the output of each FPB to the 2D feature space, we add an adaptive average pooling of size 1 in the depth dimension at the end of each block. With this setting, feature selection and dimension reduction are performed at different scales, and the decoder processes only 2D features in the selected dimensions. This allows the model to learn the 3D structure of the data while being able to perform the segmentation in 2D. In addition, to overcome memory constraints and avoid overfitting, we reduce by half the number of kernels in each convolutional block.  Training Losses. As reconstruction loss, we use negative mean structural similarity index (NMSSIM) <ref type="bibr" target="#b22">[23]</ref>. We empirically found that this loss performs equally or better than modern perceptual losses (e.g. LPIPS <ref type="bibr" target="#b24">[25]</ref>) for our approach. NMSSIM loss can be defined as L NMSSIM (x, y) = -1 HW h,w SSIM(x hw , y hw ), where x hw and y hw denote image patches of images x and y centered on the pixel with coordinates (h, w), h ∈ H and w ∈ W , and SSIM(x hw , y hw ) is the SSIM map for those patches, as described in <ref type="bibr" target="#b22">[23]</ref>. As segmentation loss, we use the direct sum of Dice loss and Binary Cross-Entropy. These two losses are standard for binary segmentation tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Datasets. Experiments were performed using three datasets (Table <ref type="table" target="#tab_0">1</ref>). GA-M samples come from a clinical study on GA progression. OCT and SLO images were automatically co-registered by the imaging device, while FAF images were registered with SLO using an in-house pipeline based on aligning retinal vessel segmentation. FAF and SLO images were cropped and resized to the same area and resolution as the OCT en-face projection. GA-M-S (35 samples) is a subset of GA-M with GA en-face masks annotated by a retinal expert on the OCT B-Scans. GA-S is composed of OCT volumes from another study with en-face GA annotations created by a retinal expert on the OCT B-scans. This dataset is divided patient-wise into two subsets: GA-S-2, containing volumes with annotations of two different experts, and GA-S-1, of only one. RPD-S is composed of OCT volumes with en-face RPD annotations created by retinal experts.</p><p>Training and Evaluation Details. OCT volumes were flattened along the Bruch's membrane, rescaled depth-wise to 128 voxels, and then Z-score normalized along the cross-sectional plane. To make FAF and GA masks more similar and thus facilitate fine-tuning, FAF images were inverted. In all cases, models were trained for 800 epochs using SGD with a learning rate of 0.1 and a momentum of 0.9. Batch size was set to 4 for reconstruction and 8 for segmentation.</p><p>All datasets were split patient-wise into training (60%), validation (10%) and test (30%). For reconstruction, models were trained on GA-M. For GA segmentation, they were trained/fine-tuned on GA-S-1 and evaluated on GA-S-1, GA-S-2 and GA-M-S. For RPD segmentation, RPD-S was used. To evaluate the performance under label scarcity, we train with 5%, 10%, 20% and 100% of the data in GA-S, and 20% and 100%, in RPD-S. More details about the hardware used and the carbon footprint of our method are included in the Supplement.</p><p>To reduce inference variability, we average the predictions of the top-5 checkpoints of the models in terms of Dice (validation). Segmentations are evaluated via Dice and absolute area difference (Area diff.) of predicted and manual masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>Baseline Comparison. We compared our approach to current state-of-the-art methods (IPN <ref type="bibr" target="#b10">[11]</ref>, IPNv2 <ref type="bibr" target="#b11">[12]</ref>, Lachinov et al. <ref type="bibr" target="#b8">[9]</ref>, and ReSensNet <ref type="bibr" target="#b19">[20]</ref>), showing that we greatly improve the state of the art in GA segmentation in scenarios with limited labeled data (Fig. <ref type="figure" target="#fig_3">4</ref>). When using only 5% of the data (40 samples), the mean Dice score was 23% higher than the best state-of-the-art approach. Even without SSL, the proposed CNN improves the Dice score by 8%. This gain is even greater in terms of Area diff. The improvement is also visible in the predicted segmentation masks (Fig. <ref type="figure" target="#fig_4">5</ref>). When using our approach, the number of false positives and negatives is highly reduced. On the other hand, the improvement for RPD segmentation is more modest (in this case, we only compared with the current state-of-the-art-method: Lachinov et al. <ref type="bibr" target="#b8">[9]</ref>). This can be explained by the greater visibility of GA features compared to RPD features in FAF and SLO (see Figs. <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_4">5</ref>). This suggests that SSL benefits from images with similar pathomorphological manifestations. A table with all means and standard deviations, as well as the results of a Wilcoxon signed rank test between our proposal and the others is included in the Supplement. SSL Effect. To further assess the effect of the SSL, we also applied the strategy to the CNN by Lachinov et. al <ref type="bibr" target="#b8">[9]</ref>. Figure <ref type="figure" target="#fig_3">4</ref> shows that SSL clearly improves the GA and RPD segmentation performance of both proposed and Lachinov et al. methods. These results are in line with the qualitative results in Fig. <ref type="figure" target="#fig_4">5</ref>. This demonstrates that the SSL strategy is beneficial regardless of the architecture and the data. Notwithstanding, as discussed in the baseline comparison, the proposed SSL is more beneficial for GA segmentation than for RPD.</p><p>Reconstructed Modality Effect. We also conducted experiments to assess the effect of the reconstructed modality (SLO and FAF). Figure <ref type="figure" target="#fig_3">4</ref> shows that using FAF for SSL usually leads to better segmentation performance than using SLO. However, in multiple cases, the differences were not statistically significant. This is important because, unlike FAF, SLO does not require an external registration method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Labeled data scarcity is one of the main limiting factors for the application of deep learning in medical imaging. In this work, we have proposed a new model and SSL strategy for label-efficient 3D→2D segmentation. The proposed approach was validated in two tasks with clinical relevance: the en-face segmentation of GA and RPD in OCT. The results demonstrate that: (1) the proposed CNN architecture clearly outperforms the state of the art when there is limited annotated data, (2) regardless of the architecture and the modality to be reconstructed, the proposed SSL strategy improves the performance of the models on the target tasks in those cases; (3) despite the greater diagnostic utility of FAF over SLO, SSL with FAF does not always result in a significant gain in model performance, with the advantage of the latter not requiring a supplementary registration method. On the other hand, although the proposed approach shows promising results in the en-face segmentation of RPD, further evaluation is needed.</p><p>Based on our findings, we believe that the proposed approach has the potential to be used in other common 3D→2D tasks, such as the prediction of retinal sensitivity in OCT, the segmentation of different structures in OCT-A, or the segmentation of intravascular ultrasound (IVUS). In addition, we also believe that the proposed SSL strategy could be easily extended to other imaging domains, such as magnetic resonance, where multi-modal data is widely used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. From left to right: OCT slice (B-scan) with the corresponding ground truth annotations overlaid in green, ground truth, SLO with the location of the B-scan indicated in yellow and a zoom-in view in red, and FAF. Top: GA. Bottom: RPD. (Color figure online)</figDesc><graphic coords="3,57,96,53,87,337,00,134,83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the proposed approach for 3D→2D segmentation. A novel 3D→2D model is trained for reconstructing image pairs of modalities with different dimensionality in a SSL setting, and then fine-tuned in the target segmentation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Proposed 3D→2D CNN. Each residual encoder block has 8 3D convolutional layers, and each residual decoder block has 4 2D layers (number of feature maps also shown). The proposed feature projection block (FPB, in red) projects 3D features to the 2D feature space. FPBs have a variable number of 1 × 1 × 3 convolutions followed by a 1 × 1 × 4 convolution and a depth-wise adaptive average pooling of size 1. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Segmentation results of the models trained with different amounts of data. The title of each plot indicates the test dataset. If a model was pre-trained with SSL, the pre-training modality is shown in parentheses.A table with all means and standard deviations, as well as the results of a Wilcoxon signed rank test between our proposal and the others is included in the Supplement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of GA (top) and RPD (bottom) segmentations from different models using the 5% and the 20% of the training data, respectively. True positives are depicted in green; true negatives, in black; false positives, in red; and false negatives, in blue. (Color figure online)</figDesc><graphic coords="8,44,31,54,29,335,83,109,90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Study data details. Dimensions: en-face height × en-face width × depth. FAF and SLO characteristics are after cropping to the same OCT en-face region projection.</figDesc><table><row><cell cols="5">Dataset Scans Patients (eyes) Modality Device</cell><cell>Area (mm)</cell><cell>Size (px)</cell></row><row><cell cols="2">GA-M 967</cell><cell>100 (184)</cell><cell>OCT</cell><cell cols="2">Spectralis 6.68 × 6.68 × 1.92 49 × 1024 × 496</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SLO</cell><cell cols="2">Spectralis 6.68 × 6.68</cell><cell>1024 × 1024</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FAF</cell><cell cols="2">Spectralis 6.68 × 6.68</cell><cell>1024 × 1024</cell></row><row><cell>GA-S</cell><cell>270</cell><cell>149 (166)</cell><cell>OCT</cell><cell cols="2">Spectralis 6.02 × 6.03 × 1.92 49 × 512 × 496</cell></row><row><cell cols="2">RPD-S 23</cell><cell>19 (23)</cell><cell>OCT</cell><cell cols="2">Spectralis 5.73 × 5.72 × 1.92 97 × 1024 × 496</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/j-morano/multimodal-ssl-fpn.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by the <rs type="funder">Christian Doppler Research Association</rs>, <rs type="funder">Austrian Federal Ministry for Digital and Economic Affairs</rs>, the <rs type="funder">National Foundation for Research, Technology and Development, and Heidelberg Engineering</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_56.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised pretraining for 2D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalapos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gyires-Tóth</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25082-8_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25082-8_31" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13807</biblScope>
			<biblScope unit="page" from="472" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Denoising pretraining for semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Brempong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4175" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fundus autofluorescence and optical coherence tomography biomarkers associated with the progression of geographic atrophy secondary to age-related macular degeneration</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T A</forename><surname>Bui</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41433-021-01747-z</idno>
		<ptr target="https://doi.org/10.1038/s41433-021-01747-z" />
	</analytic>
	<monogr>
		<title level="j">Eye</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2013" to="2019" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 2020</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS 2020</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking ImageNet pre-training</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retinal image understanding emerges from self-supervised multimodal reconstruction</title>
		<author>
			<persName><forename type="first">Á</forename><forename type="middle">S</forename><surname>Hervella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rouco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00928-1_37" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning the retinal anatomy from scarce annotated data using self-supervised multimodal reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Hervella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rouco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ortega</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2020.106210</idno>
		<ptr target="https://doi.org/10.1016/j.asoc.2020.106210" />
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">106210</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CHAOS challenge -combined (CT-MR) healthy abdominal organ segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Kavur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Gezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barış</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Groza</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101950</idno>
		<ptr target="https://doi.org/10.1016/j.media.2020.101950" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101950</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Projective skip-connections for segmentation along a subset of dimensions in retinal OCT</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lachinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Goldbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bogunovic</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_41" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="431" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1706.00120</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1706.00120" />
		<title level="m">Superhuman accuracy on the SNEMI3D connectomics challenge</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image projection network: 3D to 2D image segmentation in OCTA images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2020.2992244</idno>
		<ptr target="https://doi.org/10.1109/TMI.2020.2992244" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3343" to="3354" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">OCTA-500: a retinal dataset for optical coherence tomography angiography study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense segmentation in selected dimensions: application to retinal optical coherence tomography</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liefers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>González-Gonzalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Klaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2nd International Conference on Medical Imaging with Deep Learning. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</editor>
		<meeting>The 2nd International Conference on Medical Imaging with Deep Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2014.2377694</idno>
		<ptr target="https://doi.org/10.1109/TMI.2014.2377694" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal transfer learning-based approaches for retinal vascular segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Morano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Álvaro</surname></persName>
		</author>
		<author>
			<persName><surname>Hervella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rouco</surname></persName>
		</author>
		<idno type="DOI">10.3233/FAIA200303</idno>
		<ptr target="https://doi.org/10.3233/FAIA200303" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th European Conference on Artificial Intelligence (ECAI 2020)</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Giacomo</surname></persName>
		</editor>
		<meeting>the 24th European Conference on Artificial Intelligence (ECAI 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1866" to="1873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">REFUGE challenge: a unified framework for evaluating automated methods for glaucoma assessment from fundus photographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barbosa Breda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Keer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Bathula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diaz-Pinto</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.101570</idno>
		<ptr target="https://doi.org/10.1016/j.media.2019.101570" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101570</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transfusion: understanding transfer learning for medical imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting the potential of unlabeled endoscopic video data with self-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ross</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-018-1772-0</idno>
		<ptr target="https://doi.org/10.1007/s11548-018-1772-0" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="925" to="933" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural history of geographic atrophy progression secondary to age-related macular degeneration (geographic atrophy progression study)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schmitz-Valckenberg</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ophtha.2015.09.036</idno>
		<ptr target="https://doi.org/10.1016/j.ophtha.2015.09.036" />
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="368" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linking function and structure with ReSensNet: predicting retinal sensitivity from OCT using deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.oret.2022.01.021</idno>
		<ptr target="https://doi.org/10.1016/j.oret.2022.01.021" />
	</analytic>
	<monogr>
		<title level="j">Ophthalmol. Retina</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="501" to="511" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph-based IVUS segmentation with efficient computer-aided refinement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Beichel</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2013.2260763</idno>
		<ptr target="https://doi.org/10.1109/TMI.2013.2260763" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1536" to="1549" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Embracing imperfect datasets: a review of deep learning solutions for medical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jeyaseelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101693</idno>
		<ptr target="https://doi.org/10.1016/j.media.2020" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101693</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two potentially distinct pathways to geographic atrophy in agerelated macular degeneration characterized by quantitative fundus autofluorescence</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41433-022-02332-8</idno>
		<ptr target="https://doi.org/10.1038/s41433-022-02332-8" />
	</analytic>
	<monogr>
		<title level="j">Eye</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00068</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00068" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
