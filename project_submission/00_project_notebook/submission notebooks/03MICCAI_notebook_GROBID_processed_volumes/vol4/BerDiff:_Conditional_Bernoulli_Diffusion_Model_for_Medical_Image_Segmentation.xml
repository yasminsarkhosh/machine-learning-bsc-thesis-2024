<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation</title>
				<funder ref="#_fpQNatE">
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
				<funder ref="#_7QRygTy">
					<orgName type="full">Natural Science Foundation of Shanghai</orgName>
				</funder>
				<funder>
					<orgName type="full">Shanghai Center for Brain Science and Brain-inspired Technology</orgName>
				</funder>
				<funder ref="#_Xba9kUA">
					<orgName type="full">ZJLab, Shanghai Municipal of Science and Technology Project</orgName>
				</funder>
				<funder ref="#_Y45BW7q">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenhui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hongming</forename><surname>Shan</surname></persName>
							<email>hmshan@fudan.edu.cn</email>
							<idno type="ORCID">0000-0002-0604-3197</idno>
							<affiliation key="aff0">
								<orgName type="department">Institute of Science and Technology for Brain-Inspired Intelligence and MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence (Fudan University)</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Center for Brain Science and Brain-Inspired Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="491" to="501"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">71A8B22EEFFD82E8AC2400C652EC589D</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_47</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Conditional diffusion</term>
					<term>Bernoulli noise</term>
					<term>Medical image segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image segmentation is a challenging task with inherent ambiguity and high uncertainty attributed to factors such as unclear tumor boundaries and multiple plausible annotations. The accuracy and diversity of segmentation masks are both crucial for providing valuable references to radiologists in clinical practice. While existing diffusion models have shown strong capacities in various visual generation tasks, it is still challenging to deal with discrete masks in segmentation. To achieve accurate and diverse medical image segmentation masks, we propose a novel conditional Bernoulli Diff usion model for medical image segmentation (BerDiff). Instead of using the Gaussian noise, we first propose to use the Bernoulli noise as the diffusion kernel to enhance the capacity of the diffusion model for binary segmentation tasks, resulting in more accurate segmentation masks. Second, by leveraging the stochastic nature of the diffusion model, our BerDiff randomly samples the initial Bernoulli noise and intermediate latent variables multiple times to produce a range of diverse segmentation masks, which can highlight salient regions of interest that can serve as a valuable reference for radiologists. In addition, our BerDiff can efficiently sample sub-sequences from the overall trajectory of the reverse diffusion, thereby speeding up the segmentation process. Extensive experimental results on two medical image segmentation datasets with different modalities demonstrate that our BerDiff outperforms other recently published state-of-the-art methods. Source code is made available at https://github.com/takimailto/BerDiff.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation plays a crucial role in enabling better diagnosis, surgical planning, and image-guided surgery <ref type="bibr" target="#b8">[8]</ref>. The inherent ambiguity and high uncertainty of medical images pose significant challenges <ref type="bibr" target="#b4">[5]</ref> for accurate segmentation, attributed to factors such as unclear tumor boundaries in brain Magnetic Resonance Imaging (MRI) images and multiple plausible annotations of lung nodule in Computed Tomography (CT) images. Existing medical image segmentation methods typically provide a single, deterministic, most likely hypothesis mask, which may lead to misdiagnosis or sub-optimal treatment. Therefore, providing accurate and diverse segmentation masks as valuable references <ref type="bibr" target="#b17">[17]</ref> for radiologists is crucial in clinical practice.</p><p>Recently, diffusion models <ref type="bibr" target="#b10">[10]</ref> have shown strong capacities in various visual generation tasks <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>. However, how to better deal with discrete segmentation tasks needs further consideration. Although many researches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">26]</ref> have combined diffusion model with segmentation tasks, all these methods do not take full account of the discrete characteristic of segmentation task and still use Gaussian noise as their diffusion kernel.</p><p>To achieve accurate and diverse segmentation masks, we propose a novel Conditional Bernoulli Diff usion model for medical image segmentation (BerDiff). Instead of using the Gaussian noise, we first propose to use the Bernoulli noise as the diffusion kernel to enhance the capacity of the diffusion model for segmentation, resulting in more accurate segmentation masks. Moreover, by leveraging the stochastic nature of the diffusion model, our BerDiff randomly samples the initial Bernoulli noise and intermediate latent variables multiple times to produce a range of diverse segmentation masks, highlighting salient regions of interest (ROI) that can serve as a valuable reference for radiologists. In addition, our BerDiff can efficiently sample sub-sequences from the overall trajectory of the reverse diffusion based on the rationale behind the Denoising Diffusion Implicit Models (DDIM) <ref type="bibr" target="#b25">[25]</ref>, thereby speeding up the segmentation process.</p><p>The contributions of this work are summarized as follows. 1) Instead of using the Gaussian noise, we propose a novel conditional diffusion model based on the Bernoulli noise for discrete binary segmentation tasks, achieving accurate and diverse medical image segmentation masks. 2) Our BerDiff can efficiently sample sub-sequences from the overall trajectory of the reverse diffusion, thereby speeding up the segmentation process. 3) Experimental results on LIDC-IDRI and BRATS 2021 datasets demonstrate that our BerDiff outperforms other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we first describe the problem definitions, and then demonstrate the Bernoulli forward and diverse reverse processes of our BerDiff, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Finally, we provide an overview of the training and sampling procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>Let us assume that x ∈ R H×W×C denotes the input medical image with a spatial resolution of H × W and C channels. The ground-truth mask is represented as y 0 ∈ {0, 1}</p><p>H×W , where 0 represents background while 1 ROI. Inspired by diffusion-based models such as denoising diffusion probabilistic model (DDPM) and DDIM, we propose a novel conditional Bernoulli diffusion model, which can be represented as p θ (y 0 |x) := p θ (y 0:T |x)dy 1:T , where y 1 , . . . , y T are latent variables of the same size as the mask y 0 . For medical binary segmentation tasks, the diverse reverse process of our BerDiff starts from the initial Bernoulli noise y T ∼ B(y T ; 1 2 •1) and progresses through intermediate latent variables constrained by the input medical image x to produce segmentation masks, where 1 denotes an all-ones matrix of the size H ×W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bernoulli Forward Process</head><p>In previous generation-related diffusion models, Gaussian noise is progressively added with increasing timestep t. However, for segmentation tasks, the groundtruth masks are represented by discrete values. To address this, our BerDiff gradually adds more Bernoulli noise using a noise schedule β 1 , . . . , β T , as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The Bernoulli forward process q(y 1:T |y 0 ) of our BerDiff is a Markov chain, which can be represented as:</p><formula xml:id="formula_0">q (y 1:T | y 0 ) := T t=1 q (y t | y t-1 ) , (<label>1</label></formula><formula xml:id="formula_1">)</formula><formula xml:id="formula_2">q (y t | y t-1 ) :=B(y t ; (1 -β t )y t-1 + β t /2), (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where B denotes the Bernoulli distribution with the probability parameters (1 -</p><formula xml:id="formula_4">β t )y t-1 + β t /2.</formula><p>Using the notation α t = 1β t and ᾱt = t τ =1 α τ , we can efficiently sample y t at an arbitrary timestep t in closed form:</p><formula xml:id="formula_5">q (y t | y 0 ) = B(y t ; ᾱt y 0 + (1 -ᾱt )/2)).</formula><p>(</p><formula xml:id="formula_6">) Algorithm 1. Training repeat (x, y0) ∼ q (x, y0) t ∼ Uniform({1, . . . , T }) ∼ B( ; (1 -ᾱt)/2) yt = y0 ⊕ Calculate Eq. (4) Estimate ˆ (yt, t, x) Calculate Eq. (6) Take gradient descent on ∇ θ (L Total ) until converged Algorithm 2. Sampling yT ∼ B(yT ; 1 2 • 1) for t = T to 1 do μ(yt, t, x) = FC (yt, ˆ (yt, t, x)) For DDPM: yt-1 ∼ B(yt-1; μ(yt, t, x)) For DDIM: yt-1 ∼ B(yt-1; σtyt+(ᾱt-1-σt ᾱt)|yt- ˆ (yt, t, x)| + ((1 -ᾱt-1) -(1 -ᾱt)σt)/2) end for return y0<label>3</label></formula><p>To ensure that the objective function described in Sect. 2.4 is tractable and easy to compute, we use the sampled Bernoulli noise ∼ B( ; 1-ᾱt 2 •1) to reparameterize y t of Eq. ( <ref type="formula" target="#formula_6">3</ref>) as y 0 ⊕ , where ⊕ denotes the logical operation of "exclusive or (XOR)". Additionally, let denote elementwise product, and Norm(•) denote normalizing the input data along the channel dimension and then returning the second channel. The concrete Bernoulli posterior can be represented as:</p><formula xml:id="formula_7">q(y t-1 | y t , y 0 ) = B(y t-1 ; θ post (y t , y 0 )),<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">θ post (y t , y 0 ) = Norm([α t [1 -y t , y t ] + 1-αt 2 ] [ᾱ t-1 [1 -y 0 , y 0 ] + 1-ᾱt-1<label>2</label></formula><p>]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Diverse Reverse Process</head><p>The diverse reverse process p θ (y 0:T ) can also be viewed as a Markov chain that starts from the Bernoulli noise y T ∼ B(y T ; 1 2 • 1) and progresses through intermediate latent variables constrained by the input medical image x to produce diverse segmentation masks, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The concrete diverse reverse process of our BerDiff can be represented as:</p><formula xml:id="formula_9">p θ (y 0:T | x) := p(y T ) T t=1 p θ (y t-1 | y t , x), (<label>5</label></formula><formula xml:id="formula_10">)</formula><formula xml:id="formula_11">p θ (y t-1 | y t , x) := B(y t-1 ; μ(y t , t, x)). (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>Specifically, we utilize the estimated Bernoulli noise ˆ (y t , t, x) of y t to parameterize μ(y t , t, x) via a calibration function F C , as follows: <ref type="bibr" target="#b7">(7)</ref> where | • | denotes the absolute value operation. The calibration function aims to calibrate the latent variable y t to a less noisy latent variable y t-1 in two steps: 1) estimating the segmentation mask y 0 by computing the absolute deviation between y t and the estimated noise ˆ ; and 2) estimating the distribution of y t-1 by calculating the Bernoulli posterior, p(y t-1 |y t , y 0 ), using Eq. (4).</p><formula xml:id="formula_13">μ(y t , t, x) = F C (y t , ˆ (y t , t, x)) = θ post (y t , |y t -ˆ (y t , t, x)|),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Detailed Procedure</head><p>Here, we provide an overview of the training and sampling procedure in Algorithms 1 and 2. During the training phase, given an image and mask data pair {x, y 0 }, we sample a random timestep t from a uniform distribution {1, . . . , T }, which is used to sample the Bernoulli noise .</p><p>We then use to sample y t from q(y t | y 0 ), which allows us to obtain the Bernoulli posterior q(y t-1 | y t , y 0 ). We pass the estimated Bernoulli noise ˆ (y t , t, x) through the calibration function F C to parameterize p θ (y t-1 | y t , x). Based on the variational upper bound on the negative log-likelihood in previous diffusion models <ref type="bibr" target="#b2">[3]</ref>, we adopt Kullback-Leibler (KL) divergence and binary cross-entropy (BCE) loss to optimize our BerDiff as follows:</p><formula xml:id="formula_14">L KL = E q(x,y0) E q(yt|y0) [D KL [q(y t-1 | y t , y 0 ) p θ (y t-1 | y t , x)]], (<label>8</label></formula><formula xml:id="formula_15">)</formula><formula xml:id="formula_16">L BCE = -E ( ,ˆ ) H,W i,j [ i,j log ˆ i,j + (1 -i,j ) log (1 -ˆ i,j )].<label>(9)</label></formula><p>Finally, the overall objective function is presented as:</p><formula xml:id="formula_17">L Total = L KL + λ BCE L BCE , where λ BCE is set to 1 in our experiments.</formula><p>During the sampling phase, our BerDiff first samples the initial latent variable y T , followed by iterative calculation of the probability parameters of y t-1 for different t. In Algorithm 2, we present two different sampling strategies from DDPM and DDIM for the latent variable y t-1 . Finally, our BerDiff is capable of producing diverse segmentation masks. By taking the mean of these masks, we can further obtain a saliency segmentation mask to highlight salient ROI that can serve as a valuable reference for radiologists. Note that our BerDiff has a novel parameterization technique, i.e. calibration function, to estimate the Bernoulli noise of y t , which is different from previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b24">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Dataset and Preprocessing. The data used in this experiment are obtained from LIDC-IDRI <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">7]</ref> and BRATS 2021 <ref type="bibr" target="#b3">[4]</ref> datasets. LIDC-IDRI contains 1,018 lung CT scans with plausible segmentation masks annotated by four radiologists. We adopt a standard preprocessing pipeline for lung CT scans and the trainvalidation-test partition as in previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b23">23]</ref>. BRATS 2021 consists of four different sequence (T1, T2, FlAIR, T1CE) MRI images for each patient. All 3D scans are sliced into axial slices and discarded the bottom 80 and top 26 slices. Note that we treat the original four types of brain tumors as one type following previous work <ref type="bibr" target="#b25">[25]</ref>, converting the multi-target segmentation problem into binary. Our training set includes 55,174 2D images scanned from 1,126 patients, and the test set comprises 3,991 2D images scanned from 125 patients. Finally, the sizes of images from LIDC-IDRI and BRAST 2021 are resized to a resolution of 128 × 128 and 224 × 224, respectively. Implementation Details. We implement all the methods with the PyTorch library and train the models on NVIDIA V100 GPUs. All the networks are trained using the AdamW <ref type="bibr" target="#b19">[19]</ref> optimizer with a mini-batch size of 32. The initial learning rate is set to 1 × 10 -4 for BRATS 2021 and 5 × 10 -5 for LIDC-IDRI. The Bernoulli noise estimation U-net network in Fig. <ref type="figure" target="#fig_0">1</ref> of our BerDiff is the same as previous diffusion-based models <ref type="bibr" target="#b20">[20]</ref>. We employ a linear noise schedule for T = 1000 timesteps for all the diffusion models. And we use the sub-sequence sampling strategy of DDIM to accelerate the segmentation process. During minibatch training of LIDC-IDRI, our BerDiff learns diverse expertise by randomly sampling one from four annotated segmentation masks for each image. Four metrics are used for performance evaluation, including Generalized Energy Distance (GED), Hungarian-Matched Intersection over Union (HM-IoU), Soft-Dice and Dice coefficient. We compute GED using varying numbers of segmentation samples <ref type="bibr">(1, 4, 8, and 16)</ref>, HM-IoU and Soft-Dice using 16 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>We start by conducting ablation experiments to demonstrate the effectiveness of different losses and estimation targets, as shown in Table <ref type="table" target="#tab_0">1</ref>. All experiments are trained for 21,000 training iterations on LIDC-IDRI. We first explore the selection of losses in the top three rows. We find that the combination of KL divergence and BCE loss can achieve the best performance. Then, we explore the selection of estimation targets in the bottom two rows. We observe that estimating Bernoulli noise, instead of directly estimating the ground-truth mask, is The U-net has the same architecture as the noise estimation network in our BerDiff and previous diffusion-based models. Fig. <ref type="figure">2</ref>. Diverse segmentation masks and the corresponding saliency mask of two lung nodules randomly selected in LIDC-IDRI. y i 0 and y i gt refer to the i-th generated and ground-truth segmentation masks, respectively. Saliency Mask is the mean of diverse segmentation masks. more suitable for our binary segmentation task. All of these findings are consistent with previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">10]</ref>.</p><p>Here, we conduct ablation experiments on our BerDiff with Gaussian or Bernoulli noise, and the results are shown in Table <ref type="table" target="#tab_1">2</ref>. For discrete segmentation tasks, we find that using Bernoulli noise can produce favorable results when training iterations are limited (e.g. 21,000 iterations) and even outperform using Gaussian noise when training iterations are sufficient (e.g. 86,500 iterations). We also provide a more detailed performance comparison between Bernoulliand Gaussian-based diffusion models over training iterations in Fig. <ref type="figure" target="#fig_2">S3</ref>.   <ref type="table" target="#tab_2">3</ref>, and find that our BerDiff performs well for discrete segmentation tasks. Probabilistic U-net (Prob.U-net), Hierarchical Prob.U-net (Hprob.U-net), and Joint Prob.U-net (JPro.U-net) use conditional variational autoencoder (cVAE) to accomplish segmentation tasks. Calibrated Adversarial Refinement (CAR) employs generative adversarial networks (GAN) to refine segmentation. PixelSeg is based on autoregressive models, while SegDiff and MedSegDiff are diffusion-based models. There are also methods that attempt to model multi-annotators explicitly <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b27">27]</ref>. We have the following three observations: 1) diffusion-based methods demonstrate significant superiority over traditional approaches based on VAE, GAN, and autoregression models for discrete segmentation tasks; 2) our BerDiff outperforms other diffusion-based models that use Gaussian noise as the diffusion kernel; and 3) our BerDiff also outperforms the methods that explicitly model the annotator, striking a good balance between diversity and accuracy. At the same time, we present comparison segmentation results in Fig. <ref type="figure">2</ref>. Compared to other models, our BerDiff can effectively learn diverse expertise, resulting in more diverse and accurate segmentation masks. Especially for small nodules that can create ambiguity, such as the lung nodule on the left, our BerDiff approach produces segmentation masks that are more in line with the ground-truth masks.</p><p>Results on BRATS 2021. Here, we present the quantitative and qualitative results of BRATS 2021 in Table <ref type="table" target="#tab_3">4</ref> and Fig. <ref type="figure" target="#fig_1">3</ref>, respectively. We conducted a comparative analysis of our BerDiff with other models such as nnUnet, transformer-based models like TransU-net and Swin UNETR, as well as diffusion-based methods like SegDiff. First, we find that diffusion-based methods show superior performance compared to traditional U-net and transformer-based approaches. Besides, the high performance achieved by U-net, which shares the same architecture as our noise estimation network, highlights the effectiveness of the backbone design in diffusion-based models. Moreover, our proposed BerDiff surpasses other diffusion-based models that use Gaussian noise as the diffusion kernel. Finally, from Fig. <ref type="figure" target="#fig_1">3</ref>, we find that our BerDiff segments more accurately on parts that are difficult to recognize by the human eye, such as the tumor in the 3rd row. At the same time, we can also generate diverse plausible segmentation masks to produce a saliency segmentation mask. We note that some of these masks may be false positives, as shown in the 1st row, but they can be filtered out due to low saliency. Please refer to Figs. <ref type="figure" target="#fig_0">S1</ref> and<ref type="figure">S2</ref> for more examples of diverse segmentation masks generated by our BerDiff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed to use the Bernoulli noise as the diffusion kernel to enhance the capacity of the diffusion model for binary segmentation tasks, achieving accurate and diverse medical image segmentation results. Our BerDiff only focuses on binary segmentation tasks and takes much time during the iterative sampling process as other diffusion-based models; e.g. our BerDiff takes 0.4 s to segment one medical image, which is ten times of traditional U-net. In the future, we will extend our BerDiff to the multi-target segmentation problem and implement additional strategies for speeding up the segmentation process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of Bernoulli forward and diverse reverse processes of our BerDiff.</figDesc><graphic coords="3,71,46,64,31,309,37,145,93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmentation masks of four MRI images randomly selected in BRATS 2021. The segmentation masks of diffusion-based models (SegDiff and ours) presented here are saliency segmentation mask.</figDesc><graphic coords="8,83,46,66,86,257,23,128,77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3. 3</head><label>3</label><figDesc>Comparison to Other State-of-the-Art Methods Results on LIDC-IDRI. Here, we present the quantitative comparison results of LIDC-IDRI in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation results of hyperparameters on LIDC-IDRI.</figDesc><table><row><cell>Loss</cell><cell cols="2">Estimation Target GED</cell><cell></cell><cell></cell><cell>HM-IoU</cell></row><row><cell></cell><cell></cell><cell>16</cell><cell>8</cell><cell>4</cell><cell>1</cell><cell>16</cell></row><row><cell>LKL</cell><cell>Bernoulli noise</cell><cell cols="4">0.332 0.365 0.430 0.825 0.517</cell></row><row><cell>LBCE</cell><cell>Bernoulli noise</cell><cell cols="4">0.251 0.287 0.359 0.785 0.566</cell></row><row><cell cols="2">LBCE + LKL Bernoulli noise</cell><cell cols="4">0.249 0.287 0.358 0.775 0.575</cell></row><row><cell cols="6">LBCE + LKL Ground-truth mask 0.277 0.317 0.396 0.866 0.509</cell></row><row><cell cols="6">The best and second best results are highlighted in bold and underlined,</cell></row><row><cell>respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation results of diffusion kernel on LIDC-IDRI.</figDesc><table><row><cell cols="3">Training Diffusion GED</cell><cell></cell><cell></cell><cell></cell><cell>HM-IoU</cell></row><row><cell cols="2">Iteration Kernel</cell><cell>16</cell><cell>8</cell><cell>4</cell><cell>1</cell><cell>16</cell></row><row><cell>21,000</cell><cell cols="6">Gaussian 0.671 0.732 0.852 1.573 0.020</cell></row><row><cell></cell><cell cols="6">Bernoulli 0.252 0.287 0.358 0.775 0.575</cell></row><row><cell>86,500</cell><cell cols="6">Gaussian 0.251 0.282 0.345 0.719 0.587</cell></row><row><cell></cell><cell cols="6">Bernoulli 0.238 0.271 0.340 0.748 0.596</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on LIDC-IDRI.</figDesc><table><row><cell>Methods</cell><cell>GED 16</cell><cell>HM-IoU 16</cell><cell>Soft-Dice 16</cell></row><row><cell cols="4">Prob.U-net [15] 0.320 ± 0.03 0.500 ± 0.03 -</cell></row><row><cell cols="4">Hprob.U-net [16] 0.270 ± 0.01 0.530 ± 0.01 0.624 ± 0.01</cell></row><row><cell>CAR [14]</cell><cell cols="3">0.264 ± 0.00 0.592 ± 0.01 0.633 ± 0.00</cell></row><row><cell>JPro.U-net [29]</cell><cell cols="3">0.260 ± 0.00 0.585 ± 0.00 -</cell></row><row><cell>PixelSeg [28]</cell><cell cols="3">0.260 ± 0.00 0.587 ± 0.01 -</cell></row><row><cell>SegDiff [1]</cell><cell cols="3">0.248 ± 0.01 0.585 ± 0.00 0.637 ± 0.01</cell></row><row><cell cols="4">MedSegDiff [26] 0.420 ± 0.03 0.413 ± 0.03 0.453 ± 0.02</cell></row><row><cell cols="2">Zhang et al. [27] 0.400</cell><cell>0.534</cell><cell>0.599</cell></row><row><cell>Ji et al. [13]</cell><cell>0.658</cell><cell>0.447</cell><cell>0.616</cell></row><row><cell>Liao et al. [18]</cell><cell>0.593</cell><cell>0.453</cell><cell>0.587</cell></row><row><cell>BerDiff (Ours)</cell><cell cols="3">0.238 ± 0.01 0.596 ± 0.00 0.644 ± 0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results on BRATS 2021.</figDesc><table><row><cell>Methods</cell><cell>Dice</cell></row><row><cell>nnU-net [12]</cell><cell>88.2</cell></row><row><cell>TransU-net [6]</cell><cell>88.6</cell></row><row><cell cols="2">Swin UNETR [9] 89.0</cell></row><row><cell>U-net</cell><cell>89.2</cell></row><row><cell>SegDiff [1]</cell><cell>89.3</cell></row><row><cell>BerDiff (Ours)</cell><cell>89.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by <rs type="funder">Natural Science Foundation of Shanghai</rs> (No. <rs type="grantNumber">21ZR1403600</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62101136</rs>), <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs> (No. <rs type="grantNumber">2018SHZDZX01</rs>) and <rs type="funder">ZJLab, Shanghai Municipal of Science and Technology Project</rs> (No. <rs type="grantNumber">20JC1419500</rs>), and <rs type="funder">Shanghai Center for Brain Science and Brain-inspired Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7QRygTy">
					<idno type="grant-number">21ZR1403600</idno>
				</org>
				<org type="funding" xml:id="_Y45BW7q">
					<idno type="grant-number">62101136</idno>
				</org>
				<org type="funding" xml:id="_fpQNatE">
					<idno type="grant-number">2018SHZDZX01</idno>
				</org>
				<org type="funding" xml:id="_Xba9kUA">
					<idno type="grant-number">20JC1419500</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_47.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">SegDiff: image segmentation with diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaharbany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00390</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="915" to="931" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17981" to="17993" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The RSNA-ASNR-MICCAI BraTS 2021 benchmark on brain tumor segmentation and radiogenomic classification</title>
		<author>
			<persName><forename type="first">U</forename><surname>Baid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PHiSeg: capturing uncertainty in medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="119" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_14" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transunet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cancer imaging archive (TCIA): maintaining and operating a public information repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning approaches to biomedical image segmentation</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R I</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Med. Unlocked</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">100297</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Swin UNETR: swin transformers for semantic segmentation of brain tumors in MRI images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08999-2_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08999-2_22" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12962</biblScope>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Argmax flows and multinomial diffusion: Learning categorical distributions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12454" to="12465" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning calibrated medical image segmentation via multi-rater agreement modeling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12336" to="12346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Calibrated adversarial refinement for stochastic semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kassapis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nugteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7037" to="7047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A probabilistic U-Net for segmentation of ambiguous images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13077</idno>
		<title level="m">A hierarchical probabilistic U-Net for modeling multi-scale ambiguities</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated segmentation of tissues using CT and MRI: a systematic review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lenchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acad. Radiol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1695" to="1706" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Modeling annotator preference and stochastic annotation error for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13410</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hierarchical textconditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Photorealistic text-to-image diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36479" to="36494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uncertainty quantification in medical image segmentation with normalizing flows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Selvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Middleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59861-7_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59861-7_9" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2020</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12436</biblScope>
			<biblScope unit="page" from="80" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffusion models for implicit image segmentation ensembles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valmaggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1336" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.00611</idno>
		<title level="m">MedSegDiff: medical image segmentation with diffusion probabilistic model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Disentangling human error from ground truth in segmentation of medical images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15750" to="15762" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PixelSeg: pixel-by-pixel stochastic semantic segmentation for ambiguous medical images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30-th ACM International Conference on Multimedia</title>
		<meeting>the 30-th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4742" to="4750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A probabilistic model for controlling diversity and accuracy of ambiguous medical image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30-th ACM International Conference on Multimedia</title>
		<meeting>the 30-th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4751" to="4759" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
