<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Malo</forename><surname>Alefsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique de Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Paige, Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Kadoury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique de Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Centre de Recherche du CHUM</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>QC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="141" to="151"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">138486315E26939075B76D9210E6BED8</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Segmentation</term>
					<term>Semi-supervised Learning</term>
					<term>Unpaired Image-to-image Translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated medical image segmentation using deep neural networks typically requires substantial supervised training. However, these models fail to generalize well across different imaging modalities. This shortcoming, amplified by the limited availability of expert annotated data, has been hampering the deployment of such methods at a larger scale across modalities. To address these issues, we propose M-GenSeg, a new semi-supervised generative training strategy for crossmodality tumor segmentation on unpaired bi-modal datasets. With the addition of known healthy images, an unsupervised objective encourages the model to disentangling tumors from the background, which parallels the segmentation task. Then, by teaching the model to convert images across modalities, we leverage available pixel-level annotations from the source modality to enable segmentation in the unannotated target modality. We evaluated the performance on a brain tumor segmentation dataset composed of four different contrast sequences from the public BraTS 2020 challenge data. We report consistent improvement in Dice scores over state-of-the-art domain-adaptive baselines on the unannotated target modality. Unlike the prior art, M-GenSeg also introduces the ability to train with a partially annotated source modality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning methods have demonstrated their tremendous potential when it comes to medical image segmentation. However, the success of most existing architectures relies on the availability of pixel-level annotations, which are difficult to produce <ref type="bibr" target="#b0">[1]</ref>. Furthermore, these methods are known to be inadequately equipped for distribution shifts. Therefore, cross-modality generalization is needed when one imaging modality has insufficient training data. For instance, conditions such as Vestibular Schwannoma, where new hrT2 sequences are set to replace ceT1 for diagnosis to mitigate the use of contrast agents, is a sample use case <ref type="bibr" target="#b1">[2]</ref>. Recently Billot et al. <ref type="bibr" target="#b2">[3]</ref> proposed a domain randomisation strategy to segment images from a wide range of target contrasts without any fine-tuning. The method demonstrated great generalization capability for brain parcellation, but the model performance when exposed to tumors and pathologies was not quantified. This challenge could also be addressed through unsupervised domain-adaptive approaches, which transfer the knowledge available in the "source" modality S from pixel-level labels to the "target" imaging modality T lacking annotations <ref type="bibr" target="#b3">[4]</ref>.</p><p>Several generative models attempt to generalize to a target modality by performing unsupervised domain adaptation through image-to-image translation and image reconstruction. In <ref type="bibr" target="#b4">[5]</ref>, by learning to translate between CT and MR cardiac images, the proposed method jointly disentangles the domain specific and domain invariant features between each modality and trains a segmenter from the domain invariant features. Other methods <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> also integrate this translation approach, but the segmenter is trained in an end-to-end manner on the synthetic target images generated from the source modality using a Cycle-GAN <ref type="bibr" target="#b12">[13]</ref> model. These methods perform well but do not explicitly use the unannotated target modality data to further improve the segmentation.</p><p>In this paper, we propose M-GenSeg, a novel training strategy for crossmodality domain adaptation, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. This work leverages and extends GenSeg <ref type="bibr" target="#b13">[14]</ref>, a generative method that uses image-level "diseased" or "healthy" labels for semi-supervised segmentation. Given these labels, the model imposes an image-to-image translation objective between the image domain presenting tumor lesions and the domain corresponding to an absence of lesions. Therefore, like in low-rank atlas based methods <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> the model is taught to find and remove a lesion, which acts as a guide for the segmentation. We incorporate cross-modality image segmentation with an image-to-image translation objective between source and target modalities. We hypothesize both objectives are complementary since GenSeg helps localizing the tumors on unannotated target images, while modality translation enables fine-tuning the segmenter on the target modality by displaying annotated pseudo-target images. We evaluate M-GenSeg on a modified version of the BraTS 2020 dataset, in which each type of sequence (T1, T2, T1ce and FLAIR) is considered as a distinct modality. We demonstrate that our model can better generalize than other state-of-the-art methods to the target modality. Healthy-Diseased Translation. We propose to integrate image-level supervision to the cross-modality segmentation task with GenSeg, a model that introduces translation between domains with a presence (P) or absence (A) of tumor lesions. Leveraging this framework has a two-fold advantage here. Indeed, (i) training a GenSeg module on the source modality makes the model aware of the tumor appearances in the source images even with limited source pixel-level annotations. This helps to preserve tumor structures during the generation of pseudo-target samples (see Sect. 2.1). Furthermore, (ii) training a second GenSeg module on the target modality allows to further close the domain gap by extending the segmentation objective to unannotated target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">M-GenSeg: Semi-supervised Segmentation</head><p>In order to disentangle the information common to A and P, and the information specific to P, we split the latent representation of each image into a common code c and a unique code u. Essentially, the common code contains information inherent to both domains, which represents organs and other structures, while the unique code stores features like tumor shapes and location. In the two fol-lowing paragraphs, we explain P→A and A→P translations for source images. The same process is applied for target images by replacing S notation with T .</p><p>Presence to Absence Translation. Given an image S P of modality S in the presence domain P, we use an encoder E S to compute the latent representation [c S P , u S P ]. A common decoder G S com takes as input the common code c S P and generates a healthy version S PA of that image by removing the apparent tumor region. Simultaneously, both common and unique codes are used by a residual decoder G S res to output a residual image Δ S PP , which corresponds to the additive change necessary to shift the generated healthy image back to the presence domain. In other words, the residual is the disentangled tumor that can be added to the generated healthy image to create a reconstruction S PP of the initial diseased image: </p><formula xml:id="formula_0">S PA = G S com (</formula><formula xml:id="formula_1">S AA = G S com (c S A ) and S AP = S AA + G S res (c S A , u ∼ N (0, I))<label>(2)</label></formula><p>Like approaches in <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> we therefore generate diseased samples from healthy ones for data augmentation. However, M-GenSeg aims primarily at tackling cross-modality lesion segmentation tasks, which is not addressed in these studies. Furthermore, note that these methods are limited to data augmentation and do not incorporate any unannotated diseased samples when training the segmentation network, as achieved by our model with the P→A translation.</p><p>Modality Translation. Our objective is to learn to segment tumor lesions in a target modality by reusing potentially scarce image annotations in a source modality. Note that for each modality m ∈ {S, T }, M-GenSeg holds a segmentation decoder G m seg that shares most of its weights with the residual decoder G m res , but has its own set of normalization parameters and a supplementary classifying layer. Thus, through the Absence and Presence translations, these segmenters have already learned how to disentangle the tumor from the background. However, supervised training on a few example annotations is still required to learn how to transform the resulting residual representation into appropriate segmentation maps. While this is a fairly straightforward task for the source modality using pixel-level annotations, achieving this for the target modality is more complex, justifying the second unsupervised translation objective between source and target modalities. Based on the CycleGan <ref type="bibr" target="#b12">[13]</ref> approach, modality translations are performed via two distinct generators that share their encoder with the GenSeg task. More precisely, combined with the encoder E S a decoder G T enables performing S→T modality translation, while the encoder E T and a second decoder G S perform the T→S modality translation. To maintain the anatomical information, we ensure cycle-consistency by reconstructing the initial images after mapping them back to their original modality. We note </p><formula xml:id="formula_2">S T d = G T • E S (S d</formula><formula xml:id="formula_3">T ST d = G T • E S (T S d )</formula><p>for the T→S→T cycle. Note that to perform the domain adaptation, training the model to segment only the pseudo-target images generated by the S→T modality generator would suffice (in addition to the diseased/healthy target translation). However, training the segmentation on diseased source images also imposes additional constraints on encoder E S , ensuring the preservation of tumor structures. This constraint proves beneficial for the translation decoder G T as it generates pseudo-target tumoral samples that are more reliable. Segmentation is therefore trained on both diseased source images S P and their corresponding synthetic target images S T P , when provided with annotations y S . To such an extent, two segmentation masks are predicted ŷS = G S seg • E S (S P ) and ŷST = G T seg • E T (S T P ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Loss Functions</head><p>Segmentation Loss. For the segmentation objective, we compute a soft Dice loss <ref type="bibr" target="#b20">[21]</ref> on the predictions for both labelled source images and their translations:</p><formula xml:id="formula_4">L seg = Dice (y S , ŷS ) + Dice (y S , ŷST )<label>(3)</label></formula><p>Reconstruction Losses. L mod cyc and L Gen rec respectively impose pixel-level image reconstruction constraints on modality translation and GenSeg tasks. Note that L 1 refers to the standard L1 norm:</p><formula xml:id="formula_5">L mod cyc = L 1 S TS A , S A + L 1 T ST A , T A + L 1 S TS P , S P + L 1 T ST P , T P L Gen rec = L 1 (S AA , S A ) + L 1 (S PP , S P ) + L 1 (T AA , T A ) + L 1 (T PP , T P )<label>(4)</label></formula><p>Moreover, like in <ref type="bibr" target="#b13">[14]</ref> we compute a loss L Gen lat that ensures that the translation task holds the information relative to the initial image, by reconstructing their latent codes with the L1 norm. It also enforces the distribution of unique codes to match the prior N (0, I) by making u AP match u, where u AP is obtained by encoding the fake diseased sample x AP produced with random sample u.</p><p>Adversarial Loss. For the healthy-diseased translation adversarial objective, we compute a hinge loss L Gen adv as in GenSeg, learning to discriminate between pairs of real/synthetic images of the same output domain and always in the same imaging modality, e.g. S A vs S PA . In the modality translation task, the L mod adv loss is computed between pairs of images of the same modality without distinction between domains A and P , e.g. {S A , S P } vs {T S A , T S P }.</p><p>Overall Loss. The overall loss for M-GenSeg is a weighted sum of the aforementioned losses. These are tuned separately. All weights sum to 1. First, λ Gen adv , λ Gen rec , and λ Gen lat weights are tuned for successful translation between diseased and healthy images. Then, λ mod adv and λ mod cyc are tuned for successful modality translation. Finally, λ seg is tuned for segmentation performance.</p><formula xml:id="formula_6">L T otal = λ seg L seg + λ mod adv L mod adv + λ mod cyc L mod cyc +λ Gen adv L Gen adv + λ Gen rec L Gen rec + λ Gen lat L Gen lat (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation Details</head><p>Training and Hyper-Parameters. All models are implemented using PyTorch and are trained on one NVIDIA A100 GPU with 40 GB memory. We used a batch size of 15, an AMSGrad optimizer (β 1 = 0.5 and β 2 = 0.999) and a learning rate of 10 -4 . Our models were trained for 300 epochs and weights of the segmentation model with the highest validation Dice score were saved for evaluation. The same on-the-fly data augmentation as in <ref type="bibr" target="#b13">[14]</ref> was applied for all runs. Each training experiment was repeated three times with a different random seed for weight initialization. The performance reported is the mean of all test Dice scores, with standard deviation, across the three runs. The following parameters yielded both great modality and absence/presence translations: λ mod adv = 3, λ mod cyc = 20, λ Gen adv = 6, λ Gen rec = 20 and λ Gen lat = 2. Note that optimal λ seg varies depending on the fraction of pixel-level annotations provided to the network for training.</p><p>Architecture. One distinct encoder, common decoder, residual/segmentation decoder, and modality translation decoder are used for each modality. The architecture used for encoders, decoders and discriminators is the same as in <ref type="bibr" target="#b13">[14]</ref>. However, in order to give insight on the model's behaviour and properly choose the semantic information relevant for each objective, we introduced attention gates <ref type="bibr" target="#b21">[22]</ref> in the skip connections. Figure <ref type="figure" target="#fig_2">2a</ref> shows the attention maps generated for each type of decoder. As expected, residual decoders focus towards tumor areas. More interestingly, in order not to disturb the process of healthy image generation, common decoders avoid lesion locations. Finally, modality translators tend to focus on salient details of the brain tissue, which facilitates contrast redefinition needed for accurate translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Experiments were performed on the BraTS 2020 challenge dataset <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>, adapted for the cross-modality tumor segmentation problem where images are known to be diseased or healthy. Amongst the 369 brain volumes available in BraTS, 37 were allocated each for validation and test steps, while the 295 left were used for training. We split the 3D brain volumes into 2 hemispheres and extracted 2D axial slices. Any slices with at least 1% tumor by brain surface area were considered diseased. Those that didn't show any tumor lesion were labelled as healthy images. Datasets were then assembled from each distinct pair of the four MRI contrasts available (T1, T2, T1ce and FLAIR). To constitute unpaired training data, we used only one modality (source or target) per training volume. All the images are provided with healthy/diseased weak labels, distinct from the pixel-level annotations that we provide only to a subset of the data. Note that the interest for cross-sequence segmentation is limited if multi-parametric acquisitions are performed as is the case in BraTS. However, this modified version of the dataset provides an excellent study case for the evaluation of any modality adaptation method for tumor segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Evaluation</head><p>Domain Adaptation. We compared M-GenSeg with AccSegNet <ref type="bibr" target="#b9">[10]</ref> and AttENT <ref type="bibr" target="#b5">[6]</ref>, two high performance models for domain-adaptative medical image segmentation. To that extent, we performed domain-adaptation experiments with source and target modalities drawn from T1, T2, FLAIR and T1ce. We used available GitHub code for the two baselines and performed fine-tuning on our data. For each possible source/target pair, pixel-level annotations were only retained for the source modality. We show in Fig. <ref type="figure" target="#fig_2">2b</ref> several presence to absence translations and segmentation examples on different target modality images. Although no pixel-level annotations were provided for the target modality, tumors were well disentangled from the brain, resulting in a successful presence to absence translation, as well as segmentation. Note that for hypo-intense lesions (T1 and T1ce), M-GenSeg still manages to convert complex residuals into consistent segmentation maps. We plot in Fig. <ref type="figure" target="#fig_3">3</ref> the Dice performance on the target modality for (i) supervised segmentation on source data without domain adaptation, (ii) domain adaptation methods and (iii) UAGAN <ref type="bibr" target="#b25">[26]</ref>, a model designed for unpaired multi-modal datasets, trained on all source and target data. Over all modality pairs our model shows an absolute Dice score increase of 0.04 and 0.08, respectively, compared to AccSegNet and AttENT. Annotation Deficit. M-GenSeg introduces the ability to train with limited pixel-level annotations available in the source modality. We show in Fig. <ref type="figure" target="#fig_4">4</ref> the Dice scores for models trained when only 1%, 10%, 40%, or 70% of the source T1 modality and 0% of the T2 target modality annotations were available. While performance is severely dropping at 1% of annotations for the baselines, our model shows in comparison only a slight decrease. We thus claim that M-GenSeg can yield robust performance even when a small fraction of the source images is annotated.</p><p>Reaching Supervised Performance. We report that, when the target modality is completely unannotated, M-GenSeg reaches 90% of UAGAN's performance (vs 81% and 85% for AttENT and AccSegNet). Further experiments showed that with a fully annotated source modality, it is sufficient to annotate 25% of the target modality to reach 99% of the performance of fully-supervised UAGAN (e.g. M-GenSeg: 0.861 ± 0.004 vs UAGAN: 0.872 ± 0.003 for T1 → T2 experiment). Thus, the annotation burden could be reduced with M-GenSeg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Experiments</head><p>We conducted ablation tests to validate our methodological choices. We report in Table <ref type="table" target="#tab_1">1</ref> the relative loss in Dice scores on target modality as compared to the proposed model. We assessed the value of doing image-level supervision by setting all the λ Gen loss weights to 0 . Also, we showed that training modality translation only on diseased data is sufficient . However, doing it for healthy data as well provides additional training examples for this task. Likewise, performing translation from absence to presence domain is not necessary but makes more efficient use of the data. Finally, we evaluated M-GenSeg with separate latent spaces for the image-level supervision and modality translation, and we contend that M-GenSeg efficiently combines both tasks when the latent representations share model updates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose M-GenSeg, a new framework for unpaired cross-modality tumor segmentation. We show that M-GenSeg is an annotation-efficient framework that greatly reduces the performance gap due to domain shift in cross-modality tumor segmentation. We claim that healthy tissues, if adequately incorporated to the training process of neural networks like in M-GenSeg, can help to better delineate tumor lesions in segmentation tasks. However, top performing methods on BraTS are 3D models. Thus, future work will explore the use of full 3D images rather than 2D slices, along with more optimal architectures. Our code is available: https://github.com/MaloADBA/MGenSeg_2D.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. M-GenSeg: Latent representations are shared for simultaneous cross-modality translation (green) and semi-supervised segmentation (blue). Source images are passed through the source GenSeg module and the S→T*→S modality translation cycle. Domain adaptation is achieved when training the segmentation on annotated pseudotarget T* images (S T P ). It is not shown but, symmetrically, target images are treated in an other branch to train the T→S→T cyclic translation, and the target GenSeg module to further close the domain gap. (Color figure online)</figDesc><graphic coords="3,58,47,54,62,335,59,252,10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) and S TS d = G S • E T (S T d ), respectively the translation and reconstruction of S d in the S→T→S translation loop, with domain d ∈ {A, P } and • the composition operation. Similarly we have T S d = G S • E T (T d ) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Attention maps for Presence → Absence and modality translations. Red indicates areas of focus while dark blue correspond to locations ignored by the network. (b) Examples of translations from Presence to Absence domains and resulting segmentation. Each column represents a domain adaptation scenario where target modality had no pixel-level annotations provided. (Color figure online)</figDesc><graphic coords="7,55,98,54,02,340,30,194,14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Dice performance on the target modality for each possible source modality. We compare results for M-GenSeg with AccSegNet and AttENT baselines. For reference we also show Dice scores for source supervised segmentation (No adaptation) and UAGAN trained with all source and target annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. T2 domain adaptation with T1 annotation deficit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and replacing the encoded unique code for that image. The reconstruction S AA of the original image in domain A and the synthetic diseased image S AP in domain P are computed from the encoded features [c S</figDesc><table><row><cell>c S P ) and Δ S PP = G S res (c S P , u S</cell></row></table><note><p><p><p>P ) and S PP = S PA + Δ S PP</p><ref type="bibr" target="#b0">(1)</ref> </p>Absence to Presence Translation. Concomitantly, a similar path is implemented for images in the healthy domain. Given an image S A of modality S in domain A, we generate a translated version in domain P. To do so, a synthetic tumor Δ S AP is generated by sampling a code from the normal distribution N (0, I) A , u S A ] as follows:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation studies: relative Dice change on target modality.</figDesc><table><row><cell>Ablation</cell><cell>Mean</cell><cell>Std</cell></row><row><cell>No image-level supervision</cell><cell cols="2">-8.22 ± 2.71 %</cell></row><row><cell>No healthy modality translation</cell><cell cols="2">-2.41 ± 1.29 %</cell></row><row><cell cols="3">No absence to presence translation -3.84 ± 1.71 %</cell></row><row><cell>Unshared latent spaces</cell><cell cols="2">-4.39 ± 1.91 %</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_14.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Challenges related to artificial intelligence research in medical imaging and the importance of image analysis competitions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Prevedello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">180031</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CrossMoDA 2021 challenge: benchmark of cross-modality domain adaptation techniques for vestibular schwannoma and cochlea segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dorent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102628</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SynthSeg: domain randomisation for segmentation of brain scans of any contrast and resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Billot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation for medical image analysis: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Disentangle domain features for cross-modality cardiac image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102078</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AttENT: domain-adaptive medical image segmentation via attention-aware translation and adversarial entropy minimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="952" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SynSeg-Net: synthetic segmentation without target modality ground truth</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1016" to="1025" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Synergistic image and feature adaptation: towards cross-modality domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-derived organ attention for unpaired CT-MRI deep domain adaptation based MRI segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page">205001</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anatomy-constrained contrastive learning for synthetic segmentation without ground-truth</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_5" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">CyCADA: cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Task driven generative modeling for unsupervised domain adaptation: application to X-ray image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-2_67" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="599" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards annotation-efficient segmentation via image-to-image translation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gazda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102624</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lowrank atlas image analyses in the presence of pathologies</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aylward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2583" to="2591" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-rank based image analyses for pathological MR image segmentation and recovery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">333</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-slice lowrank tensor decomposition based multi-atlas segmentation: application to automatic pathological liver CT segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Changfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiancheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Haotian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heng-Da</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102152</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Medical image synthesis for data augmentation and anonymization using generative adversarial networks</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00536-8_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00536-8_1" />
	</analytic>
	<monogr>
		<title level="m">SASHIMI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gooya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Oguz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Burgos</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11037</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning data augmentation for brain tumor segmentation with coarse-to-fine generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C W</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11723-8_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11723-8_7" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2018</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Keyvan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Van Walsum</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11383</biblScope>
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Synthesis of brain tumor multicontrast MR images for improved data augmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2185" to="2198" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46976-8_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46976-8_19" />
	</analytic>
	<monogr>
		<title level="m">LABELS/DLMIA 2016</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">10008</biblScope>
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<title level="m">Attention U-Net: learning where to look for the pancreas</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">170117</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unified attentional generative adversarial network for brain tumor segmentation from multimodal unpaired images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-9_26" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
