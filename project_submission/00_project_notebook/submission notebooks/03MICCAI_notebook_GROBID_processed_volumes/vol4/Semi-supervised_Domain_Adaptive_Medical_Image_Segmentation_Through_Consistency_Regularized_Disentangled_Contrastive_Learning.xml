<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hritam</forename><surname>Basak</surname></persName>
							<email>hbasak@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaozheng</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="260" to="270"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DC101BE0DCE8B150BC2EEF85B176F026</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_25</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Contrastive Learning</term>
					<term>Style-content disentanglement</term>
					<term>Consistency Regularization</term>
					<term>Domain Adaptation</term>
					<term>Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although unsupervised domain adaptation (UDA) is a promising direction to alleviate domain shift, they fall short of their supervised counterparts. In this work, we investigate relatively less explored semi-supervised domain adaptation (SSDA) for medical image segmentation, where access to a few labeled target samples can improve the adaptation performance substantially. Specifically, we propose a twostage training process. First, an encoder is pre-trained in a self-learning paradigm using a novel domain-content disentangled contrastive learning (CL) along with a pixel-level feature consistency constraint. The proposed CL enforces the encoder to learn discriminative content-specific but domain-invariant semantics on a global scale from the source and target images, whereas consistency regularization enforces the mining of local pixel-level information by maintaining spatial sensitivity. This pretrained encoder, along with a decoder, is further fine-tuned for the downstream task, (i.e. pixel-level segmentation) using a semi-supervised setting. Furthermore, we experimentally validate that our proposed method can easily be extended for UDA settings, adding to the superiority of the proposed strategy. Upon evaluation on two domain adaptive image segmentation tasks, our proposed method outperforms the SoTA methods, both in SSDA and UDA settings. Code is available at GitHub.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite their remarkable success in numerous tasks, deep learning models trained on a source domain face the challenges to generalize to a new target domain, especially for segmentation which requires dense pixel-level prediction. This is attributed to a large semantic gap between these two domains. Unsupervised Domain Adaptation (UDA) has lately been investigated to bridge this semantic gap between labeled source domain, and unlabeled target domain <ref type="bibr" target="#b26">[27]</ref>, including adversarial learning for aligning latent representations <ref type="bibr" target="#b22">[23]</ref>, image translation networks <ref type="bibr" target="#b23">[24]</ref>, etc. However, these methods produce subpar performance because of the lack of supervision from the target domain and a large semantic gap in style and content information between the source and target domains. Moreover, when an image's content-specific information is entangled with its domain-specific style information, traditional UDA approaches fail to learn the correct representation of the domain-agnostic content while being distracted by the domain-specific styles. So, they cannot be generalized for multi-domain segmentation tasks <ref type="bibr" target="#b3">[4]</ref>.</p><p>Compared to UDA, obtaining annotation for a few target samples is worthwhile if it can substantially improve the performance by providing crucial target domain knowledge. Driven by this speculation, and the recent success of semisupervised learning (SemiSL), we investigate semi-supervised domain adaptation (SSDA) as a potential solution. Recently, Liu et al. <ref type="bibr" target="#b13">[14]</ref> proposed an asymmetric co-training strategy between a SemiSL and UDA task, that complements each other for cross-domain knowledge distillation. Xia et al. <ref type="bibr" target="#b21">[22]</ref> proposed a co-training strategy through pseudo-label refinement. Gu et al. <ref type="bibr" target="#b6">[7]</ref> proposed a new SSDA paradigm using cross-domain contrastive learning (CL) and selfensembling mean-teacher. However, these methods force the model to learn the low-level nuisance variability, which is insignificant to the task at hand, hence failing to generalize if similar variational semantics are absent in the training set. Fourier Domain Adaptation (FDA) <ref type="bibr" target="#b25">[26]</ref> was proposed to address these challenges by an effective spectral transfer method. Following <ref type="bibr" target="#b25">[26]</ref>, we design a new Gaussian FDA to handle this cross-domain variability, without feature alignment.</p><p>Contrastive learning (CL) is another prospective direction where we enforce models to learn discriminative information from (dis)similarity learning in a latent subspace <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. Liu et al. <ref type="bibr" target="#b14">[15]</ref> proposed a margin-preserving constraint along with a self-paced CL framework, gradually increasing the training data difficulty. Gomariz et al. <ref type="bibr" target="#b5">[6]</ref> proposed a CL framework with an unconventional channel-wise aggregated projection head for inter-slice representation learning. However, traditional CL utilized for DA on images with entangled style and content leads to mixed representation learning, whereas ideally, it should learn discriminative content features invariant to style representation. Besides, the instance-level feature alignment of CL is subpar for segmentation, where dense pixel-wise predictions are indispensable <ref type="bibr" target="#b0">[1]</ref>.</p><p>To alleviate these three underlined shortcomings, we propose a novel contrastive learning with pixel-level consistency constraint via disentangling the style and content information from the joint distribution of source and target domain. Precisely, our contributions are as follows: <ref type="bibr" target="#b0">(1)</ref> We propose to disentangle the style and content information in their compact embedding space using a joint-learning framework; <ref type="bibr" target="#b1">(2)</ref> We propose encoder pre-training with two CL strategies: Style CL and Content CL that learns the style and content information respectively from the embedding space; (3) The proposed CL is complemented with a pixel-level consistency constraint with dense feature propagation module, where the former provides better categorization competence whereas the later enforces effective spatial sensitivity; (4) We experimentally validate that our SSDA method can be extended in the UDA setting easily, achieving superior performance as compared to the SoTA methods on two widely-used domain adaptive segmentation tasks, both in SSDA and UDA settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>Given the source domain image-label pairs {(x i s , y i s ) Ns i=1 ∈ S}, a few image-label pairs from target domain {(x i t1 , y i t1 ) Nt1 i=1 ∈ T 1}, and a large number of unlabeled target images {(x i t2 ) Nt2 i=1 ∈ T 2}, our proposed pre-training stage learns from images in {S ∪ T ; T = T 1 ∪ T 2} in a self-supervised way, without requiring any labels. The following fine-tuning in SSDA considers image-label pairs in {S ∪T 1} for supervised learning alongside unlabeled images T 2 in the target domain for unsupervised prediction consistency. Our workflow is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gaussian Fourier Domain Adaptation (GFDA)</head><p>Manipulating the low-level amplitude spectrum of the frequency domain is the easiest way for style transfer between domains <ref type="bibr" target="#b25">[26]</ref>, without notable alteration in the visuals of high-level semantics. However, as observed in <ref type="bibr" target="#b25">[26]</ref>, the generated images consist of incoherent dark patches, caused by abrupt changes in amplitude around the rectangular mask. Instead, we propose a Gaussian mask for a smoother transition in frequency. Let, F A (•) and F P (•) be the amplitude and phase spectrum in frequency space of an RGB image, and F -1 indicates inverse Fourier transform. We define a 2D Gaussian mask g σ of the same size as F A , with σ being the standard deviation. Given two randomly sampled images x s ∼ S and x t ∼ T , our proposed GFDA can be formulated as:</p><formula xml:id="formula_0">x s→t = F -1 [F P (x s ), F A (x t ) g σ + F A (x s ) (1 -g σ )],<label>(1)</label></formula><p>where indicates element-wise multiplication. It generates an image preserving the semantic content from S but preserving the style from T . Reciprocal pair x t→s is also formulated using the same drill. The source and target images, and the style-transferred versions {x s , x s→t , x t , x t→s } are then used for contrastive pre-training below. Visualization of GFDA is shown in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CL on Disentangled Domain and Content</head><p>We aim to learn discriminative content-specific features that are invariant of the style of the source or target domain, for a better pre-training of the network for the task at hand. Hence, we propose to disentangle the style and content information from the images and learn them jointly in a novel disentangled CL paradigm: Style CL (SCL) and Content CL (CCL). The proposed SCL imposes learning of domain-specific attributes, whereas CCL enforces the model to identify the ROI, irrespective of the spatial semantics and appearance. In joint learning, they complement each other to render the model to learn domain-agnostic and content-specific information, thereby mitigating the domain dilemma. The set of images {x s , x s→t , x t , x t→s }, along with their augmented versions are passed through encoder E, followed by two parallel projection heads, namely style head (G S ) and content head (G C ) to obtain the corresponding embeddings. Two different losses: style contrastive loss L SCL and content contrastive loss L CCL , are derived below. Assuming {x s , x t→s } (along with their augmentations) having source-style representation (style A), and {x t , x s→t } (and their augmentations) having target-style representation (style B), in style CL, embeddings from the same domain (style) are grouped together whereas embeddings from different domains are pushed apart in the latent space. Considering the i th anchor point x i t ∈ T in a minibatch and its corresponding style embedding s i t ← G S (E(x i t )) (with style B), we define the positive set consisting of the same target domain representations as Λ + = {s j+ t , s j+ s→t } ← G S (E({x j t , x j s→t })), ∀j ∈ minibatch, and negative set having unalike source domain representation as Λ -= {s j- s , s j- t→s } ← G S (E({x j s , x j t→s })), ∀j ∈ minibatch. Following SimCLR <ref type="bibr" target="#b4">[5]</ref> our style contrastive loss can be formulated as:</p><formula xml:id="formula_1">L SCL = i,j -log exp(sim(s i , s j+ )/τ ) exp(sim(s i , s j+ )/τ ) + j∈Λ -exp(sim(s i , s j-)/τ ) , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where {s i , s j+ } ∈ style B; s j-∈ style A, sim(•, •) defines cosine similarity, τ is the temperature parameter <ref type="bibr" target="#b4">[5]</ref>. Similarly, we define L CCL for content head as:</p><formula xml:id="formula_3">L CCL = i,j -log exp(sim(c i , c j+ )/τ ) exp(sim(c i , c j+ )/τ ) + j∈Λ -exp(sim(c i , c j-)/τ ) , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where {c i , c j } ← G C (E({x i , x j })). These contrastive losses, along with the consistency constraint below enforce the encoder to extract domain-invariant and content-specific feature embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Consistency Constraint</head><p>The disentangled CL aims to learn global image-level representation, which is useful for instance discrimination tasks. However, segmentation is attributed to learning dense pixel-level representations. Hence, we propose an additional Dense Feature Propagation Module (DFPM) along with a momentum encoder E with exponential moving average (EMA) of parameters from E. Given any pixel m of an image x, we transform its feature f m E obtained from E by propagating other pixel features from the same image:</p><formula xml:id="formula_5">f m E = ∀n∈x K(f m E ) ⊗ cos(f m E , f n E )<label>(4)</label></formula><p>where K is a linear transformation layer, ⊗ denotes matmul operation. This spatial smoothing of learned representation is useful for structural sensitivity, which is fundamental for dense segmentation tasks. We enforce consistency between this smoothed feature fE from E and the regular feature f E from E as:</p><formula xml:id="formula_6">L Con = [d(m,n)&lt;T h] -cos( f m E , f n E ) + cos(f m E , f n E )<label>(5)</label></formula><p>where d(•, •) indicates the spatial distance, T h is a threshold. The overall pretraining objective can be summarized as:</p><formula xml:id="formula_7">L P re = λ 1 L SCL + λ 2 L CCL + L Con (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semi-supervised Fine-Tuning</head><p>The pre-training stage is followed by semi-supervised fine-tuning using a studentteacher framework <ref type="bibr" target="#b17">[18]</ref>. The pre-trained encoder E, along with a decoder D are used as a student branch, whereas an identical encoder-decoder network (but differently initialized) is used as a teacher network. We compute a supervised loss on the labeled set {S ∪ T 1} along with a regularization loss between the prediction of the student and teacher branches on the unlabeled set {T 2} as:</p><formula xml:id="formula_8">L Sup = 1 N s + N t1 x i ∈{S∪T 1} CE D S E S (x i ) , y i (7) L Reg = 1 N t2 x i ∈{T 2} CE D S E S (x i ) , D T E T (x i )<label>(8)</label></formula><p>where CE indicates cross-entropy loss, E S , D S , E T , D T indicate the student and teacher encoder and decoder networks. The student branch is updated using a consolidated loss L = L Sup + λ 3 L Reg , whereas the teacher parameters (θ T ) are updated using EMA from the student parameters (θ S ):</p><formula xml:id="formula_9">θ T (t) = αθ T (t -1) + (1 -α)θ S (t)<label>(9)</label></formula><p>where t tracks the step number, and α is the momentum coefficient <ref type="bibr" target="#b8">[9]</ref>. In summary, the overall SSDA training process contains pre-training (Subsect. 2.1-Subsect. 2.3) and fine-tuning (Subsect. 2.4), whereas, we only use the student branch (E S , D S ) for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Datasets: We evaluate our work on two different DA tasks to evaluate its generalizability: (1) Polyp segmentation from colonoscopy images in Kvasir-SEG <ref type="bibr" target="#b10">[11]</ref> and CVC-EndoScene Still <ref type="bibr" target="#b19">[20]</ref>, and (2) Brain tumor segmentation in MRI images from BraTS2018 <ref type="bibr" target="#b15">[16]</ref>. Kvasir and CVC contain 1000 and 912 images respectively and were split into 4 : 1 training-testing sets following <ref type="bibr" target="#b9">[10]</ref>. BraTS consists of brain MRIs from 285 patients with T1, T2, T1CE, and FLAIR scans. The data was split into 4 : 1 train-test ratio, following <ref type="bibr" target="#b13">[14]</ref>. Source→Target: We perform experiments on CV C → Kvasir and Kvasir → CV C for polyp segmentation, and T 2 → {T 1, T 1CE, F LAIR} for tumor segmentation. The SSDA accesses 10 -50% and 1 -5 labels from the target domain for the two tasks, respectively. For UDA, only S is used for L Sup , whereas T 1 ∪ T 2 is used for L Reg . Implementation details: Implementation is done in a PyTorch environment using a Tesla V100 GPU with 32GB RAM. We use U-Net <ref type="bibr" target="#b16">[17]</ref> backbone for the encoder-decoder structure, and the projection heads G S and G C are shallow FC layers. The model is trained for 300 epochs for pre-training and 500 epochs for fine-tuning using an ADAM optimizer with a batch size of 4 and a learning rate of 1e -4. λ1, λ2, λ3, and T h are set to 0.75, 0.75, 0.5, 0.6, respectively by validation, τ, α are set to 0.07, 0.999 following <ref type="bibr" target="#b8">[9]</ref>. Augmentations include random rotation and translation. Metrics: Segmentation performance is evaluated using Dice Similarity Score (DSC) and Hausdorff Distance (HD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Performance on SSDA</head><p>Quantitative comparison of our proposed method with different SSDA methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> for both tasks are shown in Table <ref type="table">1</ref> and<ref type="table" target="#tab_1">Table 2</ref>. ACT <ref type="bibr" target="#b13">[14]</ref> simply ignores the domain gap and only learns content semantics, resulting in substandard performance on the BraTS dataset that has a significant domain gap. FSM <ref type="bibr" target="#b23">[24]</ref>, on the other hand, is adaptable to learning explicit domain information, but Table <ref type="table">1</ref>. Comparison with state-of-the-art UDA and SSDA methods for polyp segmentation on KVASIR and CVC. SSDA results are shown for 10%-labeled (10%L) and 50%-labeled (50%L) data in the target domain. The results of cited methods are directly reported from the corresponding papers. No DA: the encoder-decoder model trained only using labeled data from the source domain is applied to the target domain without adaptation. Supervised: model is trained using all labeled data from source and target domains. The best and second-best results are highlighted in RED and BLUE, respectively. lacks strong pixel-level regularization on its prediction, resulting in subpar performance. We address both of these shortcomings in our work, resulting in superior performance on both tasks. Other methods like <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>, which are originally designed for natural images, lack critical refining abilities even after fine-tuning for medical image segmentation and hence are far behind our performance in both tasks. The margins are even higher for less labeled data (1L) on the BraTS dataset, which is promising considering the difficulty of the task. Moreover, our method produces performance close to its fully-supervised counterpart (last row in Table <ref type="table">1</ref> and Table <ref type="table" target="#tab_1">2</ref>), using only a few target labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance on UDA</head><p>Unlike SSDA methods, UDA fully relies on unlabeled data for domain-invariant representation learning. To analyze the effectiveness of DA, we extend our model to the UDA setting (explained in Sect. 3 [Source → Target]) and compare it with SoTA methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref> in Table <ref type="table">1</ref> and Table <ref type="table" target="#tab_1">2</ref>. Methods like <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref> rely on adversarial learning for aligning multi-level feature space, which is not effective for small-sized medical data. Other methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> rely on an image-translation network but fail in effective style adaptation, resulting  in source domain-biased subpar performance. Our method, although relies on FDA <ref type="bibr" target="#b25">[26]</ref>, outperforms it with a large margin of upto 12.5% DSC for polyp segmentation, owing to its superior learning ability of disentangled style and content semantics. Similar results are observed for the BraTS dataset in Table <ref type="table" target="#tab_1">2</ref>, where our work achieved a margin of upto 2.4% DSC than its closest performer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Experiments</head><p>We perform a detailed ablation experiment, as shown in Table <ref type="table" target="#tab_2">3</ref>. The effectiveness of disentangling and joint-learning of style and content information is evident from the experiment (b)&amp;(c) as compared to (a), where the introduction of SCL and CCL boosts overall performance significantly. Moreover, when combined together (experiment (d)), they provide a massive 9.54% and 8.52% DSC gain over traditional CL (experiment (a)) for CV C → Kvasir and Kvasir → CV C, respectively. This also points out a potential shortfall of traditional CL: its inability to adapt to a complex domain in DA. The proposed DFPM (experiment (e)) provides local pixel-level regularization, complementary to the global disentangled CL, resulting in a further boost in performance (∼ 1.5%). We have similar ablation study observations on the BraTS2018 dataset, which is provided in the supplementary file, along with some qualitative examples along with available ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a novel style-content disentangled contrastive learning, guided by a pixel-level feature consistency constraint for semi-supervised domain adaptive medical image segmentation. To the best of our knowledge, this is the first attempt for SSDA in medical image segmentation using CL, which is further extended to the UDA setting. Our proposed work, upon evaluation on two different domain adaptive segmentation tasks in SSDA and UDA settings, outperforms the existing SoTA methods, justifying its effectiveness and generalizability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall workflow of our proposed method. Stage 1: Encoder pre-training by GFDA and CL on disentangled style and content branches, and pixel-wise feature consistency module DFPM; Stage 2: Fine-tuning the encoder in a semi-supervised student-teacher setting.</figDesc><graphic coords="3,69,81,54,35,284,38,198,37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art UDA and SSDA methods for whole tumor segmentation on BraTS2018, where source domain is T2. SSDA results are demonstrated for 1-labeled (1L) and 5-labeled (5L) data in the target domain.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DSC↑</cell><cell></cell><cell></cell><cell>HD↓</cell></row><row><cell>Task</cell><cell cols="2">Method Target Label</cell><cell cols="6">T1 T1CE FLAIR T1 T1CE FLAIR</cell></row><row><cell>No DA</cell><cell>Source only</cell><cell>0L</cell><cell>3.9</cell><cell>6.0</cell><cell>64.4</cell><cell cols="2">56.9 50.8</cell><cell>30.4</cell></row><row><cell></cell><cell>SSCA [13]</cell><cell>0L</cell><cell cols="2">59.3 63.5</cell><cell>82.9</cell><cell cols="2">12.5 11.2</cell><cell>7.9</cell></row><row><cell></cell><cell>SIFA [3]</cell><cell>0L</cell><cell cols="2">51.7 58.2</cell><cell>68.0</cell><cell cols="2">19.6 15.0</cell><cell>16.9</cell></row><row><cell>UDA</cell><cell>DSA [8]</cell><cell>0L</cell><cell cols="2">57.7 62.0</cell><cell>81.8</cell><cell cols="2">14.2 13.7</cell><cell>8.6</cell></row><row><cell></cell><cell>DSFN [28]</cell><cell>0L</cell><cell cols="2">57.3 62.2</cell><cell>78.9</cell><cell cols="2">17.5 15.5</cell><cell>13.8</cell></row><row><cell></cell><cell>Ours</cell><cell>0L</cell><cell cols="2">60.7 64.4</cell><cell cols="3">83.3 11.1 10.9</cell><cell>7.3</cell></row><row><cell></cell><cell>DLD [21]</cell><cell>1L</cell><cell cols="2">65.8 66.5</cell><cell>81.5</cell><cell cols="2">12.0 10.3</cell><cell>7.1</cell></row><row><cell></cell><cell>ACT [14]</cell><cell>1 L</cell><cell cols="2">69.7 69.7</cell><cell>84.5</cell><cell cols="2">10.5 10.0</cell><cell>5.8</cell></row><row><cell></cell><cell>ACT-EMD [14]</cell><cell>1L</cell><cell cols="2">67.4 69.0</cell><cell>83.9</cell><cell cols="2">10.9 10.3</cell><cell>6.4</cell></row><row><cell></cell><cell>SLA [4]</cell><cell>1L</cell><cell cols="2">64.7 66.1</cell><cell>82.3</cell><cell cols="2">12.2 10.5</cell><cell>7.1</cell></row><row><cell>SSDA</cell><cell>Ours</cell><cell>1L</cell><cell cols="2">72.2 71.9</cell><cell cols="3">85.8 10.0 9.5</cell><cell>5.2</cell></row><row><cell></cell><cell>DLD [21]</cell><cell>5L</cell><cell cols="2">67.8 68.3</cell><cell>83.3</cell><cell>11.2</cell><cell>9.9</cell><cell>6.6</cell></row><row><cell></cell><cell>ACT [14]</cell><cell>5 L</cell><cell cols="2">71.3 70.8</cell><cell>85.0</cell><cell>10.0</cell><cell>9.8</cell><cell>5.2</cell></row><row><cell></cell><cell>ACT-EMD [14]</cell><cell>5L</cell><cell cols="2">70.3 69.8</cell><cell>84.4</cell><cell cols="2">10.4 10.2</cell><cell>5.7</cell></row><row><cell></cell><cell>SLA [4]</cell><cell>5L</cell><cell cols="2">67.2 71.2</cell><cell>83.1</cell><cell cols="2">11.7 10.1</cell><cell>6.8</cell></row><row><cell></cell><cell>Ours</cell><cell>5L</cell><cell cols="2">73.1 72.4</cell><cell>86.1</cell><cell>9.7</cell><cell>9.3</cell><cell>4.8</cell></row><row><cell cols="2">Supervised Source+Target</cell><cell>all labeled</cell><cell cols="2">73.6 72.9</cell><cell>86.6</cell><cell>9.5</cell><cell>9.1</cell><cell>4.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation experiment for polyp segmentation in SSDA(50%L) setting to identify the contribution of individual components. TCL: traditional CL<ref type="bibr" target="#b1">[2]</ref>, SCL: proposed style CL, CCL: proposed content CL. The last row, highlighted in RED, indicates our results.</figDesc><table><row><cell>Experiment#</cell><cell></cell><cell cols="2">Stage 1</cell><cell></cell><cell cols="4">Stage 2 CVC → Kvasir Kvasir → CVC</cell></row><row><cell></cell><cell cols="6">TCL SCL CCL DFPM SemiSL DSC↑ HD↓</cell><cell cols="2">DSC↑ HD↓</cell></row><row><cell>(a)</cell><cell></cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>81.7</cell><cell>4.4</cell><cell>82.1</cell><cell>4.2</cell></row><row><cell>(b)</cell><cell>×</cell><cell></cell><cell>×</cell><cell>×</cell><cell>83.2</cell><cell>3.9</cell><cell>84.7</cell><cell>3.5</cell></row><row><cell>(c)</cell><cell>×</cell><cell>×</cell><cell></cell><cell>×</cell><cell>84.5</cell><cell>3.8</cell><cell>85.4</cell><cell>3.1</cell></row><row><cell>(d)</cell><cell>×</cell><cell></cell><cell></cell><cell>×</cell><cell>89.5</cell><cell>2.8</cell><cell>89.1</cell><cell>2.4</cell></row><row><cell>(e)</cell><cell>×</cell><cell></cell><cell></cell><cell></cell><cell>90.6</cell><cell>2.4</cell><cell>90.8</cell><cell>2.2</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_25.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ideal: improved dense local contrastive learning for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mallipeddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contrastive learning of global and local features for medical image segmentation with limited annotations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12546" to="12558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synergistic image and feature adaptation: towards cross-modality domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation based on dual-level domain mixing for semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11018" to="11027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with contrastive learning for OCT segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gomariz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_34" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference, Part VIII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="351" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrastive semi-supervised learning for domain adaptive segmentation across similar anatomical structures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="245" to="256" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep symmetric adaptation network for cross-modality medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial contrastive Fourier domain adaptation for polyp segmentation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Huyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: a segmented polyp dataset</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-37734-2_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-37734-2_37" />
	</analytic>
	<monogr>
		<title level="m">MMM 2020</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11962</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-semantic contour adaptation for cross modality brain tumor segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>El Fakhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ACT: semi-supervised domain-adaptive medical image segmentation with asymmetric co-training</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_7" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference, Proceedings, Part V</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Margin preserving self-paced contrastive learning towards domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="638" to="647" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Healthcare Eng</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Alleviating semantic-level shift: a semi-supervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="936" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101766</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Low-resource adversarial domain adaptation for crossmodality nucleus detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Cornish</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_61" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference, Proceedings, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="639" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Source free domain adaptation for medical image segmentation with Fourier style mining</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102457</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Phase consistent ecological domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9011" to="9020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FDA: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A novel 3D unsupervised domain adaptation framework for crossmodality medical image segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4976" to="4986" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with dual-scheme fusion network for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="3291" to="3298" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
