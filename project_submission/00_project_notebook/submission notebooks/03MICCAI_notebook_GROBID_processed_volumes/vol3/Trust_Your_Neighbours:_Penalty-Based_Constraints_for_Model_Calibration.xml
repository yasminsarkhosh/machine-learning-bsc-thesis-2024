<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trust Your Neighbours: Penalty-Based Constraints for Model Calibration</title>
				<funder ref="#_4FttpKW">
					<orgName type="full">FRQNT</orgName>
				</funder>
				<funder ref="#_8TV7Gn4">
					<orgName type="full">National Science and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Compute Canada</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Balamurali</forename><surname>Murugesan</surname></persName>
							<email>balamurali.murugesan.1@ens.etsmtl.ca</email>
							<idno type="ORCID">0000-0002-3002-5845</idno>
							<affiliation key="aff0">
								<orgName type="institution">ETS Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sukesh</forename><surname>Adiga Vasudeva</surname></persName>
							<idno type="ORCID">0000-0001-9754-1548</idno>
						</author>
						<author>
							<persName><forename type="first">Bingyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Herve</forename><surname>Lombaert</surname></persName>
							<idno type="ORCID">0000-0002-3352-7533</idno>
						</author>
						<author>
							<persName><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><surname>Dolz</surname></persName>
							<idno type="ORCID">0000-0002-2436-7750</idno>
							<affiliation key="aff0">
								<orgName type="institution">ETS Montreal</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trust Your Neighbours: Penalty-Based Constraints for Model Calibration</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="572" to="581"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">83F38A0F1A54F554A31A87D6C93AC918</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Segmentation</term>
					<term>Calibration</term>
					<term>Uncertainty estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ensuring reliable confidence scores from deep networks is of pivotal importance in critical decision-making systems, notably in the medical domain. While recent literature on calibrating deep segmentation networks has led to significant progress, their uncertainty is usually modeled by leveraging the information of individual pixels, which disregards the local structure of the object of interest. In particular, only the recent Spatially Varying Label Smoothing (SVLS) approach addresses this issue by softening the pixel label assignments with a discrete spatial Gaussian kernel. In this work, we first present a constrained optimization perspective of SVLS and demonstrate that it enforces an implicit constraint on soft class proportions of surrounding pixels. Furthermore, our analysis shows that SVLS lacks a mechanism to balance the contribution of the constraint with the primary objective, potentially hindering the optimization process. Based on these observations, we propose a principled and simple solution based on equality constraints on the logit values, which enables to control explicitly both the enforced constraint and the weight of the penalty, offering more flexibility. Comprehensive experiments on a variety of well-known segmentation benchmarks demonstrate the superior performance of the proposed approach. The code is available at https://github.com/Bala93/MarginLoss.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have achieved remarkable success in important areas of various domains, such as computer vision, machine learning and natural language processing. Nevertheless, there exists growing evidence that suggests that these models are poorly calibrated, leading to overconfident predictions that may assign high confidence to incorrect predictions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. This represents a major problem, as inaccurate uncertainty estimates can have severe consequences in safety-critical applications such as medical diagnosis. The underlying cause of network miscalibration is hypothesized to be the high capacity of these models, which makes them susceptible to overfitting on the negative log-likelihood loss that is conventionally used during training <ref type="bibr" target="#b5">[6]</ref>.</p><p>In light of the significance of this issue, there has been a surge in popularity for quantifying the predictive uncertainty in modern DNNs. A simple approach involves a post-processing step that modifies the softmax probability predictions of an already trained network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. Despite its efficiency, this family of approaches presents important limitations, which include i) a dataset-dependency on the value of the transformation parameters and ii) a large degradation observed under distributional drifts <ref type="bibr" target="#b19">[20]</ref>. A more principled solution integrates a term that penalizes confident output distributions into the learning objective, which explicitly maximizes the Shannon entropy of the model predictions during training <ref type="bibr" target="#b20">[21]</ref>. Furthermore, findings from recent works on calibration <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> have demonstrated that popular classification losses, such as Label Smoothing (LS) <ref type="bibr" target="#b21">[22]</ref> and Focal Loss (FL) <ref type="bibr" target="#b9">[10]</ref>, have a favorable effect on model calibration, as they implicitly integrate an entropy maximization objective. Following these works, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> presented a unified view of state-of-the-art calibration approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> showing that these strategies can be viewed as approximations of a linear penalty imposing equality constraints on logit distances. The associated equality constraint results in gradients that continually push towards a non-informative solution, potentially hindering the ability to achieve the optimal balance between discriminative performance and model calibration. To alleviate this limitation, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref> proposed a simple and flexible alternative based on inequality constraints, which imposes a controllable margin on logit distances. Despite the progress brought by these methods, none of them explicitly considers pixel relationships, which is fundamental in the context of image segmentation.</p><p>Indeed, the nature of structured predictions in segmentation, involves pixelwise classification based on spatial dependencies, which limits the effectiveness of these strategies to yield performances similar to those observed in classification tasks. In particular, this potentially suboptimal performance can be attributed to the uniform (or near-to-uniform) distribution enforced on the softmax/logits distributions, which disregards the spatial context information. To address this important issue, Spatially Varying Label Smoothing (SVLS) <ref type="bibr" target="#b6">[7]</ref> introduces a soft labeling approach that captures the structural uncertainty required in semantic segmentation. In practice, smoothing the hard-label assignment is achieved through a Gaussian kernel applied across the one-hot encoded ground truth, which results in soft class probabilities based on neighboring pixels. Nevertheless, while the reasoning behind this smoothing strategy relies on the intuition of giving an equal contribution to the central label and all surrounding labels combined, its impact on the training, from an optimization standpoint, has not been studied.</p><p>The contributions of this work can be summarized as follows:</p><p>-We provide a constrained-optimization perspective of Spatially Varying Label Smoothing (SVLS) <ref type="bibr" target="#b6">[7]</ref>, demonstrating that it imposes an implicit constraint on a soft class proportion of surrounding pixels. Our formulation shows that SVLS lacks a mechanism to control explicitly the importance of the constraint, which may hinder the optimization process as it becomes challenging to balance the constraint with the primary objective effectively. -Following our observations, we propose a simple and flexible solution based on equality constraints on the logit distributions. The proposed constraint is enforced with a simple linear penalty, which incorporates an explicit mechanism to control the weight of the penalty. Our approach not only offers a more efficient strategy to model the logit distributions but implicitly decreases the logit values, which results in less overconfident predictions. -Comprehensive experiments over multiple medical image segmentation benchmarks, including diverse targets and modalities, show the superiority of our method compared to state-of-the-art calibration losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Formulation. Let us denote the training dataset as</p><formula xml:id="formula_0">D(X , Y) = {(x (n) , y (n) )} N n=1 , with x (n) ∈ X ⊂ R</formula><p>Ωn representing the n th image, Ω n the spatial image domain, and y (n) ∈ Y ⊂ R K its corresponding ground-truth label with K classes, provided as a one-hot encoding vector. Given an input image x (n) , a neural network parameterized by θ generates a softmax probability vector, defined as</p><formula xml:id="formula_1">f θ (x (n) ) = s (n) ∈ R Ωn×K ,</formula><p>where s is obtained after applying the softmax function over the logits l (n) ∈ R Ωn×K . To simplify the notations, we omit sample indices, as this does not lead to any ambiguity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Constrained Optimization Perspective of SVLS</head><p>Spatially Varying Label Smoothing (SVLS) <ref type="bibr" target="#b6">[7]</ref> considers the surrounding class distribution of a given pixel p in the ground truth y to estimate the amount of smoothness over the one-hot label of that pixel. In particular, let us consider that we have a 2D patch x of size d 1 × d 2 and its corresponding ground truth y<ref type="foot" target="#foot_0">1</ref> . Furthermore, the predicted softmax in a given pixel is denoted as s = [s 0 , s 1 , ..., s k-1 ]. Let us now transform the surrounding patch of the segmentation mask around a given pixel into a unidimensional vector y ∈ R d , where d = d 1 ×d 2 . SVLS employs a discrete Gaussian kernel w to obtain soft class probabilities from one-hot labels, which can also be reshaped into w ∈ R d . Following this, for a given pixel p, and a class k, SVLS <ref type="bibr" target="#b6">[7]</ref> can be defined as:</p><formula xml:id="formula_2">ỹk p = 1 | d i w i | d i=1 y k i w i . (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>Thus, once we replace the smoothed labels ỹk p in the standard cross-entropy (CE) loss, the new learning objective becomes:</p><formula xml:id="formula_4">L = - k 1 | d i w i | d i=1 y k i w i log s k p ,<label>(2)</label></formula><p>where s k p is the softmax probability for the class k at pixel p (the pixel in the center of the patch). Now, this loss can be decomposed into:</p><formula xml:id="formula_5">L = - 1 | d i w i | k y k p log s k p - 1 | d i w i | k ⎛ ⎜ ⎝ d i=1 i =p y k i w i ⎞ ⎟ ⎠ log s k p ,<label>(3)</label></formula><p>with p denoting the index of the pixel in the center of the patch. Note that the term in the left is the cross-entropy between the posterior softmax probability and the hard label assignment for pixel p. Furthermore, let us denote</p><formula xml:id="formula_6">τ k = d i=1 i =p</formula><p>y k i w i as the soft proportion of the class k inside the patch/mask y, weighted by the filter values w. By replacing τ k into the Eq. 3, and removing | d i w i | as it multiplies both terms, the loss becomes:</p><formula xml:id="formula_7">L = - k y k p log s k p CE - k τ k log s k p Constraint on τ . (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>As τ is constant, the second term in Eq. 4 can be replaced by a Kullback-Leibler (KL) divergence, leading to the following learning objective:</p><formula xml:id="formula_9">L c = L CE + D KL (τ ||s),<label>(5)</label></formula><p>where c = stands for equality up to additive and/or non-negative multiplicative constant. Thus, optimizing the loss in SVLS results in minimizing the crossentropy between the hard label and the softmax probability distribution on the pixel p, while imposing the equality constraint τ = s, where τ depends on the class distribution of surrounding pixels. Indeed, this term implicitly enforces the softmax predictions to match the soft-class proportions computed around p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed Constrained Calibration Approach</head><p>Our previous analysis exposes two important limitations of SVLS: 1) the importance of the implicit constraint cannot be controlled explicitly, and 2) the prior τ is derived from the σ value in the Gaussian filter, making it difficult to model properly. To alleviate this issue, we propose a simple solution, which consists in minimizing the standard cross-entropy between the softmax predictions and the one-hot encoded masks coupled with an explicit and controllable constraint on the logits l. In particular, we propose to minimize the following constrained objective:</p><formula xml:id="formula_10">min L CE s.t. τ = l,<label>(6)</label></formula><p>where τ now represents a desirable prior, and τ = l is a hard constraint. Note that the reasoning behind working directly on the logit space is two-fold. First, observations in <ref type="bibr" target="#b10">[11]</ref> suggest that directly imposing the constraints on the logits results in better performance than in the softmax predictions. And second, by imposing a bounded constraint on the logits values<ref type="foot" target="#foot_1">2</ref> , their magnitudes are further decreased, which has a favorable effect on model calibration <ref type="bibr" target="#b16">[17]</ref>. We stress that despite both <ref type="bibr" target="#b10">[11]</ref> and our method enforce constraints on the predicted logits, <ref type="bibr" target="#b10">[11]</ref> is fundamentally different. In particular, <ref type="bibr" target="#b10">[11]</ref> imposes an inequality constraint on the logit distances so that it encourages uniform-alike distributions up to a given margin, disregarding the importance of each class in a given patch. This can be important in the context of image segmentation, where the uncertainty of a given pixel may be strongly correlated with the labels assigned to its neighbors. In contrast, our solution enforces equality constraints on an adaptive prior, encouraging distributions close to class proportions in a given patch.</p><p>Even though the constrained optimization problem presented in Eq. 6 could be solved by a standard Lagrangian-multiplier algorithm, we replace the hard constraint by a soft penalty of the form P(|τ -l|), transforming our constrained problem into an unconstrained one, which is easier to solve. In particular, the soft penalty P should be a continuous and differentiable function that reaches its minimum when it verifies P(|τ -l|) ≥ P(0), ∀ l ∈ R K , i.e., when the constraint is satisfied. Following this, when the constraint |τ -l| deviates from 0 the value of the penalty term increases. Thus, we can approximate the problem in Eq. 6 as the following simpler unconstrained problem:</p><formula xml:id="formula_11">min L CE + λ k |τ k -l k |, (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where the penalty is modeled here as a ReLU function, whose importance is controlled by the hyperparameter λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Datasets. FLARE Challenge <ref type="bibr" target="#b11">[12]</ref> contains 360 volumes of multi-organ abdomen CT with their corresponding pixel-wise masks, which are resampled to a common space and cropped to 192 × 192 × 30. ACDC Challenge <ref type="bibr" target="#b2">[3]</ref>  Evaluation Metrics. To assess the discriminative performance of the evaluated models, we resort to standard segmentation metrics in medical segmentation, which includes the DICE coefficient (DSC) and the 95% Hausdorff Distance (HD). To evaluate the calibration performance, we employ the expected calibration error (ECE) <ref type="bibr" target="#b18">[19]</ref> on foreground classes, as in <ref type="bibr" target="#b6">[7]</ref>, and classwise expected calibration error (CECE) <ref type="bibr" target="#b8">[9]</ref>, following <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref> (more details in Supp. Material).</p><p>Implementation Details . We benchmark the proposed model against several losses, including state-of-the-art calibration losses. These models include the compounded CE + Dice loss (CE+DSC), FL <ref type="bibr" target="#b9">[10]</ref>, Entropy penalty (ECP) <ref type="bibr" target="#b20">[21]</ref>, LS <ref type="bibr" target="#b21">[22]</ref>, SVLS <ref type="bibr" target="#b6">[7]</ref> and MbLS <ref type="bibr" target="#b10">[11]</ref>. Following the literature, we consider the hyperparameters values typically employed and select the value which provided the best average DSC on the validation set across all the datasets. More concretely, for FL, γ values of 1, 2, and 3 are considered, whereas 0.1, 0.2, and 0.3 are used for α and λ in LS and ECP, respectively. We consider the margins of MbLS to be 3, 5, and 10, while fixing λ to 0.1, as in <ref type="bibr" target="#b17">[18]</ref>. In the case of SVLS, the one-hot label smoothing is performed with a kernel size of 3 and σ = [0.5, 1, 2].</p><p>For training, we fixed the batch size to 16, epochs to 100, and used ADAM <ref type="bibr" target="#b7">[8]</ref>, with a learning rate of 10 -3 for the first 50 epochs, and reduced to 10 -4 afterwards. Following <ref type="bibr" target="#b17">[18]</ref>, the models are trained on 2D slices, and the evaluation is performed over 3D volumes. Last, we use the following prior τ k = d i=1 y k i , which is computed over a 3 × 3 patch, similarly to SVLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Comparison to State-of-the-Art. Table <ref type="table" target="#tab_1">1</ref> reports the discriminative and calibration results achieved by the different methods. We can observe that, across all the datasets, the proposed method consistently outperforms existing approaches, always ranking first and second in all the metrics. Furthermore, while other methods may obtain better performance than the proposed approach in a single metric, their superiority strongly depends on the selected dataset. For example, ECP <ref type="bibr" target="#b20">[21]</ref> yields very competitive performance on the FLARE dataset, whereas it ranks among the worst models in ACDC or BraTS.</p><p>To have a better overview of the performance of the different methods, we follow the evaluation strategies adopted in several MICCAI Challenges, i.e., sumrank <ref type="bibr" target="#b13">[14]</ref> and mean-case-rank <ref type="bibr" target="#b12">[13]</ref>. As we can observe in the heatmaps provided in Fig. <ref type="figure" target="#fig_0">1</ref>, our approach yields the best rank across all the metrics in both strategies,  clearly outperforming any other method. Interestingly, some methods such as FL or ECP typically provide well-calibrated predictions, but at the cost of degrading their discriminative performance.</p><p>Ablation Studies. 1-Constraint over logits vs softmax. Recent evidence <ref type="bibr" target="#b10">[11]</ref> suggests that imposing constraints on the logits presents a better alternative than its softmax counterpart. To demonstrate that this observation holds in our model, we present the results of our formulation when the constraint is enforced on the softmax distributions, i.e., replacing l by s (Table <ref type="table" target="#tab_2">2</ref>, top), which yields inferior results. 2-Choice of the penalty. To solve the unconstrained problem in Eq. 7, we can approximate the second term with a liner penalty, modeled as a ReLU function. Nevertheless, we can resort to other polynomial penalties, e.g., quadratic penalties, whose main difference stems from the more aggressive behavior of quadratic penalties over larger constraint violations. The results obtained when the linear penalty is replaced by a quadratic penalty are reported in Table <ref type="table" target="#tab_2">2</ref> (middle). From these results, we can observe that, while a quadratic penalty could achieve better results in a particular dataset (e.g., ACDC or calibration performance on BraTS), a linear penalty yields more consistent results across datasets. 3-Patch size. For a fair comparison with SVLS, we used a patch of size 3 × 3 in our model. Nevertheless, we now investigate the impact of employing a larger patch to define the prior τ , whose results are presented in Table <ref type="table" target="#tab_2">2</ref> (bottom). Even though a larger patch seems to bring comparable results in one dataset, the performance on the other two datasets is largely degraded, which potentially hinders its scalability to other applications. We believe that this is due to the higher degree of noise in the class distribution, particularly when multiple organs overlap, as the employed patch covers a wider region.</p><p>Impact of the Prior. A benefit of the proposed formulation is that diverse priors can be enforced on the logit distributions. Thus, we now assess the impact of different priors τ in our formulation (See Supplemental Material for a detailed explanation). The results presented in Table <ref type="table" target="#tab_3">3</ref> reveal that selecting a suitable prior can further improve the performance of our model. Magnitude of the Logits. To empirically demonstrate that the proposed solution decreases the logit values, we plot average logit distributions across classes on the FLARE test set (Fig. <ref type="figure" target="#fig_1">2</ref>). In particular, we first separate all the voxels based on their ground truth labels. Then, for each category, we average the per-voxel vector of logit predictions (in absolute value). We can observe that, compared to SVLS and MbLS, -which also imposes constraints on the logits-, our approach leads to much lower logit values, particularly compared to SVLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented a constrained-optimization perspective of SVLS, which has revealed two important limitations of this method. First, the implicit constraint enforced by SVLS cannot be controlled explicitly. And second, the prior imposed in the constraint is directly derived from the Gaussian kernel used, which makes it hard to model. In light of these observations, we have proposed a simple alternative based on equality constraints on the logits, which allows to control the importance of the penalty explicitly, and the inclusion of any desirable prior in the constraint. Our results suggest that the proposed method improves the quality of the uncertainty estimates, while enhancing the segmentation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sum-rank and mean-rank evaluation. Ranking of the different methods based on the sum-rank (left) and mean of case-specific (right) approaches. The lower the value, the better the performance.</figDesc><graphic coords="7,61,74,225,02,315,49,103,66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of logit values. From left to right: MbLS, SVLS and ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>consists of 100 patient exams containing cardiac MR volumes and their respective segmentation masks. Following the standard practices on this dataset, 2D slices are extracted from the volumes and resized to 224 × 224. BraTS-19 Challenge<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15]</ref> contains 335 multi-modal MR scans (FLAIR, T1, T1-contrast, and T2) with their corresponding segmentation masks, where each volume of dimension 155 × 240 × 240 is resampled to 128 × 192 × 192. More details about these datasets, such as the train, validation and testing splits, can be found in Supp. Material.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 . Comparison to state-of-the-art.</head><label>1</label><figDesc>Discriminative (DSC ↑, HD ↓) and calibration (ECE ↓, CECE ↓) performance obtained by the different models (best method in bold, and second best in bold and underlined).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Empirical results to motivate our methodological and technical choices.</figDesc><table><row><cell></cell><cell>FLARE</cell><cell>ACDC</cell><cell>BraTS</cell></row><row><cell></cell><cell cols="3">DSC HD ECE CECE DSC HD ECE CECE DSC HD ECE CECE</cell></row><row><cell cols="4">Constraint on s 0.862 5.14 0.043 0.030 0.840 2.66 0.068 0.071 0.802 8.28 0.145 0.104</cell></row><row><cell>L2-penalty</cell><cell cols="3">0.851 5.48 0.065 0.054 0.871 1.78 0.059 0.080 0.851 7.90 0.078 0.091</cell></row><row><cell cols="4">Patch size: 5 × 5 0.875 5.96 0.032 0.031 0.813 3.50 0.078 0.077 0.735 7.45 0.119 0.092</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Impact of using different priors (τ ) in Eq. 7.</figDesc><table><row><cell></cell><cell>FLARE</cell><cell>ACDC</cell><cell>BraTS</cell></row><row><cell>Prior τ</cell><cell cols="3">DSC HD ECE CECE DSC HD ECE CECE DSC HD ECE CECE</cell></row><row><cell>Mean</cell><cell cols="3">0.868 4.88 0.033 0.031 0.854 2.55 0.048 0.061 0.850 5.78 0.112 0.097</cell></row><row><cell cols="4">Gaussian 0.860 5.40 0.033 0.032 0.876 2.92 0.042 0.053 0.813 7.01 0.140 0.106</cell></row><row><cell>Max</cell><cell cols="3">0.859 4.95 0.038 0.036 0.876 1.74 0.046 0.054 0.833 8.25 0.114 0.094</cell></row><row><cell>Min</cell><cell cols="3">0.854 5.42 0.034 0.033 0.881 1.80 0.040 0.053 0.836 7.23 0.104 0.092</cell></row><row><cell cols="4">Median 0.867 5.90 0.033 0.032 0.835 3.29 0.075 0.075 0.837 7.53 0.095 0.089</cell></row><row><cell>Mode</cell><cell cols="3">0.854 5.41 0.035 0.034 0.876 1.62 0.045 0.056 0.808 8.21 0.135 0.113</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For the sake of simplicity, we consider a patch as an image x (or mask y), whose spatial domain Ω is equal to the patch size, i.e., d1 × d2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that the proportion priors are generally normalized.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is supported by the <rs type="funder">National Science and Engineering Research Council of Canada (NSERC)</rs>, via <rs type="grantName">its Discovery Grant program</rs> and <rs type="funder">FRQNT</rs> through the <rs type="programName">Research Support for New Academics program</rs>. We also thank <rs type="person">Calcul Quebec</rs> and <rs type="funder">Compute Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8TV7Gn4">
					<orgName type="grant-name">its Discovery Grant program</orgName>
				</org>
				<org type="funding" xml:id="_4FttpKW">
					<orgName type="program" subtype="full">Research Support for New Academics program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 55.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Local temperature scaling for probability calibration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatially varying label smoothing: capturing uncertainty from expert annotations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78191-052" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12729</biblScope>
			<biblScope unit="page" from="677" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond temperature scaling: obtaining well-calibrated multi-class probabilities with dirichlet calibration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The devil is in the margin: marginbased label smoothing for network calibration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Abdomenct-1K: is abdominal organ segmentation a solved problem?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="6695" to="6714" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ISLES 2015 -a public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="250" to="269" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MRBrainS challenge: online evaluation framework for brain image segmentation in 3T MRI scans</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendrik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Calibrating deep neural networks using focal loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Calibrating segmentation networks with margin-based label smoothing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Murugesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">102826</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Can you trust your model&apos;s uncertainty? evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ovadia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Post-hoc uncertainty calibration for domain drift scenarios</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Buettner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mix-n-match: ensemble and compositional methods for uncertainty calibration in deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
