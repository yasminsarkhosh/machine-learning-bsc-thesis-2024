<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications</title>
				<funder>
					<orgName type="full">Universit√† Campus Bio-Medico di Roma</orgName>
				</funder>
				<funder ref="#_sxv2cs9">
					<orgName type="full">PNRR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Pennisi</surname></persName>
							<idno type="ORCID">0000-0002-6721-4383</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">PeRCeiVe Lab</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<region>Catania</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Federica</forename><surname>Proietto Salanitri</surname></persName>
							<idno type="ORCID">0000-0002-6122-4249</idno>
						</author>
						<author>
							<persName><forename type="first">Giovanni</forename><surname>Bellitto</surname></persName>
							<idno type="ORCID">0000-0002-1333-8348</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">PeRCeiVe Lab</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<region>Catania</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simone</forename><surname>Palazzo</surname></persName>
							<idno type="ORCID">0000-0002-2441-0982</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">PeRCeiVe Lab</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<region>Catania</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ulas</forename><surname>Bagci</surname></persName>
							<idno type="ORCID">0000-0001-7379-6829</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and BME</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
							<idno type="ORCID">0000-0001-6653-2577</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">PeRCeiVe Lab</orgName>
								<orgName type="institution">University of Catania</orgName>
								<address>
									<region>Catania</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="422" to="431"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">BF710FD893489C07B3D2D9858FB41F86</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>generative models</term>
					<term>privacy-preserving</term>
					<term>latent navigation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) have demonstrated their ability to generate synthetic samples that match a target distribution. However, from a privacy perspective, using GANs as a proxy for data sharing is not a safe solution, as they tend to embed nearduplicates of real samples in the latent space. Recent works, inspired by k-anonymity principles, address this issue through sample aggregation in the latent space, with the drawback of reducing the dataset by a factor of k. Our work aims to mitigate this problem by proposing a latent space navigation strategy able to generate diverse synthetic samples that may support effective training of deep models, while addressing privacy concerns in a principled way. Our approach leverages an auxiliary identity classifier as a guide to non-linearly walk between points in the latent space, minimizing the risk of collision with near-duplicates of real samples. We empirically demonstrate that, given any random pair of points in the latent space, our walking strategy is safer than linear interpolation. We then test our path-finding strategy combined to k-same methods and demonstrate, on two benchmarks for tuberculosis and diabetic retinopathy classification, that training a model using samples generated by our approach mitigate drops in performance, while keeping privacy preservation. Code is available at: https://github.com/perceivelab/PLAN</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of deep learning for medical data analysis has demonstrated its potential to become a core component of future diagnosis and treatment methodologies. However, in spite of the efforts devoted to improve data efficiency <ref type="bibr" target="#b13">[14]</ref>, the most effective models still rely on large datasets to achieve high accuracy and generalizability. An effective strategy for obtaining large and diverse datasets is to leverage collaborative efforts based on data sharing principles; however, current privacy regulations often hinder this possibility. As a consequence, small private datasets are still used for training models that tend to overfit, introduce biases and generalize badly on other data sources addressing the same task <ref type="bibr" target="#b23">[24]</ref>. As a mitigation measure, generative adversarial networks (GANs) have been proposed to synthesize highly-realistic images, extending existing datasets to include more (and more diverse) examples <ref type="bibr" target="#b16">[17]</ref>, but they pose privacy concerns as real samples may be encoded in the latent space. K-same techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref> attempt to reduce this risk by following the k-anonymity principle <ref type="bibr" target="#b20">[21]</ref> and replacing real samples with synthetic aggregations of groups of k samples. As a downside, these methods reduce the dataset size by a factor of k, which greatly limits their applicability.</p><p>To address this issue, we propose an approach, complementing k-same techniques, for generating an extended variant of a dataset by sampling a privacypreserving walk in the GAN latent space. Our method directly optimizes latent points, through the use of an auxiliary identity classifier, which informs on the similarity between training samples and synthetic images corresponding to candidate latent points. This optimized navigation meets three key properties of data synthesis for medical applications: 1) equidistance, encouraging the generation of diverse realistic samples suitable for model training; 2) privacy preservation, limiting the possibility of recovering original samples, and, 3) class-consistency, ensuring that synthesized samples contain meaningful clinical information. To demonstrate the generalization capabilities of our approach, we experimentally evaluate its performance on two medical image tasks, namely, tuberculosis classification using the Shenzhen Hospital X-ray dataseet <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and diabetic retinopathy classification on the APTOS dataset <ref type="bibr" target="#b12">[13]</ref>. On both tasks, our approach yields classification performance comparable to training with real samples and significantly better than existing k-same techniques such as k-SALSA <ref type="bibr" target="#b8">[9]</ref>, while keeping the same robustness to membership inference attacks.</p><p>Contributions: 1) We present a latent space navigation approach that provides a large amount of diverse and meaningful images for model training; 2) We devise an optimization strategy of latent walks that enforces privacy; 3) We carry out several experiments on two medical tasks, demonstrating the effectiveness of our generative approach on model's training and its guarantees to privacy preservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Conventional methods to protect identity in private images have involved modifying pixels through techniques like masking, blurring, and pixelation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>. However, these methods have been found to be insufficient for providing adequate privacy protection <ref type="bibr" target="#b0">[1]</ref>. As an alternative, GANs have been increasingly explored to synthesize high-quality images that preserve information from the original distribution, while disentangling and removing privacy-sensitive components <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. However, these methods have been mainly devised for face images and cannot be directly applicable to medical images, since there is no clear distinction between identity and non-identity features <ref type="bibr" target="#b8">[9]</ref>.</p><p>Recent approaches, based on the k-same framework <ref type="bibr" target="#b14">[15]</ref>, employ GANs to synthesize clinically-valid medical images principle by aggregating groups of real samples into synthetic privacy-preserving examples <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. In particular, k-SALSA <ref type="bibr" target="#b8">[9]</ref> uses GANs for generating retinal fundus images by proposing a local style alignment strategy to retain visual patterns of the original data. The main downside of these methods is that, in the strive to ensure privacy preservation following the k-anonymity <ref type="bibr" target="#b20">[21]</ref> principle, they significantly reduce the size of the original dataset.</p><p>Our latent navigation strategy complements these approaches by synthesizing large and diverse samples, suitable for downstream tasks. In general, latent space navigation in GANs manipulates the latent vectors to create new images with specific characteristics. While many works have explored this concept to control semantic attributes of generated samples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>, to the best of our knowledge, no method has tackled the problem from a privacy-preservation standpoint, especially on a critical domain such as medical image analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The proposed Privacy-preserving LAtent Navigation (PLAN) strategy envisages three separate stages: 1) GAN training using real samples; 2) latent privacy-preserving trajectory optimization in the GAN latent space; 3) privacypreserving dataset synthesis for downstream applications. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the overall framework and provides a conceptual interpretation of the optimization objectives.</p><p>Formally, given a GAN generator G : W ‚Üí X , we aim to navigate its latent space W to generate samples in image space X in a privacy-preserving way, i.e., avoiding latent regions where real images might be embedded. The expected result is a synthetic dataset that is safe to share, while still including consistent clinical features to be used by downstream tasks (e.g., classification).</p><p>Our objective is to find a set of latent points W ‚äÇ W from which it is safe to synthesize samples that are significantly different from training points: given the training set X ‚äÇ X and a metric d on X , we want to find W such that min x‚àà X d (G ( w) , x) &gt; Œ¥, ‚àÄ w ‚àà W, for a sufficiently large Œ¥. Manually searching for W, however, may be unfeasible: generating a large W is computationally expensive, as it requires at least | W| forward passes through G, and each synthesized image should be compared to all training images; moreover, randomly sampled latent points might not satisfy the above condition.</p><p>To account for latent structure, one could explicitly sample away from latent vectors corresponding to real data. Let ≈¥i ‚äÇ W be the set of latent vectors that produce near-duplicates of a training sample x i ‚àà X , such that G( ≈µi ) ‚âà x i , ‚àÄ ≈µi ‚àà ≈¥i . We can thus define ≈¥ = N i=1 ≈¥i as the set of latent points corresponding to all N samples of the training set: knowledge of ≈¥ can be used to move the above constraint from X to W, by finding W such that min ≈µ‚àà ≈¥ d ( w, ≈µ) &gt; Œ¥, ‚àÄ w ‚àà W. In practice, although ≈¥i can be approximated through latent space projection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref> from multiple initialization points, its cardinality | ≈¥i | cannot be determined a priori as it is potentially unbounded.</p><p>From these limitations, we pose the search of seeking privacy-preserving latent points as a trajectory optimization problem, constrained by a set of objectives that mitigate privacy risks and enforce sample variability and class consistency. Given two arbitrary latent points (e.g., provided by a k-same aggregation method), w a , w b ‚àà W, we aim at finding a latent trajectory WT = [w a = w1 , w2 , . . . , wT -1 , w b = wT ] that traverses the latent space from w a to w b in T steps, such that none of its points can be mapped to any training sample. We design our navigation strategy to satisfy three requirements, which are then translated into optimization objectives:</p><p>1. Equidistance. The distance between consecutive points in the latent trajectory should be approximately constant, to ensure sample diversity and mitigate mode collapse. We define the equidistance loss, L dist , as follows:</p><formula xml:id="formula_0">L dist = T -1 i=1 wi , wi+1<label>2 2 (1)</label></formula><p>where ‚Ä¢ 2 is the L 2 norm. Note that without any additional constraint, L dist converges to the trivial solution of linear interpolation, which gives no guarantee that the path will not contain points belonging to ≈¥. 2. Privacy preservation. To navigate away from latent regions corresponding to real samples, we employ an auxiliary network œÜ id , trained on X to perform identity classification. We then set the privacy preservation constraint by imposing that a sampled trajectory must maximize the uncertainty of œÜ id , thus avoiding samples that could be recognizable from the training set.</p><p>Assuming œÜ id to be a neural network with as many outputs as the number of identities in the original dataset, this constraint can be mapped to a privacypreserving loss, L id , defined as the Kullback-Leibler divergence between the softmax probabilities of œÜ id and the uniform distribution U:</p><formula xml:id="formula_1">L id = T i=1 KL [œÜ id (G( wi )) U(1/n id )]<label>(2)</label></formula><p>where n id is the number of identities. This loss converges towards points with enhanced privacy, on which a trained classifier is maximally uncertain. 3. Class consistency. The latent navigation strategy, besides being privacypreserving, needs to retain discriminative features to support training of downstream tasks on the synthetic dataset. In the case of a downstream classification task, given w a and w b belonging to the same class, all points along a trajectory between w a and w b should exhibit the visual features of that specific class. Moreover, optimizing the constraints in Eq. 1 and Eq. 2 does not guarantee good visual quality, leading to privacy-preserving but useless synthetic samples. Thus, we add a third objective that enforces class-consistency on trajectory points. We employ an additional auxiliary classification network œÜ class , trained to perform classification on the original dataset, to ensure that sampled latent points share the same visual properties (i.e., the same class) of w a and w b . The corresponding loss L class is as follows:</p><formula xml:id="formula_2">L class = T i=1 CE [œÜ class (G( wi )), y] (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where CE is the cross-entropy between the predicted label for each sample and the target class label y.</p><p>Overall, the total loss for privacy-preserving latent navigation is obtained as:</p><formula xml:id="formula_4">L PLAN = L dist + Œª 1 L id + Œª 2 L label (4)</formula><p>where Œª 1 and Œª 2 weigh the three contributions.</p><p>In a practical application, we employ PLAN in conjunction with a privacypreserving method that produces synthetic samples (e.g., a k-same approach). We then navigate the latent space between random pairs of such samples, and increase the size of the dataset while retaining privacy preservation. The resulting extended set is then used to train a downstream classifier œÜ down on synthetic samples only. Overall, from an input set of N samples, we apply PLAN to N/2 random pairs, thus sampling T N/2 new points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We demonstrate the effectiveness and privacy-preserving properties of our PLAN approach on two classification tasks, namely, tuberculosis classification and diabetic retinopathy (DR) classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and Evaluation Procedure</head><p>Data Preparation. For tuberculosis classification, we employ the Shenzhen Hospital X-ray set<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> that includes 662 frontal chest X-ray images (326 negatives and 336 positives). For diabetic retinopathy classification, we use the APTOS fundus image dataset <ref type="bibr" target="#b12">[13]</ref> of retina images labeled by ophthalmologists with five grades of severity. We downsample it by randomly selecting 950 images, equally distributed among classes, to simulate a typical scenario with low data availability (as in medical applications), where GAN-based synthetic sampling, as a form of augmentation, is more needed. All images are resized to 256 √ó 256 and split into train, validation and test set with 70%, 10%, 20% proportions.</p><p>Baseline Methods. We evaluate our approach from a privacy-preserving perspective and by its capability to support downstream classification tasks. For the former, given the lack of existing methods for privacy-preserving GAN latent navigation, we compare PLAN to standard linear interpolation. After assessing privacy-preserving performance, we measure the impact of our PLAN sampling strategy when combined to k-SALSA <ref type="bibr" target="#b8">[9]</ref> and the latent cluster interpolation approach from <ref type="bibr" target="#b17">[18]</ref> (LCI in the following) on the two considered tasks. Implementation Details. We employ StyleGAN2-ADA <ref type="bibr" target="#b10">[11]</ref> as GAN model for all baselines, trained in a label-conditioned setting on the original training sets. For all classifiers (œÜ id , œÜ class and œÜ down ) we employ a ResNet-18 network <ref type="bibr" target="#b5">[6]</ref>. Classifiers œÜ id and œÜ class are trained on the original training set, while œÜ down (i.e., the task classifier, one for each task) is trained on synthetic samples only. For œÜ id , we apply standard data augmentation (e.g., horizontal flip, rotation) and add five GAN projections for each identity, to mitigate the domain shift between real and synthetic images. œÜ down is trained with a learning rate of 0.001, a batch size of 32, for 200 (Shenzhen) and 500 (APTOS) epochs. Model selection is carried out at the best validation accuracy, and results are averaged over 5 runs. When applying PLAN on a pair of latent points, we initialize a trajectory of T = 50 points through linear interpolation, and optimize Eq. 4 for 100 steps using Adam with a learning rate of 0.1; Œª 1 and Œª 2 are set to 0.1 and 1, respectively. Experiments are performed on an NVIDIA RTX 3090.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>To measure the privacy-preserving properties of our approach, we employ the membership inference attack (MIA) <ref type="bibr" target="#b19">[20]</ref>, which attempts to predict if a sample was used in a classifier's training set. We use attacker model and settings defined in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, training the attacker on 30% of the training set (seen by PLAN through œÜ id and œÜ class ) and 30% of the test set (unseen by PLAN); as a test set for MIA, we reserve 60% of the original test set, leaving 10% as a validation set to select the best attacker. Ideally, if the model preserves privacy, the attacker achieves chance performance (50%), showing inability to identify samples used for training. We also report the FID of the generated dataset, to measure its level of realism, and the mean of the minimum LPIPS <ref type="bibr" target="#b24">[25]</ref> ("mmL" for short) distances between each generated sample and its closest real image, to measure how generated samples differ from real ones. We compare PLAN to a linear interpolation between arbitrary pairs of start and end latent points, and compute the above measures on the images corresponding to the latent trajectories obtained by two approaches. We also report the results of the classifier trained on real data to provide additional bounds for both classification accuracy and privacy-preserving performance.</p><p>Results in Table <ref type="table" target="#tab_0">1</ref> demonstrate that our approach performs similarly to training with real data, but with higher accuracy with respect to the linear baseline. Privacy-preserving results, measured through MIA and mmL, demonstrate the reliability of our PLAN strategy in removing sensitive information, reaching the ideal lower bound of MIA accuracy.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows how, for given start and end points, PLAN-generated samples keep high quality but differ significantly from real samples, while latent linear interpolation may lead to near-duplicates. This is confirmed by the higher LPIPS distance between generated samples and the most similar real samples for PLAN. After verifying the generative and privacy-preserving capabilities of our approach, we evaluate its contribution to classification accuracy when combined with existing k-same methods, namely k-SALSA <ref type="bibr" target="#b8">[9]</ref> and LCI <ref type="bibr" target="#b17">[18]</ref>. Both methods apply latent clustering to synthesize a privacy-preserving dataset, but exhibit low performance transferability to classification tasks, due to the reduced size of the resulting synthetic dataset. We carry out these experiments on APTOS, using k = 5 and k = 10, for comparison with <ref type="bibr" target="#b8">[9]</ref> 2 . Results are given in Table <ref type="table" target="#tab_1">2</ref> and show how our PLAN strategy enhances performance of the two baseline methods, reaching performance similar to training the retinopathy classifier with real samples (i.e., 50.74 on real data vs 44.95 when LCI <ref type="bibr" target="#b17">[18]</ref> is combined with PLAN) and much higher than the variants without PLAN. We also measured MIA accuracy between the variants with and without PLAN, and we did not observe significant change among the different configurations: accuracy was at the chance level in all cases, suggesting their privacy-preserving capability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented PLAN, a latent space navigation strategy designed to reduce privacy risks when using GANs for training models on synthetic data. Experimental results, on two medical image analysis tasks, demonstrate how PLAN is robust to membership inference attacks while effectively supporting model training with performance comparable to training on real data. Furthermore, when PLAN is combined with state-of-the-art k-anonymity methods, we observe a mitigation of performance drop while maintaining privacy-preservation properties. Future research directions will address the scalability of the method to large datasets with a high number of identities, as well as learning latent trajectories with arbitrary length to maximize privacy-preserving and augmentation properties of the synthetic datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the PLAN approach. Using real samples, we train a GAN, an identity classifier œÜ id and an auxiliary classifier œÜ class . Given two arbitrary latent points, wa and w b , PLAN employs œÜ id and œÜ class to gain information on latent space structure and generate a privacy-preserving navigation path (right image), from which synthetic samples can be sampled (far right images, zoom-in for details).</figDesc><graphic coords="3,42,30,394,43,339,40,139,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Linear vs PLAN navigation between two arbitrary points. For each step of the latent trajectory, we compute the LPIPS distance between each synthetic sample and its closest real image. On the right, a qualitative comparison of images at step 35 and their closest real samples: the synthetic image obtained with PLAN differs significantly from its closest real sample; in linear interpolation, synthetic and real samples look similar. Bottom images show synthetic samples generated by linear interpolation and PLAN at the same steps (zoom-in for details).</figDesc><graphic coords="8,56,46,54,14,339,28,173,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between the downstream classifier (œÜ down ) model trained with real samples and those trained with synthetic samples generated from the linear path and privacy path, respectively.</figDesc><table><row><cell></cell><cell>Shenzhen</cell><cell></cell><cell></cell><cell>Aptos</cell></row><row><cell></cell><cell>Acc. (%)(‚Üë) MIA (‚Üì)</cell><cell cols="3">FID (‚Üì) mmL (‚Üë) Acc. (%)(‚Üë) MIA (‚Üì)</cell><cell>FID (‚Üì) mmL (‚Üë)</cell></row><row><cell>Real</cell><cell cols="2">81.23¬±1.03 71.41¬±3.59 -</cell><cell>-</cell><cell cols="2">50.74¬±2.85 73.30¬±4.04 -</cell><cell>-</cell></row><row><cell cols="3">Linear 82.14¬±1.40 56.28¬±1.60 63.85</cell><cell cols="3">0.125 41.58¬±2.11 50.53¬±3.06 85.17 0.118</cell></row><row><cell cols="3">PLAN 83.85¬±1.33 50.13¬±3.99 63.22</cell><cell cols="3">0.159 46.95¬±3.06 48.51¬±2.85 90.81</cell><cell>0.131</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Impact of our navigation strategy on k-same methods on the APTOS dataset. Performance are reported in terms of accuracy.</figDesc><table><row><cell cols="2">k-SALSA [9] k-SALSA LCI [18]</cell><cell>LCI</cell></row><row><cell></cell><cell>+PLAN</cell><cell>+PLAN</cell></row><row><cell>k = 5 25.58¬±6.32</cell><cell cols="2">36.59¬±3.48 38.74¬±4.51 43.16¬±2.71</cell></row><row><cell>k = 10 27.47¬±3.42</cell><cell cols="2">34.21¬±1.62 36.42¬±3.77 44.95¬±1.61</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This dataset was released by the National Library of Medicine, NIH, Bethesda, USA.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was supported by <rs type="funder">PNRR</rs> <rs type="projectName">MUR</rs> project <rs type="grantNumber">PE0000013-FAIR</rs>. <rs type="person">Matteo Pennisi</rs> is a PhD student enrolled in the National PhD in Artificial Intelligence, XXXVII cycle, course on Health and life sciences, organized by <rs type="funder">Universit√† Campus Bio-Medico di Roma</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_sxv2cs9">
					<idno type="grant-number">PE0000013-FAIR</idno>
					<orgName type="project" subtype="full">MUR</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Refacing: reconstructing anonymized facial features using GANS</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abramian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eklund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Symposium on Biomedical Imaging, ISBI 2019</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-04-11">8-11 April 2019. 2019</date>
			<biblScope unit="page" from="1104" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ReStyle: a residual-based styleGAN encoder via iterative refinement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6711" to="6720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A technique for the deidentification of structural brain MR images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bischoff-Grethe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="892" to="903" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Candemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="577" to="590" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two public chest X-ray datasets for computer-aided screening of pulmonary diseases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Candemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X J</forename><surname>W√°ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Imaging Med. Surg</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">475</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic tuberculosis screening using chest radiographs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">k-SALSA: k-anonymous synthetic averaging of retinal images via local style alignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19803-8_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19803-8" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ciss√©</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13681</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MemGuard: defending against black-box membership inference attacks via adversarial examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="259" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of styleGAN</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Aptos 2019 blindness detection</title>
		<author>
			<persName><forename type="first">Maggie</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename></persName>
		</author>
		<ptr target="https://kaggle.com/competitions/aptos2019-blindness-detection" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Few shot learning for medical imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kotwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mangrulkar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-50641-4_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-50641-47" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning Algorithms for Industrial Applications</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Das</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Dey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A.-E</forename><surname>Hassanien</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">907</biblScope>
			<biblScope unit="page" from="107" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">k-Same-Net: k-anonymity with generative deep neural networks for face deidentification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Meden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">≈Ω</forename><surname>Emer≈°iƒç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>≈†truc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine learning with membership privacy using adversarial regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Houmansadr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2018 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="634" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-improving classification performance through GAN distillation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pennisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page" from="1640" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GAN latent space manipulation and aggregation for federated learning in medical imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pennisi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-18523-6_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-18523-67" />
	</analytic>
	<monogr>
		<title level="m">Distributed, Collaborative, and Federated Learning, and Affordable AI and Healthcare for Resource Diverse Global Health, DeCaF FAIR 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13573</biblScope>
			<biblScope unit="page" from="68" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An overview of face de-identification in still images and videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ribaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pavesic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">04</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">k-anonymity: a model for protecting privacy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Uncertain. Fuzziness Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="557" to="570" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GANobfuscator: mitigating information leakage under GAN via differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2358" to="2371" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PATE-GAN: generating synthetic data with differential privacy guarantees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Zech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Badgeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Titano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Oermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Med</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1002683</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
