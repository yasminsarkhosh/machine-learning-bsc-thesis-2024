<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images</title>
				<funder ref="#_mm2RwX2 #_YMNJSnC">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_EkdGrjz">
					<orgName type="full">Natural Science Foundation of Shanghai</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yufei</forename><surname>Chen</surname></persName>
							<email>yufeichen@tongji.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Yue</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Artificial Intelligence Institute of Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">VLN Lab</orgName>
								<orgName type="institution">NAVI MedTech Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electronics and Information Engineering</orgName>
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Changhai Hospital of Shanghai</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="305" to="315"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">D2721B0CD8AB3FED5BBD7F748915918A</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Near out-of-distribution detection</term>
					<term>Uncertainty estimation</term>
					<term>Reconciled evidence representation</term>
					<term>Evidential neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Near Out-of-Distribution (OOD) detection is a crucial issue in medical applications, as misdiagnosis caused by the presence of rare diseases inevitablely poses a significant risk. Recently, several deep learning-based methods for OOD detection with uncertainty estimation, such as the Evidential Deep Learning (EDL) and its variants, have shown remarkable performance in identifying outliers that significantly differ from training samples. Nevertheless, few studies focus on the great challenge of near OOD detection problem, which involves detecting outliers that are close to the training distribution, as commonly encountered in medical image application. To address this limitation and reduce the risk of misdiagnosis, we propose an Evidence Reconciled Neural Network (ERNN). Concretely, we reform the evidence representation obtained from the evidential head with the proposed Evidential Reconcile Block (ERB), which restricts the decision boundary of the model and further improves the performance in near OOD detection. Compared with the state-of-the-art uncertainty-based methods for OOD detection, our method reduces the evidential error and enhances the capability of near OOD detection in medical applications. The experiments on both the ISIC2019 dataset and an in-house pancreas tumor dataset validate the robustness and effectiveness of our approach. Code for ERNN has been released at https://github.com/KellaDoe/ERNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Detecting out-of-distribution (OOD) samples is crucial in real-world applications of machine learning, especially in medical imaging analysis where misdiagnosis can pose significant risks <ref type="bibr" target="#b6">[7]</ref>. Recently, deep neural networks, particularly ResNets <ref type="bibr" target="#b8">[9]</ref> and U-Nets <ref type="bibr" target="#b14">[15]</ref>, have been widely used in various medical imaging applications such as classification and segmentation tasks, achieving state-ofthe-art performance. However, due to the typical overconfidence seen in neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, deep learning with uncertainty estimation is becoming increasingly important in OOD detection.</p><p>Deep learning-based OOD detection methods with uncertainty estimation, such as Evidential Deep Learning (EDL) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref> and its variants <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b23">24]</ref>, have shown their superiority in terms of computational performance, efficiency, and extensibility. However, most of these methods consider identifying outliers that significantly differ from training samples(e.g. natural images collected from ImageNet <ref type="bibr" target="#b4">[5]</ref>) as OOD samples <ref type="bibr" target="#b0">[1]</ref>. These approaches overlook the inherent near OOD problem in medical images, in which instances belong to categories or classes that are not present in the training set <ref type="bibr" target="#b20">[21]</ref> due to the differences in morbidities. Failing to detect such near OOD samples poses a high risk in medical application, as it can lead to inaccurate diagnoses and treatments. Some recent works have been proposed for near OOD detection based on density models <ref type="bibr" target="#b19">[20]</ref>, preprocessing <ref type="bibr" target="#b13">[14]</ref>, and outlier exposure <ref type="bibr" target="#b15">[16]</ref>. Nevertheless, all of these approaches are susceptible to the quality of the training set, which cannot always be guaranteed in clinical applications.</p><p>To address this limitation, we propose an Evidence Reconciled Neural Network (ERNN), which aims to reliably detect those samples that are similar to the training data but still with different distributions (near OOD), while maintain accuracy for In-Distribution (ID) classification. Concretely, we introduce a module named Evidence Reconcile Block (ERB) based on evidence offset. This module cancels out the conflict evidences obtained from the evidential head, maximizes the uncertainty of derived opinions, thus minimizes the error of uncertainty calibration in OOD detection. With the proposed method, the decision boundary of the model is restricted, the capability of medical outlier detection is improved and the risk of misdiagnosis in medical images is mitigated. Extensive experiments on both ISIC2019 dataset and in-house pancreas tumor dataset demonstrate that the proposed ERNN significantly improves the reliability and accuracy of OOD detection for clinical applications. Code for ERNN can be found at https://github.com/KellaDoe/ERNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we introduce our proposed Evidence Reconciled Neural Network (ERNN) and analyze its theoretical effectiveness in near OOD detection. In our approach, the evidential head firstly generates the original evidence to support the classification of each sample into the corresponding class. And then, the proposed Evidence Reconcile Block (ERB) is introduced, which reforms the derived evidence representation to maximize the uncertainty in its relevant opinion and better restrict the model decision boundary. More details and theorical analysis of the model are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Evidence Generation</head><p>Traditional classifiers typically employ a softmax layer on top of feature extractor to calculate a point estimation of the classification result. However, the point estimates of softmax only ensure the accuracy of the prediction but ignore the confidence of results. To address this problem, EDL utilizes the Dirichlet distribution as the conjugate prior of the categorical distribution and replaces the softmax layer with an evidential head which produces a non-negative output as evidence and formalizes an opinion based on evidence theory to explicitly express the uncertainty of generated evidence.  </p><formula xml:id="formula_0">= e S = α -1 S , u = K S ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">S = K k=1 (e k +1) = K k=1 α k is the Dirichlet strength.</formula><p>Based on the fact that the parameters of the categorical distribution should obey Dirichlet distribution, the model prediction ŷ and the expected cross entropy loss L ece on Dirichlet distributioncan be inferred as:</p><formula xml:id="formula_2">ŷ = E Dir(p|α ) E cat(y |p) p = α/S</formula><p>(2)</p><formula xml:id="formula_3">L ece = E Dir(p|α ) K i=1 y i log p i = K i=1 y i (ψ(S) -ψ(α i )).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evidence Reconcile Block</head><p>In case of OOD detection, since the outliers are absent in the training set, the detection is a non-frequentist situation. Referring to the subjective logic <ref type="bibr" target="#b9">[10]</ref>, when a variable is not governed by a frequentist process, the statical accumulation of supporting evidence would lead to a reduction in uncertainty mass. Therefore, traditional evidence generated on the basis accumulation is inapplicable and would lead to bad uncertainty calibration in OOD detection. Moreover, the higher the similarity between samples, the greater impact of evidence accumulation, which results in a dramatic performance degradation in medical near OOD detection.</p><p>To tackle the problem mentioned above, we propose an Evidence Reconcile Block (ERB) that reformulates the representation of original evidence and minimizes the deviation of uncertainty in evidence generation. In the proposed ERB, different pieces of evidence that support different classes are canceled out by transforming them from subjective opinion to epistemic opinion and the theoretical maximum uncertainty mass is obtained.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the simplex corresponding to K-class opinions has K dimensions corresponding to each category and an additional dimension representing the uncertainty in the evidence, i.e., vacuity in EDL. For a given opinion ω, its projected predictive probability is shown as p with the direction determined by prior a. To ensure the consistency of projection probabilities, epistemic opinion ω should also lie on the direction of projection and satisfy that at least one belief mass of ω is zero, corresponding to a point on a side of the simplex. Let ü denotes the maximum uncertainty, it should satisfy:</p><formula xml:id="formula_4">ü = min i p i a i , for i ∈ {1, . . . , K}, (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Since a is a uniform distribution defined earlier, the transformed belief mass can be calculated as: b = bb min , where b min is the minimum value in the original belief mass b. Similarly, the evidence representation ë in our ERB, based on epistemic opinion ω, can be formulated as:</p><formula xml:id="formula_6">ë = e -min i [ e i a i</formula><p>]a = e -min i e, f or i ∈ {1, . . . , K}.</p><p>(</p><p>After the transformation by ERB, the parameters α = ë + 1 of Dirichlet distribution associate with the reconciled evidence can be determined, and the reconciled evidential cross entropy loss L rece can be inferred as <ref type="bibr" target="#b5">(6)</ref>, in which S = K i=1 αi .</p><formula xml:id="formula_8">L rece = E Dir(p| α ) K k=1 y i log p i = K i=1 y i (ψ( S) -ψ(α i )). (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>By reconciling the evidence through the transformation of epistemic opinion in subjective logic, this model can effectively reduce errors in evidence generation caused by statistical accumulation. As a result, it can mitigate the poor uncertainty calibration in EDL, leading to better error correction and lower empirical loss in near OOD detection, as analysized in Sect. 2.3. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, we utilize samples from three Gaussian distributions to simulate a 3-classification task and generate evidences based on the probability density of each class. When using the traditional CNN to measure the uncertainty of the output with predictive entropy, the model is unable to distinguish far OOD due to the normalization of softmax. While the introduction of evidence representation in the vacuity of EDL allows effective far OOD detections. However, due to the aforementioned impact of evidence accumulation, we observe that the EDL has a tendency to produce small uncertainties for outliers close to in-distribution (ID) samples, thus leading to failures in detecting near OOD samples. Our proposed method combines the benefits of both approaches, the evidence transformed by ERB can output appropriate uncertainty for both near and far OOD samples, leading to better identification performance for both types of outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Theorical Analysis of ERB</head><p>To further analyze the constraint of the proposed model in OOD detection, we theoretically analyze the difference between the loss functions before and after the evidence transformation, as well as why it can improve the ability of near OOD detection. Detailed provements of following propositons are provided in Supplements.</p><p>Proposition 1. For a given sample in K-classification with the label c and</p><formula xml:id="formula_10">K i=1 α i = S, for any α c ≤ S K , L rece &gt; L ece is satisfied.</formula><p>The misclassified ID samples with p c = α c /S ≤ 1/K are often located at the decision boundary of the corresponding categories. Based on Proposition 1, the reconciled evidence can generate a larger loss, which helps the model focus more on the difficult-to-distinguish samples. Therefore, the module can help optimize the decision boundary of ID samples, and promote the ability to detect near OOD.</p><p>Proposition 2. For a given sample in K-classification with the label c and K i=1 α i = S, for any α c &gt; S K , L rece &lt; L ece is satisfied. Due to the lower loss derived from the proposed method, we achieve better classification accuracy and reduce empirical loss, thus the decision boundary can be better represented for detecting outliers. Proposition 3. For a given sample in K-classification and Dirichlet distribution parameter α, when all values of α equal to const α, L rece ≥ L ece is satisfied.</p><p>During the training process, if the prediction p of ID samples is identical to the ideal OOD outputs, the proposed method generates a greater loss to prevent such evidence from occurring. This increases the difference in predictions between ID and OOD samples, thereby enhancing the ability to detect OOD samples using prediction entropy.</p><p>In summary, the proposed Evidence Reconciled Neural Network (ERNN) optimizes the decision boundary and enhances the ability to detect near OOD samples. Specifically, our method improves the error-correcting ability when the probability output of the true label is no more than 1/K, and reduces the empirical loss when the probability output of the true label is greater than 1/K. Furthermore, the proposed method prevents model from generating same evidence for each classes thus amplifying the difference between ID and OOD samples, resulting in a more effective near OOD detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets. We conduct experiments on ISIC 2019 dataset <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref> and an inhouse dataset. ISIC 2019 consists of skin lesion images in JPEG format, which are categorized into NV (12875), MEL (4522), BCC (3323), BKL (2624), AK (867), SCC (628), DF (239) and VASC (253), with a long-tailed distribution of classes. In line with the settings presented in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, we define DF and VASC, for which samples are relatively scarce as the near-OOD classes. The in-house pancreas tumor dataset collected from a cooperative hospital is composed of eight classes: PDAC (302), IPMN (71), NET (43), SCN (37), ASC (33), CP <ref type="bibr" target="#b5">(6)</ref>, MCN (3), and PanIN (1). For each sequence, CT slices with the largest tumor area are picked for experiment. Similarly, PDAC, IPMN and NET are chosen as ID classes, while the remaining classes are reserved as OOD categories.</p><p>Implementations and Evaluation Metrics. To ensure fairness, we used pretrained Resnet34 <ref type="bibr" target="#b8">[9]</ref> as backbone for all methods. During our training process, the images were first resized to 224 × 224 pixels and normalized, then horizontal and vertical flips were applied for augmentation. The training was performed using one GeForce RTX 3090 with a batch size of 256 for 100 epochs using the AdamW optimizer with an initial learning rate of 1e-4 along with exponential decay. Note that we employed five-fold cross-validation on all methods, without using any additional OOD samples during training. Furthermore, we selected the precision(pre), recall(rec), and f1-score(f1) as the evaluation metrics for ID samples, and used the Area Under Receiver Operator Characteristic (AUROC) as OOD evaluation metric, in line with the work of <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with the Methods</head><p>In the experiment, we compare the OOD detection performance of ERNN to several uncertainty-based approaches:</p><p>• Prototype Network described in <ref type="bibr" target="#b21">[22]</ref>, where the prototypes of classes are introduced and the distance is utilized for uncertainty estimation. • Prior Network described in <ref type="bibr" target="#b11">[12]</ref>, in which the second order dirichlet distribution is utlized to estimate uncertainty. • Evidential Deep Learning described in <ref type="bibr" target="#b16">[17]</ref>, introduces evidence representation and estimates uncertainty through subjective logic. • Posterior Network described in <ref type="bibr" target="#b1">[2]</ref>, where density estimators are used for generating the parameters of dirichlet distributions.</p><p>Inspired by <ref type="bibr" target="#b13">[14]</ref>, we further compare the proposed method with Mixup-based methods:</p><p>• Mixup: As described in <ref type="bibr" target="#b22">[23]</ref>, mix up is applied to all samples.</p><p>• MT-mixup: Mix up is only applied to mid-class and tail-class samples.</p><p>• MTMX-Prototype: On the basis of MT mixup, prototype network is also applied to estimate uncertainty.</p><p>The results on two datasets are shown in Table <ref type="table" target="#tab_0">1</ref>. We can clearly observe that ERNN consistently achieves better OOD detection performance than other uncertainty-based methods without additional data augmentation. Even with using Mixup, ERNN exhibits near performance with the best method (MTMX-Prototype) on ISIC 2019 and outperforms the other methods on in-house datasets. All of the experimental results verify that our ERNN method improves OOD detection performance while maintaining the results of ID classification even without any changes to the existing architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>In this section, we conduct a detailed ablation study to clearly demonstrate the effectiveness of our major technical components, which consist of evaluation of evidential head, evaluation of the proposed Evidence Reconcile Block on both ISIC 2019 dataset and our in-house pancreas tumor dataset. Since the Evidence Reconcile Block is based on the evidential head, thus there are four combinations, but only three experimental results were obtained. As shown in Table <ref type="table" target="#tab_1">2</ref>, It is clear that a network with an evidential head can improve the OOD detection capability by 6% and 1% on ISIC dataset and in-house pancreas tumor dataset respectively. Furthermore, introducing ERB further improves the OOD detection performance of ERNN by 1% on ISIC dataset. And on the more challenging inhouse dataset, which has more similarities in samples, the proposed method improves the AUROC by 2.3%, demonstrating the effectiveness and robustness of our model on more challenging tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose a simple and effective network named Evidence Reconciled Nueral Network for medical OOD detection with uncertainty estimation, which can measure the confidence in model prediction. Our method addresses the failure in uncertainty calibration of existing methods due to the similarity of near OOD with ID samples. With the evidence reformation in the proposed Evidence Reconcile Block, the error brought by accumulative evidence generation can be mitigated. Compared to existing state-of-the-art methods, our method can achieve competitive performance in near OOD detection with less loss of accuracy in ID classification. Furthermore, the proposed plug-and-play method can be easily applied without any changes of network, resulting in less computation cost in identifying outliers. The experimental results validate the effectiveness and robustness of our method in the medical near OOD detection problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Workflow of the proposed Evidence Reconciled Neural Network. In the Dirichlet distribution derived from evidence, the greater the predictive entropy (i.e., uncertainty), the closer the distribution expectation is to the center.</figDesc><graphic coords="3,65,46,244,46,312,82,131,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Formally, the evidence</head><label></label><figDesc>for K-classification task uniquely associates with a multinomial opinion ω = (b, u, a) which can be visualized shown in Fig 1 (in case of K = 3). In this opinion, b = (b 1 , . . . , b K ) T represents the belief degree, a = (a 1 , . . . , a K ) T indicates the prior preference of the model over classes, and u denotes the overall uncertainty of generated evidence which is also called vacuity in EDL. The triplet should satisfy K k=1 b k + u = 1, and the prediction probability can be defined as p k = b k + a k u. Typically, in OOD detection tasks, all values of the prior a are set to 1/K with a non-informative uniform distribution. Referring to subjective logic [10], the opinion ω can be mapped into a Dirichlet distribution Dir (p |α ), where α = (α 1 , ..., α K ) T represents the Dirichlet parameters and we have α = e + aK, e = (e 1 , . . . , e K ) T represents the evidence that is a measure of the amount of support collected from data in favor of a sample to be classified into known categories. When there is no preference over class, the Dirichlet parameters α = e + 1, then the belief mass and uncertainty are calculated as: b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Uncertainty estimations for OOD detection in (a) Classical CNN, (b) EDL and (c) proposed ERNN on synthetic gaussian data. The red, green and blue points denote the samples of 3 different classes that follow Gaussian distributions. The blue area represents samples with higher uncertainty (i.e., OOD samples), while the red area represents samples with lower uncertainty (i.e., ID samples). (Color figure online)</figDesc><graphic coords="5,60,48,334,97,332,02,78,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with other methods. ID metrics -Precision (pre), Recall (rec), and f1-score(f1); OOD metric -AUROC(%).</figDesc><table><row><cell></cell><cell cols="2">ISIC2019</cell><cell></cell><cell></cell><cell cols="3">In-house Pancreas tumor</cell><cell></cell></row><row><cell></cell><cell cols="8">ID(pre) ID(rec) ID(f1) OOD(AUROC) ID(pre) ID(rec) ID(f1) OOD(AUROC)</cell></row><row><cell>Baseline [9]</cell><cell>0.86</cell><cell>0.86</cell><cell>0.86</cell><cell>68.15</cell><cell>0.76</cell><cell>0.78</cell><cell>0.76</cell><cell>54.39</cell></row><row><cell cols="2">Prototype Networks [22] 0.85</cell><cell>0.86</cell><cell>0.86</cell><cell>72.84</cell><cell>0.75</cell><cell>0.75</cell><cell>0.74</cell><cell>52.86</cell></row><row><cell>Prior Networks [12]</cell><cell>0.87</cell><cell>0.87</cell><cell>0.87</cell><cell>74.54</cell><cell>0.72</cell><cell>0.72</cell><cell>0.71</cell><cell>49.79</cell></row><row><cell>EDL [17]</cell><cell>0.88</cell><cell>0.88</cell><cell cols="2">0.88 72.51</cell><cell>0.78</cell><cell>0.79</cell><cell cols="2">0.78 55.39</cell></row><row><cell>Posterior Networks [2]</cell><cell>0.36</cell><cell>0.51</cell><cell>0.35</cell><cell>57.50</cell><cell>0.74</cell><cell>0.72</cell><cell>0.73</cell><cell>47.25</cell></row><row><cell>ERNN(ours)</cell><cell>0.88</cell><cell>0.88</cell><cell cols="2">0.88 75.11</cell><cell>0.78</cell><cell>0.77</cell><cell>0.77</cell><cell>57.63</cell></row><row><cell>Mixup [23]</cell><cell>0.87</cell><cell>0.88</cell><cell cols="2">0.88 71.72</cell><cell>0.78</cell><cell>0.79</cell><cell cols="2">0.76 56.32</cell></row><row><cell>MT-mixup [14]</cell><cell>0.85</cell><cell>0.85</cell><cell>0.86</cell><cell>73.86</cell><cell>0.76</cell><cell>0.76</cell><cell>0.74</cell><cell>58.50</cell></row><row><cell cols="2">MTMX-Prototype [14] 0.85</cell><cell>0.86</cell><cell>0.86</cell><cell>76.37</cell><cell>0.75</cell><cell>0.77</cell><cell>0.74</cell><cell>54.18</cell></row><row><cell>MX-ERNN(ours)</cell><cell>0.86</cell><cell>0.87</cell><cell>0.86</cell><cell>76.34</cell><cell>0.78</cell><cell>0.74</cell><cell>0.71</cell><cell>60.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study. We present f1 metric for ID validation and AUROC metric for OOD detection on both ISIC 2019 and in-house Pancreas tumor dataset. " " means ERNN with the corresponding component, "-" means "not applied".</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">ISIC2019</cell><cell cols="2">In-house</cell></row><row><cell cols="6">Backbone EH ERB ID(f1) OOD(AUROC) ID(f1) OOD(AUROC)</cell></row><row><cell>-</cell><cell>-</cell><cell>0.86</cell><cell>68.15</cell><cell>0.76</cell><cell>54.39</cell></row><row><cell></cell><cell></cell><cell cols="2">0.88 74.15</cell><cell cols="2">0.78 55.39</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">0.88 75.11</cell><cell>0.77</cell><cell>57.63</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62173252</rs>, <rs type="grantNumber">61976134</rs>), and <rs type="funder">Natural Science Foundation of Shanghai</rs> (No. <rs type="grantNumber">21ZR1423900</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mm2RwX2">
					<idno type="grant-number">62173252</idno>
				</org>
				<org type="funding" xml:id="_YMNJSnC">
					<idno type="grant-number">61976134</idno>
				</org>
				<org type="funding" xml:id="_EkdGrjz">
					<idno type="grant-number">21ZR1423900</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_30.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Confidence-Based Out-of-Distribution Detection: A Comparative Study and Analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87735-4_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87735-4_12" />
	</analytic>
	<monogr>
		<title level="m">UNSURE/PIPPI -2021</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12959</biblScope>
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Posterior network: uncertainty estimation without OOD samples via density-based pseudo-counts</title>
		<author>
			<persName><forename type="first">B</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1356" to="1367" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hosted by the International skin Imaging Collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Skin lesion analysis toward melanoma detection: a challenge at the 2017 International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
	<note>IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02288</idno>
		<title level="m">Bcn20000: dermoscopic lesions in the wild</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>CVPR09</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recent advances in open set recognition: A survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3614" to="3631" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transfer Learning for Domain Adaptation in MRI: Application in Brain Lesion Segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-7_59" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Duchesne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page" from="516" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Subjective logic</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jøsang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-42337-1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-42337-1" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Trusted multi-view deep learning with opinion aggregation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="7585" to="7593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reverse kl-divergence training of prior networks: improved uncertainty and adversarial robustness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Information Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection for long-tailed and fine-grained skin lesion images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_69</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-6_69" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="732" to="742" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Does your dermatology classifier know what it doesn&apos;t know? detecting the long-tail of unseen conditions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102274</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Information Process. Syst</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On mixup training: improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Information Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trust issues: uncertainty estimation does not enable reliable OOD detection on medical tabular data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meijerink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cinà</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Health</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="341" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05566</idno>
		<title level="m">Contrastive training for improved out-of-distribution detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust classification with convolutional prototype learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3474" to="3482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">Mixup: beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06864</idno>
		<title level="m">Quantifying classification uncertainty using regularized evidential neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
