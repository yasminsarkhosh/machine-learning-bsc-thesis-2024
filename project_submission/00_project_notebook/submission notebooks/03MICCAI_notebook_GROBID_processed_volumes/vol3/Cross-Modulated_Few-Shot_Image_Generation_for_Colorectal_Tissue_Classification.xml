<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification</title>
				<funder>
					<orgName type="full">MVR Cancer Center and Research Institute, Kozhikode, India</orgName>
				</funder>
				<funder ref="#_spEQdtw">
					<orgName type="full">Aster Mother Hospital Kozhikode, India</orgName>
				</funder>
				<funder>
					<orgName type="full">North Devon District Hospital, UK</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Amandeep</forename><surname>Kumar</surname></persName>
							<email>amandeep.kumar@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="institution">MBZUAI</orgName>
								<address>
									<settlement>Masdar City</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ankan</forename><forename type="middle">Kumar</forename><surname>Bhunia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sanath</forename><surname>Narayan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Technology Innovation Institute</orgName>
								<address>
									<settlement>Masdar City</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MBZUAI</orgName>
								<address>
									<settlement>Masdar City</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rao</forename><forename type="middle">Muhammad</forename><surname>Anwer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MBZUAI</orgName>
								<address>
									<settlement>Masdar City</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<settlement>Espoo</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jorma</forename><surname>Laaksonen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Aalto University</orgName>
								<address>
									<settlement>Espoo</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MBZUAI</orgName>
								<address>
									<settlement>Masdar City</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Linköping University</orgName>
								<address>
									<settlement>Linköping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="128" to="137"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">85F777D18C05A76808D6134A191BBBDD</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_13</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Few-shot Image generation • Cross Modulation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a few-shot colorectal tissue image generation method for addressing the scarcity of histopathological training data for rare cancer tissues. Our few-shot generation method, named XM-GAN, takes one base and a pair of reference tissue images as input and generates high-quality yet diverse images. Within our XM-GAN, a novel controllable fusion block densely aggregates local regions of reference images based on their similarity to those in the base image, resulting in locally consistent features. To the best of our knowledge, we are the first to investigate few-shot generation in colorectal tissue images. We evaluate our few-shot colorectral tissue image generation by performing extensive qualitative, quantitative and subject specialist (pathologist) based evaluations. Specifically, in specialist-based evaluation, pathologists could differentiate between our XM-GAN generated tissue images and real images only 55% time. Moreover, we utilize these generated images as data augmentation to address the few-shot tissue image classification task, achieving a gain of 4.4% in terms of mean accuracy over the vanilla few-shot classifier. Code: https://github.com/VIROBO-15/XM-GAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Histopathological image analysis is an important step towards cancer diagnosis. However, shortage of pathologists worldwide along with the complexity of histopathological data make this task time consuming and challenging. Therefore, developing automatic and accurate histopathological image analysis methods that leverage recent progress in deep learning has received significant attention in recent years. In this work, we investigate the problem of diagnosing colorectal cancer, which is one of the most common reason for cancer deaths around the world and particularly in Europe and America <ref type="bibr" target="#b22">[23]</ref>.</p><p>Existing deep learning-based colorectal tissue classification methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22</ref>] typically require large amounts of annotated histopathological training data for all tissue types to be categorized. However, obtaining large amount of training data is challenging, especially for rare cancer tissues. To this end, it is desirable to develop a few-shot colorectal tissue classification method, which can learn from seen tissue classes having sufficient training data, and be able to transfer this knowledge to unseen (novel) tissue classes having only a few exemplar training images.</p><p>While generative adversarial networks (GANs) <ref type="bibr" target="#b5">[6]</ref> have been utilized to synthesize images, they typically need to be trained using large amount of real images of the respective classes, which is not feasible in aforementioned few-shot setting. Therefore, we propose a few-shot (FS) image generation approach for generating high-quality and diverse colorectal tissue images of novel classes using limited exemplars. Moreover, we demonstrate the applicability of these generated images for the challenging problem of FS colorectal tissue classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>We propose a few-shot colorectal tissue image generation framework, named XM-GAN, which simultaneously focuses on generating highquality yet diverse images. Within our tissue image generation framework, we introduce a novel controllable fusion block (CFB) that enables a dense aggregation of local regions of the reference tissue images based on their congruence to those in the base tissue image. Our CFB employs a cross-attention based feature aggregation between the base (query) and reference (keys, values) tissue image features. Such a cross-attention mechanism enables the aggregation of reference features from a global receptive field, resulting in locally consistent features. Consequently, colorectal tissue images are generated with reduced artifacts.</p><p>To further enhance the diversity and quality of the generated tissue images, we introduce a mapping network along with a controllable cross-modulated layer normalization (cLN) within our CFB. Our mapping network generates 'metaweights' that are a function of the global-level features of the reference tissue image and the control parameters. These meta-weights are then used to compute the modulation weights for feature re-weighting in our cLN. This enables the cross-attended tissue image features to be re-weighted and enriched in a controllable manner, based on the reference tissue image features and associated control parameters. Consequently, it results in improved diversity of the tissue images generated by our transformer-based framework (see Fig. <ref type="figure" target="#fig_1">3</ref>).</p><p>We validate our XM-GAN on the FS colorectral tissue image generation task by performing extensive qualitative, quantitative and subject specialist (pathologist) based evaluations. Our XM-GAN generates realistic and diverse colorectal tissue images (see Fig. <ref type="figure" target="#fig_1">3</ref>). In our subject specialist (pathologist) based evaluation, pathologists could differentiate between our XM-GAN generated colorectral tissue images and real images only 55% time. Furthermore, we evaluate the effectiveness of our generated tissue images by using them as data augmentation during training of FS colorectal tissue image classifier, leading to an absolute gain of 4.4% in terms of mean classification accuracy over the vanilla FS classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The ability of generative models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> to fit to a variety of data distributions has enabled great strides of advancement in tasks, such as image generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>, and so on. Despite their success, these generative models typically require large amount of data to train and avoid overfitting. In contrast, fewshot (FS) image generation approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref> strive to generate natural images from disjoint novel categories from the same domain as in the training. Existing FS natural image generation approaches can be broadly divided into three categories based on transformation <ref type="bibr" target="#b0">[1]</ref>, optimization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> and fusion <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. The transformation-based approach learns to perform generalized data augmentations to generate intra-class images from a single conditional image. On the other hand, optimization-based approaches typically utilize meta-learning techniques to adapt to a different image generation task by optimizing on a few reference images from the novel domain. Different from these two paradigms that are better suited for simple image generation task, fusion-based approaches first aggregate latent features of reference images and then employ a decoder to generate same class images from these aggregated features.</p><p>Our Approach: While the aforementioned works explore FS generation in natural images, to the best of our knowledge, we are the first to investigate FS generation in colorectal tissue images. In this work, we look into multi-class colorectal tissue analysis problem, with low and high-grade tumors included in the set. The corresponding dataset <ref type="bibr" target="#b13">[14]</ref> used in this study is widely employed for multi-class texture classification in colorectal cancer histology and comprises eight types of tissue: tumor epithelium, simple stroma, complex stroma, immune cells, debris, normal mucosal glands, adipose tissue and background (no tissue). Generating colorectal tissue images of these diverse categories is a challenging task, especially in the FS setting. Generating realistic and diverse tissue images require ensuring both global and local texture consistency (patterns). Our XM-GAN densely aggregates features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> from all relevant local regions of the reference tissue images at a global-receptive field along with a controllable mechanism for modulating the tissue image features by utilizing meta-weights computed from the input reference tissue image features. As a result, this leads to high-quality yet diverse colorectal tissue image generation in FS setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Problem Formulation: In our few-shot colorectal tissue image generation framework, the goal is to generate diverse set of images from K input examples X of a unseen (novel) tissue classes. Let D s and D u be the set of seen and unseen classes, respectively, where D s ∩ D u = ∅. In the training stage, we sample images from D s and train the model to learn transferable generation ability to produce new tissue images for unseen classes. During inference, given K images from an unseen class in D u , the trained model strives to produce diverse yet plausible images for this unseen class without any further fine-tuning. Overall Architecture: Figure <ref type="figure" target="#fig_0">1</ref> shows the overall architecture of our proposed framework, XM-GAN. Here, we randomly assign a tissue image from X as a base image x b , and denote the remaining K-1 tissue images as reference {x ref i } K-1 i=1 . Given the input images X, we obtain feature representation of the base tissue image and each reference tissue image by passing them through the shared encoder F E . Next, the encoded feature representations h are input to a controllable fusion block (CFB), where cross-attention <ref type="bibr" target="#b19">[20]</ref> is performed between the base and reference features, h b and h ref i , respectively. Within our CFB, we introduce a mapping network along with a controllable cross-modulated layer normalization (cLN) to compute meta-weights w i , which are then used to generate the modulation weights used for re-weighting in our cLN. The resulting fused representation f is input to a decoder F D to generate tissue image x. The whole framework is trained following the GAN <ref type="bibr" target="#b16">[17]</ref> paradigm. In addition to L adv and L cl , we propose to use a guided perceptual loss term L p , utilizing the control parameters α i . Next, we describe our CFB in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Controllable Fusion Block</head><p>Figure <ref type="figure">2</ref> shows the architecture of our proposed CFB, comprises of a shared crosstransformer followed by a feature fusion mechanism. Here, the cross-transformer is based on multi-headed cross-attention mechanism that densely aggregates relevant input image features, based on pairwise attention scores between each position in the base tissue image with every region of the reference tissue image. The query embeddings q m ∈ R n×d are computed from the base features h b ∈ R n×D , while keys k m i ∈ R n×d and values v m i ∈ R n×d are obtained from the reference features h ref i ∈ R n×D , where d = D /M with M as the number of attention heads. Next, a cross-attention function maps the queries to outputs r m i using the keyvalue pairs. Finally, the outputs r m i from all M heads are concatenated and processed by a learnable weight matrix W ∈ R D×D to generate cross-attended features c i ∈ R n×D given by</p><formula xml:id="formula_0">c i = [r 1 i ; • • • ; r M i ]W + h b , where r m i = softmax( q m k m i √ d )v m i . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Fig. <ref type="figure">2</ref>. Cross-attending the base and reference tissue image features using controllable cross-modulated layer norm (cLN) in our CFB.</p><p>Here, a reference feature h ref i , noise z and control parameter αi are input to a mapping network for generating meta-weights wi. The resulting wi modulates the features via λ(wi) and β(wi) in our cLN. As a result of this controllable feature modulation, the output features fi enable the generation of tissue images that are diverse yet aligned with the semantics of the input tissue images.</p><p>Next, we introduce a controllable feature modulation mechanism in our crosstransformer to further enhance the diversity and quality of generated images.</p><p>Controllable Feature Modulation: The standard cross-attention mechanism, described above, computes locally consistent features that generate images with reduced artifacts. However, given the deterministic nature of the cross-attention and the limited set of reference images, simultaneously generating diverse and high-quality images in the few-shot setting is still a challenge. To this end, we introduce a controllable feature modulation mechanism within our CFB that aims at improving the diversity and quality of generated images. The proposed modulation incorporates stochasticity as well as enhanced control in the feature aggregation and refinement steps. This is achieved by utilizing the output of a mapping network for modulating the visual features in the layer normalization modules in our crosstransformer.</p><p>Mapping Network: The meta-weights w i ∈ R D are obtained by the mapping network as,</p><formula xml:id="formula_2">w i = g ref i ψ α (α i ) + ψ z (z),<label>(2)</label></formula><p>where ψ α (•) and ψ z (•) are linear transformations, z ∼ N (0, 1) is a Gaussian noise vector, and α i is control parameter. g ref i is global-level feature computed from the reference features h ref i through a linear transformation and a global average pooling operation. The meta-weights w i are then used for modulating the features in our cross-modulated layer normalization, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controllable Cross-modulated Layer Normalization (cLN):</head><p>Our cLN learns sample-dependent modulation weights for normalizing features since it is desired to generate images that are similar to the few-shot samples. Such a dynamic modulation of features enables our framework to generate images of high-quality and diversity. To this end, we utilize the meta-weights w i for computing the modulation parameters λ and β in our layer normalization modules. With the cross-attended feature c i as input, our cLN modulates the input to produce an output feature o i ∈ R n×D , given by</p><formula xml:id="formula_3">o i = cLN(c i , w i ) = λ(w i ) c i -μ σ + β(w i ),<label>(3)</label></formula><p>where μ and σ 2 are the estimated mean and variance of the input c i . Here, λ(w i ) is computed as the element-wise multiplication between meta-weights w i and sample-independent learnable weights λ ∈ R D , as λ w i . A similar computation is performed for β(w i ). Consequently, our proposed normalization mechanism achieves a controllable modulation of the input features based on the reference image inputs and enables enhanced diversity and quality in the generated images. The resulting features o i are then passed through a feed-forward network (FFN) followed by another cLN for preforming point-wise feature refinement, as shown in Fig. <ref type="figure">2</ref>. Afterwards, the cross-attended features f i are aggregated using control parameters α i to obtain the fused feature representation</p><formula xml:id="formula_4">f = i α i f i , where i ∈ [1, • • • , K -1].</formula><p>Finally, the decoder F D generates the final image x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training and Inference</head><p>Training: The whole framework is trained end-to-end following the hinge version GAN <ref type="bibr" target="#b16">[17]</ref> formulation. With generator F G denoting our encoder, CFB and decoder together, and discriminator F Dis , the adversarial loss L adv is given by</p><formula xml:id="formula_5">L adv FDis = E x∼real [max(0, 1 -F Dis (x))] + E x∼fake [max(0, 1 + F Dis (x))]</formula><p>and</p><formula xml:id="formula_6">L adv FG = -E x∼fake [F Dis (x)].<label>(4)</label></formula><p>Additionally, to encourage the generated image x to be perceptually similar to the reference images based on the specified control parameters α, we use a parameterized formulation of the standard perceptual loss <ref type="bibr" target="#b10">[11]</ref>, given by</p><formula xml:id="formula_7">L p = i α i L p i ,</formula><p>where</p><formula xml:id="formula_8">L p i = E[ φ(x) -φ(x ref i ) 2 ].<label>(5)</label></formula><p>Moreover, a classification loss L cl enforces that the images generated by the decoder are classified into the corresponding class of the input few-shot samples.</p><p>Our XM-GAN is then trained using the formulation:</p><formula xml:id="formula_9">L = L adv + η p L p + η cl L cl ,</formula><p>where η p and η cl are hyperparameters for weighting the loss terms.</p><p>Inference: During inference, multiple high-quality and diverse images x are generated by varying the control parameter α i for a set of fixed K-shot samples. While a base image x b and α i can be randomly selected, our framework enables a user to have control over the generation based on the choice of α i values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on human colorectal cancer dataset <ref type="bibr" target="#b13">[14]</ref>. The dataset consist of 8 categories of colorectal tissues, Tumor, Stroma, Lymph, Complex, Debris, Mucosa, Adipose, and Empty with 625 per categories. To enable few-shot setting, we split the 8 categories into 5 seen (for training) and 3 unseen categories (for evaluation) with 40 images per category. We evaluate our approach using two metrics: Frèchet Inception Distance (FID) <ref type="bibr" target="#b7">[8]</ref> and Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b23">[24]</ref>. Our encoder F E and decoder F D both have five convolutional blocks with batch normalization and Leaky-ReLU activation, as in <ref type="bibr" target="#b6">[7]</ref>. The input and generated image size is 128 × 128. The linear transformation ψ(•) is implemented as a 1 × 1 convolution with input and output channels set to D. The weights η p and η cl are set to 50 and 1. We set K = 3 in all the experiments, unless specified otherwise. Our XM-GAN is trained with a batchsize of 8 using the Adam optimizer and a fixed learning rate of 10 -4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">State-of-the-Art Comparison</head><p>FS Tissue Image Generation: In Tab. 1, we compare our XM-GAN approach for FS tissue image generation with state-of-the-art LoFGAN <ref type="bibr" target="#b6">[7]</ref> on <ref type="bibr" target="#b13">[14]</ref> dataset.</p><p>Our proposed XM-GAN that utilizes dense aggregation of relevant local information at a global receptive field along with controllable feature modulation outperforms LoFGAN with a significant margin of 30.1, achieving FID score of 55.8. Furthermore, our XM-GAN achieves a better LPIPS score. In Fig. <ref type="figure" target="#fig_1">3</ref>, we present a qualitative comparison of our XM-GAN with LoFGAN <ref type="bibr" target="#b6">[7]</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Here, we present our ablation study to validate the merits of the proposed contributions. Table <ref type="table" target="#tab_2">3</ref> shows the baseline comparison on the <ref type="bibr" target="#b13">[14]</ref> dataset. Our Baseline comprises an encoder, a standard cross-transformer with standard Layer normalization (LN) layers and a decoder. This is denoted as Baseline.</p><p>Baseline+PL refers to extending the Baseline by also integrating the standard perceptual loss. We conduct an additional experiment using random values of α i s.t. i α i = 1 for computing the fused feature f and parameterized perceptual loss (Eq. 5). We refer to this as Baseline+PPL. Our final XM-GAN referred here as Baseline+PPL+cLN contains the novel CFB. Within our CFB, we also validate the impact of the reference features for feature modulation by computing the meta-weights w i using only the Gaussian noise z in Eq. 2. This is denoted here as Baseline+PPL+cLN † . Our approach based on the novel CFB achieves the best performance amongst all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human Evaluation Study</head><p>We conducted a study with a group of ten pathologists having an average subject experience of 8.5 years. Each pathologist is shown a random set of 20 images (10 real and 10 XM-GAN generated) and asked to identify whether they are real or generated. The study shows that pathologists could differentiate between the AI-generated and real images only 55% time, which is comparable with a random prediction in a binary classification problem, indicating the ability of our proposed generative framework to generate realistic colorectal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We proposed a few-shot colorectal tissue image generation approach that comprises a controllable fusion block (CFB) which generates locally consistent features by performing a dense aggregation of local regions from reference tissue images based on their similarity to those in the base tissue image. We introduced a mapping network together with a cross-modulated layer normalization, within our CFB, to enhance the quality and diversity of generated images. We extensively validated our XM-GAN by performing quantitative, qualitative and human-based evaluations, achieving state-of-the-art results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our XM-GAN comprises a CNN encoder, a transformer-based controllable fusion block (CFB), and a CNN decoder for tissue image generation. For K-shot setting, a shared encoder FE takes a base tissue image x b along with K-1 reference tissue images {x ref i } K-1 i=1 and outputs visual features h b and {h ref i } K-1 i=1, respectively. Within our CFB, a mapping network computes meta-weights wi which are utilized to generate the modulation weights for feature re-weighting during cross-attention. The cross-attended features fi are fused and input to a decoder FD that generates an image x.</figDesc><graphic coords="4,74,61,70,76,262,06,103,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. On the left: few-shot input images of colorectal tissues. In the middle: images generated by LoFGAN. On the right: images generated by our XM-GAN. Compared to LoFGAN, our XM-GAN generates images that are high-quality yet diverse. Best viewed zoomed in. Additional results are provided in the supplementary material.</figDesc><graphic coords="8,70,47,200,81,311,35,81,46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Our</figDesc><table><row><cell>Low-Data Classification: Here, we</cell><cell></cell><cell></cell><cell></cell></row><row><cell>evaluate the applicability of the tissue</cell><cell></cell><cell></cell><cell></cell></row><row><cell>images generated by our XM-GAN as</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a source of data augmentation for the downstream task of low-data colorec-tal tissue classification for unseen cate-gories. The unseen dataset is split into D tr , D val , D test . Images of an unseen</cell><cell cols="3">XM-GAN achieves consis-tent gains in performance on both FID and LPIPS scores, outperforming LoFGAN on [14] dataset. Method FID(↓) LPIPS(↑)</cell></row><row><cell>class are split into 10:15:15. Follow-</cell><cell>LoFGAN [7]</cell><cell>85.9</cell><cell>0.44</cell></row><row><cell>ing [7], seen categories are used for ini-tializing the ResNet18 backbone and</cell><cell cols="2">Ours: XM-GAN 55.8</cell><cell>0.48</cell></row><row><cell>a new classifier is trained using D tr .</cell><cell></cell><cell></cell><cell></cell></row><row><cell>We refer to this as Standard. Then, we</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">augment D tr with 30 tissue images generated by our XM-GAN using the same</cell></row><row><cell cols="4">D tr as few-shot samples for each unseen class. Table 2 shows the classification</cell></row><row><cell cols="4">performance comparison. Compared to the LoFGAN [7], our XM-GAN achieves</cell></row><row><cell>absolute gains of 2.8%.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Low-data image classification. The proposed XM-GAN achieves superior classification performance compared to recently introduced LoFGAN.</figDesc><table><row><cell>Method</cell><cell>Accuracy (%)</cell></row><row><cell>Standard</cell><cell>68.1</cell></row><row><cell>LoFGAN [7]</cell><cell>69.7</cell></row><row><cell>Ours: XM-GAN</cell><cell>72.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Impact of integrating parameterized perceptual loss (PPL) and cLN to the baseline. Please refer to Sect. 4.2 for more details.</figDesc><table><row><cell>Method</cell><cell cols="2">FID(↓) LPIPS(↑)</cell></row><row><cell>Baseline</cell><cell>73.6</cell><cell>0.451</cell></row><row><cell>Baseline + PL</cell><cell>69.2</cell><cell>0.467</cell></row><row><cell>Baseline + PPL</cell><cell>66.5</cell><cell>0.471</cell></row><row><cell>Baseline + PPL + cLN  †</cell><cell>62.1</cell><cell>0.475</cell></row><row><cell>Ours: Baseline + PPL + cLN</cell><cell>55.8</cell><cell>0.482</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. We extend our heartfelt appreciation to the pathologists who made significant contributions to our project. We are immensely grateful to <rs type="person">Dr. Hima Abdurahiman</rs> from <rs type="affiliation">Government Medical College-Kozhikode, India</rs>; <rs type="person">Dr. Sajna PV</rs> from <rs type="funder">MVR Cancer Center and Research Institute, Kozhikode, India</rs>; <rs type="person">Dr. Binit Kumar Khandelia</rs> from <rs type="funder">North Devon District Hospital, UK</rs>; <rs type="person">Dr. Nishath PV</rs> from <rs type="funder">Aster Mother Hospital Kozhikode, India</rs>; <rs type="person">Dr. Mithila Mohan</rs> from <rs type="person">Dr. Girija</rs>'s <rs type="institution">Diagnostic Laboratory and Scans, Trivandrum, India</rs>; <rs type="person">Dr. Kavitha</rs> from <rs type="person">Aster MIMS</rs>, Kozhikode, India; and several other unnamed pathologists who provided their expert advice, valuable suggestions, and insightful feedback throughout various stages of our research work. This work was partially supported by the <rs type="programName">MBZUAI-WIS research program</rs> via project grant <rs type="grantNumber">WIS P008</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_spEQdtw">
					<idno type="grant-number">WIS P008</idno>
					<orgName type="program" subtype="full">MBZUAI-WIS research program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04340</idno>
		<title level="m">Data augmentation generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Few-shot generative modelling with generative matching networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICAIS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">FIGR: few-shot image generation with reptile</title>
		<author>
			<persName><forename type="first">L</forename><surname>Clouâtre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02199</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LofGAN: fusing local representations for few-shot image generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MatchingGAN: matching-based few-shot image generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICME</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">F2GAN: fusing-andfilling GAN for few-shot image generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6_43" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kather</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.53169</idno>
		<ptr target="https://doi.org/10.5281/zenodo.53169" />
		<title level="m">Collection of textures in colorectal cancer histology</title>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational Bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dawson: a domain adaptive few shot generation framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00576</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<title level="m">Geometric GAN</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel transfer learning approach for the classification of histological images of colorectal cancer</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Ohata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V S D</forename><surname>Chagas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Bezerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H C</forename><surname>De Albuquerque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P R</forename><surname>Filho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Supercomput</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">NVAE: a deep hierarchical variational autoencoder</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Histopathological image classification with bilinear convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<biblScope unit="page" from="4050" to="4053" />
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>In: 2017 39th Annual International Conference of the</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate diagnosis of colorectal cancer based on histopathology images using artificial intelligence</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate recognition of colorectal cancer with semi-supervised deep learning on pathological images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6311</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
