<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chest X-ray Image Classification: A Causal Perspective</title>
				<funder ref="#_z5d398D">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weizhi</forename><surname>Nie</surname></persName>
							<email>weizhinie@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
							<email>zhangchen001@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Song</surname></persName>
							<email>dan.song@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunpeng</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Cardiac Surgery</orgName>
								<orgName type="department" key="dep2">Clinical school of Thoracic</orgName>
								<orgName type="institution" key="instit1">Chest Hospital</orgName>
								<orgName type="institution" key="instit2">Tianjin University</orgName>
								<orgName type="institution" key="instit3">Tianjin Medical University</orgName>
								<address>
									<postCode>300052</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keliang</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Critical Care Medicine</orgName>
								<orgName type="department" key="dep2">Department of Anesthesiology, and Tianjin Institute of Anesthesiology</orgName>
								<orgName type="institution">Tianjin Medical University General Hospital</orgName>
								<address>
									<postCode>300052</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An-An</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<postCode>300072</postCode>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chest X-ray Image Classification: A Causal Perspective</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="25" to="35"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A2A566A86A953960B05B997F28350602</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical image processing</term>
					<term>Causal inference</term>
					<term>Chest X-ray image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The chest X-ray (CXR) is a widely used and easily accessible medical test for diagnosing common chest diseases. Recently, there have been numerous advancements in deep learning-based methods capable of effectively classifying CXR. However, assessing whether these algorithms truly capture the cause-and-effect relationship between diseases and their underlying causes, or merely learn to map labels to images, remains a challenge. In this paper, we propose a causal approach to address the CXR classification problem, which involves constructing a structural causal model (SCM) and utilizing backdoor adjustment to select relevant visual information for CXR classification. Specifically, we design various probability optimization functions to eliminate the influence of confounding factors on the learning of genuine causality. Experimental results demonstrate that our proposed method surpasses the performance of two open-source datasets in terms of classification performance. To access the source code for our approach, please visit: https:// github.com/zc2024/Causal CXR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chest X-ray (CXR) is a non-invasive diagnostic test frequently utilized by medical practitioners to identify thoracic diseases. In clinical practice, the interpretation of CXR results is typically performed by expert radiologists, which can be time-consuming and subject to individual medical abilities <ref type="bibr" target="#b0">[1]</ref>. Consequently, researchers have sought automated and accurate CXR classification technologies based on machine learning, aiming to assist physicians in achieving more precise diagnoses <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. However, there are some inherent problems with CXR images that are difficult to solve, such as high interclass similarity <ref type="bibr" target="#b15">[16]</ref>, dirty atypical data, complex symbiotic relationships between diseases <ref type="bibr" target="#b20">[21]</ref>, and long-tailed or imbalanced data distribution <ref type="bibr" target="#b25">[26]</ref>.</p><p>Some examples are shown in Fig. <ref type="figure" target="#fig_0">1</ref> from the NIH dataset, we can find previous methods performed not stable when dealing with some tough cases. For example, the label of Fig. <ref type="figure" target="#fig_0">1(d</ref>) is cardiomegaly but the predicting results generated by a traditional CNN-based model is infiltration, which fits the statistical pattern of symbiosis between these two pathologies <ref type="bibr" target="#b20">[21]</ref>. The black-box nature of deep learning poses challenges in determining whether the learned representations truly capture causality, even when the proposed models demonstrate satisfactory performance. Unfortunately, some recent efforts such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref> already notice part of the above problems but only try to solve it by data pre-processing or designing complicated model, these approaches have not succeeded in enabling deep models to effectively capture genuine causality.</p><p>To effectively address the aforementioned challenges, we approach the task of CXR image classification from a causal perspective. Our approach involves elucidating the relationships among causal features, confounding features, and the classification outcomes. In essence, our fundamental idea revolves around the concept of "borrowing from others." To illustrate this concept, let us consider an example involving letters in an image. Suppose a portion of the image contains marked letters, which can impact the classification of the unmarked portion. We perceive these letters as confounders. By borrowing the mark from the marked portion and adding it to the unmarked part, we effectively eliminate the confounding effect: "If everyone has it, it's as if no one has it." The same principle applies to other confounding assumptions we have mentioned.</p><p>Towards this end, we utilize causal inference to minimize the confounding effect and maximize the causal effect to achieve a stable and decent performance. Specifically, we utilize CNN-based modules to extract the feature from the input CXR images, and then apply Transformer based cross-attention mechanism <ref type="bibr" target="#b19">[20]</ref> to produce the estimations of the causal and confounding features from the feature maps. After that, we parameterize the backdoor adjustment by causal theory <ref type="bibr" target="#b13">[14]</ref>, which combines every causal estimation with different confounding estimations and encourages these combinations to remain a stable classification performance via the idea of "borrowing from others". It tends to facilitate the invariance between the causal patterns and the classification results.</p><p>We evaluate our method on multiple datasets and the experimental results consistently demonstrate the superior performance of our approach. The contributions of our work can be summarized as follows:</p><p>-We take a casual look at the chest X-ray multi-label classification problem and model the disordered or easily-confused part as the confounder. -We propose a framework based on the guideline of backdoor adjustment and presented a novel strategy for chest X-ray image classification. It allows our properly designed model to exploit real and stable causal features while removing the effects of filtrable confounding patterns. -Extensive experiments on two large-scale public datasets justify the effectiveness of our proposed method. More visualizations with detailed analysis demonstrate the interpretability and rationalization of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we first define the causal model, then identify the strategies to eliminate confounding effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Causal View on CXR Images</head><p>From the above discussion, we construct a Structural Causal Model (SCM) <ref type="bibr" target="#b1">[2]</ref> in Fig. <ref type="figure">2</ref> to solve the spurious correlation problems in CXR. It contains the causalities about four elements: Input CXR image D, confounding feature C, causal feature X, and prediction Y , where the arrows between elements stand for cause and effect: cause → effect. We have the following explanations:</p><p>• C ← D → X: X denotes the causal feature which really contributes to the diagnosis, C denotes the confounding feature which may mislead the diagnosis and is usually caused by data bias and other complex situations mentioned above. The arrows denote feature extraction process, C and X usually coexist in the medical data D, these causal effects are built naturally.</p><p>Fig. <ref type="figure">2</ref>. SCM for CXR image classification. "D" is the input data, "C" denotes the confounding features, "X" is the causal features and "Y" is the prediction results. Confounding factors can block backdoor path between causal variables, so after adjustment, the path is blocked, shown in right part.</p><p>• C → Y ← X: We denote Y as the classification result which should have been caused only by X but inevitably disturbed by confounding features. The two arrows can be implemented by classifiers.</p><p>The goal of the model should capture the true causality between X and Y , avoiding the influence of C. However, the conventional correlation P (Y |X) fails to achieve that because of the backdoor path <ref type="bibr" target="#b12">[13]</ref> </p><formula xml:id="formula_0">X ← D → C → Y .</formula><p>Therefore, we apply the causal intervention to cut off the backdoor path and use P (Y |do(X)) to replace P (Y |X), so the model is able to exploit causal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Causal Intervention via Backdoor Adjustment</head><p>Here, we propose to use the backdoor adjustment <ref type="bibr" target="#b1">[2]</ref> to implement P (Y |do(X)) and eliminate the backdoor path, which is shown on the right of Fig. <ref type="figure">2</ref>. The backdoor adjustment assumes that we can observe and stratify the confounders, i.e., C = {c 1 , c 2 , ..., c n }, where each c is a stratification of the confounder feature. We can then exploit the powerful do-calculus on causal feature X by estimating P b (Y |X) = P (Y |do(X)), where the subscript b denotes the backdoor adjustment on the SCM. Causal theory <ref type="bibr" target="#b13">[14]</ref> provides us with three key conclusions:</p><p>• P (c) = P b (c): the marginal probability is invariant under the intervention, because C will remain unchanged when cutting the link between D and X. Based on the conclusions, the backdoor adjustment for the SCM in Fig. <ref type="figure">2</ref> is:</p><formula xml:id="formula_1">• P b (Y |X, c) = P (Y |X, c): Y 's</formula><formula xml:id="formula_2">P (Y |do(X)) = P b (Y |X) = c∈C P b (Y |X, c)P b (c|X) = c∈C P b (Y |X, c)P b (c) = c∈C P (Y |X, c)P (c), (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where C is the confounder set, P (c) is the prior probability of c. We approximate the formula by a random sample operation which will be detailed next.  <ref type="figure">3</ref>. Overview of our network. Firstly, we apply CNN with modified attention to extract the image feature, where the n in Convn denotes the kernel size of the convolutional operation, "+", "×", and "C" denote add, multiply, and concatenate operations, respectively. "GAP" means global average pooling, "RS" is the random sample operation, and "Cls" denotes the classifier. The cross-attention module inside the transformer decoder disentangles the causal and confounding feature, then we can apply parameterized backdoor adjustment to achieve causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Object</head><p>Till now, we need to provide the implementations of Eq. ( <ref type="formula" target="#formula_2">1</ref>) in a parameterized method to fit the deep learning model. However, in the medical scenario, C is complicated and hard to obtain, so we simplify the problem and assume a uniform distribution of confounders. Traditionally, the effective learning of useful knowledge in deep models heavily relies on the design of an appropriate loss function. Then, towards effective backdoor adjustment, we utilize different loss functions to drive our deep model to learn causal and spurious features respectively. Figure <ref type="figure">3</ref> illustrates the proposed network. Note that the channel and position attention is implemented by adopting an efficient variant of self-attention <ref type="bibr" target="#b11">[12]</ref>. We will break the whole framework down in detail below.</p><p>Given x ∈ R H0×W0×3 as input, we extract its spatial feature F ∈ R H×W ×v using the backbone, where H 0 × W 0 , H × W represent the height and width of the CXR image and the feature map respectively, and v denotes the hidden dimension of the network. Then, we use zero-initialized Q 0 ∈ R k×v as the queries in the cross-attention module inside the transformer, where k is the number of categories, each decoder layer l updates the queries Q l-1 from its previous layer. Here, we denote Q as the causal features and Q as the confounding features:</p><formula xml:id="formula_4">Q l = sof tmax( Q l-1 F / dim F )F, Q l = (1 -sof tmax( Q l-1 F / dim F ))F, (2)</formula><p>where the tilde means position encodings, the disentangled features yield two branches, which can be fed separately into a point-wise Multi-layer perceptron (MLP) network and get corresponding classification logits via a sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disentanglement.</head><p>As shown in Fig. <ref type="figure">3</ref>, we try to impel the model to learn both causal and confounding features via the designed model structure and loss function. Specifically, we adopt a CNN-based model to extract the feature of input images, then capture the causal feature and confounding feature by crossattention mechanism. Thus we can make the prediction via MLP and classifiers:</p><formula xml:id="formula_5">h c = MLP conf ounding (Q l ), z c = Φ c (h c ), h x = MLP causal (Q l ), z x = Φ x (h x ),<label>(3)</label></formula><p>where h ∈ R v×k , Φ(•) represents classifier, and z denotes logits. The causal part aims to estimate the really useful feature, so we apply the supervised classification loss in a cross-entropy format:</p><formula xml:id="formula_6">L sl = - 1 |D| d∈D y log(z x ), (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where d is a sample and D is the training data, y is the corresponding label. The confounding part is undesirable for classification, so we follow the work in CAL <ref type="bibr" target="#b18">[19]</ref> and push its prediction equally to all categories, then the confounding loss is defined as:</p><formula xml:id="formula_8">L conf = - 1 |D| d∈D KL(y unif orm , z c ), (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where KL is the KL-Divergence, and y unif orm denotes a predefined uniform distribution.</p><p>Causal Intervention. The idea of the backdoor adjustment formula in Eq. ( <ref type="formula" target="#formula_2">1</ref>) is to stratify the confounder and combine confounding and causal features manually, which is also the implementation of the random sample in Fig. <ref type="figure">3</ref>. For this propose, we stratify the extracted confounding feature and randomly add it to the other CXR images' features, then feed into the classifier as shown in Eq. ( <ref type="formula">6</ref>), and get a "intervened graph", then we have the following loss guided by causal inference: ẑc = Φ(h x + ĥc ), ( <ref type="formula">6</ref>)</p><formula xml:id="formula_10">L bd = - 1 |D| • | D| d∈D d∈ D y log(ẑ c ),<label>(7)</label></formula><p>where ẑc is the prediction from a classifier on the "intervened graph", ĥc is the stratification feature via Eq. (3), D is the estimated stratification set contains trivial features. The training objective of our framework can be defined as:</p><formula xml:id="formula_11">L = L sl + α 1 L conf + α 2 L bd ,<label>(8)</label></formula><p>where α 1 and α 2 are hyper-parameters, which decide how powerful disentanglement and backdoor adjustment are. It pushes the prediction stable because of the shared image features according to our detailed results in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>We evaluate the common thoracic diseases classification performance on the NIH ChestX-ray14 <ref type="bibr" target="#b20">[21]</ref> and CheXpert <ref type="bibr" target="#b5">[6]</ref> data sets. NIH consists of 112,120 frontalview CXR images with 14 diseases and we follow the official data split for a fair comparison, and the latter dataset consists of 224,316 images.</p><p>In our experiments, we adopt ResNet101 <ref type="bibr" target="#b4">[5]</ref> as the backbone. Our experiment is operated by using NVIDIA GeForce RTX 3090 with 24 GB memory. We use the Adam optimizer <ref type="bibr" target="#b7">[8]</ref> with a weight decay of 1e-2 and the max learning rate is 1e-3. On the NIH data set, we resize the original images to 512 × 512 as the input and 320 × 320 on CheXpert. We evaluate the classification performance of our method with the area under the ROC curve (AUC) for the whole test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head><p>Table <ref type="table" target="#tab_0">1</ref> illustrates the overall performance of the NIH Chest-Xray14 dataset of our proposed method compared with other previous state-of-art works, the best performance of each pathology is shown in bold. From the experiments on the NIH data set, we can conclude that we eliminate some spurious relationships within and among CXR images from the classification results. Specifically, we can find that we are not only making progress in most categories but also dealing with some pathologies with high symbiotic dependence such as cardiomegaly and infiltration <ref type="bibr" target="#b20">[21]</ref>. The visualization results in Fig. <ref type="figure" target="#fig_0">1</ref> prove that the issues raised were addressed. We conduct experiments on the random addition ratio of "confounding features" and found that the ratio of 30% to 40% is appropriate. Besides, the α 1 in Eq. 8 works well around 0.4 to 0.7, and α 2 works well around 0.4 to 0.5.</p><p>Ablation studies on the NIH data set are shown in Table . 2. Where "+" denotes utilizing the module whereas "-" denotes removing the module. We demonstrate the efficiency of our method from the ablation study, and we can find that our feature extraction and causal learning module play significant roles, respectively. Besides, during the training process, Fig. <ref type="figure" target="#fig_3">4</ref> shows the fluctuation of the classification effect of three classifiers, where the three lines in the diagram correspond to the three classifiers in Fig. <ref type="figure">3</ref>. We can find the performance of the confounding classifier goes up at first and then down. At the same time, the other two classifiers' performance increased gradually, which is in line with our expectations. After visualization, we found that confounding factors could be "beneficial" for classification in some cases (e.g., certain diseases require patients to wear certain medical devices during X-rays), but this is the wrong shortcut, we expect the model to get causal features. Our causal learning framework successfully discards the adverse effect of confounding features and makes the prediction stable.</p><p>The results on CheXpert also prove the superiority of our method, we achieve the mean AUC of 0.912 on the five challenging pathologies <ref type="bibr" target="#b5">[6]</ref>, which surpasses the performance of previous SOTA works such as <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b14">[15]</ref>. Our method may be general and can be applied to many other medical scenario such as glaucoma <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and segmentation task <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. We will apply contrast learning or self supervised learning in our future works inspired by above-mentioned papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In conclusion, we present a novel causal inference-based chest X-ray image multilabel classification framework from a causal perspective, which comprises a feature learning module and a backdoor adjustment-based causal inference module. We find that previous deep learning based strategies are prone to make the final prediction via some spurious correlation, which plays a confounder role then damages the performance of the model. We evaluate our proposed method on two public data sets, and experimental results indicate that our proposed framework and method are superior to previous state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Some tough cases in NIH dataset. Each column is the same CXR image, and each row from top to bottom shows the original image with a pathological bounding box, weighted heat maps of traditional CNN-based deep learning, and our method. Four difficult situations such as (a): letters on images, (b): irregular images, (c): medical devices on images, and (d): easily confused between classes.</figDesc><graphic coords="2,91,80,54,02,240,82,188,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>response to X and C has no connection with the causal effect between X and C.• P b (c|X) = P b (c): X and C are independent after backdoor adjustment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.</head><label></label><figDesc>Fig.3. Overview of our network. Firstly, we apply CNN with modified attention to extract the image feature, where the n in Convn denotes the kernel size of the convolutional operation, "+", "×", and "C" denote add, multiply, and concatenate operations, respectively. "GAP" means global average pooling, "RS" is the random sample operation, and "Cls" denotes the classifier. The cross-attention module inside the transformer decoder disentangles the causal and confounding feature, then we can apply parameterized backdoor adjustment to achieve causal inference.</figDesc><graphic coords="5,56,28,75,29,340,15,66,61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Fluctuation of classification effect of three classifiers.</figDesc><graphic coords="9,173,07,56,00,119,26,96,37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparation of AUC scores with previous SOTA works. We report the AUC with a 95% confidence interval (CI) of our method.</figDesc><table><row><cell>Abnormality</cell><cell>DNetLoc</cell><cell>Xi et al.</cell><cell>ImageGCN</cell><cell>DGFN</cell><cell>Ours</cell></row><row><cell></cell><cell>[4]</cell><cell>[11]</cell><cell>[10]</cell><cell>[3]</cell><cell></cell></row><row><cell>Atelectasis</cell><cell>0.77</cell><cell>0.77</cell><cell>0.80</cell><cell>0.82</cell><cell>0.81 (0.81, 0.82)</cell></row><row><cell cols="2">Cardiomegaly 0.88</cell><cell>0.87</cell><cell>0.89</cell><cell>0.93</cell><cell>0.94 (0.93, 0.95)</cell></row><row><cell>Effusion</cell><cell>0.83</cell><cell>0.83</cell><cell>0.87</cell><cell>0.88</cell><cell>0.91 (0.91, 0.92)</cell></row><row><cell>Infiltration</cell><cell>0.71</cell><cell>0.71</cell><cell>0.70</cell><cell>0.75</cell><cell>0.75 (0.74, 0.77)</cell></row><row><cell>Mass</cell><cell>0.82</cell><cell>0.83</cell><cell>0.84</cell><cell>0.88</cell><cell>0.89 (0.88, 0.90)</cell></row><row><cell>Nodule</cell><cell>0.76</cell><cell>0.79</cell><cell>0.77</cell><cell>0.79</cell><cell>0.76 (0.74, 0.79)</cell></row><row><cell>Pneumonia</cell><cell>0.73</cell><cell>0.82</cell><cell>0.72</cell><cell>0.78</cell><cell>0.82 (0.80, 0.83)</cell></row><row><cell cols="2">Pneumothorax 0.85</cell><cell>0.88</cell><cell>0.90</cell><cell>0.89</cell><cell>0.91 (0.91, 0.93)</cell></row><row><cell cols="2">Consolidation 0.75</cell><cell>0.74</cell><cell>0.80</cell><cell>0.81</cell><cell>0.82 (0.81, 0.83)</cell></row><row><cell>Edema</cell><cell>0.84</cell><cell>0.84</cell><cell>0.88</cell><cell>0.89</cell><cell>0.90 (0.89, 0.90)</cell></row><row><cell>Emphysema</cell><cell>0.90</cell><cell>0.94</cell><cell>0.92</cell><cell>0.94</cell><cell>0.94 (0.93, 0.95)</cell></row><row><cell>Fibrosis</cell><cell>0.82</cell><cell>0.83</cell><cell>0.83</cell><cell>0.82</cell><cell>0.84 (0.84, 0.85)</cell></row><row><cell cols="2">Pleural Thicken 0.76</cell><cell>0.79</cell><cell>0.79</cell><cell>0.81</cell><cell>0.77 (0.75, 0.78)</cell></row><row><cell>Hernia</cell><cell>0.90</cell><cell>0.91</cell><cell>0.94</cell><cell>0.92</cell><cell>0.94 (0.92, 0.95)</cell></row><row><cell>Mean AUC</cell><cell>0.807</cell><cell>0.819</cell><cell>0.832</cell><cell>0.850</cell><cell>0.857 (0.849, 0.864)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on NIH data set.</figDesc><table><row><cell cols="2">Model Feature</cell><cell>Causal</cell><cell>AUC</cell></row><row><cell></cell><cell>Learning</cell><cell>Learning</cell><cell></cell></row><row><cell>1</cell><cell>-</cell><cell>-</cell><cell>0.812</cell></row><row><cell>2</cell><cell>-</cell><cell>+</cell><cell>0.833</cell></row><row><cell>3</cell><cell>+</cell><cell>-</cell><cell>0.824</cell></row><row><cell>4</cell><cell>+</cell><cell>+</cell><cell>0.857</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62272337</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_z5d398D">
					<idno type="grant-number">62272337</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discrepancy and error in radiology: concepts, causes and consequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Ó</forename><surname>Laoide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ulster Med. J</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Causal Inference in Statistics: A Primer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Wiley</publisher>
			<pubPlace>Hoboken</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable Gabor feature networks for biomedical image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to recognize abnormalities in chest x-rays with location-aware dense networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gündel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-13469-3_88</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-13469-388" />
	</analytic>
	<monogr>
		<title level="m">CIARP 2018</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Vera-Rodriguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Morales</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11401</biblScope>
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CheXtransfer: performance and parameter efficiency of ImageNet models for chest x-ray interpretation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Health, Inference, and Learning</title>
		<meeting>the Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SDFN: segmentation-based deep fusion network for thoracic disease classification in chest x-ray images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageGCN: multi-relational image graph convolutional networks for disease identification with chest x-rays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1990" to="2003" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning hierarchical attention for weakly-supervised chest xray abnormality localization and diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2698" to="2710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the integration of self-attention and convolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="815" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpretation and identification of causal mediation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">459</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Models, reasoning and inference</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interpreting chest x-rays via CNNs that exploit hierarchical disease dependencies and uncertainty labels</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">437</biblScope>
			<biblScope unit="page" from="186" to="194" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training deep learning algorithms with weakly labeled pneumonia chest x-ray data for covid-19 detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attentiondriven spatial transformer network for abnormality detection in chest x-ray images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pedrosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campilho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendonça</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 35th International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="252" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classification of chest diseases from x-ray images on the CheXpert dataset</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">U</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Khalid</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-16-0749-3_64</idno>
		<ptr target="https://doi.org/10.1007/978-981-16-0749-364" />
	</analytic>
	<monogr>
		<title level="m">Innovations in Electrical and Electronic Engineering</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Mekhilef</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Favorskaya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Pandey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shaw</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">756</biblScope>
			<biblScope unit="page" from="837" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Causal attention for interpretable and generalizable graph classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1696" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Chestx-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SeATrans: learning segmentation-assisted diagnosis model via transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_65</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-765" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
	<note>MICCAI 2022</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03016</idno>
		<title level="m">Calibrate the inter-observer segmentation uncertainty via diagnosisfirst principle</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Opinions vary? Diagnosis first!</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_58</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-758" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
	<note>MICCAI 2022</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Leveraging undiagnosed data for glaucoma classification with teacher-student learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_71</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-871" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="731" to="740" />
		</imprint>
	</monogr>
	<note>Medical Image Computing and Computer Assisted Intervention -MICCAI 2020</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04596</idno>
		<title level="m">Deep long-tailed learning: a survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
