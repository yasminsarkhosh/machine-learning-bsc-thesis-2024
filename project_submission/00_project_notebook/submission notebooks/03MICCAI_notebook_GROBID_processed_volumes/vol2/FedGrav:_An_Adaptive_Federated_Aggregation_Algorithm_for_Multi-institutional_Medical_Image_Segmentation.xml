<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation</title>
				<funder ref="#_AMn4X2N">
					<orgName type="full">Fund for Innovation and Transformation of Haidian District, Beijing, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhifang</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">National Pilot Software Engineering School)</orgName>
								<orgName type="institution">University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dandan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">National Pilot Software Engineering School)</orgName>
								<orgName type="institution">University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shi</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ultrasound</orgName>
								<orgName type="institution">Peking University Third Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ultrasound</orgName>
								<orgName type="institution">Peking University Third Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xueguang</forename><surname>Yuan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaohong</forename><surname>Huang</surname></persName>
							<email>huangxh@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">National Pilot Software Engineering School)</orgName>
								<orgName type="institution">University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Zhongguancun Laboratory</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangwei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">HTA Co., Ltd</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="170" to="180"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">0AFC457FB26BECAC7387C96DC591DAC8</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_16</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Federated Learning</term>
					<term>Brain Tumor Segmentation</term>
					<term>FedGrav</term>
					<term>Model Affinity</term>
					<term>Graph Distance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the increasingly strengthened data privacy acts and the difficult data centralization, Federated Learning (FL) has become an effective solution to collaboratively train the model while preserving each client's privacy. FedAvg is a standard aggregation algorithm that makes the proportion of the dataset size of each client an aggregation weight. However, it can't deal with non-independent and identically distributed (non-IID) data well because of its fixed aggregation weights and the neglect of data distribution. The paper presents a new aggregation strategy called FedGrav, which is designed to handle non-IID datasets and is inspired by the law of universal gravitation in physics. FedGrav can dynamically adjust the aggregation weights based on the training condition of local models throughout the entire training process, making it an effective solution for non-IID data. The model affinity is creatively proposed by considering both the differences of sample size on the client and the discrepancies among local models. It considers the client sample size as the mass of the local model and defines the model graph distance based on neural network topology. By calculating the affinity among local models, FedGrav can explore internal correlations of them and improve the aggregation weights. The proposed FedGrav has been applied to the CIFAR-10 and the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2021 datasets, and the validation results show that our method outperforms the previous state-of-the-art by 1.54 mean DSC and 2.89 mean HD95. The source code will be available on Github.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The demand for precise medical data analysis has led to the widespread use of deep learning methods in the medical field. However, accompanied by the promulgation of data acts and the strengthening of data privacy, it has become increasingly challenging to train models in large-scale centralized medical datasets. As one of the solutions, federated learning provides a new way out of the dilemma and attracts significant attention from researchers.</p><p>Federated learning (FL) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> is a distributed machine learning paradigm in which all clients train a global model collaboratively while preserving their data locally. As a crucial core of them, the aggregation algorithm plays an important role in releasing data potential and improving global model performance. FedAvg <ref type="bibr" target="#b0">[1]</ref>, as pioneering work, was a simple and effective aggregation algorithm, which makes the proportions of local datasets size as the aggregation weights of local models. But in the real world, not only the numbers of datasets held by clients is different, but also their data distribution may be diverse, which leads to the fact that the data in the federated learning is non-Independent Identically Distribution (non-IID). The naive aggregation algorithms maybe have worse performance because of the non-IID data <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. In medical image segmentation, <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref> took the lead in discussing the application and safety of federated learning in brain tumor segmentation (BraTS). To solve the non-IID challenges of FL in the medical image field, FedDG <ref type="bibr" target="#b11">[11]</ref> and FedMRCM <ref type="bibr" target="#b12">[12]</ref> were proposed to address the domain shift issue between the source domain and the target domain, but the sharing of latent features may cause privacy concerns. Auto-FedRL <ref type="bibr" target="#b13">[13]</ref> and Auto-FedAvg <ref type="bibr" target="#b14">[14]</ref> were proposed to deal with the non-IID problem by using an optimization algorithm to learn super parameters and aggregate weights. IDA <ref type="bibr" target="#b15">[15]</ref> introduced the Inverse Distance of local models and the average model of all clients to handle non-IID data. The work <ref type="bibr" target="#b16">[16]</ref><ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref> proposed corresponding aggregation methods from the perspectives of clustering, frequency domain, Bayesian, and representation similarity analysis. More than this, the first computational competition on federated learning, Federated Tumor Segmentation (FeTS) Challenge<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b20">[20]</ref> was held to measure the performance of different aggregation algorithms on glioma segmentation <ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref>. Leon et al. <ref type="bibr" target="#b25">[25]</ref> proposed FedCostWAvg get a notable improvement compared to FedAvg by including the cost function decreased during the last round and won the challenge. However, most of these methods improve the performance by adding other regular terms to the aggregation method, without considering all factors as a whole, which may limit the performance of the global model. Different from the above methods, inspired by the concept of the law of universal gravitation in physics, in this paper, we propose a novel aggregation strategy, FedGrav, which unifies the differences in sample size and the discrepancies of local models among clients by defining the concept of model affinity. Specifically, we take the client sample size as the mass of the local model, and the discrepancies among the local models as their distance, which is quantified from the topological perspective of neural networks. Last, the formula 1 is employed to calculate the affinity and explore the internal correlation between the local models. The proposed method promotes a more effective aggregation of local models by unifying the difference between sample size and local model between clients.</p><p>The primary contributions of this paper can be summarized as: <ref type="bibr" target="#b0">(1)</ref> We propose FedGrav, a novel aggregation strategy that unifies the difference both in sample size and local model among clients by defining the concept of model affinity; <ref type="bibr" target="#b1">(2)</ref> We propose Model Graph Distance, a new method to quantify model differences from the perspective of neural network topology. <ref type="bibr" target="#b2">(3)</ref> We propose an aggregation algorithm that introduces the concept of affinity and graph into federated learning, and the aggregation weights can be adjusted adaptively; <ref type="bibr" target="#b3">(4)</ref> The superior performance is achieved by the proposed method, on the public CIFAR-10 and FeTS challenge datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Suppose K clients with private data cooperate to train a global model and share the same neural network structure, 3D-Unet <ref type="bibr" target="#b26">[26]</ref>, which is provided by the FeTS challenge and kept unchanged. For the clients, every client trains a local model w i for local E epochs and then delivers the local model to the server. The server aggregates local models to a global model by computing the aggregation weights with the proposed FedGrav and assigns it to all clients. Specifically, given K local models, we first make graph mapping to map the network model to the topology graph, and then the graph distance is obtained after the graph pruning and comparison. For the model affinity computation, FedGrav takes the sample size of every client as the mass of the local model and combines the given graph distance to calculate the affinity between models according to the formula 1. After that, a symmetric Model Affinity Matrix A ∈ R K×K is analyzed to compute aggregation weights. Last, The server aggregates local models to a global model according to the aggregation weights and assigns it to all clients. Repeat and until T rounds or other limits. An overview of the method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">FedGrav</head><p>Model Affinity. Inspired by the law of universal gravitation, we assume that there is similar gravitation between any two local models. We define it as model affinity in federated learning. It can be described that the affinity between two local models is proportional to the sample size of the client corresponding to the local model, and inversely proportional to the distance between two models. The equation for model affinity takes the form:</p><formula xml:id="formula_0">A ik = M n i n k d 2 ik (1)</formula><p>where A ik is the affinity between i-th and k-th local models, n i and n k are the sample size of i-th and k-th client, and d ik is the distance between two local models, which is quantified from the perspective of neural network topology and will be described in the following section. M is the affinity constant, it can be simplified in the subsequent analysis, so this paper will not set specific values for it. The model affinity depicts the internal correlation between two local models, which lays the foundation for accurate aggregation weights.</p><p>Graph Distance. The distance is defined to quantify model differences. The differences in local models reflect the discrepancies in the distribution of client data to a certain extent. If the differences in local models can be accurately measured, the more appropriate aggregation weights will be assigned to local models to aggregate a better global model. The key motivation is to measure the internal correlations of local models as accurately as possible. We explore the model distance from the perspective of neural network topology in this paper and define it as model graph distance. In FedGrav, the computation of graph distance goes through the following steps:</p><p>(1) Graph Mapping. Suppose the server has received local models trained by local data, and we map them into the topological graph. Inspired by <ref type="bibr" target="#b27">[27]</ref>, take the j-th convolutional layer of k-th local model with 3D-Unet structure as an example, whose kernel dimension is 3  27 . And then, we make every node W as scalar by averaging or summing, which can be formulated as:</p><formula xml:id="formula_1">× 3 × 3 × C in × C out ,</formula><formula xml:id="formula_2">w sum = 2 d=0 2 h=0 2 w=0 W dhw . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>It can be mapped into a graph whose structure is similar to the full connection layer after the scalarization of the convolutional layer. Given a 3×3×3×C in ×C out convolutional layer, the dimensions of its input and output are C in and C out respectively. So, we obtain a weight matrix W t ∈R Cin×Cout after averaging or summing the weights of convolution kernel. We take the C in and C out as the number of nodes, and the weight summation w sum is the edge weight.</p><p>(2) Graph Pruning. The server collects local models from clients and makes the graph mapping on them to get K graphs which have the same structure except for the edge weights. These graphs contain all the information of local models, including the part of universality and the part of characteristics of the client data. To make the graphs more distinctive, the graph pruning is conducted.</p><p>In detail, we differentiated these graphs by setting an adaptive threshold δ, where the edge will be removed if the weight difference of each layer between the local models and global model in the last round is less than the threshold, otherwise, the edge will exist. It can be simplified as:</p><formula xml:id="formula_4">edge = w kj , | w t kj -w t-1 gj |&gt; δ, 0,</formula><p>otherwise.</p><p>(3)</p><formula xml:id="formula_5">δ = Sort(| w t kj -w t-1 gj |)[ λ • C in × C out ], 0 ≤ λ &lt; 1.<label>(4)</label></formula><p>where in Eq. 3, 0 denotes the edge is removed, w t kj denotes edge weight of the j-th layer from the k-th graph in the t-th round, also the weight summation of the j-th layer from the k-th local model in the t-th round, w t-1 gj is the weight summation of the j-th layer from the global model in (t -1)-th round. The threshold δ varies adaptively with the weights of local models, and λ is the pruning ratio which is responsible for adjusting the degree of pruning. After that we get K discriminative graphs</p><formula xml:id="formula_6">G i , i ∈ [1, K].</formula><p>(3) Graph Comparison. In order to measure the degree of correlation between two graphs, we measure the similarity between pairs of graphs by computing matching between their sets of embeddings, where the Pyramid Match Graph Kernel <ref type="bibr" target="#b28">[28]</ref> is employed. We take the reciprocal of the correlation degree as the distance between them. The distance is defined as follows:</p><formula xml:id="formula_7">d ik = 1 P yramidM atch(G i , G k )<label>(5)</label></formula><p>Aggregation Weights. According to the above process, the Affinity Matrix A is obtained, which reports the correlation among local models and is symmetric. The element A ik in matrix A denotes the affinity of G i and G k . The elements in Table <ref type="table">1</ref>. Comparisons with other state-of-the-art methods on the CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy (%)</head><p>FedAvg <ref type="bibr" target="#b0">[1]</ref> 88.37 ± 0.04 FedProx <ref type="bibr" target="#b5">[6]</ref> 87.93 ± 0.19 FedNova <ref type="bibr" target="#b29">[29]</ref> 88.68 ± 0.26 Auto-FedAvg <ref type="bibr" target="#b14">[14]</ref>  </p><formula xml:id="formula_8">α k = K i=1 A ik K k=1 K i=1 A ik (6)</formula><p>In federated learning, clients send the updated local models back to the server each round. In round t, α k is represented as α t k . The global model w t+1 g is aggregated by the server:</p><formula xml:id="formula_9">w t g = K k=1 α t k • w t k (7)</formula><p>then, the server assigns the global model w t g to all clients. Repeat and until T rounds or other limits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datesets and Settings</head><p>CIFAR-10. The first dataset to verify the validity of our algorithm is CIFAR-10. We partition the training set into 8 clients with heterogeneous data by sampling from a Dirichlet distribution (α = 0.5) as in <ref type="bibr" target="#b9">[10]</ref> to simulate the non-IID distribution, and the test set in CIFAR-10 is considered as the global test set to evaluate the performance of different algorithms. VGG-9 <ref type="bibr" target="#b30">[30]</ref> is employed for image classification, and the other detailed settings are as follows: initial learning rate of 1e -2; total rounds of 100; local epochs of 20; batch size of 64; SGD optimizer for clients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MICCAI FeTS2021</head><p>Training Data. The real-world dataset used in experiments is provided by the FeTS Challenge organizer, which is the training set of the whole dataset about brain tumor segmentation. In order to evaluate the performance of FedGrav, we partition the dataset composed of 341 data samples </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Experiment Results on the CIFAR-10. We first validate the proposed method on the CIFAR-10 dataset. Table <ref type="table">1</ref> shows the quantitative results of the state-of-the-art FL methods in terms of the average accuracy, such as FedAvg <ref type="bibr" target="#b0">[1]</ref>, FedProx <ref type="bibr" target="#b5">[6]</ref>, FedNova <ref type="bibr" target="#b29">[29]</ref>, and Auto-FedAvg <ref type="bibr" target="#b14">[14]</ref>. As can be seen from the table, the proposed FedGrav method outperforms the other competing FL aggregation methods including Auto-FedAvg, a learning-based aggregation method, which indicates the potential and superiority of FedGrav.</p><p>Experiment Results on MICCAI FeTS2021 Training Dataset. In order to verify the robustness of our method and its performance in real-world data, we conduct the experiment on the MICCAI FeTS2021 Training dataset. We evaluate the performance of our algorithm by comparing six indicators: the Dice Similarity Coefficient(DSC) and Hausdorff Distance-95th percentile(HD95) of whole tumor(WT), enhancing tumor(ET), and tumor core(TC). As is shown in Table <ref type="table" target="#tab_2">2</ref>, we list the average results of FedAvg, FedCostWAvg(shortened to FCW), the champion method of FeTS Challenge 2021, and the proposed Fed-Grav. Different from the original FedCostWAvg which changed the activation function of networks, our re-implemented version made the network unchanged to ensure a fair comparison. Through the quantitative comparison in Table <ref type="table" target="#tab_2">2</ref>, we can find that the proposed method FedGrav has achieved the best results in all indicators except the HD95 TC. Moreover, compared with FedCostWAvg, FedGrav has significantly improved the evaluation of segmentation performance, especially in the enhancing tumor segmentation.</p><p>The visualization results are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It can be seen that our Fed-Grav achieves better segmentation results, even in the hard example, compared to FedCostWAvg and FedAvg. The results proved that the proposed method FedGrav can explore the correlations of local models better and achieved more excellent aggregation performance compared with other methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To evaluate the effectiveness and find the better configuration of FedGrav, we conduct the ablation study on the FeTS datasets, and the results are shown in Fig. <ref type="figure" target="#fig_2">3</ref>. As we can see, the mean DSC shows a trend of rising first and then falling, because more irrelevant and redundant information will be saved in the model when pruning is not performed. The different values of λ denote the loose degree of graphs, with the gradual increase of λ, the redundant information in local models is gradually eliminated, and the unique information of each local model is preserved. While, when the pruning ratio λ increases to a certain extent, the models lack key information, which makes the model affinity inaccurate, resulting in a decline in segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we introduced FedGrav, a novel aggregation strategy inspired by the law of universal gravitation in physics. FedGrav improves local model aggregation by considering both the differences in sample size and discrepancies among local models. It can adaptively adjust the aggregation weights and explore the internal correlations of local models more effectively. We evaluated our method on CIFAR-10 and real-world MICCAI Federated Tumor Segmentation Challenge (FeTS) datasets, and the superior results demonstrated the effectiveness and robustness of our FedGrav.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed FedGrav. The FedGrav defines the concept of model affinity by unifying the difference in both sample size and local model among clients to aggregates local models and explore the correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The visual comparisons with previous state-of-the-art methods on the MICCAI FeTS2021 Training dataset.</figDesc><graphic coords="8,80,46,54,32,291,55,218,83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of different pruning ratio λ in FedGrav on FeTS datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>89.16 FedGrav 89.35 ± 0.23 the k-th row represent the Affinity among G k and all graphs, so we can get the affinity of the k-th graph with all graphs, which denotes the correlations of G k with the whole graphs. last, we normalize A k as the aggregation weight of the k-th local model or layer.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparisons with other state-of-the-art methods on the MICCAI FeTS2021 Training dataset. D denotes DICE, H95 denotes HD95, and M denotes mean. and validation set according to the ratio of 8 : 2, and the data is unevenly distributed between 17 data clients. The segmentation network, 3D-Unet, is provided by FeTS and kept unchanged, the learning rate is 1e -4 and the local epochs are 10. Limited by the framework and official code mechanism, the total number of rounds of training is set to 70, although the performance of the algorithm does not converge to the best.</figDesc><table><row><cell>Method</cell><cell cols="4">D WT D ET D TC H95 WT H95 ET H95 TC M D</cell><cell>M H95</cell></row><row><cell cols="2">FedAvg [1] 90.49 73.03 69.38 4.82</cell><cell>33.88</cell><cell>39.00</cell><cell cols="2">77.63 ± 0.573 25.90 ± 2.731</cell></row><row><cell cols="2">FCW [21] 90.88 73.15 70.56 3.74</cell><cell>40.79</cell><cell>17.16</cell><cell cols="2">78.20 ± 0.749 20.56 ± 0.311</cell></row><row><cell>FedGrav</cell><cell>91.26 77.21 70.75 2.80</cell><cell>27.40</cell><cell>22.8</cell><cell cols="2">79.74 ± 0.595 17.67 ± 1.692</cell></row><row><cell cols="2">into training set</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://fets-ai.github.io/Challenge/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">Fund for Innovation and Transformation of Haidian District, Beijing, China</rs>(No. <rs type="grantNumber">HDCXZHKC2021201</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AMn4X2N">
					<idno type="grant-number">HDCXZHKC2021201</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="1273" to="1282" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Federated machine learning: concept and applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol. (TIST)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Civin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00582</idno>
		<title level="m">Federated learning with non-IID data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FedBN: federated learning on non-IID features via local batch normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Federated optimization in heterogeneous networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Mach. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="429" to="450" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust and communication-efficient federated learning from non-IID data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Maluller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3400" to="3413" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Scaffold: stochastic controlled averaging for federated learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Karimireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed training with heterogeneous data: bridging median-and mean-based algorithms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-institutional deep learning modeling without sharing patient data: a feasibility study on brain tumor segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sheller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11723-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11723-8_9" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2018</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Keyvan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Van Walsum</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11383</biblScope>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Privacy-preserving federated brain tumour segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLMI 2019</title>
		<editor>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11861</biblScope>
			<biblScope unit="page" from="133" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32692-0_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32692-0_16" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feddg: federated domain generalization on medical image segmentation via episodic learning in continuous frequency space</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1013" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-institutional collaborations for improving deep learning-based magnetic resonance image reconstruction using federated learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Auto-FedRL: federated hyperparameter optimization for multiinstitutional medical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06338</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Auto-FedAvg: learnable federated averaging for multi-institutional medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10195</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inverse distance aggregation for federated learning with non-IID data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeganeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60548-3_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-60548-3_15" />
	</analytic>
	<monogr>
		<title level="m">DART/DCL -2020</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12444</biblScope>
			<biblScope unit="page" from="150" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FedSim: similarity guided model aggregation for Federated Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Palihawadana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wiratunga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wijekoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">483</biblScope>
			<biblScope unit="page" from="432" to="445" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">FedBE: making Bayesian model ensemble applicable to federated learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Personalized retrogress-resilient framework for real-world medical federated learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_33" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Federated contrastive learning for decentralized unlabeled medical images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Voiculescu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_36</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_36" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="378" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Pati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05874</idno>
		<title level="m">The federated tumor segmentation (fets) challenge</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Open: an open-source framework for federated learning</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Reina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06413</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sheller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Brats toolkit: translating brats brain tumor segmentation algorithms into clinical and scientific practice</title>
		<author>
			<persName><forename type="first">F</forename><surname>Koer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FedCostWAvg: a new averaging for better Federated Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ezhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kofler</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-09002-8_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-09002-8_34" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2021, Part II</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">12963</biblScope>
			<biblScope unit="page" from="383" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_49" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Topological Data Analysis of Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Gabrielsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weights on Images</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching node embeddings for graph similarity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tackling the objective inconsistency problem in heterogeneous federated optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
