<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Debiasing Medical Visual Question Answering via Counterfactual Training</title>
				<funder ref="#_abn5dQf">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenlu</forename><surname>Zhan</surname></persName>
							<email>chenlu.22@intl.zju.edu.cn</email>
							<idno type="ORCID">0000-0001-9839-7162</idno>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Peng</surname></persName>
							<idno type="ORCID">0000-0001-7062-1150</idno>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanrong</forename><surname>Zhang</surname></persName>
							<idno type="ORCID">0000-0003-0957-6703</idno>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyue</forename><surname>Sun</surname></persName>
							<idno type="ORCID">0009-0002-4903-8715</idno>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunnan</forename><surname>Shang</surname></persName>
							<idno type="ORCID">0009-0005-1840-4673</idno>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
							<idno type="ORCID">0000-0003-3303-180X</idno>
							<affiliation key="aff2">
								<orgName type="department">Department of Cardiology</orgName>
								<orgName type="institution">Chinese PLA General Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongsen</forename><surname>Wang</surname></persName>
							<idno type="ORCID">0000-0002-0081-8641</idno>
							<affiliation key="aff2">
								<orgName type="department">Department of Cardiology</orgName>
								<orgName type="institution">Chinese PLA General Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaoang</forename><surname>Wang</surname></persName>
							<idno type="ORCID">0000-0002-0081-8641</idno>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
							<email>hongweiwang@zju.edu.cn</email>
							<idno type="ORCID">0000-0001-6118-6505</idno>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ZJU-UIUC Institute</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Debiasing Medical Visual Question Answering via Counterfactual Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B507DF636804D7CCAF518FC6D7CDF018</idno>
					<idno type="DOI">10.1007/978-3-031-43895-036.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Vision Question Answering</term>
					<term>Language Bias</term>
					<term>Counterfactual Sample Generation</term>
					<term>Counterfactual Training</term>
					<term>SLAKE-CP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical Visual Question Answering (Med-VQA) is expected to predict a convincing answer with the given medical image and clinical question, aiming to assist clinical decision-making. While today's works have intention to rely on the superficial linguistic correlations as a shortcut, which may generate emergent dissatisfactory clinic answers. In this paper, we propose a novel DeBiasing Med-VQA model with Coun-terFactual training (DeBCF) to overcome language priors comprehensively. Specifically, we generate counterfactual samples by masking crucial keywords and assigning irrelevant labels, which implicitly promotes the sensitivity of the model to the semantic words and visual objects for bias-weaken. Furthermore, to explicitly prevent the cheating linguistic correlations, we formulate the language prior into counterfactual causal effects and eliminate it from the total effect on the generated answers. Additionally, we initiatively present a newly splitting bias-sensitive Med-VQA dataset, Semantically-Labeled Knowledge-Enhanced under Changing Priors (SLAKE-CP) dataset through regrouping and re-splitting the train-set and test-set of SLAKE into the different prior distribution of answers, dedicating the model to learn interpretable objects rather than overwhelmingly memorizing biases. Experimental results on two public datasets and SLAKE-CP demonstrate that the proposed DeBCF outperforms existing state-of-the-art Med-VQA models and obtains significant improvement in terms of accuracy and interpretability. To our knowledge, it's the first attempt to overcome language priors in Med-VQA and construct the bias-sensitive dataset for evaluating debiased ability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical visual question answering (Med-VQA) has attracted considerable attention in recent years. It seeks to discover the plausible answer by evaluating the visual information of a medical image and a clinical query regarding the image. The Med-VQA technology can considerably enhance the efficiency of medical professionals and fulfill the growing demand for medical resources <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>. However, numerous researches have found that general VQA models are significantly influenced by superficial linguistic correlations in training set, lacking adequate visual grounding <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. Since most of the existing Med-VQA datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> are manually spitted and annotated, the spurious over-reliant bias factor also exists in Med-VQA, as the Fig. <ref type="figure">1</ref> shown. Recent general VQA works dedicate to reducing the language priors through enhancing the visual information <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">27]</ref> or data balancing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>, there is bare attempt to prevent the language priors in medical domain. Current Med-VQA works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> devote to construct effective models and most Med-VQA datasets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> simply balance the medical images to mitigate the inherent bias. These works all neglect the cheating factors that the Med-VQA models typically resort to linguistic distributions priors, consequently ignoring the semantic clinic objects. This problem accordingly leads to disastrous results in clinic application consequences.</p><p>Therefore, we propose a novel unbiased and interpretable Med-VQA model and preliminarily construct a bias-sensitive Med-VQA dataset to address the problems mentioned above. First, with the aim of forcing the model to focus on clinic objects rather than superficial correlations, we prepare the counterfactual samples by masking clinic words with "[MASK]" tokens and meanwhile assign the irrelevant answers for implicit bias-weaken. Further, for explicitly reducing the linguist bias, we treat the language bias as the causal effect of the clinic Train What organ system is pictured?</p><p>What organ system is pictured? Baseline: Chest DeBCF: Neck Answer: Chest Fig. <ref type="figure">1</ref>. The baseline generates incorrect answer "Chest" relying on the majority prior "Chest" in train-set of the publicly available SLAKE <ref type="bibr" target="#b15">[16]</ref> dataset rather than real semantic image objects. The proposed DeBCF overcomes the language priors and generates the reliable answer with correct semantic parts. question on the generated answer and then subtract it from the total causal effect for counterfactual training. It is noted that both the original data and generated counterfactual data will be used for counterfactual training. In this way, the model may not tend to provide answers over-rely on the largest proportions of candidate answers in train-set when tested, thus concentrating on entanglement of the visual objects and language information.</p><p>Additionally, we conduct a bias-sensitive Med-VQA dataset Semantically-Labeled Knowledge-Enhanced-Changing Priors (SLAKE-CP) for evaluating the ability of disentangling the memorized linguist priors and semantic visual information. Qualitative and quantitative experimental results illustrate that our proposed model is superior to the state-of-the-art Med-VQA models on the two public benchmarks and can obtain more obvious improvements on the newly constructed SLAKE-CP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Figure <ref type="figure" target="#fig_0">2</ref> illustrates the proposed Med-VQA method which consists of implicit and explicit counterfactual debiased stages: the counterfactual training data preparation stage to improve the sensitivity of the critical clinic objects for implicit bias-weaken. Along with the counterfactual causal effect training stage to directly migrate the language priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Counterfactual Training Data Preparation</head><p>To implicitly weaken the language bias, we follow CSS <ref type="bibr" target="#b2">[3]</ref> to prepare counterfactual training samples for improving the sensitivity of clinic objects. First, we extract the question type (e.g. "Where" in Fig. <ref type="figure" target="#fig_0">2</ref>) of each question and calculate the importance s of the remaining words w i in clinical question q to the label a as: s(a, w i ) = S(P (a|q, v),</p><formula xml:id="formula_0">w i ) := (∇ wi P (a|q, v)) T 1<label>(1)</label></formula><p>where P (a|q, v) represents the probability of predicting answer a through Med-VQA model with image v and question q, ∇ is the gradient operator, S is the cosine similarity, and 1 is the all-ones vector. The top-K clinic words with the highest importance s are defined as critical words. Then, we construct counterfactual samples Q -by replacing the critical words with "[MASK]". We also assign the Q -with an answer A -, and the detailed assigning procedure is as follows. We first generate the probability of predicting answer P + (a) with the question Q + which replaces the marginal words with "[MASK]" (all but the question type labels and the critical words), and then pick up top-N candidate answers with the highest probability as A + . The rest answers are denoted as</p><formula xml:id="formula_1">A -= {a i |a i ∈ A, a i / ∈ A + } and are assigned to Q -.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Counterfactual Cause Effect Training Procedure</head><p>For explicitly subtracting the language priors, following <ref type="bibr" target="#b23">[24]</ref>, we introduce casual effect <ref type="bibr" target="#b20">[21]</ref> to translate priors into quantified expressions. The causal effects can directly reflect the comparisons between the outputs with different treatments (e.g. X = x represents with-treatment and X = x * represents the counterfactual situation where is without the treatment). The total effect (TE) of X = x on Y can be defined as two different conditions that with or without the input:</p><formula xml:id="formula_2">T E = Y X=x,M (X=x) -Y X=x * ,M (X=x * ) (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where M is the mediator between the variables X and Y . Note that the total effect can be composed of the natural direct effect (NDE) and total indirect effect(TIE). Between them, the NDE concentrates the exclusive effect of X = x on Y and prohibit the effect through M :</p><formula xml:id="formula_4">NDE = Y X=x,M (X=x * ) -Y X=x * ,M (X=x * )<label>(3)</label></formula><p>Thus TIE can reflect the reduction of language bias by subtracting the NDE from the TE:</p><formula xml:id="formula_5">T IE = T E -NDE = Y X=x,M (X=x) -Y X=x,M (X=x * )<label>(4)</label></formula><p>Based on the above definition, we translate the Med-VQA task into a causal effect graph as Fig. <ref type="figure" target="#fig_0">2</ref> (c)(d) shown, aiming to directly formulate the language bias and subtract it. The answer set A = {a} is caused by direct effect from medical image V = v and clinic question Q = q, also the indirect effect of fusion knowledge K(Q = q, V = v) through the cross-modal fusion module. We define the notations that:</p><formula xml:id="formula_6">Y q ,v ,k = Y (Q = q, V = v, K = k).</formula><p>Through subtracting NDE of Q = q on A from the TE of V = v, Q = q and K = k on the answer, we can explicitly capture language bias and remove it via TIE, which is defined below. In the inference stage, we choose the answer with the maximum TIE as the prediction.</p><formula xml:id="formula_7">T IE = T E -NDE = Y q ,v ,k -Y q ,v * ,k * (5)</formula><p>where k * = K(V = v * , Q = q * ), v * and q * is the counterfactual situation where model is without v, q as inputs. The Y q ,v ,k = log σ(Z q + Z v + Z k ), where the <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_8">Z v = E V (v), Z q = E Q (q), Z k = E F (q, v) are calculated from the image encoder E V , question encoders E Q and the fusion module E F respectively. The E V , E Q , E F can be updated by L cls</formula><formula xml:id="formula_9">L cls (q, v, a) = L V QA (q, v, a) + L QA (q, a) + L V A (v, a)<label>(6)</label></formula><p>where L V QA , L QA and L V A are corss-entropy losses over Y q ,v ,k , Z q and Z v .</p><p>The complete objective of our method is optimized to minimize the L DeBCF which combines the L cls over both the original and the counterfactual data:</p><formula xml:id="formula_10">L DeBCF = αL cls (V, Q, A) + (1 -α)L cls (V, Q -, A -) (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where α is the hyperparameter which control the ratio of counterfactual samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SLAKE-CP: Construction and Analysis</head><p>For further evaluating the debiasing ability of Med-VQA, we follow VQA-CP <ref type="bibr" target="#b0">[1]</ref> to create a bias-sensitive Med-VQA dataset which can be called SLAKE-CP. The SLAKE-CP can be further adopted by future debiased Med-VQA researches.</p><p>Grouping. We first construct all image-question-answer samples of train-set and test-set in SLAKE <ref type="bibr" target="#b15">[16]</ref> into a whole set together. We start by labeling each question with a question type (first few words). If the samples have the same question type and answer, then these samples can be divided into same group.</p><p>Re-Splitting. We re-split the SLAKE <ref type="bibr" target="#b15">[16]</ref> dataset to construct disparate distribution as Fig. <ref type="figure" target="#fig_0">2 (a)</ref> shows. In detail, we first assign 1 group to the test-set. Among the remaining groups, if there is a group with a different question type or answer from the groups in test-set, this group will be assigned to test-set otherwise to train-set, aiming to vary the prior distributions of the train and test while remaining unchanged distributions of the images. The iteration stops when the test-set approximately reaches 1/7rd of the whole set, and the remaining are added to the train-set. We ensure the newly constructed test-set and train-set cover the majority of question types ("Is", "What", "Where", "Which", etc.) after these procedures. Most of the data attributes of SLAKE-CP are consistent with SLAKE, such as the train-test splitting, and open-close type splitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Implementation Details</head><p>Datasets. SLAKE <ref type="bibr" target="#b15">[16]</ref> is a knowledge-augmented Med-VQA dataset, consisting of 642 images and 7033 question-answer samples. The VQA-RAD <ref type="bibr" target="#b11">[12]</ref> is a manually annotated dataset validated by clinicians, which contains 315 radiographic images and 3,515 question-answer samples. We followed the original data partition, where questions are divided into closed-ended and open-ended types.</p><p>Implementation Details. For implementation, we apply Pytorch library with 6 NVIDIA TITAN 24 GB Xp GPUs. We employ the MEVF <ref type="bibr" target="#b18">[19]</ref> as baseline.  The vision encoders are initialized by MAML <ref type="bibr" target="#b6">[7]</ref> and CDAE <ref type="bibr" target="#b17">[18]</ref>, and LSTM is adopted as question encoder. The BAN <ref type="bibr" target="#b9">[10]</ref> is adopted as the fusion module E F . The medical images are resized into 224 × 224, and questions are cut to 12 words and then embed into 300 dimensions through Golve. The proposed model is trained for 200 epochs with 64 batch size and optimized with Adam whose learning rate is 1e -3 . In Sect. 2.1, we choose top-1 candidate answer as A + and mask top-1 critical clinic word, the hyperparameter α is set to 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Comparison with State-of-the-Art Methods. We compare our DeBCF with 10 state-of-the-art Med-VQA models on the SLAKE <ref type="bibr" target="#b15">[16]</ref> and VQA-RAD <ref type="bibr" target="#b11">[12]</ref> public benchmarks as the Table <ref type="table" target="#tab_1">1</ref> shown. The proposed model obviously outperform the existing advanced models, attaining 82.6% and 71.6% mean accuracy respectively. Specifically, the results of our proposed model have prominent improvements over the attention-based models MFB <ref type="bibr" target="#b29">[29]</ref>, SAN <ref type="bibr" target="#b28">[28]</ref>, BAN <ref type="bibr" target="#b9">[10]</ref>. Further, the improvements over MEVF+BAN <ref type="bibr" target="#b30">[30]</ref> and CPRD+BAN <ref type="bibr" target="#b14">[15]</ref> which adopt the same fusion model BAN <ref type="bibr" target="#b9">[10]</ref> as ours are 4.0%, 1.5% overall accuracy on SLAKE respectively. In particular, the proposed model conspicuously improved the overall accuracy by 2.5% and compared with the advanced CLIPQCR <ref type="bibr" target="#b5">[6]</ref>. Moreover, our model has significant superior with other debiasing models, including RUBi <ref type="bibr" target="#b1">[2]</ref> and LPF <ref type="bibr" target="#b12">[13]</ref>, GGE <ref type="bibr" target="#b7">[8]</ref>. Although these works can effectively reduce language bias, they reckon without visuallinguist explicable information and contrarily weaken the inference ability. For ours, we explicitly subtract the language bias through causal effect and generate counterfactual samples to implicitly improve the sensitivity of clinical words and visual objects for inference.</p><p>Discussion of SLAKE-CP. Table <ref type="table" target="#tab_2">2</ref> illustrates the superiority of the DeBCF on the newly constructed SLAKE-CP datasets which is the linguistic-bias sensitive evaluation. In particular, the DeBCF yields 34.2% mean overall accuracy on SLAKE-CP datasets. The performance of all the models has prominently dropped in the newly unbiased SLAKE-CP datasets compared with the SLAKE. It is obviously observed that the DeBCF significantly outperforms the baselines <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref> and the debiasing methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>. The proposed model is also superior to the advanced models CLIPQCR <ref type="bibr" target="#b5">[6]</ref> and CPRD+BAN <ref type="bibr" target="#b14">[15]</ref>, over-passing 4.2% and 3.8% overall accuracy. Within the bias-sensitive benchmarks, the comparisons demonstrate that our model may have the superiority to overcome the linguistic priors and force the model to generate more creditable answers rather than taking the superficial linguistic correlations as a shortcut.</p><p>Ablation Analysis. Table <ref type="table" target="#tab_4">3</ref> demonstrates the ablation study which verifies the effectiveness of devised methods. We adopt MEVF+BAN <ref type="bibr" target="#b9">[10]</ref> as the baseline in index 1. The baseline equipped with the counterfactual data preparation stage gains 1.5% and 0.6% overall accuracy on SLAKE-CP and SLAKE datasets. This illustrates that masking critical clinical objects contributes to the implicit suppression of linguistic bias in Med-VQA. In addition, the comparison between index 3 and 1 demonstrates that subtracting the cause-effect of the question can explicitly weaken the prior and modifies the model to focus on intrinsically meaningful objects rather than superficial counterfactual correlations. Moreover, the model which combines the counterfactual masking samples into the counterfactual causal effect training procedure in index 4 obtains significant gains by up to 5.2% and 2.8% overall accuracy on SLAKE-CP and SLAKE, illustrating that we have built a robust unbiased Med-VQA model to overcome language priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of Hyperparameters.</head><p>The influence results of the top-K and the hyperparameter α are conducted in Table <ref type="table" target="#tab_5">4</ref> and 5, which reveal that choosing top-1 critical words and α = 0.6 achieves the best performance respectively. Crucially, masking top-1 critical clinic word can disentangle the linguistic bias and redundancy masking may result in interference.</p><p>Quantitative Analysis. As Fig. <ref type="figure">3</ref> shown, we conduct a quantitative comparison analysis to illustrate the ability to disengage the language prior to our proposed model through Grad-CAM maps <ref type="bibr" target="#b21">[22]</ref>. For example 1 in row 1, the proposed DeBCF sensitively recognizes the precise critical keywords "Where, is, liver" and corresponding visual image objects to predict the correct answer with the highest probability score, while the advanced model MEVF+BAN <ref type="bibr" target="#b18">[19]</ref> is subjected to the language prior that generate the wrong answer according to the superficial context "Where is" and ignore the reliable visual objects. Additionally, we also conduct the comparison of sensitivity to the visual grounds in Fig. <ref type="figure">4</ref>.</p><p>Given the same question but different medical images and answers, the proposed model correctly predict the various answers while the MEVF+BAN <ref type="bibr" target="#b18">[19]</ref> fails. The detailed comparisons illustrate the debiased ability of the proposed model to overcome language priors and ingeniously grasp the critical parts (clinic keywords and visual objects) for a precise explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel debiasing Med-VQA model that prepares the counterfactual data by masking critical clinic words and combines it into the counterfactual training stage which subtracting the causal effect of language priors directly, aiming to migrate the linguistic-bias in Med-VQA. Additionally, we construct a linguistic-bias sensitive Med-VQA dataset SLAKE-CP by disintegrating the language priors from training. Experimental results demonstrate the superior debiasing and interpretive performance of the proposed model. It's the first attempt to construct a preliminary bias-sensitive Med-VQA dataset, which will be elaborated in our future work. The codes will be released.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Various distribution priors of train-set and test-set in SLAKE-CP. (b) Counterfactual training data preparation. (c) Traditional Med-VQA causal graph with total effect. (d) Counterfactual Med-VQA causal graph with natural direct effect.</figDesc><graphic coords="3,41,79,54,23,340,33,191,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Quantitative comparison analysis. The darker parts, the more contributions.</figDesc><graphic coords="8,59,46,199,61,333,43,85,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The comparison results. * indicates our re-implemented result, including the mean accuracy and standard deviation by 5 runs under 5 different seeds.</figDesc><table><row><cell>Methods</cell><cell>SLAKE</cell><cell></cell><cell></cell><cell>VQA-RAD</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Open</cell><cell>Closed</cell><cell>All</cell><cell>Open</cell><cell>Closed</cell><cell>All</cell></row><row><cell>MFB [29]</cell><cell>72.2</cell><cell>75.0</cell><cell>73.3</cell><cell>14.5</cell><cell>74.3</cell><cell>50.6</cell></row><row><cell>SAN [28]</cell><cell>74.0</cell><cell>79.1</cell><cell>76.0</cell><cell>31.3</cell><cell>69.5</cell><cell>54.3</cell></row><row><cell>BAN [10]</cell><cell>74.6</cell><cell>79.1</cell><cell>76.3</cell><cell>37.4</cell><cell>72.1</cell><cell>58.3</cell></row><row><cell>LPF(*) [13]</cell><cell cols="6">74.8±1.4% 77.0±1.1% 74.9±1.3% 41.7±1.3% 72.1±1.1% 60.9±1.3%</cell></row><row><cell>RUBi(*) [2]</cell><cell cols="6">75.1±1.2% 77.6±1.3% 75.8±1.3% 42.4±1.2% 73.2±1.0% 61.5±1.2%</cell></row><row><cell>GGE(*) [8]</cell><cell cols="6">76.4±1.1% 78.7±1.2% 76.6±1.2% 44.6±1.4% 74.5±1.1% 63.8±1.1%</cell></row><row><cell cols="2">MEVF+SAN [19] 75.3</cell><cell>78.4</cell><cell>76.5</cell><cell>49.2</cell><cell>73.9</cell><cell>64.1</cell></row><row><cell cols="2">MEVF+BAN [19] 77.8</cell><cell>79.8</cell><cell>78.6</cell><cell>49.2</cell><cell>77.2</cell><cell>66.1</cell></row><row><cell cols="7">CLIPQCR(*) [6] 78.2±1.3% 82.6±1.5% 80.1±1.3% 58.0±1.4% 79.6±1.1% 71.1±1.2%</cell></row><row><cell cols="2">CPRD+BAN [15] 79.5</cell><cell>83.4</cell><cell>81.1</cell><cell>52.5</cell><cell>77.9</cell><cell>67.8</cell></row><row><cell>Ours</cell><cell cols="6">80.8±0.9% 84.9±0.7% 82.6±0.9% 58.6±1.1% 80.9±0.8% 71.6±1.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The additional comparison of experimental results on the SLAKE-CP dataset.</figDesc><table><row><cell>Methods</cell><cell>SLAKE-CP</cell><cell></cell></row><row><cell></cell><cell>Open</cell><cell>Closed</cell><cell>All</cell></row><row><cell>MFB(*) [29]</cell><cell cols="3">10.9±1.0% 22.1±0.8% 21.5±0.8%</cell></row><row><cell>SAN(*) [28]</cell><cell cols="3">11.2±1.2% 22.7±1.1% 23.2±1.1%</cell></row><row><cell>BAN(*) [10]</cell><cell cols="3">11.9±1.2% 24.4±0.9% 24.5±1.0%</cell></row><row><cell>RUBi(*) [2]</cell><cell cols="3">12.2±1.3% 26.9±1.2% 26.4±1.3%</cell></row><row><cell>LPF(*) [13]</cell><cell cols="3">13.1±1.4% 29.7±1.4% 29.2±1.3%</cell></row><row><cell>GGE(*) [8]</cell><cell cols="3">13.9±1.1% 30.9±1.3% 30.2±1.3%</cell></row><row><cell cols="4">MEVF+SAN(*) [19] 12.6±1.1% 29.6±1.0% 28.7±1.0%</cell></row><row><cell cols="4">MEVF+BAN(*) [19] 13.0±1.4% 29.8±1.2% 29.1±1.3%</cell></row><row><cell>CLIPQCR(*) [6]</cell><cell cols="3">13.4±1.2% 30.5±1.1% 30.0±1.2%</cell></row><row><cell cols="4">CPRD+BAN(*) [15] 13.9±1.3% 31.2±1.5% 30.4±1.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Ours 18.6±1.1% 35.4±1.0% 34.2±1.2%</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation results. "CT D : counterfactual training data preparation. "CCE : counterfactual cause effect training procedure.</figDesc><table><row><cell cols="4">Index CTD CCE SLAKE-CP</cell><cell></cell><cell></cell><cell>SLAKE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Open</cell><cell>Closed</cell><cell>Overall</cell><cell>Open</cell><cell>Closed</cell><cell>Overall</cell></row><row><cell>1</cell><cell>×</cell><cell>×</cell><cell cols="4">13.0±0.6% 29.8±0.9% 29.1±0.7% 78.6±1.2% 80.5±1.0% 79.8±1.0%</cell></row><row><cell>2</cell><cell></cell><cell>×</cell><cell cols="4">14.2±1.3% 31.3±0.9% 30.6±1.0% 79.4±1.3% 81.0±1.1% 80.4±1.1%</cell></row><row><cell>3</cell><cell>×</cell><cell></cell><cell cols="4">16.7±0.8% 33.9±0.7% 32.9±0.8% 80.1±1.1% 81.9±1.3% 81.5±1.2%</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell cols="4">18.6±1.1% 35.4±1.0% 34.2±1.2% 80.6±0.9% 84.4 ±0.7% 82.6±0.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of different top-K critical words in Sect. 2.1.</figDesc><table><row><cell cols="2">top-K SLAKE</cell><cell></cell></row><row><cell></cell><cell>Open</cell><cell>Closed</cell><cell>All</cell></row><row><cell>1</cell><cell cols="3">80.8±1.1% 84.9±0.8% 82.6±1.0%</cell></row><row><cell>2</cell><cell cols="3">80.5±1.0% 84.5±0.8% 82.4±0.9%</cell></row><row><cell>3</cell><cell cols="3">79.9±1.2% 83.7±1.1% 82.1±1.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Evaluations of hyperparameter α.</figDesc><table><row><cell>α</cell><cell>SLAKE</cell><cell></cell></row><row><cell></cell><cell>Open</cell><cell>Closed</cell><cell>All</cell></row><row><cell cols="4">0.3 80.1±1.0% 83.2±0.9% 81.9±1.1%</cell></row><row><cell cols="4">0.4 80.2±1.2% 83.5±1.1% 82.0±1.2%</cell></row><row><cell cols="4">0.5 80.7±1.2% 84.6±0.9% 82.5±1.1%</cell></row><row><cell cols="4">0.6 80.8±1.1% 84.9±1.0% 82.6±1.0%</cell></row><row><cell cols="4">0.7 80.6±1.3% 84.2±1.0% 82.3±1.0%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> (<rs type="grantNumber">LDT23F02023F02</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_abn5dQf">
					<idno type="grant-number">LDT23F02023F02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: overcoming priors for visual question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4971" to="4980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">RUBi: reducing unimodal biases for visual question answering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Counterfactual samples synthesizing for robust visual question answering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10800" to="10809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-modal masked autoencoders for medical vision-and-language pre-training</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_65</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-965" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13435</biblScope>
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>MIC-CAI 2022</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiple meta-model quantifying for medical visual question answering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-37" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="64" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Does clip benefit visual question answering in the medical domain as much as</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meinel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13906</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>it does in the general domain? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Greedy gradient ensemble for robust visual question answering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer vision</title>
		<meeting>the IEEE/CVF International Conference on Computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1584" to="1593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overcoming language priors in VQA via decomposed linguistic representations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11181" to="11188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reducing language biases in visual question answering with visually-grounded question encoder</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58601-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58601-02" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12358</biblScope>
			<biblScope unit="page" from="18" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2018.251</idno>
		<ptr target="https://doi.org/10.1038/sdata.2018.251" />
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180251</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LPF: a language-prior feedback objective function for debiased visual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1955" to="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to contrast the counterfactual samples for robust visual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3285" to="3292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive pre-training and representation distillation for medical visual question answering based on radiology images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-320" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SLAKE: a semanticallylabeled knowledge-enhanced dataset for medical visual question answering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI48211.2021.9434010</idno>
		<ptr target="https://doi.org/10.1109/ISBI48211.2021.9434010" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1650" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Medical visual question answering via conditional reasoning and contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2022.3232411</idno>
		<ptr target="https://doi.org/10.1109/TMI.2022.3232411" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked convolutional autoencoders for hierarchical feature extraction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-21735-7_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-21735-77" />
	</analytic>
	<monogr>
		<title level="m">ICANN 2011</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Honkela</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Duch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Girolami</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6791</biblScope>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overcoming data limitation in medical visual question answering</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-957" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Counterfactual VQA: a cause-effect look at language bias</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12700" to="12710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Direct and indirect effects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 17th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="411" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Taking a hint: Leveraging explanations to make vision and language models more grounded</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2591" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Consistency-preserving visual question answering in medical imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tascon-Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-137" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="386" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning what makes a difference from counterfactual examples and gradient supervision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Abbasnedjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12355</biblScope>
			<biblScope unit="page" from="580" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58607-2_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58607-234" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-critical reasoning for robust visual question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with coattention learning for visual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Medical visual question answering via conditional reasoning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2345" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
