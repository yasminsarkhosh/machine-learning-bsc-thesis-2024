<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Dwarikanath</forename><surname>Mahapatra</surname></persName>
							<email>dwarikanath.mahapatra@inceptioniai.org</email>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonio</forename><forename type="middle">Jose</forename><surname>Jimeno Yepes</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shiba</forename><surname>Kuanar</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Mayo Clinic</orgName>
								<address>
									<settlement>Rochester</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sudipta</forename><surname>Roy</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Jio Institute</orgName>
								<orgName type="institution" key="instit2">Navi Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Behzad</forename><surname>Bozorgtabar</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Lausanne University Hospital (CHUV)</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mauricio</forename><surname>Reyes</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">University of Bern</orgName>
								<address>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
							<affiliation key="aff8">
								<orgName type="laboratory">AIM for Health Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<region>Victoria</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="276" to="286"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CB272A60E0CCC5AE5041B27B90D5FB23</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-label</term>
					<term>GZSL</term>
					<term>Text Embeddings</term>
					<term>chest x-rays</term>
					<term>feature synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robustness of medical image classification models is limited by its exposure to the candidate disease classes. Generalized zero shot learning (GZSL) aims at correctly predicting seen and unseen classes and most current GZSL approaches have focused on the single label case. It is common for chest x-rays to be labelled with multiple disease classes. We propose a novel multi-label GZSL approach using: 1) class specific feature disentanglement and 2) semantic relationship between disease labels distilled from BERT models pre-trained on biomedical literature. We learn a dictionary from distilled text embeddings, and leverage them to synthesize feature vectors that are representative of multi-label samples. Compared to existing methods, our approach does not require class attribute vectors, which are an essential part of GZSL methods for natural images but are not available for medical images. Our approach outperforms state of the art GZSL methods for chest xray images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning methods provide state-of-the-art (SOTA) performance for a variety of medical image analysis tasks such as diabetic retinopathy grading <ref type="bibr" target="#b6">[7]</ref>, and chest X-ray diagnosis <ref type="bibr" target="#b9">[10]</ref>, to name a few. SOTA fully supervised methods have access to all classes as part of the training data whereas most real world clinical applications do not provide access to all classes which leads to unseen classes being wrongly diagnosed as one of the seen classes. Zero-Shot Learning (ZSL) aims to classify unseen test data by learning their plausible representations from seen class features, and in Generalized Zero-Shot Learning (GZSL) the model should accurately classify both seen and unseen classes during test time.</p><p>Previous works on GZSL in medical images have focused on the single class scenario where an image is assigned a single disease class <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21]</ref>. However, chest X-ray images have multiple labels and single-label methods do not work well in this setting. Hence we propose a multi-label GZSL approach that takes into account the semantic relationship between the multiple disease labels and learns a highly discriminative feature representation.</p><p>GZSL for natural images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">22]</ref> have the advantage of providing attribute vectors for all classes that enables a model to correlate between attribute vectors and corresponding feature representations of the seen classes. Defining unambiguous attribute vectors for medical images requires deep clinical expertise and time. This is more challenging for the multi-label scenario, where many disease conditions have similar appearances and textures. For example, in lung X-ray diagnosis, many conditions frequently co-occur with labels such as Atelectasis, Effusion, and Infiltration. An effective class attribute vector should be able to precisely identify individual labels and differentiate them from other co-occurring disease labels, which is very challenging to define. To overcome the above challenges, we make the following contributions:</p><p>1. We propose a novel feature disentanglement method where a given image is decomposed into class-specific and class agnostic features. This enables better feature learning of different classes and subsequently contributes to better feature synthesis in the multi-label scenario. 2. We use text embedding similarities to learn the semantic relationships between different labels. This contributes to more accurate learning of multilabel interactions at a global scale and guide feature generation to synthesize feature vectors that are realistic and preserve the multi-label relationship between disease labels. 3. We solve the GZSL classification problem in terms of cluster assignment.</p><p>Class specific feature disentanglement performs better for multi-label classification <ref type="bibr" target="#b10">[11]</ref> and we use this concept to synthesize unseen class features and subsequently perform classification.</p><p>Prior Work: GZSL's objective is to recognize images from known and unknown classes. Many works have shown promising results using GANs <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b26">26]</ref>, and Intra-Class Compactness Enhancement <ref type="bibr" target="#b11">[12]</ref>. Recent works on multi-label zero-shot learning (ML-ZSL) use information propagation <ref type="bibr" target="#b13">[14]</ref>, attention mechanisms <ref type="bibr" target="#b8">[9]</ref> and co-occurrence statistics with weighted combinations of seen classes <ref type="bibr" target="#b19">[19]</ref>. ZSL in medical image analysis is a much less explored topic with limited applications such as registration <ref type="bibr" target="#b12">[13]</ref>, segmentation <ref type="bibr" target="#b0">[1]</ref>, gleason grading <ref type="bibr" target="#b15">[16]</ref> and artifact reduction <ref type="bibr" target="#b3">[4]</ref>. <ref type="bibr" target="#b21">[21]</ref> used multi-modal images and medical reports for GZSL of chest xray (CXR) images while <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref> used saliency maps and GANs for GZSL using only CXRs.Recently, language models pre-trained on large corpora have also been considered for GZSL of CXRs <ref type="bibr" target="#b7">[8]</ref>. However all the above works operate in the single label setting, while we solve the multi-label problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Method Overview: Given training data with seen classes we: 1) create a dictionary from the text embedddings; 2) disentangle the image into class specific and class agnostic features; 3) use class specific features to generate features of seen and unseen classes using the Mixup approach <ref type="bibr" target="#b28">[28]</ref>; 4) for a given test image apply feature disentanglement and feature similarity analysis to identify the different class labels in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embeddings:</head><p>We generate embeddings of image class labels using BioBERT <ref type="bibr" target="#b14">[15]</ref>, a BERT <ref type="bibr" target="#b4">[5]</ref>-like pre-trained model. BioBERT <ref type="bibr" target="#b14">[15]</ref> is pre-trained on biomedical literature, more specifically the model available from Huggingface<ref type="foot" target="#foot_0">1</ref> , which is a base and cased model. We consider a pooled set that produces a single 768 dimension vector for a label. We then calculate the cosine similarity between each of the labels and represent it as a matrix, which we refer to as Dict T extdictionary for text embeddings, shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Disentanglement</head><p>Our feature disentanglement method is inspired from <ref type="bibr" target="#b20">[20]</ref> which decomposes the feature space into shape and texture for domain adaptation applications. We decompose the feature space of the seen class samples into 'class-specific' . This is achieved by having two heads instead of one (as in conventional architectures). Both vectors are combined and fed to the decoder, G n , which reconstructs the original input. The disentanglement network is trained using the following loss:</p><formula xml:id="formula_0">L Disent = L Rec + λ 1 L spec + λ 2 L agn + λ 3 L agn-spec (1)</formula><p>Reconstruction Loss: L Rec , is the commonly used image reconstruction loss:</p><formula xml:id="formula_1">L Rec = l E xi∼p l x l i -G l (E l (x l i )) .</formula><p>It is a sum of the reconstruction losses from the class specific autoencoders. We train different autoencoders for images of each class in order to obtain class specific features and refer to them as 'Classspecific autoencoders'.</p><p>Class Specific Loss: For given class l the class specific component z spec l i will have high similarity with samples from the same class and low similarity with the z spec k i of other classes k. These two conditions are incorporated as follows:</p><formula xml:id="formula_2">L spec = i,j ⎛ ⎝ l ⎛ ⎝ 1 -z spec l i , z spec l j + k =l z spec l i , z spec k j ⎞ ⎠ ⎞ ⎠<label>(2)</label></formula><p>where . denotes cosine similarity. The sum is calculated for all classes indexed by l and over all samples indexed by i, j.</p><p>Class Agnostic Loss: Class agnostic features of different classes have similar semantic content and have high cosine similarity. L agn is defined as</p><formula xml:id="formula_3">L agn = i,j l k =l 1 -z agn l i , z agn k j (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>We want class specific and class agnostic features of same-class samples to be mutually complementary and have minimal overlap in semantic content, i.e.,</p><formula xml:id="formula_5">L agn-spec = l z agn l i , z spec l j (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Since the above loss terms are minimized it helps us achieve our stated objectives.  Feature Generation Network: After disentangling the different seen class samples into their class specific components we create a distribution of each seen class feature. We generate synthetic class specific features of unseen classes using the following approach inspired by Mixup <ref type="bibr" target="#b28">[28]</ref>:</p><formula xml:id="formula_7">z specU = l Λ l z specS l ; ŷ = y l (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where z specU k is the class specific synthetic vector for unseen classes k( = l), z specS l is a feature sampled from the distribution of seen class l, Λ l is a random number drawn from a beta distribution. ŷ is a one-hot encoded vector and is a sum of the one-hot label vectors of individual classes. Hence we do not need a weight when combining the label vectors. The weights Λ l are such that l Λ l = 1. Generating unseen class features through Mixup without additional constraints can generate unrealistic features. We use the dictionary of text embeddings to guide the feature generation process. As synthetic features of the seen and unseen classes are generated we cluster them using the online self supervised learning based SwAV method <ref type="bibr" target="#b2">[3]</ref> and calculate the centroids of each cluster. The semantic similarity of the centroid clusters should be such that their cosine similarity values are close to those obtained in Table <ref type="table" target="#tab_0">1</ref>, i.e., we define a loss:</p><formula xml:id="formula_9">L ML-Cluster = 1 N 2 i j Dict T ext (i, j) -Cent All (i, j)<label>(6)</label></formula><p>where Cent All refers to the changing matrix of cluster centroid similarities for all seen and unseen classes. N is the total number of classes. The final loss term for clustering all class samples is L Clust = L(x s , x t ) + λ 4 L ML-Cluster where L(x s , x t ) is the SwAV loss function defined in <ref type="bibr" target="#b2">[3]</ref>. We add only those synthetic samples to classifier training data that reduce L Clust . This formulation ensures that the cluster output is well separated semantically and the cluster centroids follow the semantic relationship between all classes in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Training, Inference and Implementation: For a given test image we use the pre-trained L class specific autoencoders to get the class specific features.</p><p>An input 256 × 256 image is passed through the Encoder having 3 convolution layers (64, 32, 32 3 × 3 filters ) each followed by max pooling. The Decoder is symmetric to the Encoder. z agn and z spec are 256-dimension vectors. We then calculate the cosine similarity of the class specific features with the corresponding class centroids. If the cosine similarity is above 0.5 then the sample is assigned to the class. Following standard practice for GZSL, average class accuracies are calculated for the seen (Acc S ) and unseen (Acc U ) classes, and also the harmonic mean H = 2×AccU ×AccS AccU +AccS .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Dataset Description. We demonstrate our method's effectiveness on the following chest xray datasets for multi-label classification tasks: 1.NIH Chest X-ray Dataset <ref type="bibr" target="#b24">[24]</ref>: having 112, 120 expert-annotated frontal-view X-rays from 30, 805 unique patients and has 14 disease labels. Original images were resized to 224 × 224. Hyperparameter values are λ 1 = 1.1, λ 2 = 0.7, λ 3 = 0.9, λ 4 = 1.2. Comparison Methods: We compare our method's performance with multiple GZSL methods -single label and multi-label techniques -employing different feature generation approaches such as CVAE or GANs. Our method is denoted as ML-GZSL (Multi Label GZSL). Our benchmark is a fully supervised learning (FSL) based method of <ref type="bibr" target="#b27">[27]</ref> which is the top ranked method for <ref type="bibr" target="#b9">[10]</ref>, where the ranking is based on AUC. It builds upon a DenseNet-121 trained for multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CheXpert</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generalized Zero Shot Learning Results</head><p>Classification results for medical images in Table <ref type="table" target="#tab_2">2</ref> show our proposed method significantly outperforms all competing GZSL methods. Note that we use the cluster centroids in place of attribute vectors for these feature synthesis methods. This significant difference in performance can be explained by the fact that the complex architectures that worked for natural images will not be equally effective for medical images which have less information. Absence of attribute vectors for medical images is another contributing factor. The class attributes provide a rich source of information about natural images which can be leveraged using existing architectures. Since those are not available for medical images these methods do not perform equally well. Different combinations of 7 seen and unseen classes are taken, and for each combination we run our model 5 times and the final reported numbers are the average of different combinations. ML-GZSL's performance is almost equal to that of the benchmark fully supervised method FSL. Although GZSL methods generally perform inferior to FSL methods, our use of class specific features significantly improves performance. Additionally, the use of semantic relation between text embeddings significantly improves the performance due to better feature synthesis. The average accuracy is obtained by first calculating True Positive, False Positive, True Negative, False Negative values and using these values to get the global accuracy. Furthermore the AUC(and F1) values for CheXpert data are as follows: FSL-93.0(91.7), ML-GZSL-92.8(91.6), <ref type="bibr" target="#b18">[18]</ref>-91.9(90.0), <ref type="bibr" target="#b7">[8]</ref>-84.3(82.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Studies</head><p>Table <ref type="table" target="#tab_3">3</ref> shows results for ablation studies. We exclude each of the three loss terms related to feature disentanglement -L agn ,L spec and L agn-spec -and report the results as ML-GZSL wLagn , ML-GZSL wLspec , and ML-GZSL wLagn-spec . We also compare with the results of using image features obtained from a CNN based feature extractor (ResNet50 trained on Imagenet), which we denote as 'pretrain'. We observe that the class specific features has the greatest influence on the results and excluding it, ML-GZSL wLspec , results in significant performance degradation compared to ML-GZSL. ML-GZSL wLagn-spec and ML-GZSL wLagn also show significantly lower performance. These results highlight the importance of the class specific features and at the same time illustrate class agnostic features have an important influence on the method's performance.</p><p>We also investigate the influence of L ML-Cluster (Eq. 6) in the clustering process. The numbers in Table <ref type="table" target="#tab_3">3</ref> show that ML-GZSL wL ML-Cluster (which is essentially the original SwAV algorithm) performs much worse. This proves the significant contribution of the text embedding dictionary in our multi-label GZSL framework. Hyperparameter Selection: The λ's were varied between [0.4-1.5] in steps of 0.05 and the performance on a separate test set of 10, 000 images were monitored. We optimize Eq. 1 by setting λ 2 = λ3 = λ 4 = 1, and select the optimum value of λ 1 . After fixing λ 1 we determine optimal λ 2 , and subsequently λ 3 , λ 4 .</p><p>Realism of Synthetic Features. We reconstruct the xray images from the synthetic feature vectors using the feature disentanglement autoencoders' decoder part. We select 1000 such synthetic images from 14 classes of the NIH dataset and ask two trained radiologists, having 12 and 14 years experience in examining chest xray images for abnormalities, to identify whether the images are realistic or not. Each radiologist was blinded to the other's answers. Results for ML-GZSL show one radiologist (RAD 1) identified 912/1000 (91.2%) images as realistic while RAD 2 identified 919 (91.9%) generated images as realistic. Both of them had a high agreement with 890 common images (89.0%) identified as realistic. Considering both RAD 1 and RAD 2 feedback, a total of 941 (94.1%) unique images were identified as realistic and 59/1000 (5.9%) images were not identified as realistic by any of the experts. ML-GZSL showed the highest agreement between RAD 1 and RAD 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our experiments demonstrate that our approach of multi label GZSL is more accurate than using conventional approaches that solve the single-label scenario. We propose a novel feature disentanglement approach that obtains class specific and class agnostic features from the training images. Additionally, the relationship between text embeddings of disease labels is used to create a dictionary that guides clustering and feature synthesis. Classification results on multiple publicly available chest xray datasets demonstrate the improved performance obtained by using class specific features. The synthetic features obtained by our method are realistic since a major percentage of the corresponding reconstructed images are validated as realistic by trained clinicians.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Architecture of class specific feature disentanglement network. Given training images from different classes of the same domain we disentangle features into class specific and class agnostic using autoencoders. T-sne results comparison between original image features and feature disentanglement output: (b) Original image features; (c) Class specific features. The classes in the tsne plot correspond to Atelectasis, Consolidation, Effusion, Infiltration and Nodule, as per the standard classes used for CheXpert.</figDesc><graphic coords="5,41,79,53,87,340,21,129,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1 (b) shows the t-sne plots of image features (taken from the fully connected layer of a multi-label DenseNet-121 image classifier) while Fig. 1 (c) shows the plot using class specific features. Plots of original features show overlapping clusters which makes it challenging to have good classification. Clusters obtained using class specific features are well separated with minimal overlap between different clusters. This clearly demonstrates the efficacy of our feature disentanglement method. The features are taken from images belonging to 5 classes from the NIH dataset. We chose 5 classes to clearly demonstrate the output and avoid cluttering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Dataset [10]: consisting of 224, 316 chest radiographs of 65, 240 patients labeled for the presence of 14 common chest conditions. Original images were resized to 224 × 224. Hyperparameter values are λ 1 = 1.2, λ 2 = 0.8, λ 3 = 1.1, λ 4 = 1.1. 3. PadChest Dataset [2]: consisting of 160, 868 from 67, 625 patients. Hyperparameter values are λ 1 = 1.3, λ 2 = 0.9, λ 3 = 0.9, λ 4 = 1.3. A 70/10/20 split at patient level was done to get training, validation and test sets for both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Cosine similarity of the labels' BioBERT embeddings Atl. Card. Cons. Edema Eff. Emph. Fibr. Hernia Inf. Mass No Find Nodule PT Pne. Pneu.</figDesc><table><row><cell>Atelectasis</cell><cell>1.00 0.84 0.93 0.92</cell><cell>0.66 0.99</cell><cell>0.77 0.99</cell><cell>0.93 0.93 0.49</cell><cell>0.70</cell><cell>0.79 0.99 0.89</cell></row><row><cell>Cardiomegaly</cell><cell>0.84 1.00 0.97 0.97</cell><cell>0.93 0.88</cell><cell>0.98 0.83</cell><cell>0.95 0.97 0.81</cell><cell>0.96</cell><cell>0.98 0.87 0.60</cell></row><row><cell>Consolidation</cell><cell>0.93 0.97 1.00 0.99</cell><cell>0.84 0.95</cell><cell>0.93 0.92</cell><cell>0.99 0.99 0.69</cell><cell>0.88</cell><cell>0.93 0.94 0.72</cell></row><row><cell>Edema</cell><cell>0.92 0.97 0.99 1.00</cell><cell>0.86 0.95</cell><cell>0.93 0.91</cell><cell>0.99 0.99 0.70</cell><cell>0.89</cell><cell>0.94 0.94 0.71</cell></row><row><cell>Effusion</cell><cell>0.66 0.93 0.84 0.86</cell><cell>1.00 0.71</cell><cell>0.96 0.65</cell><cell>0.84 0.85 0.91</cell><cell>0.98</cell><cell>0.95 0.70 0.40</cell></row><row><cell>Emphysema</cell><cell>0.99 0.88 0.95 0.95</cell><cell>0.71 1.00</cell><cell>0.82 0.99</cell><cell>0.95 0.95 0.54</cell><cell>0.75</cell><cell>0.83 0.99 0.86</cell></row><row><cell>Fibrosis</cell><cell>0.77 0.98 0.93 0.93</cell><cell>0.96 0.82</cell><cell>1.00 0.76</cell><cell>0.91 0.93 0.87</cell><cell>0.98</cell><cell>0.99 0.80 0.52</cell></row><row><cell>Hernia</cell><cell>0.99 0.83 0.92 0.91</cell><cell>0.65 0.99</cell><cell>0.76 1.00</cell><cell>0.92 0.91 0.48</cell><cell>0.70</cell><cell>0.78 0.99 0.91</cell></row><row><cell>Infiltration</cell><cell>0.93 0.95 0.99 0.99</cell><cell>0.84 0.95</cell><cell>0.91 0.92</cell><cell>1.00 0.99 0.68</cell><cell>0.87</cell><cell>0.92 0.95 0.73</cell></row><row><cell>Mass</cell><cell>0.93 0.97 0.99 0.99</cell><cell>0.85 0.95</cell><cell>0.93 0.91</cell><cell>0.99 1.00 0.70</cell><cell>0.88</cell><cell>0.94 0.95 0.72</cell></row><row><cell>No Finding</cell><cell>0.49 0.81 0.69 0.70</cell><cell>0.91 0.54</cell><cell>0.87 0.48</cell><cell>0.68 0.70 1.00</cell><cell>0.91</cell><cell>0.85 0.53 0.23</cell></row><row><cell>Nodule</cell><cell>0.70 0.96 0.88 0.89</cell><cell>0.98 0.75</cell><cell>0.98 0.70</cell><cell>0.87 0.88 0.91</cell><cell>1.00</cell><cell>0.97 0.74 0.45</cell></row><row><cell cols="2">Pleural Thickening 0.79 0.98 0.93 0.94</cell><cell>0.95 0.83</cell><cell>0.99 0.78</cell><cell>0.92 0.94 0.85</cell><cell>0.97</cell><cell>1.00 0.82 0.54</cell></row><row><cell>Pneumonia</cell><cell>0.99 0.87 0.94 0.94</cell><cell>0.70 0.99</cell><cell>0.80 0.99</cell><cell>0.95 0.95 0.53</cell><cell>0.74</cell><cell>0.82 1.00 0.87</cell></row><row><cell>Pneumothorax</cell><cell>0.89 0.60 0.72 0.71</cell><cell>0.40 0.86</cell><cell>0.52 0.91</cell><cell>0.73 0.72 0.23</cell><cell>0.45</cell><cell>0.54 0.87 1.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and 'class-agnostic' features. Class specific features encode information specific to the particular class, and have low similarity between different classes. Class agnostic features have high similarity across all classes, and have minimal semantic overlap with class specific features. The disentangled features allow for greater accuracy in identifying the multiple labels in a sample. Figure1 (a)shows the architecture of our feature disentanglement network (FDN) consisting of L encoder-decoder networks corresponding to the L classes in the training data. The encoders and decoders (generators) are denoted, respectively, as E l (•) and G l (•)). Similar to a classic autoencoder, the encoder, E n , produces a latent code z i for image x i ∼ p. Furthermore, we divide the latent code, z i , into two vectors: class specific component, z spec l</figDesc><table><row><cell>z agn l i</cell><cell>i</cell><cell>for class l, and a class agnostic component,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 . GZSL Results For chest xray Images in Multi-Label setting:</head><label>2</label><figDesc>Average per-class classification accuracy (%) and harmonic mean accuracy (H) of generalized zero-shot learning when test samples are from seen or unseen classes. Results demonstrate the superior performance of our proposed method.</figDesc><table><row><cell>Method</cell><cell cols="2">NIH X-ray</cell><cell></cell><cell></cell><cell cols="2">CheXpert</cell><cell></cell><cell></cell><cell cols="2">PadChest</cell><cell></cell></row><row><cell></cell><cell>S</cell><cell>U</cell><cell>H</cell><cell>p</cell><cell>S</cell><cell>U</cell><cell>H</cell><cell>p</cell><cell>S</cell><cell>U</cell><cell>H</cell><cell>p</cell></row><row><cell></cell><cell cols="6">Single Label GZSL Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>f-VAEGAN [26]</cell><cell cols="12">82.9 80.0 81.4 0.002 88.5 87.6 88.0 0.001 81.0 78.4 79.7 0.001</cell></row><row><cell>SDGN [25]</cell><cell cols="12">84.4 81.1 82.7 0.003 89.8 88.3 89.0 0.003 82.3 80.0 81.1 0.004</cell></row><row><cell>Feng [6]</cell><cell cols="12">84.7 81.4 83.0 0.0012 90.2 88.6 89.4 0.0017 82.5 80.2 81.3 0.0021</cell></row><row><cell>Kong [12]</cell><cell cols="12">84.8 81.2 82.9 0.0031 90.0 88.7 89.3 0.0034 82.7 80.5 81.6 0.0029</cell></row><row><cell>Su [22]</cell><cell cols="12">84.5 81.4 82.9 0.004 90.3 88.6 89.4 0.0045 82.3 79.8 81.03 0.0041</cell></row><row><cell></cell><cell cols="6">Multi Label GZSL Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hayat [8]</cell><cell cols="12">79.1 69.2 73.8 0.005 81.2 79.8 80.5 0.0056 77.3 68.1 72.4 0.006</cell></row><row><cell>Lee [14]</cell><cell cols="12">85.1 81.3 83.1 0.008 87.4 85.7 86.5 0.0075 82.9 78.4 80.6 0.008</cell></row><row><cell>Huynh [9]</cell><cell cols="12">84.7 80.8 82.7 0.0065 86.9 85.1 86.0 0.0071 82.5 77.3 79.8 0.0073</cell></row><row><cell></cell><cell cols="7">Proposed Method And Benchmarks</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ML-GZSL</cell><cell cols="4">86.2 85.0 85.6 -</cell><cell cols="4">90.8 90.2 90.5 -</cell><cell cols="4">88.2 86.1 87.1 -</cell></row><row><cell cols="13">FSL(Multi Label) 86.0 85.1 85.5 0.061 90.8 90.5 90.6 0.068 88.4 86.5 87.4 0.058</cell></row><row><cell>Mahapatra [18]</cell><cell cols="8">84.3 83.2 83.7 0.014 88.9 88.5 88.7 0.01</cell><cell cols="4">86.2 84.1 85.1 0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation Results: Average per-class classification accuracy (%) and harmonic mean accuracy (H) of generalized zero-shot learning when test samples are from seen (Setting S) or unseen (Setting U ) classes.</figDesc><table><row><cell>Method</cell><cell cols="2">NIH X-ray</cell><cell></cell><cell></cell><cell cols="2">CheXpert</cell><cell></cell><cell></cell><cell cols="2">PadChest</cell><cell></cell></row><row><cell></cell><cell>S</cell><cell>U</cell><cell>H</cell><cell>p</cell><cell>S</cell><cell>U</cell><cell>H</cell><cell>p</cell><cell>S</cell><cell>U</cell><cell>H</cell><cell>p</cell></row><row><cell></cell><cell cols="5">Our Proposed Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ML-GZSL</cell><cell cols="4">86.2 85.0 85.6 -</cell><cell cols="4">90.8 90.2 90.5 -</cell><cell cols="4">88.2 86.1 87.1 -</cell></row><row><cell></cell><cell cols="7">Feature Disentanglement Effects</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wL agn-spec</cell><cell cols="12">83.8 81.9 82.8 0.012 88.6 86.3 87.4 0.009 85.5 82.0 83.7 0.014</cell></row><row><cell>pre-train</cell><cell cols="12">83.4 82.0 82.7 0.017 88.2 85.3 86.7 0.009 85.1 81.7 83.4 0.011</cell></row><row><cell>wLagn</cell><cell cols="12">84.5 82.1 83.3 0.008 89.1 86.9 88.0 0.0094 86.5 83.4 84.9 0.011</cell></row><row><cell>wLspec</cell><cell cols="12">84.0 82.2 83.1 0.02 88.8 86.2 87.5 0.018 86.1 83.0 84.5 0.014</cell></row><row><cell></cell><cell cols="5">Effect of Text Dictionary</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">wL ML-Cluster 82.6 80.7 81.6 0.009 87.0 84.5 85.7 0.011 84.2 80.8 82.5 0.015</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://huggingface.co/dmis-lab/biobert-v1.1.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 26.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation meets zero-shot learning: an annotation-efficient approach to multi-modality medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1043" to="1056" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PadChest: A large chest x-ray image dataset with multi-label annotated reports</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pertusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De La Iglesia-Vayá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">101797</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zero-shot medical image artifact reduction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI45749.2020.9098566</idno>
		<ptr target="https://doi.org/10.1109/ISBI45749.2020.9098566" />
	</analytic>
	<monogr>
		<title level="m">IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="862" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-generative generalized zeroshot learning via task-correlated disentanglement and controllable samples synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9336" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2016.17216</idno>
		<ptr target="https://doi.org/10.1001/jama.2016.17216" />
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-label generalized zero shot learning for the classification of disease in chest radiographs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lashen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shamout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Machine Learning for Healthcare Conference</title>
		<meeting>eeding of the Machine Learning for Healthcare Conference</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="461" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A shared multi-attention framework for multi-label zeroshot learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00880</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00880" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="8773" to="8783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">CheXpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07031</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning disentangled label representations for multi-label classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2212.01461</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2212.01461" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">En-compactness: self-distillation embedding and contrastive generation for generalized zero-shot learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.00909</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.00909" />
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9296" to="9305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Zero shot learning for multi-modal real time image registration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krishnamurthi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06213</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00170</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00170" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1576" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-supervised multimodal generalized zero shot learning for Gleason grading</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuanar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<editor>Albarqouni, S., et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<idno type="DOI">10.1007/978-3-030-87722-4_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87722-45" />
	</analytic>
	<monogr>
		<title level="m">DART/FAIR -2021</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12968</biblScope>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Medical image classification using generalized zero shot learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3344" to="3353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised generalized zero shot learning for medical image classification using novel interpretable saliency maps</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2022.3163232</idno>
		<ptr target="https://doi.org/10.1109/TMI.2022.3163232" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2443" to="2456" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">COSTA: co-occurrence statistics for zeroshot classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.313</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.313" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2441" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swapping autoencoder for deep image manipulation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalized zero-shot chest x-ray diagnosis through trait-guided multi-view semantic embedding with self-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2021.3054817</idno>
		<ptr target="https://doi.org/10.1109/TMI.2021.3054817" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2642" to="2655" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinguishing unseen from seen for generalized zero-shot learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.00773</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.00773" />
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7875" to="7884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized zero-shot learning via synthesized examples</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4281" to="4289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ChestX-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-supervised domainaware generative network for generalized zero-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12767" to="12776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">F-VAEGAN-D2: a feature generating framework for any-shot learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10275" to="10284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale robust deep AUC maximization: A new surrogate loss and empirical studies on medical image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3020" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixup: beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
