<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation</title>
				<funder ref="#_jmgN8v3">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder>
					<orgName type="full">Lower Saxony Ministry of Science and Culture and the Endowed Chair of Applied Artificial Intelligence</orgName>
					<orgName type="abbreviated">AAI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Md</forename><forename type="middle">Abdul</forename><surname>Kadir</surname></persName>
							<idno type="ORCID">0000-0002-8420-2536</idno>
						</author>
						<author>
							<persName><forename type="first">Hasan</forename><forename type="middle">Md Tusfiqur</forename><surname>Alam</surname></persName>
							<idno type="ORCID">0000-0003-1479-7690</idno>
						</author>
						<author role="corresp">
							<persName><forename type="first">Daniel</forename><surname>Sonntag</surname></persName>
							<email>daniel.sonntag@dfki.de</email>
							<idno type="ORCID">0000-0002-8857-8709</idno>
							<affiliation key="aff0">
								<orgName type="department">German Research Center for Artificial Intelligence (DFKI)</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Oldenburg</orgName>
								<address>
									<settlement>Oldenburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="79" to="89"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6254FB7481F34F0C0A45F73924E14CB9</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active Learning</term>
					<term>Deep Learning</term>
					<term>Segmentation</term>
					<term>OCT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Active learning algorithms have become increasingly popular for training models with limited data. However, selecting data for annotation remains a challenging problem due to the limited information available on unseen data. To address this issue, we propose EdgeAL, which utilizes the edge information of unseen images as a priori information for measuring uncertainty. The uncertainty is quantified by analyzing the divergence and entropy in model predictions across edges. This measure is then used to select superpixels for annotation. We demonstrate the effectiveness of EdgeAL on multi-class Optical Coherence Tomography (OCT) segmentation tasks, where we achieved a 99% dice score while reducing the annotation label cost to 12%, 2.3%, and 3%, respectively, on three publicly available datasets (Duke, AROI, and UMN). The source code is available at https://github.com/Mak-Ta-Reque/EdgeAL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, Deep Learning (DL) based methods have achieved considerable success in the medical domain for tasks including disease diagnosis and clinical feature segmentation <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b29">28]</ref>. However, their progress is often constrained as they require large labelled datasets. Labelling medical image data is a labour-intensive and time-consuming process that needs the careful attention of clinical experts. Active learning (AL) can benefit the iterative improvement of any intelligent diagnosis system by reducing the burden of extensive annotation effort <ref type="bibr" target="#b20">[19,</ref><ref type="bibr" target="#b26">25]</ref>.</p><p>Ophthalmologists use the segmentation of ocular Optical Coherence Tomography (OCT) images to diagnose, and treatment of eye diseases such as Diabetic Retinopathy (DR) and Diabetic Macular Edema (DME) <ref type="bibr" target="#b6">[6]</ref>. Here, we propose a novel Edge estimation-based Active Learning EdgeAL framework for OCT image segmentation that leverages prediction uncertainty across the boundaries of the semantic regions of input images. The Edge information is one of the image's most salient features, and it can boost segmentation accuracy when integrated into neural model training <ref type="bibr" target="#b14">[13]</ref>. We formulate a novel acquisition function that leverages the variance of the predicted score across the gradient surface of the input to measure uncertainty. Empirical results show that EdgeAL achieves state-of-the-art performance with minimal annotation samples, using a seed set as small as 2% of unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Active learning is a cost-effective strategy that selects the most informative samples for annotation to improve model performance based on uncertainty <ref type="bibr" target="#b12">[11]</ref>, data distribution <ref type="bibr" target="#b23">[22]</ref>, expected model change <ref type="bibr" target="#b4">[4]</ref>, and other criteria <ref type="bibr" target="#b0">[1]</ref>. A simpler way to measure uncertainty can be realized using posterior probabilities of predictions, such as selecting instances with the least confidence <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b12">11]</ref>, or computing class entropy <ref type="bibr" target="#b15">[14]</ref>. Some uncertainty-based approaches have been directly used with deep neural networks <ref type="bibr" target="#b25">[24]</ref>. Gal et al. <ref type="bibr" target="#b8">[7]</ref> propose dropout-base Monte Carlo (MC) sampling to obtain uncertainty estimation. It uses multiple forward passes with dropout at different layers to generate uncertainty during inference. Ensemble-based methods also have been widely used where the variance between the prediction outcomes from a collection of models serve as the uncertainty <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b28">27]</ref>.</p><p>Many AL methods have been adopted for segmentation tasks <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b19">18]</ref>. Gorriz et al. <ref type="bibr" target="#b9">[8]</ref> propose an AL framework Melanoma segmentation by extending Cost-Effective Active Learning (CEAL) <ref type="bibr" target="#b27">[26]</ref> algorithm where complimentary samples of both high and low confidence are selected for annotation. Mackowiak et al. <ref type="bibr" target="#b16">[15]</ref> use a region-based selection approach and estimate model uncertainty using MC dropout to reduce human-annotation cost. Nath et al. <ref type="bibr" target="#b19">[18]</ref> propose an ensemble-based method where multiple AL frameworks are jointly optimized, and a query-by-committee approach is adopted for sample selection. These methods do not consider any prior information to estimate uncertainty. Authors in <ref type="bibr" target="#b25">[24]</ref> propose an AL framework for multi-view datasets <ref type="bibr" target="#b18">[17]</ref> segmentation task where model uncertainty is estimated based on Kullback-Leibler (KL) divergence of posterior probability distributions for a disjoint subset of prior features such as depth, and camera position.</p><p>However, viewpoint information is not always available in medical imaging. We leverage edge information as a prior for AL sampling based on previous studies where edge information has improved the performance of segmentation tasks <ref type="bibr" target="#b14">[13]</ref>. To our knowledge, there has yet to be any exploration of using image edges as an a priori in active learning.</p><p>There has not been sufficient work other than <ref type="bibr" target="#b13">[12]</ref> related to Active Learning for OCT segmentation. Their approach requires foundation models <ref type="bibr" target="#b11">[10]</ref> to be pre-trained on large-scale datasets in similar domains, which could be infeasible to collect due to data privacy. On the other hand, our method requires a few samples (∼2%) for initial training, overcoming the limitation of the need for a large dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows that our active learning technique consists of four major stages. First, we train the network on a subset of labeled images, usually a tiny percentage of the total collection (e.g., 2%). Following that, we compute uncertainty values for input instances and input areas. Based on this knowledge, we select superpixels to label and obtain annotations from a simulated oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Segmentation Network</head><p>We trained our OCT semantic segmentation model using a randomly selected small portion of the labeled data D s , seed set, keeping the rest for oracle imitation. We choose Y-net-gen-ffc (YN * ) without pre-retrained weight initialization as our primary architecture due to its superior performance <ref type="bibr" target="#b6">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Uncertainty in Prediction</head><p>EdgeAL seeks to improve the model's performance by querying uncertain areas on unlabeled data D u after training it on a seed set D s . To accomplish this, we have created a novel edge-based uncertainty measurement method. We compute the edge entropy score and edge divergence score -to assess the prediction ambiguity associated with the edges. Figure <ref type="figure" target="#fig_1">2</ref> depicts examples of input OCT, measured edge entropy, and edge kl-divergence corresponding to the input.</p><p>Entropy Score on Edges. Analyzing the edges of raw OCT inputs yields critical information on features and texture in images. They may look noisy, but they summarize all the alterations in a picture. The Sobel operator can be used to identify edges in the input image <ref type="bibr" target="#b14">[13]</ref>. Let us define the normalized absolute value of edges of an image I i by S i . |∇I i | is the absolute gradient.</p><formula xml:id="formula_0">S i = |∇I i | -min(|∇I i |) max(|∇I i |) -min(|∇I i |)</formula><p>To determine the probability that each pixel in an image belongs to a particular class c, we use the output of our network, denoted as P (m,n) i (c). We adopt Monte Carlo (MC) dropout simulation for uncertainty sampling and average predictions over |D| occurrence from <ref type="bibr" target="#b8">[7]</ref>. Consequently, an MC probability distribution depicts the chance of a pixel at location (m, n) in picture I i belonging to a class c, and C is the set of segmentation classes. We run MC dropouts |D| times during the neural network assessment mode and measure P (m,n) i (c) using Eq. 1.</p><formula xml:id="formula_1">P (m,n) i (c) = 1 |D| D d=1 P (m,n) i,d (c)<label>(1)</label></formula><p>Following Zhao et al. <ref type="bibr" target="#b31">[30]</ref>, we apply contextual calibration on P (m,n) i (c) by S i to prioritize significant input surface variations. Now, S i is linked with a probability distribution, with φ (m,n) i (c) having information about the edges of input. This formulation makes our implementation unique from other active learning methods in image segmentation.</p><formula xml:id="formula_2">φ m,n i (c) = e P (m,n) i (c)•Si(m,n) k∈C e P (m,n) i (k)•S (m,n) i (2)</formula><p>We name φ m,n i (c) as contextual probability and define our edge entropy by following entropy formula of <ref type="bibr" target="#b15">[14]</ref>.</p><formula xml:id="formula_3">EE m,n i = - c∈C φ m,n i (c) log(φ m,n i (c))<label>(3)</label></formula><p>Divergence Score on Edges. In areas with strong edges/gradients, edge entropy reflects the degree of inconsistency in the network's prediction for each input pixel. However, the degree of this uncertainty must also be measured. KLdivergence is used to measure the difference in inconsistency between P (m,n) i and φ (m,n) i for a pixel (m, n) in an input image based on the idea of self-knowledge distillation I i <ref type="bibr" target="#b30">[29]</ref>. The edge divergence ED m,n i score can be formalized using Eq. 1 and 2. ED</p><formula xml:id="formula_4">(m,n) i = D KL P (m,n) i ||φ (m,n) i</formula><p>where</p><formula xml:id="formula_5">D KL P (m,n) i ||φ (m,n) i</formula><p>measures the difference between model prediction probability and contextual probability for pixels belonging to edges of the input (Fig. <ref type="figure" target="#fig_1">2c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Superpixel Selection</head><p>Clinical images have sparse representation, which can be beneficial for active learning annotation <ref type="bibr" target="#b16">[15]</ref>. We use a traditional segmentation technique, SEEDS <ref type="bibr" target="#b1">[2]</ref>, to leverage the local structure from images for finding superpixels. Annotating superpixels and regions for active learning may be more beneficial to the user than annotating the entire picture <ref type="bibr" target="#b16">[15]</ref>.</p><p>We compute mean edge entropy EE r i and mean edge divergence ED d i for a particular area r within a superpixel. Where |r| is the amount of pixels in the superpixel region. We use regional entropy to find the optimal superpixel for our selection strategy and pick the one with the most significant value based on the literature <ref type="bibr" target="#b25">[24]</ref>.</p><formula xml:id="formula_6">EE r i = 1 |r| (m,n)∈r EE (m,n) i (4) ED r i = 1 |r| (m,n)∈r ED (m,n) i (5)</formula><p>(i, r) = arg max (j,s) EE s j <ref type="bibr" target="#b6">(6)</ref> Following <ref type="bibr" target="#b25">[24]</ref>, we find the subset of superpixels in the dataset with a 50% overlap (r, i). Let us call it set R. We choose the superpixels with the largest edge divergence to determine the ultimate query (sample) for annotation.</p><p>(p, q) = arg max</p><formula xml:id="formula_7">(j,s)∈R ED s j | (j, s) ∩ (i, r); (i, r) ∈ D u )}<label>(7)</label></formula><p>After each selection, we remove the superpixels from R. The selection process runs until we have K amount of superpixels being selected from R.</p><p>After getting the selected superpixel maps, we receive the matching ground truth information for the selected superpixel regions from the oracle. The model is then freshly trained on the updated labeled dataset for the next active learning iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>This section will provide a detailed overview of the datasets and architectures employed in our experiments. Subsequently, we will present the extensive experimental results and compare them with other state-of-the-art methods to showcase the effectiveness of our approach. We compare our AL method with nine well-known strategies: softmax margin (MAR) <ref type="bibr" target="#b10">[9]</ref>, softmax confidence (CONF) <ref type="bibr" target="#b27">[26]</ref>, softmax entropy (ENT) <ref type="bibr" target="#b15">[14]</ref>, MC dropout entropy (MCDR) <ref type="bibr" target="#b8">[7]</ref>, Core-set selection (CORESET) <ref type="bibr" target="#b24">[23]</ref>, (CEAL) <ref type="bibr" target="#b9">[8]</ref>, and regional MC dropout entropy (RMCDR) <ref type="bibr" target="#b16">[15]</ref>, maximum representations (MAXRPR) <ref type="bibr" target="#b28">[27]</ref>, and random selection (Random).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Networks</head><p>To test EdgeAL, we ran experiments on Duke <ref type="bibr" target="#b3">[3]</ref>, AROI <ref type="bibr" target="#b17">[16]</ref>, and UMN <ref type="bibr" target="#b22">[21]</ref> datasets in which experts annotated ground truth segmentations. Duke contains 100 B-scans from 10 patients, AROI contains 1136 B-scans from 24, and UMN contains 725 OCT B-scans from 29 patients. There are nine, eight, and two segmentation classes in Duke, AROI, and UMN, respectively. These classes cover fluid and retinal layers. Based on convention and dataset guidelines <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b17">16]</ref>, we use a 60:20:20 training: testing: validation ratio for the experiment without mixing one patient's data in any of the splits. Further, we resized all the images and ground truths to 224 × 224 using Bilinear approximation. Moreover, we run a 5fold cross-validation (CV) on the Duke dataset without mixing individual patient data in each fold's training, testing, and validation set. Table <ref type="table" target="#tab_0">1</ref> summarizes the 5-fold CV results. We run experiments using Y-net(YN) <ref type="bibr" target="#b6">[6]</ref>, U-net (UN) <ref type="bibr" target="#b11">[10]</ref>, and DeepLab-V3 (DP-V3) <ref type="bibr" target="#b25">[24]</ref> with ResNet and MobileNet backbones <ref type="bibr" target="#b11">[10]</ref>. We present the results in Table <ref type="table" target="#tab_2">2</ref>. No pre-trained weights were employed in the execution of our studies other than the ablation study presented in Table <ref type="table" target="#tab_2">2</ref>. We apply mixed loss of dice and cross-entropy and Adam as an optimizer, with learning rates of 0.005 and weight decay of 0.0004, trained for 100 epochs with a maximum batch size of 10 across all AL iterations. We follow the hyperparameter settings and evaluation metric (dice score) of <ref type="bibr" target="#b6">[6]</ref>, which is the baseline of our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons</head><p>Figure <ref type="figure">3</ref> compares the performance of EdgeAL with other contemporary active learning algorithms across three datasets. Results show EdgeAL outperforms other methods on all 3 datasets. Our method can achieve 99% of maximum model performance consistently with about 12% (∼8 samples), 2.3% (∼16 samples), and 3% (∼14 samples) labeled data on Duke, AROI, and UNM datasets.</p><p>Other AL methods, CEAL, RMCDR, CORESET, and MAR, do not perform consistently in all three datasets. We used the same segmentation network YN * and hyperparameters (described in Sect. 3) for a fair comparison.</p><p>Our 5-fold CV result in Table <ref type="table" target="#tab_0">1</ref> also concludes similarly. We see that after training on a 2% seed set, all methods have similar CV performance; however, Furthermore, to scrutinize if EdgeAL is independent of network architecture and weight initialization, we run experiments on four network architectures with default weight initialization of PyTorch (LeCun initialization)<ref type="foot" target="#foot_0">1</ref> and imagenet weight initialization. Table <ref type="table" target="#tab_2">2</ref> presents the test performance after training on 12% of actively selected data. These results also conclude that EdgeAL's performance is independent of the architecture and weight choices, while other active learning methods (RMCDR, MAXRPR) only perform well in pre-trained models (Table <ref type="table" target="#tab_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>EdgeAL is a novel active learning technique for OCT image segmentation, which can accomplish results similar to full training with a small amount of data by utilizing edge information to identify regions of uncertainty. Our method can reduce the labeling effort by requiring only a portion of an image to annotate and is particularly advantageous in the medical field, where labeled data can be scarce. EdgeAL's success in OCT segmentation suggests that a significant amount of data is not always required to learn data distribution in medical imaging. Edges are a fundamental image characteristic, allowing EdgeAL to be adapted for other domains without significant modifications, which leads us to future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The figure above illustrates the workflow of our AL framework. It first computes an OCT image's edge entropy and edge divergence maps. Later, it calculates the overlaps between superpixels based on the divergence and entropy map to recommend an annotation region.</figDesc><graphic coords="3,65,31,56,60,297,31,119,11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The figures depict an example of (a) OCT slice with corresponding (b) edge entropy map, (c) edge divergence map, (d) query regions for annotation by our EdgeAL. The figures reveal that there is less visibility of retinal layer separation lines on the right side of the OCT slice, which could explain the model's high uncertainty in that region.</figDesc><graphic coords="5,57,48,215,72,337,27,82,27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Figures show sample OCT (Duke) test images with human-annotated ground truth segmentation maps and our prediction results, trained on just 12% of the samples.</figDesc><graphic coords="8,42,30,54,44,339,88,172,27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The Table summarizes 5-fold cross-validation results (mean dice) for active learning methods and EdgeAL on the Duke dataset. EdgeAL outperforms other methods, achieving 99% performance with just 12% annotated data.</figDesc><table><row><cell cols="2">GT(%) RMCDR</cell><cell>CEAL</cell><cell>CORESET EdgeAL</cell><cell>MAR</cell><cell>MAXRPR</cell></row><row><cell>2%</cell><cell cols="3">0.40 ± 0.05 0.40 ± 0.05 0.38 ± 0.04 0</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.40 ± 0.05 0.40 ± 0.09 0.41 ± 0.04</head><label></label><figDesc></figDesc><table><row><cell>12%</cell><cell>0.44 ± 0.04 0.54 ± 0.04 0.44 ± 0.05 0.82 ± 0.03 0.44 ± 0.03 0.54 ± 0.09</cell></row><row><cell>22%</cell><cell>0.63 ± 0.05 0.54 ± 0.04 0.62 ± 0.04 0.83 ± 0.03 0.58 ± 0.04 0.67 ± 0.07</cell></row><row><cell>33%</cell><cell>0.58 ± 0.07 0.55 ± 0.06 0.57 ± 0.04 0.81 ± 0.04 0.67 ± 0.03 0.61 ± 0.03</cell></row><row><cell>43%</cell><cell>0.70 ± 0.03 0.79 ± 0.03 0.69 ± 0.03 0.83 ± 0.02 0.70 ± 0.04 0.80 ± 0.04</cell></row><row><cell>100%</cell><cell>0.82 ± 0.03 0.82 ± 0.03 0.82 ± 0.03 0.82 ± 0.02 0.83 ± 0.02 0.83 ± 0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The table summarizes the test performance (mean dice) of various active learning algorithms on different deep learning architectures, including pre-trained weights, trained on only 12% actively selected data from the Duke dataset. The results (mean ± sd) are averaged after running two times in two random seeds. Superscript 'r' represents ResNet, 'm' represents MobileNet version 3 backbones, and ' †' indicates that the networks are initialized with pre-trained weights from ImageNet<ref type="bibr" target="#b5">[5]</ref>.</figDesc><table><row><cell>Arch.</cell><cell>p100</cell><cell>EdgeAL</cell><cell>CEAL</cell><cell>CORESET RMCDR</cell><cell>MAXRPR</cell></row><row><cell>YN * [6]</cell><cell cols="5">0.83 ± 0.02 0.83 ± 0.01 0.52 ± 0.01 0.45 ± 0.02 0.44 ± 0.01 0.56 ± 0.01</cell></row><row><cell>YN [6]</cell><cell cols="5">0.82 ± 0.02 0.81 ± 0.02 0.48 ± 0.01 0.47 ± 0.02 0.45 ± 0.01 0.53 ± 0.01</cell></row><row><cell>UN [10]</cell><cell cols="5">0.79 ± 0.02 0.80 ± 0.01 0.39 ± 0.01 0.48 ± 0.02 0.63 ± 0.01 0.51 ± 0.01</cell></row><row><cell>DP-V3 r</cell><cell cols="5">0.74 ± 0.04 0.74 ± 0.02 0.62 ± 0.01 0.49 ± 0.01 0.57 ± 0.01 0.61 ± 0.01</cell></row><row><cell cols="6">DP-V3 m 0.61 ± 0.01 0.61 ± 0.01 0.28 ± 0.02 0.25 ± 0.01 0.59 ± 0.02 0.51 ± 0.01</cell></row><row><cell cols="6">DP-V3 r, † 0.78 ± 0.01 0.79 ± 0.01 0.29 ± 0.01 0.68 ± 0.01 0.68 ± 0.01 0.73 ± 0.01</cell></row><row><cell cols="6">DP-V3 m, † 0.78 ± 0.01 0.79 ± 0.01 0.18 ± 0.01 0.57 ± 0.01 0.79 ± 0.02 0.75 ± 0.02</cell></row><row><cell cols="6">Fig. 3. EdgeAL's and other AL methods' performances (mean dice score) compared to</cell></row><row><cell cols="6">baselines for Duke, AROI, and UNM datasets. Solid and dashed lines represent model</cell></row><row><cell cols="5">performance and 99% of it with 100% labeled data.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://pytorch.org.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partially funded by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> under grant number <rs type="grantNumber">16SV8639</rs> (<rs type="projectName">Ophthalmo-AI</rs>) and supported by the <rs type="funder">Lower Saxony Ministry of Science and Culture and the Endowed Chair of Applied Artificial Intelligence (AAI) of the University of Oldenburg</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_jmgN8v3">
					<idno type="grant-number">16SV8639</idno>
					<orgName type="project" subtype="full">Ophthalmo-AI</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 8.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discrepancy-based active learning for weakly supervised bleeding segmentation in wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-13" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SEEDS: superpixels extracted via energy-driven sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Capitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">7578</biblScope>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33786-4_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-33786-42" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kernel regression based segmentation of optical coherence tomography images with diabetic macular edema</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Allingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Mettu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1172" to="1194" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Suggestive annotation of brain tumour images with gradient-guided sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-116" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Y-net: a spatiospectral dualencoder network for medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeganeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehlbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="582" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-756" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Bayesian active learning with image data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cost-effective active learning for melanoma segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gorriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carlier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Faure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09168</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-class active learning for image classification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2372" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey of the recent architectures of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sohail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Zahoora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="5455" to="5516" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A robust and effective approach towards accurate metastasis detection and pN-stage classification in breast cancer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paeng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_93</idno>
		<idno>978-3-030-00934-2 93</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="841" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with self-selected active learning for cross-domain OCT image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92270-2_50</idno>
		<idno>978-3-030-92270-2 50</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ICONIP 2021</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Mantoro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ayu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Hidayanto</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13109</biblScope>
			<biblScope unit="page" from="585" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-attention segmentation networks combined with the sobel operator for medical images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">2546</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent structured active learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cereals-cost-effective regionbased active learning for semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mackowiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ghori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Diego</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09726</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AROI: annotated retinal oct images database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Melinščak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radmilovič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Vatavuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lončarić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 44th International Convention on Information, Communication and Electronic Technology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Active learning with multiple views</title>
		<author>
			<persName><forename type="first">I</forename><surname>Muslea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="203" to="233" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diminishing uncertainty within the training pool: active learning for medical image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2534" to="2547" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Warm start active learning with proxy labels and selection via semi-supervised fine-tuning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-129" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="297" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A visually explainable learning system for skin lesion detection using multiscale input with attention U-net</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ezema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nunnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sonntag</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58285-2_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58285-228" />
	</analytic>
	<monogr>
		<title level="m">KI 2020. LNCS (LNAI)</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Schmid</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Klügl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Wolter</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12325</biblScope>
			<biblScope unit="page" from="313" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully-automated segmentation of fluid regions in exudative age-related macular degeneration subjects: kernel graph cut in neutrosophic domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rashno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nazari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Koozekanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Drayna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">186949</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variational adversarial active learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Samrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sayna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Trevor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Active learning for convolutional neural networks: a core-set approach</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00489</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Viewal: active learning with viewpoint entropy for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9433" to="9443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">DRG-net: interactive joint learning of multi-lesion segmentation and classification for diabetic retinopathy grading</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Tusfiqur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T N</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2212.14615</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2212.14615" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cost-effective active learning for deep image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2591" to="2600" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Suggestive annotation: a deep active learning framework for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-7" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiscale unsupervised retinal edema area segmentation in oct images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ning</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-764" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularizing class-wise predictions via selfknowledge distillation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
