title,paper name,vol,notes ,Info outside article,name of dataset,normalized_datasets
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,14,1,"We use an in-house dataset of contrast-enhanced abdominal computed tomography images (CTs). 

The cohort consists of 141 patients with pancreatic ductal adenocarcinoma, of an equal ratio of male to female patients. Given a 3D arterial CT of the abdominal area, we automatically extract the vertebrae [15,18] and semi-automatically extract the ribs, which have similar intensities as arteries in arterial CTs and would otherwise occlude the vessels.

For more information about the dataset, see [6].

","For cross-validation, the imaging data were partitioned into four even folds yielding a size of 35 or 36 samples per subset. For each cross-validation, two subsets served as train set, one as validation set and the remaining one as test set, rotated such that each partition was used for testing and validation exactly once and twice for training. Age at diagnosis and sex were obtained for all patients using the hospital’s information system. The mean age of the patients analyzed in this study was  years and the cohort consisted in equal parts of male and female patients (71 male, 72 female).

The clinical and imaging data were collected according to the principles set in Declaration of Helsinki and Good Clinical Practice. Written conformed consent was waived and the study approved by the institutional ethics review board of the Technical University of Munich, Faculty of Medicine ",none,Name of Dataset not Provided
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,45,9,"A dataset of 64-videos (635 images) were recorded, and annotated for anatomical structures through multi-round expert consensus.

Images come from 64-videos of endoscopic pituitary surgery where the sellar phase is present [9], recorded between 30 Aug 2018 and 20 Feb 2021 from The National Hospital of Neurology and Neurosurgery, London, United Kingdom. All patients have provided informed consent, and the study was registered with the local governance committee

",Participant demographics collected included training grade and country of practice. The collected data regarding the surgical workflow were quantitative (whether participants agree it is complete and accurate) and qualitative (additional suggestions or comments). Summary statistics (e.g. frequencies) were generated for participants demographics.,none,Name of Dataset not Provided
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,46,8,"Following IRB approval for this study, we search for patients with metastatic breast cancer who had a breast cancer MRI performed between 2010 and 2020 and had morphologically positive BP on the MRI report from our electronic medical records (EMR) in * hospital. Totally, 189 patients including 141 normal patients and 41 abnormal ones are obtained. The range of the age are varying from 15 to 85 years old. The female patient number and male patient number are almost even. Their weights by kg are in the range of [40.8 kg, 145 kg]. All patient experiences three kinds of MRI sequences including T2, T1 and post-gadolinium.  Patients are required to maintain a decubitus position while scanning.",None,none,Name of Dataset not Provided
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,74,3,"We compared the performance of our proposed model against the stateof- the-art (SOTA) techniques in histopathology risk stratification in two cancer datasets. Our results suggest that the proposed model is capable of stratifying patients into statistically significant risk groups (p < 0.01 across the two datasets) with clinical utility while competing models fail to achieve a statistical significance endpoint (p = 0.148 − 0.494).

This paper aims to investigate the potential of extracting fine and coarse features from histopathology slides and integrating them for risk stratification in cancer patients. Therefore, the contributions of this work can be summarized as: 1) a novel graph-based model for predicting survival that extracts both local and global properties by identifying morphological super-nodes; 2) introducing a fine-coarse feature distillation module with 3 various strategies to aggregate interactions at different scales; 3) outperforming SOTA approaches in both risk prediction and patient stratification scenarios on two datasets; 4) publishing two large and rare prostate cancer datasets containing more than 220 graphs for active surveillance and 240 graphs for brachytherapy cases. The code and graph embeddings are publicly available at https://github.com/pazadimo/ALL-IN

We utilize two prostate cancer (PCa) datasets to evaluate the performance of our proposed model. The first set (PCa-AS) includes 179 PCa patients who were managed with Active Surveillance (AS). Radical therapy is considered overtreatment in these patients, so they are instead monitored with regular serum prostate-specific antigen (PSA) measurements, physical examinations, sequential biopsies, and magnetic resonance imaging [23]. However, AS may be over- or under-utilized in low- and intermediate-risk PCa due to the uncertainty of current methods to distinguish indolent from aggressive cancers [11]. Although majority of patients in our cohort are classified as low-risk based on NCCN guidelines [21], a significant subset of them experienced disease upgrade that triggered definitive therapy (range: 6.2 to 224 months after diagnosis).

The second dataset (PCa-BT) includes 105 PCa patients with low to high risk disease who went through brachytherapy. 

We also utilized the Prostate cANcer graDe Assessment (PANDA) Challenge dataset [7] that includes more than 10,000 PCa needle biopsy slides (no outcome data) as an external dataset for training the encoder of our model.",None,"pca-as, pca-bt, panda challenge dataset","PANDAS Dataset, PCa-BT Dataset, PCa-AS Dataset"
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,17,1,"*mention patient in conclusion

We evaluate our method on the Brain Tumor Segmentation challenge (BraTS) dataset [1,2,14], which contains 2,000 cases, each of which includes four 3D volumes from four different MRI modalities. The official data split divides these cases by the ratio of 8:1:1 for training, validation, and testing (5,802 positive and 1,073 negative images). In order to evaluate the performance, we use the validation set as our test set and report statistics on it. We preprocess the data by slicing each volume along the z-axis to form a total of 193,905 2D images, following the approach of Kang et al. [10] and Dey and Hong [6]. We use the ground-truth segmentation masks only in the final evaluation, not in the training process.

Experimental results on the four modalities of the 2021 BraTS dataset demonstrate the superiority of our approach compared with other CAM-based weakly-supervised segmentation methods. Specifically, AME-CAM achieves the highest dice score for all patients in all datasets and modalities. These results indicate the effectiveness of our proposed approach in accurately segmenting
brain tumors from MRI images using only class labels

","The BraTS dataset describes a collection of brain tumor MRI scans acquired from multiple different centers under standard clinical conditions, but with different equipment and imaging protocols, resulting in a vastly heterogeneous image quality reflecting diverse clinical practice across different institutions. However, we designed the following tumor annotation protocol, in order to make it possible to create similar ground truth delineations across various annotators.","brats dataset, 2021 brats","BraTS Dataset, BraTS Dataset 2021"
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,68,6,"Table 1:
ID Age Gender Race ECOG Smoking PY pStage Cancer Site Cancer Subsite
Case1 49 Male White 3 Current 21 1 Oral Cavity Ventral Tongue
Case2 64 Male White 3 Former 20 4 Larynx Vocal Cord
Case3 60 Male Black 2 Current 45 4 Larynx False Vocal Cord
Case4 53 Male White 1 Current 68 4 Larynx Supraglottic
Case5 38 Male White 0 Never 0 4 Oral Cavity Lateral Tongue
Case6 76 Female White 1 Former 30 2 Oral Cavity Lateral Tongue
Case7 73 Male White 1 Former 100 3 Larynx Glottis
Case8 56 Male White 0 Never 0 2 Oral Cavity Tongue

The complete staining protocols for this dataset are given in the accompanying supplementary material. Images were acquired at 20× magnification at Moffitt Cancer Center. The demographics and other relevant information for all eight head-and-neck squamous cell carcinoma patients is given in Table 1.

For the purpose of training and testing all the models, we extract four images of size 256 × 256 from each tile due to the size of the external IHC images, resulting in a total of 1072 images. We randomly extracted tiles from the LYON19 challenge dataset [14] to use as style IHC images. Using these images, we created a dataset of synthetically generated IHC images from the hematoxylin and its marker image as shown in Fig. 3.

We evaluated the effectiveness of our synthetically generated dataset (stylized IHC images and corresponding segmented/classified masks) using our generated dataset with the NuClick training dataset (containing manually segmented CD3/CD8 cells) [6]. We randomly selected 840 and 230 patches of size 256×256 from the created dataset for training and validation, respectively. NuClick training and validation sets [6] comprise 671 and 200 patches, respectively, of size 256×256 extracted from LYON19 dataset [14]. LYON19 IHC CD3/CD8 images are taken from breast, colon, and prostate cancer patients.",None,"lyon19 challenge dataset, nuclick training dataset","Lyon19 Challenge Dataset, NUCLICK Training Dataset"
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,56,6,"*info about the availability of the dataset is ambigous 

To the best of our knowledge, it is the first publicized thyroid cytopathology dataset of both image-wise and pixel-wise labels. We construct a clinical thyroid cytopathology dataset with images of both image-wise and pixel-wise labels as a benchmark (appear in GitHub upon acceptance) Some representative images are presented in Fig. 2, together with the profile of the dataset. The dataset comprises 4,965H&E stained image patches and labels of TBSRTC, where a subset of 1,473 images was densely annotated for nuclei boundaries by three experienced cytopathologists and reached a total number of 31,064 elaborately annotated nuclei. Patient-level images were partitioned first for training and test images, and patch-level curation was performed. We divided the dataset with image-wise labels into 80% training samples and the remaining 20% testing samples. Our collection of thyroid cytopathology images was granted with an Ethics Approval document.",None,none,Name of Dataset not Provided
Anatomy-Driven Pathology Detection on Chest X-rays,6,1,"* they reference to the datasets, however do not articulate whether they are public in the section of Dataset

** ambiguity: if the chest ImaGenome data is derived from MIMIC-CXR, does it mean it's a subset of that dataset. If so, what differs from the two? 

Training Dataset.
 We train on the Chest ImaGenome dataset [4,21,22] consisting of roughly 240 000 frontal chest X-ray images with corresponding scenegraphs automatically constructed from free-text radiology reports. It is derived from the MIMIC-CXR dataset [9,10], which is based on imaging studies from 65 079 patients performed at Beth Israel Deaconess Medical Center in Boston, US.

Evaluation Dataset and Class Mapping. 
We evaluate our method on the subset of 882 chest X-ray images with pathology bounding boxes, annotated by radiologists, from the NIH ChestXray-8 (CXR8) dataset [20]3 from the National Insti tutes of Health Clinical Center in the US.",None,"chest imagenome dataset, mimic-cxr dataset, nih chestxray-8 (cxr8) dataset","MIMIC-CXR Dataset, NIH ChestXray-8 Dataset"
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,50,7,"774 consecutive bi-parametric prostate MRI examinations are included in this study, which were acquired in-house during the clinical routine. The ethics committee of the Medical Faculty Heidelberg approved the study (S-164/2019) and waived informed consent to enable analysis of a consecutive cohort. All experiments were performed in accordance with the declaration of Helsinki [2] and relevant data privacy regulations. For every exam, PI-RADS v2 [24 interpretation was performed by a board-certified radiologist. Every patient underwent extended systematic and targeted MRI trans-rectal ultrasound-fusion transperineal biopsy. Malignancy of the segmented lesions was determined from a systematic-enhanced lesion ground-truth histopathological assessment, which has demonstrated reliable ground-truth assessment with sensitivity comparable to radical prostatectomy [17]. The samples were evaluated according to the International Society of Urological Pathology (ISUP) standards under the supervision of a dedicated uropathologist. Clinically significant prostate cancer (csPCa) was defined as ISUP grade 2 or higher. Based on the biopsy results, every csPCa lesion was segmented on the T2-weighted sequences retrospectively by multiple in-house investigators under the supervision of a board-certified radiologist. In addition to the lesions, the rectum and the bladder segmentations were automatically predicted by a model built upon nnU-Net [8] trained iteratively on an in-house cohort initially containing a small portion of our cohort. Multiple radiologists confirmed the quality of the predicted segmentations.

",None,none,Name of Dataset not Provided
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,21,3,"Note: The Japanese Society of Radiological Technology (JSRT) - a location?
**Ambigous: they say they use three datasets, however interpreting this line from the paper might seem like there is 4 datasets in total?  
Data comes from patients diagnosed with coronary artery disease, COVID, or healthy volunteers. The dataset is split into a training/validation set (80/20) and an independent test set from different sites, comprised of 994 echocardiograms from 684 patients and 368 echocardiograms from 206 patients, respectively.

In addition to being directly interpretable, our uncertainty estimation method outperforms previous methods on three datasets using two different image modalities. Code is available at: https://github.com/ThierryJudge/contouring-uncertainty.

The CAMUS dataset [20] contains cardiac ultrasounds from 500 patients.Manual annotations for the endocardium and epicardium borders of the left ventricle (LV) and the left atrium were obtained from a cardiologist for the end-diastolic (ED) and end-systolic (ES) frames. The dataset is split into 400 training patients, 50 validation patients, and 50 testing patients. 

Private Cardiac US. This is a proprietary multi-site multi-vendor dataset containing 2D echocardiograms of apical two and four chambers from 890 patients. Data comes from patients diagnosed with coronary artery disease, COVID, or healthy volunteers. The dataset is split into a training/validation set (80/20) and an independent test set from different sites, comprised of 994 echocardiograms from 684 patients and 368 echocardiograms from 206 patients, respectively.

JSRT. The Japanese Society of Radiological Technology (JSRT) dataset consists of 247 chest X-Rays [26]. We used the 120 points for the lungs and heart annotation made available by [10]. The set of points contains specific anatomical points for each structure (4 for the right lung, 5 for the left lung, and 4 for the heart) and equally spaced points between each anatomical point. We reconstructed the segmentation map with 3 classes (background, lungs, heart) with these points and used the same train-val-test split of 70%–10%–20% as [10].

With the exception of the Private Cardiac US dataset, the skewed normal distribution model shows very similar or improved results for both correlation and mutual information compared to the univariate and bivariate models. It can be noted, however, that in specific instances, the asymmetric model performs better on Private Cardiac US dataset (c.f. column 2 and 3 in Fig. 3). This confirms that it is better capturing asymmetric errors over the region of every contour point.",None,"camus dataset, jsrt dataset, the japanese society of radiological technology dataset","CAMUS Dataset, JSRT Dataset"
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,23,8,"The method is evaluated on 21 openly available healthy subjects from the Human Connectome Project and an internal dataset of ten
neurosurgical cases.

The proposed technique was tested on a healthy-subject dataset and on a dataset containing tumor cases. The first comprises 21 subjects of the human connectome project (HCP) that were used for testing the automated methods TractSeg and Classifyber [3,18].

To test the proposed method on pathological data, we used an in-house dataset containing ten presurgical scans of patients with brain tumors. ",None,"human connectome project, hcp",Human Connectome Project
Automated CT Lung Cancer Screening Workflow Using 3D Camera,40,7,"Note: difficult to determine the quantity of datasets used

CT scan dataset, 3D camera dataset and CT scan, CT scan from seperate site in Europe

Our CT scan dataset consists of 62, 420 patients from 16 different sites across North America, Asia and Europe. Our 3D Camera dataset consists of 2, 742 pairs of depth image and CT scan from 2, 742 patients from 6 different sites across North America and Europe acquired using a ceiling-mounted Kinect 2 camera. Our evaluation set consists of 110 pairs of depth image and CT scan from 110 patients from a separate site in Europe.

We trained our AutoDecoder model on our unpaired CT scan dataset of 62, 420 patients with a latent vector of size 32. The encoder was trained on our paired CT scan and depth image dataset of 2, 742 patients.",None,none,Name of Dataset not Provided
Automatic Bleeding Risk Rating System of Gastric Varices,1,5,"We also collect a GV bleeding risks rating dataset (GVbleed) with 1678 gastroscopy images from 411 patients that are jointly annotated in three levels of risks by senior clinical endoscopists. The experiments on our collected dataset show that our method can improve the rating accuracy by nearly 5% compared to the baseline. Codes and dataset will be available at https://github.com/LuyueShi/gastric-varices.

The GVBleed dataset contains 1678 endoscopicimages with gastric varices from 527 cases. All of these cases are collected from 411 patients in a Grade-III Class-A hospital during the period from 2017 to 2022. In the current version, images from patients with ages elder than 18 are retained[1]. The images are selected from the raw endoscopic videos and frames. To maximize the variations, non-consecutive frames with larger angle differences are selected. To ensure the quality of our dataset, senior endoscopists are invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and NBI pictures.

The GVBleed dataset is partitioned into training and testing sets for evaluation, where the training set contains 1337 images and the testing set has 341 images. The detailed statistics of the three levels of GV bleeding risk in each set are shown in Table 1. The dataset is planned to be released in the future.

"," Dataset details such as gender and age in supplementary material (outside of paper, however references to it in paper for more dataset details)",gvbleed dataset,GVBLEED Dataset
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,47,4,"Extensive experimental results on two medical image segmentation datasets.

The data used in this experiment are obtained from LIDC-IDRI [2,7] and BRATS 2021 [4] datasets. LIDC-IDRI contains 1,018 lung CT scans with plausible segmentation masks annotated by four radiologists.

BRATS 2021 consists of four different sequence (T1, T2, FlAIR, T1CE) MRI images for each patient.

Our training set includes 55,174 2D images scanned from 1,126 patients, and the test set comprises 3,991 2D images scanned from 125 patients",None,"lidc-idri dataset, brats 2021 dataset","LIDC-IDRI Dataset, BraTS Dataset 2021"
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,71,1,"*datasets are from authors: naylor et al., hou et al., irshad et al., awan et al.

The code and dataset are released at: https://github.com/NeuronXJTU/MBDA-CellSeg.

We collect four pathology image datasets to validate our proposed approach. Firstly, we acquire 50 images from a cohort of patients with
Triple Negative Breast Cancer (TNBC), which is released by Naylor et al [18]. Hou et al. [10] publish a dataset of nucleus segmentation containing 5,060 segmented slides from 10 TCGA cancer types. In this work, we use 98 images from invasive carcinoma of the breast (BRCA). We have also included 463 images of Kidney Renal Clear cell carcinoma (KIRC) in our dataset, which are made publicly available by Irshad et al [11]. Awan et al. [2] publicly release a dataset containing tissue slide images and associated clinical data on colorectal cancer (CRC), from which we randomly select 200 patches for our study. In our experiments, we transfer knowledge from three black-box models trained on different source domains to a new target domain model (e.g.,from CRC, TNBC, KIRC to BRCA). The backbone network for the student model and source domain black-box predictors employ the widely adopted residual U-Net [12], which is commonly used for medical image segmentation. For each source domain network, we conduct full-supervision training on the corresponding source domain data and directly evaluate its performance on target domain data. The upper performance metrics (Source-only upper) are shown in the Table 1. To ensure the reliability of the results, we use the same data for training, validation, and testing, which account for 80%, 10%, and 10% of the original data respectively. For the target domain network, we use unsupervised and semi-supervised as our task settings respectively. In semi-supervised domain adaptation, we only use 10% of the target domain data as labeled data.",None,"tnbn dataset, naylor et al [18], tcga dataset,  hou et al. [10],  kirc dataset, irshad et al [11], crc dataset, awan et al. [2] ","KIRC Dataset, CRC Dataset, TNBN Dataset, TCGA Dataset"
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,11,8,"The pretext model is trained on 9,544 MRI scans from the public Alzheimer’s Disease Neuroimaging Initiative (ADNI) [12] without any category label information, yielding a generalizable encoder. The downstream model is further fine-tuned on target MRIs for CI progression prediction. Experiments are performed on two CI-related studies with 391 subjects, with results suggesting the efficacy of BAR compared with state-ofthe- art (SOTA) methods.

Data and Preprocessing. The pretext model is trained via a tissue segmentation task on auxiliary MRIs (without category label) from ADNI. A total of 9,544 T1-weighted MRIs from 2,370 ADNI subjects with multiple scans are used in this work. To provide accurate brain anatomy, we perform image preprocessing and brain tissue segmentation for these MRIs to generate ground-truth segmentation of three tissues, i.e., white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF), using an in-house toolbox iBEAT [16] with manual verification. The downstream model is trained on 1) a late-life depression (LLD) study with 309 subjects from two sites [17,18], and 2) a type 2 diabetes mellitus (DM)
study with 82 subjects from the First Affiliated Hospital of Guangzhou University of Chinese Medicine. Subjects in LLD are categorized into three groups: 1) 89 non-depressed cognitively normal (CN), 2) 179 depressed but cognitively normal (CND), 3) 41 depressed subjects (called CI) who developed cognitive impairment or even dementia in the follow-up years. Category labels in the LLD study are determined based on subjects’ 5-year follow-up diagnostic information, while MRIs are acquired at baseline time. The DM contains 1) 45 health control (HC) subjects and 2) 37 diabetes mellitus patients with mild CI (MCI). Detailed image acquisition protocols are given in Table SI of Supplementary Materials.
All MRIs are preprocessed via the following pipeline: 1) bias field correction, 2) skull stripping, 3) affine registration to the MNI space, 4) resampling to 1×1×1 mm3, 5) deformable registration to AAL3 [19] with SyN [20], and 6) warping 166 regions-of-interest (ROIs) of AAL3 back to MRI volumes.",None,"lld study, dm study, alzheimer’s disease neuroimaging initiative (adni) dataset, adni dataset","LLD Dataset, ADNI Dataset, DM Dataset"
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,33,4,"*table showing location (source) of each dataset

The proposed approach was compared to nine BUS classification methods and evaluated using seven quantitative metrics on a dataset of 3,320 BUS images.

We evaluated the performance of Hybrid-MT-ESTAN using four public datasets, HMSS [9], BUSI [10], BUSIS [20], and Dataset B [6]. We combined all four datasets to build a large and diverse dataset with a total of 3,320 B-mode BUS
images, of which 1,664 contain benign tumors and 1,656 have malignant tumors. Table 1 shows the detailed information for each dataset. HMSS dataset does not provide the segmentation ground-truth masks, and for this study we arranged with a group of experienced radiologists to prepare the masks for HMSS. Refer to the original publications of the datasets for more details.

BUS dataset No. of images Distribution Source
HMSS 1,948 b:812, m:1136 Netherlands
BUSI 647 b:437, m:210 Egypt
BUSIS 562 b:306, m:256 China
Dataset B 163 b:109, m:54 Spain
Total 3,320 b: 1,664, m: 1,656

All BUS images in the dataset were zero-padded and reshaped to form square images. To avoid data leakage and bias, we selected the train, test, and validation sets based on the cases, i.e., the images from one case (patient) were assigned to only one of the training, validation, and test sets",None,"hmss dataset, busi dataset, busis dataset, dataset b","BUSIS Dataset, Dataset B, BUSI Dataset, HMSS Dataset"
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,53,7,"Ex-vivo: Data is collected from fresh breast tissue samples from the patients referred to BCS at Kingston Health Sciences Center over two years. The study is approved by the institutional research ethics board and patients consent to be included. Peri-operatively, a pathologist guides and annotates the ex-vivo pointburns, referred to as spectra, from normal or cancerous breast tissue immediately after excision. In addition to spectral data, clinicopathological details such as the status of hormone receptors is also provided post-surgically. In total 51 cancer and 149 normal spectra are collected and stratified into five folds (4 for cross validation and 1 prospectively) with each patient restricted to one fold only.

Intra-operative: A stream of iKnife data is collected during a BCS case (27 min) at Kingston Health Sciences Center. At the sampling rate of 1Hz, a total of 1616 spectra are recorded. Each spectrum is then labeled based both on surgeons comments during the operation and post-operative pathology report.

",None,none,Name of Dataset not Provided
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,35,5,"Note: dataset collected from two different online resources

Our dataset NSCLC-TCIA for lung cancer histological subtype classification is sourced from two online resources of The Cancer Imaging Archive (TCIA) [5]: NSCLC Radiomics [1] and NSCLC Radiogenomics [2]. Exclusion criteria involves patients diagnosed with large cell carcinoma or not otherwise specified, along with cases that have contouring inaccuracies or lacked tumor delineation [9, 13]. Finally, a total of 325 available cases (146 ADC cases and 179 SCC cases) are used for our study. We evaluate the performance of NSCLC classification in five-fold cross validation on the NSCLC-TCIA dataset, and measure accuracy (Acc), sensitivity (Sen), specificity (Spe), and the area under the receiver operating characteristic (ROC) curve (AUC) as evaluationmetrics.We also conduct analysis including standard deviations and 95% CI, and DeLong statistical test for further AUC comparison.

For preprocessing, given that the CT data from NSCLC-TCIA has an in-plane resolutionof 1mm×1mmand a slice thickness of 0.7–3.0 mm, we resample the CT images using trilinear interpolation to a common resolution of 1mm × 1mm × 1mm.",None,"nsclc-tcia dataset, the cancer imaging archive (tcia), nsclc radiomics and nsclc radiogenomics","NSCLC-TCIA Dataset, TCIA Dataset"
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,1,4,"To efficiently enrich the unlabeled pool, seeking support from other centers is a viable solution, as illustrated in Fig. 1. Yet, due to differences in imaging protocols and variations in patient demographics, this solution usually introduces data heterogeneity, leading to a quality problem. Such heterogeneity may impede the performance of SSL which typically assumes that the distributions of labeled data and unlabeled data are independent and identically distributed (i.i.d.) [16]. Thus, proper mechanisms are called for this practical but challenging SSL scenario.

We utilize prostate T2-weighted MR images from six different clinical centers (C1–6) [1,4,5] to perform a retrospective evaluation. 
The heterogeneity comes from the differences in scanners, field strengths, coil types, disease and in-plane/through-plane resolution. Compared to C1 and C2, scans from C3 to C6 are taken from patients with prostate cancer, either for detection or staging purposes, which can cause inherent semantic differences in the prostate region to further aggravate heterogeneity.",None,none,Name of Dataset not Provided
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,20,2,"*the article say there are two publicly available colorectal cancer datasets, however the referred article (that contain the details of dataset does not show they are public. However, the referred article talks about 2 other publicly prostate datasets)

We evaluate the proposed network using colorectal cancer datasets that were collected under different environments.

Two publicly available colorectal cancer datasets [9] were employed to evaluate the effectiveness of the proposed CaFeNet. Table 1 shows the details of the datasets. Both datasets provide colorectal pathology images with ground truth labels for cancer grading. The ground labels are benign (BN), well-differentiated (WD) cancer, moderately differentiated (MD) cancer, and poorly-differentiated (PD) cancer. The first dataset includes 1600 BN, 2322 WD, 4105 MD, and 1830 PD image patches that were collected between 2006 and 2008. This dataset is divided into a training dataset (CTrain), validation dataset (CValidation), and a test dataset (CTestI). The second dataset, designated as CTestII. These were acquired between 2016.

However, the experiments were only conducted on two public colorectal cancer datasets from a single institute
","the dataset (that is referred to in the paper) labels subjects as patients, location and location information from sup. material., data collected from kangbuk samsung hospital",none,Name of Dataset not Provided
Certification of Deep Learning Models for Medical Image Segmentation,58,4,"*demographic information such as age, gender and location details  were outside paper's content, and study subjects labelled as patients. 

We conduct extensive experiments on five public datasets of chest X-rays, skin lesions, and colonoscopies, and empirically show that we are able to maintain high certified Dice scores even for highly perturbed images.

Datasets: We perform experiments on 5 different publicly available datasets. All datasets were divided to 70% for training, 10% for validation, and 20% for testing. The testing set is the one used to compute certified results.

Chest X-rays Datasets: JSRT dataset [31] with annotations of lung, heart, and clavicles provided by [35] is used. This dataset contains
247 images. For lung segmentation only, we use both the Montgomery and Shenzen datasets [21]. Montgomery consists of 138 and Shenzen of 662 annotated images.

Skin Lesion: Skin images with their annotations provided by the ISIC 2018 boundary segmentation challenge [10] were used. This dataset consists of 2694 RGB dermatoscopy images.

Colonoscopy Images: CVC-ClinicDB dataset [6] containing 612 colonoscopy images in RGB together with their annotations were utilized.

","{chest x-rays datasets: x-rays,  ISIC 2018: rgb dermatocopy,  cvc-clinicDB dataset: colonoscopy}, {chest x-rays datasets: chest, lung, heart, ISIC 2018: skin, cvc-clinicDB dataset: colon}, {chest x-rays datasets: Shenzhen No.3 People’s Hospital, ISIC 2018: unknown, cvc-clinicDB dataset:  Hospital Clinic of Barcelona} 

Chest X-rays Datasets: JSRT dataset
Original posteroanterior chest films for this database were collected from 13 medical centers in Japan and one institution in the United States under the following conditions: only one nodule on an image for nodule cases; confirmation of presence or absence of a lung nodule by CT examination; and nodule classification as malignant based on histologic and cytologic examination or as benign based on histology, definitive isolation of a pathogenic organism, shrinkage and disappearance with the use of antibiotics, or no change observed during a follow-up period of 2 year.
Of the patients with nodules, 68 were men and 86 were women, whereas patients without nodules included 51 men and 42 women. The average age of patients with nodules was 60 years old. Two patients were 21-30 years old, seven were 31-40 years old, 24 were 41-50 years old, 37 were 51-60 years old, 53 were 61-70 years old, and 29 were 71 years old or older. Two patients' ages were unknown.

MC dataset
The MC set has been collected in collaboration with the Department of Health and Human Services, Montgomery County, Maryland, USA. The set contains 138 frontal chest X-rays from Montgomery County’s Tuberculosis screening program, of which 80 are normal cases and 58 are cases with manifestations of TB.
Each reading contains the patient’s age, gender, and abnormality seen in the lung, if any.

The Shenzhen dataset 
was collected in collaboration with Shenzhen No.3 People’s Hospital, Guangdong Medical College, Shenzhen, China. The chest X-rays are from outpatient clinics and were captured as part of the daily hospital routine within a 1-month period.
Each reading contains the patient’s age, gender, and abnormality seen in the lung, if any.

ISIC 2018:
Unknown

CVC-ClinicDB database 
We introduce in this paper the CVC-ClinicDB database built in collaboration with Hospital Clinic of Barcelona, Spain.","jsrt dataset, montgomery dataset, shenzen dataset, isic 2018 dataset, cvc-clinicdb dataset","Montgomery Dataset, ISIC 2018 Dataset, JSRT Dataset, Shenzen Dataset, CVC-ClinicDB Dataset"
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,38,7,"Dataset. LIDC-IDRI [1] is a dataset for pulmonary nodule classification or detection based on low-dose CT, which involves 1,010 patients. According to the annotations, we extracted 2, 026 nodules, and all of them were labeled with scores from 1 to 5, indicating the malignancy progression.  
In this paper,we construct three sub-datasets: LIDC-A contains three classes of nodules both in training and test sets; according to [11], we construct the LIDC-B,whichcontains three classes ofnodules only in the training set, andthe test set contains benign and malignant nodules;LIDC-C includes benign and malignant nodules both in training and test sets.",None,lidc-idri dataset,LIDC-IDRI Dataset
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,15,5,"Dataset and Ground Truth. Our study analyzed a dataset of CT scans collected from Guangdong Province People’s Hospital between years 2018 and 2020, with 2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. We used the latest patients in the second half of 2020 as a hold-out test set, resulting in a training set of 687 gastric cancer and 1,204 normal cases, and a test set of 100 gastric cancer and 148 normal cases. We randomly selected 20% of the training data as an internal validation set. To further evaluate specificity in a larger population, we collected an external test set of 903 normal cases from Shengjing Hospital. Cancer cases were confirmed through endoscopy (and pathology) reports, while normal cases were confirmed by radiology reports and a two-year follow-up. All patients underwent multi-phase CTs ",None,none,Name of Dataset not Provided
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,51,10,"Experimental results on multiple datasets, including Magnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of our proposed method.
For our study, we selected the ICBM 152 Nonlinear Symmetric template as our atlas [9]. We reoriented all MRI scans of the T1 sequence to the RAS orientation with a resolution of 1mm×1mm×1mm and align the images to atlas using the mri robust register tool in FreeSurfer [19]. We then cropped the resulting MRI scans to a size of 160×192×144, without any image augmentation. To evaluate our approach, we employed a 5-fold cross-validation method and divided our data into training and test sets in an 8:2 ratio.

3D Brain MRI. 
OASIS-1 [12] includes 416 cross-sectional MRI scans from individuals aged 18 to 96, with 100 of them diagnosed with mild to moderate Alzheimer’s disease. BraTS2020 [13] provides 369 expert-labeled pre-operative MRI scans of glioblastomas and low-grade gliomas, acquired from multiple institutions for routine clinical use.

3D Pseudo Brain MRI. 
To evaluate the performance of atlas-based registration, it is essential to have the correct mapping of pathological regions to healthy brain regions. To create such a mapping, we created a pseudo dataset by utilizing images from the OASIS-1 and BraTS2020. From the resulting t1 sequences, a pseudo dataset of 300 images was randomly selected for further analysis. Appendix B provides a detailed process for creating the pseudo dataset.

Real Data with Landmarks. 
BraTS-Reg 2022 [2] provides extensive annotations of landmarks points within both the pre-operative and the follow-up scans that have been generated by clinical experts. A total of 140 images are provided, of which 112 are for training, and 28 for testing.",None,"oasis-1 dataset, brats2020 dataset, brats-reg 2022",OASIS-1 Dataset
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,62,6,"The model achieved an average testing AUROC of 86.53% on a large curated dataset with over 1.1 million stroma patches.

Our experimental results indicate that stromal alterations are detectable in the presence of prostate cancer and highlight the potential for tumor-associated stroma to serve as a diagnostic biomarker in prostate cancer.

In our study, we utilized three datasets for tumor-associated stroma analysis. (1) Dataset A comprises 513 tiles extracted from the whole mount slides of 40 patients, sourced from the archives of the Pathology Department at Cedars-Sinai Medical Center (IRB# Pro00029960). It combines two sets of tiles: 224 images from 20 patients featuring stroma, normal glands, low-grade and highgrade cancer [22], along with 289 images from 20 patients with dense high-grade cancer (Gleason grades 4 and 5) and cribriform/non-cribriform glands [23]. Each tile measures 1200×1200 pixels and is extracted from whole slide images captured at 20x magnification (0.5 microns per pixel). The tiles were annotated at the pixel-level by expert pathologists to generate stroma tissue segmentation masks and were cross-evaluated and normalized to account for stain variability. (2) Dataset B included 97 whole mount slides with an average size of over 174,000×142,000 pixels at 40x magnification. The prostate tissue within these slides had an average tumor area proportion of 9%, with an average tumor area of 77 square mm. An expert pathologist annotated the tumor region boundaries at the region-level, providing exhaustive annotations for all tumor foci. (3) Dataset C comprised 6134 negative biopsy slides obtained from 262 patients’ biopsy procedures, where all samples were diagnosed as negative. These slides are presumed to contain predominantly normal stroma tissues without phenotypic alterations in response to cancer.",None,none,Name of Dataset not Provided
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,64,8,"We analyze the behavior of CDVaDE on multiple datasets and compare it to other deep clustering algorithms.

The Colored MNIST is an extension to the classic MNIST dataset [3], which contains binary images of handwritten digits. The Colored MNIST includes colored images of the same digits, where each number and background have a color assignment. We present results of the experiments with five distinct colors and five digits of MNIST (0–4). To enhance computational efficiency and expedite experiments, we utilized only 1% of the MNIST images, which were sampled at random.

HER2 Dataset. 
Human epidermal growth factor receptor 2 (HER2 or HER2/neu) is a protein involved in normal cell growth, which plays an important role in the diagnosis and treatment of breast cancer [8]. The dataset consists of 241 patches extracted from 64 digitized slides of breast cancer tissue which were stained with HER2 antibody. Each tissue slide has been digitized at three different sites using three different whole slide imaging systems, evaluated by 7 pathologists on a 0–100 scale, and following clinical practice labeled as HER2 Class 1, 2, or 3 (based on mean pathologists’ scores with cut-points at 33 and 66). 
We use a subset of this dataset consisting of 672 images (the remainder is held out for future research). Because the intended purpose is finding subgroupsin the given dataset only, a separate test set is not used.  We refer to [4,8] for more details about this dataset.
This retrospective human subject dataset has been made available to us by the authors of the prior studies [4,8], who are not associated with this paper. Appropriate ethical approval for the use of this material in research has been obtained.

We evaluate the performance and behavior of the DEC, VaDE, and CDVaDE models on the HER2 dataset. We investigate whether the models will learn to distinguish the HER2 class labels, the scanner labels, or other potentially meaningful data subgroups in a fully unsupervised fashion.

"," Acknowledgement: The authors would like to thank Dr. Marios Gavrielides for providing access to the HER2 dataset and for helpful discussion. This project was supported n part by an appointment to the Research Participation Program at the U.S. Food and Drug Administration administered by the Oak Ridge Institute for Science and Education through an interagency agreement between the U.S. Department of Energy and the U.S. Food and Drug Administration. XS acknowledges support from the Hightech Agenda Bayern.

MNIST dataset (referenced link was not accessable)","colered mnist dataset, her2 dataset","HER2 Dataset, Colored MNIST Dataset"
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,16,5,"*location details dervied outside paper

Grand Challenge Camelyon data [16] is used for model training (770 WSIs of which 293 WSIs contain metastases) and in-domain testing (629 WSIs of which
289 WSIs contain metastases). For domain shift test data, we extracted 302 WSIs (of which 111 contain metastases) from AIDA BRLN dataset [12]. To evaluate clinically realistic domain shifts, we divided the dataset in two different ways, creating four subsets:

1a. 161 WSIs from sentinel node biopsy cases (54 WSIs with metastases): a small shift as it is the same type of lymph nodes as in Grand Challenge Camelyon data [16].
1b. 141 WSIs from axillary nodes dissection cases (57 WSIs with metastases): potentially large shift as some patients have already started neoadjuvant treatment as well as the tissue may be affected from the procedure of sentinel lymph node removal.
2a. 207 WSIs with ductal carcinoma (83 WSIs with metastases): a small shift as it is the most common type of carcinoma and relatively easy to diagnose.
2b. 68 WSIs with lobular carcinoma (28 WSIs with metastases): potentially large shift as it is a rare type of carcinoma and relatively difficult to diagnose.

Moreover, discussions with pathologists led to
the conclusion that it is clinically relevant to evaluate the performance difference between ductal and lobular carcinoma. Our method is intended to avoid requiring dedicated WSI labelling efforts. We deem that the information needed to do this type of subset divisions would be available without labelling since the patient cases in a clinical setting would already contain such information. All datasets are publicly available to be used in legal and ethical medical diagnostics research.

","The data set for CAMELYON17 is collected from 5 medical centres in the Netherlands. 
{CAMELYON17:netherlands},
AIDA BRLN dataset:
{AIDA BRLN dataset: department of clinical pathology} ,  {AIDA BRLN dataset: Linköping Region Östergötland}
The cases are anonymised and exported from the digital archive at the Department of Clinical Pathology in Linköping, Region Östergötland.","grand challenge camelyon dataset, aida brln dataset","AIDA Dataset, Camelyon Dataset"
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,25,9,"To further validate the proposed solution, we acquired and publicly released two datasets captured using a custom-designed, portable stereo laparoscope system.

our first newly acquired dataset, named Jerry, contains 1200 sets of images. Since it is important to report errors in 3D and in millimeters, we recorded another dataset similar to Jerry but also including ground truth depth map for all frames by using structured-lighting system [8]—namely the Coffbee dataset.

[8] https://arxiv.org/pdf/2208.08407.pdf
Provides info about 2 datasets: SCARED and LATTE, not Coffbee.",None,"jerry dataset, coffbee dataset","COFFBEE Dataset, Jerry Dataset"
Detection of Basal Cell Carcinoma in Whole Slide Images,26,6,"*they say they use on skin cancer dataset, however tested their model on the ChestMNIST and DermaMINIST subsets of MedMNISTv2

we analyze skin cancer images using the optimal network obtained by neural architecture search (NAS) on the skin cancer dataset.

The dataset, comprised of 194 skin slides acquired from the Southern Sun Pathology laboratory, includes 148 BCC cases and 46 other types (common nevus, SCC), all manually annotated by a dermatopathologist. BCC slides served as positive samples and the rest as negatives.

We tested our model’s generalization on the ChestMNIST and DermaMNIST subsets of MedMNISTv2 [25], following established protocols. As shown in Table 4, our s ResNet50 surpassed the original ResNet50 on all datasets, gaining 2.3% and 1.8% more AUC on ChestMNIST and DermaMNIST respectively, proving the model’s robust generalization.

Performance comparison on ChestMNIST and DermaMNIST datasets.",None,"chestmnist dataset, dermamnist dataset, medmnistv2 dataset","DERMAMNIST Dataset, MEDMNISTv2 Dataset, ChestMNIST Dataset"
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,46,3,"*location details of data collection outside the paper's content.

qubiq dataset: prostate, brain, kidney
we use QUBIQ 2020, which contains 7 segmentation tasks in 4 differen CT and MR datasets: Prostate (55 cases, 2 tasks, 6 raters), Brain Growth (39 cases, 1 task, 7 raters), Brain Tumor (32 cases, 3 tasks, 3 raters), and Kidney 24 cases, 1 task, 3 raters).

lits dataset: iver
LiTS contains 201 high-quality CT scans of liver tumors. Out of these, 131 cases are designated for training and 70 for testing. As the ground-truth labels for the test set are not publicly accessible, we only use the training set.

kits dataset: 
KiTS includes 210 annotated CT scans of kidney tumors from different patients

Our extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks validate that incorporating soft labels leads to higher Dice score and lower calibration error, indicating that these losses can find wide application in diverse medical image segmentation problems.

","{lits: Rechts der Isar Hospital), {lits: the Technical University of Munich, Polytechnique Montreal and CHUM Research Center,  Sheba Medical Center, the Hebrew University of Jerusalem, Hadassah University Medical Center,  IRCAD}, {lits: Germany, Netherlands, Canada, Israel, France}, 
The image data for the LiTS challenge are collected from seven clinical sites all over the world, including a) Rechts der Isar Hospital, the Technical University of Munich in Germany, b) Radboud University Medical Center, the Netherlands, c) Polytechnique Montr´eal and CHUM Research Center in Canada, d) Sheba Medical Center in Israel, e) the Hebrew University of Jerusalem in Israel, f) Hadassah University Medical Center in Israel, and g) IRCAD in France. The distribution of the number of scans per institution is described in Table 2. The LiTS benchmark dataset contains 201 computed tomography images of the abdomen, of which 194 CT scans contain lesions. All data are anonymized, and the images have been reviewed visually to preclude the presence of personal identifiers

kits dataset: public,  kidney (abdominal cts)
KiTS includes 210 annotated CT scans of kidney tumors from different patients","qubiq 2020 dataset, lits dataset, kits dataset","LiTS Dataset, QUBIQ 2020 Dataset, KiTS Dataset"
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,19,6,"Extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the superiority of our method.

The proposed DiffDPis extensively evaluated on a clinical dataset consisting of 130 rectum cancer patients, and the results demonstrate that our approach outperforms other state-of-the-art methods.

Dataset and Evaluations. We measure the performance of our model on an in-house rectum cancer dataset which contains 130 patients who underwent volumetric modulated arc therapy (VMAT) treatment at West China Hospital. Concretely, for every patient, the CT images, PTV segmentation, OARs segmentations, and the clinically planned dose distribution are included. Additionally, there are four OARs of rectum cancer containing the bladder, femoral head R, femoral head L, and small intestine.We randomly select 98 patients for model training, 10 patients for validation, and the remaining 22 patients for test. The thickness of the CTs is 3 mm and all the images are resized to the resolutionof 256 × 256 before the training procedure. We measure the performance of our proposed",None,none,Name of Dataset not Provided
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,10,4,"*age and race provided outside paper's content

We evaluate the proposed method for tumor segmentation on public breast cancer DCE-MRI dataset.

To demonstrate the effectiveness of our proposed DKM, we evaluate our method on 4D DCE-MRI breast cancer segmentation using the Breast-MRINACT-Pilot dataset [13], which contains a total of 64 patients with the contrastenhanced MRI protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time points (As shown in Fig. 3). Each MR volume consists of 60 slices and the size of each slice is 256×256. Regarding preprocessing, we conduct zeromean unit-variance intensity normalization for the whole volume. We divided the original dataset into training (70%) and test set (30%) based on the scans. Ground truth segmentations of the data are provided in the dataset for tumor annotation. No data augmentation techniques are used to ensure fairness.
","Breast-MRI-NACT-Pilot
Recurrence-free survival (RFS) was assessed for each patient at 6-month or 1-year intervals following surgery. Other clinical and endpoint data includes patient age, lesion characteristics including pretreatment tumor size, histologic type, pathologic size, tumor subtype, and lymph node involvement. These data are included as supplemental information for the collection in the accompanying clinical information workbook.

Clinical and DFS Metadata shows: race and age",breast-mrinact-pilot dataset,Breast-MRINACT-Pilot Dataset
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,72,10,"*gender/sex info, location details of data collection  found outside paper 

Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining. We make our training code and data publicly available.

dataset 1:
Our neural network is trained using patches from the “Gold Atlas- Male Pelvis - Gentle Radiotherapy” [14] dataset, which is comprised of 18 patients each with a CT, MR T1, and MR T2 volumes. 

Consequently, for the purpose of ensuring reproducibility, all evaluations presented in this paper exclusively pertain to the model trained solely on the public MR-CT dataset.

dataset 2:
Evaluation of Cerebral Tumors (RESECT) MICCAI challenge dataset [23]. This dataset consists of 22 pairs of pre operative brain MRs and intra-operative ultrasound volumes.

dataset 3: 
Our second application is the Abdomen MR-CT task of the Learn2Reg challenge 2021 [8]. The dataset comprises 8 sets of MR and CT volumes, both depicting the abdominal region of a single patient and exhibiting notable deformations

We are using a heterogeneous dataset of 27 cases, comprising liver cancer patients and healthy volunteers, different ultrasound machines, as well as optical

","Gold Atlas: public,  Magnetic resonance images (MRI) (T1w and T2w) with flat table top, and CT images of 19 male patients. The imaging was performed over the pelvic region. The patients were recruited and included from three different Swedish radiotherapy departments. Male patients with prostate or rectal cancer referred for curative radiotherapy were eligible for inclusion. Patients with locally advanced tumors (prostate cT3-4 and rectal cT4) were not included.

RESECT: public,  All medical images used for the challenge were acquired for routine clinical care at St Olavs University Hospital (Trondheim, Norway), after patients gave their informed consent.

Learn2Reg: public","gold atlas - male pelvis - gentle radiotherapy dataset, resect dataset, cerebral tumors (resect) miccai challenge dataset, learn2reg challenge dataset  ","RESECT Dataset, GOLD ATLAS Dataset"
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,74,5,"*The article contain the word 'tumor', but the main research area is brain strokes. 
We verified the effectiveness of our approach on the actual clinical scans acquired for clinical care and not just for research purposes, suggesting that the methods and findings in the current study should be generalizable to routine clinical practice conditions and potentially other types of clinical image-based diagnosis (e.g., brain tumor) as well.

Our dataset included MRI brain scans from 226 patients performed at an urban tertiary referral academic medical center that is a comprehensive stroke center. Clinical scans of adult patients aged 18–89 years with recent (acute or subacute) strokes were identified between 1/1/2013 and 1/1/2021 for inclusion in this study via a search of the Philips Performance Bridge. Scans meeting this criteria were downloaded and simultaneously anonymized to preserve patient anonymity and prevent disclosure of protected health information as part of this IRB exempt study. No patient demographic information was retained for the scans, as it was considered to represent an unnecessary risk for accidental release of protected health information. The diffusion weighted images with a gradient of B=1000 were utilized for the analysis (see the Supplement1 for information about the MRI scanner and parameter settings).

While the whole dataset includes 4,883 (74.7%) normal slices and 1,650 (25.3%) stroke slices, we further randomly split them into training/validation/test sets using the ratio 80%/10%/10%. For the training set, we implemented data augmentation strategies by rotating or flipping each slice. Finally, the training/validation/test set contains 31,356/653/654 slices, correspondingly.

We implemented DRL to both CNN and ViT models. For the CNN model, we used a ResNet-18 [9] architecture, while for the ViT model, we first pre-trained a 4-layer ViT using a self-supervised pre-training method called Masked Autoencoder (MAE) [8], using the T1/T2-weighted brain MR images in the IXI dataset [1].

As our dataset is unbalanced, we also considered the Area Under Precision-Recall Curve (AUPRC).We ran the experiments 3 times using different random seeds.

","{ixi dataset: hammersmith hospital, guy's hospital, institute of psychiatry}
{ixi dataset:  institute of psychiatry}
{ixi data: london}",none,Name of Dataset not Provided
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,15,10,"*dataset quantity not clear

Forty 128×128×40 3D Zubal brain phantoms [24] were used in the simulation study as ground truth, and one clinical patient brain images with different
dose level were used for the robust analysis. Two tumors with different size were added in each Zubal brain phantom. Among them, 1320 (33 samples) were used in training, 200 (5 samples) for testing, and 80 (2 samples) for validation. A total of 5 realizations were simulated and each was trained/tested independently for bias and variance calculation[15]. We used peak signal to noise ratio (PSNR), structural similarityindex (SSIM) and root mean square error (RMSE) for overall quantitative analysis.The contrast recovery coefficient (CRC) [25] was used for the comparison of reconstruction results in the tumor region of interest (ROI) area.",None,none,Name of Dataset not Provided
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,62,4,"An axial dataset includes 250 distinct subjects, each underwent initial standard clinical liver MRI protocol examinations with corresponding pre-contrast images (T2FS [4mm]) and DWI [4mm]) was collected. The ground truth was reviewed by two abdominal radiologists with 10 and 22 years of experience in liver imaging, respectively. If any interpretations demonstrated discrepancies between the reviewers, they would re-evaluate the examinations together and reach a consensus. To align the paired images of T2 and DWI produced at different times. We set the T2 as the target image and the DWI as the source image to perform the pre-processing of non-rigid registration between T2 and DWI by using the Demons non-rigid registration method. It has been widely used in the field of medical image registration since it was proposed by Thirion [21]. ",None,none,Name of Dataset not Provided
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,38,5,"We validate our approach on a dataset with 16,113 exams and further demonstrate that it effectively captures patterns of changes from prior mammograms, such as changes in breast density, resulting in improved short-term and long-term breast cancer risk prediction.

The method is trained and evaluated on a large and diverse dataset of over 9,000 patients and shown to outperform a model based on state-of-the art risk prediction techniques for mammography [33]

We compiled an in-house mammography dataset comprising 16,113 exams (64,452 images) from 9,113 patients across institutions from the United States, gathered between 2010 and 2021. Each mammogram includes at least one prior mammogram. The dataset has 3,625 biopsy-proven cancer exams, 5,394 biopsyproven benign exams, and 7,094 normal exams. Mammograms were captured using Hologic (72.3%) and Siemens (27.7%) devices. We partitioned the dataset by patient to create training, validation, and test sets. The validation set contains 800 exams (198 cancer, 210 benign, 392 normal) from 400 patients, and the test set contains 1,200 exams (302 cancer, 290 benign, 608 normal) from 600 patients. All data was de-identified according to the U.S HHS Safe Harbor Method. Therefore, the data has no PHI (Protected Health Information) and IRB (Institutional Review Board) approval is not required.",None,none,Name of Dataset not Provided
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,68,2,"Our XAI technique was applied to explain the MTANN model’s decision in a liver tumor segmentation task [20]. Dynamic contrast-enhanced liver CT scans consisting of 42 patients with 194 liver tumors in the portal venous phase from the LiTS database [21] were used in this study. Each slice of the CT volumes in the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of 0.60–1.00 mm and thicknesses of 0.20–0.70 mm. The dataset consists of the original hepatic CT image with the liver mask and the “gold-standard” liver tumor region manually segmented by a radiologist, as illustrated in Fig. 2.",None,lits dataset,LiTS Dataset
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,58,2,"we collected 453 CE scans with non-standard GBCA doses in the set of {10%, 20%, 33%} along with the corresponding standard-dose (0.1 mmol/kg) scan after applying the remaining contrast agent. Using this dataset, we aim at the semantic interpolation of the GBCA signal at various fractional dose levels. To this end, we use GANs to learn the contrast enhancement behavior from the dataset collective and thereby enable the synthesis of contrast signals at various dose levels for individual cases. Further, to minimize the smoothing effect [19] of typical content losses (e.g. 1 or perceptual [16]), we develop a noise-preserving content loss function based on the Wasserstein distance between paired image patches calculated using a Sinkhornstyle algorithm. This novel loss enables a faithful generation of noise, which is important for the identification of enhancing pathologies and their usability as additional training data.

To simplify learning of the contrast accumulation behavior, we adapt the preprocessing pipeline of BraTS [6]. Further details of the dataset and the preprocessing are in the supplementary material.

Further details of the dataset, model and training can be found in the supplementary.",None,brats dataset,BraTS Dataset
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,56,9,"Due to highly imbalanced label representation in the surgical cavity, the model employed solely FLIm data from healthy surgical cavity tissue for
training and classified the residual tumors as an anomaly. FLIm data from N = 22 patients undergoing upper aerodigestive oncologic surgery were used to train and validate the classification model using leave-one-patient-out cross-validation.

This study used a multispectral fluorescence lifetime imaging (FLIm) device to acquire data [14]

The research was performed under the approval of the UC Davis Institutional Review Board (IRB) and with the patient’s informed consent. All Patients were anesthetized, intubated, and prepared for surgery as part of the standard of care. N = 22 patients are represented in this study, comprising HNC in the palatine tonsil (N=15) and the base of the tongue (N = 7). For each patient, the operating surgeon conducted an en bloc surgical tumor resection procedure (achieved by TORS-electrocautery instruments), and the resulting excised specimenwas sent to a surgical pathology room for grossing. The tissue specimenwas serially sectioned to generate tissue slices,which were then formalin-fixed, paraffin-embedded, sectioned, and stained to create Hematoxylin & Eosin (H&E) slides
for pathologist interpretation (see Fig. 1).",None,none,Name of Dataset not Provided
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,66,9,"*location details of data collection found outside paper's content.

Developed and validated with the public RESECT clinical database.

we used the RESECT (REtro-Spective Evaluation of Cerebral Tumors) dataset [16], MRI and iUS scans at different surgical stages from 23 subjects who underwent low-grade glioma resection surgeries. we took 22 cases with T2FLAIR MRI that better depicts tumor boundaries and iUS acquired before resection. An example of an MRI-iUS pair from a patient is shown in Fig. 1

One limitation of our work lies in the limited patient data, as public iUS datasets are scarce, while the settings and properties of US scanners can vary, potentially affecting the DL model designs.

"," Pre-operative magnetic resonance images (MRI), and intra-operative ultrasound (US) scans were acquired from 23 patients with low-grade gliomas who underwent surgeries at St. Olavs University Hospital between 2011 and 2016","resect dataset, resect (retro-spective evaluation of cerebral tumors) dataset",RESECT Dataset
Gall Bladder Cancer Detection from US Images with only Image Level Labels,20,1,"Gallbladder Cancer Detection in Ultrasound Images: We use the public GBC US dataset [3] consisting of 1255 image samples from 218 patients.
The dataset contains 990 non-malignant (171 patients) and 265 malignant (47 patients) GB images (see Fig. 2 for some sample images). The dataset contains image labels as well as bounding box annotations showing the malignant regions. Note that, we use only the image labels for training. We report results on 5-fold cross-validation. We did the cross-validation splits at the patient level, and all images of any patient appeared either in the train or validation split.

Polyp Detection in Colonoscopy Images: We use the publicly available Kvasir-SEG [17] dataset consisting of 1000 white light colonoscopy images showing polyps (see Fig. 2). Since Kvasir-SEG does not contain any control images, we add 600 non-polyp images randomly sampled from the PolypGen [1] dataset. Since the patient information is not available with the data, we use random stratified splitting for 5-fold cross-validation.",None,"gbc us dataset, kvasir-seg dataset, polypgen dataset","PolypGen Dataset, GBC US Dataset, Kvasir-SEG Dataset"
Gene-Induced Multimodal Pre-training for Image-Omic Classification,49,6,"We verify the effectiveness of our method on The Caner Genome Atlas (TCGA) non-small cell lung cancer (NSCLC) dataset, which contains two cancer subtypes, i.e., Lung Squamous Cell Carcinoma (LUSC) and Lung Adeno carcinoma (LUAD).
 TCGA-NSCLC dataset, i.e., only pathological WSIs are included as input.",None,"the caner genome atlas (tcga), nsclc dataset, tcga-nsclc dataset ","TCGA Dataset, TCGA-NSCLC"
Geometry-Invariant Abnormality Detection,29,1,"*demographics and data collection details (location) found outside paper's content

For this work we leveraged whole-body PET/CT data from different sources to explore the efficacy of our approach for varying image geometries. 211 scans from NSCLC Radiogenomics [2,3,10,16] combined with 83 scans from a proprietary dataset constitute our lower resolution dataset.

For evaluation, we use four testing sets: a lower resolution set derived from both the NSCLC and the private dataset. a testing set with random crops of the same NSCLC/private testing dataset and finally a testing set that has been rotated through 90◦ using the high resolution testing data.



"," : 
Acknowledgements: The models were trained on the NVIDIA Cambridge-1. Private dataset was obtained through King’s College London (14/LO/0220 ethics application number). 

NSCLC
{nsclc: standford university school of medicine, palo alto veterans affairs healtcare system}
Between 2008 and 2012, we collected clinical and imaging data for 211 subjects referred for surgical treatment and obtained tissue samples from the excised tumors, where available. Tissue samples were analyzed to produce molecular phenotypes using gene microarrays, RNA sequencing technology, or both, in addition to standard-of-care NSCLC mutational testing. We also collected clinical data, such as: age, gender, weight, ethnicity, smoking status, TNM stage, histopathological grade. 

The R01 cohort consisted of 162 NSCLC subjects (38 females, 124 males, age at scan: mean 68, range 42–86) from Stanford University School of Medicine (69) and Palo Alto Veterans Affairs Healthcare System (93). Subjects were recruited between April 7th, 2008 and September 15th, 2012.","nsclc radiogenomics, nsclc dataset",Name of Dataset not Provided
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,11,5,"We evaluated our method with two studies on retrospectively collected patient datasets that were manually annotated by an expert radiologist.

DLIVER dataset, DLUNG dataset.

Lung and liver CT studies were retrospectively obtained from two medical centers (Hadassah Univ Hosp Jerusalem Israel) during the routine clinical examination of patients with metastatic disease. Each patient study consists of at least 3 scans.

",None,"dliver dataset, dlung dataset","DLUNG Dataset, DLIVER Dataset"
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,61,3,"*sex, age was found outside the paper's content, and the location details of data collections.

We conduct our study on the MSD Spleen and the AutoPET datasets to explore the segmentation of both anatomy (spleen) and pathology (tumor lesions).

We trained and evaluated all of our models on the openly available AutoPET [1] and MSD Spleen [2] datasets. MSD Spleen [2] contains 41 CT volume.  AutoPET [1] consists of 1014 PET/CT volumes with annotated tumor lesions of melanoma, lung cancer, or lymphoma. We discard the 513 tumor-free patients, leaving us with 501 volumes. We also only use PET data for our experiments. 
","{msd spleen: memorial sloan kettering cancer center, autopet: unknown}

MSD spleen
Task09_Spleen The spleen dataset was comprised of patients undergoing chemotherapy treatment for liver metastases at Memorial Sloan Kettering Cancer Center (New York, NY, USA) and previously reported [32]. 
AutoPet
Publication of anonymized data was approved by the institutional ethics committee of the Medical Faculty of the University of Tübingen as well as the institutional data security and privacy review board. Data from 1,014 whole-body FDG-PET/CT examinations of 900 patients acquired between 2014 and 2018 as part of a prospective registry study9 were included in this dataset.

The selection criteria for positive and negative samples were: age >18 years patient characteristics: https://www.nature.com/articles/s41597-022-01718-3/tables/2","msd spleen dataset, autopet dataset","MSD Spleen Dataset, AutoPET Dataset"
Histopathology Image Classification Using Deep Manifold Contrastive Learning,66,6,"We tested our proposed method on two different tasks: (1) intrahepatic cholangiocarcinomas( IHCCs) subtype classification and (2) liver cancer type classification. 

The dataset for the former task was collected from 168 patients with 332 WSIs from Seoul National University hospital. IHCCs can be further categorized into small duct type (SDT) and large duct type (LDT). Using gene mutation information as prior knowledge, we collected WSIs with wild KRAS and mutated IDH genes for use as training samples in SDT, and WSIs with mutated KRAS and wild IDH genes for use in LDT. The rest of the WSIs were used as testing samples. 
The liver cancer dataset for the latter task was composed of 323 WSIs, in which the WSIs can be further classified into hepatocellular carcinomas (HCCs) (collected from Pathology AI Platform [1]) and IHCCs.We collected 121 WSIs for the training set, and the remaining WSIs were used as the testing set.",None,none,Name of Dataset not Provided
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,24,5,"We extensively evaluated and compared the proposed method with existing methods in the multicenter (n=4) dataset with 1,070 patients with PDAC,

In this study, we used data from Shengjing Hospital to train our method with 892 patients, and data from three other centers, including Guangdong Provincial People’s Hospital, Tianjin Medical University and Sun Yatsen University Cancer Center for independent testing with 178 patients. The contrast-enhanced CT protocol included non-contrast, pancreatic, and portal venous phases. PDAC masks for 340 patients were manually labeled by a radiologist from Shengjing Hospital with 18 years of experience in pancreatic cancer, while the rest were predicted using self-learning models [11,24] and checked by the same annotator. Other vessel masks were generated using the same semisupervised segmentation models. C-index was used as our primary evaluation metric for survival prediction. We also reported the survival AUC, which estimates the cumulative area under the ROC curve for the first 36 months.",None,none,Name of Dataset not Provided
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,45,1,"We use publicly available data collected from a breast phantom (Model 059, CIRS: Tissue Simulation & Phantom Technology, Norfolk, VA) using an Alpinion E-Cube R12 research US machine (Bothell, WA, USA). The center frequency was 8MHz and the sampling frequency was 40MHz. The Young’s modulus of the experimental phantom was 20 kPa and contains several inclusions with Young’s modulus of higher than 40 kPa. This data is available online at
http://code.sonography.ai in [16].
In vivo data was collected at Johns Hopkins hospital from patients with liver cancer during open-surgical RF thermal ablation by a research Antares Siemens system using a VF 10-5 linear array with the sampling frequency of 40MHz and
the center frequency of 6.67 MHz. The institutional review board approved the study with the consent of the patients. ",None,none,Name of Dataset not Provided
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,42,3,"Evaluating the proposed solution on the LIDC-IDRI dataset shows that it combines increased interpretability with above state-of-the-art
prediction performance.

The proposed approach is evaluated using the publicly available LIDCIDRI dataset consisting of 1018 clinical thoracic CT scans from patients with Non-Small Cell Lung Cancer (NSCLC) [2,3]. Each lung nodule with a minimum size of 3mm was segmented and annotated with a malignancy score ranging from 1-highly unlikely to 5-highly suspicious by one to four expert raters.
Nodules were also scored according to their characteristics with respect to predefined attributes, namely subtlety (difficulty of detection, 1-extremely subtle, 5-obvious), internal structure (1-soft tissue, 4-air ), pattern of calcification (1- popcorn, 6-absent), sphericity (1-linear, 5-round), margin (1-poorly defined, 5- sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no spiculation, 5-marked spiculation), and texture (1-non-solid, 5-solid). The pylidc framework [7] is used to access and process the data. The mean attribute annotation and the mean and standard deviation of the malignancy annotations are calculated. The latter was used to fit a Gaussian distribution, which serves as the ground truth label for optimization. Samples with a mean expert malignancy score of 3-indeterminate or annotations from fewer than three experts were excluded in consistency with the literature [8,9,11].
",None,"lidc-idri dataset, ","LIDC-IDRI Dataset, Name of Dataset not Provided"
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,8,5,"We collect a large-scale dataset with both tumor and non-tumor subjects, where the non-tumor subjects includes not only healthy ones, but also patients with various diffuse liver diseases such as steatosis and hepatitis to improve the robustness of the algorithm

Our dataset contains 810 normal subjects and 939 patients with liver tumors. Each normal subject has a non-contrast (NC) CT, while each patient has a dynamic contrast-enhanced (DCE) CT scan with NC, arterial, and venous phases.

We first train an nnU-Net on public datasets to segment liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and pancreas), and then crop the liver region to train PLAN.

The NC test set contains 198 tumor cases, 202 completely normal cases, and 100 “hard” non-tumor cases which may have larger image noise, artifact, ascites, diffuse liver diseases such as hepatitis and steatosis. These cases are used to test the robustness of the model in real-world screening scenario with diverse tumor-free images.",None,none,Name of Dataset not Provided
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,45,2,"Through extensive experiments on three multi-organ segmentation datasets, we demonstrate that integrating LRC to an existing self-supervised method in a limited annotation setting significantly improves segmentation performance.

During both global and local pre-training stages, we pre-train the encoders on the Abdomen-1K [17] dataset. It contains over one thousand CT images which equates to roughly 240,000 2D slices. The CT images have been curated from 12 medical centers and include multi-phase, multi-vendor, and multi-disease cases.Although segmentation masks for liver, kidney, spleen, and pancreas are provided.

During the fine-tuning stage, we perform extensive experiments on three datasets with respect to different regions of the human body: abd-110 dataset: abdomen, thorax-85 dataset: thorax, HaN dataset: head and neck

ABD-110 is an abdomen dataset from [25] that contains 110 CT images from patients with various abdominal tumors and these CT images were taken during the treatment planning stage. We report the average DSC on 11 abdominal organs (large bowel, duodenum, spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney, stomach and gallbladder).
Thorax-85 is a thorax dataset from [5] that contains 85 thoracic CT images. We report the average DSC on 6 thoracic organs (esophagus, trachea, spinal cord, left lung, right lung, and heart).
HaN is from [24] and contains 120 CT images covering the head and neck region. We report the average DSC on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right optical nerve, left parotid, right parotid, left submandibular gland, and right submandibular gland).

"," AbdomenCT-1K
Most existing abdominal organ segmentation datasets have limitations in diversity and scale. In this paper, we present a large-scale dataset that is closer to real-world applications and has more diverse abdominal CT cases. consists of 1112 3D CT scans from five existing datasets: LiTS (201 cases) [16], KiTS (300 cases) [17], MSD Spleen (61 cases) and Pancreas (420 cases) [20], NIH Pancreas (80 cases) [21], [22], [23], and a new dataset from Nanjing University (50 cases). The 50 CT scans in the Nanjing University dataset are from 20 patients with pancreas cancer, 20 patients with colon cancer, and 10 patients with liver cancer. The number of plain phase, artery phase, and portal phase scans are 18, 18, and 14 respectively

Thorax-85
The data sources included in-house CT images (540 cases) and public data derived from the Cancer Image Archive (TCIA [21], 215 cases).","abdomen-1k dataset, abd-110 dataset, thorax-85 dataset, han dataset","ABD-110 Dataset, THORAX-85 Dataset, HaN Dataset, ABDOMEN-1K Dataset"
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,62,2,"Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution. 

This study used an imaging-only cohort from the NLST [28] and three multimodal cohorts from our home institution with IRB approval (Table 1).
For the NLST cohort (https://cdas.cancer.gov/nlst/), we identified cases who had a biopsy-confirmed diagnosis of lung malignancy and controls who had a positive screening result for an SPN but no lung malignancy. We randomly sampled from the control group to obtain a 4:6 case control ratio. Next, EHRPulmonary was the unlabeled dataset used to learn clinical signatures in an unsupervised manner. We searched all records in our EHR archives for patients who had billing codes from a broad set of pulmonary conditions, intending to capture pulmonary conditions beyond just malignancy. Additionally, Image-EHR was a labeled dataset with paired imaging and EHRs. We searched our institution’s imaging archive for patients with three chest CTs within five years. In the EHR-Image cohort, malignant cases were labeled as those with a billing code for lung malignancy and no cancer of any type prior. Importantly, this case criteria includes metastasis from cancer in non-lung locations. Benign controls were those who did not meet this criterion. Finally, Image-EHR-SPN was a subset of Image-EHR with the inclusion criteria that subjects had a billing code for an SPN and no cancer of any type prior to the SPN. We labeled malignant cases as those with a lung malignancy billing code occurring within three years after any scan and only used data collected before the lung malignancy code. All data within the five-year period were used for controls. We removed all billing codes relating to lung malignancy. A description of the billing codes used to define SPN and lung cancer events are provided in Supplementary 1.2.",None,nlst dataset,NLST Dataset
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,7,7,"We demonstrate the effectiveness of our approach through experiments using mammography datasets, which show the superiority of Mammo-Net.

Mammogram Dataset. Our experiments were conducted on CBIS-DDSM [12] and INbreast [16]. The CBIS-DDSM dataset contains 1249 exams that have been divided based on the presence or absence of masses, which we used to perform mass classification. The INbreast dataset contains 115 exams with both masses and micro-calcifications, on which we performed benign and malignant classification. We split the INbreast dataset into training and testing sets in a 7:3 ratio. It is worth noting that the official INbreast dataset does not provide image-level labels, so we obtained these labels following Shen et al. [20].

Eye Gaze Dataset. Eye movement data was collected by reviewing all cases in INbreast using a Tobii Pro Nano eye tracker. The scenario is shown in Appendix and can be accessed at https://github.com/JamesQFreeman/MicEye. Participated radiologist has 11 years of experience in mammography screening.",None," cbis-ddsm dataset, inbreast dataset, eye gaze dataset","INbreast Dataset, CBIS-DDSM Dataset, Eye Gaze Dataset"
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,16,1,"*demographic info such as age, gender, details of location of data collection provided outside of paper's content. study subjects are labelled as patients outside paper's content. whether datasets are public or private was to find in the section for ""References""

We evaluate our method on T2-weighted brain MR and chest X-ray datasets to provide direct comparisons to state-of-the-art methods over a wide range of real anomalies. 
For brain MRI we train on the Human Connectome Project (HCP) dataset [28] which consists of 1113 MRI scans of healthy, young adults acquired as part of a scientific study. 
To evaluate, we use the Brain Tumor Segmentation Challenge 2017 (BraTS) dataset [1], containing 285 cases with either high or low grade glioma, and the ischemic stroke lesion segmentation challenge 2015 (ISLES) dataset [13], containing 28 cases with ischemic stroke lesions. The data from both test sets was acquired as part of clinical routine. The HCP dataset was resampled to have 1mm isotropic spacing to match the test datasets. 
For chest X-rays we use the VinDr-CXR dataset [18] including 22 differen local labels.

","HCP
Our decision to acquire data from twins and non-twin siblings will enable analyses of the heritability of brain circuits and will greatly increasethe power of genetic analyses. However, due to the relatively small size and localized geography of the subject population, HCP faces some extra challenges with respect to subject confidentiality and privacy, especially regarding sensitive data. One likely scenario is that the publicly released HCP dataset will include all neuroimagingdata and most behavioral data, along with subject sex and age range (e.g., 5-year grouping). Information about family relationships, ethnic and racial identity, exact age (year), and potentially sensitive behavioral measures would be restricted to qualified investigators who agree to appropriate limits on storage and distribution of sensitive data. The publicly released data could also include a dataset consisting of only one individual per family, thereby allowing analyses not confounded by unspecified family relationships.

Brats
In 2017, thanks to additional contributions to the BraTS dataset, from CBICA@UPenn and the
University of Alabama in Birmingham (UAB), a validation set was included to facilitate algorithm
fine-tuning following a ML paradigm of training, validation, and testing datasets. Notably, in 2017
the number of cases was doubled with respect to the previous year, amounting to 477 cases, which
was further increased in 2018 with 542 cases, thanks to contributions from MD Anderson Cancer
Center in Texas, the Washington University School of Medicine in St. Louis, and the Tata Memorial
Center in India

VinDr-CXR
{vin-dr cxr: HMUH, H108}
{vin-dr cxr: vietnam}
The building of VinDr-CXR dataset, as visualized in Fig. 1, is divided into three main steps: (1) data collection, (2) data filtering, and (3) data labeling. Between 2018 and 2020, we retrospectively collected more than 100,000 CXRs in DICOM format from local PACS servers of two hospitals in Vietnam, HMUH and H108. Imaging data were acquired from a wide diversity of scanners from well-known medical equipment manufacturers, including Phillips, GE, Fujifilm, Siemens, Toshiba, Canon, Samsung, and Carestream. The ethical clearance of this study was approved by the Institutional Review Boards (IRBs) of the HMUH and H108 before the study started. The need for obtaining informed patient consent was waived because this retrospective study did not impact clinical care or workflow at these two hospitals and all patient-identifiable information in the data has been removed.

dataset provides info about patient's sex, age, size, weights","human connectome project (hcp) dataset, hcp,  brain tumor segmentation challenge 2017 (brats) dataset, brats 2017 dataset, ischemic stroke lesion segmentation challenge 2015 (isles) dataset, isles dataset, vindr-cxr dataset ","VINDR-CXR Dataset, BraTS Dataset 2017, Human Connectome Project, ISLES Dataset"
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,39,6,"*demographic information provided in supplementary information.
** training and testing model with patients divided by medical centers: patients from chum and chuv used for testing, and others for training
Extensive experiments on the public dataset of HEad and neCK TumOR segmentation and outcome prediction challenge (HECKTOR 2022) demonstrate that our XSurv outperforms state-of-the-art survival prediction methods.

We adopted the training dataset of HECKTOR 2022 (refer to https://hecktor.grand-cha llenge.org/), including 488 H&N cancer patients acquired from seven medical centers [7], while the testing dataset was excluded as its ground-truth labels are not released. Each patient underwent pretreatment PET/CT and has clinical indicators. We present the distributions of all clinical indicators in the supplementary materials. Recurrence- Free Survival (RFS), including time-to-event in days and censored-or-not status, was provided as ground truth for survival prediction, while PT and MLN annotations were provided for segmentation. The patients from two centers (CHUM and CHUV) were used for testing and other patients for training, which split the data into 386/102 patients in training/testing sets. We trained and validated models using 5-fold cross-validation within the training set and evaluated them in the testing set.

","HECKTOR 2022
canada, switzerland, france, usa
The data used in this challenge is multi-centric (9 centers in total), including four centers in Canada [Vallières et al. 2017], two centers in Switzerland [Castelli et al. 2017; Bogowicz et al. 2017], two centers in France [Hatt et al. 2019; Legot et al. 2018], and one center in USA [Ger et al. 2019] for a total of XXX patients with annotated GTVp and GTVn.

The clinical information for each patient is contained in the hecktor2022_clinical_info_training.csv, including center, gender, age, weight, tobacco and alcohol consumption, performance status (Zubrod), HPV status, treatment (surgery and/or chemotherapy in addition to the radiotherapy that all patients underwent).  Note that some information may be missing for some patients. 

Data were collected from 9 centers :
Hôpital général juif, Montréal, CA (HGJ)
Centre hospitalier universitaire de Sherbooke, Sherbrooke, CA (CHUS)
Hôpital Maisonneuve-Rosemont, Montréal, CA (HMR)
Centre hospitalier de l’Université de Montréal, Montréal, CA (CHUM)
Centre Hospitalier Universitaire Vaudois, CH (CHUV)
Centre Hospitalier Universitaire de Poitiers, FR (CHUP)
MD Anderson Cancer Center, Houston, Texas, USA (MDA)
UniversitätsSpital Zürich, CH (USZ)
Centre Henri Becquerel, Rouen, FR (CHB)",hecktor 2022 dataset,HECKTOR 2022 Dataset
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,59,1,"We use the publicly available Decath-Pancreas dataset of 273 segmentations from patients who underwent pancreatic mass resection [24].


","Images were provided by Memorial Sloan Kettering Cancer Center (New York, NY, USA) and
were previously reported in radiomic applications [24, 25, 26]. Four hundred and twenty portal venous phase CT scans were obtained with the following reconstruction and acquisition parameters: pitch/table speed 0.984–1.375/39.37–27.50 mm; automatic tube current modulation range, 220–380 mA; noise index, 12.5–14;20 kVp; tube rotation speed, 0.7–0.8 ms; scan delay, 80–85 s; and axial slices reconstructed at 2.5 mm intervals. The pancreatic parenchyma and pancreatic mass (cyst or tumour) were manually segmented in each slice by an expert abdominal radiologist using the Scout application [27].",decath-pancreas dataset ,DECATH-Pancreas Dataset
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,46,6,"Based on stateof- the-art multiple instance learning architectures and two thyroid cancer data sets, an exhaustive study was conducted considering a range of common data augmentation strategies.
In this work, we aimed at distinguishing different nodular lesions of the thyroid, focusing especially on benign follicular nodules (FN) and papillary carcinomas (PC).
The data set utilized in the experiments consists of 80 WSIs overall. One half (40) of the data set consists of frozen and the other half (40) of paraffin sections [5]), representing the different modalities. All images were acquired during clinical routine at the Kardinal Schwarzenberg Hospital. Procedures were approved by the ethics committee of the county of Salzburg (No. 1088/2021). The mean and median age of patients at the date of dissection was 47 and 50 years, respectively. The data set comprised 13 male and 27 female patients, corresponding to a slight gender imbalance. They were labeled by an expert pathologist with over 20 years experience.",None,none,Name of Dataset not Provided
Multi-scale Prototypical Transformer for Whole Slide Image Classification,58,6,"To evaluate the effectiveness ofMSPT, we conducted experiments on two public dataset, namely Camelyon16 [24] and TCGA-NSCLC. Camelyon16 is a WSI dataset for the automated detection of metastases in lymph node tissue slides. It includes 270 training
samples and 129 testing samples. 
The TCGA-NSCLC dataset includes two sub-types of lung cancer, i.e., Lung Squamous Cell Carcinoma (TGCA-LUSC) and Lung Adenocarcinoma (TCGA-LUAD). We collected a total of 854 diagnostic slides from the National Cancer Institute Data Portal (https:// portal.gdc.cancer.gov). ",None,"camelyon16 dataset, tcga-nsclc dataset","Camelyon16 Dataset, TCGA-NSCLC"
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,55,1,"We train our approach at large scale with more than 50,000 computed tomography (CT) scans and validate it on two different applications: 1) Tracking of generic lesions based on the DeepLesion dataset, including liver tumors, lung nodules, enlarged lymph-nodes, for which we report highest matching accuracy of 92%, with localization accuracy that is nearly 10% higher than the state-ofthe-
art; and 2) Tracking of lung nodules based on the NLST dataset for which we achieve similarly high performance.

we (1) train a pixel-wise self-supervised system using a very large and diverse dataset of 52,487 CT volumes and (2) evaluate on two publicly available datasets.

We train the universal and fine-grained anatomical point matching model using an in-house CT dataset (VariousCT). The training dataset contains 52,487 unlabeled 3D CT volumes capturing various anatomies, including chest, head, abdomen, pelvis, and more.

For NLST, we randomly selected a subset of 1045 test images coming from 420 patients with up to 3 studies

The authors thank the National Cancer Institute for access to NCI’s data collected by the National Lung Screening Trial (NLST). The statements contained herein are solely those of the authors and do not represent or imply concurrence or endorsement by NCI.",None,"nlst dataset, deeplesion dataset","NLST Dataset, DeepLesion Dataset"
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,71,4,"head and neck (HaN)

Our implementation within the nnU-Net framework shows promising results on a dataset of CT and MR image pairs from the same patients.

Image Datasets. The proposed methodology was evaluated on two publicly available datasets: our recently released HaN-Seg dataset [14] and the PDDCA dataset [15]. The HaN-Seg dataset comprises CT and T1-weighted MR images of 56 patients, which were deformably registered with the SimpleElastix registration tool, and corresponding curated manual delineations of 30 OARs (for details, please refer to [14]). Although only a subset of images is publicly available1 due to the ongoing HaN-Seg challenge2, both the publicly available training as well as the privately withheld test images were used in our 4-fold cross-validation experiments. On the other hand, to evaluate the generalization ability of our method, we also conducted experiments on the CT-only PDDCA dataset (for details, please refer to [15]), from which we collected 15 images from the offand on-site test sets of the corresponding challenge for our evaluation. As this dataset is widely used for evaluating the performance of automatic HaN OAR segmentation methods, it serves as a valuable benchmark for comparison with other state-of-the-art methods. Note that none of the images from the CT-only PDDCA dataset were used for training, and as our model expects two inputs, we substituted the missing MR modality with an empty matrix (i.e. zeros).",None,"han-seg dataset, pddca dataset","HaN-SEG Dataset, PDDCA Dataset"
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,62,5,"We collect a renal tumor US dataset of 179 cases from two medical centers, which is split into the training and validation sets. We further collect 36 cases from the two medical centers mentioned above (14 benign cases) and another center (Fujian Provincial Hospital, 22 malignant cases) to form the test set. Each case has a video with simultaneous imaging of B-mode and CEUS-mode.

Note: the two medical centers mentioned above are not to be found in the content of the paper (unless they mean in the section under title name, which isnt obvious)",None,none,Name of Dataset not Provided
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,46,10,"We apply the method to a public dataset and to an in-house dataset and show that it matches the performance of a supervised approach and outperforms it when measurement noise is present in the data.

We validated our method on publicly available data [15] against a supervised approach [6] and applied it to an internal clinical dataset of 30 lung cancer patients. We explore different dataset sizes to understand their effects on the reconstructed images.

First, we used the SPARE Varian dataset to study whether Noise2Aliasing can match the performance of the supervised baseline and if it can outperform it when adding noise to the projections. Then, we use the internal dataset to explore
the requirements for the method to be applied to an existing clinical dataset. These required around 64 GPU days on NVIDIA A100 GPUs. The Datasets used in this study are two:
1. The SPARE Varian dataset was used to provide performance results on publicly available patient data. To more closely resemble normal respiratory motion per projection image, the 8 min scan has been used from each patient (five such scans are available in the dataset). Training is performed over 4 patients while 1 patient is used as a test set. The hyperparameters are optimized over the training dataset.

2. An internal dataset (IRB approved) of 30 lung cancer patients’ 4DCBCTs  from  2020 to 2022, originally used for IGRT, with 25 patients for training and 5 patients for testing. 

The method removes noise more reliably when the dataset size is increased, however further analysis is required to establish a good quantitative measurement of this phenomenon",None,spare varian dataset,SPARE Dataset
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,71,10,"Extensive experiments with seven public datasets show that our NICE-Trans outperforms state-of-the-art registration methods on both registration accuracy and runtime.

We evaluated the proposed NICE-Trans on the task of inter-patient brain MRI registration, which is a common benchmark task in medical image registration studies [7–9, 12–18].We followed the dataset settings in [18]: 2,656 brainMRI images acquired from four public datasets (ADNI [27], ABIDE [28], ADHD [29], and IXI [30]) were used for training; two public brainMRI datasets with anatomical segmentation (Mindboggle [31] and Buckner [32]) were used for validation and testing. TheMindboggle dataset contains 100 MRI images and were randomly split into 50/50 images for validation/testing. The Buckner dataset contains 40 MRI images and were used for testing only. In addition to the original settings of [18], we adopted an additional public brain MRI dataset (LPBA [33]) for testing, which contains 40 MRI images. We performed brain extraction and intensity normalization for each MRI image with FreeSurfer [32]. Each image was placed at the same position via Center of Mass (CoM) initialization [34], and then was cropped into 144 × 192 × 160 voxels.",None,"adni dataset abide dataset, adhd dataset, ixi dataset, mindboggle dataset, buckner dataset, lpba dataset","ADHD Dataset, Mindboggle Dataset, Buckner Dataset, LPBA Dataset, IXI Dataset"
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,58,9,paper does not contain any clear dataset section containing information of implemented dataset,None,none,Name of Dataset not Provided
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,29,8,"GSP1000 Processed Connectome. It publicly released preprocessed restingstate fMRI data of 1000 healthy right-handed subjects with an average age 21.5±2.9 years and approximately equal numbers of males and females from the Brain Genomics Superstruct Project (GSP) [5], where the concrete image acquisition parameters and preprocessing procedures can be found as well. Specifically, a slightly modified version of Yeo’s Computational Brain Imaging Group (CBIG) fMRI preprocessing pipeline (https://github.com/bchcohenlab/CBIG) was employed to obtain either one or two preprocessed resting-state fMRI runs of each subject that had 120 time points per run and were spatially normalized into the MNI152 template with 2mm3 voxel size. We downloaded and used the first-run preprocessed resting-state fMRI of each subject for the following analysis.

BraTS 2020. It provided an open-access pre-operative imaging training dataset to segment brain tumors of glioblastoma (GBM, belonging to high grade glioma) and low grade glioma (LGG) patients, as well as to predict overall survival time of GBM patients [18]. This training dataset contained 133 LGG and 236 GBM patients, and each patient had four MRI modalities, including T1, post-contrast T1-weighted, T2-weighted, and T2 Fluid Attenuated Inversion Recovery",None,"brain genomics superstruct project (gsp), gsp dataset, brats 2020 dataset","Brain Genomics Superstruct Project, BraTS Dataset 2020"
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,20,5,"To meet the needs of both lowdose and noncontrast screening, we collect a large-scale dataset of 12,852 and 4,029 nodules from low-dose and noncontrast CTs respectively, each with pathology- or follow-up-confirmed labels. Experiments on several datasets demonstrate that our method achieves advanced screening performance on both low-dose and noncontrast scenarios.

For the LDCT, we annotate more than 12,852 nodules from 8,271 patients from the NLST dataset [14]. For the NCCT, we annotate over 4,029 nodules from over 2,565 patients from our collaborating hospital. Experimental results on several datasets demonstrate that our method achieves outstanding performance on both LDCT and NCCT screening scenarios.

NLST is the first large-scale LDCT dataset for low-dose CT lung cancer screening purpose [14]. There are 8,271 patients enrolled in this study. An experienced radiologist chose the last CT scan of each patient, and localized and labeled the nodules in the scan as benign or malignant based on the rough candidate nodule location and whether the patient develops lung cancer provided by NLST metadata. The nodules with a diameter smaller than 4mm were excluded. The in-house cohort was retrospectively collected from 2,565 patients at our collaborating hospital between 2019 and 2022. Unlike NLST, this dataset is noncontrast chest CT, which is used for routine clinical care. Segmentation annotation: We provide the segmentation mask for our in-house data, but not for the NLST data considering its high cost of pixel-level labeling. The nodule mask of each in-house data was manually annotated with the assistance of CT labeler [20] by our radiologists, while other contextual masks such as lung, vessel, and trachea were generated using the TotalSegmentator [21].

Train-Val-Test: The training set contains 9,910 (9,413 benign and 497 malignant) nodules from 6,366 patients at NLST, and 2,592 (843 benign and 1,749 malignant) nodules from 2,113 patients at the in-house cohort. The validation set contains 1,499 (1,426 benign and 73 malignant) nodules from 964 patients at NLST. The NLST test set has 1,443 (1,370 benign and 73 malignant) nodules from 941 patients. The in-house test set has 1,437 (1,298 benign and 139 malignant) nodules from 452 patients. We additionally evaluate our method on the LUNGx [2] challenge dataset, which is usually used for external validation in previous work [6,11,24]. LUNGx contains 83 (42 benign and 41 malignant) nodules, part of which (13 scans) were contrast-enhanced. Segmentation: We also evaluate the segmentation performance of our method on the public nodule segmentation dataset LIDC-IDRI [3], which has 2,630 nodules with nodule segmentation mask. Evaluation metrics: The area under the receiver operating characteristic curve (AUC) is used to evaluate the malignancy prediction performance.",None,"nlst dataset,  lungx challenge dataset, lidc-idri dataset","NLST Dataset, LUNGx Challenge Dataset, LIDC-IDRI Dataset"
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,7,5,"We evaluate our method on two datasets using two state-of-the-art MIL methods as baselines.

Conducting extensive experiments on two separate datasets. Our method was seamlessly integrated with two prior state-of-the-art methods, demonstrating its compatibility and adaptability. The experiments resulted in improved performance, indicating that our method enhances the efficacy of these existing approaches.

CD-ITB Dataset. CD-ITB is a private dataset consisting of 853 slides from 163 patients, with binary patient-level labels of CD or ITB in a ratio of 103:60 and tri-class slide-level labels of CD, ITB, and normal slides in a ratio of 436:121:296, respectively. On average, there were 5 slides per patient. The slides were scanned at a magnification of 40× (0.25 μm/px), and annotations were curated by experienced pathologists. We adopted a patient-level stratification approach for 5-fold cross-validation, with 20% of the training set randomly assigned as the validation set for each fold. The dataset comprises an average of 2.3k instances per bag, with the largest bag containing over 16k instances.

Camelyon17 Dataset. Camelyon17 [1] is a publicly dataset, and its training set comprises 500 slides from 100 breast cancer patients with lymph node metastases. The slides are classified into four distinct categories, namely negative, ITC, micro, and macro, in proportions of 318:36:59:87, respectively. There were 5 slides per patient on average. The patients are divided into two groups based on their pN stage, namely lymph node positive and lymph node negative,
in proportions of 24:76, respectively. The data folding method is the same as the CD-ITB dataset. The average number of instances per bag is approximately 6.1k, and the largest bag contains over 23k instances.",None,"cd-itb dataset, camelyon17 dataset","Camelyon17 Dataset, CD-ITB Dataset"
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,30,9,"We built a dataset containing 100 CT scans with fractured pelvis and manually annotated the fractures. A five-fold cross-validation experiment shows that our method outperformed max-flow segmentation and network without distance weighting, achieving a global Dice of 99.38%, a local Dice of 93.79%, and an Hausdorff distance of 17.12 mm. We have made our dataset and source code publicly available and expect them to facilitate further pelvic research, especially reduction planning.

We established a comprehensive pelvic fracture CT dataset and provided ground-truth annotations. Our dataset and source code are publicly available at https://github.com/YzzLiu/FracSegNet. We expect them to facilitate further pelvis-related research, including but not limited to fracture identification, segmentation, and subsequent reduction planning.

Therefore, we curated a dataset of 100 preoperative CT scans covering all common types of pelvic fractures. These data is collected from 100 patients (aged 18–74 years, 41 females) who were to undergo pelvic reduction surgery at Beijing Jishuitan Hospital between 2018 and 2022, under IRB approval (202009-04). The CT scans were acquired on a Toshiba Aquilion scanner. The average voxel spacing is 0.82×0.82×0.94 mm3. The average image shape is 480 × 397 × 310. To generate ground-truth labels for bone fragments, a pre-trained segmentation network was used to create initial segmentations for the ilium and sacrum [13]. Then, these labels were further modified and annotated by two annotators and checked by a senior expert.",None,none,Name of Dataset not Provided
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,1,3,"Experiments on low-dose CT and heart MR datasets.

We conducted experiments on two common image enhancement tasks: denoising and SR. To mimic the real-world setting, the diffusion models were trained on a diverse dataset, including images from different centers and scanners.
The testing set (e.g., MR images) is from a new medical center that has not appeared in the training set. Experiments show that our model can generalize to these unseen images. Specifically, the denoising task is based on the AAPM Low Dose CT Grand Challenge abdominal dataset [19], which can be also used for SR [33]. The heart MR SR task is based on three datasets: ACDC [1], M&Ms1-2 [3], and CMRxMotion [27]. Notably, the presented framework eliminates the requirement of paired data. For the CT image enhancement task, we trained a diffusion model [21] based on the full-dose dataset that contains 5351 images, and the hold-out quarter-dose images were used for testing. For the MR enhancement task, we used the whole ACDC [1] and M&Ms1-2 [3] for training the diffusion model and the CMRxMotion [27] dataset for testing. The testing images were downsampled by operator H with factors of 4× and 8× to produce low-resolution images, and the original images served as the ground truth.

testing dataset, aapm low dose ct grand challenge abdominal dataset, acdc, m&ms1-2, cmrxmotion, full-dose dataset

We also thank the organizers of AAPM Low Dose CT Grand Challenge [20], ACDC [1], M&Ms1-2 [3], and CMRxMothion [27] for making the datasets publicly available.",None,"aapm low dose ct grand challenge abdominal dataset, acdc dataset, m&ms1-2 dataset, cmrxmotion dataset","M&Ms Dataset, ACDC Dataset, CMRxMotion Dataset"
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,55,8,"CoNSeP1 [10] is a colorectal nuclear dataset with three types, consisting of 41H&E stained image tiles from 16 colorectal adenocarcinoma whole-slide images (WSIs). 
BRCA-M2C2 [1] is a breast cancer dataset with three types and consists of 120 image tiles from 113 patients. 
Lizard3 [9] has 291 histology images of colon tissue from six datasets, containing nearly half a million labeled nuclei in H&E stained colon tissue. ",None,"consep1 dataset, brca-m2c2 dataset, lizard3 dataset","BRCA-M2C2 Dataset, CONSEP1 Dataset, LIZARD3 Dataset"
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,33,9,"This dataset consists of supine breast MR images simulating surgical deformations of 11 breasts from 7 healthy volunteers. Volunteers (ages 23–57) were enrolled in a study approved by the Institutional Review Board at Vanderbilt University.

This dataset consists of supine breastMR images simulating surgical deformations from one breast cancer patient. A 71-year-old patient with invasive mammary carcinoma in the left breast was enrolled in a study approved by the Institutional Review Board at Vanderbilt University.",None,none,Name of Dataset not Provided
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,15,8,"We evaluate RDSI using both ex vivo monkey and in vivo human brain MTE data.
We used an ex vivo monkey dMRI dataset[2] collected with a 7T MRI scanner [19].
One healthy subject and three patients with gliomas were scanned using a Philips Ingenia CX 3T MRI scanner with a gradient strength of 80mT/m and switching rates of 200 mT/m/ms.
","Data was collected from an ex vivo fixed Vervet (Chlorocebus aethiops) monkey brain, obtained from the Montreal Monkey Brain Bank. The monkey, cared for on the island of St. Kitts, had been treated in line with a protocol approved by The Caribbean Primate Center of St. Kitts. The brain had previously been stored and prepared according to Dyrby et al. [1]. The data was collected with a Bruker Biospec 70/20 7.0 T scanner (Billerica, Massachusetts, USA) using a quadrature RF coil (300 MHz). The brain was let to reach room temperature and to mechanically stabilize prior the start of the acquisition.","ex vivo monkey dmri dataset,  in vivo human brain mte dataset","In Vivo Human Brain MTE Dataset, Ex Vivo Monkey DMRI Dataset"
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,29,5,"We trained our model using two publicly available brain T1w MRI datasets, including FastMRI+ (131 train, 15 val, 30 test) and IXI (581 train samples), to capture the healthy distribution. Performance evaluation was done on a large stroke T1-weighted MRI dataset, ATLAS v2.0 [14], containing 655 images",None,"fastmri+ dataset,  atlas v2.0 dataset","FastMRI+ Dataset, ATLAS v2.0 Dataset"
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,63,6,"Dataset. For cervical cell detection, our dataset includes 3761 images of 1024× 1024 pixels cropped from WSIs. Our private dataset was collected and qualitycontrolled according to a standard protocol involving three pathologists: A, B, and C. Pathologist A had 33 years of experience in reading cervical cytology images, while pathologists B and C had 10 years of experience each. Initially, the images were randomly assigned to pathologist B or C for initial labeling. Later,the assigned pathologist’s annotations were reviewed and verified by the other pathologist. Any discrepancies found were checked and re-labeled by pathologist A. These images were divided into the training set and the testing set according to the ratio of 9:1. We also collect a new dataset of 5000 positive and negative 224 × 224 cell patches to train the PCN.",None,none,Name of Dataset not Provided
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,13,5,"*Inmetabolite analysis, some studies have shown that the decrease in NAA concentration is related to chronic inflammation, damage, and tumors in the brain [18].

Dataset and Preprocessing: The T2-weighted MR images of 39 participants including 23 patients with NPSLE and 16 HCs were gathered from our affiliated hospital. All images were acquired at an average age of 30.6 years on a SIGNA 3.0T scanner with an eight-channel standard head coil.

",None,none,Name of Dataset not Provided
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,2,7,"We created a synthetic mask dataset that simulates a protuberance, and trained a segmentation network to separate the protruded regions from the normal kidney regions.

The proposed method was evaluated on a publicly available KiTS19 dataset, which contains 108 NCCT images, and showed that our method achieved a higher dice score of 0.615 (+0.097) and sensitivity of 0.721 (+0.103) compared to 3D-UNet.

The release of two public CT image datasets with kidney and tumor masks from the 2019/2021 Kidney and Kidney Tumor Segmentation challenge [8] (KiTS19, KiTS21) attracted researchers to develop various methods for segmentation.

We used a dataset from KiTS19 [8] which contains both CECT and NCCT images. For CECT images, there are 210 images for training and validation and, 90 images for testing. For NCCT images, there are 108 images, which are different series of the 210 images. The ground truth masks are only available for the 210 CECT images. Thus, we transfer the masks to NCCT images. This is achieved by extracting kidney masks and adjusting the height of each kidney.
The ground truth mask contains a kidney label and a kidney tumor label. Cysts are not annotated separately and included in the kidney label on this dataset. The data can be downloaded from The Cancer Imaging Archive (TCIA) [4,9].

During the Step2 phase of the training, where we used the synthetic dataset, we created 10,000 masks using the method from Sect. 3.2. We applied some augmentations during training to input masks to simulate the incoming inputs
from the base network.",None,"kits19 dataset,  kits21 dataset","KiTS21 Dataset, KiTS19 Dataset"
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,49,7,"We evaluated SFT on 11 clinical tasks using a real-world dataset of pNENs and achieved higher performance than other state-ofthe- art multi-label models with mAUCs of 0.68 and 0.76 on internal and external datasets, respectively.

Real-World pNENs Dataset. We validated our method on a real-world pNENs dataset from two centers. All patients with arterial phase Computed Tomography (CT) images were included. The dataset contained 264 and 28 patients in center 1 and center 2, and a senior radiologist annotated the bounding boxes for all 408 and 28 lesions. We extracted 37 labels from clinical reports, including survival, immunohistochemical (IHC), CT findings, etc. Among them, 1)RECIST drug response (RS), 2)tumor shrink (TS), 3)durable clinical benefit (DCB), 4)progression-free survival (PFS), 5)overall survival (OS), 6)grade (GD), 7)somatostatin receptor subtype 2(SSTR2), 8)Vascular Endothelial Growth Factor Receptor 2 (VEFGR2), 9)O6-methylguanine methyltransferase (MGMT), 10)metastatic foci (MTF), and 11)surgical recurrence (RT) are main tasks, and the remaining are auxiliary tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics features of them were extracted, of which 162 features were selected and binarized as auxiliary tasks because of its statistically significant correlation with the main labels. The label distribution and the overlap ratio (Jaccard index) of lesions between pairs of labels are shown in Fig. 3. It is obvious that the real-world dataset has a large number of labels with randomly missing data, thus, we used an adjusted 5-fold cross-validation. Taking a patient as a sample, we chose the dataset from center 1 as the internal dataset, of which the samples with most of the main labels were used as Dataset 1 (219 lesions) and was split into 5 folds, and the remaining samples are randomly divided into the training set Dataset 2 (138 lesions) and the validation set Dataset 3 (51 lesions), the training set and the validation set of the corresponding folds were added during cross-validation, respectively. All samples in Center 2 left as external test set. Details of each dataset are in the Supplementary Material.

Dataset Evaluation Metrics. We evaluate the performance of our method on the 10 main tasks for internal dataset, and due to missing labels and too few SSTR2 labels, only the performance of predicting RT, PFS, OS, GD, MTF are evaluated for external dataset. We employ accuracy (ACC), sensitivity (SEN), specificity (SPC), F1-score (F1) and area under the receiver operating characteristic (AUC) for each task, and compute the mean value of them (e.g. mAUC).",None,pnen dataset,PNEN Dataset
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,3,5,"* The paper does not say whether the dataset is public,  they just referenced to it

This work uses the breast cancer histopathological image database (BreaKHis)1 [20]. This dataset includes four distinct histological types of benign breast tumors: adenosis (A), fibroadenoma (F), phyllodes tumor (PT), and tubular adenona (TA); and four malignant tumors (breast cancer): carcinoma (DC), lobular carcinoma (LC), mucinous carcinoma (MC) and papillary carcinoma (PC). The original dataset is randomly divided into training set and testing set for each magnification at a ratio of 7:3 following previous work
","BreakHis
The BreaKHis database contains microscopic biopsy images of benign and malignant breast tumors. Images were collected through a clinical study from January 2014 to December 2014.
All patients referred to the P&D Laboratory, Brazil, during this period of time, with a clinical indication of BC were invited to participate in the study. The institutional review board approved the study and all patients  gave written informed consent. All the data were anonymized. Samples are generated from breast tissue biopsy slides, stained with hematoxylin and eosin (HE). The samples are collected by SOB, prepared for histological study, and labeled by pathologists of the P&D Lab.",breakhis dataset,BreakHis Dataset
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,54,4,"The proposed method is evaluated on our collected DCE-MRI dataset containing 206 patients with biopsy-proven
breast cancers.

The method obtained a Dice value of 83% using the interval-slice annotation, on a testing dataset containing only 28 patients We evaluate our method on a collected DCE-MRI dataset containing 206 subjects.

Dataset. We evaluated our method on an in-house breast DCE-MRI dataset collected from the Cancer Center of Sun Yat-Sen University. In total, we collected 206 DCE-MRI scans with biopsy-proven breast cancers. All MRI scans were examined with 1.5T MRI scanner. The DCE-MRI sequences (TR/TE = 4.43ms/1.50 ms, and flip angle=10◦) using gadolinium-based contrast agent were performed with the T1-weighted gradient echo technique, and injected 0.2ml/kg intravenously at 2.0 ml/s followed by 20ml saline. The DCE-MRI volumes have two kinds of resolution, 0.379×0.379×1.700 mm3 and 0.511×0.511×1.000 mm3.
All cancerous regions and extreme points were manually annotated by an experienced radiologist via ITK-SNAP [26] and further confirmed by another radiologist. We randomly divided the dataset into 21 scans for training and the
remaining scans for testing1. Before training, we resampled all volumes into the same target spacing 0.600×0.600×1.000 mm3 and normalized all volumes as zero mean and unit variance.",None,none,Name of Dataset not Provided
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,17,8,"Trained within minutes on a single commodity GPU, our model provides realistic super-resolution across different pairs of contrasts in our experiments with three datasets.

We extensively evaluate our method on multiple brain MRI datasets and show that it achieves high visual quality for different contrasts and views and preserves pathological details, highlighting its potential clinical usage.

To enable fair evaluation between our predictions and the reference HR ground truths, the in-plane SNR between the LR input scan and corresponding ground truth has to match. To synthetically create 2D LR images, it is necessary to downsample out-of-plane in the image domain anisotropically [32] while preserving in-plane resolution. Consequently, to mimic realistic 2D clinical protocol, which often has higher in-plane details than that of 3D scans, we use spline interpolation to model partial volume and downsampling.We demonstrate our network’s modeling capabilities for different contrasts (T1w, T2w, FLAIR, DIR), views (axial, coronal, sagittal), and pathologies (MS, brain tumor). We conduct experiments on two public datasets, BraTS [16], and MSSEG [4], and an in-house clinical MS dataset (cMS). In each dataset, we select 25 patients that fulfill the isotropic acquisition criteria for both ground truth HR scans. Note
that we only use the ground truth HR for evaluation, not anywhere in training. We optimize separate INRs for each subject with supervision from only its two LR scans. If required, we employ skull-stripping [12] and rigid registration to the MNI152 (MSSEG, cMS) or SRI24 (BraTS) templates. For details, we refer to Table 2 in the supplementary.
",None,"brats dataset, msseg dataset","BraTS Dataset, MSSEG Dataset"
Skin Lesion Correspondence Localization in Total Body Photography,25,7,"We evaluated the framework quantitatively on both a public and a private dataset, for which our success rates (at 10mm criterion) are comparable to the only reported longitudinal study.

We evaluated our methods on two datasets. The first dataset is from Skin3D [26] (annotated 3DBodyTex [18,19]). The second dataset comes from a 2D Imaging- Rich Total Body Photography system (IRTBP), from which the 3D textured meshes are derived from photogrammetry 3D reconstruction. The number of vertices is on average 300K and 600K for Skin3D and IRTBP datasets respectively. The runtime using 10 iterations is several minutes (on average) on an Intel i7-11857G7 processor. Example data of the two datasets can be found in the supplement.

The framework is evaluated on a private dataset and a public dataset with success rates that are comparable to those of the state-of-the-art method.",None,skin3d dataset,Skin3D Dataset
SLPD: Slide-Level Prototypical Distillation for WSIs,25,1,"We conduct experiments on two public WSI datasets2. TCGANSCLC dataset includes two subtypes in lung cancer, Lung Squamous Cell Carcinoma and Lung Adenocarcinoma, with a total of 1,054 WSIs. TCGA-BRCA dataset includes two subtypes in breast cancer, Invasive Ductal and Invasive
Lobular Carcinoma, with a total of 1,134 WSIs",None,"tcga-nsclc dataset,  tcga-brca dataset ","TCGA-BRCA Dataset, TCGA-NSCLC"
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,50,10,"We evaluate our proposed method on real datasets and the experimental results suggest that it can outperform existing state-of-the-art reconstruction approaches significantly.

To illustrate the efficiency of our proposed approach, we conduct rigorous experiments on several real clinical datasets; the experimental results reveal the advantages of our approach over several state-of-the-art CT reconstruction methods.

First, our proposed approaches are evaluated on the “Mayo-Clinic low-dose CT Grand Challenge” (Mayo-Clinic) dataset of lung CT images [19]. The dataset contains 2250 two dimensional slices from 9 patients for training, and the remaining 128 slices from 1 patient are reserved for testing.  To evaluate the generalization of our model, we also consider another dataset RIDER with nonsmall cell lung cancer under two CT scans [36] for testing. We randomly select 4
patients with 1827 slices from the dataset.

To evaluate the stability and generalization of our model and the baselines trained on Mayo-Clinic dataset, we also test them on the RIDER dataset. The results are shown in Table 2. Due to the bias in the datasets collected from different facilities, the performances of all the models are declined to some extents. But our proposed approach still outperforms the other models for most testing cases.",None,"mayo-clinic low-dose ct grand challenge dataset, rider dataset","Mayo Clinic Low-Dose CT Grand Challenge Dataset, RIDER Dataset"
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,69,1,"* training, testing and valitation set divided by hospital: hospital 1-3 for training, hospital 4 for validation and hospital 5 for test set

We evaluated SAMix on two medical image datasets. Fundus [5,14] is an optic disc and cup segmentation task. Following [21], we consider images collected from different scanners as distinct domains. The source domain contains 400 images of the REFUGE [14] training set.We took 400 images from the REFUGE validation set and 159 images of RIM-One [5] to form the target domain 1 & 2. We center crop and resize the disc region to 256 × 256 as network input. Camelyon [1] is a tumor tissue binary classification task across 5 hospitals. We use the training set of Camelyon as the source domain (302, 436 images from hospitals 1 − 3) and consider the validation set (34, 904 images from hospital 4) and test set (85, 054 images from the hospital 5) as the target domains 1 and 2, respectively. 
",None,"refuge dataset, camelyon dataset, ","Name of Dataset not Provided, REFUGE Dataset, Camelyon Dataset"
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,6,10,"We collected 270 volumetric T1-weighted MRI and 267 thinslice CT head scans with bony reconstruction performed in pediatric patients under routine scanning protocols1. We targeted the age group from 6–24 months since pediatric patients are more susceptible to ionizing radiation and experience a greater cancer risk (up to 24% increase) from radiation exposure [7].

The dataset comprises brain MR and CT volumes from 262 subjects. 13 MRICT volumes from the same patients that were captured less than three months apart are registered using rigid registration algorithms. The dataset is divided into 249, 1 and 12 subjects for training, validating and testing se",None,none,Name of Dataset not Provided
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,8,7,"This study was approved by Institutional Review Board of our cancer institute with a waiver of informed consent.We retrospectively collected 765 patients with breast cancer presenting at our cancer institute from January 2015 to November 2020, all patients had biopsy-proven breast cancers (all cancers included in this study were invasive breast cancers, and ductal carcinoma in situ had been excluded). The MRIs were acquired with Philips Ingenia 3.0-T scanners, and overall, three sequences were present in the in-house dataset, including T1- weighted fat-suppressed MRI, contrast enhanced T1-weighted MRI and DWI

Based on the ratio of 8:2, the training set and independent test set of the in-house dataset have 612 and 153 cases, respectively.",None,none,Name of Dataset not Provided
Text-Guided Foundation Model Adaptation for Pathological Image Classification,27,5,"Through extensive experiments on the PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE achieves leading performance compared with various baselines especially when training data is scarce.

We adopt the PatchGastric [25] dataset, which includes histopathological image patches extracted from H&E stained whole slide images (WSI) of stomach adenocarcinoma endoscopic biopsy specimens. There are 262,777 patches of size 300 × 300 extracted from 991 WSIs at x20 magnification. The dataset contains 9 subtypes of gastric adenocarcinoma. We choose 3 major subtypes including “well differentiated tubular adenocarcinoma”, “moderately differentiated tubular adenocarcinoma”, and “poorly differentiated adenocarcinoma” to form a 3-class grading-like classification task with 179,285 patches from 693 WSIs. We randomly split the WSIs into train (20%) and validation (80%) subsets for measuring the model performance. To extend our evaluation into the real-world setting with insufficient data, we additionally choose 1, 2, 4, 8, or 16 WSIs with the largest numbers of patches from each class as the training set. The evaluation metric is patient-wise accuracy, where the prediction of a WSI is obtained by a soft vote over the patches, and accuracy is averaged class-wise.",None,patchgastric stomach tumor pathological image dataset,PatchGastric Stomach Tumor Pathological Image Dataset
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,16,6,"MSKUS Dataset Collection. The MSKUS data were collected for patientssuspected of metatarsal gout in Nanjing Drum Tower Hosptial. Informed written consent was obtained at the time of recruitment. Dataset totally contains 1127 US images from different patients including 509 gout images and 618 healthy images. The resolution of the MSKUS images were resized to 224 × 224. During experiments, we randomly divided 10% of the dataset into testing sets, then
the remaining data was divided equally into two parts for the different phases of the training. We used 5-fold cross validation to divide the training sets andvalidation sets.

Gaze Data Collection. We collected the eye movement data with the Tobii 4C eye-tracker operating at 90Hz. The MSKUS images were displayed on a 1920 × 1080 27-inch LCD screen. The eye tracker was attached beneath the screen with a magnetic mounting bracket. Sonographers were seated in front of the screen and free to adjust the chair’s height and the display’s inclination.
",None,"mskus dataset, gaze dataset, ","Name of Dataset not Provided, Gaze Dataset, MSKUS Dataset"
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,17,6,"Our dataset contained 282 consecutive patients who underwent thyroid nodule examination at Nanjing Drum Tower Hospital. All patients performed dynamic CEUS examination by an experienced sonographer using an iU22 scanner (Philips Healthcare, Bothell, WA) equipped with a linear transducer L9-3 probe. These 282 cases included 147 malignant nodules and 135 benign nodules. On the one hand, the percutaneous biopsy based pathological examination was implemented to determine the ground-truth of malignant and benign. On the other hand, a sonographer with more than 10 years of experience manually annotated the nodule lesion mask to obtain the pixel-level groundtruth of thyroid nodules segmentation. All data were approved by the Institutional Review Board of Nanjing Drum Tower Hospital, and all patients signed the informed consent before enrollment into the study.",None,none,Name of Dataset not Provided
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,64,9,"We employed the publicly available EASY-RESECT (REtroSpective Evaluation of Cerebral Tumors) dataset [10 (https://archive.sigma2.no/pages/ public/dataset Detail.jsf?id 10.11582/2020.00025) to train and evaluate our proposed method. This dataset is a deep-learning-ready version of the original RESECT database, and was released as part of the 2020 Learn2Reg Challenge [24]. Specifically, EASY-RESECT contains MRI and intra-operative US scans (before resection) of 22 subjects who have undergone low grade glioma resection surgeries. 

To train our DL model, we made subject-wise division of the entire dataset into 70%:15%:15% as the training, validation, and testing sets, respectively.

Table 1 lists the mean and standard deviation of landmark identification errors (in mm) between the predicted position and the ground truth in intra-operative US for each patient of the RESECT dataset.",None,"easy-resect (retrospective evaluation of cerebral tumors) dataset, resect dataset, 2020 learn2reg challenge","RESECT Dataset, Easy-RESECT Dataset"
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,29,10,"To validate our method, six tracked sequences were acquired from an ex vivo swine liver. A manually manipulated IVUS catheter was used (8 Fr lateral firing AcuNavTM 4–10 MHz) connected to an ultrasound system (ACUSON S3000 HELX Touch, Siemens Healthineers, Germany), both commercially available. An electromagnetic tracking system (trakSTARTM, NDI, Canada) was used along with a 6 DoF sensor (Model 130) embedded close to the tip of the catheter, and the PLUS toolkit [17] along with 3D Slicer [18] were used to record the sequences.",None,none,Name of Dataset not Provided
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,59,6,"that the distribution of interaction weights are similar in both domains. We evaluated the performance of our method on the BRCA cohort derived from the Cancer Genome Atlas (TCGA)

Datasets. We conducted our experiments on the breast invasive carcinoma (BRCA) dataset from The Cancer Genome Atlas (TCGA). Specifically, the BRCA dataset includes 661 patients with hematoxylin and eosin (HE)-stained pathological imaging and corresponding survival information. Among the collected BRCA patients in TCGA, the number of ER positive(ER+) and ER negative(ER−) patients are 515 and 146, respectively. We hope to investigate if the proposed T2UDA could be used to help improve the prognosis performance of (ER+) or (ER−) with the aid of the survival information on its counterpart.",None,the cancer genome atlas (tcga),TCGA Dataset
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,32,2,"Extensive experiments are conducted, in which we achieve an overall accuracy of 90.9% on an in-house dataset of four CT phases and seven liver lesion classes. 
The employed single-phase annotated dataset is collected from Sir Run Run Shaw Hospital (SRRSH), affiliated with the Zhejiang University School of Medicine, and has received the ethics approval of IRB. The collection process can be found in supplementary materials.

After the pre-processing unit with window Dice threshold of 0.3, we screen 761 lesions from 444 patients with four phases of CTs, seven types of lesions (13.2% of HCC, 5.3% of HM, 11.3% of ICC, 22.6%of HH, 31.1% of HC, 8.7% of FNH, and 7.8% of HA), and totally 4820 slices. To handle the imbalance of dataset, we randomly select 586 lesions as the training and validation set with no more than 700 axial slices in each lesion type, and the rest 175 lesions constitute the test set. Lesions from the same patient are either assigned to the training and validation set or the test set, but not both.",None,none,Name of Dataset not Provided
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,77,6,"The cohort employed in this study was composed of pre-treatment tumor biopsy specimens from patients with NSCLC from five centers (two centers for training (St) and three centers for independent validation (Sv)). The entire analysis was carried out using 122 patients in Experiment 1 (73 in St, and 49 in Sv) and 135 patients in Experiment 2 (81 in St, and 54 in Sv). Specimens were analyzed with a multiplexed quantitative immunofluorescence (QIF) panel using the method described in [22]. From each whole slide image, 7 representative tiles were obtained and used to train the software InForm to define background, tumor and stromal compartments. Then, individual cells were segmented based on nuclear DAPI staining and the segmentation performance was controlled by direct visualization of samples by a trained observer. Next, the software was trained to identify cell subtypes based on marker expression (CD8, CD4, CD20, CK for tumor epithelial cells and absence of these markers for stromal cells).

NSCLC: non-small cell lung cancer",None,none,Name of Dataset not Provided
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,4,4,"The experiments on the public datasets demonstrate that our UML outperforms existing methods in terms of both accuracy and robustness.

We evaluate the our UML network on two datasets REFUGE [14] and ISPY-1 [13]. REFUGE contains two tasks, classification of glaucoma and segmentation of optic disc/cup in fundus images. The overall 1200 images were equally divided for training, validation, and testing. The tasks of ISPY-1 are the pCR prediction and the breast tumor segmentation. A total of 157 patients who suffer the breast cancer are considered - 43 achieve pCR and 114 non-pCR. 

","REFUGE dataset
The REFUGE challenge database consists of 1200 retinal CFPs stored in JPEG format, with 8 bits per color channel, acquired by ophthalmologists or technicians from patients sitting upright and using one of two devices- These pictures correspond to Chinese patients (52% and 55% female in offline and online test sets, respectively) visiting eye clinics, and were retrieved retrospectively from multiple sources, including several hospitals and clinical studies. Only high-quality images were selected to ensure a proper labelling, and any personal and/or device information was removed for anonymization.

ISPY-1 dataset
Clinical and MRI data from the ISPY1 clinical trial of patients with breast cancer. 
Clinical Data as a XLS file with the following fields:

Age (Years)
Race, encoded as: 1 = Caucasian,  3 = African American, 4 = Asian, 5 = Native Hawaiian, 6 = American Indian, 50 = Multiple race","refuge dataset,  ispy-1 dataset","REFUGE Dataset, ISPY-1 Dataset"
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,58,3,"*note: paper refs to wrong dataset (look further down)
** 4 datasets, since one dataset is collected from two public datasets
We used the public COVID-19 segmentation benchmark [15] to verify the proposed UCI. It is collected from two public resources [5,8] on chest CT images available on The Cancer Imaging Archive (TCIA) [4]. All CT images were acquired without intravenous contrast enhancement from patients with positive Reverse Transcription Polymerase Chain Reaction (RT-PCR) for SARSCoV-2. In total, we used 199 CT images including 149 training images and 50 test images. We also used two chest x-ray-based classification datasets including ChestX-ray14 [18] and ChestXR [1] to assist the UCI training. The ChestXray14 dataset comprises 112,120 X-ray images showing positive cases from 30,805 patients, encompassing 14 disease image labels pertaining to thoracic and lung ailments. An image may contain multiple or no labels. The ChestXR dataset consists of 21,390 samples, with each sample classified as healthy, pneumonia, or COVID-19.

","Two public resources[5,8]
Cinical data: age, sex, race, zip, weight.
The average age in the cohort was 54.3 years old (range 19–91) with an even sex distribution (52 Male, 53 Female). The worldwide incidence is reported to be close to 1:1, with a 50% increase in hospitalizations, ICU stays, and mortality among males10. The racial characteristics of the cohort are presented in Fig. 1 in comparison to the total population of Arkansas and the current characteristics of the state-wide infected population. The average BMI in the cohort is 33.1 (18.7–64.9), well within obese range (30.0 or higher). Key Comorbidities include burns (2%), malnutrition (3%), pregnancy (4%), chronic kidney disease (11%), diabetes (21%), organ transplant (3%), and cancer (24%).
The overall ICU admission rate was 28% (29/105). The Average age among those admitted to the ICU was 58 (range 25–89), slightly higher than the average for the cohort as a whole. The racial breakdown of ICU admissions included 28% of the white patients, 25% of the black, 50% of other, and 100% of Pacific islanders. The ICU population was 66% male and 33% female and included 1 pregnant patient. Forty three percent of patients admitted to the ICU had BMI greater than 30. Of the black patients, 21% had diabetes and 29% chronic renal disease, while among white patients the highest percentage comorbidities were cancer (10%) and diabetes (10%). The ICU mortality rate was 34.4% (10/29) which is 1.5 times the national average of 23.6%11. The overall mortality rate was 10% (10/105) and all 10 patients in the mortality group were first admitted to ICU. Arkansan males were 1.95 times more likely to go to ICU (19/52 vs. 10/53). Our data shows an almost even overall mortality rate among males (5/53) and females (5/52). The data also suggest that once in the ICU, female mortality occurs at a rate 1.9 times that of males (5/19 vs. 5/10). 
COVID-19-AR image collection: Image data were extracted from the clinical PACS (Sectra AB, Linkoping, Sweden) at the University of Arkansas for Medical Sciences using Digital Imaging and Communications in Medicine (DICOM) query/retrieve software. All image data were de-identified and stored in DICOM standard format17 on TCIA as collection COVID-19-AR18. All clinical data were obtained from the Arkansas Clinical Data Repository (AR-CDR)19. The AR-CDR is a research data warehouse that provides a single and secure source of data for use in clinical and translational research; housing data for this project that are extracted from the EPIC (Epic Systems Inc, Verona, WI) electronic health record (EHR) system and legacy systems.

https://www.nature.com/articles/s41467-020-17971-2/tables/1
Patient cohorts in model development and testing: demographic values, center/location, age, gender, demographic distribution pr training, validation and testing 

TCIA
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3824915/
ChestX-ray14 and ChestXR
ChestX-ray14 refs to Chestx-ray8 in the paper and not ray14. Ray8 contains (taken from ref): In this paper, we present a new chest X-ray database, namely “ChestX-ray8”, which comprises 108,948 frontalview X-ray images of 32,717 unique patients with the textmined eight disease image labels (where each image can have multi-labels), from the associated radiological reports
using natural language processing

The link to ray 14 describes the dataset as: ChestX-ray14 is a medical imaging dataset which comprises 112,120 frontal-view X-ray images of 30,805 (collected from the year of 1992 to 2015) unique patients with the text-mined fourteen common disease labels, mined from the text radiological reports via NLP techniques. It expands on ChestX-ray8 by adding six additional thorax diseases: Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia.
ChestXR "," covid-19 segmentation benchmark, the cancer imaging archive (tcia),  chestx-ray14 dataset, chestxr dataset, chestxray14 dataset ","NIH ChestXray-14 Dataset, COVID-19 Segmentation Benchmark, ChestXR Dataset, TCIA Dataset"
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,7,1,"We trained our networks using a subset of the open-access IntrA dataset1 published by Yang et al. in 2020 [32]. This subset consisted of 1694 healthy vessel segments reconstructed from 2D MRA images of patients. We converted 3D meshes into a binary tree representation and used the network extraction script from the VMTK toolkit2 to extract the centerline coordinates of each vessel model. The centerline points were determined based on the ratio between the sphere step and the local maximum radius, which was computed using the advancement ratio specified by the user. The radius of the blood vessel conduit at each centerline sample was determined using the computed crosssections assuming a maximal circular shape (See Fig. 2). To improve computational efficiency during recursive tree traversal, we implemented an algorithm that balances each tree by identifying a new root. We additionally trimmed trees to a depth of ten in our experiments. This decision reflects a balance between the computational demands of depth-first tree traversal in each training step and the complexity of the training meshes. ",None,intra dataset,Intra Dataset
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,66,10,"Manifold Learning. We trained our model with a large dataset of 3500 CTs of patients with head-and-neck cancer, more exactly 2297 patients from the publicly available The Cancer Imaging Archive (TCIA) [1,6,16,17,28,32] and 1203 from private internal data, after obtention of ethical approbations. We split this data into 3000 cases for training, 250 for validation, and 250 for testing. We focused CT scans on the head and neck region above shoulders, with a resolution of 80×96×112, and centered on the mouth after automatic segmentation using a pre-trained U-Net [22]. The CTs were preprocessed by min-max normalization after clipping between -1024 and 2000 Hounsfield Units (HU).

3D Reconstruction. To evaluate our approach, we used an external private cohort of 80 patients who had undergone radiotherapy for head-and-neck cancer, with their consent. Planning CT scans were obtained for dose preparation, and CBCT scans were obtained at each treatment fraction for positioning with full gantry acquisition. As can be seen in Fig. 3 and the supplementary material, all these cases are challenging as there are large changes between the original CT scan and the CBCT scans. We identified these cases automatically by comparing the CBCTs with the planning CTs",None,the cancer imaging archive (tcia),TCIA Dataset
