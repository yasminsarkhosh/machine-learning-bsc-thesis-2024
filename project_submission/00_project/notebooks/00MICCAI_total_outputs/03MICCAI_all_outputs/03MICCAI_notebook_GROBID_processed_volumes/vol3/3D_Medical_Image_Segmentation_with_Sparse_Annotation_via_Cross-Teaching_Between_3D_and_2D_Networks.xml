<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks</title>
				<funder ref="#_yPE9auY">
					<orgName type="full">Science</orgName>
				</funder>
				<funder ref="#_uj3GDdd">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_bAvMnCz">
					<orgName type="full">Jiangsu NSF</orgName>
				</funder>
				<funder ref="#_X2PQbzd #_sWvfhpx #_rNyqtjd">
					<orgName type="full">NSFC Program</orgName>
				</funder>
				<funder ref="#_XbHTQT7">
					<orgName type="full">CCF-Lenovo Bule Ocean Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Heng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Novel Software Technology</orgName>
								<orgName type="institution" key="instit1">National Institute of Health-care Data Science</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Qi</surname></persName>
							<email>qilei@seu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Yu</surname></persName>
							<email>yuqian@sdwu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Shandong Women&apos;s University</orgName>
								<address>
									<settlement>Jinan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Novel Software Technology</orgName>
								<orgName type="institution" key="instit1">National Institute of Health-care Data Science</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
							<email>gaoy@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Novel Software Technology</orgName>
								<orgName type="institution" key="instit1">National Institute of Health-care Data Science</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="614" to="624"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">D285E5B267527CFD32537680458FF464</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_59</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D segmentation</term>
					<term>Sparse annotation</term>
					<term>Cross-teaching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical image segmentation typically necessitates a large and precisely annotated dataset. However, obtaining pixel-wise annotation is a labor-intensive task that requires significant effort from domain experts, making it challenging to obtain in practical clinical scenarios. In such situations, reducing the amount of annotation required is a more practical approach. One feasible direction is sparse annotation, which involves annotating only a few slices, and has several advantages over traditional weak annotation methods such as bounding boxes and scribbles, as it preserves exact boundaries. However, learning from sparse annotation is challenging due to the scarcity of supervision signals. To address this issue, we propose a framework that can robustly learn from sparse annotation using the cross-teaching of both 3D and 2D networks. Considering the characteristic of these networks, we develop two pseudo label selection strategies, which are hard-soft confidence threshold and consistent label fusion. Our experimental results on the MMWHS dataset demonstrate that our method outperforms the state-of-the-art (SOTA) semi-supervised segmentation methods. Moreover, our approach achieves results that are comparable to the fully-supervised upper bound result. Our code is available at https://github.com/HengCai-NJU/3D2DCT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation is greatly helpful to diagnosis and auxiliary treatment of diseases. Recently, deep learning methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref> has largely improved the performance of segmentation. However, the success of deep learning methods typically relies on large densely annotated datasets, which require great efforts from domain experts and thus are hard to obtain in clinical applications.</p><p>To this end, many weakly-supervised segmentation (WSS) methods are developed to alleviate the annotation burden, including image level <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, bounding box <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref>, scribble <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> and even points <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>. These methods utilize weak label as supervision signal to train the model and produce segmentation results. Unfortunately, the performance gap between these methods and its corresponding upper bound (i.e., the result of fully-supervised methods) is still large. The main reason is that these annotation methods do not provide the information of object boundaries, which are crucial for segmentation task.</p><p>A new annotation strategy has been proposed and investigated recently. It is typically referred as sparse annotation and it only requires a few slice of each volume to be labeled. With this annotation way, the exact boundaries of different classes are precisely kept. It shows great potential in reducing the amount of annotation. And its advantage over traditional weak annotations has been validated in previous work <ref type="bibr" target="#b1">[2]</ref>. To enlarge the slice difference, we annotate slices from two different planes instead of from a single plane.</p><p>Most existing methods solve the problem by generating pseudo label through registration. <ref type="bibr" target="#b1">[2]</ref> trains the segmentation model through an iterative step between propagating pseudo label and updating segmentation model. <ref type="bibr" target="#b10">[11]</ref> adopts meanteacher framework as segmentation model and utilizes registration module to produce pseudo label. <ref type="bibr" target="#b2">[3]</ref> proposes a co-training framework to leverage the dense pseudo label and sparse orthogonal annotation. Though achieving remarkable results, the limitation of these methods cannot be ignored. These methods rely heavily on the quality of registration result. When the registration suffers due to many reasons (e.g., small and intricate objects, large variance between adjacent slices), the performance of segmentation models will be largely degraded.</p><p>Thus, we suggest to view this problem from the perspective of semisupervised segmentation (SSS). Traditional setting of 3D SSS is that there are several volumes with dense annotation and a large number of volumes without any annotation. And now there are voxels with annotation and voxels without annotation in every volume. This actually complies with the idea of SSS, as long as we view the labeled and unlabeled voxels as labeled and unlabeled samples, respectively.</p><p>SSS methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref> mostly fall into two categories, 1) entropy minimization and 2) consistency regularization. And one of the most popular paradigms is co-training <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. Inspired by these co-training methods, we propose our method based on the idea of cross-teaching. As co-training theory conveys, the success of co-training largely lies on the view-difference of different networks <ref type="bibr" target="#b16">[17]</ref>. Some works encourage the difference by applying different transformation to each network. <ref type="bibr" target="#b13">[14]</ref> directly uses two type of networks (i.e., CNN and transformer) to guarantee the difference. Here we further extend it by adopting networks of different dimensions (i.e., 2D CNN and 3D CNN). 3D network and 2D network work largely differently for 3D network involves the inter-slice information while 2D network only utilize inner-slice information. The 3D network is trained on volume with sparse annotation and we use two 2D networks to learn from slices of two different planes. Thus, the view difference can be well-preserved. However, it is still hard to directly train with the sparse annotation due to limited supervision signal. So we utilize 3D and 2D networks to produce pseudo label to each other. In order to select more credible pseudo label, we specially propose two strategies for the pseudo label selection of 3D network and 2D networks, respectively. For 3D network, simply setting a prediction probability threshold can exclude those voxels with less confidence, which are more likely to be false prediction. However, Some predictions with high quality but low confidence are also excluded. Thus, we estimate the quality of each prediction, and design hard-soft thresholds. If the prediction is of high quality, the voxels that overpass the soft threshold are selected as pseudo label. Otherwise, only the voxels overpassing the hard threshold can be used to supervise 2D networks. For 2D networks, compared with calculating uncertainty which introduces extra computation cost, we simply use the consistent prediction of two 2D networks. As the two networks are trained on slices of different planes, thus their consistent predictions are very likely to be correct. We validate our method on the MMWHS <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> dataset, and the results show that our method is superior to SOTA semi-supervised segmentation methods in solving sparse annotation problem. Also, our method only uses 16% of labeled slices but achieves comparable results to the fully supervised method.</p><p>To sum up, our contributions are three folds:</p><p>-A new perspective of solving sparse annotation problem, which is more versatile compared with recent methods using registration. -A novel cross-teaching paradigm which imposes consistency on the prediction of 3D and 2D networks. Our method enlarges the view difference of networks and boosts the performance. -A pseudo label selection strategy discriminating between reliable and unreliable predictions, which excludes error-prone voxels while keeping credible voxels though with low confidence. 2 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cross Annotation</head><p>Recent sparse annotation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> only label one slice for each volume, however, this annotation has many limitations. 1) The segmentation object must be visible on the labeled slice. Unfortunately, in most cases, the segmentation classes cannot be all visible in a single slice, especially in multi-class segmentation tasks. 2) Even though there is only one class and is visible in the labeled slice, the variance between slices might be large, and thus the information provided by a single slice is not enough to train a well-performed segmentation model. Based on these two observations, we label multiple slices for each volume. Empirically, the selection of slices should follow the rule that they should be as variant as possible in order to provide more information and have a broader coverage of the whole data distribution. Thus, we label slices from two planes (e.g., transverse plane and coronal plane) because the difference involved by planes is larger than that involved by slice position on a single plane. The annotation looks like crosses from the third plane, so we name it Cross Annotation. The illustration of cross annotation is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Furthermore, in order to make the slices as variant as possible, we simply select those slices with a same distance. And the distance is set according to the dataset. For example, the distance can be large for easy segmentation task with lots of volumes. Otherwise, the distance should be closer for difficult task or with less volumes. And here we provide a simple strategy to determine the distance (Fig. <ref type="figure" target="#fig_1">2</ref>). First label one slice for each plane in a volume, and train the model to monitor its performance on validation set, which has ground truth dense annotation. Then halve the distance (i.e., double the labeling slice), and test the trained model on validation set again. The performance gain can be calculated. Then repeat the procedure until the performance gain is less than half of the previous gain. The current distance is the final distance. The performance gain is low by labeling more slices.</p><p>The aim of the task is to train a segmentation model on dataset D consists of L volumes X 1 , X 2 , ..., X L with cross annotation Y 1 , Y 2 , ..., Y L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">3D-2D Cross Teaching</head><p>Our framework consists of three networks, which are a 3D networks and two 2D networks. We leverage the unlabeled voxels through the cross teaching between 3D network and 2D networks. Specifically, the 3D network is trained on volumes and the 2D networks are trained with slices on transverse plane and coronal plane, respectively. The difference between 3D and 2D network is inherently in their network structure, and the difference between 2D networks comes from the different plane slices used to train the networks.</p><p>For each sample, 3D network directly use it as input. Then it is cut into slices from two directions, resulting in transverse and coronal plane slices, which are used to train the 2D networks. And the prediction of each network, which is denoted as P , is used as pseudo label for the other network after selection. The selection strategy is detailedly introduced in the following part.</p><p>To increase supervision signal for each training sample, we mix the selected pseudo label and ground truth sparse annotation together for supervision. And it is formulated as:</p><formula xml:id="formula_0">Ŷ = MIX(Y, P ),<label>(1)</label></formula><p>where MIX(•, •) is a function that replaces the label in P with the label in Y for those voxels with ground truth annotation.</p><p>Considering that the performance of 3D network is typically superior to 2D networks, we further introduce a label correction strategy. If the prediction of 3D network and the pseudo label from 2D networks differ, no loss on that particular voxel should be calculated as long as the confidence of 3D networks is higher than both 2D networks. We use M to indicate how much a voxel contribute to the loss calculation, and the value of position i is 0 if the loss of voxel i should not be calculated, otherwise 1 for ground truth annotation and w for pseudo label, where w is a value increasing from 0 to 0.1 according to ramp-up from <ref type="bibr" target="#b7">[8]</ref>.</p><p>The total loss consists of cross-entropy loss and dice loss:</p><formula xml:id="formula_1">L ce = - 1 H×W ×D i=1 m i H×W ×D i=1 m i y i log p i ,<label>(2)</label></formula><p>and</p><formula xml:id="formula_2">L dice = 1 - 2 × H×W ×D i=1 m i p i y i H×W ×D i=1 m i (p 2 i + y 2 i ) , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where p i , y i is the output and the label in Ŷ of voxel i, respectively. m i is the value of M at position i. H, W, D denote the height, width and depth of the input volume, respectively. And the total loss is denoted as:</p><formula xml:id="formula_4">L = 1 2 L ce + 1 2 L dice . (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pseudo Label Selection</head><p>Hard-Soft Confidence Threshold. Due to the limitation of supervision signal, the prediction of 3D model has lots of noisy label. If it is directly used as pseudo label for 2D networks, it will cause a performance degradation on 2D networks. So we set a confidence threshold to select voxels which are more likely to be correct. However, we find that this may also filter out correct prediction with lower confidence, which causes the waste of useful information. If we know the quality of the prediction, we can set a lower confidence threshold for the voxels in the prediction of high-quality in order to utilize more voxels. However, the real accuracy R acc of prediction is unknown, for the dense annotation is unavailable during training. What we can obtain is the pseudo accuracy P acc calculated with the prediction and the sparse annotation. And we find that R acc and P acc are completely related on the training samples. Thus, it is reasonable to estimate R acc using P acc :</p><formula xml:id="formula_5">R acc ≈ P acc = H×W ×D i=1 I(p i = y i )/(H × W × D),<label>(5)</label></formula><p>where I(•) is the indicator function and pi is the one-hot prediction of voxel i. Now we introduce our hard-soft confidence threshold strategy to select from 3D prediction. We divide all prediction into reliable prediction (i.e., with higher P acc ) and unreliable prediction (i.e., with lower P acc ) according to threshold t q . And we set different confidence thresholds for these two types of prediction, which are soft threshold t s with lower value and hard threshold t h with higher value. In reliable prediction, voxels with confidence higher than soft threshold can be selected as pseudo label. The soft threshold aims to keep the less confident voxels in reliable prediction and filter out those extremely uncertain voxels to reduce the influence of false supervision. And in unreliable prediction, only those voxels with confidence higher than hard threshold can be selected as pseudo label. The hard threshold is set to choose high-quality voxels from unreliable prediction. The hard-soft confidence threshold strategy achieves a balance between increasing supervision signals and reducing label noise.</p><p>Consistent Prediction Fusion. Considering that 2D networks are not able to utilize inter-slice information, their performance is typically inferior to that of 3D network. Simply setting threshold or calculating uncertainty is either of limited use or involving large extra calculation cost. To this end, we provide a selection strategy which is useful and introduces no additional calculation. The 2D networks are trained on slices from different planes and they learn different  patterns to distinguish foreground from background. So they will produce predictions with large diversity for a same input sample and the consensus of the two networks are quite possible to be correct. Thus, we use the consistent part of prediction from the two networks as pseudo label for 3D network.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>MMWHS Dataset. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> is from the MICCAI 2017 challenge, which consists of 20 cardiac CT images with publicly accessible annotations that cover seven whole heart substructures. We split the 20 volumes into 12 for training, 4 for validation and 4 for testing. And we normalize all volumes through z-score normalization. All volumes are reshaped to <ref type="bibr">[192,</ref><ref type="bibr">192,</ref><ref type="bibr">96]</ref> with linear interpolation.</p><p>Implementation Details. We adopt Adam <ref type="bibr" target="#b5">[6]</ref> with a base learning rate of 0.001 as optimizer and the weight decay is 0.0001. Batch size is 1 and training iteration is 6000. We adopt random crop as data augmentation strategy and the patch size is <ref type="bibr">[176,</ref><ref type="bibr">176,</ref><ref type="bibr">96]</ref>. And the hyper-parameters are t q = 0.98, t h = 0.9, t s = 0.7 according to experiments on validation set. For 3D and 2D networks, we use V-Net <ref type="bibr" target="#b14">[15]</ref> and U-Net <ref type="bibr" target="#b17">[18]</ref> as backbone, respectively. All experiments are conducted using PyTorch and 3 NVIDIA GeForce RTX 3090 GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with SOTA Methods</head><p>As previous sparse annotation works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> cannot leverage sparse annotation where there are more than one labeled slice in a volume, to verify the effectiveness of our method, we compare it with SOTA semi-supervised segmentation methods, including Mean Teacher (MT) <ref type="bibr" target="#b20">[21]</ref>, Uncertainty-aware Mean-Teacher (UAMT) <ref type="bibr" target="#b24">[25]</ref>, Cross-Pseudo Supervision (CPS) <ref type="bibr" target="#b3">[4]</ref> and Cross Teaching Between CNN and Transformer (CTBCT) <ref type="bibr" target="#b13">[14]</ref>. The transformer network in CTBCT is implemented as UNETR <ref type="bibr" target="#b4">[5]</ref>. Our method uses the prediction of 3D network as result. For fairer comparisons, all experiments are implemented in 3D manners with the same setting. For the evaluation and comparisons of our method and other methods, we use Dice coefficient, Jaccard coefficient, 95% Hausdorff Distance (HD) and Average Surface Distance (ASD) as quantitative evaluation metrics. The results are required through three runs with different random dataset split and they are reported as mean value ± standard deviation.</p><p>The quantitative results and qualitative results are shown in Table <ref type="table" target="#tab_0">1</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We also investigate how hyper-parameters t q , t h and t s affect the performance of the method. We conduct quantitative ablation study on the validation set.</p><p>The results are shown in Table <ref type="table" target="#tab_1">2</ref>. Bold font presents best results and underline presents the second best. Both {1,2,3} and {4,5,6} show that t s = 0.7 obtain the best performance. The result complies with our previous analysis. When t s is too high, correctly predicted voxels in reliable prediction are wasted. And when t s is low, predictions with extreme low confidence are selected as pseudo label, which introduces much noise to the cross-teaching. Setting t q = 0.98 performs better than setting t q = 0.95, and it indicates the criterion of selecting reliable prediction cannot be too loose. The result of hyper-parameters set 7 shows that when we set hard and soft thresholds equally low, the performance is largely degraded, and it validates the effectiveness of our hard-soft threshold strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we extend sparse annotation to cross annotation to suit more general real clinical scenario. We label slices from two planes and it enlarges the diversity of annotation. To better leverage the cross annotation, we view the problem from the perspective of semi-supervised segmentation and we propose a novel cross-teaching paradigm which imposes consistency on the prediction of 3D and 2D networks. Furthermore, to achieve robust cross-supervision, we propose new strategies to select credible pseudo label, which are hard-soft threshold for 3D network and consistent prediction fusion for 2D networks. And the result on MMWHS dataset validates the effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of cross annotaion. (a) and (b) are typical annotations of transverse plane slices; (c) and (d) are typical annotations of coronal plane slices; (e) is a typical annotation of saggital plane slices and (f) is the 3D view of cross annotation.</figDesc><graphic coords="3,48,30,54,62,327,88,62,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Overview of the proposed 3D-2D cross-teaching framework. For a volume with cross annotation, 3D network and 2D networks give predictions of it. We use hardsoft threshold and consistent prediction fusion to select credible pseudo label. Then the pseudo label is mixed with ground truth sparse annotation to supervise other networks.</figDesc><graphic coords="4,56,46,54,56,339,28,144,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual examples of segmentation results on MMWHS dataset.</figDesc><graphic coords="7,58,29,160,28,307,69,103,03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison result on MMWHS dataset.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell cols="2">Labeled slices/volume Metrics</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dice (%)↑</cell><cell cols="2">Jaccard(%)↑ HD (voxel)↓ ASD (voxel)↓</cell></row><row><cell>Semi-supervised MT [21]</cell><cell>NIPS'17</cell><cell>16</cell><cell cols="3">76.25±4.63 64.89±4.59 19.40±9.63 5.65±2.28</cell></row><row><cell cols="3">UAMT [25] MICCAI'19 16</cell><cell cols="3">72.19±11.69 61.54±11.46 18.39±8.16 4.97±2.30</cell></row><row><cell>CPS [4]</cell><cell>CVPR'21</cell><cell>16</cell><cell cols="3">77.19±7.04 66.96±5.90 13.10±3.60 4.00±2.06</cell></row><row><cell cols="2">CTBCT [14] MIDL'22</cell><cell>16</cell><cell cols="3">74.20±6.04 63.04±5.36 17.91±3.96 5.15±1.38</cell></row><row><cell>Ours</cell><cell cols="2">this paper 16</cell><cell cols="3">82.67±4.99 72.71±5.51 12.81±0.74 3.72±0.43</cell></row><row><cell>Fully-supervised V-Net [15]</cell><cell>3DV'16</cell><cell>96</cell><cell cols="3">81.69±4.93 71.36±6.40 16.15±3.13 5.01±1.29</cell></row><row><cell>MT</cell><cell>UAMT</cell><cell>CPS</cell><cell>CTBCT</cell><cell>Ours</cell><cell>GT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on hyper-parameters.</figDesc><table><row><cell cols="2">Parameters set tq</cell><cell>th</cell><cell>ts</cell><cell>Metrics</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dice (%)↑</cell><cell>Jaccard(%)↑ HD (voxel)↓ ASD (voxel)↓</cell></row><row><cell>1</cell><cell cols="4">0.98 0.90 0.85 82.25±9.00 72.76±10.73 9.66±5.22</cell><cell>3.08±1.69</cell></row><row><cell>2</cell><cell cols="4">0.98 0.90 0.70 83.64±9.39 74.63±11.10 8.60 ± 4.75 2.77 ± 1.71</cell></row><row><cell>3</cell><cell cols="4">0.98 0.90 0.50 82.60 ± 9.50 73.40 ± 11.21 10.74±6.03 3.40±1.95</cell></row><row><cell>4</cell><cell cols="4">0.95 0.90 0.85 81.08±9.93 71.98±11.52 10.77±5.86 3.42±1.95</cell></row><row><cell>5</cell><cell cols="4">0.95 0.90 0.70 82.22±9.93 73.31±11.72 7.69±3.75 2.60±1.40</cell></row><row><cell>6</cell><cell cols="4">0.95 0.90 0.50 82.17±10.99 73.12±12.42 8.88±4.53</cell><cell>3.10±1.77</cell></row><row><cell>7</cell><cell>-</cell><cell cols="3">0.50 0.50 80.82±11.60 71.22±13.26 11.17±6.85 3.58±2.11</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by the <rs type="funder">Science</rs> and <rs type="projectName">Technology Innovation 2030 New Generation Artificial Intelligence Major Projects</rs> (<rs type="grantNumber">2021ZD0113303</rs>), <rs type="funder">NSFC Program</rs> (<rs type="grantNumber">62222604</rs>, <rs type="grantNumber">62206052</rs>, <rs type="grantNumber">62192783</rs>), <rs type="funder">China Postdoctoral Science Foundation</rs> Project (<rs type="grantNumber">2021M690609</rs>), <rs type="funder">Jiangsu NSF</rs> Project (<rs type="grantNumber">BK20210224</rs>), <rs type="person">Shandong NSF</rs> (<rs type="grantNumber">ZR2023MF037</rs>) and <rs type="funder">CCF-Lenovo Bule Ocean Research Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_yPE9auY">
					<idno type="grant-number">2021ZD0113303</idno>
					<orgName type="project" subtype="full">Technology Innovation 2030 New Generation Artificial Intelligence Major Projects</orgName>
				</org>
				<org type="funding" xml:id="_X2PQbzd">
					<idno type="grant-number">62222604</idno>
				</org>
				<org type="funding" xml:id="_sWvfhpx">
					<idno type="grant-number">62206052</idno>
				</org>
				<org type="funding" xml:id="_rNyqtjd">
					<idno type="grant-number">62192783</idno>
				</org>
				<org type="funding" xml:id="_uj3GDdd">
					<idno type="grant-number">2021M690609</idno>
				</org>
				<org type="funding" xml:id="_bAvMnCz">
					<idno type="grant-number">BK20210224</idno>
				</org>
				<org type="funding" xml:id="_XbHTQT7">
					<idno type="grant-number">ZR2023MF037</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What&apos;s the point: semantic segmentation with point supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46478-7_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46478-7_34" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d image segmentation with sparse annotation by self-training and internal registration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bitarafan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Baghshah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2665" to="2672" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Orthogonal annotation benefits barely-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3302" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unetr: transformers for 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Box2Seg: attention weighted loss and discriminative feature learning for weakly supervised segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58583-9_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58583-9_18" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12372</biblScope>
			<biblScope unit="page" from="290" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ficklenet: weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5267" to="5276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anti-adversarially manipulated attributions for weakly and semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4071" to="4080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PLN: parasitic-like network for barely supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="582" to="593" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for panoptic segmentation with pointbased supervision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="4552" to="4568" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scribblesup: scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation via cross teaching between cnn and transformer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="820" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Background-aware pooling and noise-aware loss for weakly-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6913" to="6922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep co-training for semisupervised image recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="135" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inconsistency-aware uncertainty estimation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="608" to="620" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Box-driven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101766</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scribble-supervised semantic segmentation inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15354" to="15363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting weak-to-strong consistency in semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7236" to="7246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_67" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised 3d abdominal multi-organ segmentation via deep multi-planar co-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="121" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multivariate mixture model for myocardial segmentation combining multi-source images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2933" to="2946" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-scale patch and multi-modality atlases for whole heart segmentation of mri</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
