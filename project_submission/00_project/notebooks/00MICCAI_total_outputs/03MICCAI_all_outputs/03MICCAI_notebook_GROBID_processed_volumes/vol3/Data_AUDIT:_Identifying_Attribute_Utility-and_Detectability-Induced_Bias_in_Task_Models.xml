<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models</title>
				<funder>
					<orgName type="full">U.S. Food and Drug Administration</orgName>
				</funder>
				<funder ref="#_xz3Gpja">
					<orgName type="full">Oak Ridge Institute for Science and Education through an interagency agreement between the U.S. Department of Energy</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mitchell</forename><surname>Pavlak</surname></persName>
							<email>mpavlakl@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Devices and Radiological Health</orgName>
								<orgName type="institution">U.S. Food and Drug Administration</orgName>
								<address>
									<settlement>Silver Spring</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Drenkow</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Petrick</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Devices and Radiological Health</orgName>
								<orgName type="institution">U.S. Food and Drug Administration</orgName>
								<address>
									<settlement>Silver Spring</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Mehdi</forename><surname>Farhangi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Devices and Radiological Health</orgName>
								<orgName type="institution">U.S. Food and Drug Administration</orgName>
								<address>
									<settlement>Silver Spring</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><surname>Unberath</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B1FABCDC87C2FF2E183A2E57E8FB6E0E</idno>
					<idno type="DOI">10.1007/978-3-031-43898-143.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bias</term>
					<term>shortcut learning</term>
					<term>fairness</term>
					<term>algorithmic auditing</term>
					<term>datasets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To safely deploy deep learning-based computer vision models for computer-aided detection and diagnosis, we must ensure that they are robust and reliable. Towards that goal, algorithmic auditing has received substantial attention. To guide their audit procedures, existing methods rely on heuristic approaches or high-level objectives (e.g., non-discrimination in regards to protected attributes, such as sex, gender, or race). However, algorithms may show bias with respect to various attributes beyond the more obvious ones, and integrity issues related to these more subtle attributes can have serious consequences. To enable the generation of actionable, data-driven hypotheses which identify specific dataset attributes likely to induce model bias, we contribute a first technique for the rigorous, quantitative screening of medical image datasets. Drawing from literature in the causal inference and information theory domains, our procedure decomposes the risks associated with dataset attributes in terms of their detectability and utility (defined as the amount of information the attribute gives about a task label). To demonstrate the effectiveness and sensitivity of our method, we develop a variety of datasets with synthetically inserted artifacts with different degrees of association to the target label that allow evaluation of inherited model biases via comparison of performance against true counterfactual examples. Using these datasets and results from hundreds of trained models, we show our screening method reliably identifies nearly imperceptible bias-inducing artifacts. Lastly, we apply our method to the natural attributes of a popular skin-lesion dataset and demonstrate its success. Our approach provides a means to perform more systematic algorithmic audits and guide future data collection efforts in pursuit of safer and more reliable models. Full code is available at https://github. com/mpavlak25/data-audit.</p><p>M. Pavlak and N. Drenkow-Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continual advancement of deep learning algorithms for medical image analysis has increased the potential for their adoption at scale. Across a wide range of medical applications including skin lesion classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>, detection of diabetic retinopathy in fundus images <ref type="bibr" target="#b13">[14]</ref>, detection of large vessel occlusions in CT <ref type="bibr" target="#b19">[20]</ref>, and detection of pneumonia in chest x-ray <ref type="bibr" target="#b23">[24]</ref>, deep learning algorithms have pushed the boundaries close to or beyond human performance.</p><p>However, with these innovations has come increased scrutiny of the integrity of these models in safety critical applications. Prior work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref> has found that deep neural networks are capable of exploiting spurious features and other shortcuts in the data that are not causally linked to the task of interest such as using dermascopic rulers as cues to predict melanoma <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> or associating the presence of a chest drain with pneumothorax in chest X-ray analysis <ref type="bibr" target="#b20">[21]</ref>. The exploitation of such shortcuts by DNNs may have serious bias/fairness implications <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> and negative ramifications for model generalization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>As attention to these issues grows, recent legislation has been proposed that would require the algorithmic auditing and impact assessment of ML-based automated decision systems <ref type="bibr" target="#b36">[37]</ref>. However, without clearly defined strategies for selecting attributes to audit for bias, impact assessments risk being constrained to only legally protected categories and may miss more subtle shortcuts and data flaws that prevent the achievement of important model goals <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>. Our goal in this work is to develop objective methods for generating data-driven hypotheses about the relative level of risk of various attributes to better support the efficient, comprehensive auditing of any model trained on the same data.</p><p>Our method generates targeted hypotheses for model audits by assessing (1) how feasible it is for a downstream model to detect and exploit the presence of a given attribute from the image alone (detectability), and (2) how much information the model would gain about the task labels if said attribute were known (utility). Causally irrelevant attributes with high utility and detectability become top priorities when performing downstream model audits. We demonstrate high utility complicates attempts to draw conclusions about the detectability of attributes and show our approach succeeds where unconditioned approaches fail. We rigorously validate our approach using a range of synthetic artifacts which allow us to expedite the auditing of models via the use of true counterfactuals. We then apply our method to a popular skin lesion dataset where we identify a previously unreported potential shortcut.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Issues of bias and fairness are of increasing concern in the research community. Recent works such as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> identify cases where trained DNNs exhibit performance disparities across protected groups for chest x-ray classification tasks.</p><p>Of interest to this work, <ref type="bibr" target="#b21">[22]</ref> used Mutual Information-based analysis to examine the robustness of DNNs on dermascopy data and observed performance disparities with respect to typical populations of interest (i.e., age, sex) as well as less commonly audited dataset properties (e.g., image hue, saturation). In addition to observing biased performance in task models, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> show that patient race (and potentially other protected attributes) may be implicitly encoded in representations extracted by DNNs on chest x-ray images. A more general methodology for performing algorithmic audits in medical imaging is also proposed in <ref type="bibr" target="#b17">[18]</ref>. In contrast to our work, these methods focus on individual, biased task models without considering the extent to which those biases are induced by the causal structure of the training/evaluation data.</p><p>In addition to model auditing methods, a number of metrics have been proposed to quantify bias <ref type="bibr" target="#b8">[9]</ref>. A recent study <ref type="bibr" target="#b0">[1]</ref> compared several and recommend normalized pointwise mutual information due to its ability to measure associations in the data while accounting for chance. Also relevant to this work, <ref type="bibr" target="#b15">[16]</ref> provides an analysis of fairness metrics and guidelines for metric selection in the presence of dataset bias. However, these studies focus primarily on biases identifiable through dataset attributes alone and do not consider whether those attributes are detectable in the image data itself.</p><p>Lastly, <ref type="bibr" target="#b27">[28]</ref> found pervasive data cascades where data quality issues compound and cause adverse downstream impacts for vulnerable groups. However, their study was qualitative and no methods for automated dataset auditing were introduced. Bissoto et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> consider the impact of bias in dermatological data by manipulating images to remove potential causally-relevant features while measuring a model's ability to still perform the lesion classification task. Closest to our work, <ref type="bibr" target="#b24">[25]</ref> takes a causal approach to shortcut identification by using conditional dependence tests to determine whether DNNs rely on specific dataset attributes for their predictions. In contrast, our work focuses on screening datasets for attributes that induce bias in task models. As a result, we directly predict attribute values to act as a strong upper bound on detectability and use normalized, chance-adjusted dependence measures to obtain interpretable metrics that we show correlate well with the performance of task models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>To audit at the dataset level, we perform a form of causal discovery to identify likely relationships between the task labels, dataset attributes represented as image metadata, and features of the images themselves (as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>We start from a set of labels {Y }, attributes {A}, and images {X}. We assume that Y (the disease) is the causal parent of X (the image) given that the disease affects the image appearance but not vice versa <ref type="bibr" target="#b5">[6]</ref>. Then the dataset auditing procedure aims to assess the existence and relative strengths of the following two relationships: (1) Utility: A ↔ Y and (2) Detectability: A → X. The utility measures whether a given attribute shares any relationship with the label. The presence of this relationship for A that are not clinically relevant (e.g., sensor type or settings) represents increased potential for biased outcomes. However, not every such attribute carries the same risk for algorithmic bias. Crucially, relationship (2) relates to the detectability of the attribute itself. If our test for (2) finds the existence of relationship A → X is probable, we consider the attribute detectable. Dataset attributes identified as having positive utility with respect to the label (1) and detectable in the image (2) are classified as potential shortcuts and pose the greatest risk to models trained on this dataset.</p><p>Causal Discovery with Mutual Information. Considering attributes in isolation, we assess attribute utility and detectability from an information theoretic perspective. In particular, we recognize first that the presence of a relationship between A and Y can be measured via their Mutual Information: MI(A; Y ) = H(Y )-H(Y |A). MI measures the information gained (or reduction in uncertainty) about Y by observing A (or vice versa) and MI(A; Y ) = 0 occurs when A and Y are independent. We rely on the faithfulness assumption which implies that a causal relationship exists between A and Y when MI(A; Y ) &gt; 0. From an auditing perspective, we aim to identify the presence and relative magnitude of the relationship but not necessarily the nature of it.</p><p>Attributes identified as having a relationship with Y are then assessed for their detectability (i.e., condition (2)). We determine detectability by training a DNN on the data to predict attribute values. Because we wish to audit the entire dataset for bias, we cannot rely on a single train/val/test split. Instead, we partition the dataset into k folds (typically 3) and finetune a sufficiently expressive DNN on the train split of each fold to predict the given attribute A. We then generate unbiased predictions for the entire dataset by taking the output Â from each DNN evaluated on their respective test split. We measure the Conditional Mutual Information over all predictions: CM I( Â; A|Y ) = H( Â|Y )-H( Â|A, Y ). CM I(A, Â|Y ) measures information shared between attribute A and its prediction Â when controlling for information provided by Y . Since relationship A and Y was established via MI(A; Y ), we condition on label Y to understand the extent to which attribute A can be predicted from images when accounting for features associated with Y that may also improve the prediction of A. Similar to MI, CM I( Â; A|Y ) &gt; 0 implies A → Â exists.</p><p>To determine independence and account for bias and dataset specific effects, we include permutation-based shuffle tests from <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. These approaches replace values of A with close neighbors to approximate the null hypothesis that the given variables are conditionally independent. By calculating the percentile of CM I(A; Â|Y ) among all CM I(A π ; Â|Y ) (where A π are permutations of A), we estimate the probability our samples are independent while adjusting for estimator bias and dataset-specific effects. To make CMI and MI statistics interpretable for magnitude-based comparison between attributes, we include adjustments for underlying distribution entropy and chance as per <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref> (See supplement). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Discussion</head><p>To demonstrate the effectiveness of our method, we first conduct a series of experiments using synthetically-altered skin lesion data from the HAM10000 dataset where we precisely create, control, and assess biases in the dataset. After establishing the accuracy and sensitivity of our method on synthetic data, we apply our method to the natural attributes of HAM10000 in Experiment 5 (Sect. 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We use publicly available skin lesion data from the HAM10000 <ref type="bibr" target="#b32">[33]</ref> dataset with additional public metadata from <ref type="bibr" target="#b1">[2]</ref>. The dataset consists of 10,015 dermascopic images collected from two sites, we filter so only one image per lesion is retained, leaving 7,387 images. The original dataset has seven diagnostic categories: we focus on predicting lesion malignancy as a challenging and practical task. While we recognize the importance of demonstrating the applicability of the methodology over many datasets, here we use trials where we perturb this dataset with a variety of synthetic, realistic artifacts (e.g., Fig. <ref type="figure" target="#fig_1">2</ref>), and control association with the malignant target label. With this procedure, we create multiple variants of the dataset with attributes that have known utility and detectability as well as ground truth counterfactuals for task model evaluation. Further details are available in the supplementary materials.</p><p>Training Protocol: For attribute prediction networks used by our detectability procedure, we finetune ResNet18 <ref type="bibr" target="#b14">[15]</ref> models with limited data augmentation. For the malignancy prediction task, we use Swin Transformer <ref type="bibr" target="#b18">[19]</ref> tiny models with RandAugment augmentation to show detectability results generalize to stronger architectures. All models were trained using class-balanced sampling with a batch size of 128 and the AdamW optimizer with a learning rate of 5e-5, linear decay schedule, and default weight decay and momentum parameters. For each trial, we use three-fold cross-validation and subdivide each training fold in a (90:10) train:validation split to select the best models for the relevant test fold. By following this procedure, we get unbiased artifact predictions over the entire dataset for use by MI estimators by aggregating predictions over all test folds. We generally measure model performance via the Receiver Operating Characteristic Area Under the Curve (AUC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment 1: Induced Bias Versus Relationship Strength</head><p>For this experiment, we select an artifact that we are certain is visible (JPEG compression at quality 30 applied to 1000 images), and seek to understand how the relationship between attribute and task label influences the task model's reliance on the attribute. The artifact is introduced with increasing utility such that the probability of the artifact is higher for cases that are malignant. Then, we create a worst case counterfactual set, where each malignant case does not have the artifact, and each benign case does. In Fig. <ref type="figure" target="#fig_2">3a</ref>, we see performance rapidly declines below random chance as utility increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment 2: Detectability of Known Invisible Artifacts</head><p>In the previous section, we showed that the utility A ↔ Y directly impacts the task model bias, given A is visible in images. However, it is not always obvious whether an attribute is visible. In Reading Race, Gichoya et al. showed racial identity can be predicted with high AUC from medical images where this information is not expected to be preserved. Here, we show CMI represents a promising method for determining attribute detectability while controlling for attribute information communicated through labels and not through images.</p><p>Specifically, we consider the case of an "invisible artifact". We make no changes to the images, but instead create a set of randomized labels for our nonexistent artifact that have varying correlation with the task labels (A ↔ Y ). As seen in Fig. <ref type="figure" target="#fig_3">4</ref>, among cases where the invisible artifact and task label have a reasonable association, models tasked with predicting the invisible artifact perform well above random chance, seemingly indicating that these artifacts are visible in images. However, by removing the influence of task label and related image features by calculating MI(A, Â|Y ), we clearly see that the artifact predictions are independent of the labels, meaning there is no visible attribute. Instead, all information about the attribute is inferred from the task label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment 3: Conditioned Detectability Versus Ground Truth</head><p>To verify that the conditional independence testing procedure does not substantially reduce our ability to correctly identify artifacts that truly are visible, we introduce Gaussian noise with standard deviation decreasing past human perceptible levels. In Experiment 2 (Fig 4A ) the performance of detecting artifact presence is artificially inflated because of a relationship between disease and artifact. Here the artifact is introduced at random so AUC is an unbiased measure of detectability. In Table <ref type="table" target="#tab_0">1</ref>, we see the drop in CMI percentile from conditioning is minimal, indicating sensitivity even to weakly detectable attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment 4: Relationship and Detectability vs Induced Bias</head><p>Next, we consider how utility and detectability together relate to bias. We introduce a variety of synthetic artifacts and levels of bias and measure the drop in   AUC that occurs when evaluated on a test set with artifacts introduced in the same ratio as training versus the worst case ratio as defined in Experiment 1.</p><p>Of 36 unique attribute-bias combinations trialed, 32/36 were correctly classified as visible via permutation test with 95% cutoff percentile. The remaining four cases were all compression at quality 90 and had negligible impact on task models (mean drop in AUC of -0.0003 ± .0006). In Fig. <ref type="figure" target="#fig_2">3b</ref>, we see the relative strength of the utility (A ↔ Y ) correlates with the AUC drop observed. This implies utility represents a useful initial metric to predict the risk of an attribute. The detectability, CM I(A; Â|Y ), decreases as utility, MI(A; Y ), increases, implying the two are not independent. Intuitively, when A and Y are strongly related (Utility is high), knowledge of the task label means A is nearly determined, so learning Â does not convey much new information and detectability is smaller. To combat this, we use a conditional permutation method <ref type="bibr" target="#b26">[27]</ref> for judging whether or not an artifact is present. Further, detectability among attributes with equal utility for each level above 0 have statistically significant correlations with drops in AUC (Kendall's τ of 0.800, 0.745, 0.786, 0.786, 0.716 respectively). From this, we expect that for attributes with roughly equal utility, more detectable attributes are more likely to result in biased task models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiment 5: HAM10000 Natural Attributes</head><p>Last, we run our screening procedure over the natural attributes of HAM10000 and find that all pass the conditional independence tests of detectability. Based on our findings, we place the attributes in the following order of concern: (1) Data source, (2) Fitzpatrick Skin Scale, (3) Ruler Presence, (4) Gentian marking presence (we skip localization, age and sex due to clinical relevance <ref type="bibr" target="#b4">[5]</ref>). From Fig. <ref type="figure" target="#fig_5">5</ref> we see data source is both more detectable and higher in utility than other variables of interest, representing a potential shortcut. To the best of our knowledge, we are the first to document this concern, though recent independent work supports our result that differences between the sets are detectable <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Our proposed method marks a positive step forward in anticipating and detecting unwanted bias in machine learning models. By focusing on dataset screening, we aim to prevent downstream models from inheriting biases already present and exploitable in the data. While our screening method naturally includes common auditing hypotheses (e.g., bias/fairness for vulnerable groups), it is capable of generating targeted hypotheses on a much broader set of attributes ranging from sensor information to clinical collection site. Future work could develop unsupervised methods for discovering additional high risk attributes without annotations. The ability to identify and investigate these hypotheses provides broad benefit for research, development, and regulatory efforts aimed at producing safe and reliable AI models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Relationships assessed in the attribute screening protocol. DNNs are trained to predict Â from X for use in estimating attribute detectability.</figDesc><graphic coords="3,108,81,459,44,206,44,53,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of synthetic artifacts with varying image effects.</figDesc><graphic coords="5,42,30,196,76,339,52,74,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Performance of task models trained on data with known detectable artifacts introduced with various positive correlations to the malignant class and evaluated on our worst-case counterfactual set. (b) Performance drop on worst-case counterfactual test set of models trained with various artifacts of unknown detectability. For each, MI(A, Y ), CM I(A, Â|Y ) values are estimated empirically with normalization and adjustment for chance applied.</figDesc><graphic coords="6,85,47,383,36,281,32,139,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (A) AUC for attribute prediction vs utility (MI(A; Y )). The models learn to fit a non-existent artifact given sufficient utility A ↔ Y . (B) CM I(A; Â|Y ) reported with 95% CIs (calculated via bootstrap) for the same models and predictions, each interval includes zero, suggesting conditional independence. (C) CM I statistic percentile vs 1000 trials with data permuted to be conditionally independent.</figDesc><graphic coords="7,42,30,439,37,339,28,108,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1. 0</head><label>0</label><figDesc>± 0 1.0 ± 0 1.0 ± 0 1.0 ± 0 1.0 ± 0 1.0 ± 0 0.71 ± .075 0.53 ± .017 0.52 ± .036 CMI(A; Â|Y )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Detectability vs. utility for natural attributes in HAM10000. Attributes with high CM I and MI which are non-causally related to the disease pose the greatest risk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Detectability of Gaussian noise with varying strength vs independence testing-based percentile and CMI(A; Â|Y ), normalized and adjusted for chance. Anecdotally, σ = 0.05 is the minimum level that is visible (see supplement).</figDesc><table><row><cell cols="4">Noise Detectability vs Ground Truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell cols="3">Gaussian Noise σ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>.5</cell><cell>.4</cell><cell>.3</cell><cell>.2</cell><cell>.1</cell><cell>.05</cell><cell>.01</cell><cell>.001</cell><cell>0</cell></row><row><cell>Attribute</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prediction AUC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This project was supported in part by an appointment to the <rs type="programName">Research Participation Program</rs> at the <rs type="institution">U.S. Food and Drug Administration</rs> administered by the <rs type="funder">Oak Ridge Institute for Science and Education through an interagency agreement between the U.S. Department of Energy</rs> and the <rs type="funder">U.S. Food and Drug Administration</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xz3Gpja">
					<orgName type="program" subtype="full">Research Participation Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring model biases in the absence of ground truth</title>
		<author>
			<persName><forename type="first">O</forename><surname>Aka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bauerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI/ACM AIES</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Skin deep unlearning: artefact and instrument debiasing in the context of melanoma classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">(De) constructing bias on skin lesion datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bissoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fornaciali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPRW</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Debiasing skin lesion datasets and models? Not so fast</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bissoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IEEE CVPRW</publisher>
			<biblScope unit="page" from="740" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Epidemiology and risk factors of melanoma</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wernberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surg. Clin. North Am</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causality matters in medical imaging</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-020-17478-w</idno>
		<ptr target="https://doi.org/10.1038/s41467-020-17478-w" />
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3673</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ai for radiographic COVID-19 detection selects shortcuts over signal</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Janizek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-021-00338-7</idno>
		<idno>42256-021-00338-7</idno>
		<ptr target="https://doi.org/10.1038/s" />
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="610" to="619" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on bias in visual datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fabbrizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ntoutsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">223</biblScope>
			<biblScope unit="page">103552</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AI recognition of patient race in medical imaging: a modelling study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Gichoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="406" to="e414" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Algorithmic encoding of protected characteristics in image-based models for disease detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winzeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winzeck</surname></persName>
		</author>
		<title level="m">Risk of bias in chest x-ray foundation models</title>
		<imprint>
			<date type="published" when="2022-09">September 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE/CVPR</publisher>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Henry Hinnefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cooman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mammo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deese</surname></persName>
		</author>
		<title level="m">Evaluating fairness metrics in the presence of dataset bias</title>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning applied to chest X-rays: exploiting and preventing shortcuts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jabbour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kazerooni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Sjoding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiens</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="750" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The medical algorithmic audit</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mccradden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Denniston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oakden-Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="384" to="e397" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>IEEE/CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Artificial intelligence to diagnose ischemic stroke and identify large vessel occlusions: a systematic review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. NeuroInterv. Surg</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="156" to="164" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hidden stratification causes clinically meaningful failures in machine learning for medical imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oakden-Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Health, Inference, and Learning</title>
		<meeting>the ACM Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluating neural network robustness for melanoma classification using mutual information</title>
		<author>
			<persName><forename type="first">M</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bukowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pezeshk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>SPIE</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The fallacy of AI functionality</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">CheXNet: radiologist-level pneumonia detection on chest Xrays with deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional dependence tests reveal the usage of ABCD rule features and bias variables in automatic skin lesion classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Penzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPRW</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Causal network reconstruction from time series: from theoretical assumptions to practical estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">75310</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<editor>AISTATS. PMLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">everyone wants to do the model work, not the data work&quot;: data cascades in highstakes AI</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sambasivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kapania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Highfill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Akrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Aroyo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CheXclusion: fairness gaps in deep chest X-ray classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seyyed-Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Symposium on Biocomputing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seyyed-Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B A</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2176" to="2182" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using deep learning for dermatologist-level detection of suspicious pigmented skin lesions from wide-field images</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Soenksen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Transl. Med</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">3652</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Handling dataset dependence with model ensembles for skin lesion classification from dermoscopic and clinical images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Somfai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Imaging Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="556" to="571" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180161</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: variants, properties, normalization and correction for chance</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Association between surgical skin markings in dermoscopic images and diagnostic performance of a deep learning convolutional neural network for melanoma recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Dermatol</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1135" to="1141" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Association between different scale bars in dermoscopic images and diagnostic performance of a market-approved deep learning convolutional neural network for melanoma recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Cancer</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="146" to="154" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Algorithmic accountability act of</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wyden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Booker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Clarke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
