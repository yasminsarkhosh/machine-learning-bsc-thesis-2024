<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation</title>
				<funder ref="#_BMAVT7W #_gZbeG7U">
					<orgName type="full">Foundation of Hope</orgName>
				</funder>
				<funder ref="#_5gBrZue">
					<orgName type="full">Korean government (MSIT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tingting</forename><surname>Dan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<postCode>27599</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minjeong</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Greensboro</orgName>
								<address>
									<postCode>27402</postCode>
									<settlement>Greensboro</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Won</forename><forename type="middle">Hwa</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science and Engineering/Graduate School of AI</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<postCode>37673</postCode>
									<region>Pohang</region>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guorong</forename><surname>Wu</surname></persName>
							<email>guorong_wu@med.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Psychiatry</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<postCode>27599</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<postCode>27599</postCode>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="77" to="86"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">13A0811E96858BF212B89F6EAC5B95D9</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep neural network</term>
					<term>Complex system</term>
					<term>Variational analysis</term>
					<term>Total variation</term>
					<term>Alzheimer&apos;s disease</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Alzheimer's disease (AD) is characterized by the propagation of tau aggregates throughout the brain in a prion-like manner. Tremendous efforts have been made to analyze the spatiotemporal propagation patterns of widespread tau aggregates. However, current works focus on the change of focal patterns in lieu of a system-level understanding of the tau propagation mechanism that can explain and forecast the cascade of tau accumulation. To fill this gap, we conceptualize that the intercellular spreading of tau pathology forms a dynamic system where brain region is ubiquitously wired with other nodes while interacting with the build-up of pathological burdens. In this context, we formulate the biological process of tau spreading in a principled potential energy transport model (constrained by brain network topology), which allows us to develop an explainable neural network for uncovering the spatiotemporal dynamics of tau propagation from the longitudinal tau-PET images. We first translate the transport equation into a backbone of graph neural network (GNN), where the spreading flows are essentially driven by the potential energy of tau accumulation at each node. Further, we introduce the total variation (TV) into the graph transport model to prevent the flow vanishing caused by the 2 -norm regularization, where the nature of system's Euler-Lagrange equations is to maximize the spreading flow while minimizing the overall potential energy. On top of this min-max optimization scenario, we design a generative adversarial network (GAN) to depict the TV-based spreading flow of tau aggregates, coined TauFlowNet. We evaluate TauFlowNet on ADNI dataset in terms of the prediction accuracy of future tau accumulation and explore the propagation mechanism of tau aggregates as the disease progresses. Compared to current methods, our physics-informed method yields more accurate and interpretable results, demonstrating great potential in discovering novel neurobiological mechanisms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tau accumulation in the form of neurofibrillary tangles in the brain is an important pathology hallmark in Alzheimer's disease (AD) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. With the rapid development of imaging technology, tau positron emission tomography (PET) allows us to measure the local concentration level of tau pathology in-vivo, which is proven to be a valuable tool for the differential diagnosis of dementia in routine clinical practice. As the converging consensus that the disease progression is closely associated with the spreading of tau aggregates <ref type="bibr" target="#b2">[3]</ref>, it is vital to characterize the spatiotemporal patterns of tau propagation from the longitudinal tau-PET scans.</p><p>The human brain is a complex system that is biologically wired by white matter fibers <ref type="bibr" target="#b3">[4]</ref>. Such an optimized information-exchanging system supports transient self-organized functional fluctuations. Unfortunately, the concept of fast transport also applies to toxic tau proteins that hijack the network to spread rapidly throughout the brain. As shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>, the stereotypical spreading of the tau pathology facilitates the increase of whole-brain tau SUVR (standard uptake value ratio) as the stage of the disease progresses from mild to severe. In this context, many graph diffusion models have been proposed to model the temporal patterns of tau propagation. For example, the network diffusion model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> has been used to predict the future accumulation of pathological burdens where the spreading pathways are constrained by the network topology. However, current computational models usually assume the system dynamics is a linear process <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, where such gross simplification might be responsible for the inconsistent findings on tau propagation. For instance, the eigenvectors of graph Laplacian matrix (corresponding to the adjacency matrix of the underlying brain network) have been widely used as the basis functions to fit the longitudinal changes of pathological burdens on each brain region. Supposing that future changes follow the same dynamics, we can forecast the tau accumulations via extrapolation in the temporal domain. It is clear that these methods only model the focal change at each node, with no power to explain the tau propagation mechanism behind the longitudinal change, such as the questions "Which regions are actively disseminating tau aggregates?", "Does no change of tau SUVR indicate not being affected or just passing on the tau aggregates?".</p><p>To answer these fundamental questions, we put our spotlight on the spreading flows of tau aggregates. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), it is computationally challenging to find the directed region-to-region flows that can predict the tau accumulations over time. We cast it into a well-posed problem by assuming the local development of tau pathology and the spreading of tau aggregates form a dynamic energy transport system <ref type="bibr" target="#b9">[10]</ref>. In the analogy of gravity that makes water flow downward, the cascade of tau build-up generates a potential energy field (PEF) that drives the spreading of tau aggregates to propagate from high to low tau SUVR regions. As we constrain the tau spreading flows on top of the network topology, we translate the tau-specific transport equation into an equivalent graph neural network (GNN) <ref type="bibr" target="#b10">[11]</ref>, where the layer-by-layer manner allows us to effectively characterize the tau spreading flows from a large amount of longitudinal tau-PET scans. Since the deep model of GNN often yields over-smoothed PEF, we further tailor a new transport equation by introducing the total variation (TV) on the gradient of PEF, which prompts a new deep model (coined TauFlowNet) free of the vanishing flow issue. Specifically, we trace the root cause of the over-smoothing issue in GNN up to the 2 -norm Lagrangian mechanics of graph diffusion process that essentially encourages minimizing the squared energy changes. Thus, one possible solution is to replace the 2 -based regularization term with the TV constraint on the gradient of PEF. After that, the Euler-Lagrange (E-L) equation of the new Lagrangian mechanics describes new dynamics of tau propagation steered by a collection of max flows that minimize the absolute value of overall potential energies in the system. In this regard, we present a generative adversarial network (GAN) to find the max flows (in the discriminator model) that (i) follow the physics principle of transport equation (in the generator model) and (ii) accurately predict the future tau accumulation (as part of the loss function). Therefore, our TauFlowNet is an explainable deep model to the extent that the physics principle provides the system-level underpinning of tau spreading flows and the application value (such as prediction accuracy) is guaranteed by the mathematics insight and the power of deep learning.</p><p>We have applied our TauFlowNet on the longitudinal neuroimaging data in ADNI dataset. We compare the prediction accuracy of future tau accumulation with the counterpart methods and explore the propagation mechanism of tau aggregates as the disease progresses, where our physics-informed deep model yields more accurate and interpretable results. The promising results demonstrate great potential in discovering novel neurobiological mechanisms of AD through the lens of machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In the following, we first elucidate the relationship between GNN, E-L equation, and Lagrangian mechanics, which sets the stage for the method formulation and deep model design of our TauFlowNet in Sect. 2.1. Then, we propose the TV-based graph regularization for GAN-based deep learning in Sect. 2.2, which allows us to characterize the spreading flow of tau aggregates from longitudinal tau-PET scans.</p><p>Suppose the brain network is represented by a graph G = (V , W ) with N nodes (brain regions) V = {v i |i = 1, . . . , N } and the adjacency matrix W = w ij N i,j=1 ∈ R N ×N describing connectivity strength between any two nodes. For each node v i , we have a graph embedding vector</p><formula xml:id="formula_0">x i ∈ R m . The gradient (∇ G x) ij = w ij (x i -x j )</formula><p>indicates the feature difference between v i and v j weighed by the connectivity strength w ij . Thus, the graph diffusion process <ref type="bibr" target="#b11">[12]</ref> can be formulated as ∂x (t)  ∂t = div(∇ G x(t)), where the evolution of embedding vectors x = [x i ] N i=1 is due to network flux measured by the divergence. Several decades ago, the diffusion process ∂x (t)  ∂t = div(∇x(t)) has been widely studied in image processing <ref type="bibr" target="#b12">[13]</ref>, which is the E-L equation of the functional min x |∇x| 2 dx. By replacing the 1D gradient operator (∇x) ij = x ix j defined in the Euclidean space with the graph gradient (∇ G x) ij , it is straightforward to find that the governing equation in graph diffusion process</p><formula xml:id="formula_1">∂x(t) ∂t = div(∇ G x(t)) is the E-L equation of functional min x G ∇ G x</formula><p>2 dx on top of the graph topology.</p><p>The GNN depth is blamed for over-smoothing <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> in graph representation learning. We attribute this to the isotropic smoothing mechanism formulated in the 2 -norm. Connecting GNN to calculus of variations provides a principled way to design new models with guaranteed mathematics and explainability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation for Discovering Spreading Flow of Tau Propagation</head><p>Neuroscience Assumption. Our brain's efficient information exchange facilitates the rapid spread of toxic tau proteins throughout the brain. To understand this spread, it's essential to measure the intrinsic flow information (such as flux and bandwidth) of tau aggregates in the complex brain network.</p><p>Problem Formulation from the Perspective of Machine Learning. The overarching goal is to estimate the time-dependent flow field</p><formula xml:id="formula_2">f (t) = f ij (t) N i,j=1 of tau spreading such that x i (t + 1) = x i (t) + N j=1 f ij (t)</formula><p>, where f ij (t) stands for the directed flow from the region v i to v j . As the toy example shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, there are numerous possible solutions for F given the longitudinal change x. To cast this ill-posed problem into a well-defined formulation, we conceptualize that the tau propagation in each brain forms a unique dynamic transport system of the brain network, and the spreading flow is driven by a tau-specific potential energy field</p><formula xml:id="formula_3">u(t) = [u i (t)] N i=1</formula><p>, where u i (t) is output of a nonlinear process φ reacting to the tau accumulation x i at the underlying region v i , i.e., u i = φ(x i ). The potential energy field drives the flow of tau aggregates in the brain, similar to the gravity field driving water flow. Thereby the spreading of tau is defined by the gradient of potential energy between connected regions:</p><formula xml:id="formula_4">f ij (t) = -∇ G u(t) ij = -w ij u i (t) -u j (t) ,<label>(1)</label></formula><p>Thus, the fundamental insight of our model is that the spreading flow f ij (t) is formulated as an "energy transport" process of the tau potential energy field. Taking together, the output of our model is a mechanistic equation M (•) of the dynamic system that can predict the future flow based on the history flow sequences, i.e.,</p><formula xml:id="formula_5">f ij (t T ) = M f ij (t 1 ), . . . , f ij (t T -1 ) .</formula><p>Transport Equation for Tau Propagation in the Brain. A general continuity transport equation <ref type="bibr" target="#b9">[10]</ref> can be formulated in a partial differential equation (PDE) as:</p><formula xml:id="formula_6">dx dt + div(q) = 0 ( 2 )</formula><p>where q is the flux of the potential energy u (conserved quantity). The intuition of Eq. 2 is that the change of energy density (measured by regional tau SUVR x) leads to the energy transport throughout the brain (measured by the flux of PEF). As flux is often defined as the rate of flow, we further define the energy flow as q ij = α • f ij , where α is a learnable parameter characterizing the contribution of the tau flow f ij to the potential energy flux q ij . By plugging u t = φ(x t ) and q ij = α • f ij into Eq. 2, the energy transport process of tau spreading flow f can be described as:</p><formula xml:id="formula_7">∂u ∂t = -φ α -1 div(f ) . (<label>3</label></formula><formula xml:id="formula_8">)</formula><p>Note, φ and α are trainable parameters that can be optimized through the supervised learning schema described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TauFlowNet: An Explainable Deep Model Principled with TV-Based Lagrangian Mechanics</head><p>To solve the flow field f in Eq. 3, the naïve deep model is a two-step approach (shown in the left red panel of Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>(1) Estimate the PEF u by fixing the flow f . By letting f = ∇ G u (in Eq. 1), the solution of u follows a reaction process u = φ(x) and a graph diffusion process ∂u ∂t = -α -1 div ∇ G u = -α -1 u, where = div(∇ G ) is the graph Laplacian operator. The parameters of φ and α can be learned using a multi-layer perceptron (MLP) and a graph convolution layer (GCN), respectively. Thus, the input is the observed tau SUVR x t and the loss function aims to minimize the prediction error from x t to x t+1 .</p><p>(2) Calculate spreading flow f . Given u, it is straightforward to compute each flow f ij (t) by Eq. 1. In Sect. 2.1, we have pointed out that the GNN architecture is equivalent to the graph diffusion component in Eq. 3. Since the PDE of the graph diffusion process ∂u ∂t =u is essentially the Euler-Lagrange (E-L) equation of the quadratic functional J (u) = min u ∇ G u 2 du, the major issue is the "over-smoothness" in u that might result in vanishing flows (i.e., f → 0).</p><p>To address the over-smoothing issue, we propose to replace the quadratic Laplacian regularizer with total variation, i.e., J TV (u) = min u ∇ G u du, which has been successfully applied in image denoising <ref type="bibr" target="#b16">[17]</ref> and reconstruction <ref type="bibr" target="#b17">[18]</ref>. Since |•| in J TV is not differentiable at 0, we introduce the latent flow variable f and reformulate the TVbased functional as J TV (u, f ) = min u (f ⊗∇ G u)du, where ⊗ is Hadamard operation between two matrices. Recall that the flow f ij has directionality. Thus, the engineering trick of element-wise operation f ij ∇ G u ij keeps the degree always non-negative as we take the absolute value, which allows us to avoid the undifferentiable challenge.</p><p>After that, we boil down the minimization of J TV (u) into a dual min-max functional as</p><formula xml:id="formula_9">J TV (u, f ) = min u max f f • ∇ G u du,</formula><p>where we maximize f such that J TV (u, f ) is close enough to J TV (u). In this regard, the E-L equation from the Gâteaux variations leads to two coupled PDEs:</p><formula xml:id="formula_10">⎧ ⎨ ⎩ max f df dt = ∇ G u min u du dt = div(f ) (4)</formula><p>The alternative solution for Eq. 4 is that we minimize PEF u through the Lagrangian mechanics defined in the transport equation ∂u ∂t = -φ α -1 div(f ) where the system dynamics is predominated by the maximum flow field f . Since the accurate estimation of flow field f (t) and PEF u(t) is supposed to predict the future tau accumulation x(t +1) by</p><formula xml:id="formula_11">x i (t + 1) = φ -1 u i (t) + N j=1 f ij (t)</formula><p>, we can further tailor the min-max optimization for Eq. 4 into a supervised learning scenario as the TauNetFlow described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TauFlowNet: A GAN Network Architecture of TV-Based Transport</head><p>Equation. Here, we present an explainable deep model to uncover the spreading flow of tau aggregates f from the longitudinal tau-PET scans. Our deep model is trained to learn the system dynamics (in Eq. 4), which can predict future tau accumulations. The overall network architecture of TauFlowNet is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, which consists of a generator (left) and a discriminator module (right). The generator is essentially our initial GNN model of the transport equation that consists of a reaction process φ and a graph diffusion process. Specifically, the generator consists of (i) a MLP to project the input regional tau SUVR x t into the potential energy filed u t through a nonlinear reaction process u t = φ(x t ) (green dashed box), (ii) a GCN layer to transport potential energy along the connectome pathways, resulting in the u t+1 = GCN (u t ) (purple dashed box), and (iii) another MLP to generate x t+1 from u t+1 via x t+1 = φ -1 (u t+1 ) (red dashed box). The discriminator module is designed to synthesize the future PEF u t+1 based on the current PEF u t and current estimation of tau spreading flow f ij (orange dash box), i.e., ũt+1 = u t + N j=1 f ij (t). Then, we train another GCN layer to generate the synthesized xt+1 from ũt+1 via xt+1 = GCN (ũ t+1 ) (blue dashed box). The driving force of our TauFlowNet is to minimize (1) the MAE (mean absolute error) between the output of the generator x t+1 and the observed tau SUVR, and (2) the distance between the synthesized tau SUVR xt+1 (from the discriminator) and the output of generator x t+1 (from the transport equation). In the spirit of probabilistic GAN <ref type="bibr" target="#b18">[19]</ref>, we use one loss function L D = D(x t+1 ) + [m -D(G(x t ))] + to train the discriminator (D) and the other one L G = D(G(x t )) to train the generator (G), where m denotes the positive margin and the operator [•] + = max(0, •). Minimizing L G is similar to maximizing the second term of L D except the non-zero gradient when D(G(x t )) ≥ m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we evaluated the performance of the proposed TauFlowNet for uncovering the latent flow of tau spreading on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset (https://adni.loni.usc.edu/). In total, 163 subjects with longitudinal tau-PET scans are used for training and testing the deep model. In addition, each subject has T1-weighted MRI and diffusion-weighted imaging (DWI) scan, from which we construct the structural connectome. Destrieux atlas <ref type="bibr" target="#b19">[20]</ref> is used to parcellate each brain into 160 regions of interest (ROIs), which consist of 148 cortical regions (frontal lobe, insula lobe, temporal lobe, occipital lobe, parietal lobe, and limbic lobe) and 12 sub-cortical regions (left and right hippocampus, caudate, thalamus, amygdala, globus pallidum, and putamen). Following the clinical outcomes, we partition the subjects into the cognitive normal (CN), early-stage mild cognitive impairment (EMCI), late-stage MCI (LMCI), and AD groups. We compare our TauFlowNet with classic graph convolutional network (GCN) <ref type="bibr" target="#b20">[21]</ref> and deep neural network (DNN) <ref type="bibr" target="#b21">[22]</ref>. We use 5-fold cross-validation to evaluate the prediction performance and examine the spreading patterns of tau aggregates in different clinic groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluate the Prediction Accuracy of Future Tau Accumulation</head><p>We first evaluate the prediction performance between the ground truth and the estimated SUVR values, where we use the mean absolute error (MAE) to quantify the prediction accuracy. The statistics of MAE by our TauFlowNet, GCN, and DNN are shown in the first column (with shade) of Table <ref type="table" target="#tab_0">1</ref>. To further validate the robustness of our model, we add uncorrelated additive Gaussian noises to the observed SUVR measurements. The prediction accuracies with respect to different noise levels are listed in the rest columns of Table <ref type="table" target="#tab_0">1</ref>. It is clear that our TauFlowNet consistently outperforms the other two deep models. The performance of GCN is worse than DNN within the same network depth, which might be due to the over-smoothing issue.</p><p>As part of the ablation study, we implement the two-step approach (the beginning of Sect. 2.2), where we train the model (MLP + GCN) shown in the left panel of Fig. <ref type="figure" target="#fig_1">2</ref> to obtain further tau accumulation. Since the deep model in this two-step approach is formalized from the PDE, we call this degraded version as PDENet. We display the result of in the last row of Table <ref type="table" target="#tab_0">1</ref>. Compared to PDENet, our TauFlowNet (in GAN architecture) takes advantage of TV constraint to avoid over-smoothing and integrates two steps (i.e., estimating PEF and uncovering spreading flows) into a unified neural network, thus significantly enhancing the prediction accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Examine Spatiotemporal Patterns of the Spreading Flow of Tau Aggregates</head><p>We examine the pattern of spreading flows on an individual basis (Fig. <ref type="figure" target="#fig_2">3a</ref>) and cross populations (Fig. <ref type="figure" target="#fig_2">3b</ref>). First, we visualize the top flows (ranked in terms of flow volume) uncovered in a CN subject. It is apparent that subcortex-cortex flows are the predominant patterns, where most of the tau aggregates spread from subcortical regions (globus pallidus, hippocampus, and putamen) to the temporal lobe, limbic lobe, parietal lobe, and insula lobe. Note, we find inferior temporal gyrus (t 6 ) and entorhinal cortex (t 8 ) are actively involved in the subcortex-cortex flows, which are the footprints of early stage tau propagation frequently reported in many pathology studies <ref type="bibr" target="#b22">[23]</ref>. Second, we show the top-ranked population-wise average tau spreading flows for CN, EMCI, LMCI, and AD groups in Fig. <ref type="figure" target="#fig_2">3b</ref>. As the disease progresses, the subcortex-cortex flows gradually switch to cortex-cortex flows. After tau aggregates leave the temporal lobe, the tau propagation becomes widespread throughout the entire cortical region. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a physics-informed deep neural network (TauFlowNet) by combining the power of dynamic systems (with well-studied mechanisms) and machine learning (fine-tuning the best model) to discover the novel propagation mechanism of tau spreading flow from the longitudinal tau-PET scans. We have evaluated our TauFlowNet on ADNI dataset in forecasting tau accumulation and elucidating the spatiotemporal patterns of tau propagation in the different stages of cognitive decline. Our physics-informed deep model outperforms existing state-of-the-art methods in terms of prediction accuracy and model explainability. Since the region-to-region spreading flow provides rich information for understanding the tau propagation mechanism, our learning-based method has great applicability in current AD studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a). Tau spreading across the brain network facilitates the build-up of tau aggregates in the disease progression. (b). The illustration of the computational challenge for estimating the spreading flow of tau aggregates based on the change of focal patterns.</figDesc><graphic coords="2,85,29,324,35,248,80,136,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The GAN architecture for min-max optimization in the TauFlowNet. (Color figure online)</figDesc><graphic coords="6,41,79,461,36,340,33,81,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of tau spreading flows in an individual cognitive normal subject (a) and the population-wise average spreading flows for CN, early/late-MCI, and AD groups.</figDesc><graphic coords="8,72,30,359,90,279,37,176,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The prediction performance (MAE) between observed and predicted tau.</figDesc><table><row><cell>Noise level</cell><cell>-</cell><cell>std = 0.02</cell><cell>std = 0.04</cell><cell>std = 0.08</cell><cell>std = 0.1</cell></row><row><cell cols="6">TauFlowNet 0.049 ± 0.02 0.058 ± 0.03 0.066 ± 0.03 0.079 ± 0.04 0.081 ± 0.04</cell></row><row><cell>GCN</cell><cell cols="5">0.124 ± 0.08 0.128 ± 0.08 0.130 ± 0.08 0.142 ± 0.08 0.161 ± 0.10</cell></row><row><cell>DNN</cell><cell cols="5">0.070 ± 0.03 0.072 ± 0.04 0.080 ± 0.04 0.104 ± 0.06 0.112 ± 0.06</cell></row><row><cell>PDENet</cell><cell cols="5">0.110 ± 0.05 0.120 ± 0.05 0.128 ± 0.05 0.134 ± 0.05 0.155 ± 0.06</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported by <rs type="funder">Foundation of Hope</rs>, <rs type="grantNumber">NIH R01AG068399</rs>, <rs type="grantNumber">NIH R03AG073927</rs>. <rs type="person">Won Hwa Kim</rs> was partially supported by <rs type="grantNumber">IITP-2019-0-01906</rs> (<rs type="programName">AI Graduate Program at POSTECH</rs>) funded by the <rs type="funder">Korean government (MSIT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BMAVT7W">
					<idno type="grant-number">NIH R01AG068399</idno>
				</org>
				<org type="funding" xml:id="_gZbeG7U">
					<idno type="grant-number">NIH R03AG073927</idno>
				</org>
				<org type="funding" xml:id="_5gBrZue">
					<idno type="grant-number">IITP-2019-0-01906</idno>
					<orgName type="program" subtype="full">AI Graduate Program at POSTECH</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NIA-AA research framework: toward a biological definition of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimers Dement</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="535" to="562" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toxic tau: structural origins of tau aggregation in Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>Al Mamun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Regen. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1417</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Propagation of Tau aggregates and neurodegeneration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goedert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Eisenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Crowther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="210" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Network neuroscience</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bassett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="364" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A network diffusion model of disease progression in dementia</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuceyeski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1204" to="1215" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A network-guided reaction-diffusion model of AT [N] biomarkers in Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Network diffusion model of progression predicts longitudinal patterns of atrophy and metabolism in Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="369" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Network model of pathology spread recapitulates neurodegeneration and selective vulnerability in Huntington&apos;s disease</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page">118008</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spread of pathological tau proteins through communicating neurons in human Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2612</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mathematical Methods of Classical Mechanics</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Arnold</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4757-1693-1</idno>
		<ptr target="https://doi.org/10.1007/978-1-4757-1693-1" />
	</analytic>
	<monogr>
		<title level="s">Graduate Texts in Mathematics Mathematics</title>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph neural networks: a review of methods and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grand: graph neural diffusion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image restoration by a fractional reaction-diffusion process</title>
		<author>
			<persName><forename type="first">H</forename><surname>Matallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lakhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="709" to="724" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeepGCNs: can GCNs go as deep as cnns?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Total variation image restoration: overview and recent developments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1007/0-387-28831-7_2</idno>
		<ptr target="https://doi.org/10.1007/0-387-28831-7_2" />
	</analytic>
	<monogr>
		<title level="m">Handbook of Mathematical Models in Computer Vision</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="17" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</title>
		<author>
			<persName><forename type="first">C</forename><surname>Destrieux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification of graph convolutional networks with Laplacian rank constraints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi layer perceptron</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lernen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Lab Special Lecture</title>
		<imprint>
			<biblScope unit="page" from="7" to="24" />
			<date type="published" when="2014">2014</date>
			<publisher>University of Freiburg</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regional Aβ-tau interactions promote onset and acceleration of Alzheimer&apos;s disease tau spreading</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
