<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning</title>
				<funder ref="#_9DeP2MG">
					<orgName type="full">Korea</orgName>
				</funder>
				<funder>
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_RaHEBhQ #_G3Bkbm7">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea Medical Device Development Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Trade, Industry and Energy</orgName>
				</funder>
				<funder ref="#_SvjUdUm">
					<orgName type="full">Ministry of Food and Drug Safety</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea government</orgName>
				</funder>
				<funder>
					<orgName type="full">MSIT (Ministry of Science and ICT)</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Health &amp; Welfare</orgName>
				</funder>
				<funder ref="#_ApxCAnt">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Saehyun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">In-Seok</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Korea University Anam Hospital</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Seung</forename><forename type="middle">Jun</forename><surname>Baek</surname></persName>
							<email>sjbaek@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="703" to="713"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1EEC45989069F3DB26C0E9F3ECB37D9F</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_67</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tooth segmentation</term>
					<term>Cone-Beam Computed Tomography (CBCT)</term>
					<term>3D Deep Learning</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate segmentation of teeth is crucial for effective treatment planning. Previous approaches attempted to segment a tooth as a whole, which has limitations because most treatments involve internal structures of teeth. In this paper, we propose fully automated segmentation of internal tooth structure, including enamel, dentin, and pulp, which is the first attempt to the best of our knowledge. The task is challenging, because a total of 96 classes of tooth structures need to be identified from a CBCT image. We design a 3-stage process of coarse-tofine segmentation of tooth structures without compromising the original resolution. We propose Dual-Hierarchy U-Net (DHU-Net) in order to capture hierarchical structures of teeth, and to effectively fuse encoder and decoder features from higher and lower hierarchies. Experiments demonstrate that our method outperforms state-of-the-art methods in both tasks of segmenting the whole tooth and internal tooth structure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cone Beam Computed Tomography (CBCT) is widely used in dental clinics, as it provides volumetric views of tooth structures for diagnosis, treatment, and surgery. Despite the extensive research on teeth segmentation from CBCT images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>, segmenting an individual tooth as a whole has limited applications, e.g., predicting tooth movement in orthodontics. Most dental treatments, including caries, prosthodontics and endodontics, focus on the internal structures of teeth. Thus, the task of segmenting and representing internal tooth structure is important, and can better assist dental diagnosis and treatment planning <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this paper, we propose an end-to-end framework for tooth segmentation including internal structures from CBCT which, to the best of our knowledge, is the first work to do so. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), a tooth consists of enamel, dentin and pulp which we refer to the internal structure. Since there are a total of 32 tooth classes (including wisdom teeth), the model should be capable of identify 96 tooth classes from a CBCT voxel. Considering the size of CBCT data, the problem poses significant challenges from the perspective of not only segmentation performance, but also computational complexity.</p><p>We take a hierarchical approach to tackle the challenges, and propose a 3stage process in order to accurately extract structures without compromising the original resolution of CBCT data. Each stage performs precise detection and segmentation for each level of hierarchy in the tooth structure. We propose a novel module called Dual-Hierarchy U-Net (DHU-Net) which is designed to extract and combine hierarchical features so as to effectively leverage hierarchy in the internal tooth structure. The segmentation performance of our model is evaluated for internal structures as well as the whole teeth. Experiments show that our method outperforms state-of-the-art (SOTA) baselines in both cases.</p><p>Our contributions are summarized as follows: 1) a fully automated, end-toend model for internal tooth segmentation for the first time; 2) a novel 3-stage method with Dual-Hierarchy U-Net module leveraging the hierarchical structures of teeth; 3) the superiority of our model over SOTA baselines.</p><p>Related Work. 3D tooth segmentation has been actively studied, including knowledge-based approaches, e.g., graph cut <ref type="bibr" target="#b9">[10]</ref> and level set methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref> which rely on intensity discrepancies between tooth and non-tooth regions. However, these methods can suffer at regions where teeth meet or where intensity values of roots are similar to jawbone. Internal tooth segmentation methods have been proposed, e.g., enamel-dentin segmentation based on watershed algorithm <ref type="bibr" target="#b12">[13]</ref>, or tooth pulp cavity segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>, which however are sensitive to intensity thresholds, and do not simultaneously segment the entire structure. Recently, fully automated segmentation based on deep-learning has been actively explored. ToothNet <ref type="bibr" target="#b2">[3]</ref> performs fully automated tooth segmentation using Mask R-CNN <ref type="bibr" target="#b8">[9]</ref> which however had limitations, e.g., applicable only to down-sampled CBCT images. Coarse-to-fine segmentation was proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, which initially down-sampled and subsequently the full-resolution CBCT images process. SGANet <ref type="bibr" target="#b15">[16]</ref> used semantic graph attention based on Graph Convolutional Network <ref type="bibr" target="#b20">[21]</ref> to learn and exploit the spatial association among teeth. Prior two-stage approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> extract tooth patches in the 1st stage, and segment the tooth ROIs in the 2nd stage. However, such approaches not only are insufficient for internal segmentation, but also focus on segmenting the individual tooth as a whole, not internal structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Three-Stage Segmentation Process</head><p>We propose a 3-stage process for the internal tooth segmentation from CBCT images, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Teeth are categorized into 32 classes of incisors, canines, premolars and molars. Each tooth consists of enamel, dentin and pulp. The union of enamel, dentin and pulp is called the whole tooth. Our method performs coarse to fine segmentation based on the following three levels of hierarchy of tooth structures: see Fig. <ref type="figure" target="#fig_0">1(b)</ref>. (1) a CBCT voxel is classified into tooth and non-tooth; (2) teeth is categorized into 32 classes; (3) a whole tooth is classified into enamel, dentin, and pulp.</p><p>Each stage performs the task associated with each level of hierarchy. In Stage 1, a bounding box containing the set of teeth is extracted from CBCT. In Stage 2, 3D patches of individual tooth in 32 classes are extracted. In Stage 3, a tooth patch is segmented into enamel, dentin and pulp structures. We perform a binary segmentation of teeth (versus non-tooth) instead of a simple bounding box regression, considering the importance of extracting accurate bounding boxes. After segmentation, we find a tight bounding box around the teeth set which is then zero-padded for extra margins.</p><p>We use 3D U-Net <ref type="bibr" target="#b1">[2]</ref> for the segmentation of the CBCT image temporarily down-sampled to 128 × 128 × 128 for computational efficiency. Previous methods also proposed to isolate the tooth region, e.g., heuristic thresholding based on Maximum Intensity Projection of CBCT <ref type="bibr" target="#b0">[1]</ref>. Our approach may demand more resources, but leads to improved performance, which we show by experiments. In addition, the decoder layers of C-Net utilize Hierarchical Feature Fusion (HFF) module for effective fusion of the features from P-Net and C-Net, as explained in the next section. DHU-Net is inspired by double U-Net <ref type="bibr" target="#b10">[11]</ref>, however differs from it in several ways: the supervision of P-Net and C-Net outputs with labels at high-and low-level hierarchies, the way input and output of P-Net are combined, and the existence of HFF module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hierarchical Feature Fusion (HFF) Module</head><p>One of the properties that made U-Net successful is the combination of encoder and decoder features through skip connections. In the proposed Hierarchical Feature Fusion (HFF) module, the decoder layers at C-Net combines two encoder features from both hierarchies, i.e., P-Net and C-Net: see Fig. <ref type="figure" target="#fig_2">3(a)</ref>. HFF facilitates the propagation of hierarchical features over the network.</p><p>As shown in Fig. <ref type="figure" target="#fig_2">3</ref>(b), the concept of Channel Attention <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> is used in HFF. Attention vectors are created by mixing pooled features using MLPMixer <ref type="bibr" target="#b19">[20]</ref>. The feature maps are scaled in a channel-wise manner by the attention vectors, and then fused after applying spatial attention. The overall process allows the model to effectively highlight important channel and spatial features from multiple hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function</head><p>The loss function of DHU-Net is given by</p><formula xml:id="formula_0">L total = L P + λ 1 • L C + λ 2 • L FTM<label>(1)</label></formula><p>L P and L C are binary cross-entropy (CE) loss for P-Net and CE + DICE loss for C-Net, respectively. λ 1 and λ 2 are hyperparameters for balancing losses.</p><p>The λ 1 and λ 2 are hyperparameters for balancing losses which are set to 2 and 5, respectively. L FTM is Focal Tree-Min Loss <ref type="bibr" target="#b14">[15]</ref>, a hierarchical loss function encouraging the model to capture hierarchical relationships between the features extracted by P-Net and C-Net, e.g., the features of a whole tooth and its internal structures in Stage 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The dataset consisted of 70 anonymized cases of 3D dental CBCT images collected from the Korea University Anam Hospital. This study was approved by the Institutional Review Board of of the same hospital (IRB No. 2020AN0410). The dimension of CBCT images is 768 × 768 × 576 with the voxel size 0.3 × 0.3 × 0.3 mm 3 . We clipped the intensity values of CBCT images to [-1000, 2500] and applied intensity normalization. All the internal structures of teeth in CBCT images were individually labelled as enamel, dentin, and pulp. The labeling was performed by two experts and cross-checked, with a final inspection performed by a oral &amp; maxillofacial surgeon. The dataset is split in 3:1:1 for train, validation and test with 5-fold nested cross-validation. We use the following metrics: Dice similarity coefficient (DSC), Jaccard index, and Hausdorff distance (HD95). We evaluate the accuracy of tooth identification (in 32 classes) during the patch extraction in Stage 2. We define the metric of detection precision (DP) as DP = |D ∩ G|/|D ∪ G|, where D represents the set of predicted tooth classes in Stage 2, and G represents the ground truth set. All the results are averaged over 10 repetitions of experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>We evaluated the performance of our model for internal tooth segmentation by comparing with two commonly used models in medical segmentation: U-Net <ref type="bibr" target="#b1">[2]</ref> and Attention U-Net <ref type="bibr" target="#b16">[17]</ref>. We consider the cases of two-and three-stage process for baselines. For two stages, baselines perform extraction of tooth patches from the CBCT image in the 1st stage, and internal tooth segmentation from the patch in the 2nd stage. The three-stage process is identical to our model, except that the segmentation networks are replaced by the baseline models.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the segmentation performance of internal tooth structures. Our method outperforms the baselines across all the metrics (comparison of Jaccard is provided in Supplementary Materials). By comparing 2-stage and 3-stage processes for baselines, we observe that the 3-stage process leads to the better performance. This shows the importance of reducing detection errors, i.e., accurate extraction of tooth patches in turn enhances the final segmentation performance. Indeed, 3-stage process improves the DP metric in all cases. In addition, by comparing with 3-stage baselines, we observe that DHU-Net outperforms U-Net and Attention U-Net. The results demonstrate the effectiveness of the hierarchical design of deep learning models for analyzing internal tooth structure. Ablation analysis on some components of DHU-Net, i.e., HFF module and hierarchical loss function, is provided in Supplementary Materials.</p><p>We conducted a qualitative analysis of segmentation results as shown in Fig. <ref type="figure">4</ref>. We found that the U-Net baseline had a problem of missed detection of teeth, while the 2-stage Attention U-Net showed a cut-out problem. Our model resulted in better representations of the root parts of dentin and pulp compared to the 3-stage baselines, perhaps because our model was better at dealing with the problem of similar intensity values of teeth and the jaw bone.</p><p>Next, we evaluate the segmentation performance of the whole tooth, which also is an important problem. Our model provides the prediction of the whole tooth, i.e., we can simply take a union of the predicted enamel, dentin and pulp. We selected state-of-the-art methods for tooth segmentation as baselines: C2FSeg <ref type="bibr" target="#b4">[5]</ref>, MWTNet <ref type="bibr" target="#b0">[1]</ref> and SGANet <ref type="bibr" target="#b15">[16]</ref>. As shown in Table <ref type="table" target="#tab_1">2</ref>, our approach outperformed the baselines, and proved to be effective for segmenting the whole tooth as well.</p><p>We observe that by comparing Table <ref type="table" target="#tab_0">1</ref> and 2, the segmentation performance of whole tooth is higher than that of internal structures. This is reasonable, because the segmentation of finer structures tend to be harder. For example, suppose our model incorrectly classified an enamel voxel as dentin. This does not affect the accuracy of the whole tooth prediction, however, the accuracy of both enamel and dentin predictions will drop in the internal segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we proposed a fully automated segmentation of internal tooth structures, which, to the best of our knowledge, is the first attempt. We proposed a 3-stage process to reduce detection error and overcome difficulties in segmentation and computational complexity. We introduced DHU-Net, a segmentation network capable of effectively learning hierarchical features of tooth structures, demonstrating improved segmentation performance for both the whole tooth and internal structures. Our future work include the segmentation of additional structures from CBCT, such as mandible or maxilla, simultaneously with teeth.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Internal Structure of Tooth and its Hierarchy</figDesc><graphic coords="2,69,30,54,38,285,88,149,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Three-Stage Process of Internal Tooth Segmentation</figDesc><graphic coords="3,79,47,54,14,294,04,152,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Model Architecture</figDesc><graphic coords="4,65,31,54,20,294,04,255,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Stage 2 :</head><label>2</label><figDesc>Tooth Patch Extraction. Individual tooth patches are extracted from the tooth region received from Stage 1. A patch is a 3D bounding box around an individual tooth. To extract patches, a segmentation of tooth into 32 classes is performed. Similar to Stage 1, the purpose of segmentation is precise extraction of patches. The individual tooth patch is created by padding the segmented tooth into size 64 × 64 × 96. We propose Dual-Hierarchy U-Net (DHU-Net) for precise segmentation which leverages hierarchical properties of tooth features aiming at accurate identification and extraction of ROIs. Detailed architecture of DHU-Net is described in Sect. 2.2. Stage 3: Internal Tooth Segmentation. The individual tooth patch is segmented into enamel, dentin and pulp. DHU-Net is again used in Stage 3 for a precise segmentation, which is explained in Sect. 2.2. Design Insights. Our method prunes non-tooth regions from the CBCT in Stage 1, and extracts tight tooth patches in Stage 2. The process reduces falsenegatives (missed detection of teeth), and false-positives (segmenting irrelevant regions), promoting a precise segmentation of internal structures in Stage 3. 2.2 Dual-Hierarchy U-Net (DHU-Net) Dual-Hierarchy U-Net (DHU-Net) consists of two cascaded U-Net which are called Parent Network (P-Net) and Child Network (C-Net) as shown in Fig. 3(a). P-Net (resp. C-Net) learns features of the higher (resp. lower) level of hierarchy. Importantly, both P-Net and C-Net output segmentation maps which are supervised by the labels of corresponding hierarchy: -Stage 2: The P-Net output is supervised by binary (tooth and non-tooth) labels. The C-Net output is supervised by 32-class teeth labels. -Stage 3: The P-Net output is supervised by binary (whole tooth and background) labels. The C-Net output is supervised by the labels of internal structures. The output from P-Net promotes improved segmentation in C-Net as follows. Let I p and I c denote the inputs to P-Net and C-Net respectively, and Z p denote the output feature of P-Net, respectively. The input of C-Net, I c , is defined as I c = I p ⊕ (I p ⊗ σ(Z p )) where ⊕, ⊗ and σ represent concatenation, element-wise multiplication and sigmoid function respectively. I p ⊗ σ(Z p ) is the output from P-Net gated by the segmentation map. Thus, C-Net receives the input with the highlighted ROIs (the entire teeth in Stage 2 or a whole tooth in Stage 3) in addition to the raw input, which facilitates the fine-level segmentation at C-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,85,47,199,13,281,68,362,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of Internal Tooth Segmentation</figDesc><table><row><cell></cell><cell>Enamel</cell><cell></cell><cell>Dentin</cell><cell></cell><cell>Pulp</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>DSC</cell><cell>HD(95%)</cell><cell>DSC</cell><cell>HD(95%)</cell><cell>DSC</cell><cell>HD(95%)</cell><cell>DP(%)</cell></row><row><cell>2-Stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">3D UNet 79.08 ± 0.81 2.74 ± 0.38 82.84 ± 0.85 2.17 ± 0.33 75.91 ± 1.13 2.81 ± 0.29 93.64</cell></row><row><cell cols="8">Att UNet 81.74 ± 1.20 2.11 ± 0.32 84.51 ± 1.10 1.91 ± 0.26 75.54 ± 1.04 2.57 ± 0.34 94.79</cell></row><row><cell>3-Stage</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">3D UNet 83.36 ± 0.62 1.56 ± 0.24 86.42 ± 0.32 1.61 ± 0.12 77.02 ± 0.66 2.58 ± 0.11 96.53</cell></row><row><cell cols="8">Att UNet 83.66 ± 0.24 1.53 ± 0.17 86.00 ± 0.43 1.71 ± 0.19 77.35 ± 0.51 2.49 ± 0.13 97.68</cell></row><row><cell>Ours</cell><cell cols="7">85.65 ± 0.29 1.37 ± 0.26 88.05 ± 0.31 1.45 ± 0.16 78.58 ± 0.38 2.12 ± 0.12 98.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the Whole Tooth Segmentation</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell>Jaccard</cell><cell>HD95</cell></row><row><cell>C2FSeg [5]</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>88.68 ± 1.43 80.59 ± 2.09 3.12 ± 1.45 MWTNet [1] 90.18 ± 1.04 82.62 ± 1.16 2.78 ± 1.38 SGANet [16] 92.16 ± 0.45 86.48 ± 0.78 2.24 ± 0.54 Ours 93.91 ± 0.34 88.67 ± 0.68 1.32 ± 0.30 Fig. 4. Qualitative Analysis of Internal Tooth Segmentation</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was supported by the <rs type="funder">MSIT (Ministry of Science and ICT)</rs>, <rs type="funder">Korea</rs>, under the <rs type="programName">ICT Creative Consilience program</rs> (<rs type="grantNumber">IITP-2020-0-01819</rs>) supervised by the <rs type="institution">IITP (Institute for Information &amp; communications Technology Planning &amp; Evaluation)</rs>, the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Korea government (MSIT)</rs> (No. <rs type="grantNumber">2021R1A2C1007215</rs> and No.<rs type="grantNumber">2022R1A5A1027646</rs>), and the <rs type="funder">Korea Medical Device Development Fund</rs> grant funded by the <rs type="funder">Korea government</rs> (the <rs type="institution">Ministry of Science and ICT</rs>, the <rs type="funder">Ministry of Trade, Industry and Energy</rs>, the <rs type="funder">Ministry of Health &amp; Welfare</rs>, the <rs type="funder">Ministry of Food and Drug Safety</rs>) (Project Number: <rs type="grantNumber">1711195279</rs> , <rs type="grantNumber">RS-2021-KD000009</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9DeP2MG">
					<idno type="grant-number">IITP-2020-0-01819</idno>
					<orgName type="program" subtype="full">ICT Creative Consilience program</orgName>
				</org>
				<org type="funding" xml:id="_RaHEBhQ">
					<idno type="grant-number">2021R1A2C1007215</idno>
				</org>
				<org type="funding" xml:id="_G3Bkbm7">
					<idno type="grant-number">2022R1A5A1027646</idno>
				</org>
				<org type="funding" xml:id="_SvjUdUm">
					<idno type="grant-number">1711195279</idno>
				</org>
				<org type="funding" xml:id="_ApxCAnt">
					<idno type="grant-number">RS-2021-KD000009</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_67.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic segmentation of individual tooth in dental cbct images from tooth surface map by a multi-task fcn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="97296" to="97309" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D U-net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_49" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toothnet: automatic tooth instance segmentation and identification from cone beam ct images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6368" to="6377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clinically applicable artificial intelligence system for dental diagnosis with cbct</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ezhov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">15006</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric segmentation of teeth in cone-beam ct</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ezhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zakirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gusarev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="52" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tooth and alveolar bone segmentation from dental computed tomography images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="196" to="204" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward accurate tooth segmentation from computed tomography images using a hybrid level set model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="27" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Individual tooth segmentation from ct images using level set method with shape and intensity prior</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2406" to="2417" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tooth segmentation from cone-beam ct using graph cut</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hiew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second APSIPA Annual Summit and Conference</title>
		<meeting>the Second APSIPA Annual Summit and Conference<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="272" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Doubleu-net: a deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dental pulp segmentation from cone-beam computed tomography images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fourth International Symposium on Image Computing and Digital Medicine</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="80" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dental segmentation in cone-beam computed tomography images using watershed and morphology operators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kakehbaraei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seyedarabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Zenouz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Signals Sensors</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">119</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tooth instance segmentation from conebeam ct images through point-based detection and gaussian disentanglement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="18327" to="18342" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep hierarchical semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1246" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic graph attention with explicit anatomical association modeling for tooth segmentation from cbct images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3116" to="3127" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Attention u-net: learning where to look for the pancreas</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A symmetric fully convolutional residual network with dcrf for accurate tooth segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="92028" to="92038" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel deep learning system for multi-class tooth segmentation and classification on cone beam computed tomography: a validation study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shaheen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Dentistry</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">103865</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mlp-mixer: an all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24261" to="24272" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cbct image based segmentation method for tooth pulp cavity region extraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dentomaxillofacial Radiol</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20180236</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eca-net: efficient channel attention for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11534" to="11542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cbam: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Individual tooth segmentation from ct images scanned with contacts of maxillary and mandible teeth</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Prog. Biomed</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
