<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner</title>
				<funder ref="#_4zjpf58">
					<orgName type="full">Key Research and Development Program of Shaanxi Province, China</orgName>
				</funder>
				<funder ref="#_89JuMGQ">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_uaqfpVr">
					<orgName type="full">Ningbo Clinical Research Center for Medical Imaging</orgName>
				</funder>
				<funder ref="#_Ay3J5SV">
					<orgName type="full">Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University</orgName>
				</funder>
				<funder ref="#_4EdW8GQ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiwen</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutong</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<settlement>Adelaide</settlement>
									<region>SA</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianpeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziyang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yong</forename><surname>Xia</surname></persName>
							<email>yxia@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Ningbo Institute of Northwestern Polytechnical University</orgName>
								<address>
									<postCode>315048</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="508" to="518"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E77D733409B10D26F1E45AA02B64A0B3</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Prompt learning</term>
					<term>Universal model</term>
					<term>Medical image segmentation Y. Ye and Y. Xie-Contributed equally</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The universal model emerges as a promising trend for medical image segmentation, paving up the way to build medical imaging large model (MILM). One popular strategy to build universal models is to encode each task as a one-hot vector and generate dynamic convolutional layers at the end of the decoder to extract the interested target. Although successful, it ignores the correlations among tasks and meanwhile is too late to make the model 'aware' of the ongoing task. To address both issues, we propose a prompt-driven Universal Segmentation model (UniSeg) for multi-task medical image segmentation using diverse modalities and domains. We first devise a learnable universal prompt to describe the correlations among all tasks and then convert this prompt and image features into a task-specific prompt, which is fed to the decoder as a part of its input. Thus, we make the model 'aware' of the ongoing task early and boost the task-specific training of the whole decoder. Our results indicate that the proposed UniSeg outperforms other universal models and single-task models on 11 upstream tasks. Moreover, UniSeg also beats other pre-trained models on two downstream datasets, providing the community with a high-quality pretrained model for 3D medical image segmentation. Code and model are available at https://github.com/yeerwen/UniSeg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed the remarkable success of deep learning in medical image segmentation. However, although the performance of deep learning models even surpasses the accuracy of human exports on some segmentation tasks, two challenges still persist. (1) Different segmentation tasks are usually tackled separately by specialized networks (see Fig. <ref type="figure" target="#fig_0">1(a)</ref>), leading to distributed research efforts. <ref type="bibr" target="#b1">(2)</ref> Most segmentation tasks face the limitation of a small labeled dataset, especially for 3D segmentation tasks, since pixel-wise 3D image annotation is labor-intensive, time-consuming, and susceptible to operator bias. Train one model on n datasets using task-specific prompts. We use purple to highlight where to add the task-related information.</p><p>Several strategies have been attempted to address both challenges. First, multi-head networks (see Fig. <ref type="figure" target="#fig_0">1(b)</ref>) were designed for multiple segmentation tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>. A typical example is Med3D <ref type="bibr" target="#b3">[4]</ref>, which contains a shared encoder and multiple task-specific decoders. Although they benefit from the encoder parameter-sharing scheme and the rich information provided by multiple training datasets, multi-head networks are less-suitable for multi-task co-training, due to the structural redundancy caused by the requirement of preparing a separate decoder for each task. The second strategy is the multi-class model, which formulates multiple segmentation tasks into a multi-class problem and performs it simultaneously. To achieve this, the CLIP-driven universal model <ref type="bibr" target="#b15">[16]</ref> (see Fig. <ref type="figure" target="#fig_0">1(c</ref>)) introduces the text embedding of all labels as external knowledge, obtained by feeding medical prompts to CLIP <ref type="bibr" target="#b4">[5]</ref>. However, CLIP has limited ability to generalize in medical scenarios due to the differences between natural and medical texts. It is concluded that the discriminative ability of text prompts is weak in different tasks, and it is difficult to help learn task-specific semantic information. The third strategy is dynamic convolution. DoDNet <ref type="bibr" target="#b30">[29]</ref> and its variants <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">25]</ref> present universal models, which can perform different segmentation tasks based on using task encoding and a controller to generate dynamic convolutions (see Fig. <ref type="figure" target="#fig_0">1(d)</ref>). The limitations of these models are two-fold. <ref type="bibr" target="#b0">(1)</ref> Different tasks are encoded as one-hot vectors, which are mutually orthogonal, ignoring the correlations among tasks. (2) The task-related information (i.e., dynamic convolution parameters) is introduced at the end of the decoder. It may be too late for the model to be 'aware' of the ongoing task, making it difficult to decode complex targets.</p><p>In this paper, we propose a prompt-driven Universal Segmentation model (UniSeg) to segment multiple organs, tumors, and vertebrae on 3D medical images with diverse modalities and domains. UniSeg contains a vision encoder, a fusion and selection (FUSE) module, and a prompt-driven decoder. The FUSE module is devised to generate the task-specific prompt, which enables the model to be 'aware' of the ongoing task (see Fig. <ref type="figure" target="#fig_0">1(e)</ref>). Specifically, since prompt learning has a proven ability to represent both task-specific and task-invariant knowledge <ref type="bibr" target="#b24">[24]</ref>, a learnable universal prompt is designed to describe the correlations among tasks. Then, the universal prompt and the features extracted by the vision encoder are fed to the FUSE module to generate task prompts for all tasks. The task-specific prompt is selected according to the ongoing task. Moreover, to introduce the prompt information to the model early, we move the task-specific prompt from the end of the decoder to the start of the decoder (see Fig. <ref type="figure" target="#fig_1">2</ref>). Thanks to both designs, we can use a single decoder and a segmentation head to predict various targets under the supervision of the corresponding ground truths. We collected 3237 volumetric data with three modalities (CT, MR, and PET) and various targets (eight organs, vertebrae, and tumors) from 11 datasets as the upstream dataset. On this dataset, we evaluated our UniSeg model against other universal models, such as DoDNet and the CLIP-driven universal model. We also compared UniSeg to seven advanced single-task models, such as CoTr <ref type="bibr" target="#b26">[26]</ref>, nnFormer <ref type="bibr" target="#b31">[30]</ref>, and nnUNet <ref type="bibr" target="#b11">[12]</ref>, which are trained independently on each dataset. Furthermore, to verify its generalization ability on downstream tasks, we applied the trained UniSeg to two downstream datasets and compared it to other pre-trained models, such as MG <ref type="bibr" target="#b32">[31]</ref>, DeSD <ref type="bibr" target="#b28">[28]</ref>, and UniMiSS <ref type="bibr" target="#b27">[27]</ref>. Our results indicate that UniSeg outperforms all competing methods on 11 upstream tasks and two downstream tasks.</p><p>Our contributions are three-fold: <ref type="bibr" target="#b0">(1)</ref> We design a universal prompt to describe the correlations among different tasks and use it to generate task prompts for all tasks. (2) We utilize the task-related prompt information as the input of the decoder, facilitating the training of the whole decoder, instead of just the last few layers. (3) The proposed UniSeg can be trained on and applied to various 3D medical image tasks with diverse modalities and domains, providing a highquality pre-trained 3D medical image segmentation model for the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>Let {D 1 , D 2 , ..., D N } be N datasets. Here,</p><formula xml:id="formula_0">D i = {X ij , Y ij } ni</formula><p>j=1 represents that the i-th dataset has a total of n i image-label pairs, and X ij and Y ij are the image and the corresponding ground truth, respectively. Straightforwardly, N tasks can be completed by training N models on N datasets, respectively. This solution faces the issues of ( <ref type="formula" target="#formula_3">1</ref>) designing an architecture for each task, (2) distributing research effort, and (3) dropping the benefit of rich information from other tasks. Therefore, we propose a universal framework called UniSeg to solve multiple tasks with a single model, whose architecture was shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoder-Decoder Backbone</head><p>The main architecture of UniSeg is based on nnUNet <ref type="bibr" target="#b11">[12]</ref>, which consists of an encoder and a decoder shared by different tasks. The encoder has six stages, each containing two convolutional blocks, to extract features and gradually reduce the feature resolution. The convolutional block includes a convolutional layer followed by instance normalization and a ReakyReLU activation, and the first convolution layer of each stage is usually set to reduce the resolution with a stride of 2, except for the first stage. To accept the multi-modality inputs, we reform the first convolution layer and set up three different convolution layers to handle the input with one, two, or four channels, respectively. After the encoder process, we obtain the sample-specific features</p><formula xml:id="formula_1">F ∈ R C× D 16 × H 32 × W</formula><p>32 , where C is the number of channels and D, H, and W are the depth, height, and width of the input, respectively. Symmetrically, in each stage of the decoder, the upsampling operation implemented by a transposed convolution layer is applied to the input feature map to improve its resolution and reduce its channel number. The upsampled feature map is concatenated with the output of the corresponding encoder stage and then fed to a convolutional block. After the decoder process, the output of each decoder stage is passed through a segmentation head to predict segmentation maps for deep supervision, which is governed by the sum of the Dice loss and cross-entropy loss. Note that the channel number of multi-scale segmentation maps is set to the maximum number of classes among all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Universal Prompt</head><p>Following the simple idea that everything is correlated, we believe that the correlations among different segmentation tasks must exist undoubtedly, though they are ignored by DoDNet which uses a set of orthogonal and one-hot task codes. Considering the correlations among tasks are extremely hard to handcraft, we propose a learnable prompt called universal prompt to describe them and use that prompt to generate task prompts for all tasks, aiming to encourage interaction and fusion among different task prompts. We define the shape of the universal prompt as</p><formula xml:id="formula_2">F uni ∈ R N × D 16 × H 32 × W</formula><p>32 , where N is the number of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dynamic Task Prompt</head><p>Before building a universal network, figuring out a way to make the model 'aware' of the ongoing task is a must. DoDNet adopts a one-hot vector to encode each task, and the CLIP-driven universal model <ref type="bibr" target="#b15">[16]</ref> uses masked back-propagation to optionally optimize the task-related segmentation maps. By contrast, we first obtain N features by passing the concatenation of F uni and F through three convolutional blocks, shown as follows</p><formula xml:id="formula_3">{F task1 , F task2 , ..., F taskN } = Split(f (cat(F uni , F ))) N , (<label>1</label></formula><formula xml:id="formula_4">)</formula><p>where F taski denotes the prompt features belonging to the i-th task, cat(, ) is a concatenation operation, f (•) denotes the feed forward process, and Split(•) N means splitting features along the channel to obtain N features with the same shape. Then, we select the target features, called task-specific prompt F tp , from {F task1 , F task2 , ..., F taskN } according to the ongoing task. Finally, we concatenate F and selected F tp as the decoder input. In this way, we introduce task-related prior information into the model, aiming to boost the training of the whole decoder rather than only the last few convolution layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Transfer Learning</head><p>After training UniSeg on upstream datasets, we transfer the pre-trained encoderdecoder and randomly initialized segmentation heads to downstream tasks. The model is fine-tuned in a fully supervised manner to minimize the sum of the Dice loss and cross-entropy loss.</p><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Metric</head><p>Datasets. For this study, we collected 11 medical image segmentation datasets as the upstream dataset to train our UniSeg and single-task models. The Liver and Kidney datasets are from LiTS <ref type="bibr" target="#b2">[3]</ref> and KiTS <ref type="bibr" target="#b10">[11]</ref>, respectively. The Hepatic Vessel (HepaV), Pancreas, Colon, Lung, and Spleen datasets are from Medical Segmentation Decathlon (MSD) <ref type="bibr" target="#b0">[1]</ref>. VerSe20 <ref type="bibr" target="#b18">[19]</ref>, Prostate <ref type="bibr" target="#b17">[18]</ref>, BraTS21 <ref type="bibr" target="#b1">[2]</ref>, and AutoPET <ref type="bibr" target="#b7">[8]</ref> datasets have annotations of the vertebrae, prostate, brain tumors, and whole-body tumors, respectively. We used the binary version of the VerSe20 dataset, where all foreground classes are regarded as one class. Moreover, we dropped the samples without tumors in the AutoPET dataset. Meanwhile, We use BTCV <ref type="bibr" target="#b13">[14]</ref> and VS datasets <ref type="bibr" target="#b19">[20]</ref> as downstream datasets to verify the ability of UniSeg to generalize to other medical image segmentation tasks. BTCV contains the annotations of 13 abdominal organs, including the spleen (Sp), right kidney (RKi), left kidney (LKi), gallbladder (Gb), esophagus (Es), liver (Li), stomach (St), aorta(Ao), inferior vena cava (IVC), portal vein and splenic vein (PSV), pancreas (Pa), right adrenal gland (RAG), and left adrenal gland (LAG). The VS dataset contains the annotations of the vestibular schwannoma. More details are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Evaluation Metric. The Dice similarity coefficient (Dice) that measures the overlap region of the segmentation prediction and ground truth is employed to evaluate the segmentation performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Both pre-training on eleven upstream datasets and fine-tuning on two downstream datasets were implemented based on the nnUNet framework <ref type="bibr" target="#b11">[12]</ref>. During pre-training, we adopted the SGD optimizer and set the batch size to 2, the initial learning rate to 0.01, the default patch size to 64 × 192 × 192, and the maximum training epoch to 1000 with a total of 550,000 iterations. Moreover, we adopted a uniform sampling strategy to sample training data from upstream datasets. In the inference stage, we employed the sliding window strategy, in which the shape of the window is the same as the training patch size, to obtain the whole average segmentation map. During fine-tuning, We set the batch size to 2, the initial learning rate to 0.01, the default patch size to 48 × 192 × 192, and the maximum training iterations to 25,000 for all downstream datasets. The sliding window strategy was also employed when inference on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Comparing to Single-Task and Universal Models. Our UniSeg was compared with advanced single-task models and universal models. The former includes UNETR <ref type="bibr" target="#b8">[9]</ref>, nnFormer <ref type="bibr" target="#b31">[30]</ref>, PVTv2-B1 <ref type="bibr" target="#b22">[23]</ref>, CoTr <ref type="bibr" target="#b26">[26]</ref>, UXNet <ref type="bibr" target="#b14">[15]</ref>, Swin UNETR <ref type="bibr" target="#b21">[22]</ref>, and nnUNet <ref type="bibr" target="#b11">[12]</ref>. The latter includes DoDNet <ref type="bibr" target="#b30">[29]</ref>, CLIP  DoDNet, which replaces the one-hot vectors with CLIP embeddings obtained by following <ref type="bibr" target="#b15">[16]</ref>, and CLIP-driven universal model <ref type="bibr" target="#b15">[16]</ref>. For a fair comparison, the maximum training iterations of single-task models on each task are 50,000, and the patch size is 64 × 192 × 192, except for Swin UNETR, whose patch size is 64 × 160 × 160 due to the limitation of GPU memory. The backbones of the competing universal models and our UniSeg are the same. As shown in Table <ref type="table" target="#tab_1">2</ref>, Our UniSeg achieves the highest Dice on eight datasets, beating the second-best models by 1.9%, 0.7%, 0.8%, 0.4%, 0.4%, 1.0%, 0.3%, 1.2% on the Liver, Kidney, HepaV, Pancreas, Colon, Lung, Prostate, and AutoPET datasets, respectively. Moreover, UniSeg also presents superior performance with an average margin of 1.0% and 1.6% on eleven datasets compared to the second-best universal model and single-task model, respectively, demonstrating its superior performance.</p><p>Comparing to Other Pre-trained Models. We compared our UniSeg with advanced unsupervised pre-trained models, such as MG <ref type="bibr" target="#b32">[31]</ref>, SMIT <ref type="bibr" target="#b12">[13]</ref>, UniMiSS <ref type="bibr" target="#b27">[27]</ref>, DeSD <ref type="bibr" target="#b28">[28]</ref>, and GVSL <ref type="bibr" target="#b9">[10]</ref>, and supervised pre-trained models, such as AutoPET and DoDNet <ref type="bibr" target="#b30">[29]</ref>. The former are officially released with different backbones while the latter are trained using the datasets and backbone used in our UniSeg. To verify the benefit of training on multiple datasets, we also report the performance of the models per-trained on AutoPET and BraTS21, respectively. The results in Table <ref type="table" target="#tab_2">3</ref> reveal that almost all pre-trained models achieve performance gains over their baselines, which were trained from scratch. More important, thanks to the powerful baseline and small gap between the pretext and downstream tasks, UniSeg achieves the best performance and competitive performance gains on downstream datasets, demonstrating that it has learned a strong representations ability. Furthermore, another advantage of UniSeg against other unsupervised pre-trained models is that it is more resource-friendly, requiring only one GPU of 11 GB memory for implementation, while unsupervised pre-trained models usually require tremendous computational resources, such as eight and four V100 for UniMiSS and SMIT, respectively.</p><p>Comparison of Different Variants. We attempted three UniSeg variants, including Fixed Prompt, Multiple Prompts, and UniSeg-T, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The results in Table <ref type="table" target="#tab_3">4</ref> suggest that (1) learnable universal prompt is helpful for building valuable prompt features; (2) using one universal prompt instead of multiple task-independent prompts boosts the interaction and fusion among all tasks, resulting in better performance; (3) adding task-related information in advance facilitates handling complex prediction situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This study proposes a universal model called UniSeg (a single model) to perform multiple organs, tumors, and vertebrae segmentation on images with multiple modalities and domains. To solve two limitations existing in preview universal models, we design the universal prompt to describe correlations among all tasks and make the model 'aware' of the ongoing task early, boosting the training of the whole decoder instead of just the last few layers. Thanks to both designs, our UniSeg achieves superior performance on 11 upstream datasets and two downstream datasets, setting a new record. In our future work, we plan to design a universal model that can effectively process multiple dimensional data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Five strategies for multi-task medical image segmentation. (a) Multiple Models: Train n models for n tasks; (b) Multi-head Models: Train the model with one shared encoder and n task-specific decoders; (c) CLIP-driven Model: Train one model on n datasets by masking label-unavailable predictions; (d) Dynamic Convolution: Train one model on n datasets using a one-hot vector as the task-related information; (e) Ours:Train one model on n datasets using task-specific prompts. We use purple to highlight where to add the task-related information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Technical pipeline of our UniSeg, including a vision encoder, FUSE module, and a prompt-driven decoder. The sample-specific features produced by the encoder are concatenated with a learnable universal prompt as the input of the FUSE module. Then the FUSE module produces the task-specific prompt, which enables the model to be 'aware' of the ongoing task.</figDesc><graphic coords="4,80,46,54,02,291,88,149,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Diagram of UniSeg, Fixed Prompt, Multiple Prompts, and UniSeg-T. Fixed Prompt initializes a zero prompt with no update. Multiple Prompts adopts multiple task-specific prompts. UniSeg-T adds the task-related prompt at the end of the decoder. We use purple to highlight where to add the task-related information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Details of eleven upstream datasets and two downstream datasets.</figDesc><table><row><cell>Dataset</cell><cell>Upstream</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results</figDesc><table><row><cell>Method</cell><cell cols="10">Liver Kidney HepaV Pancreas Colon Lung Spleen VerSe20 Prostate BraTS21 AutoPET Mean</cell></row><row><cell>Single-task Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNETR [9]</cell><cell>62.6 69.9</cell><cell>53.8</cell><cell>44.1</cell><cell>6.0</cell><cell>56.0 94.2</cell><cell>86.0</cell><cell>85.3</cell><cell>83.5</cell><cell>62.2</cell><cell>64.0</cell></row><row><cell>nnFormer [30]</cell><cell>70.7 80.0</cell><cell>61.3</cell><cell>57.9</cell><cell>18.8</cell><cell>66.8 92.2</cell><cell>84.3</cell><cell>87.0</cell><cell>82.0</cell><cell>61.0</cell><cell>69.3</cell></row><row><cell>PVTv2-B1 [23]</cell><cell>67.7 83.8</cell><cell>65.1</cell><cell>59.6</cell><cell>39.8</cell><cell>68.5 95.3</cell><cell>84.7</cell><cell>88.5</cell><cell>83.4</cell><cell>61.4</cell><cell>72.5</cell></row><row><cell>CoTr [26]</cell><cell>74.7 85.1</cell><cell>67.2</cell><cell>65.8</cell><cell>33.8</cell><cell>66.9 95.2</cell><cell>87.1</cell><cell>88.0</cell><cell>82.9</cell><cell>58.8</cell><cell>73.2</cell></row><row><cell>UXNet [15]</cell><cell>75.4 82.2</cell><cell>67.3</cell><cell>59.4</cell><cell>39.8</cell><cell>59.5 95.7</cell><cell>87.1</cell><cell>88.8</cell><cell>84.3</cell><cell>68.2</cell><cell>73.4</cell></row><row><cell>Swin UNETR [22]</cell><cell>76.1 81.2</cell><cell>67.1</cell><cell>58.0</cell><cell>42.6</cell><cell>65.7 95.3</cell><cell>86.9</cell><cell>88.3</cell><cell>84.3</cell><cell>64.6</cell><cell>73.6</cell></row><row><cell>nnUNet [12]</cell><cell>77.2 87.5</cell><cell>69.6</cell><cell>68.8</cell><cell>49.0</cell><cell>68.4 96.2</cell><cell>87.2</cell><cell>89.4</cell><cell>84.4</cell><cell>64.6</cell><cell>76.6</cell></row><row><cell>Universal Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLIP DoDNet</cell><cell>62.1 83.6</cell><cell>57.0</cell><cell>53.3</cell><cell>19.6</cell><cell>43.8 51.4</cell><cell>80.2</cell><cell>89.3</cell><cell>83.1</cell><cell>65.6</cell><cell>62.6</cell></row><row><cell cols="2">Universal Model [16] 74.7 80.7</cell><cell>62.2</cell><cell>63.5</cell><cell>52.1</cell><cell>62.1 94.5</cell><cell>74.8</cell><cell>87.6</cell><cell>82.6</cell><cell>60.0</cell><cell>72.3</cell></row><row><cell>DoDNet [29]</cell><cell>76.7 87.2</cell><cell>70.4</cell><cell>70.5</cell><cell>54.6</cell><cell>69.9 96.5</cell><cell>86.1</cell><cell>89.1</cell><cell>83.2</cell><cell>65.3</cell><cell>77.2</cell></row><row><cell>UniSeg</cell><cell>79.1 88.2</cell><cell>71.2</cell><cell>70.9</cell><cell cols="2">55.0 70.9 96.4</cell><cell>86.1</cell><cell>89.7</cell><cell>83.3</cell><cell>69.4</cell><cell>78.2</cell></row></table><note><p>of single-task models and universal models on eleven datasets. We use Dice (%) on each dataset and Mean Dice (%) on all datasets as metrics. The best results on each dataset are in bold.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of self-supervised models and supervised pre-trained models. AutoPET and BraTS21 present the model per-trained on AutoPET and BraTS21 datasets, respectively. We use italicized numbers to indicate the performance gain using pre-trained weights. We repeat all experiments three times and report mean values. DoDNet<ref type="bibr" target="#b30">[29]</ref> 96.4 94.5 89.7 68.3 76.9 96.8 86.5 89.8 87.7 76.1 81.9 73.2 75.2 84.1 +0.9 91.8 +1.1 UniSeg 96.2 94.4 91.6 68.4 77.9 96.7 87.8 90.1 87.6 76.7 83.3 73.4 75.1 84.6 +1.4 92.9 +2.1</figDesc><table><row><cell>Dataset</cell><cell>BTCV</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results of baseline, Fixed Prompt, Multiple Prompts, UniSeg-T, and our UniSeg. The baseline means the performance of our encoder-decoder backbone respectively trained on each dataset. We compare the mean Dice (%) of eleven datasets.</figDesc><table><row><cell cols="6">Method Baseline Fixed Prompt Multi. Prompts UniSeg-T UniSeg</cell></row><row><cell>Dice</cell><cell>76.6</cell><cell>77.4</cell><cell>77.5</cell><cell>76.9</cell><cell>78.2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by the <rs type="funder">Key Research and Development Program of Shaanxi Province, China</rs>, under Grant <rs type="grantNumber">2022GY-084</rs>, in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62171377</rs>, in part by the <rs type="funder">Ningbo Clinical Research Center for Medical Imaging</rs> under Grant <rs type="grantNumber">2021L003</rs> (<rs type="projectName">Open</rs> Project <rs type="grantNumber">2022LYKFZD06</rs>), and in part by the <rs type="funder">Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University</rs> under Grant <rs type="grantNumber">CX2022056</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4zjpf58">
					<idno type="grant-number">2022GY-084</idno>
				</org>
				<org type="funding" xml:id="_89JuMGQ">
					<idno type="grant-number">62171377</idno>
				</org>
				<org type="funded-project" xml:id="_uaqfpVr">
					<idno type="grant-number">2021L003</idno>
					<orgName type="project" subtype="full">Open</orgName>
				</org>
				<org type="funding" xml:id="_Ay3J5SV">
					<idno type="grant-number">2022LYKFZD06</idno>
				</org>
				<org type="funding" xml:id="_4EdW8GQ">
					<idno type="grant-number">CX2022056</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_49.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05735</idno>
		<title level="m">The medical segmentation decathlon</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The RSNA-ASNR-MICCAI BraTS 2021 benchmark on brain tumor segmentation and radiogenomic classification</title>
		<author>
			<persName><forename type="first">U</forename><surname>Baid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04056</idno>
		<title level="m">The liver tumor segmentation benchmark (LiTs)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00625</idno>
		<title level="m">Med3D: transfer learning for 3D medical image analysis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Omni-Seg: a single dynamic network for multi-label renal pathology image segmentation using partially labeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12665</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-organ segmentation over partially labeled datasets with multi-scale feature abstraction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3619" to="3629" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A whole-body FDG-PET/CT dataset with manually annotated tumor lesions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gatidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">601</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric visual similarity learning in 3D medical image selfsupervised pre-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: results of the KiTS19 challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101821</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tringale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_53" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="556" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MICCAI multi-atlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<meeting>MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">D UX-Net: a large Kernel volumetric convnet modernizing hierarchical transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00785</idno>
		<title level="m">Clip-driven universal model for organ segmentation and tumor detection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02098</idno>
		<title level="m">Universal segmentation of 33 anatomies</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MS-Net: multi-site network for improving prostate segmentation with heterogeneous MRI data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2713" to="2724" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VERSE: a vertebrae labelling and segmentation benchmark for multi-detector ct images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sekuboyina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102166</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation of vestibular schwannoma from magnetic resonance imaging: an open annotated dataset and baseline algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shapey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cancer Imaging Archive</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Marginal loss and exclusion loss for partially supervised multi-organ segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">101979</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training of Swin transformers for 3D medical image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20730" to="20740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PVT v2: improved baselines with pyramid vision transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="424" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to prompt for continual learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="139" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tgnet: a task-guided network architecture for multiorgan and tumour segmentation from partially labelled datasets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CoTr: efficiently bridging CNN and transformer for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_16" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UniMiss: universal medical self-supervised learning via breaking dimensionality barrier</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19803-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19803-8_33" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13681</biblScope>
			<biblScope unit="page" from="558" to="575" />
		</imprint>
	</monogr>
	<note>ECCV 2022</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DeSD: self-supervised learning with deep selfdistillation for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="545" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_52" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DoDNet: learning to segment multi-organ and tumors from multiple partially labeled datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">nnFormer: interleaved transformer for volumetric segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03201</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Models genesis. Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101840</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
