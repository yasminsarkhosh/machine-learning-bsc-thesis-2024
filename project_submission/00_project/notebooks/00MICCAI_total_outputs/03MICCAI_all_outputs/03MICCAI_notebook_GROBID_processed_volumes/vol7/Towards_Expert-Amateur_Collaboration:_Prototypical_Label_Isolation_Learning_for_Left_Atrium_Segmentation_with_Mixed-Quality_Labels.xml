<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels</title>
				<funder ref="#_9Ft5858 #_ukh8DHq">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_94MjpeV">
					<orgName type="full">General Research Fund from Research Grant Council of Hong Kong</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangpeng</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Donghuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Healthcare Co</orgName>
								<address>
									<addrLine>Jarvis Lab</addrLine>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Bioengineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Luo</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Healthcare Co</orgName>
								<address>
									<addrLine>Jarvis Lab</addrLine>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raymond Kai-Yu</forename><surname>Tong</surname></persName>
						</author>
						<title level="a" type="main">Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="99" to="109"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">AF4C5AD8A29A9F1BCC975BF574148E87</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Segmentation</term>
					<term>Class Prototype</term>
					<term>Label Noises</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning-based medical image segmentation usually requires abundant high-quality labeled data from experts, yet, it is often infeasible in clinical practice. Without sufficient expert-examined labels, the supervised approaches often struggle with inferior performance. Unfortunately, directly introducing additional data with lowquality cheap annotations (e.g., crowdsourcing from non-experts) may confuse the training. To address this, we propose a Prototypical Label Isolation Learning (PLIL) framework to robustly learn left atrium segmentation from scarce high-quality labeled data and massive low-quality labeled data, which enables effective expert-amateur collaboration. Particularly, PLIL is built upon the popular teacher-student framework. Considering the structural characteristics that the semantic regions of the same class are often highly correlated and the higher noise tolerance in the high-level feature space, the self-ensembling teacher model isolates clean and noisy labeled voxels by exploiting their relative feature distances to the class prototypes via multi-scale voting. Then, the student follows the teacher's instruction for adaptive learning, wherein the clean voxels are introduced as supervised signals and the noisy ones are regularized via perturbed stability learning, considering their large intra-class variation. Comprehensive experiments on the left atrium segmentation benchmark demonstrate the superior performance of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Segmenting the left atrium (LA) from magnetic resonance images (MRI) is critical in treating atrial fibrillation. Recent success of deep learning (DL)-based methods usually requires a large amount of high-quality (HQ) labeled data (termed as Set-HQ). However, since labeling medical images is expertisedemanding and laborious, acquiring massive HQ labeled data from experts is expensive and not always feasible. Without sufficient HQ labels, the DL approaches often struggle with inferior performance. Despite the recent success of semi-supervised learning (SSL) that leverages abundant unlabeled data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, it is still difficult for SSL to accurately propagate label information at the voxel level especially when the HQ labeled data is extremely scarce. Thus, an intuitive cost-efficient alternative is to collect additional labels via cheaper ways, e.g., crowdsourcing from non-experts, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. Unfortunately, the quality of cheap labels is always unsatisfactory. Directly introducing additional data with low-quality (LQ) noisy labels (termed as Set-LQ) may mislead the model training, easily causing performance degradation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. Such a pervasive dilemma poses a challenging yet practical scenario: how to robustly learn segmentation from scarce HQ labeled data and abundant LQ noisy labeled data?</p><p>The existing works on mining LQ labeled data for medical image segmentation can be categorized by two distinct application scenarios: (i) HQ-agnostic, e.g., Set-HQ and Set-LQ are mixed as one dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. TriNet <ref type="bibr" target="#b23">[24]</ref> uses a tri-network that integrates predictions from two peer networks to supervise the third network; PNL <ref type="bibr" target="#b27">[28]</ref> introduces an image-level label quality evaluation module to identify clean labels to tune the network. (ii) HQ-aware, e.g., recruiting experts to obtain a reasonable amount of HQ labeled data and thus Set-HQ and Set-LQ are separate. Such scenario extends SSL <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> to further exploit the potentially useful information of LQ labels, which will be more beneficial when the HQ labeled data is extremely scarce (as detailed in Sect. 3). Luo et al. <ref type="bibr" target="#b9">[10]</ref> proposed to implicitly decouple the learning processes for Set-HQ and Set-LQ using two separate decoders; KDEM <ref type="bibr" target="#b4">[5]</ref> extends <ref type="bibr" target="#b9">[10]</ref> with knowledge distillation and entropy minimization regularization. However, this implicit decoupling strategy is experimentally hard-to-control. Thus, MTCL <ref type="bibr" target="#b17">[18]</ref> estimates the joint distribution matrix between observed and latent true labels to explicitly characterize mislabeled locations for smooth label refurbishment. However, MTCL is based on the class-conditional noise (CCN) assumption that the noise is independent of input features given the true label, which may be impractical <ref type="bibr" target="#b1">[2]</ref>. Considering the clinical practice, we advocate the HQ-aware scenario because: (a) HQ/LQ labeled data can be separated since the sources of medical annotation are usually recorded and acquiring a reasonable amount of HQ labels from radiologists is feasible; (b) the separation may implicitly embed rewarding prior knowledge on discriminating HQ/LQ labeled data into training.</p><p>Tailoring for the HQ-aware scenario, in this work, we propose the Prototypical Label Isolation Learning (PLIL) framework for left atrium segmentation, enabling effective expert-amateur collaboration. Specifically, PLIL is built upon the popular teacher-student framework. Besides the prime supervised signals from HQ labeled data, PLIL robustly exploits the additional LQ labeled data via two steps: (i) Considering the structural characteristics that semantic regions of the same class are often highly correlated and the higher noise tolerance in the high-level feature space <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>, the self-ensembling teacher model isolates clean and noisy labeled voxels by exploiting their relative feature distances to the class prototypes via multi-scale voting. Besides the advantage of explicit spatial isolation, this strategy takes the input features into account, which is more realistic compared to <ref type="bibr" target="#b17">[18]</ref> as the mislabeled voxels often present difficult and ambiguous regions in the image. (ii) Synergistically, the student follows the teacher's instruction for adaptive learning, wherein the clean voxels are further introduced as supervised signals and the noisy ones are especially regularized via perturbed stability learning, considering their vulnerable large intra-class variation in general. Comprehensive experiments on left atrium segmentation under extreme budget settings demonstrate the superior performance of our approach. The ablation study further verifies the effectiveness of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>Our PLIL framework is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. Following the HQ-aware scenario, we have access to scarce expert-examined HQ labeled data</p><formula xml:id="formula_0">S h = x h(i) , y h(i) M i=1</formula><p>that only contains M samples, and abundant non-expert LQ noisy labeled data S l = x l(i) , y l(i) N i=M +1 that consists of N -M (usually M ) samples, where x h(i) , x l(i) ∈ R Ωi denote the images and y h(i) , y l(i) ∈ {0, 1} Ωi×C are the given HQ or LQ label (C denotes the class number). Our goal is to learn segmentation with scarce Set-HQ and abundant Set-LQ by optimizing the following loss function:</p><formula xml:id="formula_1">L = L HQ + λL LQ ,<label>(1)</label></formula><p>where L HQ and L LQ denote the guidance from HQ and LQ labeled data, respectively. λ is a trade-off weight for L LQ , scheduled by the time-dependent ramp-up</p><p>Gaussian function <ref type="bibr" target="#b3">[4]</ref> </p><formula xml:id="formula_2">λ(t) = e -5(1-t tmax ) 2</formula><p>, where t is the current iteration and t max is the maximal iteration. Since our method heavily relies on the manipulation in the feature space, such weighting schedule can reduce the interference of LQ labeled data to the feature space learning at the early training stage. The HQ labeled data provides prime HQ supervised guidance L hs , i.e., L HQ = L hs . Following <ref type="bibr" target="#b20">[21]</ref>, we adopt the cross-entropy loss L ce and Dice loss L dice with equal weights for L hs . To further exploit Set-LQ while alleviating confirmation bias <ref type="bibr" target="#b9">[10]</ref>, we aim to spatially isolate the clean and noisy labeled voxels and make better use of the suspected noisy labeled voxels rather than discarding them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prototypical Label Isolation for Adaptive Learning</head><p>Teacher-Student Architecture. Our framework is built upon the popular teacher-student architecture <ref type="bibr" target="#b14">[15]</ref>, where the student model F s is updated by back-propagation and the teacher F t is updated by the exponential moving average (EMA) weights of the student θ across training steps. Denoting the weights of the teacher model at step t as θt , θt is updated by: θt = α θt-1 + (1α)θ t , where α is the EMA decay rate and empirically set to 0.99 <ref type="bibr" target="#b14">[15]</ref>. As such, the teacher model owns the self-ensembling property <ref type="bibr" target="#b3">[4]</ref>, which can avoid sharp deterioration of the feature quality and thus suits our following prototypical label isolation strategy that appreciates high-quality and smooth embedding space.</p><p>Multi-scale Voting-Based Prototypical Label Isolation. Considering the structural characteristics that the targeted segmentation regions of the same class are often highly correlated and the higher noise tolerance in the high-level feature space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>, our label isolation strategy is inherently motivated by the assumption that for a clean labeled voxel, its features should lie closer to its corresponding class prototype (class-wise feature centroid); otherwise, a potential noisy labeled voxel is suspected. Specifically, we determine whether a voxel-wise label is a clean one by exploiting the relative feature distances to the class prototypes. Considering that different layers perceive the entire image with different perspectives, a multi-scale voting mechanism is introduced. Technically, given a medical scan x l of Set-LQ and its noisy label y l , we denote the last i-th feature map from the teacher model F t as e temp i , which is then upsampled to e i ∈ R H×W ×Z×Li (H, W and D denote height, width and depth of x l , respectively, and L i is the channel number) to be consistent with the size of segmentation mask via trilinear interpolation. Then, we resort to the pseudo label from the teacher model as the "mask" for the target class, which will be utilized to extract the class features. Denoting the teacher's prediction of x l as F t (x l ), the pseudo label corresponds to the class with the maximal posterior probability. Since the HQ labeled data is scarce which makes it hard to obtain confident prediction, Monte Carlo dropout <ref type="bibr" target="#b6">[7]</ref> based model uncertainty is leveraged to calibrate the pseudo label. For the LQ labeled image x l , K stochastic forward inferences through F t are performed with random dropout. Then, the normalized predictive entropy of the mean of the K softmax predictions is regarded as the uncertainty map u <ref type="bibr" target="#b20">[21]</ref>. When the uncertainty u v at voxel v is smaller than a threshold η, i.e., u v &lt; η, this voxel will be used as the final pseudo mask Ft (x l ). As such, at the i-th scale, the object prototype q obj i can be obtained via the masked average pooling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> as:</p><formula xml:id="formula_3">q obj i = v F obj t(v) •p obj t(v) •e i(v) v F obj t(v) •p obj t(v)</formula><p>, where the predicted probabilities of object p obj t(v) from the teacher model weight the contribution of voxel v to prototype generation. Similarly, the background prototype q bg i can be also obtained. Then, the relative feature distances d obj i(v) and d bg i(v) between the feature vector of voxel v and the prototypes are defined as:</p><formula xml:id="formula_4">d obj i(v) = e i(v) -q obj i 2</formula><p>and</p><formula xml:id="formula_5">d bg i(v) = e i(v) -q bg i 2 . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>Intuitively, if the given label y l(v) at voxel v is object (background) yet its feature vector e v lies closer to the background (object) prototype than the object (background) prototype, this voxel will be isolated to the noisy group. Otherwise, it will be selected as the clean labeled one. Formally, the i-th scale determines the clean-label selection mask m i for image x l as:</p><formula xml:id="formula_7">m i(v) = 1[y l(v) = 1] • 1[d obj i(v) &lt; d bg i(v) ] + 1[y l(v) = 0] • 1[d obj i(v) &gt; d bg i(v) ].<label>(3)</label></formula><p>We select the last three scales of features from the teacher model to perform multi-scale voting. Thus, for the final clean-label selection mask,</p><formula xml:id="formula_8">m v = 1 if 3 i m i(v) ≥ 2. The noisy-label selection mask m is the negation of m.</formula><p>Adaptive Learning Scheme for Isolated Voxels. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the additional supervised loss for Set-LQ (L ls ) is applied to the isolated clean labeled voxels, which takes the form of m-masked cross-entropy loss and Dice loss as:</p><formula xml:id="formula_9">L ls = v (m v • L ce,v + m v • L dice,v ). (<label>4</label></formula><formula xml:id="formula_10">)</formula><p>For the noisy group, since it is extremely difficult to perfectly find out the noisy labels, we do not advocate label refinement as in <ref type="bibr" target="#b17">[18]</ref> to avoid additional error propagation. Instead, we regularize the model behavior on these ambiguous noisy voxels via perturbed stability learning <ref type="bibr" target="#b14">[15]</ref>, i.e., encouraging consistent presoftmax predictions between the student and teacher model for the same input with different perturbations ξ and ξ , formulated as:</p><formula xml:id="formula_11">L nsl = v mv F t(v) (x l + ξ) -F s(v) (x l + ξ ) 2 v mv . (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>The design of m-masked stability loss is motivated by the fact that the estimated noisy group correlates with the voxels with large intra-class variation, wherein these voxels often exhibit difficult and ambiguous nature, which potentially have serious instability problem. Besides, compared to <ref type="bibr" target="#b14">[15]</ref>, such a noise-selective stability learning avoids the distraction by the redundant easy regions, considering this loss takes the form of mean squared error (MSE) with the average nature. As such, the LQ loss L LQ in Eq. 1 can be formulated as L LQ = L ls + βL nsl , where β is a tradeoff weight for the two learning manners. By combining L HQ and L LQ , the model can not only receive HQ supervision from the scarce Set-HQ but also adaptively exploit different kinds of productive information in Set-LQ towards effective expert-amateur collaboration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Materials. The left atrium (LA) segmentation dataset <ref type="bibr" target="#b16">[17]</ref> provides 100 3D gadolinium-enhanced magnetic resonance images (GE-MRIs) with expert labels. The images have the isotropic resolution of 0.625 × 0.625 × 0.625 mm 3 . Following the same data preprocessing and split in <ref type="bibr" target="#b20">[21]</ref>, 80 samples are selected for training and the remaining 20 samples for testing. All the images are cropped to the center of the heart region and the intensities are normalized to zero mean and unit variance. We investigate the scenarios of scarce HQ labeled data, where only 4 (5%) or 6 (7.5%) samples are used as Set-HQ and the rest is utilized as non-expert Set-LQ, simulated by the commonly used label corruption scheme <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> including random erosion and dilation with 3-15 voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation and Evaluation Metrics. The framework is based on</head><p>PyTorch using an NVIDIA GeForce RTX 3090 GPU. 3D V-Net <ref type="bibr" target="#b11">[12]</ref> is adopted as the backbone, referring to <ref type="bibr" target="#b20">[21]</ref>. We randomly crop patches of 112 × 112 × 80 voxels as the input and use sliding window strategy with stride of 18 × 18 × 4 voxels for inference. The batch size is set to 4 including 2 labeled samples and 2 unlabeled samples. t max is set to 8,000. K, η and β are empirically set to 8, 0.1 and 0.1. The learning rate is initialized as 0.01 and decayed by multiplication with (1.0t/t max ) 0.9 . Data augmentation, including random flip and rotation, is applied. Four metrics, including Dice, Jaccard, average surface distance (ASD) and 95% Hausdorff distance (95HD), are adopted for comprehensive evaluation. The code will be available at https://github.com/lemoshu/PLIL.</p><p>Comparison Study. The quantitative results are presented in Table <ref type="table" target="#tab_0">1</ref>. H-Sup denotes the supervised baseline that only Set-HQ is utilized, while HL-Sup denotes that Set-HQ and Set-LQ are mixed for supervised learning. We also include recent SSL methods (UAMT <ref type="bibr" target="#b20">[21]</ref>, CPS <ref type="bibr" target="#b2">[3]</ref>, CPCL <ref type="bibr" target="#b19">[20]</ref> and URPC <ref type="bibr" target="#b10">[11]</ref>), HQ-agnostic noisy label learning (NLL) methods (TriNet <ref type="bibr" target="#b23">[24]</ref> and PNL <ref type="bibr" target="#b27">[28]</ref>) and HQ-aware NLL methods (Decoupled <ref type="bibr" target="#b9">[10]</ref> and MTCL <ref type="bibr" target="#b17">[18]</ref>). All the methods are implemented with the same backbone and training protocols to ensure fairness. As observed, H-Sup performs poorly with scarce Set-HQ, yet, HL-Sup even further degrades, implying that our simulated LQ labels have led to serious confirmation bias. Relying on some model assumptions <ref type="bibr" target="#b15">[16]</ref>, SSL methods ignore the LQ labels and exploit the image information only from Set-LQ. Despite effectiveness, it is still difficult for SSL to accurately propagate voxel-level label information when the HQ labeled data is scarce. For the HQ-agnostic methods, TriNet and PNL show effectiveness in alleviating the negative effects brought by the agnostic LQ labels, yet, even fall behind some SSL methods given the violent simulated label noises, revealing that the HQ-agnostic setting may be sub-optimal. For the HQ-aware scenario, Decoupled <ref type="bibr" target="#b9">[10]</ref> and MTCL <ref type="bibr" target="#b17">[18]</ref> perform well under both labeling settings, demonstrating the benefits of HQ-aware strategy. Our PLIL relies on the manipulation in the feature space, more HQ labeled data will help the network learn more discriminative representations towards accurate isolation. As observed in the 6-HQ-sample setting, PLIL achieves the Dice of 87.66%, only 3.59% away from the upper bound trained with all 80 HQ labeled data. Despite less-discriminative features learned under the 4-HQ-sample setting, PLIL can still achieve respectable results, demonstrating its robustness. The impact of varying expert labeling budgets is further illustrated in Fig. <ref type="figure" target="#fig_2">2(c</ref>) while Fig. <ref type="figure" target="#fig_2">2</ref>(a) presents exemplar results of our PLIL and other approaches under the 6-HQ-sample setting. Consistently, the predicted mask of our PLIL fits more accurately with the ground truth. To better understand our method, we visualize the estimated noisy-label selection mask m for a dilated LQ label y l of LA in Fig. <ref type="figure" target="#fig_2">2(b)</ref>, where it can be observed that most dilated regions are wellcharacterized, further demonstrating the efficacy of our label selection strategy.</p><p>Ablation Study and Discussions. To further investigate how our method works, we perform an ablation study under the 4-HQ-sample setting (as presented in Table <ref type="table" target="#tab_1">2</ref>) with the following variants:  <ref type="table" target="#tab_1">2</ref>, the HQ-agnostic input has interfered with the network training and led to obvious performance degradation, showing the efficacy of our separate strategy. We also observe that the arbitration-based multi-scale voting mechanism enables more reliable isolation due to the consideration of different perspectives of the images. When removing L ls , considerable performance degradation can be observed, revealing that our strategy effectively finds out the clean labeled voxels. Besides the productive guidance provided by the isolated clean labeled voxels, the noisy group exploited by the stability learning can further provide informative clues to boost the performance. Empirically, the noisy labeled regions often appear in the challenging areas, which are more sensitive to the perturbations and therefore exploring their perturbed stability during training is rewarding and can enhance the generalizability of the model <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. However, as a methodological study, we only evaluated the methods with the commonly used simulated LQ noisy labels. As observed, the violent simulated noises lead to serious confirmation bias. Some existing NLL methods cannot handle such violent noises well, but some SSL methods, which discard LQ labels, achieve appealing performance. Thus, further clinical validation with real-world amateur noises is an important future work. Besides, to facilitate practical expert-amateur collaboration, we should further consider two intertwined problems in the future: (i) how to cost-efficiently edu-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we proposed a novel Prototypical Label Isolation Learning (PLIL) framework to robustly learn left atrium segmentation from scarce highquality labeled data and massive low-quality labeled data. Taking advantage of our multi-scale voting-based prototypical label isolation and adaptive learning scheme for clean and suspected noisy labeled voxels, our approach can robustly exploit the additional low-quality labeled data (e.g., via cheap crowdsourcing), which enables effective expert-amateur collaboration. Comprehensive experiments on the left atrium segmentation benchmark demonstrated the superior performance of our method as well as the effectiveness of each proposed component.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our prototypical label isolation learning (PLIL) framework for robustly learning segmentation with scarce HQ labeled data and abundant LQ labeled data. m is the estimated clean-label selection mask; m is the noisy-label selection mask.</figDesc><graphic coords="3,64,47,58,91,323,44,127,09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(i) PLIL (HQ-agnostic): mixing Set-HQ and Set-LQ instead of the separate strategy; (ii) w/o multi-scale voting: only utilizing the features before the penultimate convolution for autocratic label isolation; (iii) w/o L ls : removing the m-masked supervised loss for the identified clean labeled group; (iv) w/o L nsl : removing the m-masked (noiseselective) stability loss for the identified noisy labeled group. First, our PLIL is tailored for the HQ-aware scenario. As shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Examples of LA segmentation results with only 6 HQ labeled data. Grey color represents the inconsistency between the prediction and the ground truth (GT). (b) An example of the dilated LQ label y l (white in 3D with fused red GT) and the estimated noisy-label selection mask m (yellow in 3D with fused red GT). (c) Segmentation performances (indicated by Dice score) with varying expert labeling budgets. (Color figure online)</figDesc><graphic coords="8,41,85,416,18,245,59,88,63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison study. Cross-subject standard deviations are shown in parentheses. * indicates p ≤ 0.05 from Wilcoxon signed rank test when comparing ours with the second best under the HQ-aware setting. The best results are in bold.</figDesc><table><row><cell>Methods</cell><cell>Settings</cell><cell></cell><cell></cell><cell>Metrics</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Set-HQ Set-LQ HQ-aware? Dice [%] ↑</cell><cell cols="3">Jaccard [%] ↑ 95HQ [voxel] ↓ ASD [voxel] ↓</cell></row><row><cell cols="2">Sup H-Sup (Upper Bound) 80</cell><cell>0</cell><cell>-</cell><cell>91.25 (1.93)</cell><cell>83.36 (3.24)</cell><cell>6.32 (6.45)</cell><cell>1.50 (0.61)</cell></row><row><cell>H-Sup</cell><cell>4</cell><cell>0</cell><cell>-</cell><cell>77.84 (8.29)</cell><cell>64.02 (9.60)</cell><cell>22.60 (14.57)</cell><cell>5.03 (1.22)</cell></row><row><cell>HL-Sup</cell><cell>4</cell><cell>76</cell><cell>-</cell><cell>77.21 (7.52)</cell><cell>63.47 (9.66)</cell><cell>21.59 (10.72)</cell><cell>6.35 (2.31)</cell></row><row><cell>SSL UAMT [21]</cell><cell>4</cell><cell>76</cell><cell>-</cell><cell>79.88 (9.69)</cell><cell cols="2">67.46 (11.93) 24.11 (14.75)</cell><cell>3.21 (1.56)</cell></row><row><cell>CPS [3]</cell><cell>4</cell><cell>76</cell><cell>-</cell><cell>81.54 (6.56)</cell><cell>69.33 (8.89)</cell><cell>24.54 (16.47)</cell><cell>3.86 (1.08)</cell></row><row><cell>CPCL [20]</cell><cell>4</cell><cell>76</cell><cell>-</cell><cell>82.46 (8.15)</cell><cell cols="2">70.92 (11.02) 21.59 (14.49)</cell><cell>3.23 (1.32)</cell></row><row><cell>URPC [11]</cell><cell>4</cell><cell>76</cell><cell>-</cell><cell>78.41 (8.53)</cell><cell cols="2">65.27 (14.36) 21.74 (15.03)</cell><cell>6.17 (2.33)</cell></row><row><cell>NLL TriNet [24]</cell><cell>4</cell><cell>76</cell><cell>×</cell><cell>80.12 (7.11)</cell><cell>68.11 (8.36)</cell><cell>20.13 (11.74)</cell><cell>4.85 (1.24)</cell></row><row><cell>PNL [28]</cell><cell>4</cell><cell>76</cell><cell>×</cell><cell>78.01 (7.23)</cell><cell>65.01 (8.22)</cell><cell>18.57 (13.05)</cell><cell>4.78 (1.33)</cell></row><row><cell>Decoupled [10]</cell><cell>4</cell><cell>76</cell><cell></cell><cell>80.74 (6.73)</cell><cell>68.23 (7.31)</cell><cell>16.55 (13.54)</cell><cell>4.90 (1.22)</cell></row><row><cell>MTCL [18]</cell><cell>4</cell><cell>76</cell><cell></cell><cell>83.36 (6.12)</cell><cell>71.53 (8.48)</cell><cell>16.81 (14.13)</cell><cell>3.44 (1.51)</cell></row><row><cell>PLIL (ours)</cell><cell>4</cell><cell>76</cell><cell></cell><cell cols="4">84.91 (3.32)* 73.93 (5.07)* 15.49 (12.00) 3.10 (1.31)</cell></row><row><cell>Sup H-Sup</cell><cell>6</cell><cell>0</cell><cell>-</cell><cell>79.41 (4.86)</cell><cell>65.02 (5.11)</cell><cell>24.36 (14.02)</cell><cell>2.78 (1.01)</cell></row><row><cell>HL-Sup</cell><cell>6</cell><cell>74</cell><cell>-</cell><cell>78.09 (4.45)</cell><cell>64.27 (5.78)</cell><cell>13.18 (9.63)</cell><cell>4.14 (0.76)</cell></row><row><cell>SSL UAMT [21]</cell><cell>6</cell><cell>74</cell><cell>-</cell><cell>83.72 (7.15)</cell><cell>73.10 (9.77)</cell><cell>16.44 (14.07)</cell><cell>2.75 (1.10)</cell></row><row><cell>CPS [3]</cell><cell>6</cell><cell>74</cell><cell>-</cell><cell>82.15 (6.89)</cell><cell>70.26 (9.27)</cell><cell>27.61 (15.07)</cell><cell>2.92 (0.90)</cell></row><row><cell>CPCL [20]</cell><cell>6</cell><cell>74</cell><cell>-</cell><cell>83.99 (5.40)</cell><cell>73.55 (7.56)</cell><cell>19.60 (11.80)</cell><cell>2.79 (0.97)</cell></row><row><cell>URPC [11]</cell><cell>6</cell><cell>74</cell><cell>-</cell><cell>80.52 (7.77)</cell><cell>68.14 (9.53)</cell><cell>22.81 (13.67)</cell><cell>6.18 (1.54)</cell></row><row><cell>NLL TriNet [24]</cell><cell>6</cell><cell>74</cell><cell>×</cell><cell>84.82 (3.68)</cell><cell>74.04 (7.29)</cell><cell>15.37 (7.62)</cell><cell>3.01 (1.19)</cell></row><row><cell>PNL [28]</cell><cell>6</cell><cell>74</cell><cell>×</cell><cell>80.05 (4.72)</cell><cell>68.08 (8.53)</cell><cell>17.02 (10.23)</cell><cell>3.58 (0.83)</cell></row><row><cell>Decoupled [10]</cell><cell>6</cell><cell>74</cell><cell></cell><cell>85.01 (3.76)</cell><cell>74.58 (6.43)</cell><cell>12.35 (8.36)</cell><cell>3.37 (1.02)</cell></row><row><cell>MTCL [18]</cell><cell>6</cell><cell>74</cell><cell></cell><cell>86.06 (4.78)</cell><cell>75.73 (7.18)</cell><cell>12.47 (10.06)</cell><cell>2.87 (1.09)</cell></row><row><cell>PLIL (ours)</cell><cell>6</cell><cell>74</cell><cell></cell><cell cols="4">87.66 (2.61) 78.12 (4.15)* 10.93 (9.29)* 2.41 (0.83)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study with 5% HQ labeled data. The best mean results are in bold.cate the amateurs on good medical annotation; (ii) how to automatically perform quality controls for the crowdsourced pixel-level labels<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Metrics</cell><cell></cell></row><row><cell></cell><cell>Dice [%] ↑</cell><cell cols="2">Jaccard [%] ↑ 95HQ [voxel] ↓ ASD [voxel] ↓</cell></row><row><cell>PLIL (HQ-aware)</cell><cell cols="3">84.91 (3.32) 73.93 (5.07) 15.49 (12.00) 3.10 (1.31)</cell></row><row><cell>PLIL (HQ-agnostic)</cell><cell cols="2">79.64 (7.46) 66.76 (9.50) 19.48 (11.01)</cell><cell>6.13 (2.55)</cell></row><row><cell cols="3">w/o multi-scale voting 83.36 (6.83) 72.11 (7.93) 17.36 (11.54)</cell><cell>3.65 (1.77)</cell></row><row><cell>w/o L ls</cell><cell cols="2">80.65 (9.36) 68.47 (11.53) 23.11 (13.66)</cell><cell>3.12 (1.11)</cell></row><row><cell>w/o L nsl</cell><cell cols="2">83.22 (7.36) 71.86 (9.59) 19.69 (14.26)</cell><cell>3.26 (1.67)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 H. Greenspan et al. (Eds.): MICCAI 2023, LNCS 14226, pp. 99-109, 2023. https://doi.org/10.1007/978-3-031-43990-2_10</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This research was done with <rs type="institution">Tencent Jarvis Lab and Tencent Healthcare (Shenzhen) Co</rs>., LTD and supported by <rs type="funder">General Research Fund from Research Grant Council of Hong Kong</rs> (No. <rs type="grantNumber">14205419</rs>) and the <rs type="funder">National Key R&amp;D Program of China</rs> (No. <rs type="grantNumber">2020AAA0109500</rs> and No. <rs type="grantNumber">2020AAA0109501</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_94MjpeV">
					<idno type="grant-number">14205419</idno>
				</org>
				<org type="funding" xml:id="_9Ft5858">
					<idno type="grant-number">2020AAA0109500</idno>
				</org>
				<org type="funding" xml:id="_ukh8DHq">
					<idno type="grant-number">2020AAA0109501</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-322" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="225" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond class-conditional assumption: A primary attempt to combat instance-dependent label noise</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11442" to="11450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised brain lesion segmentation with an adapted mean teacher model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20351-1_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-20351-1" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2019</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11492</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teach me to segment with mixed supervision: confident students become masters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78191-040" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12729</biblScope>
			<biblScope unit="page" from="517" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint class-affinity loss correction for robust medical image segmentation with noisy labels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-856" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="588" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">What uncertainties do we need in Bayesian deep learning for computer vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04977</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Agreement between experts and an untrained crowd for identifying dermoscopic features using a gamified app: reader feasibility study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR Med. Inform</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">38412</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Superpixel-guided iterative learning from noisy labels for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-250" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="525" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation via strong-weak dualbranch network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58558-7_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58558-7" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12350</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-330" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DAT: training deep networks robust to label-noise by matching the feature distributions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6821" to="6829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Van Engelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="373" to="440" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101832</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Noisy labels are treasure: mean-teacher-assisted confident learning for hepatic vessel segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_1</idno>
		<idno>978- 3-030-87193-2 1</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ambiguity-selective consistency regularization for mean-teacher semisupervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">102880</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">All-around real label supervision: cyclic prototype consistency learning for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3174" to="3184" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-867" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Characterizing label errors: confident learning for noisy-labeled image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_70</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-870" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="721" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust medical image segmentation from non-expert annotations with tri-network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_25</idno>
		<idno>978-3-030-59719-1 25</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="249" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SG-One: similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3855" to="3865" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distilling effective supervision from severe label noise</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9294" to="9303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning with noisy labels via sparse regularization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pick-and-learn: automatic quality evaluation for noisylabeled image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32226-764" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
