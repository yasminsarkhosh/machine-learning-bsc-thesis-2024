<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lanzhuju</forename><surname>Mei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai United Imaging Intelligence Co. Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai United Imaging Intelligence Co. Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiming</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Deng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Shanghai Ninth People&apos;s Hospital</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nizhuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuming</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiqiang</forename><surname>Zhan</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai United Imaging Intelligence Co. Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai United Imaging Intelligence Co. Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maurizio</forename><surname>Tonetti</surname></persName>
							<email>maurizio.tonetti@ergoperio.eu</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Shanghai Ninth People&apos;s Hospital</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai United Imaging Intelligence Co. Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Shanghai Clinical Research and Trial Center</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="54" to="63"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">08367679C57B54B49FB678E08C184DAF</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate periodontal disease classification from panoramic X-ray images is of great significance for efficient clinical diagnosis and treatment. It has been a challenging task due to the subtle evidence in radiography. Recent methods attempt to estimate bone loss on these images to classify periodontal diseases, relying on the radiographic manual annotations to supervise segmentation or keypoint detection. However, these radiographic annotations are inconsistent with the clinical golden standard of probing measurements and thus can lead to measurement errors and unstable classifications. In this paper, we propose a novel hybrid classification framework, HC-Net, for accurate periodontal disease classification from X-ray images, which consists of three components, i.e., tooth-level classification, patient-level classification, and a learnable adaptive noisy-OR gate. Specifically, in the tooth-level classification, we first introduce instance segmentation to capture each tooth, and then classify the periodontal disease in the tooth level. As for the patient level, we exploit a multi-task strategy to jointly learn patientlevel classification and classification activation map (CAM) that reflects the confidence of local lesion areas upon the panoramic X-ray image. Eventually, the adaptive noisy-OR gate obtains a hybrid classification by integrating predictions from both levels. Extensive experiments on the dataset collected from real-world clinics demonstrate that our proposed HC-Net achieves state-of-the-art performance in periodontal disease classification and shows great application potential. Our code is available at https://github.com/ShanghaiTech-IMPACT/Periodental_Disease.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Periodontal disease is a set of inflammatory gum infections damaging the soft tissues in the oral cavity, and one of the most common issues for oral health <ref type="bibr" target="#b3">[4]</ref>.</p><p>If not diagnosed and treated promptly, it can develop into irreversible loss of the bone and tissue that support the teeth, eventually causing tooth loosening or even falling out. Thus, it is of great significance to accurately classify periodontal disease in an early stage. However, in clinics, dentists have to measure the clinical attachment loss (CAL) of each tooth by manual probing, and eventually determine the severity and progression of periodontal disease mainly based on the most severe area <ref type="bibr" target="#b13">[14]</ref>. This is excessively time-consuming, laborious, and over-dependent on the clinical experience of experts. Therefore, it is essential to develop an efficient automatic method for accurate periodontal disease diagnosis from radiography, i.e., panoramic X-ray images.</p><p>With the development of computer techniques, computer-aided diagnosis has been widely applied for lesion detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and pathological classification <ref type="bibr" target="#b9">[10]</ref> in medical image analysis. However, periodontal disease diagnosis from panoramic X-ray images is a very challenging task. While reliable diagnosis can only be provided from 3D probing measurements of each tooth (i.e. clinical golden standard), evidence is highly subtle to be recognized from radiographic images. Panoramic X-ray images make it even more difficult with only 2D information, along with severe tooth occlusion and distortion. Moreover, due to this reason, it is extremely hard to provide confident and consistent radiographic annotations on these images, even for the most experienced experts. Many researchers have already attempted to directly measure radiographic bone loss from panoramic X-ray images for periodontal disease diagnosis. Chang et al. <ref type="bibr" target="#b1">[2]</ref> employ a multi-task framework to simulate clinical probing, by detecting bone level, cementoenamel junction (CEJ) level, and tooth long axis. Jiang et al. <ref type="bibr" target="#b6">[7]</ref> propose a two-stage network to calculate radiographic bone loss with tooth segmentation and keypoint object detection. Although these methods provide feasible strategies, they still rely heavily on radiographic annotations that are actually not convincing. These manually-labeled landmarks are hard to accurately delineate and usually inconsistent with clinical diagnosis by probing measurements. For this reason, the post-estimated radiographic bone loss is easily affected by prediction errors and noises, which can lead to incorrect and unstable diagnosis.</p><p>To address the aforementioned challenges and limitations of previous methods, we propose HC-Net, a novel hybrid classification framework for automatic periodontal disease diagnosis from panoramic X-ray images, which significantly learns from clinical probing measurements instead of any radiographic manual annotations. The framework learns upon both tooth-level and patient-level with three major components, including tooth-level classification, patient-level classification, and an adaptive noisy-OR gate. Specifically, tooth-level classification first applies tooth instance segmentation, then extracts features from each tooth and predicts a tooth-wise score. Meanwhile, patient-level classification provides patient-wise prediction with a multi-task strategy, simultaneously learning a classification activation map (CAM) to show the confidence of local lesion areas upon the panoramic X-ray image. Most importantly, a learnable adaptive noisy-OR gate is designed to integrate information from both levels, with the tooth-level scores and patient-level CAM. Note that our classification is only supervised by the clinical golden standard, i.e., probing measurements. We provide comprehensive learning and integration on both tooth-level and patient-level classification, eventually contributing to confident and stable diagnosis. Our proposed HC-Net is validated on the dataset from real-world clinics. Experiments have demonstrated the outstanding performance of our hybrid structure for periodontal disease diagnosis compared to state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>An overview of our proposed framework, HC-Net, is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We first formulate our task (Sect. 2.1), and then elaborate the details of tooth-level classification (Sect. 2.2), patient-level classification (Sect. 2.3), and adaptive noisy-OR gate (Sect. 2.4), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Formulation and Method Overview</head><p>In this paper, we aim to classify each patient into seriously periodontal (including periodontitis stage II-IV) or not (including health, gingivitis, and periodontitis stage I), abbreviated below as 'positive' or 'negative'. We collect a set of panoramic X-ray images X</p><formula xml:id="formula_0">= {X 1 , X 2 , . . . , X N } with their patient-level labels Y = {Y 1 , Y 2 , . . . , Y N } from clinical diagnosis,</formula><p>where Y i ∈ {0, 1} indicates whether the i-th patient is negative (0) or positive <ref type="bibr" target="#b0">(1)</ref>. For the i-th patient, we acquire corresponding tooth-level labels T i = {T 1 i , T 2 i , . . . , T Ki i } from clinical golden standard, where K i denotes the number of teeth, and T j i ∈ {0, 1} indicates whether the j-th tooth of the i-th patient is positive or negative.</p><p>Briefly, our goal is to build a learning-based framework to predict the probability P i ∈ [0, 1] of the i-th patient from panoramic X-ray image. An intuitive solution is to directly perform patient-level classification upon panoramic X-ray images. However, it fails to achieve stable and satisfying results (See Sect. 3.2), mainly for the following two reasons. Firstly, evidence is subtle to be recognized in the large-scale panoramic X-ray image (notice that clinical diagnosis relies on tedious probing around each tooth). Secondly, as we supervise the classification with clinical golden standard (i.e., probing measurements), a mapping should be well designed and established from radiography to this standard, since the extracted discriminative features based on radiography may not be well consistent with the golden standard. Therefore, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we propose a novel hybrid classification framework to learn upon both tooth-level and patient-level, and a learnable adaptive noisy-OR gate that integrates the predictions from both labels and returns the final classification (i.e., positive or negative).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tooth-Level Classification</head><p>Given the panoramic X-ray image of the i-th patient, we propose a two-stage structure for tooth-level classification, which first captures each tooth with tooth instance segmentation, and then predicts the classification of each tooth. Tooth instance segmentation aims to efficiently detect each tooth with its centroid, bounding box, and mask, which are later used to enhance tooth-level learning. It introduces a detection network with Hourglass <ref type="bibr" target="#b11">[12]</ref> as the backbone, followed by three branches, including tooth center regression, bounding box regression, and tooth semantic segmentation. Specifically, the first branch generates tooth center heatmap H. We obtain the filtered heatmap H to get center points for each tooth, by a kernel that retains the peak value for every 8-adjacent, described as</p><formula xml:id="formula_1">H pc = H pc , if H pc ≥ H pj , ∀p j ∈ p 0, otherwise ,<label>(1)</label></formula><p>where we denote p = {p c + e i } 8 i=1 as the set of 8-adjacent, where p c is the center point and {e i } 8 i=1 is the set of direction vectors. The second branch then uses the center points and image features generated by the backbone to regress the bounding box offsets. The third branch utilizes each bounding box to crop the original panoramic X-ray image and segment each tooth. Eventually, with the image patch and corresponding mask A j i for the j-th tooth of the i-th patient, we employ a classification network (i.e., feature extractor and MLP) to predict the probability T j i , if the tooth being positive. To train the tooth-level framework, we design a multi-term objective function to supervise the learning process. Specifically, for tooth center regression, we employ the focal loss of <ref type="bibr" target="#b15">[16]</ref> to calculate the heatmap error, denoted as L ctr . For bounding box regression, we utilize L1 loss to calculate the regression error, denoted as L bbx . For tooth semantic segmentation, we jointly compute the cross-entropy loss and dice loss, denoted as L seg = 0.5 × (L segCE + L segDice ). We finally supervise the tooth-level classification with a cross-entropy loss, denoted as L clst . Therefore, the total loss of the tooth-level classification is formulated as L tooth = L ctr +0.1×L bbx +L seg +L clst .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Patient-Level Classification</head><p>As described in Sect. 2.1, although patient-level diagnosis is our final goal, direct classification is not a satisfying solution, and thus we propose a hybrid classification network on both tooth-level and patient-level. Additionally, to enhance patient-level classification, we introduce a multi-task strategy that simultaneously predicts the patient-level classification and a classification activation map (CAM). The patient-level framework first utilizes a backbone network to extract image features for its following two branches. One branch directly determines whether the patient is positive or negative through an MLP, which makes the extracted image features more discriminative. We mainly rely on the other branch, which transforms the image features into CAM to provide local confidence upon the panoramic X-ray image.</p><p>Specifically, for the i-th patient, with the predicted area {A j i } Ki j=1 of each tooth and the CAM M i , the intensity I of the j-th tooth can be obtained, described as</p><formula xml:id="formula_2">I j i = C(M i , A j i ),<label>(2)</label></formula><p>where C(•, * ) denotes the operation that crops • with the area of * . To supervise the CAM, we generate a distance map upon the panoramic X-ray image, based on Euclidean Distance Transform with areas of positive tooth masks. In this way, we train patient-level classification in a multi-task scheme, jointly with direct classification and CAM regression, which increases the focus on possible local areas of lesions and contributes to accurate classification. We introduce two terms to train the patient-level framework, including a cross-entropy loss L clsp to supervise the classification, and a mean squared loss L CAM to supervise the regression for CAM. Eventually, the total loss of the patient-level classification is</p><formula xml:id="formula_3">L patient = L clsp + L CAM .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learnable Adaptive Noisy-OR Gate</head><p>We finally present a learnable adaptive noisy-OR gate <ref type="bibr" target="#b12">[13]</ref> to integrate toothlevel classification and patient-level classification. To further specify the confidence of local lesion areas on CAM, we propose to learn dummy probabilities D j i for each tooth with its intensity</p><formula xml:id="formula_4">I j i D j i = Φ(I j i ),<label>(3)</label></formula><p>where Φ denotes the pooling operation.</p><p>In this way, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we have obtained tooth-wise probabilities predicted from both tooth-level (i.e., probabilities { T j i } Ki j=1 ) and patient-level (i.e., dummy probabilities {D j i } Ki j=1 ). We then formulate the final diagnosis as hybrid classification, by designing a novel learnable adaptive noisy-OR gate to aggregate these probabilities, described as</p><formula xml:id="formula_5">Ỹi = 1 - j∈Gi D j i (1 -T j i ), (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where Ỹi is the final prediction of the i-th patient, G i is the subset of tooth numbers. We employ the binary cross entropy loss L gate to supervise the learning of adaptive noisy-OR Gate. Eventually, the total loss L of our complete hybrid classification framework is formulated as</p><formula xml:id="formula_7">L = L tooth + L patient + L gate .</formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metrics</head><p>To evaluate our framework, we collect 426 panoramic X-ray images of different patients from real-world clinics, with the same size of 2903 × 1536. Each patient has corresponding clinical records of golden standard, measured and diagnosed by experienced experts. We randomly split these 426 scans into three sets, including 300 for training, 45 for validation, and 81 for testing. To quantitatively evaluate the classification performance of our method, we report the following metrics, including accuracy, F1 score, and AUROC. Accuracy directly reflects the performance of classification. F1 score further supports the accuracy with the harmonic mean of precision and recall. AUROC additionally summarizes the performance over all possible classification thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with Other Methods</head><p>We mainly compare our proposed HC-Net with several state-of-the-art classification networks, which can be adapted for periodontal disease diagnosis.</p><p>ResNet <ref type="bibr" target="#b4">[5]</ref>, DenseNet <ref type="bibr" target="#b5">[6]</ref>, and vision transformer <ref type="bibr" target="#b2">[3]</ref> are three of the most representative classification methods, which are used to perform patient-level classification as competing methods. We implement TC-Net as an approach for toothlevel classification, which extracts features respectively from all tooth patches, and all features are concatenated together to directly predict the diagnosis. Moreover, we notice the impressive performance of the multi-task strategy in medical imaging classification tasks <ref type="bibr" target="#b14">[15]</ref>, and thus adopt MTL <ref type="bibr" target="#b0">[1]</ref> to perform multi-task learning scheme. Note that we do not include <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> in our comparisons, as they do not consider the supervision by golden standard and heavily rely on unconvincing radiographic manual annotations, which actually cannot be applied in clinics. We employed the well-studied CenterNet <ref type="bibr" target="#b15">[16]</ref> for tooth instance segmentation, achieving promising detection (mAP50 of 93%) and segmentation (DICE of 91%) accuracy. As shown in Table <ref type="table" target="#tab_0">1</ref>, our HC-Net outperforms all other methods by a large margin. Compared to the patient-level classification methods (such as, ResNet <ref type="bibr" target="#b4">[5]</ref>, DenseNet <ref type="bibr" target="#b5">[6]</ref> and transformer-based xViTCOS <ref type="bibr" target="#b10">[11]</ref>) and the toothlevel classification method (TC-Net), MTL <ref type="bibr" target="#b0">[1]</ref> achieves better performance and robustness in terms of all metrics, showing the significance of learning from both levels with multi-task strategy. Compared to MTL, we exploit the multi-task strategy with CAM in the patient-level, and design an effective adaptive noisy-OR gate to integrate both levels. Although the DeLong test doesn't show a significant difference, the boosting of all metrics (e.g., accuracy increase from 87.65% to 92.59%) demonstrates the contributions of our better designs that can aggregate both levels more effectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>We conduct ablative experiments to validate the effectiveness of each module in HC-Net, including patient-level multi-task strategy with classification activation map (CAM) and hybrid classification with adaptive noisy-OR gate. We first define the baseline network, called B-Net, with only patient-level classification. Then, we enhance B-Net with the multi-task strategy, denoted as M-Net, which involves CAM for joint learning on the patient level. Eventually, we extend B-Net to our full framework HC-Net, introducing the tooth-level classification and the adaptive noisy-OR gate.</p><p>Effectiveness of Multi-task Strategy with CAM. We mainly compare M-Net to B-Net to validate the multi-task strategy with CAM. We show the classification activation area of both methods as the qualitative results in Fig. <ref type="figure" target="#fig_1">2</ref>. Obviously, the activation area of B-Net is almost evenly distributed, while M-Net concentrates more on the tooth area. It shows great potential in locating evidence on local areas of the large-scale panoramic X-ray image, which discriminates the features to support classification. Eventually, it contributes to more accurate qualitative results, as shown in Table <ref type="table" target="#tab_1">2</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>.  Effectiveness of Hybrid Classification with Noisy-OR Gate. We eventually utilize hybrid classification with adaptive noisy-OR Gate, comparing our full framework HC-Net to M-Net. In Table <ref type="table" target="#tab_1">2</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>, we observe that all metrics are dramatically improved. Specifically, the accuracy and F1 score are boosted from 90.12% and 91.67%, to 92.59% and 93.61%, respectively. Note that the AUROC is also significantly increased to 95.81%, which verifies that hybrid classification with noisy-OR gate can improve both the accuracy and robustness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Our framework is implemented based on the PyTorch platform and is trained with a total of 200 epochs on the NVIDIA A100 GPU with 80GB memory. The feature extractors are based on DenseNet <ref type="bibr" target="#b5">[6]</ref>. We use the Adam optimizer with the initial learning rate of 0.001, which is divided by 10 every 50 epochs. Note that in the learnable noisy-or gate, we utilize the probabilities of the top 3 teeth to make predictions for the final outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a hybrid classification network, HC-Net, for automatic periodontal disease diagnosis from panoramic X-ray images. In tooth-level, we introduce instance segmentation to help extract features for tooth-level classification. In patient-level, we adopt the multi-task strategy that jointly learns the patientlevel classification and CAM. Eventually, a novel learnable adaptable noisy-OR gate integrates both levels to return the final diagnosis. Notice that we significantly utilize the clinical golden standard instead of unconvincing radiographic annotations. Extensive experiments have demonstrated the effectiveness of our proposed HC-Net, indicating the potential to be applied in real-world clinics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of our HC-Net.</figDesc><graphic coords="3,43,29,54,38,337,33,237,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of classification activation maps to validate the effectiveness of multi-task strategy with CAM. The first and second columns are visualized respectively from B-Net and M-Net.</figDesc><graphic coords="7,56,31,359,39,311,23,124,93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the ROC curves for the comparison and ablation.</figDesc><graphic coords="8,55,98,416,96,340,21,140,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison with representative classification methods for periodontal disease classification.</figDesc><table><row><cell>Method</cell><cell cols="3">Accuracy (%) F1 Score (%) AUROC (%)</cell></row><row><cell>ResNet [5]</cell><cell>80.25</cell><cell>83.33</cell><cell>91.24</cell></row><row><cell cols="2">DenseNet [6] 86.42</cell><cell>87.36</cell><cell>93.30</cell></row><row><cell>TC-Net</cell><cell>85.19</cell><cell>87.50</cell><cell>93.49</cell></row><row><cell cols="2">xViTCOS [11] 86.42</cell><cell>88.17</cell><cell>94.24</cell></row><row><cell>MTL [1]</cell><cell>87.65</cell><cell>89.80</cell><cell>95.12</cell></row><row><cell>HC-Net</cell><cell>92.59</cell><cell>93.61</cell><cell>95.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison for ablation study.</figDesc><table><row><cell>B-Net 86.42</cell><cell>87.36</cell><cell>93.30</cell></row><row><cell>M-Net 90.12</cell><cell>91.67</cell><cell>95.37</cell></row><row><cell>HC-Net 92.59</cell><cell>93.61</cell><cell>95.81</cell></row></table><note><p>Method Accuracy (%) F1 Score (%) AUROC (%)</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-task learning for detection and classification of cancer in screening mammography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Sainz De Cea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Diedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bakalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Richmond</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_24" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="241" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning hybrid method to automatically diagnose periodontal bone loss and stage periodontitis</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prevalence of periodontitis in adults in the united states: 2009 and</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Eke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Dye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">O</forename><surname>Thornton-Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Genco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Dent. Res</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="914" to="920" />
			<date type="published" when="2010">2010. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">A two-stage deep learning architecture for radiographic assessment of periodontal bone loss</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeNTNet: deep neural transfer network for the detection of periodontal bone loss using panoramic dental radiographs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for the radiographic detection of periodontal bone loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image analysis and machine learning in digital pathology: challenges and opportunities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="170" to="175" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">xViTCOS: explainable vision transformer based COVID-19 screening using radiography</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prathosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Trans. Eng. Health Med</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46484-8_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46484-8_29" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A generalization of the noisy-or model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="208" to="215" />
			<date type="published" when="1993">1993</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Staging and grading of periodontitis: framework and proposal of a new classification and case definition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Tonetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kornman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Periodontol</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="159" to="S172" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-task deep learning for medical image computing and analysis: a review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page">106496</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
