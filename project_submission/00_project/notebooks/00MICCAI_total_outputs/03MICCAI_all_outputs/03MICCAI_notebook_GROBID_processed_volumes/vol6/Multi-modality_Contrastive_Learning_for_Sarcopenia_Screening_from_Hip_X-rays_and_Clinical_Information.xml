<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information</title>
				<funder ref="#_jBZKMNh #_pd72GuN">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_8a2T8j7">
					<orgName type="full">National Key Technology R&amp;D Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiangguo</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changjiang</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Cui</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Information Technology</orgName>
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changming</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">CSIRO Data61</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu-Wei</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Orthopedics</orgName>
								<orgName type="institution" key="instit1">Wan Fang Hospital</orgName>
								<orgName type="institution" key="instit2">Taipei Medical University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi-Jie</forename><surname>Kuo</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Orthopedics</orgName>
								<orgName type="institution" key="instit1">Wan Fang Hospital</orgName>
								<orgName type="institution" key="instit2">Taipei Medical University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Orthopedics</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="department" key="dep3">College of Medicine</orgName>
								<orgName type="institution">Taipei Medical University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Xuan</surname></persName>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Shantou University</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leilei</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ran</forename><surname>Su</surname></persName>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">School of Computer Software</orgName>
								<orgName type="department" key="dep2">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leyi</forename><surname>Wei</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Shandong University</orgName>
								<address>
									<settlement>Shandong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Henry</forename><forename type="middle">B L</forename><surname>Duh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Information Technology</orgName>
								<orgName type="institution">La Trobe University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-Pin</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Orthopedics</orgName>
								<orgName type="institution" key="instit1">Wan Fang Hospital</orgName>
								<orgName type="institution" key="instit2">Taipei Medical University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of Orthopedics</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="department" key="dep3">College of Medicine</orgName>
								<orgName type="institution">Taipei Medical University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="85" to="94"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F1009C296947C016CF6030C40946675C</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sarcopenia screening</term>
					<term>Contrastive learning</term>
					<term>Multi-modality feature fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sarcopenia is a condition of age-associated muscle degeneration that shortens the life expectancy in those it affects, compared to individuals with normal muscle strength. Accurate screening for sarcopenia is a key process of clinical diagnosis and therapy. In this work, we propose a novel multi-modality contrastive learning (MM-CL) based method that combines hip X-ray images and clinical parameters for sarcopenia screening. Our method captures the longrange information with Non-local CAM Enhancement, explores the correlations in visual-text features via Visual-text Feature Fusion, and improves the model's feature representation ability through Auxiliary contrastive representation. Furthermore, we establish a large in-house dataset with 1,176 patients to validate the effectiveness of multi-modality based methods. Significant performances with an AUC of 84.64%, ACC of 79.93%, F1 of 74.88%, SEN of 72.06%, SPC of 86.06%, and PRE of 78.44%, show that our method outperforms other singlemodality and multi-modality based methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sarcopenia is a progressive and skeletal muscle disorder associated with loss of muscle mass, strength, and function <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">8]</ref>. The presence of sarcopenia increases the risk of hospitalization and the cost of care during hospitalization. A systematic analysis of the world's population showed that the prevalence of sarcopenia is approximately 10% in healthy adults over the age of 60 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">13]</ref>. However, the development of sarcopenia is insidious, without overt symptoms in the early stages, which means that the potential number of patients at risk for adverse outcomes is very high. Thus, early identification, screening, and diagnosis are of great necessity to improve treatment outcomes, especially for elderly people.</p><p>The development of effective, reproducible, and cost-effective algorithms for reliable quantification of muscle mass is critical for diagnosing sarcopenia. However, automatically identifying sarcopenia is a challenging task due to several reasons. First, the subtle contrast between muscle and fat mass in the leg region makes it difficult to recognize sarcopenia from X-ray images. Second, although previous clinical studies <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11]</ref> show that patient information, such as age, gender, education level, smoking and drinking status, physical activity (PA), and body mass index (BMI), is crucial for correct sarcopenia diagnosis, there is no generalizable standard. It is of great importance to develop a computerized predictive model that can fuse and mine diagnostic features from heterogeneous hip X-rays and tabular data containing patient information. Third, the number of previous works on sarcopenia diagnosis is limited, resulting in limited usable data.</p><p>Deep learning attracted intensive research interests in various medical diagnosis domains <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. For instance, Zhang et al. <ref type="bibr" target="#b19">[19]</ref> proposed an attention residual learning CNN model (ARLNet) for skin lesion classification to leverage multiple ARL blocks to tackle the challenge of data insufficiency, inter-class similarities, and intra-class variations. For multi-modality based deep learning, PathomicFusion (PF) <ref type="bibr" target="#b2">[3]</ref> fused multimodal histology images and genomic (mutations, CNV, and RNA-Seq) features for survival outcome prediction in an end-to-end manner. Based on PF <ref type="bibr" target="#b2">[3]</ref>, Braman et al. <ref type="bibr" target="#b1">[2]</ref> proposed a deep orthogonal fusion model to combine information from multiparametric MRI exams, biopsy-based modalities, and clinical variables into a comprehensive multimodal risk score. Despite the recent success in various medical imaging analysis tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">19]</ref>, sarcopenia diagnosis by deep learning based algorithms is still under study. To the best of our knowledge, recent work by Ryu et al. <ref type="bibr" target="#b12">[12]</ref> is the most relevant to our proposed method. Ryu et al. <ref type="bibr" target="#b12">[12]</ref> first used three ensembled deep learning models to test appendicular lean mass (ALM), handgrip strength (HGS), and chair rise test (CRT) performance using chest X-ray images. Then they built machine learning models to aggregate predicted ALM, HGS, and CRT performance values along with basic tabular features to diagnose sarcopenia. However, the major drawback of their work lies in the complex two-stage workflow and the tedious ensemble training. Besides, since sarcopenia is defined by low appendicular muscle mass, measuring muscle wasting through hip X-ray images, which have the greatest proportion of muscle mass, is much more appropriate for screening sarcopenia.</p><p>In this work, we propose a multi-modality contrastive learning (MM-CL)<ref type="foot" target="#foot_1">1</ref> model for sarcopenia diagnosis from hip X-rays and clinical information. Different from Ryu et al.'s model <ref type="bibr" target="#b12">[12]</ref>, our MM-CL can process multi-modality images and clinical data and screen sarcopenia in an end-to-end fashion. The overall framework is given in Fig. <ref type="figure" target="#fig_0">1</ref>. The major components include Non-local CAM Enhancement (NLC), Visual-text Feature Fusion (VFF), and Auxiliary contrastive representation (ACR) modules. Non-local CAM Enhancement enables the network to capture global long-range information and assists the network to concentrate on semantically important regions generated by class activation maps (CAM). Visual-text Feature Fusion encourages the network to improve the multi-modality feature representation ability. Auxiliary contrastive representation utilizes unsupervised learning and thus improves its ability for discriminative representation in the high-level latent space. The main contributions of this paper are summarized as follows. First, we propose a multi-modality contrastive learning model, which enhances the feature representation ability via integrating extra global knowledge, fusing multi-modality information, and joint unsupervised and supervised learning. Second, to address the absence of multi-modality datasets for sarcopenia screening, we select 1,176 patients from the Taipei Municipal Wanfang Hospital. To the best of our knowledge, our dataset is the largest for automated sarcopenia diagnosis from images and tabular information to date. Third, we experimentally show the superiority of the proposed method for predicting sarcopenia from hip X-rays and clinical information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Collection</head><p>In this retrospective study, we collected anonymized data from patients who underwent sarcopenia examinations at the Taipei Municipal Wanfang Hospital. The data collection was approved by an institutional review board. The demographic and clinical characteristics of this dataset are shown in Table <ref type="table" target="#tab_0">1</ref>. 490 of 1,176 eligible patients who had developed sarcopenia were annotated as positive, while the remaining 686 patients were labeled as negative. The pixel resolution of these images varies from 2266 × 2033 to 3408 × 3408. Each patient's information was collected from a standardized questionnaire, including age, gender, height, weight, BMI, appendicular skeletal muscle index (ASMI), total lean mass, total fat, leg lean mass, and leg fat. We use 5 numerical variables including age, gender, height, weight, and BMI as clinical information for boosting learning as suggested by the surgeon. To the best of our knowledge, this is the largest dataset for automated sarcopenia diagnosis from images and tabular information to date. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, MM-CL consists of three major components. The Non-local CAM Enhancement module is proposed to force the network to learn from attentional spatial regions learned from class activation map (CAM) <ref type="bibr" target="#b20">[20]</ref> to enhance the global feature representation ability. Then, we fuse the heterogeneous images and tabular data by integrating clinical variables through a Visual-text Feature Fusion module. Finally, we present an unsupervised contrastive representation learning strategy to assist the supervised screening by Auxiliary contrastive representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Non-local CAM Enhancement</head><p>Considering the large proportion of muscle regions in hip X-ray images, capturing longrange dependencies is of great importance for sarcopenia screening. In this work, we adopt the non-local module <ref type="bibr" target="#b16">[16]</ref> (NLM) and propose using coarse CAM localization maps as extra information to accelerate learning. We have two hypotheses. First, the long-range dependency of the left and right legs should be well captured; Second, the CAM may highlight part of muscle regions, providing weak supervision to accelerate the convergence of the network. Figure <ref type="figure" target="#fig_0">1(a)</ref> shows the overall structure of the Non-local CAM Enhancement.</p><p>CAM Enhancement: First, each training image X ∈ R 3×H×W is sent to the CAM generator as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a) to generate coarse localization map X m ∈ R 1×H×W . We use the Smooth Grad-CAM++ <ref type="bibr" target="#b10">[10]</ref> technique to generate CAM via the ResNet18 <ref type="bibr" target="#b9">[9]</ref> architecture. After the corresponding CAM is generated, the training image X is enhanced by its coarse localization map X m via smooth attention to the downstream precise prediction network. The output image X f is obtained as:</p><formula xml:id="formula_0">X f = X • (1 + sigmoid(X m )),<label>(1)</label></formula><p>where sigmoid denotes the Sigmoid function. The downstream main encoder is identical to ResNet18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-local Module:</head><p>Given a hip X-ray image X and the corresponding CAM map X m , we apply the backbone of ResNet18 to extract the high-level feature maps x ∈ R C×H ×W . The feature maps are then treated as inputs for the non-local module. For output xi from position index i, we have</p><formula xml:id="formula_1">xi = H W j=1 a ij g (x j ) + x i , a ij = ReLU w T f concat(θ (x i ) , φ (x j )) ,<label>(2)</label></formula><p>where concat denotes concatenation, w f is a weight vector that projects the concatenated vector to a scalar, ReLU is the ReLU function, a ij denotes the non-local feature attention that represents correlations between the features at two locations (i.e., x i and x j ), θ, φ, and g are mapping functions as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual-Text Feature Fusion</head><p>After capturing the global information, we aim to fuse the visual and text features in the high-level latent space. We hypothesize that the clinical data may have a positive effect to boost the visual prediction performance. The overall structure of this strategy is given in Fig. <ref type="figure" target="#fig_0">1(b</ref>). We extract the clinical features using a simple network, termed as TextNet. Finally, we propose a visual-text fusion module inspired by self-attention <ref type="bibr" target="#b15">[15]</ref> to fuse the concatenated visual-text features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual-Text Fusion Module:</head><p>In order to learn from clinical data, we first encode 5 numerical variables as a vector and send it to TextNet. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), TextNet consists of two linear layers, a batch normalization layer, and a sigmoid linear unit (SiLU) layer. We then expand and reshape the output feature xt ∈ R C t of TextNet to fit the size of C × H × W . The text and visual representations are then concatenated as xvt = concat(x, reshape(x t )) before sending it to the visual-text fusion module, where x ∈ R C×H ×W denotes the output features from Non-local CAM Enhancement. Feature vector xvt i ∈ R C vt encodes information about the combination of a specific location i in image and text features with C vt = C + C t . The visual-text self-attention module first produces a set of query, key, and value by 1 × 1 convolutional transformations as</p><formula xml:id="formula_2">q i = W q xvt i , k i = W k xvt i , and v i = W v xvt i at each spatial location i, where W q , W k ,</formula><p>and W v are part of the model parameters to be learned. We compute the visual-text self-attentive feature ẑvt i at position i as</p><formula xml:id="formula_3">ẑvt i = H W j=1 s ij v j + v i , s ij = Softmax q T j • k i . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>The softmax operation indicates the attention across each visual and text pair in the multi-modality feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Auxiliary Contrastive Representation</head><p>Inspired by unsupervised representation learning <ref type="bibr" target="#b3">[4]</ref>, we present a contrastive representation learning strategy that encourages the supervised model to pull similar data samples close to each other and push the different data samples away in the high-level embedding space. By such means, the feature representation ability in the embedding space could be further improved.</p><p>During the training stage, given N samples in a mini-batch, we obtain 2N samples by applying different augmentations (AutoAugment <ref type="bibr" target="#b6">[6]</ref>) on each sample. Two augmented samples from the same sample are regarded as positive pairs, and others are treated as negative pairs. Thus, we have a positive sample and 2N -2 negative samples for each patch. We apply global average pooling and linear transformations (Projection Head in Fig. <ref type="figure" target="#fig_0">1(c</ref>)) to the visual-text embeddings ẑvt in sequence, and obtain transformed features ôvt . Let ôvt+ and ôvtdenote the positive and negative embeddings of ôvt , the formula of contrastive loss is defined as</p><formula xml:id="formula_5">L vtcl = -log exp (sim (ô vt , ôvt+ ) /τ ) exp (sim (ô vt , ôvt+ ) /τ ) + ôvt-∈N exp (sim (ô vt , ôvt-) /τ ) , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where N is the set of negative counterparts of ôvt , the sim(•, •) is the cosine similarity between two representations, and τ is the temperature scaling parameter. Note that all the visual-text embeddings in the loss function are 2 -normalized. Finally, we integrate the auxiliary contrastive learning branch into the main Classification Head as shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>), which is a set of linear layers. We use weighted cross-entropy loss L cls as our classification loss. The overall loss function is calculated as L total = L cls + βL vtcl , where β is a weight factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details and Evaluation Measures</head><p>Our method is implemented in PyTorch using an NVIDIA RTX 3090 graphic card. We set the batch size to 32. Adam optimizer is used with a polynomial learning rate policy, where the initial learning rate 2.5 × 10 -4 is multiplied by 1 -epoch total_epoch power with power as 0.9. The total number of training epochs is set to 100, and early stopping is adopted to avoid overfitting. Weight factor β is set to 0.01. The temperature constant τ is set to 0.5. Visual images are cropped to 4/5 of the original height and resized to 224 × 224 after different online augmentation. The backbone is initialized with the weights pretrained on ImageNet.</p><p>Extensive 5-fold cross-validation is conducted for sarcopenia diagnosis. We report the diagnosis performance using comprehensive quantitative metrics including area under the receiver operating characteristic curve (AUC), F1 score (F1), accuracy (ACC), sensitivity (SEN), specificity (SPC), and precision (PRE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative and Qualitative Comparison</head><p>We implement several state-of-the-art single-modality (ResNet, ARLNet, MaxNet <ref type="bibr" target="#b1">[2]</ref>, Support vector machine (SVM), and K-nearest neighbors(KNN)) and multi-modality methods (PF <ref type="bibr" target="#b2">[3]</ref>) to demonstrate the effectiveness of our MM-CL. For a fair comparison, we use the same training settings.  When compared to the single-modality models, MM-CL outperforms state-of-the-art approaches by at least 6% on ACC. Among all the single-modality models, MaxNet <ref type="bibr" target="#b1">[2]</ref>, SVM, and KNN gain better results than image-only models. When compared to the multi-modality models, MM-CL also performs better than these methods by a large margin, which proves the effectiveness of our proposed modules.</p><p>We further visualize the AUC-ROC and Precision-Recall curves to intuitively show the improved performance. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the MM-CL achieves the best AUC and average precision (AP), which demonstrates the effectiveness of the proposed MM-CL.</p><p>We have three observations: (1) Multi-modality based models outperform singlemodality based methods, and we explain this finding that multiple modalities complement each other with useful information. (2) MaxNet <ref type="bibr" target="#b1">[2]</ref> gains worse results than traditional machine learning methods. One primary reason is that MaxNet contains a large number of parameters to be learned, the tabular information only includes 5 factors, which could result in overfitting. (3) With the help of NLC, VFF, and ACR, our MM-CL achieves substantial improvement over all the other methods. We also conduct ablation studies to validate each proposed component i.e., NLC, VFF, and ACR. CAM/NLM of NLC denotes the CAM enhancement/non-local module. Results are shown in Table <ref type="table" target="#tab_3">3</ref>. Utilizing CAM in the network as an enhancement for optimization improves 0.93% for average ACC, when compared to the baseline model (ResNet18). Meanwhile, capturing long-range dependencies via NLM brings improvement on AUC, ACC, F1, and SEN. Equipped with the text information via VFF, our method can lead to significant performance gains on ACC compared with image-only experiments, e.g., 79.16% vs. 73.80%. Lastly, applying ACR to the network improves the average ACC score from 79.16% to 79.93%. We also visualize the ability of feature representation in the high-level semantic latent feature space before the final classification via t-SNE <ref type="bibr" target="#b14">[14]</ref>. As can be seen in Fig. <ref type="figure" target="#fig_2">3</ref>, by gradually adding the proposed modules, the feature representation ability of our model becomes more and more powerful, and the high-level features are better clustered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study of the Proposed Method</head><p>Our first finding is that fusing visual and text knowledge brings significant improvement, which demonstrates that the extra tabular information could help substantially in learning. Second, incorporating unsupervised contrastive learning in the supervised learning framework could also improve the feature representation ability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In conclusion, we propose a multi-modality contrastive learning model for sarcopenia screening using hip X-ray images and clinical information. The proposed model consists of a Non-local CAM Enhancement module, a Visual-text Feature Fusion module, and an Auxiliary contrastive representation for improving the feature representation ability of the network. Moreover, we collect a large dataset for screening sarcopenia from heterogeneous data. Comprehensive experiments and explanations demonstrate the superiority of the proposed method. Our future work includes the extension of our approach to other multi-modality diagnosis tasks in the medical imaging domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The framework of our proposed MM-CL. MM-CL is composed of (a) Non-local CAM Enhancement, (b) Visual-text Feature Fusion, and (c) Auxiliary contrastive representation. The output feature size of each block is given in the channel size × height × width (C × H × W ) format. GAP denotes the global average pooling, and CAM denotes the class activation map.</figDesc><graphic coords="3,58,47,76,70,322,72,161,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. AUC-ROC (a) and Precision-Recall (b) curves for comparison with state-of-the-art methods.</figDesc><graphic coords="7,80,97,447,38,290,92,121,45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual interpretation of high-level features using t-SNE. The red and blue circles are sarcopenia and non-sarcopenia instances respectively. (Color figure online)</figDesc><graphic coords="8,41,79,509,96,340,09,70,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Demographic and clinical characteristics of sarcopenia patients.</figDesc><table><row><cell>Characteristics</cell><cell>Type</cell><cell>Entire cohort ( n = 1176 )</cell></row><row><cell>Gender</cell><cell>Male</cell><cell>272 (23.12%)</cell></row><row><cell></cell><cell cols="2">Female 904 (76.88%)</cell></row><row><cell>Age at diagnosis</cell><cell></cell><cell>71 [63-81]</cell></row><row><cell>BMI</cell><cell></cell><cell>22.8 [20.5-25.2]</cell></row><row><cell>Height (cm)</cell><cell></cell><cell>155.9 [150.2-162]</cell></row><row><cell>Weight (kg)</cell><cell></cell><cell>55 [49.5-63]</cell></row><row><cell cols="3">Note: indicates the median values [interquartile range,</cell></row><row><cell cols="2">25th-75th percentile].</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Sarcopenia diagnosis performance of recently proposed methods.</figDesc><table><row><cell>Method</cell><cell cols="6">Modality AUC (%) ACC (%) F1 (%) SEN (%) SPC (%) PRE (%)</cell></row><row><cell cols="2">ResNet18 [9] Image</cell><cell>76.86</cell><cell>72.53</cell><cell>64.58 60.56</cell><cell>79.35</cell><cell>69.46</cell></row><row><cell cols="2">ARLNet [19] Image</cell><cell>76.73</cell><cell>72.87</cell><cell>65.33 61.72</cell><cell>80.78</cell><cell>69.08</cell></row><row><cell cols="2">MaxNet [2] Text</cell><cell>80.05</cell><cell>72.44</cell><cell>58.30 47.05</cell><cell>90.68</cell><cell>78.10</cell></row><row><cell>SVM</cell><cell>Text</cell><cell>82.72</cell><cell>73.63</cell><cell>65.47 60.43</cell><cell>83.26</cell><cell>72.08</cell></row><row><cell>KNN</cell><cell>Text</cell><cell>80.69</cell><cell>73.21</cell><cell>66.94 65.59</cell><cell>78.80</cell><cell>68.80</cell></row><row><cell>PF [3]</cell><cell>Multi</cell><cell>77.88</cell><cell>73.98</cell><cell>67.33 65.11</cell><cell>80.30</cell><cell>70.65</cell></row><row><cell>MM-CL</cell><cell>Multi</cell><cell>84.64</cell><cell>79.93</cell><cell>74.88 72.06</cell><cell>86.06</cell><cell>78.44</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 outlines</head><label>2</label><figDesc>the performance of all methods. As shown, our model achieves the best AUC of 84.64% and ACC of 79.93% among all the methods in comparison.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Sarcopenia diagnosis performance with ablation studies.</figDesc><table><row><cell>Modality</cell><cell>NLC</cell><cell cols="7">VFF ACR AUC (%) ACC (%) F1 (%) SEN (%) SPC (%) PRE (%)</cell></row><row><cell cols="2">Image Text CAM NLM √ √ √ √ √ √ √ √ √ √ √ √ √ √</cell><cell>√ √</cell><cell>√</cell><cell>76.86 77.09 77.86 84.21 84.64</cell><cell>72.53 73.46 73.80 79.16 79.93</cell><cell>64.58 60.56 65.55 60.83 66.23 62.85 75.13 76.63 74.88 72.06</cell><cell>79.35 82.55 81.22 80.69 86.06</cell><cell>69.46 71.53 70.93 74.03 78.44</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 H. Greenspan et al. (Eds.): MICCAI 2023, LNCS 14225, pp. 85-94, 2023. https://doi.org/10.1007/978-3-031-43987-2_9</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Source code will be released at https://github.com/qgking/MM-CL.git.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported by the <rs type="funder">Fundamental Research Funds for the Central Universities</rs>, the <rs type="funder">National Natural Science Foundation of China</rs> [Grant No. <rs type="grantNumber">62201460</rs> and No. <rs type="grantNumber">62072329</rs>], and the <rs type="funder">National Key Technology R&amp;D Program of China</rs> [Grant No. <rs type="grantNumber">2018YFB1701700</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jBZKMNh">
					<idno type="grant-number">62201460</idno>
				</org>
				<org type="funding" xml:id="_pd72GuN">
					<idno type="grant-number">62072329</idno>
				</org>
				<org type="funding" xml:id="_8a2T8j7">
					<idno type="grant-number">2018YFB1701700</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Screening, diagnosis and monitoring of sarcopenia: when to use which tool?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Ackermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Nutrition ESPEN</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep orthogonal fusion: multimodal prognostic biomarker discovery integrating radiology, pathology, genomic, and clinical data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Braman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W H</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Goossens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Venkataraman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_64" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="667" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pathomic fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="757" to="770" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sarcopenia: revised European consensus on definition and diagnosis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Cruz-Jentoft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Age Ageing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="31" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">AutoAugment: learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prevalence and incidence of sarcopenia in the very old: findings from the Newcastle 85+ study</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Granic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Kirkwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jagger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Sayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cachexia, Sarcopenia Muscle</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="237" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sarcopenia: diagnosis and management, state of the art and contribution of ultrasound</title>
		<author>
			<persName><forename type="first">S</forename><surname>Giovannini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Med</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">5552</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Smooth grad-CAM++: an enhanced inference level visualization technique for deep convolutional neural network models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Omeiza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Speakman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weldermariam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01224</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prevalence and associated factors of Sarcopenia in Singaporean adultsthe Yishun Study</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W J</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Med. Direct. Assoc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="885" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Chest X-ray-based opportunistic screening of sarcopenia using deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cachexia, Sarcopenia Muscle</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="418" to="428" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prevalence of sarcopenia in the world: a systematic review and meta-analysis of general population studies</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keshtkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ahadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Larijani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heshmat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Diab. Metab. Disord</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using tree-based algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pretp-2l: identification of therapeutic peptides and their types using two-layer ensemble learning framework</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">samppred-gat: prediction of antimicrobial peptide by graph attention network and predicted peptide structure</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">715</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention residual learning for skin lesion classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2092" to="2103" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
