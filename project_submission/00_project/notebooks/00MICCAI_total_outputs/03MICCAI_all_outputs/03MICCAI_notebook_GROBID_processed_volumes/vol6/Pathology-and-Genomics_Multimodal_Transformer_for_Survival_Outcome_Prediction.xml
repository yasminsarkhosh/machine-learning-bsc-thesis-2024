<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kexin</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UNC at Charlotte</orgName>
								<address>
									<settlement>Charlotte</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
							<email>zhangshaoting@pjlab.org.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="622" to="631"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B3AED88FDA45952E10CF4C79E06EF7F2</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_60</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Histopathological image analysis</term>
					<term>Multimodal learning</term>
					<term>Cancer diagnosis</term>
					<term>Survival prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Survival outcome assessment is challenging and inherently associated with multiple clinical factors (e.g., imaging and genomics biomarkers) in cancer. Enabling multimodal analytics promises to reveal novel predictive patterns of patient outcomes. In this study, we propose a multimodal transformer (PathOmics) integrating pathology and genomics insights into colon-related cancer survival prediction. We emphasize the unsupervised pretraining to capture the intrinsic interaction between tissue microenvironments in gigapixel whole slide images (WSIs) and a wide range of genomics data (e.g., mRNA-sequence, copy number variant, and methylation). After the multimodal knowledge aggregation in pretraining, our task-specific model finetuning could expand the scope of data utility applicable to both multi-and singlemodal data (e.g., image-or genomics-only). We evaluate our approach on both TCGA colon and rectum cancer cohorts, showing that the proposed approach is competitive and outperforms state-of-the-art studies. Finally, our approach is desirable to utilize the limited number of finetuned samples towards data-efficient analytics for survival outcome prediction. The code is available at https://github.com/Cassie07/PathOmics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cancers are a group of heterogeneous diseases reflecting deep interactions between pathological and genomics variants in tumor tissue environments <ref type="bibr" target="#b24">[24]</ref>. Different cancer genotypes are translated into pathological phenotypes that could be assessed by pathologists <ref type="bibr" target="#b24">[24]</ref>. High-resolution pathological images have proven their unique benefits for improving prognostic biomarkers prediction via exploring the tissue microenvironmental features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b25">25]</ref>. Meanwhile, genomics data (e.g., mRNA-sequence) display a high relevance to regulate cancer progression <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">29]</ref>. For instance, genome-wide molecular portraits are crucial for cancer prognostic stratification and targeted therapy <ref type="bibr" target="#b16">[16]</ref>. Despite their importance, seldom efforts jointly exploit the multimodal value between cancer image morphology and molecular biomarkers. In a broader context, assessing cancer prognosis is essentially a multimodal task in association with pathological and genomics findings. Therefore, synergizing multimodal data could deepen a crossscale understanding towards improved patient prognostication.</p><p>The major goal of multimodal data learning is to extract complementary contextual information across modalities <ref type="bibr" target="#b3">[4]</ref>. Supervised studies <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> have allowed multimodal data fusion among image and non-image biomarkers. For instance, the Kronecker product is able to capture the interactions between WSIs and genomic features for survival outcome prediction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. Alternatively, the coattention transformer <ref type="bibr" target="#b5">[6]</ref> could capture the genotype-phenotype interactions for prognostic prediction. Yet these supervised approaches are limited by feature generalizability and have a high dependency on data labeling. To alleviate label requirement, unsupervised learning evaluates the intrinsic similarity among multimodal representations for data fusion. For example, integrating image, genomics, and clinical information can be achieved via a predefined unsupervised similarity evaluation <ref type="bibr" target="#b3">[4]</ref>. To broaden the data utility, the study <ref type="bibr" target="#b28">[28]</ref> leverages the pathology and genomic knowledge from the teacher model to guide the pathology-only student model for glioma grading. From these analyses, it is increasingly recognized that the lack of flexibility on model finetuning limits the data utility of multimodal learning. Meanwhile, the size of multimodal medical datasets is not as large as natural vision-language datasets, which necessitates the need for data-efficient analytics to address the training difficulty.</p><p>To tackle above challenges, we propose a pathology-and-genomics multimodal framework (i.e., PathOmics) for survival prediction (Fig. <ref type="figure" target="#fig_0">1</ref>). We summarized our contributions as follows. (1) Unsupervised multimodal data fusion. Our unsupervised pretraining exploits the intrinsic interaction between morphological and molecular biomarkers (Fig. <ref type="figure" target="#fig_0">1a</ref>). To overcome the gap of modality heterogeneity between images and genomics, we project the multimodal embeddings into the same latent space by evaluating the similarity among them. Particularly, the pretrained model offers a unique means by using similarity-guided modality fusion for extracting cross-modal patterns. (2) Flexible modality finetuning. A key contribution of our multimodal framework is that it combines benefits from both unsupervised pretraining and supervised finetuning data fusion (Fig. <ref type="figure" target="#fig_0">1b</ref>). As a result, the task-specific finetuning broadens the dataset usage (Fig <ref type="figure" target="#fig_0">1b</ref> and<ref type="figure">c</ref>), which is not limited by data modality (e.g., both singleand multi-modal data). (3) Data efficiency with limited data size. Our approach could achieve comparable performance even with fewer finetuned data (e.g., only use 50% of the finetuned data) when compared with using the entire finetuning dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Overview. Figure <ref type="figure" target="#fig_0">1</ref> illustrates our multimodal transformer framework. Our method includes an unsupervised multimodal data fusion pretraining and a supervised flexible-modal finetuning. From Fig. <ref type="figure" target="#fig_0">1a</ref>, in the pretraining, our unsupervised data fusion aims to capture the interaction pattern of image and genomics features. Overall, we formulate the objective of multimodal feature learning by converting image patches and tabular genomics data into groupwise embeddings, and then extracting multimodal patient-wise embeddings. More specifically, we construct group-wise representations for both image and genomics modalities. For image feature representation, we randomly divide image patches into groups; Meanwhile, for each type of genomics data, we construct groups of genes depending on their clinical relevance <ref type="bibr" target="#b22">[22]</ref>. Next, as seen in Fig. <ref type="figure" target="#fig_0">1b</ref> and<ref type="figure">c</ref>, our approach enables three types of finetuning modal modes (i.e., multimodal, image-only, and genomics-only) towards prognostic prediction, expanding the downstream data utility from the pretrained model. Group-Wise Image and Genomics Embedding. We define the group-wise genomics representation by referring to N = 8 major functional groups obtained from <ref type="bibr" target="#b22">[22]</ref>. Each group contains a list of well-defined molecular features related to cancer biology, including transcription factors, tumor suppression, cytokines and growth factors, cell differentiation markers, homeodomain proteins, translocated cancer genes, and protein kinases. The group-wise genomics representation is defined as G n ∈ R 1×dg , where n ∈ N , d g is the attribute dimension in each group which could be various. To better extract high-dimensional group-wise genomics representation, we use a Self-Normalizing Network (SNN) together with scaled exponential linear units (SeLU) and Alpha Dropout for feature extraction to generate the group-wise embedding G n ∈ R 1×256 for each group.</p><p>For group-wise WSIs representation, we first cropped all tissue-region image tiles from the entire WSI and extracted CNN-based (e.g., ResNet50) d idimensional features for each image tile k as h k ∈ R 1×di , where d i = 1, 024, k ∈ K and K is the number of image patches. We construct the group-wise WSIs representation by randomly splitting image tile features into N groups (i.e., the same number as genomics categories). Therefore, group-wise image representation could be defined as I n ∈ R kn×1024 , where n ∈ N and k n represents tile k in group n. Then we apply an attention-based refiner (ABR) <ref type="bibr" target="#b17">[17]</ref>, which is able to weight the feature embeddings in the group, together with a dimension deduction (e.g., fully-connected layers) to achieve the group-wise embedding. The ABR and the group-wise embedding I n ∈ R 1×256 are defined as:</p><formula xml:id="formula_0">a k = epx{w T (tanh(V 1 h k ) (sigm(V 2 h k ))} K j=1 epx{w T (tanh(V 1 h j ) (sigm(V 2 h j ))}<label>(1)</label></formula><p>where w,V1 and V2 are the learnable parameters.</p><formula xml:id="formula_1">I n = K k=1 a k h k (2)</formula><p>Patient-Wise Multimodal Feature Embedding. To aggregate patient-wise multimodal feature embedding from the group-wise representations, as shown in Fig. <ref type="figure" target="#fig_0">1a</ref>, we propose a pathology-and-genomics multimodal model containing two model streams, including a pathological image and a genomics data stream.</p><p>In each stream, we use the same architecture with different weights, which is updated separately in each modality stream. In the pathological image stream, the patient-wise image representation is aggregated by N group representations as</p><formula xml:id="formula_2">I p ∈ R N ×256</formula><p>, where p ∈ P and P is the number of patients. Similarly, the patient-wise genomics representation is aggregated as G p ∈ R N ×256 . After generating patient-wise representation, we utilize two transformer layers <ref type="bibr" target="#b27">[27]</ref> to extract feature embeddings for each modality as follows:</p><formula xml:id="formula_3">H l p = MSA(H p )<label>(3)</label></formula><p>where MSA denotes Multi-head Self-attention <ref type="bibr" target="#b27">[27]</ref> (see Appendix 1), l denotes the layer index of the transformer, and H p could either be I p or G p . Then, we construct global attention poolings <ref type="bibr" target="#b17">[17]</ref> as Eq. 1 to adaptively compute a weighted sum of each modality feature embeddings to finally construct patientwise embedding as I p embedding ∈ R 1×256 and G p embedding ∈ R 1×256 in each modality.</p><p>Multimodal Fusion in Pretraining and Finetuning. Due to the domain gap between image and molecular feature heterogeneity, a proper design of multimodal fusion is crucial to advance integrative analysis. In the pretraining stage, we develop an unsupervised data fusion strategy by decreasing the mean square error (MSE) loss to map images and genomics embeddings into the same space. Ideally, the image and genomics embeddings belonging to the same patient should have a higher relevance between each other. MSE measures the average squared difference between multimodal embeddings. In this way, the pretrained model is trained to map the paired image and genomics embeddings to be closer in the latent space, leading to strengthen the interaction between different modalities.</p><formula xml:id="formula_4">L fusion = argmin 1 P P p=1 ((I p embedding -G p embedding ) 2 )<label>(4)</label></formula><p>In the single modality finetuning, even if we use image-only data, the model is able to produce genomic-related image feature embedding due to the multimodal knowledge aggregation already obtained from the model pretraining. As a result, our cross-modal information aggregation relaxes the modality requirement in the finetuning stage. As shown in Fig. <ref type="figure" target="#fig_0">1b</ref>, for multimodal finetuning, we deploy a concatenation layer to obtain the fused multimodal feature representation and implement a risk classifier (FC layer) to achieve the final survival stratification (see Appendix 2). As for single-modality finetuning mode in Fig. <ref type="figure" target="#fig_0">1c</ref>, we simply feed I p embedding or G p embedding into risk classifier for the final prognosis prediction. During the finetuning, we update the model parameters using a log-likelihood loss for the discrete-time survival model training <ref type="bibr" target="#b5">[6]</ref>(see Appendix 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Datasets. All image and genomics data are publicly available. We collected WSIs from The Cancer Genome Atlas Colon Adenocarcinoma (TCGA-COAD) dataset (CC-BY-3.0) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">21]</ref> and Rectum Adenocarcinoma (TCGA-READ) dataset (CC-BY-3.0) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">20]</ref>, which contain 440 and 153 patients. We cropped each WSI into 512 × 512 non-overlapped patches. We also collected the corresponding tabular genomics data (e.g., mRNA sequence, copy number alteration, and methylation) with overall survival (OS) times and censorship statuses from Cbioportal <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">14]</ref>. We removed the samples without the corresponding genomics data or ground truth of survival outcomes. Finally, we included 426 patients of TCGA-COAD and 145 patients of TCGA-READ.</p><p>Experimental Settings and Implementations. We implement two types of settings that involve internal and external datasets for model pretraining and finetuning. As shown in Fig <ref type="figure" target="#fig_1">2a</ref>, we pretrain and finetune the model on the same dataset (i.e., internal setting). We split TCGA-COAD into training (80%) and holdout testing set (20%). Then, we implement four-fold cross-validation on the training set for pretraining, finetuning, and hyperparameter-tuning. The test set is only used for evaluating the best finetuned models from each cross-validation split. For the external setting, we implement pretraining and finetuning on the different datasets, as shown in Fig 2b ; we use TCGA-COAD for pretraining; Then, we only use TCGA-READ for finetuning and final evaluation. We implement a five-fold cross-validation for pretraining, and the best pretrained models are used for finetuning. We split TCGA-READ into finetuning (60%), validation (20%), and evaluation set (20%). For all experiments, we calculate the average performance on the evaluation set across the best models.</p><p>The number of epochs for pretraining and finetuning are 25, the batch size is 1, the optimizer is Adam <ref type="bibr" target="#b19">[19]</ref>, and the learning rate is 1e-4 for pretraining and 5e-5 for finetuning. We used one 32GB Tesla V100 SXM2 GPU and Pytorch. The concordance index (C-index) is used to measure the survival prediction performance. We followed the previous studies <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> to partition the overall survival (OS) months into four non-overlapping intervals by using the quartiles of event times of uncensored patients for discretized-survival C-index calculation (see Appendix 2). For each experiment, we reported the average C-index among three-times repeated experiments. Conceptionally, our method shares a similar idea to multiple instance learning (MIL) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">23]</ref>. Therefore, we include two types of baseline models, including the MIL-based models (DeepSet <ref type="bibr" target="#b30">[30]</ref>, AB-MIL <ref type="bibr" target="#b17">[17]</ref>, and TransMIL <ref type="bibr" target="#b26">[26]</ref>) and MIL multimodal-based models (MCAT <ref type="bibr" target="#b5">[6]</ref>, PORPOISE <ref type="bibr" target="#b6">[7]</ref>). We follow the same data split and processing, as well as the identical training hyperparameters and supervised fusion as above. Notably, there is no need for supervised finetuning for the baselines when using TCGA-COAD (Table <ref type="table" target="#tab_0">1</ref>), because the supervised pretraining is already applied to the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>In Table <ref type="table" target="#tab_0">1</ref>, our approach shows improved survival prediction performance on both TCGA-COAD and TCGA-READ datasets. Compared with supervised baselines, our unsupervised data fusion is able to extract the phenotype-genotype interaction features, leading to achieving a flexible finetuning for different data settings. With the multimodal pretraining and finetuning, our method outperforms state-of-the-art models by about 2% on TCGA-COAD and 4% TCGA-READ. We recognize that the combination of image and mRNA sequencing data leads to reflecting distinguishing survival outcomes. Remarkably, our model achieved positive results even using a single-modal finetuning when compared with baselines (more results in Appendix 3.1). In the meantime, on the TCGA-READ, our single-modality finetuned model achieves a better performance than multimodal finetuned baseline models (e.g., with model pretraining via image and methylation data, we have only used the image data for finetuning and achieved a C-index of 74.85%, which is about 4% higher than the best baseline models). We show that with a single-modal finetuning strategy, the model could generate meaningful embedding to combine image-and genomicrelated patterns. In addition, our model reflects its efficiency on the limited finetuning data (e.g., 75 patients are used for finetuning on TCGA-READ, which are only 22% of TCGA-COAD finetuning data). In Table <ref type="table" target="#tab_0">1</ref>, our method could yield better performance compared with baselines on the small dataset across the combination of images and multiple types of genomics data. approach broadens the scope of dataset inclusion, particularly for model finetuning and evaluation, while enhancing model efficiency on analyzing multimodal clinical data in real-world settings. In addition, the use of synthetic data and developing a foundation model training will be helpful to improve the robustness of multimodal data fusion <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b15">15]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Workflow overview of the pathology-and-genomics multimodal transformer (PathOmics) for survival prediction. In (a), we show the pipeline of extracting image and genomics feature embedding via an unsupervised pretraining towards multimodal data fusion. In (b) and (c), our supervised finetuning scheme could flexibly handle multiple types of data for prognostic prediction. With the multimodal pretrained model backbones, both multi-or single-modal data can be applicable for our model finetuning.</figDesc><graphic coords="3,41,79,266,30,340,33,185,89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Dataset usage. In a, we use TCGA-COAD dataset for model pretraining, finetuning, and evaluation. In b, we use TCGA-COAD dataset for model pretraining. Then, we use TCGA-READ dataset to finetune and evaluate the pretrained models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The comparison of C-index performance on TCGA-COAD and TCGA-READ dataset. "Methy" is used as the abbreviation of Methylation.</figDesc><table><row><cell>Model</cell><cell cols="2">Pretrain data modality TCGA-COAD</cell><cell>TCGA-READ</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Finetune data modality C-index (%) Finetune data modality C-index (%)</cell></row><row><cell>DeepSets [30]</cell><cell>image+mRNA</cell><cell>-</cell><cell>58.70 ± 1.10 image+mRNA</cell><cell>70.19 ± 1.45</cell></row><row><cell></cell><cell>image+CNA</cell><cell>-</cell><cell>51.50 ± 2.60 image+CNA</cell><cell>62.50 ± 2.52</cell></row><row><cell></cell><cell>image+Methy</cell><cell>-</cell><cell>65.61 ± 1.86 image+Methy</cell><cell>55.78 ± 1.22</cell></row><row><cell>AB-MIL [17]</cell><cell>image+mRNA</cell><cell>-</cell><cell>54.12 ± 2.88 image+mRNA</cell><cell>68.79 ± 1.44</cell></row><row><cell></cell><cell>image+CNA</cell><cell>-</cell><cell>54.68 ± 2.44 image+CNA</cell><cell>66.72 ± 0.81</cell></row><row><cell></cell><cell>image+Methy</cell><cell>-</cell><cell>49.66 ± 1.58 image+Methy</cell><cell>55.78 ± 1.22</cell></row><row><cell cols="2">TransMIL [26] image+mRNA</cell><cell>-</cell><cell>54.15 ± 1.02 image+mRNA</cell><cell>67.91 ± 2.35</cell></row><row><cell></cell><cell>image+CNA</cell><cell>-</cell><cell>59.80 ± 0.98 image+CNA</cell><cell>62.75 ± 1.92</cell></row><row><cell></cell><cell>image+Methy</cell><cell>-</cell><cell>53.35 ± 1.78 image+Methy</cell><cell>53.09 ± 1.46</cell></row><row><cell>MCAT [6]</cell><cell>image+mRNA</cell><cell>-</cell><cell>65.02 ± 3.10 image+mRNA</cell><cell>70.27 ± 2.75</cell></row><row><cell></cell><cell>image+CNA</cell><cell>-</cell><cell>64.66 ± 2.31 image+CNA</cell><cell>60.50 ± 1.25</cell></row><row><cell></cell><cell>image+Methy</cell><cell>-</cell><cell>60.98 ± 2.43 image+Methy</cell><cell>59.78 ± 1.20</cell></row><row><cell cols="2">PORPOI-SE [7] image+mRNA</cell><cell>-</cell><cell>65.31 ± 1.26 image+mRNA</cell><cell>68.18 ± 1.62</cell></row><row><cell></cell><cell>image+CNA</cell><cell>-</cell><cell>57.32 ± 1.78 image+CNA</cell><cell>60.19 ± 1.48</cell></row><row><cell></cell><cell>image+Methy</cell><cell>-</cell><cell>61.84 ± 1.10 image+Methy</cell><cell>68.80 ± 0.92</cell></row><row><cell>Ours</cell><cell>image+mRNA</cell><cell>image+mRNA</cell><cell>67.32 ± 1.69 image+mRNA</cell><cell>74.35 ± 1.15</cell></row><row><cell></cell><cell></cell><cell>image</cell><cell>63.78 ± 1.22 image</cell><cell>74.85 ± 0.37</cell></row><row><cell></cell><cell></cell><cell>mRNA</cell><cell>60.76 ± 0.88 mRNA</cell><cell>59.61 ± 1.37</cell></row><row><cell></cell><cell>image+CNA</cell><cell>image+CNA</cell><cell>61.19 ± 1.03 image+CNA</cell><cell>73.95 ± 1.05</cell></row><row><cell></cell><cell></cell><cell>image</cell><cell>58.06 ± 1.54 image</cell><cell>71.18 ± 1.39</cell></row><row><cell></cell><cell></cell><cell>CNA</cell><cell>56.43 ± 1.02 CNA</cell><cell>63.95 ± 0.55</cell></row><row><cell></cell><cell>image+Methy</cell><cell>image+Methy</cell><cell>67.22 ± 1.67 image+Methy</cell><cell>71.80 ± 2.03</cell></row><row><cell></cell><cell></cell><cell>image</cell><cell>60.43 ± 0.72 image</cell><cell>64.42 ± 0.72</cell></row><row><cell></cell><cell></cell><cell>Methy</cell><cell>61.06 ± 1.34 Methy</cell><cell>65.42 ± 0.91</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The results of this study are based on the data collected from the public TCGA Research Network: https://www.cancer.gov/tcga.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 60. Ablation Analysis. We verify the model efficiency by using fewer amounts of finetuning data in finetuning. For TCGA-COAD dataset, we include 50%, 25%, and 10% of the finetuning data. For the TCGA-READ dataset, as the number of uncensored patients is limited, we use 75%, 50%, and 25% of the finetuning data to allow at least one uncensored patient to be included for finetuning. As shown in Fig. <ref type="figure">3a</ref>, by using 50% of TCGA-COAD finetuning data, our approach achieves the C-index of 64.80%, which is higher than the average performance of baselines in several modalities. Similarly, in Fig. <ref type="figure">3b</ref>, our model retains a good performance by using 50% or 75% of TCGA-READ finetuning data compared with the average of C-index across baselines (e.g., 72.32% versus 64.23%). For evaluating the effect of cross-modality information extraction in the pretraining, we kept supervised model training (i.e., the finetuning stage) while removing the unsupervised pretraining. The performance is lower 2%-10% than ours on multi-and single-modality data. For evaluating the genomics data usage, we designed two settings: (1) combining all types of genomics data and categorizing them by groups; (2) removing category information while keeping using different types of genomics data separately. Our approach outperforms the above ablation studies by 3%-7% on TCGA-READ and performs similarly on TCGA-COAD. In addition, we replaced our unsupervised loss with cosine similarity loss; our approach outperforms the setting of using cosine similarity loss by 3%-6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Developing data-efficient multimodal learning is crucial to advance the survival assessment of cancer patients in a variety of clinical data scenarios. We demonstrated that the proposed PathOmics framework is useful for improving the survival prediction of colon and rectum cancer patients. Importantly, our approach opens up perspectives for exploring the key insights of intrinsic genotypephenotype interactions in complex cancer data across modalities. Our finetuning</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Development and validation of a weakly supervised deep learning framework to predict the status of molecular pathways and key mutations in colorectal cancer from routine histology images: a retrospective study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bilal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="763" to="e772" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cbio cancer genomics portal: an open platform for exploring multidimensional cancer genomics data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cerami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Discov</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="401" to="404" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning-based multiomics integration robustly predicts survival in liver cancer using deep learning to predict liver cancer prognosis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">B</forename><surname>Poirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Garmire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Cancer Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1248" to="1259" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning with multimodal representation for pancancer prognosis prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheerla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gevaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="446" to="454" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pathomic fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="757" to="770" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal co-attention transformer for survival prediction in gigapixel whole slide images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4015" to="4025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pan-cancer integrative histology-genomic analysis via multimodal deep learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cancer imaging archive (tcia): maintaining and operating a public information repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature-enhanced graph networks for genetic mutational prediction using histopathological images in colon cancer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="294" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59713-929" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A large-scale synthetic pathological dataset for deep learning-enabled segmentation of breast cancer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gevaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">231</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatially aware graph neural networks and cross-level molecular profile prediction in colon cancer histopathology: a retrospective multi-cohort study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="787" to="e795" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Graph convolutional networks for multi-modality medical imaging: Methods, architectures, and clinical applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08916</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrative analysis of complex cancer genomics and clinical profiles using the cbioportal</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Signaling</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">269</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training like a medical resident: universal medical image segmentation via context prior learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Meta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02416</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The prognostic landscape of genes and infiltrating immune cells across human cancers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Gentles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="938" to="945" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning can predict microsatellite instability directly from histology in gastrointestinal cancer</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1054" to="1056" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Levine: the cancer genome atlas rectum adenocarcinoma collection (tcga-read) (version 3) [data set]</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sadow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cancer Imaging Archive</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Radiology data from the cancer genome atlas colon adenocarcinoma [tcga-coad] collection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Cancer Imaging Archive</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The molecular signatures database hallmark gene set collection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liberzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Birger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Thorvaldsdóttir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Mesirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="417" to="425" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A framework for multiple-instance learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Intra-tumour heterogeneity: a looking glass for cancer?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marusyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Almendro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Cancer</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="323" to="334" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Genetic mutation and biological pathway prediction based on whole slide images in breast carcinoma using deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Precision Oncol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">87</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transmil: transformer based correlated multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discrepancy and gradientguided multi-modal knowledge distillation for pathological glioma grading</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A multi-omics machine learning framework in predicting the survival of colorectal cancer patients</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">105516</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
