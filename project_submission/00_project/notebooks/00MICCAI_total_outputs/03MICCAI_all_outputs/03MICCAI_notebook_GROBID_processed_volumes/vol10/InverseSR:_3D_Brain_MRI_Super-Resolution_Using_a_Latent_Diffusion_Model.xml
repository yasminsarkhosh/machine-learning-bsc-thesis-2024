<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model</title>
				<funder ref="#_9EXfQGF">
					<orgName type="full">Compute Canada</orgName>
				</funder>
				<funder ref="#_Vv2zGPM">
					<orgName type="full">NSERC</orgName>
				</funder>
				<funder ref="#_yaTTDPb">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jueqi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">St. Francis Xavier University</orgName>
								<address>
									<settlement>Antigonish</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Levman</surname></persName>
							<email>jlevman@stfx.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">St. Francis Xavier University</orgName>
								<address>
									<settlement>Antigonish</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Martinos Center for Biomedical Imaging</orgName>
								<orgName type="department" key="dep2">Department of Radiology</orgName>
								<orgName type="institution">Massachusetts General Hospital</orgName>
								<address>
									<settlement>Charlestown</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Nova Scotia Health Authority</orgName>
								<address>
									<settlement>Halifax</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><forename type="middle">Hugo</forename><surname>Lopez Pinaya</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Petru-Daniel</forename><surname>Tudosiu</surname></persName>
							<email>petru.tudosiu@kcl.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">Jorge</forename><surname>Cardoso</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Razvan</forename><surname>Marinescu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">269F68801A6F37EC04F34D431EC143CB</idno>
					<idno type="DOI">10.1007/978-3-031-43999-542.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>MRI Super-Resolution</term>
					<term>Latent Diffusion Model</term>
					<term>Inverse Problem</term>
					<term>Optimization Method</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-resolution (HR) MRI scans obtained from researchgrade medical centers provide precise information about imaged tissues. However, routine clinical MRI scans are typically in low-resolution (LR) and vary greatly in contrast and spatial resolution due to the adjustments of the scanning parameters to the local needs of the medical center. End-to-end deep learning methods for MRI super-resolution (SR) have been proposed, but they require re-training each time there is a shift in the input distribution. To address this issue, we propose a novel approach that leverages a state-of-the-art 3D brain generative model, the latent diffusion model (LDM) from [21] trained on UK BioBank, to increase the resolution of clinical MRI scans. The LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder D of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR (LDM); 2) for SR with less sparsity, we invert only through the LDM decoder D, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction. Our source code is available online: https://github.com/BioMedAI-UCSC/InverseSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>End-to-end convolutional neural networks (CNNs) have shown remarkable performance compared to classical algorithms <ref type="bibr" target="#b14">[14]</ref> on MRI SR. Deep CNNs have been widely applied in a variety of MRI SR situations; for instance, slice imputation on the brain, liver and prostate MRI <ref type="bibr" target="#b29">[29]</ref> and brain MRI SR reconstruction on scaling factors ×2, ×3, ×4 <ref type="bibr" target="#b32">[32]</ref>. Several techniques based on deep CNNs have been proposed to improve performance, such as densely connected networks <ref type="bibr" target="#b6">[6]</ref>, adversarial networks <ref type="bibr" target="#b4">[5]</ref>, and attention network <ref type="bibr" target="#b32">[32]</ref>. However, their supervised training requires paired images, which necessitates re-training every time there is a shift in the input distribution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">16]</ref>. As a result, such methods are unsuitable for MRI SR, as it is challenging to obtain paired training data that cover the variability in acquisition protocols and resolution of clinical brain MRI scans across institutions <ref type="bibr" target="#b14">[14]</ref>.</p><p>Building image priors through generative models has recently become a popular approach in the field of image SR, for both computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19]</ref> as well as medical imaging <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b25">25]</ref>, as they do not require re-training in the presence of several types of input distribution shifts. While these methods have shown promise in MRI SR, they have so far been limited to 2D slices <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b25">25]</ref>, rendering them unsuitable for 3D brain MRIs slice imputation.</p><p>In this study, we propose solving the MRI SR problem by building powerful, 3D-native image priors through a recently proposed HR image generative model, the latent diffusion model (LDM) <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>. We solve the inverse problem by finding the optimal latent code z in the latent space of the pre-trained generative model, which could restore a given LR MRI I, using a known corruption function f . In this study, we focus on slice imputation, yet our method could be applied to other medical image SR problems by implementing different corruption functions f . We proposed two novel strategies for MRI SR: Inverse(LDM), which additionally inverts the input image through the deterministic DDIM model, and InverseSR(Decoder) which inverts the input image through the corruption function f and through the decoder D of the LDM model. We found that for large sparsity, InverseSR(LDM) had a better performance, while for low sparsity, InverseSR(Decoder) performed best. While the LDM model was trained on UK BioBank, we demonstrate our methods on an external dataset (IXI) which was inaccessible to the pre-trained generative model. Both quantitative and qualitative results show that our method achieves significantly better performance compared to two other baseline models. Furthermore, our method can also be applied to tumour/lesion filling by creating tumour/lesion shape masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>MRI Super-Resolution. End-to-end deep training <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b32">32]</ref> has been proposed recently for MRI SR, which has achieved superior results compared to classical methods. However, these methods require paired data to train, which is hard to acquire because of the large variability present in clinical MRIs <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b23">23]</ref>. To circumvent this limitation, several unsupervised methods have been proposed without requiring access to HR scans <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b14">14]</ref>. Dalca et al. <ref type="bibr" target="#b8">[8]</ref> proposed a gaussian mixture model for sparse image patches. Brudfors et al. <ref type="bibr" target="#b2">[3]</ref> presented an algorithm which could take advantage of multimodal MRI. Iglesias et al. <ref type="bibr" target="#b14">[14]</ref> introduced a method to train a CNN for MRI SR on any given combination of contrasts, resolutions and orientations.</p><p>Solving Inverse Problems Using Generative Models. A common way to solve the inverse problem using an LDM is to use the encoder E to first encode the given image x into the latent space z 0 = E(x) <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b20">20]</ref>, followed by DDIM (Denoising Diffusion Implicit Models) Inversion <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b24">24]</ref> to encode z 0 into the noise latent code z T <ref type="bibr" target="#b20">[20]</ref>. However, this approach does not work for low-resolution images, because the encoder E has only been trained on high-resolution images.</p><p>Our work is also similar to the optimization-based generative adversarial network (GAN) inversion approach <ref type="bibr" target="#b30">[30]</ref>, trying to find the optimal latent representation z * in the latent space of GAN, which could be mapped to represent the given image x ≈ G(z * ). More recent works <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b25">25]</ref> have used diffusion models for inverse problems due to their superior performance. However, all these methods require the diffusion model to operate directly in the image space, which for large image resolutions can become GPU-memory intensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>3D Brain Latent Diffusion Models. We leverage a state-of-the-art LDM <ref type="bibr" target="#b21">[21]</ref> to create high-quality priors for 3D brain MRIs. There are two components in an LDM: an autoencoder and a diffusion model <ref type="bibr" target="#b22">[22]</ref>. An encoder E maps each highresolution T1w brain MRI x ∼ p data (x) into a latent vector z 0 = E(x) of size 20 × 28 × 20. The decoder D is trained to map the latent vectors z 0 back into the MRI image domain x. The autoencoder was trained on 31,740 T1w MRIs from the UK Biobank <ref type="bibr" target="#b26">[26]</ref> using a combination of an L1 loss, a perceptual loss <ref type="bibr" target="#b31">[31]</ref>, a patch-based adversarial loss <ref type="bibr" target="#b11">[11]</ref> and a KL regularization term in the latent space. The autoencoder was trained on pre-processed MRIs using UniRes <ref type="bibr" target="#b2">[3]</ref> into a common MNI space with a voxel size of 1 mm 3 and was then kept unchanged during the LDM training. The latent representations of the T1w brain MRIs were then used to train the LDM. A conditional U-Net θ was then trained to predict the artificial noise by the following objective: DDIM <ref type="bibr" target="#b24">[24]</ref> has been used in brain LDM to replace the denoising diffusion probabilistic models (DDPM) during inference to reduce the number of reverse steps with minimal performance loss <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b24">24]</ref>. This network ε θ is conditioned on four conditional variables C: age, gender, ventricular volume and brain volume, which are all introduced by cross-attention layers <ref type="bibr" target="#b22">[22]</ref>. Gender is a binary variable, while the rest of the covariates are scaled to [0, 1]. Finally, the pre-trained decoder maps the latent vector into an HR MRI x = D(z 0 ). The architecture of the brain LDM can be found in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><formula xml:id="formula_0">θ * = arg min θ E z∼E(x), ∼N (0,1),t || -θ (z t , t, C)|| 2 2 (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deterministic DDIM Sampling.</head><p>In order to obtain a latent representation z T capable of reconstructing a given noisy sample into a high-resolution image, we employ deterministic DDIM sampling <ref type="bibr" target="#b24">[24]</ref>:</p><formula xml:id="formula_1">z t-1 = √ α t-1 z t - √ 1 -α t • θ (z t , C, t) √ α t + 1 -α t-1 • θ (z t , C, t) (2)</formula><p>where α 1:T ∈ (0, 1] T is a time-dependent decreasing sequence, zt-</p><formula xml:id="formula_2">√ 1-αt• θ (zt,C,t) √ αt</formula><p>represents the "predicted x 0 ", and √ 1α t-1 • θ (z t , C, t) can be understood as the "direction pointing to x t " <ref type="bibr" target="#b24">[24]</ref>.</p><p>Corruption Function f . We assume a corruption function f known a-priori that is applied on the HR image x obtained from the generative model, and compute the loss function based on the corrupted image f • x and the given LR input image I. In clinical practice, a prevalent method for acquiring MR images is prioritizing high in-plane resolution while sacrificing through-plane resolution to expedite the acquisition process and reduce motion artifacts <ref type="bibr" target="#b33">[33]</ref>. To account for this procedure, we introduce a corruption function that generates masks for non-acquired slices, enabling our method to in-paint the missing slices. For instance, on 1 × 1 × 4 mm 3 undersampled volumes, we create masks for three slices every four slices on the generated HR 1 × 1 × 1 mm 3 volumes. </p><formula xml:id="formula_3">for t = T, T -1, . . . , 1 do 7: zt-1 ← √ αt-1 z t -√ 1-α t • θ (z t ,C,t) √ α t + √ 1 -αt-1 • θ (zt, C, t); 8: L ← λpercLperc(f • D(z0), I) + λmae f • D(z0) -I ; 9: C ← C -α∇CL; 10: zT ← zT -α∇z T L; 11: Set z * T ← zT ; C * ← C; 12: Return z * T , C * ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InverseSR(LDM):</head><p>In the case of high sparsity MRI SR, we optimize the noise latent code z * T and its associated conditional variables C * to restore the HR image from the given LR input image I using the optimization method:</p><formula xml:id="formula_4">z * T , C * = arg min zT ,C λ perc L perc (f • D(DDIM(z T , C, T )), I)+ λ mae f • D(DDIM(z T , C, T )) -I (3)</formula><p>where DDIM(z T , C, T ) represents T deterministic DDIM sampling steps on the latent z 0 in Eq. 2. We follow the brain LDM model to use the perceptual loss L perc and the L1 pixelwise loss. The loss function is computed on the corrupted image generated from the generative model and the given LR input. A detailed pseudocode description of this method can be found in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InverseSR(Decoder):</head><p>For low sparsity MRI SR, we directly find the optimal latent code z * T using the decoder D:</p><formula xml:id="formula_5">z * 0 = arg min z0 λ perc L perc (f • D(z 0 ), I) + λ mae f • D(z 0 ) -I (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Design</head><p>Dataset for Validation: We use 100 HR T1 MRIs from the IXI dataset (http://brain-development.org/ixi-dataset/) to validate our method, after filtering out those scans where registration failed. We note that subjects in the IXI dataset are around 10 years younger on average than those in UK Biobank.</p><p>The MRI scans from UK Biobank also had the faces masked out, while the scans from IXI did not. This caused the faces of our reconstructions to appear blurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation:</head><p>Conditional variables are all initialized to 0.5. Voxels in all input volumes are normalized to [0,1]. When sampling the pre-trained brain LDM with the DDIM sampler, we run T = 46 timesteps due to computational limitations on our hardware. For InverseSR(LDM), z T is initialized with random gaussian noise. For InverseSR(Decoder), we compute the mean latent code z0 as z0 = S i=1 1 S DDIM(z i T , C, T ) by first sampling S = 10, 000 z i T samples from N (0, I), then passing them through the DDIM model. N = 600 gradient descent steps are used for InverseSR(LDM) to guarantee converging (Algorithm 1, line 5). 600 optimization steps are also utilized in InverseSR(Decoder). We use the Adam optimizer with α = 0.07, β 1 = 0.9 and β 2 = 0.999.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the qualitative results on the coronal slices of SR from 4 and 8 mm axial scans. The advantage of our approach is clear compared to baseline methods because it is capable of restoring HR MRIs with smoothness even when the slice thickness is large (i.e., 8 mm). This is the case because the pre-trained LDM we use is able to build a powerful prior over the HR T1w MRI domain. Therefore, the generated images of our method are HR MRIs with smoothness in 3 directions: axial, sagittal and coronal, no matter how sparse the input images I are. Qualitative results of applying our method on tumour and lesion filling are available in the supplementary material.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows quantitative results on 100 HR T1 scans from the IXI dataset, which the brain LDM did not have access to during training. We investigated mean peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM) <ref type="bibr" target="#b28">[28]</ref> values and their corresponding standard deviation. We compare our method to cubic interpolation, as well as a similar unsupervised approach, UniRes <ref type="bibr" target="#b2">[3]</ref>. We show our approach and the two compared methods on two different settings of slice imputation: 4 mm and 8 mm thick-sliced axial scans representing low sparsity and high sparsity LR MRIs, respectively. All the metrics are computed on a 3D volume around the brain of size 160 × 224 × 160. For SR at 4 mm, InverseSR(Decoder) achieves the highest mean SSIM and PSNR scores among all compared methods, which are slightly higher than the scores for InverseSR(LDM). For SR at 8 mm, Inverse(LDM) achieves the highest mean SSIM and PSNR and lowest standard error than the two baseline methods, which could be attributed to the stronger prior learned by the DDIM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>One key limitation of our method is the need for large computational resources to perform the image reconstruction, in particular the long Markov chain of sampling steps required by the diffusion model to generate samples. An entire pass through the diffusion model (lines 6-8 in Algorithm 1) is required for every step in the gradient descent method. Another limitation of our method is that it is limited by the capacity and output heterogeneity of the LDM generator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this study, we have developed an unsupervised technique for MRI superresolution. We leverage a recent pre-trained Brain LDM <ref type="bibr" target="#b21">[21]</ref> for building powerful image priors over T1w brain MRIs. Unlike end-to-end supervised approaches, which require retraining each time there is a distribution shift over the input, our method is capable of being adapted to different settings of MRI SR problems at test time. This feature is suitable for MRI SR since the acquisition protocols and resolution of clinical brain MRI exams vary across or even within institutions. We proposed two novel strategies for different settings of MRI SR: InverseSR(LDM) for low sparsity MRI and InverseSR(Decoder) for high sparsity MRI. We validated our method on 100 brain T1w MRIs from the IXI dataset through slice imputation using input scans of 4 and 8 mm slice thickness, and compared our method with cubic interpolation and UniRes <ref type="bibr" target="#b2">[3]</ref>.</p><p>Experimental results have shown that our approach achieves superior performance compared to the unsupervised baselines, and could create smooth HR images with fine detail even on an external dataset (IXI). Experiments in this paper focus on slice imputation, but our method could be adapted to other MRI under-sampling problems by implementing different corruption functions f . For instance, for reconstructing k-space under-sampled MR images, a new corruption function could be designed by first converting the HR image into k-space, then masking a chosen set of k-space measurements, and then converting back to image space. Instead of estimating a single image, future work could also estimate a distribution of reconstructed images through either variational inference (like the BRGM model <ref type="bibr" target="#b18">[18]</ref>) or through sampling methods such as Markov Chain Monte Carlo (MCMC) or Langevin dynamics <ref type="bibr" target="#b15">[15]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (Left) The Brain LDM has two stages of training process. First, an autoencoder is pre-trained to map T1w brain MRIs into a latent code z0 = E(x). Then, a diffusion model is trained to generate z0 latents from this learned latent space. During inference, DDIM has been applied to reduce the sampling step with little performance drop. (Right) We proposed two methods to handle different scenarios of MRI SR, based on the architecture of brain LDM: 1) InverseSR(LDM): for SR with high sparsity, we optimize the latent code z * T and associated conditional variables C * using deterministic DDIM and decoder D to map the latent code into brain MRI. 2) InverseSR(Decoder): for SR with low sparsity, we optimize the z * 0 which only use the decoder D to map the latent code into brain MRI.</figDesc><graphic coords="4,61,47,54,23,329,38,136,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 . 1 : 2 :</head><label>112</label><figDesc>InverseSR(LDM) Input: Low-resolution MR image I. Output: Optimized noise latent code z * T and conditional variables C * 3: Initialize zT with gaussian noise from N (0, I); 4: Initialize conditional variables C = 0.5; 5: for j = 0, . . . , N -1 do 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative results of our approach (InverseSR) and the cubic and UniRes baselines on scans with 4 mm and 8 mm thickness.</figDesc><graphic coords="6,57,96,402,05,336,13,180,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation results (mean ± standard error) of our approach (InverseSR) and two baselines on 1 mm scans and corresponding SR counterpartfrom 4 and 8 mm axial scans.</figDesc><table><row><cell cols="2">Slice Thickness Methods</cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell></row><row><cell>4 mm</cell><cell>InverseSR(LDM)</cell><cell>0.797 ± 0.037</cell><cell>28.59 ± 1.61</cell></row><row><cell></cell><cell cols="3">InverseSR(Decoder) 0.803 ± 0.030 29.64 ± 1.64</cell></row><row><cell></cell><cell>Cubic</cell><cell>0.760 ± 0.052</cell><cell>23.84 ± 2.36</cell></row><row><cell></cell><cell>UniRes [3]</cell><cell>0.688 ± 0.079</cell><cell>21.49 ± 2.61</cell></row><row><cell>8 mm</cell><cell>InverseSR(LDM)</cell><cell cols="2">0.754 ± 0.038 27.92 ± 1.60</cell></row><row><cell></cell><cell>Cubic</cell><cell>0.632 ± 0.067</cell><cell>21.80 ± 2.36</cell></row><row><cell></cell><cell>UniRes [3]</cell><cell>0.633 ± 0.053</cell><cell>20.91 ± 2.29</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was fund by an <rs type="funder">NSERC</rs> <rs type="grantName">Discovery Grant</rs> to JL. Funding was also provided by a <rs type="grantName">Nova Scotia Graduate Scholarship</rs> and a <rs type="grantName">StFX Graduate Scholarship</rs> to JW. Computational resources were provided by <rs type="funder">Compute Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Vv2zGPM">
					<orgName type="grant-name">Discovery Grant</orgName>
				</org>
				<org type="funding" xml:id="_yaTTDPb">
					<orgName type="grant-name">Nova Scotia Graduate Scholarship</orgName>
				</org>
				<org type="funding" xml:id="_9EXfQGF">
					<orgName type="grant-name">StFX Graduate Scholarship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image2StyleGAN: how to embed images into the StyleGAN latent space?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4432" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Compressed sensing using generative models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Brudfors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Balbastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ashburner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01140</idno>
		<title level="m">A tool for super-resolving multimodal clinical MRI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VoxResNet: deep voxelwise residual networks for brain segmentation from 3D MR images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="446" to="455" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient and accurate MRI super-resolution using a generative adversarial network and 3D multilevel densely connected network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00928-111" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brain MRI super resolution using 3D deep densely connected neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="739" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14687</idno>
		<title level="m">Diffusion posterior sampling for general noisy inverse problems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Medical image imputation from image collections</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="504" to="514" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Direct inversion: optimization-free textdriven real image editing with diffusion models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elarabawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07825</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An image is worth one word: personalizing text-to-image generation using textual inversion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prompt-to-prompt image editing with cross attention control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Iglesias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page">118206</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust compressed sensing MRI with deep generative priors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arvinte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14938" to="14954" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Repaint: inpainting using denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11461" to="11471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayesian image reconstruction using deep generative models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pulse: self-supervised photo upsampling via latent space exploration of generative models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Null-text inversion for editing real images using guided diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09794</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Brain imaging generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H L</forename><surname>Pinaya</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-18576-2_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-18576-212" />
	</analytic>
	<monogr>
		<title level="m">DGM4MICCAI 2022</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Oksuz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13609</biblScope>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Autoencoding low-resolution MRI for semantically smooth interpolation of anisotropic MRI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102393</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Solving inverse problems in medical imaging with score-based generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08005</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sudlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Med</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enhanced generative adversarial network for 3D brain MRI super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3616" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Slice imputation: multiple intermediate slices interpolation for anisotropic 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page">105667</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gan inversion: a survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="3121" to="3138" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MR image super-resolution with squeeze and excitation reasoning attention network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13425" to="13434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Smore: a self-supervised anti-aliasing and super-resolution algorithm for MRI using deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Dewey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Calabresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="805" to="817" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
