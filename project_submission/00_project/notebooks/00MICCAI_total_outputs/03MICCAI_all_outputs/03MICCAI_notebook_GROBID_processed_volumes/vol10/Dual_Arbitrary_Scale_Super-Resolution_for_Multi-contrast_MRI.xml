<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI</title>
				<funder ref="#_SkWEcSk">
					<orgName type="full">Special Foundations for the Development of Strategic Emerging Industries of Shenzhen</orgName>
				</funder>
				<funder ref="#_KE5mqKH">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Oversea Cooperation Foundation of Tsinghua</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiamiao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yichen</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Lyu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Nursing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<addrLine>Hung Hom</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wenming</forename><surname>Yang</surname></persName>
							<email>yang.wenming@sz.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
								<address>
									<settlement>Richardson</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="282" to="292"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B01D8CE3DE479980046D105A4C5B6710</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_27</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>MRI Super-resolution</term>
					<term>Multi-contrast</term>
					<term>Arbitrary scale</term>
					<term>Implicit nerual representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging (MRI) images from partial measurement is essential to medical imaging research. Benefiting from the diverse and complementary information of multi-contrast MR images in different imaging modalities, multi-contrast Super-Resolution (SR) reconstruction is promising to yield SR images with higher quality. In the medical scenario, to fully visualize the lesion, radiologists are accustomed to zooming the MR images at arbitrary scales rather than using a fixed scale, as used by most MRI SR methods. In addition, existing multi-contrast MRI SR methods often require a fixed resolution for the reference image, which makes acquiring reference images difficult and imposes limitations on arbitrary scale SR tasks. To address these issues, we proposed an implicit neural representations based dual-arbitrary multi-contrast MRI super-resolution method, called Dual-ArbNet. First, we decouple the resolution of the target and reference images by a feature encoder, enabling the network to input target and reference images at arbitrary scales. Then, an implicit fusion decoder fuses the multi-contrast features and uses an Implicit Decoding Function (IDF) to obtain the final MRI SR results. Furthermore, we introduce a curriculum learning strategy to train our network, which improves the generalization and performance of our Dual-ArbNet. Extensive experiments in two public MRI datasets demonstrate that our method outperforms state-of-the-art approaches under different scale factors and has great potential in clinical practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic Resonance Imaging (MRI) is one of the most widely used medical imaging modalities, as it is non-invasive and capable of providing superior soft tissue contrast without causing ionizing radiation. However, it is challenging to acquire high-resolution MR images in practical applications <ref type="bibr" target="#b7">[8]</ref> due to the inherent shortcomings of the systems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref> and the inevitable motion artifacts of the subjects during long acquisition sessions.</p><p>Super-resolution (SR) techniques are a promising way to improve the quality of MR images without upgrading hardware facilities. Clinically, multi-contrast MR images, e.g., T1, T2 and PD weighted images are obtained from different pulse sequences <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>, which can provide complementary information to each other <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>. Although weighted images reflect the same anatomy, they excel at demonstrating different physiological and pathological features. Different time is required to acquire images with different contrast. In this regard, it is promising to leverage an HR reference image with a shorter acquisition time to reconstruct the modality with a longer scanning time. Recently, some efforts have been dedicated to multi-contrast MRI SR reconstruction. Zeng et al. proposed a deep convolution neural network to perform single-and multi-contrast SR reconstruction <ref type="bibr" target="#b26">[27]</ref>. Dar et al. concatenated information from two modalities into the generator of a generative adversarial network (GAN) <ref type="bibr" target="#b5">[6]</ref>, and Lyu et al. introduced a GAN-based progressive network to reconstruct multi-contrast MR images <ref type="bibr" target="#b14">[15]</ref>. Feng et al. used a multi-stage feature fusion mechanism for multi-contrast SR <ref type="bibr" target="#b6">[7]</ref>. Li et al. adopted a multi-scale context matching and aggregation scheme to gradually and interactively aggregate multi-scale matched features <ref type="bibr" target="#b11">[12]</ref>. Despite their effectiveness, these networks impose severe restrictions on the resolution of the reference image, largely limiting their applications. In addition, most existing multi-contrast SR methods only work with fixed integer scale factors and treat different scale factors as independent tasks. For example, they train a single model for a certain integer scale factor (×2, ×4). In consequence, using these fixed models for arbitrary scale SR is inadequate. Furthermore, in practical medical applications, it is common for radiologists to zoom in on MR images at will to see localized details of the lesion. Thus, there is an urgent need for an efficient and novel method to achieve superresolution of arbitrary scale factors in a single model.</p><p>In recent years, several methods have been explored for arbitrary scale super-resolution tasks on natural images, such as Meta-SR <ref type="bibr" target="#b8">[9]</ref> and Arb-SR <ref type="bibr" target="#b23">[24]</ref>. Although they can perform arbitrary up-sampling within the training scales, their generalization ability is limited when exceeding the training distribution, especially for large scale factors. Inspired by the success of implicit neural representation in modeling 3D shapes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>, several works perform implicit neural representations to the 2D image SR problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>. Since these methods can sample pixels at any position in the spatial domain, they can still perform well beyond the distribution of the training scale. Also, there is an MRI SR method that combines the meta-upscale module with GAN and performs arbitrary scale SR <ref type="bibr" target="#b21">[22]</ref>. However, the GAN-based method generates unrealistic textures, which affects the diagnosis accuracy.</p><p>To address these issues, we propose an arbitrary-scale multi-contrast MRI SR framework. Specifically, we introduce the implicit neural representation to multi-contrast MRI SR and extend the concept of arbitrary scale SR to the reference image domain. Our contributions are summarized as follows: Our Dual-ArbNet outperforms several state-of-the-art approaches on two benchmark datasets: fastMRI <ref type="bibr" target="#b25">[26]</ref> and IXI <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background: Implicit Neural Representations</head><p>As we know, computers use 2D pixel arrays to store and display images discretely. In contrast to the traditional discrete representation, the Implicit Neural Representation (INR) can represent an image I ∈ R H×W in the latent space F ∈ R H×W ×C , and use a local neural network (e.g., convolution with kernel 1) to continuously represent the pixel value at each location. This local neural network fits the implicit function of the continuous image, called Implicit Decoding Function (IDF). In addition, each latent feature represents a local piece of continuous image <ref type="bibr" target="#b3">[4]</ref>, which can be used to decode the signal closest to itself through IDF. Thus, by an IDF f (•) and latent feature F , we can arbitrarily query pixel value at any location, and restore images of arbitrary resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>The overall architecture of the proposed Dual-ArbNet is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The network consists of an encoder and an implicit fusion decoder. The encoder performs feature extraction and alignment of the target LR and the reference image. The implicit fusion decoder predicts the pixel values at any coordinate by fusing the features and decoding through IDF, thus achieving reconstruction.</p><p>Encoder. In the image encoder, Residual Dense Network (RDN) <ref type="bibr" target="#b28">[29]</ref> is used to extract image latent features for the network, and the reference image branch shares weights with the target LR image branch to achieve consistent feature extraction and reduce parameters. To aggregate the neighboring information in the reconstruction process, we further unfold the features of 3 × 3 neighborhoods around each pixel, expanding the feature channels nine times.</p><p>Since the resolution of target LR and reference image are different, we have to align them to target HR scale for further fusion. With the target image shaped H tar × W tar and reference image shaped </p><formula xml:id="formula_0">H ref × W ref ,</formula><formula xml:id="formula_1">F z↑ = U psample(RDN (I z ), S z )<label>(1)</label></formula><p>where z ∈ {ref, tar} indicates the reference and target image, I tar and I ref are the input target LR and reference image. In this way, we obtain the latent feature nearest to each HR pixel for further decoding, and our method can handle Arbitrary scale SR for target images with Arbitrary resolution of reference images (Dual-Arb).</p><p>Decoder. As described in Sect. 2.1, the INR use a local neural network to fit the continuous image representation, and the fitting can be referred to as Implicit Decoding Function (IDF). In addition, we propose a fusion branch to efficiently fuse the target and reference latent features for IDF decoding. The overall decoder includes a fusion branch and a shared IDF, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(see right).</p><p>Inspired by <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>, to better fuse the reference and target features in different dimensions, we use ResBlock with Channel Attention (CA) and Spatial Attention (SA) in our fusion branch. This 5 layers lightweight architecture can capture channel-wise and spatial-wise attention information and fuse them efficiently. The fusion process can be expressed as:</p><formula xml:id="formula_2">F (0) fusion = cat(F tar↑ , F ref ↑ ) F (i) fusion = L i (F (i-1) fusion ) + F (i-1) fusion , i = 1, 2, ..., 5<label>(2)</label></formula><p>where L i indicates the i-th fusion layer. Then, we equally divide the fused feature F The IDF in our method is stacked by convolution layer with kernel size 1 (conv 1 ) and sin activation function sin(•). The conv 1 and sin(•) are used to transform these inputs to higher dimension space <ref type="bibr" target="#b16">[17]</ref>, thus achieving a better representation of the IDF. Since conv 1 (x) can be written as W •x+b without using any adjacent features, this decoding function can query SR value at any given coordinate. Akin to many previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref>, relative coordinate information P (x, y) and scale factors S ref , S tar are necessary for the IDF to decode results continuously. At each target pixel (x, y), we only use local fused feature F fusion , which represents a local piece of continuous information, and coordinate P (x, y) relative to the nearest fused feature, as well as scale factors {S ref , S tar }, to query in the IDF. Corresponding to the fusion layer, we stack 6 convolution with activation layers. i-th layer's decoding function f (i) can be express as:</p><formula xml:id="formula_3">f (0) (x, y, z) = sin W (0) • cat (S tar , S ref , P (x, y)) + b (0) f (i) (x, y, z) = sin W (i) • f (i-1) (x, y, z) + b (i) F (i) f usion,z (x, y)<label>(3)</label></formula><p>where (x, y) is the coordinate of each pixel, and z ∈ {ref, tar} indicates the reference and target image. denotes element-wise multiplication, and cat is the concatenate operation. W (i) and b (i) are weight and bias of i-th convolution layer. Moreover, we use the last layer's output f (5) (•) as the overall decoding function f (•). By introducing the IDF above, the pixel value at any coordinates I z,SR (x, y) can be reconstructed:</p><formula xml:id="formula_4">I z,SR (x, y) = f (x, y, z) + Skip(F z↑ )<label>(4)</label></formula><p>where Skip(•) is skip connection branch with conv 1 and sin(•), z ∈ {ref, tar}.</p><p>Loss Function. An L1 loss between target SR results I target,SR and HR images I HR is utilized as reconstruction loss to improve the overall detail of SR images, named as L rec . The reconstructed SR images may lose some frequency information in the original HR images. K-Loss <ref type="bibr" target="#b29">[30]</ref> is further introduced to alleviate the problem. Specifically, K SR and K HR denote the fast Fourier transform of I target,SR and I HR . In k-space, the value of mask M is set to 0 in the highfrequency cut-off region mentioned in Sect. 3, otherwise set to 1. L2 loss is used to measure the error between K SR and K HR . K-Loss can be expressed as:</p><formula xml:id="formula_5">L K = (K SR -K HR ) • M 2<label>(5)</label></formula><p>To this end, the full objective of the Dual-ArbNet is defined as:</p><formula xml:id="formula_6">L full = L rec + λ K L K (6)</formula><p>We set λ K = 0.05 empirically to balance the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Curriculum Learning Strategy</head><p>Curriculum learning <ref type="bibr" target="#b1">[2]</ref> has shown powerful capabilities in improving model generalization and convergence speed. It mimics the human learning process by allowing the model to start with easy samples and gradually progress to complex samples. To achieve this and stabilize the training process with different references, we introduce curriculum learning to train our model, named Cur-Random. This training strategy is divided into three phases, including warm-up, pre-learning, and full-training. Although our image encoder can be fed with reference images of arbitrary resolution, it is more common to use LR-ref (scale as target LR) or HR-ref (scale as target HR) in practice. Therefore, these two scales of reference images are used as our settings.</p><p>In the warm-up stage, we fix the integer SR scale to integer (2×, 3× and 4×) and use HR-Ref to stable the training process. Then, in the pre-learning stage, we use arbitrary scale target images and HR reference images to quickly improve the network's migration ability by learning texture-rich HR images. Finally, in the full-training stage, we train the model with a random scale for reference and target images, which further improves the generalization ability of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. Two public datasets are utilized to evaluate the proposed Dual-ArbNet network, including fastMRI <ref type="bibr" target="#b25">[26]</ref> (PD as reference and FS-PD as target) and IXI dataset <ref type="bibr" target="#b0">[1]</ref> (PD as reference and T2 as target). All the complex-valued images are cropped to integer multiples of 24 (as the smallest common multiple of the test scale). We adopt a commonly used down-sampling treatment to crop the k-space. Concretely, we first converted the original image into the k-space using Fourier transform. Then, only data in the central low-frequency region are kept, and all high-frequency information is cropped out. For the down-sampling factors k, only the central 1 k 2 frequency information is kept. Finally, we used the inverse Fourier transform to convert the down-sampled data into the image domain to produce the LR image.</p><p>We compared our Dual-ArbNet with several recent state-of-the-art methods, including two multi-contrast SR methods: McMRSR <ref type="bibr" target="#b11">[12]</ref>, WavTrans <ref type="bibr" target="#b12">[13]</ref>, and three arbitrary scale image SR methods: Meta-SR <ref type="bibr" target="#b8">[9]</ref>, LIIF <ref type="bibr" target="#b3">[4]</ref>, Diinn <ref type="bibr" target="#b16">[17]</ref>.</p><p>Experimental Setup. Our proposed Dual-ArbNet is implemented in PyTorch with NVIDIA GeForce RTX 2080 Ti. The Adam optimizer is adopted for model training, and the learning rate is initialized to 10 -4 at the full-training stage for all the layers and decreases by half for every 40 epochs. We randomly extract 6 LR patches with the size of 32×32 as a batch input. Following the setting in <ref type="bibr" target="#b8">[9]</ref>, we augment the patches by randomly flipping horizontally or vertically and rotating 90 • . The training scale factors of the Dual-ArbNet vary from 1 to 4 with stride 0.1, and the distribution of the scale factors is uniform. The performance of the SR reconstruction is evaluated by PSNR and SSIM.</p><p>Quantitative Results. Table <ref type="table" target="#tab_0">1</ref> reports the average SSIM and PSNR with respect to different datasets under in-distribution and out-of-distribution large scales. Since the SR scale of McMRSR <ref type="bibr" target="#b11">[12]</ref> and WavTrans <ref type="bibr" target="#b12">[13]</ref> is fixed to 2× and 4×, we use a 2× model and down-sample the results when testing 1.5×. We use the 4× model and up-sample the results to test 6× and 8×, and down-sample the results to test 3× results. Here, we provide the results with the reference image at HR resolution. As can be seen, our method yields the best results in all datasets. Notably, for out-of-distribution scales, our method performs even significantly better than existing methods. The results confirm that our framework outperforms the state-of-the-art in terms of performance and generalizability.</p><p>Qualitative Evaluation. Figure <ref type="figure" target="#fig_3">2</ref> provides the reconstruction results and the corresponding error maps of the in-distribution scale (4×) and out-ofdistribution scale (6×). The more obvious the texture in the error map, the worse the reconstruction means. As can be observed, our reconstructed images  As can be seen that the reconstruction results of w/o coord and w/o scale are not optimal because coordinates and scale can provide additional information for the implicit decoder. We observe that w/o ref has the worst results, indicating that the reference image can provide auxiliary information for super-resolving the target image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed the Dual-ArbNet for MRI SR using implicit neural representations, which provided a new paradigm for multi-contrast MRI SR tasks. It can perform arbitrary scale SR on LR images at any resolution of reference images. In addition, we designed a new training strategy with reference to the idea of curriculum learning to further improve the performance of our model. Extensive experiments on multiple datasets show that our Dual-ArbNet achieves state-of-the-art results both within and outside the training distribution. We hope our work can provide a potential guide for further studies of arbitrary scale multi-contrast MRI SR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall architecture of the proposed Dual-ArbNet. Our Dual-ArbNet includes a share-weighted image encoder and an implicit fusion decoder which contains a lightweight fusion branch and an implicit decoding function.</figDesc><graphic coords="3,44,79,54,50,334,60,119,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>we use nearest interpolation to efficiently up-sample their feature maps to the target HR scale H HR × W HR by two different factors S ref and S tar :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>fusion by channel into F (i) f usion,tar and F (i) f usion,ref for decoding respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative results and error maps of different SR methods on fastMRI and IXI dataset. The color bar on the right indicates the value of the error map. Our method can reconstruct fewer blocking artifacts and sharper texture details.</figDesc><graphic coords="8,58,98,60,26,334,72,185,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison with other methods. Best and second best results are highlighted and underlined.</figDesc><table><row><cell cols="2">Dataset Methods</cell><cell cols="2">In distribution</cell><cell></cell><cell></cell><cell cols="2">Out-of distribution Average</cell></row><row><cell></cell><cell></cell><cell>×1.5</cell><cell>×2</cell><cell>×3</cell><cell>×4</cell><cell>×6</cell><cell>×8</cell><cell>PSNR SSIM</cell></row><row><cell>fast</cell><cell cols="7">McMRSR [12] 37.773 34.546 31.087 30.141 27.859 26.200</cell><cell>31.268 0.889</cell></row><row><cell></cell><cell cols="7">WavTrans [13] 36.390 32.841 31.153 30.197 28.360 26.722</cell><cell>30.944 0.890</cell></row><row><cell></cell><cell>Meta-SR [9]</cell><cell cols="6">37.243 33.867 31.047 29.604 27.552 24.536</cell><cell>30.642 0.880</cell></row><row><cell></cell><cell>LIIF [4]</cell><cell cols="6">37.868 34.320 31.717 30.301 28.485 26.273</cell><cell>31.494 0.892</cell></row><row><cell></cell><cell>Diinn [17]</cell><cell cols="6">37.405 34.182 31.666 30.243 28.382 24.804</cell><cell>31.114 0.887</cell></row><row><cell></cell><cell>Ours</cell><cell cols="6">38.139 34.722 32.046 30.707 28.693 26.419</cell><cell>31.788 0.896</cell></row><row><cell>IXI</cell><cell cols="7">McMRSR [12] 37.450 37.046 34.416 33.910 29.765 27.239</cell><cell>33.304 0.914</cell></row><row><cell></cell><cell cols="7">WavTrans [13] 39.118 38.171 37.670 35.805 31.037 27.832</cell><cell>34.940 0.958</cell></row><row><cell></cell><cell>Meta-SR [9]</cell><cell cols="6">42.740 36.115 32.280 29.219 25.129 23.003</cell><cell>31.414 0.916</cell></row><row><cell></cell><cell>LIIF [4]</cell><cell cols="6">41.724 36.818 33.001 30.366 26.502 24.194</cell><cell>32.101 0.934</cell></row><row><cell></cell><cell>Diinn [17]</cell><cell cols="6">43.277 37.231 33.285 30.575 26.585 24.458</cell><cell>32.569 0.936</cell></row><row><cell></cell><cell>Ours</cell><cell cols="6">43.964 40.768 38.241 36.816 33.186 29.537</cell><cell>37.085 0.979</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on different training strategies (top) and key components (bottom) under fastMRI dataset. Best results are highlighted.</figDesc><table><row><cell>TrainRef</cell><cell cols="3">TestRef ×1.5</cell><cell>×2</cell><cell>×3</cell><cell>×4</cell><cell></cell><cell>×6</cell><cell>×8</cell><cell>average</cell></row><row><cell>LR</cell><cell></cell><cell>LR</cell><cell cols="6">37.911 34.475 31.705 30.219 28.137 24.245 31.115</cell></row><row><cell>LR</cell><cell></cell><cell>HR</cell><cell cols="6">36.954 34.232 31.615 30.031 27.927 24.455 30.869</cell></row><row><cell>HR</cell><cell></cell><cell>LR</cell><cell cols="6">35.620 33.007 30.268 28.789 26.624 24.942 29.875</cell></row><row><cell>HR</cell><cell></cell><cell>HR</cell><cell cols="6">36.666 34.274 31.916 30.766 28.392 26.359 31.395</cell></row><row><cell>Random</cell><cell></cell><cell>LR</cell><cell cols="6">38.143 34.423 31.669 30.173 27.975 25.182 31.261</cell></row><row><cell>Random</cell><cell></cell><cell>HR</cell><cell cols="6">38.140 34.640 32.025 30.712 28.647 26.355 31.753</cell></row><row><cell>Cur-Random</cell><cell></cell><cell>LR</cell><cell cols="6">38.063 34.489 31.684 30.177 28.038 25.264 31.286</cell></row><row><cell>Cur-Random</cell><cell></cell><cell>HR</cell><cell cols="6">38.139 34.722 32.046 30.707 28.693 26.419 31.788</cell></row><row><cell>Setting</cell><cell cols="4">Ref Scales Coord ×1.5</cell><cell>×2</cell><cell>×3</cell><cell>×4</cell><cell>×6</cell><cell>×8 average</cell></row><row><cell>w/o ref</cell><cell>✗</cell><cell>✗</cell><cell cols="6">✓ 37.967 34.477 31.697 30.214 28.154 24.996 31.251</cell></row><row><cell cols="2">w/o scale ✓</cell><cell>✗</cell><cell cols="6">✓ 37.951 34.663 32.063 30.681 28.623 26.413 31.732</cell></row><row><cell cols="2">w/o coord ✓</cell><cell>✓</cell><cell>✗</cell><cell cols="5">38.039 34.706 32.036 30.702 28.592 26.288 31.727</cell></row><row><cell cols="2">Dual-ArbNet ✓</cell><cell>✓</cell><cell cols="6">✓ 38.139 34.722 32.046 30.707 28.693 26.419 31.788</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 (</head><label>2</label><figDesc>top). Regarding the type of reference image, we use HR, LR, Random, Cur-Random for training, and HR, LR for testing. As can be seen, the domain gap appears in inconsistent training-testing pairs, while Random training can narrow this gap and enhance the performance. In addition, the HR-Ref performs better than the LR-Ref due to its rich detail and sharp edges, especially in large scale factors. Based on the Random training, the Cur-Random strategy can further improve the performance and achieve balanced SR results. Ablation Study on Key Components. In Table 2(bottom), to evaluate the validity of the key components of Dual-ArbNet, we conducted experiments without introducing coordinate information, thus verifying the contribution of coordinate in the IDF, named w/o coord. The setting without introducing scale factors in implicit decoding is designed to verify the effect of scale factors on model performance, named w/o scale. To verify whether the reference image can effectively provide auxiliary information for image reconstruction and better restore SR images, we further designed a single-contrast variant model without considering the reference image features in the model, named w/o ref. All the settings use Cur-Random training strategy.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partly supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62171251 &amp; 62311530100</rs>), the <rs type="funder">Special Foundations for the Development of Strategic Emerging Industries of Shenzhen</rs> (Nos. <rs type="grantNumber">JCYJ20200109143010272 &amp; CJGJZD20210408092804011</rs>) and <rs type="funder">Oversea Cooperation Foundation of Tsinghua</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KE5mqKH">
					<idno type="grant-number">62171251 &amp; 62311530100</idno>
				</org>
				<org type="funding" xml:id="_SkWEcSk">
					<idno type="grant-number">JCYJ20200109143010272 &amp; CJGJZD20210408092804011</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_27.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://brain-development.org/ixi-dataset/" />
		<title level="m">Ixi dataset</title>
		<imprint>
			<date type="published" when="2023-02-20">20 Feb 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accuracy of 3-t MRI using susceptibility-weighted imaging to detect meniscal tears of the knee</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knee Surg. Sports Traumatol. Arthrosc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="198" to="204" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning continuous image representation with local implicit image function</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8628" to="8638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5939" to="5948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prior-guided image reconstruction for accelerated multi-contrast MRI via generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahdloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Ildız</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tınaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cukur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1072" to="1087" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-contrast MRI super-resolution via a multi-stage integration network</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_14" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Brain MRI super-resolution using coupled-projection residual network</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">456</biblScope>
			<biblScope unit="page" from="190" to="199" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meta-SR: a magnificationarbitrary network for super-resolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1575" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Local implicit grid representations for 3D scenes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6001" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local texture estimator for implicit representation function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1929" to="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transformer-empowered multi-scale contextual matching and aggregation for multi-contrast MRI super-resolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20636" to="20645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wavtrans: synergizing wavelet and cross-attention transformer for multi-contrast mri super-resolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_44" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VI</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="463" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the regularization of feature fusion and mapping for fast mr multi-contrast imaging via iterative networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Crozier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-contrast super-resolution MRI through a progressive network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2738" to="2749" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Occupancy networks: learning 3D reconstruction in function space</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4460" to="4470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image super-resolution via a dual interactive implicit neural network</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Beksi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4936" to="4945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepsDF: learning continuous signed distance functions for shape representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Super-resolution methods in MRI: can they improve the trade-off between resolution, signal-to-noise ratio, and acquisition time?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Plenge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1983" to="1993" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7462" to="7473" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting more for less: multi-echo mp2rage for simultaneous t1weighted imaging, t1 mapping, mapping, SWI, and QSM from a single acquisition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1178" to="1191" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Arbitrary scale super-resolution for brain MRI images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-49161-1_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-49161-1_15" />
	</analytic>
	<monogr>
		<title level="m">AIAI 2020</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Maglogiannis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Iliadis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pimenidis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">583</biblScope>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Super-resolution in magnetic resonance imaging: a review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Reeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Poh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concepts Magn. Reson. Part A</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="306" to="325" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a single network for scale-arbitrary super-resolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4801" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer vision (ECCV)</title>
		<meeting>the European Conference on Computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08839</idno>
		<title level="m">fastMRI: an open dataset and benchmarks for accelerated mri</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simultaneous singleand multi-contrast super-resolution for brain MRI images based on a convolutional neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="133" to="141" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DudorNet: learning a dual-domain recurrent network for fast MRI reconstruction with deep t1 prior</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4273" to="4282" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
