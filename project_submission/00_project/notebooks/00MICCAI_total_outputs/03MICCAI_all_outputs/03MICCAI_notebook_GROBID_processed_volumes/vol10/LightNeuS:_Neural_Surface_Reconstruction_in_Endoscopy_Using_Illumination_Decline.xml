<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline</title>
				<funder ref="#_FMPxSBz">
					<orgName type="full">EU-H2020</orgName>
				</funder>
				<funder ref="#_sqKn8WZ">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_aqtm4sx #_vZXPJDV">
					<orgName type="full">Aragón government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Víctor</forename><forename type="middle">M</forename><surname>Batlle</surname></persName>
							<email>vmbatlle@unizar.es</email>
							<affiliation key="aff0">
								<orgName type="department">Inst. Investigación en Ingeniería de Aragón</orgName>
								<orgName type="laboratory">I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">José</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inst. Investigación en Ingeniería de Aragón</orgName>
								<orgName type="laboratory">I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<email>pascal.fua@epfl.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
							<email>tardos@unizar.es</email>
							<affiliation key="aff0">
								<orgName type="department">Inst. Investigación en Ingeniería de Aragón</orgName>
								<orgName type="laboratory">I3A</orgName>
								<orgName type="institution">Universidad de Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="502" to="512"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">98C59037BFD6D10FC7128749A51E3466</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reconstruction</term>
					<term>Photometric multi-view</term>
					<term>Endoscopy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new approach to 3D reconstruction from sequences of images acquired by monocular endoscopes. It is based on two key insights. First, endoluminal cavities are watertight, a property naturally enforced by modeling them in terms of a signed distance function. Second, the scene illumination is variable. It comes from the endoscope's light sources and decays with the inverse of the squared distance to the surface. To exploit these insights, we build on NeuS [25], a neural implicit surface reconstruction technique with an outstanding capability to learn appearance and a SDF surface model from multiple views, but currently limited to scenes with static illumination. To remove this limitation and exploit the relation between pixel brightness and depth, we modify the NeuS architecture to explicitly account for it and introduce a calibrated photometric model of the endoscope's camera and light source.</p><p>Our method is the first one to produce watertight reconstructions of whole colon sections. We demonstrate excellent accuracy on phantom imagery. Remarkably, the watertight prior combined with illumination decline, allows to complete the reconstruction of unseen portions of the surface with acceptable accuracy, paving the way to automatic quality assessment of cancer screening explorations, measuring the global percentage of observed mucosa.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colorectal cancer (CRC) is the third most commonly diagnosed cancer and is the second most common cause of cancer death <ref type="bibr" target="#b22">[23]</ref>. Early detection is crucial for a good prognosis. Despite the existence of other techniques, such as virtual colonoscopy (VC), optical colonoscopy (OC) remains the gold standard for colonoscopy screening and the removal of precursor lesions. Unfortunately, we do not yet have the ability to reconstruct densely the 3D shape of large sections of the colon. This would usher exciting new developments, such as post-intervention diagnosis, measuring polyps and stenosis, and automatically evaluating exploration thoroughness in terms of the surface percentage that has been observed. This is the problem we address here. It has been shown that the colon 3D shape can be estimated from single images acquired during human colonoscopies <ref type="bibr" target="#b2">[3]</ref>. However, to model large sections of it while increasing the reconstruction accuracy, multiple images must be used. As most endoscopes contain a single camera, the natural way to do this is to use video sequences acquired by these cameras in the manner of structure-from-motion algorithms. An important first step in that direction is to register the images from the sequences. This can now be done reliably using either batch <ref type="bibr" target="#b20">[21]</ref> or SLAM techniques <ref type="bibr" target="#b7">[8]</ref>. Unfortunately, this solves only half the problem because these techniques provide very sparse reconstructions and going from there to dense ones remains an open problem. And occlusions, specularities, varying albedos, and specificities of endoscopic lighting make it a challenging one.</p><p>To overcome these difficulties, we rely on two properties of endoscopic images:</p><p>-Endoluminal cavities such as the gastrointestinal tract, and in particular the human colon, are watertight surfaces. To account for this, we represent its surface in terms of a signed distance function (SDF), which by its very nature presents continuous watertight surfaces. -In endoscopy the light source is co-located with the camera. It illuminates a dark scene and is always close to the surface. As a result, the irradiance decreases rapidly with distance t from camera to surface; more specifically it is a function of 1/t 2 . In other words, there is a strong correlation between light and depth, which remains unexploited to date.</p><p>To take advantage of these specificities, we build on the success of Neural implicit Surfaces (NeuS) <ref type="bibr" target="#b24">[25]</ref> that have been shown to be highly effective at deriving surface 3D models from sets of registered images. As the Neural Radiance Fields (NeRFs) <ref type="bibr" target="#b14">[15]</ref> that inspired them, they were designed to operate on regular images taken around a scene, sampling fairly regularly the set of possible viewing directions. Furthermore, the lighting is assumed to be static and distant so that the brightness of a pixel and its distance to the camera are unrelated. Unfortunately, none of these conditions hold in endoscopies. The camera is inside a cavity (in the colon, a roughly cylindrical tunnel) that limits viewing directions. The light source is co-located with the camera and close to the surface, which results in a strong correlation between pixel brightness and distance to the camera. In this paper, we show that, far from being a handicap, this correlation is a key information for neural network self-supervision.</p><p>NeuS training selects a pixel from an image and samples points along its projecting ray. However, the network is agnostic to the sampling distance. In LightNeuS, we explicitly feed to the renderer the distance of each one of these sampled points to the light source, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Hence, the renderer can exploit the inverse-square illumination decline. We also introduce and calibrate a photometric model for the endoscope light and camera, so that the inverse square law discussed above actually holds. Together, these two changes make the minimization problem better posed and the automatic depth estimation more reliable.</p><p>Our results show that exploiting the illumination is key to unlocking implicit neural surface reconstruction in endoscopy. It delivers accuracies in the range of 3 mm, whereas an unmodified NeuS is either 5 times less accurate or even fails to reconstruct any surface at all. Earlier methods <ref type="bibr" target="#b2">[3]</ref> have reported similar accuracies but only on very few synthetic images and on short sections of the colon. By contrast, we can handle much longer ones and provide a broad evaluation in a real dataset (C3VD) over multiple sequences. This makes us the first to show accurate results of extended 3D watertight surfaces from monocular endoscopy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>3D Reconstruction from Endoscopic Images. It can help with the effective localization of lesions, such as polyps and adenomas, by providing a complete representation of the observed surface. Unfortunately, many state-of theart SLAM techniques based on feature matching <ref type="bibr" target="#b4">[5]</ref> or direct methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> are impractical for dense endoscopic reconstruction due to the lack of texture and the inconsistent lighting that moves along with the camera. Nevertheless, sparse reconstructions by classical Structure-from-Motion (SfM) algorithms can be good starting points for refinement and densification based on Shape-from-Shading (SfS) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. However, classical multi-view and SfS methods require strong suboptimal priors on colon surface shape and reflectance.</p><p>In monocular dense reconstructions, it is common practice to encode shape priors in terms of smooth rigid surfaces <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>. Recently, <ref type="bibr" target="#b21">[22]</ref> proposes a tubular topology prior for NRSfM aimed to process endoluminal cavities where these tubular shapes are prevalent. In contrast, for the same environments, we propose the watertight prior coded by implicit SDF representations.</p><p>Recent methods for dense reconstruction rely on neural networks to predict per-pixel depth in the 2D space of each image and fuse the depth maps by using multi-view stereo (MVS) <ref type="bibr" target="#b1">[2]</ref> or a SLAM pipeline <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. However, holes in the reconstruction appear due to failures in triangulation and inaccurate depth estimation or in areas not observed in any image. Wang et al. <ref type="bibr" target="#b26">[27]</ref> show the potential of neural rendering in reconstruction from medical images, although they use a binocular static camera with fixed light source, which is not feasible in endoluminal endoscopy. Unfortunately, most of the previous 3D methods do not provide code <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, are not evaluated in biomedical settings <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>, or do not report reconstruction accuracy <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Neural Radiance Fields (NeRFs) were first proposed to reconstruct novel views of non-Lambertian objects <ref type="bibr" target="#b14">[15]</ref>. This method provides an implicit neural representation of a scene in terms of local densities and associated colors. In effect, the scene representation is stored in the weights of a neural network, usually a multilayer perceptron (MLP), that learns its shape and reflectance for any coordinate and viewing direction. NeRFs use volume rendering <ref type="bibr" target="#b8">[9]</ref>, based on ray-tracing from multiple camera positions. The volume density σ(x) can be interpreted as the differential probability of a ray terminating at an infinitesimal particle at location x. The expected color C(r) of the pixel with camera ray r(t) = o + td is the integration of the radiance emitted by the field at every traveled distance t from near to far bounds t n and t f , such that where c stands for the color. The function T denotes the accumulated transmittance along the ray from t n to t, that is the probability that the ray travels from t n to t without hitting any other particle. The authors propose two MLPs to estimate the volume density function σ : x → [0, 1] and the directional emitted color function c : (x, d) → [0, 1] 3 , so the density of a point does not depend on the viewing direction d, but the color does. This allows them to model non-Lambertian reflectance. In addition, they propose a positional encoding for location x and direction d, which allows high-frequency details in the reconstruction.</p><formula xml:id="formula_0">C(r) =</formula><p>Neural Implicit Surfaces (NeuS) were introduced in <ref type="bibr" target="#b24">[25]</ref> to improve the quality of NeRF representation modelling watertight surfaces. For that, the volume density σ is computed so as to be maximal at the zero-crossings of a signed distance function (SDF) f :</p><formula xml:id="formula_1">σ(r(t)) = max Φ s (f (r(t))) Φ s (f (r(t))) , 0 where Φ s (x) = 1 1 + e -sx<label>(2)</label></formula><p>The SDF formulation makes it possible to estimate the surface normal as n = ∇f (x). The reflectance of a material is usually determined as a function of the incoming and outgoing light directions with respect to the surface normal. Therefore, the normal is added as an input to the MLP that estimates color c : (x, d, n), as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LightNeuS</head><p>In this section, we present the key contributions that make LightNeuS a neural implicit reconstruction method suitable for endoscopy in endoluminal cavities. In this context, the light source is located next to the camera and moves with it. Furthermore, it is close to the surfaces to be modeled. As a result, for any surface point x = o+td, the irradiance decreases with the square of the distance to the camera t. Hence, we can write the color of the corresponding pixel as <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_2">I(x) = L e t 2 BRDF(x, d) cos (θ) g 1/γ<label>(3)</label></formula><p>where L e is the radiance emitted by the light source to the surface point, that was modeled and calibrated in the EndoMapper dataset <ref type="bibr" target="#b0">[1]</ref> according to the SLS model from <ref type="bibr" target="#b15">[16]</ref>. The bidirectional reflectance distribution function (BRDF) determines how much light is reflected to the camera, and the cosine term cos (θ) = -d • n weights the incoming radiance with respect to the surface normal n. Equation ( <ref type="formula" target="#formula_2">3</ref>) also takes into account the camera gain g and gamma correction γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Using Illumination Decline as a Depth Cue</head><p>The NeuS formulation of Sect. 2 assumes distant and fixed lighting. However, in endoscopy inverse-square light decline is significant, as quantified in Eq. ( <ref type="formula" target="#formula_2">3</ref>).</p><p>Accounting for this is done by modifying the original NeuS formulation as follows. Figure <ref type="figure" target="#fig_0">1</ref>   <ref type="figure">c(x,</ref><ref type="figure">d,</ref><ref type="figure">n</ref>) may learn to model non-Lambertian BRDF(x, d), including specular highlights, and the cosine term of Eq. ( <ref type="formula" target="#formula_2">3</ref>). However, if the distance t from the light to the point x is not provided to the color network, the 1/t 2 dependency cannot be learned, and surface reconstruction will fail. Our key insight is to explicitly supply this distance as input to the volume rendering algorithm, as shown in red in Fig. <ref type="figure" target="#fig_0">1</ref> and reformulate Eq. (1) as</p><formula xml:id="formula_3">C(r) = t f tn T (t) σ(r(t)) c(r(t), d, n) t 2 dt (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>This conceptually simple change, using illumination decline while training, unlocks all the power of neural surface reconstruction in endoscopy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Endoscope Photometric Model</head><p>Apart from illumination decline, there are several significant differences between the images captured by endoscopes and those conventionally used to train NeRFs and NeuS: fish-eye lenses, strong vignetting, uneven scene illumination, and postprocessing.</p><p>Endoscopes use fisheye lenses to cover a wide field of view, usually close to 170 • . These lenses produce strong deformations, making it unwise to use the standard pinhole camera model. Instead, specific models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref> must be used. Hence, we also modified the original NeuS implementation to support these models.</p><p>The light sources of endoscopes behave like spotlights. In other words, they do not emit with the same intensity in all directions, so L e in Eq. ( <ref type="formula" target="#formula_2">3</ref>) is not constant for all image pixels. This effect is similar to the vignetting effect caused by conventional lenses, that is aggravated in fisheye lenses. Fortunately, they can be accurately calibrated <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> and compensated for.</p><p>The post-processing software of medical endoscopes is designed to always display well-exposed images, so that physicians can see details correctly. An adaptive gain factor g is applied by the endoscope's internal logic and gamma correction is also used to adapt to non-linear human vision, achieving better contrast perception in mid tones and dark areas. Endoscope manufacturers know the post-processing logic of their devices, but this information is proprietary and not available to users. Again, gamma correction can be calibrated assuming it is constant <ref type="bibr" target="#b2">[3]</ref>, and the gain change between successive images can be estimated, for example, by sparse feature matching.</p><p>All these factors must be taken into account during network training. Thus, our photometric loss is computed using a normalized image:</p><formula xml:id="formula_5">I = I γ L e g 1/γ</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We validate our method on the C3VD dataset <ref type="bibr" target="#b3">[4]</ref>, which covers all different sections of the colon anatomy in 22 video sequences. This dataset contains sequences recorded with a medical video colonoscope, Olympus Evis Exera III CF-HQ190L. The images were recorded inside a phantom, a model of a human colon made of silicone. The intrinsic camera parameters are provided. The camera extrinsics for each frame are estimated by 2D-3D registration against the known 3D model. In an operational setting, we could use a structure-from-motion approach such as COLMAP <ref type="bibr" target="#b20">[21]</ref> or a SLAM technique such as <ref type="bibr" target="#b7">[8]</ref>, which have been shown to work well in endoscopic settings. The gain values were easily estimated from the dataset itself. For vignetting, we use the calibration obtained from a colonoscope of the same brand and series from the EndoMapper dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>During training, we follow the NeuS paper approach of using a few informative frames per scene, as separated as possible, by sampling each video uniformly. For each sequence, we train both the vanilla NeuS and our LighNeuS using 20 frames each time. They are extracted uniformly over the duration of the video. We use the same batch size and number of iterations as in the original NeuS paper, 512 and 300k respectively. Once the network is trained, we can extract triangulated meshes from the reconstruction. Since the C3VD dataset comprises a ground-truth triangle mesh, we compute point-to-triangle distances from all the vertices in the reconstruction to the closest ground-truth triangle.</p><p>In the first rows of Table <ref type="table" target="#tab_1">1</ref>, we report median (MedAE), mean (MAE), and root mean square (RMSE) values of these distances for all vertices seen in at least one image. Columns show the result for 22 sequences. We note 18 sequences where the camera moved at least 1 cm, and the reconstruction yielded a mean error of 2.80 mm. The other four smaller trajectories (&lt;1 cm) lack parallax and the mean error is higher <ref type="bibr">(8.23 mm)</ref>. This is in the range of reported accuracy in the literature for monocular dense non-watertight depth estimation, 1.1 mm in <ref type="bibr" target="#b13">[14]</ref> for high parallax geometry in laparoscopy, which is a much more favorable geometry than the one we have here, or 0.85 mm for the significantly smaller-size cavities of endoscopic endonasal surgery (ESS) <ref type="bibr" target="#b10">[11]</ref>.</p><p>In contrast, vanilla NeuS assumes constant illumination. The strong light changes typical of endoscopy fatally mislead the method. We only report numerical results of NeuS in two sequences because in all the rest, the SDF diverges and ends up blown out of the rendering volume, giving no result at all. The NeuS reconstruction exhibits multiple artifacts that make it unusable. Bottom: Our reconstruction is much closer to the ground truth shape. The error is shown in blue if the reconstruction is inside the surface, and in red otherwise. A fully saturated red or blue denotes an error of more than 1 cm and grey denotes no error at all. We provide a qualitative result in Fig. <ref type="figure" target="#fig_2">2</ref> and additional ones in the supplementary material. Note that the watertight prior inherent to an SDF allows the network to hallucinate unseen areas. Remarkably, these unsurveyed areas continue the tubular shape of the colon and we found them to be mostly accurate when compared to the ground truth. For example, the curved areas of the colon where a wall is occluded behind the corner of the curve is reconstructed, as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. This ability to "fill in" observation gaps may be useful in providing the endoscopist with an estimate of the percentage of unsurveyed area during a procedure.</p><p>We hypothesize that this desirable behavior stems from the fact that the network learns an empirical shape prior from the observed anatomy of the colon. However, we don't expect this behavior to hold for distant unseen parts, but only for regions closer than 20 mm to one observation. In the last rows of Table <ref type="table" target="#tab_1">1</ref>, we compute accuracy metrics for this extended region. It includes not only surveyed areas, but also neighboring areas that were not observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a method for 3D dense multi-view reconstruction from endoscopic images. We are the first to show that neural radiance fields can be used to obtain accurate dense reconstructions of colon sections of significant length. At the heart of our approach, is exploiting the correlation between depth and brightness. We have observed that, without it, neural reconstruction fails.</p><p>The current method could be used offline for post-exploration coverage analysis and endoscopist training. But real-time performance could be achieved in the future as the new NeuS2 <ref type="bibr" target="#b25">[26]</ref> converges in minutes, enabling automatic coverage reporting. Similar to other reconstruction methods, for now our approach works in areas of the colon where there is little deformation. Several sub-maps of non-deformed areas can be created if necessary. However, this limitation could be overcome by adopting the deformable NeRFs formalism <ref type="bibr" target="#b17">[18]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. From NeuS to LightNeuS. The original NeuS architecture is depicted by the black arrows. In LightNeuS, when training the network with a sampled point, we provide the sampling distance t to the renderer, that takes into account illumination decline. We also incorporate a calibrated photometric endoscope model that is used to correctly compute the photometric loss. The changes are shown in red. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t f tn T (t) σ(r(t)) c(r(t), d) dt where T (t) = exp -t tn σ(r(s)) ds (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Benefits of illumination decline. Result on the "Cecum 1 a" sequence. Top:The NeuS reconstruction exhibits multiple artifacts that make it unusable. Bottom: Our reconstruction is much closer to the ground truth shape. The error is shown in blue if the reconstruction is inside the surface, and in red otherwise. A fully saturated red or blue denotes an error of more than 1 cm and grey denotes no error at all.</figDesc><graphic coords="8,116,55,65,00,225,46,138,70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Reconstructing partially observed regions. Results on "Transcending 4 a" sequence. The camera performs a short trajectory from (a) to (b). In (c) we represent both frames and intermediate camera poses. (d) Number of frames seeing each surface point, with GT unobserved areas shown in gray. (e) We managed to reconstruct a curved section of the colon. (f) Our method plausibly estimates the wall of the colon at the right of camera (b), although it was never seen in the images.</figDesc><graphic coords="8,103,95,293,78,236,62,137,26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 . Reconstruction error [mm] on the C3VD dataset. Surveyed: points</head><label>1</label><figDesc>seen at least once. Extended: points within 20 mm of a visible point. Anatomical regions: Cecum, Descending, Sigmoid and Transverse. For NeuS, we provide two sets of numbers because the optimization failed on the other sections. In italics we mark the sequences where the camera moves less than 1 cm yielding higher errors.</figDesc><table><row><cell></cell><cell>NeuS</cell><cell cols="2">LightNeuS (ours)</cell></row><row><cell cols="3">Sequence C1a C4b C1a C1b C2a C2b C2c C3a</cell><cell>C4a C4b D4a S1a S2a</cell></row><row><cell>Sur.</cell><cell cols="2">MedAE 4.53 10.6 0.95 4.85 1.40 3.26 2.57 1.12 MAE 5.07 10.6 1.48 5.11 1.54 3.65 3.00 2.54</cell><cell>1.90 1.41 2.66 4.23 1.19 2.14 1.63 3.26 4.33 1.89</cell></row><row><cell></cell><cell cols="2">RMSE 6.40 11.6 2.01 5.63 1.87 4.39 3.74 5.49</cell><cell>2.92 2.10 4.08 4.96 2.78</cell></row><row><cell>Ext.</cell><cell cols="2">MedAE 4.68 5.35 0.83 4.89 1.41 3.32 2.54 1.27 MAE 6.24 6.74 1.26 5.10 1.56 3.70 3.01 3.83</cell><cell>1.91 1.45 4.50 4.01 1.40 2.18 1.72 6.61 4.19 2.36</cell></row><row><cell></cell><cell cols="2">RMSE 8.77 8.56 1.72 5.60 1.90 4.42 3.77 7.96</cell><cell>2.95 2.20 9.32 4.87 3.96</cell></row><row><cell></cell><cell>LightNeuS (ours)</cell><cell></cell></row><row><cell></cell><cell cols="3">S3a S3b T1a T1b T2a T2b T4a Mean T2c T3a T3b T4b Mean</cell></row><row><cell></cell><cell cols="3">2.57 3.63 3.43 2.33 2.24 2.16 1.15 2.39 5.07 6.39 11.0 1.75 6.04</cell></row><row><cell></cell><cell cols="3">2.68 4.16 3.47 2.72 2.28 2.30 2.31 2.80 5.45 8.65 12.1 6.70 8.23</cell></row><row><cell></cell><cell cols="3">3.18 4.81 4.07 3.34 2.58 2.70 3.79 3.58 6.48 10.7 14.4 11.3 10.7</cell></row><row><cell></cell><cell cols="3">2.87 3.54 3.38 2.69 2.19 2.12 1.29 2.53 4.44 6.54 13.6 8.00 8.16</cell></row><row><cell></cell><cell cols="3">3.27 4.64 3.31 3.21 2.22 2.28 2.22 3.15 5.36 8.10 14.1 10.4 9.47</cell></row><row><cell></cell><cell cols="3">4.04 6.10 3.86 3.96 2.55 2.69 3.32 4.18 6.78 9.94 15.9 13.9 11.6</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by <rs type="funder">EU-H2020</rs> grant <rs type="grantNumber">863146</rs>: ENDO MAPPER, Spanish government grants <rs type="grantNumber">PID2021-127685NB-I00</rs> and <rs type="grantNumber">FPU20/06782</rs> and by <rs type="funder">Aragón government</rs> grant <rs type="grantNumber">DGA T45-17R</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FMPxSBz">
					<idno type="grant-number">863146</idno>
				</org>
				<org type="funding" xml:id="_sqKn8WZ">
					<idno type="grant-number">PID2021-127685NB-I00</idno>
				</org>
				<org type="funding" xml:id="_aqtm4sx">
					<idno type="grant-number">FPU20/06782</idno>
				</org>
				<org type="funding" xml:id="_vZXPJDV">
					<idno type="grant-number">DGA T45-17R</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 48.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Azagra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14240</idno>
		<title level="m">EndoMapper dataset of complete calibrated endoscopy procedures</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep multi-view stereo for dense 3D reconstruction from monocular endoscopic video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_74</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-074" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="774" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photometric single-view dense 3D reconstruction in endoscopy</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Batlle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4904" to="4910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Bobrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Golhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Akshintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08903</idno>
		<title level="m">Colonoscopy 3D video dataset with paired depth from 2D-3D registration</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ORB-SLAM3: an accurate open-source library for visual, visual-inertial, and multimap SLAM</title>
		<author>
			<persName><forename type="first">C</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Elvira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Rob</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1874" to="1890" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Direct sparse odometry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="625" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LSD-SLAM: large-scale direct monocular SLAM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10605-2_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10605-2" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8690</biblScope>
			<biblScope unit="page">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SD-DefSLAM: Semi-direct monocular SLAM for deformable and intracorporeal scenes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lamarca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morlana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5170" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ray tracing volume densities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kajiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Von Herzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="174" />
			<date type="published" when="1984-01">jan 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A generic camera model and calibration method for conventional, wide-angle, and fish-eye lenses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1335" to="1340" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sage: Slam with appearance and geometry prior for endoscopy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5587" to="5593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time 3D reconstruction of colonoscopic surfaces for determining missing regions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<publisher>MIC-CAI</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">RNNSLAM: Reconstructing the 3D colon to visualize missing regions during a colonoscopy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102100</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Live tracking and dense reconstruction for handheld monocular endoscopy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hostettler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="89" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NeRF: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Light modelling and calibration in laparoscopy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Modrzejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hostettler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="859" to="866" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DTAM: dense tracking and mapping in real-time</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2320" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nerfies: deformable neural radiance fields</title>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5865" to="5874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A toolbox for easily calibrating omnidirectional cameras</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RJS International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="5695" to="5701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-9_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46487-931" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Colonoscopic 3D reconstruction by tubular non-rigid structure-from-motion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1237" to="1241" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2020: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA: Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="249" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Color-based hybrid reconstruction for endoscopy</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Tokgozoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NeuS: learning neural implicit surfaces by volume rendering for multi-view reconstruction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27171" to="27183" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">NeuS2: fast learning of neural implicit surfaces for multi-view reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.05231</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural rendering for stereo 3D reconstruction of deformable tissues in robotic surgery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-141" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="431" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Endoscopogram: a 3D model reconstructed from endoscopic video frames</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alterovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46720-7_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46720-751" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9900</biblScope>
			<biblScope unit="page" from="439" to="447" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
