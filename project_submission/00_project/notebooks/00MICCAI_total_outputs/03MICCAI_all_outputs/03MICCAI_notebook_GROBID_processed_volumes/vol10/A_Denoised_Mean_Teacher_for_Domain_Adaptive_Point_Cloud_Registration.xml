<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration</title>
				<funder ref="#_pXryYEh">
					<orgName type="full">Federal Ministry for Education and Research of Germany</orgName>
				</funder>
				<funder ref="#_Wd7WeGC">
					<orgName type="full">Federal Ministry for Economic Affairs and Climate Action of Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Bigalke</surname></persName>
							<email>alexander.bigalke@uni-luebeck.de</email>
							<idno type="ORCID">0000-0001-7824-5735</idno>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mattias</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
							<email>mattias.heinrich@uni-luebeck.de</email>
							<idno type="ORCID">0000-0002-7489-1972</idno>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Informatics</orgName>
								<orgName type="institution">University of Lübeck</orgName>
								<address>
									<settlement>Lübeck</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="666" to="676"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">D18612F8BC1C379724864172E01F0A8E</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_63</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Point cloud registration</term>
					<term>Domain adaptation</term>
					<term>Mean Teacher</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud-based medical registration promises increased computational efficiency, robustness to intensity shifts, and anonymity preservation but is limited by the inefficacy of unsupervised learning with similarity metrics. Supervised training on synthetic deformations is an alternative but, in turn, suffers from the domain gap to the real domain. In this work, we aim to tackle this gap through domain adaptation. Self-training with the Mean Teacher is an established approach to this problem but is impaired by the inherent noise of the pseudo labels from the teacher. As a remedy, we present a denoised teacherstudent paradigm for point cloud registration, comprising two complementary denoising strategies. First, we propose to filter pseudo labels based on the Chamfer distances of teacher and student registrations, thus preventing detrimental supervision by the teacher. Second, we make the teacher dynamically synthesize novel training pairs with noise-free labels by warping its moving inputs with the predicted deformations. Evaluation is performed for inhale-to-exhale registration of lung vessel trees on the public PVT dataset under two domain shifts. Our method surpasses the baseline Mean Teacher by 13.5/62.8%, consistently outperforms diverse competitors, and sets a new state-of-the-art accuracy (TRE = 2.31 mm). Code is available at https://github.com/ multimodallearning/denoised_mt_pcd_reg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent deep learning-based registration methods have shown great potential in solving medical image registration problems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">14]</ref>. Most of these methods perform the registration based on the raw volumetric intensity images, e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b32">31]</ref>. By contrast, only a few recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">21]</ref> operate on sparse, purely geometric point clouds extracted from the images, even though this representation promises multiple potential benefits, including computational efficiency, robustness against intensity shifts in the image domain, and anonymity preservation. The latter, for instance, can facilitate public data access and federated learning, as exemplified by a recently released point cloud dataset of lung vessels <ref type="bibr" target="#b21">[21]</ref> whose underlying CT scans are not publicly accessible. On the other hand, the sparsity of point clouds and the absence of intensity information make the registration problem more challenging. In particular, unsupervised learning with similarity metrics -as established for dense image registration <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">18]</ref> -was shown ineffective for deformable point cloud registration <ref type="bibr" target="#b21">[21]</ref>, as confirmed by our experiments. Since manual annotations for supervised learning are prohibitively costly, an alternative consists of training on synthetic deformations with known displacements <ref type="bibr" target="#b21">[21]</ref>, as known from dense registration <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">26]</ref>. The inevitable domain gap between synthetic and real deformations, however, involves the risk of suboptimal performance on real data. In this work, we aim to bridge this gap through domain adaptation (DA).</p><p>DA has widely been studied for classification and segmentation tasks <ref type="bibr" target="#b11">[12]</ref>, with popular techniques ranging from adversarial feature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">25]</ref> or output <ref type="bibr" target="#b24">[24]</ref> alignment to self-supervised feature learning <ref type="bibr" target="#b22">[22]</ref>. However, these methods are insufficient for the specific characteristics of the registration problem, involving a more complex output space and requiring the detection of local correspondences. Instead, recent works adapted the Mean Teacher paradigm <ref type="bibr" target="#b23">[23]</ref>, previously established for domain adaptive classification <ref type="bibr" target="#b8">[9]</ref> and segmentation <ref type="bibr" target="#b19">[19]</ref>, to the registration problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b29">29]</ref>. The basic idea is to supervise the learning student model with displacement fields (pseudo labels) provided by a teacher model, whose weights represent the exponential moving average of the student's weights. A significant limitation of this method, however, is the inevitable noise in the pseudo labels, potentially misguiding the adaptation process. Prior works addressed this problem by refining the pseudo labels <ref type="bibr" target="#b16">[16]</ref> or weighting them according to model uncertainty, estimated through Monte Carlo dropout <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b31">30]</ref>. However, even refined pseudo labels remain inaccurate, and the proposed refinement strategy <ref type="bibr" target="#b16">[16]</ref> assumes piecewise rigid motions of 3D objects and does not apply to complex deformations in medical applications. And weighting pseudo labels according to teacher uncertainty <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b31">30]</ref> does not explicitly consider the quality of the actual registrations, completely ignores the quality and certainty of the current student predictions, and can, therefore, not prevent detrimental supervision of the student through inferior teacher predictions.</p><p>Contributions. We introduce two complementary strategies to denoise the Mean Teacher for domain adaptive point cloud registration, addressing the above limitations (see Fig. <ref type="figure" target="#fig_0">1</ref>). Both strategies are based on our understanding of an optimal student-teacher relationship. First, if the student's solution to a problem is superior to that of the teacher, good teachers should not insist on their solution but accept the student's approach. To implement this, inspired by a recent technique to filter pseudo labels for human pose estimation <ref type="bibr" target="#b1">[2]</ref>, we propose to assess the quality of both the teacher and student registrations with the Chamfer distance and to provide only those registrations of the teacher as supervision to the student that are more accurate. This approach differs from previous uncertaintybased methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b31">30]</ref> in two decisive aspects: 1) It explicitly assesses the quality of final registrations, using a model-free and objective measure with little computational overhead compared to multiple forward passes in Monte Carlo dropout.</p><p>2) The selection process considers both teacher and student predictions and can thus prevent detrimental supervision by the teacher. Our second strategy follows the intuition that good teachers should not pose problems to which they do not know the solution. Instead, they should come up with novel tasks with precisely known solutions. Consequently, we propose a completely novel teacher paradigm, where predicted deformations by the teacher are used to synthesize new training pairs for the student, consisting of the original moving inputs and their warps. These input pairs come with precise noise-free displacement labels and significantly differ from static hand-crafted synthetic deformations <ref type="bibr" target="#b21">[21]</ref>. 1) The deformations are based on a real data pair that the teacher aims to align. 2) The deformations are dynamic and become more realistic as the teacher improves. Finally, we unify both strategies in a joint framework for domain adaptive point cloud registration. It is compatible with arbitrary geometric registration models, stable to train, and involves only a few hyper-parameters. We experimentally evaluate the method for inhale-to-exhale registration of lung vessel point clouds on the public PVT dataset <ref type="bibr" target="#b21">[21]</ref>, demonstrating substantial improvements over diverse competing methods and state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setup and Standard Mean Teacher</head><p>In point cloud registration, we are given fixed and moving point clouds F ∈ R NF×3 , M ∈ R NM×3 and aim to predict a displacement vector field ϕ ∈ R NM×3 that spatially aligns M to F as M + ϕ. We address the task in a domain adaptation setting with training data comprising a labeled source dataset S of triplets (M s , F s , ϕ s ) and a shifted unlabeled target dataset T of tuples (M t , F t ). While the formulation of our method is agnostic to the specific domain shift between S and T , in this work, we generate the source samples on the fly as random synthetic deformations of the target clouds using a fixed handcrafted deformation function def : R N ×3 → R N ×3 , i.e. source triplets are given as (def</p><formula xml:id="formula_0">(F t ), F t , F t -def (F t )) or (def (M t ), M t , M t -def (M t )).</formula><p>Note that def preserves point correspondences enabling ground truth computation through point-wise subtraction. Given the training data, we aim to learn a function f that predicts deformation vector fields as φ = f (M , F ) with optimal performance in the target domain.</p><p>Baseline Mean Teacher. To solve the problem, the standard Mean Teacher framework <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">23]</ref> employs two identical networks, denoted as the student f and teacher f , with parameters θ and θ . While the student's weights θ are optimized through gradient descent, the teacher's weights correspond to the exponential moving average (EMA) of the student and are updated as θ i = αθ i-1 +(1-α)θ i at iteration i with momentum α. Meanwhile, the student is trained by minimizing</p><formula xml:id="formula_1">L(θ) = λ 1 f (M s , F s ) -ϕ s 2 2 Lsup +λ 2 f (M t , F t ) -f (M t , F t ) 2 2 Lcon (1)</formula><p>consisting of the supervised loss L sup on source data and the consistency loss L con on target data, weighted by λ 1 and λ 2 . L con guides the learning of the student in the target domain with pseudo-supervision from the teacher, which, as a temporal ensemble, is expected to be superior to the student. Nonetheless, predictions by the teacher can still be noisy and inaccurate, limiting the efficacy of the adaptation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Chamfer Distance-Based Filtering of Pseudo Labels</head><p>In a worst-case scenario, the student might predict an accurate displacement field φt , which is strongly penalized by the consistency loss due to an inaccurate teacher prediction φ t . To prevent such detrimental supervision, we aim to select only those teacher predictions for supervision that are superior to the corresponding student predictions, which, however, is complicated by the absence of ground truth. We, therefore, propose to assess the quality of student and teacher registrations by measuring the similarity/distance between fixed and warped moving clouds, with higher similarities/lower distances indicating more accurate registrations. Among existing similarity measures, we opt for the symmetric Chamfer distance <ref type="bibr" target="#b28">[28]</ref>, which computes the distance between two point clouds X, Y as</p><formula xml:id="formula_2">d CD (X, Y ) = x∈X min y ∈Y x -y 2 2 + y ∈Y min x∈X x -y 2 2</formula><p>(2)</p><p>While we experimentally found the Chamfer distance insufficient as a direct loss function -presumably due to sparse differentiability and susceptibility to local minima, we still observed a strong correlation between Chamfer distance and actual registration error, making it a suitable choice for our purposes. We also explored other measures (Laplacian curvature <ref type="bibr" target="#b28">[28]</ref>, Gaussian MMD <ref type="bibr" target="#b7">[8]</ref>), which proved slightly inferior (Supp., Table <ref type="table">2</ref>). Formally, we thus measure the quality of the student prediction φt = f (M t , F t ) as d CD (M t + φt , F t ) and analogously for the teacher prediction φ t . We then define our indicator function</p><formula xml:id="formula_3">I( φt , φ t ) = 1 d CD (M t + φ t , F t ) &lt; d CD (M t + φt , F t ) 0 else<label>(3)</label></formula><p>and reformulate the consistency loss in Eq. 1 as</p><formula xml:id="formula_4">L con = I( φt , φ t ) • f (M t , F t ) -f (M t , F t ) 2 2</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Synthesizing Inputs with Noise-Free Supervision</head><p>While the above filtering strategy mitigates detrimental supervision, the selected pseudo labels are still inaccurate. </p><formula xml:id="formula_5">L syn = f (M t , M t + φ t ) -φ t 2 2<label>(5)</label></formula><p>To our knowledge, there is no prior work with a similarly "generative" teacher model. Altogether, we train the student network by minimizing the loss 3 Experiments</p><formula xml:id="formula_6">L(θ) = λ 1 L sup + λ 2 L con + λ 3 L syn<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets. We evaluate our method for inhale-to-exhale registration of lung vessel point clouds on the public PVT dataset <ref type="bibr" target="#b21">[21]</ref> (https://github.com/uncbiag/ robot, License: CC BY-NC-SA 3.0). The dataset comprises 1,010 such data pairs, which were extracted from lung CT scans as part of the IRB-approved COPDGene study (NCT00608764). Ten of these scan pairs are cases from the Dirlab-COPDGene dataset <ref type="bibr" target="#b3">[4]</ref> and thus annotated with 300 landmark correspondences. We use these cases as the test set and split the remaining unlabeled pairs into 800 cases for training and 200 for validation (on synthetic deformations only). The original point clouds in the dataset have a very high resolution (∼100k points), making the processing with deep networks computationally costly. Therefore, we extract distinctive keypoints by local density estimation followed by non-maximum suppression. We extract two sets of such keypoints for each cloud: one with the ∼8k most distinctive points for inference, and another with ∼16k points, from which we randomly sample subsets during training for increased variability (see Sect. 2.3, technical details). Finally, we pre-align each pair by matching the mean and standard deviation of the coordinates.</p><p>Implementation Details. The registration network f is implemented as the default 4-scale architecture of PointPWC-Net <ref type="bibr" target="#b28">[28]</ref>, operating on 8192 points per cloud. Following <ref type="bibr" target="#b28">[28]</ref>, we implement L sup , L con , and L syn as multi-scale losses.</p><p>Optimization is performed with the Adam optimizer. We first pre-train the network on source data (batch size 4) for 160 epochs and subsequently minimize the joint loss (Eq. 6) for 140 epochs, both with a constant learning rate of 0.001, which requires up to 11 GB and 13/23 h on an RTX2080. For joint optimization, we use mixed batches of 4 source and 4 target samples, set λ 1 = λ 2 = λ 3 = 10, and the EMA-parameter to α = 0.996. While the original PVT data pairs represent the target domain in all experiments, we consider two variants of the function def to synthesize source data pairs: a realistic task-specific 2-scale random field similar to <ref type="bibr" target="#b21">[21]</ref> and a simple rigid transformation. This enables us to evaluate our method under two differently severe domain shifts. Since real validation data are unavailable, hyper-parameters of all compared methods were tuned in a synthetic adaptation scenario, with the rigid deformations in the source and the 2-scale random field deformations in the target domain. For further implementation details, we refer to our public code.</p><p>Comparison Methods. 1) The source-only model is exclusively trained on source data without DA. 2) We adopt the standard Mean Teacher <ref type="bibr" target="#b2">[3]</ref>. 3) An uncertainty-aware Mean Teacher (UA-MT), similar to <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b31">30]</ref>. 4) As proposed in <ref type="bibr" target="#b28">[28]</ref>, we performed purely unsupervised training on target data with a Chamfer loss. However, consistent with the findings in <ref type="bibr" target="#b21">[21]</ref>, this approach could not converge for complex geometric lung structures. Instead, we use the Chamfer loss on target data as an additional loss to complement supervised source training. 5) We guide the learning on target data with the cycle-consistency method from <ref type="bibr" target="#b17">[17]</ref>. 6) As a classical algorithm, we adapt sLBP <ref type="bibr" target="#b12">[13]</ref>. 7) We collect the results of two current SOTA methods, S-Robot and D-Robot, from <ref type="bibr" target="#b21">[21]</ref>, which combine deep networks (Point U-Net, PointPWC-Net), trained on synthetic deformations, with optimal transport modules. Note, however, that the experimental setup in  <ref type="bibr" target="#b21">[21]</ref> slightly differs from our setting in terms of more input points (60k vs. 8k) and additional input features (vessel radii), thus accessing more information.</p><p>Metrics. We interpolate the predicted displacements from the moving input cloud to the annotated moving landmarks with an isotropic Gaussian kernel (σ = 5 mm) and measure the target registration error (TRE) with respect to the fixed landmarks. To assess the smoothness of the predictions, we interpolate the sparse displacement fields to the underlying image grid and measure the standard deviation of the logarithm of the Jacobian determinant (SDlogJ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Quantitative results are shown in Table <ref type="table" target="#tab_2">1</ref> and reveal the following insights: 1) The source-only model benefits from realistic synthetic deformations in the source domain, yielding a 40.9% lower TRE.</p><p>2) The standard Mean Teacher proves effective under the weaker domain shift (-40.7% TRE compared to source-only) but only achieves a slight improvement of 16.0% in the more challenging scenario, where pseudo labels by the teacher are naturally noisier, in turn limiting the efficacy of the adaptation process. 3) Our proposed strategy to filter pseudo labels (ours w/o L syn ) improves the standard teacher and its uncertainty-aware extension, particularly notable under the more severe domain shift (-59.8/-55.0% TRE). 4) Synthesizing novel data pairs with the teacher (ours w/o L con ) alone is slightly inferior to the standard teacher for realistic deformations in the source domain but substantially superior for simple rigid transformations. 5) Combining our two strategies yields further considerable improvements to TREs of 2.31 and 2.38 mm, demonstrating their complementarity. Thus, our method improves the standard Mean Teacher by 13.5/62.8%, outperforms all competitors by statistically significant margins (p &lt; 0.001 in a Wilcoxon signed-rank test), and sets a new state-of-the-art accuracy. Remarkably, our method achieves almost the same accuracy for simple rigid transformations in the source domain as for complex, realistic deformations. Thus, it eliminates the need for designing taskspecific deformation models, which requires strong domain knowledge. Qualitative results are presented in Fig. <ref type="figure" target="#fig_1">2</ref> and Supp., Fig. <ref type="figure">3</ref>, demonstrating accurate and smooth deformation fields by our method, as confirmed by the SDlogJ in Table <ref type="table" target="#tab_2">1</ref>, which takes small values for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Our work addressed domain adaptive point cloud registration to bridge the gap between synthetic source and real target deformations. Starting from the established Mean Teacher paradigm, we presented two novel strategies to tackle the noise of pseudo labels from the teacher model, which is a persistent, significant limitation of the method. Specifically, we 1) proposed to prevent detrimental supervision through the teacher by filtering pseudo labels according to Chamfer distances of student and teacher registrations and 2) introduced a novel teacherstudent paradigm, where the teacher synthesizes novel training data pairs with perfect noise-free displacement labels. Our experiments for lung vessel registration on the PVT dataset demonstrated the efficacy of our method under two scenarios, outperforming the standard Mean Teacher by up to 62.8% and setting a new state-of-the-art accuracy (TRE = 2.31 mm). As such, our method even favorably compares to popular image-based deep learning methods (VoxelMorph <ref type="bibr" target="#b0">[1]</ref> and LapIRN <ref type="bibr" target="#b18">[18]</ref>, e.g., achieve TREs of 7.98 and 4.99 mm on the original DIR-Lab CT images) but lags behind conventional image-based optimization methods <ref type="bibr" target="#b20">[20]</ref> with 0.83 mm TRE. But while the latter require run times of several minutes to process the dense intensity scans with 30M+ voxels, our method processes sparse, purely geometric point clouds with 8k points only, enabling anonymity-preservation and extremely fast inference within 0.2 s. In this light, we see two significant potential impacts of our work: First, our method could generally advance purely geometric keypoint-based medical registration, previously limited by the inefficacy of unsupervised learning with similarity metrics.</p><p>In particular, medical point cloud registration, currently primarily focusing on lung anatomies, still needs to be investigated for other anatomical structures (abdomen, brain) in future work, which might benefit from our generic approach. Second, our method is conceptionally transferable to dense image registration (e.g., intensity-based similarity metrics <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b27">27]</ref> can replace the Chamfer distance). In this context, it appears of great interest to revisit learning from synthetic deformations <ref type="bibr" target="#b6">[7]</ref> within a DA setting or to combine our method with unsupervised learning under metric supervision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our denoised Mean Teacher for domain adaptive registration. We overcome noisy supervision by the teacher with a novel pseudo label selection strategy and the synthesis of new training pairs with precisely known displacements.</figDesc><graphic coords="3,43,17,54,53,338,86,92,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative results on two sample cases of the PVT dataset. Predicted displacement fields are shown in gray. Colored dots and lines represent moving landmarks and their interpolated flow, with colors encoding the TRE (clamped to 12 mm). (Color figure online)</figDesc><graphic coords="8,79,23,60,98,313,21,136,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Therefore, we complement the strategy with a novel teacher paradigm, where the teacher dynamically synthesizes new training pairs with precisely known displacements for supervision. Specifically, given a teacher prediction φ t = f (M t , F t ), we do not only use it to supervise the student on the same input pair but also generate a new input sample (M</figDesc><table /><note><p>t , M t + φ t ) by warping M t with φ t . The underlying displacement field is naturally precisely known, enabling noise-free training of the student by minimizing</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on the PVT dataset, reported as mean TRE and 25/75% percentiles in mm and SDlogJ. † indicates a deviating experimental setup (Sect. 3.1).</figDesc><table><row><cell>Method</cell><cell>def = 2-scale rnd. field</cell><cell>def = rigid</cell></row><row><cell></cell><cell cols="2">TRE 25% 75% SDlogJ TRE 25% 75% SDlogJ</cell></row><row><cell>initial</cell><cell>23.32 13.22 31.61 -</cell><cell>23.32 13.22 31.61 -</cell></row><row><cell>pre-align</cell><cell>12.83 8.25 16.68 -</cell><cell>12.83 8.25 16.68 -</cell></row><row><cell>sLBP [13]</cell><cell>3.62 1.24 3.29 0.038</cell><cell>3.62 1.24 3.29 0.038</cell></row><row><cell>S-Robot  † [21]</cell><cell>5.48 2.86 7.14 N/A</cell><cell>N/A N/A N/A N/A</cell></row><row><cell>D-Robot  † [21]</cell><cell>2.86 1.25 3.11 N/A</cell><cell>N/A N/A N/A N/A</cell></row><row><cell>source-only</cell><cell>4.50 1.62 5.49 0.034</cell><cell>7.62 3.12 11.15 0.019</cell></row><row><cell>Chamfer loss [28]</cell><cell>3.96 1.47 4.43 0.036</cell><cell>4.18 1.54 5.27 0.043</cell></row><row><cell cols="2">cycle-consistency [17] 3.93 1.48 4.36 0.035</cell><cell>6.47 2.43 9.08 0.029</cell></row><row><cell>Mean Teacher [3]</cell><cell cols="2">2.67 1.33 3.12 0.028 6.40 2.42 9.50 0.013</cell></row><row><cell>UA-MT [29]</cell><cell>2.58 1.28 3.04 0.029</cell><cell>5.71 1.83 8.77 0.015</cell></row><row><cell>ours w/o L syn</cell><cell>2.49 1.23 2.88 0.030</cell><cell>2.57 1.22 2.93 0.027</cell></row><row><cell>ours w/o L con</cell><cell>2.96 1.27 3.29 0.035</cell><cell>3.00 1.21 3.39 0.034</cell></row><row><cell>ours</cell><cell>2.31 1.16 2.66 0.034</cell><cell>2.38 1.12 2.66 0.033</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. We gratefully acknowledge the financial support by the <rs type="funder">Federal Ministry for Economic Affairs and Climate Action of Germany</rs> (FKZ: <rs type="grantNumber">01MK20012B</rs>) and by the <rs type="funder">Federal Ministry for Education and Research of Germany</rs> (FKZ: <rs type="grantNumber">01KL2008</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Wd7WeGC">
					<idno type="grant-number">01MK20012B</idno>
				</org>
				<org type="funding" xml:id="_pXryYEh">
					<idno type="grant-number">01KL2008</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VoxelMorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bigalke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diesel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hennigs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rostalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12193</idno>
		<title level="m">Anatomy-guided domain adaptation for 3D in-bed human pose estimation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adapting the mean teacher for keypointbased lung registration under geometric domain shifts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bigalke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_27" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A reference dataset for deformable image registration spatial accuracy evaluation using the COPDgene study archive</title>
		<author>
			<persName><forename type="first">R</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2861</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TransMorph: transformer for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Segars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102615</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep learning framework for unsupervised affine and deformable image registration</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sokooti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="128" to="143" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pulmonary CT registration through supervised learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Eppenhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Geometric data analysis, beyond convolutions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feydy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Saclay Gif-sur-Yvette, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Université Paris</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-ensembling for visual domain adaptation</title>
		<author>
			<persName><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning in medical image registration: a review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno>20TR01</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">20</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptation for medical image analysis: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning based geometric registration for medical images: how accurate can we get without visual features?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPMI 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12729</biblScope>
			<biblScope unit="page" from="18" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-78191-0_2" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning in medical image registration: a survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Haskins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mind: modality independent neighbourhood descriptor for multi-modal deformable registration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1423" to="1435" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deformation and correspondence aware unsupervised synthetic-to-real scene flow estimation for point clouds</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7233" to="7243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Just go with the flow: self-supervised scene flow estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Okorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11177" to="11185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large deformation diffeomorphic image registration with Laplacian pyramid networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C W</forename><surname>Mok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_21</idno>
		<idno>978-3-030-59716-0_21</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for medical imaging segmentation with self-ensembling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Perone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimation of large motion in lung CT by integrating regularized keypoint correspondences into dense deformable registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rühaak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1746" to="1757" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate point cloud registration with robust optimal transport</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5373" to="5389" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation through self-supervision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training CNNs for image registration from few samples with model-based data augmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Uzunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Handels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ehrhardt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66182-7_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66182-7_26" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10433</biblScope>
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mutual information for unsupervised deep learning image registration</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Van Der Velden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Gilhuijs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2020: Image Processing</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11313</biblScope>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PointPWC-Net: cost volume on point clouds for (self-)supervised scene flow estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58558-7_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58558-7_6" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12350</biblScope>
			<biblScope unit="page" from="88" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Double-uncertainty guided spatial and temporal consistency regularization weighting for learning-based abdominal registration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_2" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_67" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive cascaded networks for unsupervised medical image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10600" to="10610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
