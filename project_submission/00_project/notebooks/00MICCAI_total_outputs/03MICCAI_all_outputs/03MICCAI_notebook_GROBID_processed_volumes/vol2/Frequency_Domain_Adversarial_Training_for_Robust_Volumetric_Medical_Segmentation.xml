<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Asif</forename><surname>Hanif</surname></persName>
							<email>asif.hanif@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
							<email>muzammal.naseer@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<email>salman.khan@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
							<email>fahad.khan@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="department">Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Linköping University</orgName>
								<address>
									<settlement>Linköping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="457" to="467"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">98A919C46DD72D818D4969E24D4E3971</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial attack</term>
					<term>Adversarial training</term>
					<term>Frequency domain attack</term>
					<term>Volumetric medical segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is imperative to ensure the robustness of deep learning models in critical applications such as, healthcare. While recent advances in deep learning have improved the performance of volumetric medical image segmentation models, these models cannot be deployed for realworld applications immediately due to their vulnerability to adversarial attacks. We present a 3D frequency domain adversarial attack for volumetric medical image segmentation models and demonstrate its advantages over conventional input or voxel domain attacks. Using our proposed attack, we introduce a novel frequency domain adversarial training approach for optimizing a robust model against voxel and frequency domain attacks. Moreover, we propose frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model's performance on clean and adversarial samples. Code is available at https://github.com/asif-hanif/vafa.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation of organs, anatomical structures, or anomalies in medical images (e.g. CT or MRI scans) remains one of the fundamental tasks in medical image analysis. Volumetric medical image segmentation (MIS) helps healthcare professionals to diagnose conditions more accurately, plan medical treatments, and perform image-guided procedures. Although deep neural networks (DNNs) have shown remarkable improvements in performance for different vision tasks, A model trained on voxel-domain adversarial attacks is vulnerable to frequency-domain adversarial attacks. In our proposed adversarial training method, we generate adversarial samples by perturbing their frequency-domain representation using a novel module named "Frequency Perturbation". The model is then updated while minimizing the dice loss on clean and adversarially perturbed images. Furthermore, we propose a frequency consistency loss to improve the model performance.</p><p>including volumetric MIS, their real-world deployment is not straightforward particularly due to the vulnerabilities towards adversarial attacks <ref type="bibr" target="#b26">[26]</ref>. An adversary can deliberately manipulate input data by crafting and adding perturbations to the input that are imperceptible to the human eye but cause the DNN to produce incorrect outputs <ref type="bibr" target="#b10">[10]</ref>. Adversarial attacks pose a serious security threat to DNNs <ref type="bibr" target="#b0">[1]</ref>, as they can be used to cause DNNs to make incorrect predictions in a wide range of applications, including DNN-based medical imaging systems. To mitigate these threats, various techniques have been explored, including adversarial training, input data transformations, randomization, denoising auto-encoders, feature squeezing, and robust architectural changes <ref type="bibr" target="#b0">[1]</ref>. Although significant progress has been made in adversarial defenses, however, this area is still evolving due to the development of attacks over time <ref type="bibr" target="#b2">[3]</ref>.</p><p>Ensuring the adversarial robustness of the models involved in safety-critical applications such as, medical imaging and healthcare is of paramount importance because a misdiagnosis or incorrect decision can result in life-threatening implications. Moreover, the weak robustness of deep learning-based medical imaging models will create a trust deficit among clinicians, making them reluctant to rely on the model predictions. The adversarial robustness of the medical imaging models is still an open and under-explored area <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">20]</ref>. Furthermore, most adversarial attacks and defenses have been designed for 2D natural images and little effort has been made to secure volumetric (3D) medical data <ref type="bibr" target="#b20">[20]</ref>.</p><p>In the context of 2D natural images, it has been recently observed that frequency-domain based adversarial attacks are more effective against the defenses that are primarily designed to "undo" the impact of pixel-domain adversarial noise in natural images <ref type="bibr" target="#b7">[7]</ref>. Motivated by this observation in 2D natural images, here we explore the effectiveness of frequency-domain based adversarial attacks in the regime of volumetric medical image segmentation and aim to obtain a volumetric MIS model that is robust against adversarial attacks. To achieve this goal, we propose a min-max objective for adversarial training of volumetric MIS model in frequency-domain. For maximization step, we introduce Volumetric Adversarial Frequency Attack -VAFA (Fig. <ref type="figure" target="#fig_0">1</ref>, Sect. 2.1) which operates in the frequency-domain of the data (unlike other prevalent voxeldomain attacks) and explicitly takes into account the 3D nature of the volumetric medical data to achieve higher fooling rate. For minimization step, we propose Volumetric Adversarial Frequency-domain Training -VAFT (Fig. <ref type="figure" target="#fig_0">1</ref>, Sect. 2.2) to obtain a model that is robust to adversarial attacks. In VAFT, we update model parameters on clean and adversarial (obtained via VAFA) samples and further introduce a novel frequency consistency loss to keep frequency representation of the logits of clean and adversarial samples close to each other for a better accuracy tradeoff. In summary, our contributions are as follows:</p><p>-We propose an approach with a min-max objective for adversarial training of volumetric MIS model in the frequency domain. In the maximization step, we introduce a volumetric adversarial frequency attack (VAFA) that is specifically designed for volumetric medical data to achieve higher fooling rate. Further, we introduce a volumetric adversarial frequency-domain training (VAFT) based on a frequency consistency loss in the minimization step to produce a model that is robust to adversarial attacks. -We conduct experiments with two different hybrid CNN-transformers based volumetric medical segmentation methods for multi-organ segmentation.</p><p>Related Work: There are three main types of popular volumetric MIS model architectures: CNN <ref type="bibr" target="#b23">[23]</ref>, Transformer <ref type="bibr" target="#b13">[13]</ref> and hybrid <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b24">24]</ref>. Research has shown that medical machine learning models can be manipulated in various ways by an attacker, such as adding imperceptible perturbation to the image, rotating the image, or modifying medical text <ref type="bibr" target="#b8">[8]</ref>. Adversarial attack studies on medical data have primarily focused on classification problems and voxel-domain adversaries. For example, Ma et al. <ref type="bibr" target="#b20">[20]</ref> have used four types of pixel-domain attacks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b21">21]</ref> on two-class and multi-class medical datasets. Li et al. <ref type="bibr" target="#b19">[19]</ref> and Daza et al. <ref type="bibr" target="#b5">[6]</ref> have focused on single-step and iterative adversarial attacks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b15">15]</ref> on the volumetric MIS. In constant to voxel-domain adversarial attacks, our approach works in the frequency-domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Frequency Domain Adversarial Attack and Training</head><p>We aim to train a model for volumetric medical segmentation that is robust against adversarial attacks. Existing adversarial training (AT) approaches rely on min-max optimization <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b21">21]</ref> and operate in the input space. They find adversaries by adding the adversarial perturbation to the input samples by Compared to different voxel-domain attacks (PGD <ref type="bibr" target="#b21">[21]</ref>, FGSM <ref type="bibr" target="#b10">[10]</ref>, BIM <ref type="bibr" target="#b16">[16]</ref> and GN <ref type="bibr" target="#b14">[14]</ref>), our attack (VAFA) achieves higher fooling rate (highlighted in red bounding box) while maintaining comparable perceptual similarity. Best viewed zoomed in.</p><p>maximizing the model loss (e.g., dice loss in segmentation). The loss function is then minimized on such adversaries to update the model parameters. In this work, we propose a frequency-domain adversarial attack that takes into account the 3D nature of the volumetric medical data and performs significantly better than the other voxel-domain as well as 2D frequency domain attacks (Table <ref type="table" target="#tab_0">1</ref>). Based on our attack, we then introduce a novel frequency-domain adversarial training to make the model resilient to adversarial attacks. Additionally, we observe that our approach improves/retains the performance of the robust model on clean samples when compared to the non-robust model. Our approach optimizes adversarial samples by perturbing the 3D-DCT coefficients within the frequency domain using our frequency perturbation module (Fig. <ref type="figure" target="#fig_0">1</ref>) and adversarial guidance from the segmentation loss (Sect. 2.1). We find adversarial samples with high perceptual quality by maximizing the structural similarity between clean and adversarial samples. Using clean and adversarial samples, we propose updating the model parameters by simultaneously minimizing the segmentation loss (i.e. Dice loss) and the frequency consistency loss (Eq. 4) between the clean and adversarial outputs of the segmentation model.</p><p>3D Medical Segmentation Framework: Deep learning-based 3D medical segmentation generally uses encoder-decoder architectures <ref type="bibr" target="#b18">[18]</ref>. The encoder produces a latent representation of the input sample. A segmentation map of the input sample is generated by the decoder using the latent feature representation. The decoder usually incorporates skip connections from the encoder to preserve spatial information <ref type="bibr" target="#b12">[12]</ref>. Next, we describe our proposed volumetric frequencydomain adversarial attack in Sect. 2.1 and then training in Sect. 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Volumetric Adversarial Frequency Attack (VAFA)</head><p>Generally, adversarial attacks operate in the voxel domain by adding an imperceptible perturbation to the input data. In contrast, our attack perturbs the 3D-DCT coefficient to launch a frequency-domain attack for 3D medical image segmentation. Our Frequency Perturbation Module (FPM) transforms voxel-domain data into frequency-domain by using discrete cosine transforms (DCTs) and perturbs the DCT coefficients using a learnable quantization. It then takes an inverse DCT of the perturbed frequency-domain data and returns voxel-domain image.</p><p>We keep the model in a "frozen" state while maximizing the dice loss <ref type="bibr" target="#b25">[25]</ref> for segmentation and minimizing structural similarity loss <ref type="bibr" target="#b27">[27]</ref> for perceptual quality.</p><p>We represent a 3D (volumetric) single channel clean sample by X ∈ R 1×H×W ×D and its ground-truth binary segmentation mask by Y ∈ {0, 1} NumClass×H×W ×D , where "NumClass" is the number of classes. We split X into n 3D patches i.e. X → {x i } n i=1 , where</p><formula xml:id="formula_0">x i ∈ R h×w×d and h ≤ H, w ≤ W, d ≤ D, h = w = d.</formula><p>We apply our frequency perturbation module to each of these patches.</p><p>Frequency Perturbation Module: We apply a 3D discrete cosine transform (DCT), represented as D(•), to each patch x i . The resulting DCT coefficients are then processed through a function ϕ(•), which performs three operations: quantization, differentiable rounding (as described in <ref type="bibr" target="#b9">[9]</ref>), and subsequent de-quantization. ϕ(•) utilizes a learnable quantization table q ∈ Z h×w×d to modify the DCT coefficients, setting some of them to zero. In particular, ϕ(D(x), q) := D(x) q q, where DCT coefficients of a patch (i.e. D(x)) are element-wise divided by quantization table q. After the division operation, the result undergoes rounding using a differentiable rounding operation <ref type="bibr" target="#b9">[9]</ref>, resulting in some values being rounded down to zero. The de-quantization step involves element-wise multiplication of D(x) q with the same quantization table q. This step allows us to reconstruct the quantized DCT coefficients. Since quantization table is in the denominator of the division operation, therefore, higher quantization table values increase the possibility of more DCT coefficients being rounded down to zero. To control the number of DCT coefficients being set to zero, we can constrain the values of the quantization table to a maximum threshold (constraint in Eq. 2). In other words, ϕ(•) performs a 3D adversarial lossy compression on input through a learnable quantization table. Finally, a 3D inverse DCT (IDCT) is performed on the output of ϕ(•) in order to obtain an adversarially perturbed voxel-domain representation, denoted by x . We show our frequency perturbation module in Eq. 1 as follows:</p><p>x → D(x) → ϕ(D(x), q) quantization, rounding and de-quantization</p><formula xml:id="formula_1">→ D I (ϕ(•)) → x<label>(1)</label></formula><p>We repeat the above mentioned sequence of transformations for all patches and then merge {x i } n i=1 to form adversarial image X ∈ R H×W ×D . Quantization Constraint: We learn quantization table q by maximizing the L dice while ensuring that q ∞ ≤ q max . Quantization threshold q max controls the Algorithm 1. Volumetric Adversarial Frequency Attack (VAFA)</p><formula xml:id="formula_2">1: Number of Steps: T , Quantization Threshold: qmax 2: Input: X ∈ R H×W ×D , Y ∈ {0, 1} NumClass×H×W ×D Output: X ∈ R H×W ×D 3: function VAFA(X,Y) 4: q i ← 1 ∀ i ∈ {1, 2, . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , n}</head><p>Initialize all quantization tables with ones. 5:</p><p>for t ← 1 to T do 6:</p><formula xml:id="formula_3">{xi} n i=1 ← Split(X) Split X into 3D patches of size (h × w × d) 7: x i ← DI ϕ(D(xi), q i ) ∀ i ∈ {1, 2, . . . , n} Frequency Perturbation 8: X ← Merge({x i } n i=1 )</formula><p>Merge all adversarial patches to form X 9:</p><p>L(X, X , Y) = L dice (M θ (X ), Y) -Lssim(X, X ) 10:</p><formula xml:id="formula_4">q i ← q i + sign(∇q i L) ∀ i ∈ {1, 2, . . . , n} 11: q i ← clip(q i , min=1, max=qmax) ∀ i ∈ {1, 2, . . . , n} 12:</formula><p>end for 13: end function 14: Return X Algorithm 2. Volumetric Adversarial Frequency Training (VAFT)</p><formula xml:id="formula_5">1: Train Dataset: X = {(Xi, Yi)} N i=1 , Xi ∈ R H×W ×D , Yi ∈ {0, 1} NumClass×H×W ×D 2: NumSamples=N , BatchSize=B, Target Model: M θ , AT Robust Model: M 3: for i ← 1 to NumEpochs do 4: for j ← 1 to N/B do 5: Sample a mini-batch B ⊆ X of size B 6: X ← VAFA(X, Y) ∀(X, Y) ∈ B Adv.</formula><p>Freq. Attack on clean images. 7:</p><formula xml:id="formula_6">L = L dice (M θ (X), Y) + L dice (M θ (X ), Y) + L fr (M θ (X), M θ (X )) 8:</formula><p>Backward pass and update M θ 9:</p><p>end for 10: end for 11: M ← M θ AT robust model after training completion. 12: Return M extent to which DCT coefficients are perturbed. The higher the value of q max , the more information is lost. The drop in perception quality of the adversarial sample and the accuracy of the model are directly proportional to the value of q max . To increase the perceptual quality of adversarial samples, we also minimize the structural similarity loss <ref type="bibr" target="#b27">[27]</ref> between clean and adversarial samples, denoted by L ssim (X, X ), in optimization objective. Our attack optimizes the following objective to fool a target model M θ :</p><formula xml:id="formula_7">maximize q L dice (M θ (X ), Y) -L ssim (X, X ) s.t. q ∞ ≤ q max , (2)</formula><p>where L ssim (X, X ) = 1 -1 n n i=1 SSIM(x i , x i ) is structural similarity loss <ref type="bibr" target="#b27">[27]</ref>. Algorithm 1 presents our volumetric adversarial frequency attack (VAFA). An overview of the attack can be found in maximization step of Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Volumetric Adversarial Frequency Training (VAFT)</head><p>The model parameters are then updated by minimizing the segmentation loss on both clean and adversarial samples (Eq. 3). Since our attack disrupts the frequency domain to find adversaries, we develop a novel frequency consistency loss (Eq. 4) to encourage frequency domain representation of the model's output (segmentation logits) for the clean sample close to the adversarial sample. Our frequency consistency loss not only boosts the robustness of the model against adversarial attacks but also improves/retains the performance of the robust model on clean images (Sect. 3). We present our volumetric adversarial frequency training (VAFT) in Algorithm 2.</p><formula xml:id="formula_8">minimize θ L dice (M θ (X), Y) + L dice (M θ (X ), Y) + L fr (M θ (X), M θ (X )),<label>(3)</label></formula><formula xml:id="formula_9">L fr (M θ (X), M θ (X )) = D(M θ (X)) -D(M θ (X )) 1 ,<label>(4)</label></formula><p>where X = VAFA(X, Y) and D(•) is 3D DCT function. An overview of the adversarial training can be found in minimization step of Fig. <ref type="figure" target="#fig_0">1</ref>. Figure <ref type="figure" target="#fig_1">2</ref> presents a qualitative results of adversarial examples under different attacks on the standard UNETR model. We highlight areas by red bounding box in Fig. <ref type="figure" target="#fig_1">2</ref> to show the impact of each attack on the model performance, when compared with prediction on clean sample. Our attack (VAFA) achieves higher fooling rate as compared to other voxel-domain attacks, while maintaining comparable perceptual similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Implementation Details: We demonstrate the effectiveness of our approach using two medical segmentation models: UNETR <ref type="bibr" target="#b12">[12]</ref>, UNETR++ <ref type="bibr" target="#b24">[24]</ref> and two datasets: Synapse (18-12 split) <ref type="bibr" target="#b17">[17]</ref>, and ACDC <ref type="bibr" target="#b1">[2]</ref>. Using pre-trained models from open-source Github repositories by the corresponding authors, we launch different adversarial attacks and conduct adversarial training with default parameters. We use the Pytorch framework and single NVIDIA A100-SXM4-40GB GPU for our experiments. For a pixel/voxel range [0, 255], we create l ∞ adversarial examples under perturbation budgets of ∈ {4, 8} for voxel-domain attacks following <ref type="bibr" target="#b7">[7]</ref> and compare it with our attack VAFA. Unless otherwise specified, all attacks are run for a total of 20 optimization steps. More details about the parameters of the attacks used in different experiments can be found in Appendix. We use mean Dice Similarity Score (DSC), mean 95% Hausdorff Distance (HD95). We also report perceptual similarity between clean and adversarial sample (LPIPS) <ref type="bibr" target="#b28">[28]</ref>.</p><p>Results: For each evaluation metric, we take mean across all classes (including background) and test images. In each table (where applicable), green values show DSC and HD95 on clean images. Table <ref type="table" target="#tab_0">1</ref> shows comparison of voxel-domain attacks (e.g. PGD <ref type="bibr" target="#b21">[21]</ref>, FGSM <ref type="bibr" target="#b10">[10]</ref>, BIM <ref type="bibr" target="#b16">[16]</ref>, GaussianNoise(GN) <ref type="bibr" target="#b14">[14]</ref>) with VAFA-2D (2D DCT in FPM applied on each scan independently) and VAFA on UNETR model (Synapse). VAFA achieves a higher fooling rate as compared to other attacks with comparable LPIPS. We posit that VAFA-2D on volumetric MIS data is sub-optimal and it does not take into account the 3D nature of the data and model's reliance on the 3D neighborhood of a voxel to predict its class. Further details are provided in the supplementary material. We show impacts of different parameters of VAFA e.g. quantization threshold (q max ), steps, and patch size (h × w × d) on DSC and LPIPS in Table <ref type="table" target="#tab_1">2</ref>, 3 and 4 respectively. DSC and LPIPS decrease when these parameters values are increased. Table <ref type="table" target="#tab_4">5</ref> shows a comparison of VAFA (patch size = 32 × 32 × 32) with other voxeldomain attacks on UNETR and UNETR++ models. For adversarial training experiments, we use q max = 20 (for Synapse), q max = 10 (for ACDC) and patchsize of 32 × 32 × 32 (chosen after considering the trade-off between DSC and LPIPS from Table <ref type="table" target="#tab_3">4</ref>) for VAFA. For voxel-domain attacks, we use = 4 (for Synapse) and = 2 (for ACDC) by following the work of <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b22">22]</ref>. Table <ref type="table" target="#tab_5">6</ref> presents a comparison of the performance (DSC) of various adversarially trained models against different attacks. M VAFA-FR , M VAFA denote our robust models which were adversarially trained with and without frequency consistency loss (L fr , Eq. 4) respectively. In contrast to other voxel-domain robust models, our approach demonstrated robustness against both voxel and frequency-based attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present a frequency-domain based adversarial attack and training for volumetric medical image segmentation. Our attack strategy is tailored to the 3D nature of medical imaging data, allowing for a higher fooling rate than voxelbased attacks while preserving comparable perceptual similarity of adversarial samples. Based upon our proposed attack, we introduce a frequency-domain adversarial training method that enhances the robustness of the volumetric segmentation model against both voxel and frequency-domain based attacks. Our training strategy is particularly important in medical image segmentation, where the accuracy and reliability of the model are crucial for clinical decision making.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of Adversarial Frequency Attack and Training: A model trained on voxel-domain adversarial attacks is vulnerable to frequency-domain adversarial attacks. In our proposed adversarial training method, we generate adversarial samples by perturbing their frequency-domain representation using a novel module named "Frequency Perturbation". The model is then updated while minimizing the dice loss on clean and adversarially perturbed images. Furthermore, we propose a frequency consistency loss to improve the model performance.</figDesc><graphic coords="2,41,79,54,65,340,33,160,06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative multi-organ segmentation comparison under different attacks on the UNETR [12] model. Top row shows example images and bottom row shows the corresponding segmentation masks predicted by the model under different attacks.Compared to different voxel-domain attacks (PGD<ref type="bibr" target="#b21">[21]</ref>, FGSM<ref type="bibr" target="#b10">[10]</ref>, BIM<ref type="bibr" target="#b16">[16]</ref> and GN<ref type="bibr" target="#b14">[14]</ref>), our attack (VAFA) achieves higher fooling rate (highlighted in red bounding box) while maintaining comparable perceptual similarity. Best viewed zoomed in.</figDesc><graphic coords="4,41,79,71,69,340,21,125,86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Voxel vs. Freq. Attacks</figDesc><table><row><cell>Attack</cell><cell cols="2">DSC↓ LPIPS↑</cell></row><row><cell>-</cell><cell>74.31</cell><cell>-</cell></row><row><cell>PGD</cell><cell cols="2">62.67 98.94</cell></row><row><cell>FGSM</cell><cell>62.77</cell><cell>98.82</cell></row><row><cell>BIM</cell><cell>62.76</cell><cell>98.93</cell></row><row><cell>GN</cell><cell>74.19</cell><cell>97.71</cell></row><row><cell cols="2">VAFA-2D 61.66</cell><cell>98.43</cell></row><row><cell>VAFA</cell><cell cols="2">52.54 97.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Impact of qmax on VAFA</figDesc><table><row><cell cols="3">qmax DSC↓ LPIPS↑</cell></row><row><cell>-</cell><cell>74.31</cell><cell>-</cell></row><row><cell>10</cell><cell cols="2">65.95 99.10</cell></row><row><cell>20</cell><cell>56.24</cell><cell>98.70</cell></row><row><cell>30</cell><cell>50.96</cell><cell>98.33</cell></row><row><cell>40</cell><cell>49.58</cell><cell>97.90</cell></row><row><cell>60</cell><cell>48.83</cell><cell>96.60</cell></row><row><cell>80</cell><cell cols="2">48.76 94.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Impact of steps on VAFA</figDesc><table><row><cell cols="3">Steps DSC↓ LPIPS↑</cell></row><row><cell>-</cell><cell>74.31</cell><cell>-</cell></row><row><cell>10</cell><cell cols="2">61.33 98.85</cell></row><row><cell>20</cell><cell>56.24</cell><cell>98.70</cell></row><row><cell>30</cell><cell>54.37</cell><cell>98.64</cell></row><row><cell>40</cell><cell>53.31</cell><cell>98.59</cell></row><row><cell>50</cell><cell>52.97</cell><cell>98.54</cell></row><row><cell>60</cell><cell cols="2">52.25 98.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Impact of patch size on VAFA</figDesc><table><row><cell cols="3">Size DSC↓ LPIPS↑</cell></row><row><cell>-</cell><cell>74.31</cell><cell>-</cell></row><row><cell>4</cell><cell>63.48</cell><cell>98.90</cell></row><row><cell>8</cell><cell>56.24</cell><cell>98.70</cell></row><row><cell>16</cell><cell>41.30</cell><cell>98.14</cell></row><row><cell>32</cell><cell>32.40</cell><cell>97.49</cell></row><row><cell>48</cell><cell>28.19</cell><cell>97.16</cell></row><row><cell>96</cell><cell>28.08</cell><cell>96.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison of VAFA with other voxel-domain attacks (Synapse dataset).</figDesc><table><row><cell cols="2">Models →</cell><cell></cell><cell>UNETR</cell><cell></cell><cell></cell><cell>UNETR++</cell><cell></cell></row><row><cell cols="2">Attacks ↓</cell><cell>DSC↓</cell><cell>HD95↑</cell><cell>LPIPS↑</cell><cell>DSC↓</cell><cell>HD95↑</cell><cell>LPIPS↑</cell></row><row><cell cols="2">Clean Images</cell><cell>74.3</cell><cell>14.0</cell><cell>-</cell><cell>84.7</cell><cell>12.7</cell><cell>-</cell></row><row><cell cols="2">PGD ( = 4/8)</cell><cell>62.7/50.8</cell><cell>40.4/64.5</cell><cell>98.9/95.3</cell><cell>77.5/67.1</cell><cell>48.1/78.3</cell><cell>95.7/85.1</cell></row><row><cell cols="2">FGSM ( = 4/8)</cell><cell>62.8/53.9</cell><cell>34.8/48.7</cell><cell>98.8/94.7</cell><cell>73.1/67.1</cell><cell>37.3/43.2</cell><cell>94.7/82.2</cell></row><row><cell cols="2">BIM ( = 4/8)</cell><cell>62.8/50.7</cell><cell>39.9/65.8</cell><cell>98.8/95.3</cell><cell>77.3/66.8</cell><cell>46.6/78.1</cell><cell>95.8/85.3</cell></row><row><cell>GN</cell><cell>(σ = 4/8)</cell><cell>74.2/73.9</cell><cell>17.0/15.4</cell><cell>97.7/91.1</cell><cell>84.7/84.3</cell><cell>12.3/13.4</cell><cell>93.3/78.2</cell></row><row><cell cols="2">VAFA (qmax = 20/30)</cell><cell>32.2/29.8</cell><cell>57.6/59.9</cell><cell>97.5/96.9</cell><cell>45.3/39.3</cell><cell>73.9/85.2</cell><cell>94.2/94.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Performance of different attacks on adversarially trained (robust) models.</figDesc><table><row><cell></cell><cell>Attacks →</cell><cell></cell><cell></cell><cell>UNETR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>UNETR++</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Models ↓</cell><cell>Clean</cell><cell>PGD</cell><cell>FGSM</cell><cell>BIM</cell><cell>VAFA</cell><cell>Clean</cell><cell>PGD</cell><cell>FGSM</cell><cell>BIM</cell><cell>VAFA</cell></row><row><cell></cell><cell>M PGD</cell><cell>73.47</cell><cell>65.53</cell><cell>65.68</cell><cell>65.51</cell><cell>42.47</cell><cell>75.43</cell><cell>67.81</cell><cell>67.82</cell><cell>67.80</cell><cell>38.22</cell></row><row><cell>Synapse</cell><cell>M FGSM M BIM M GN</cell><cell>72.44 75.12 73.17</cell><cell>64.80 67.78 61.40</cell><cell>66.31 68.32 61.77</cell><cell>64.76 67.78 61.29</cell><cell>39.02 45.97 30.00</cell><cell>81.06 74.80 80.05</cell><cell>73.84 67.58 76.23</cell><cell>74.76 67.46 70.96</cell><cell>73.77 67.57 74.51</cell><cell>37.48 35.72 41.44</cell></row><row><cell></cell><cell>M VAFA</cell><cell>74.67</cell><cell>64.83</cell><cell>65.49</cell><cell>64.73</cell><cell>66.31</cell><cell>81.88</cell><cell>69.09</cell><cell>65.40</cell><cell>68.90</cell><cell>76.47</cell></row><row><cell></cell><cell>M VAFA-FR</cell><cell>75.66</cell><cell>65.90</cell><cell>66.79</cell><cell>65.83</cell><cell>66.33</cell><cell>82.65</cell><cell>70.61</cell><cell>67.00</cell><cell>70.41</cell><cell>78.19</cell></row><row><cell>ACDC</cell><cell>M VAFA M VAFA-FR</cell><cell>81.95 83.44</cell><cell>60.77 60.63</cell><cell>68.16 69.33</cell><cell>60.75 60.61</cell><cell>69.76 73.05</cell><cell>89.00 91.36</cell><cell>76.28 85.42</cell><cell>80.41 87.42</cell><cell>76.56 83.90</cell><cell>88.45 91.23</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_43.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Threat of adversarial attacks on deep learning in computer vision: a survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="14410" to="14430" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2206" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards robust general medical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-4_1" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Advdrop: adversarial attack to DNNs by dropping information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7506" to="7515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial attacks on medical machine learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Zittrain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Beam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kohane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="issue">6433</biblScope>
			<biblScope unit="page" from="1287" to="1289" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Differentiable soft quantization: bridging full-precision and low-bit neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4852" to="4861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolution-free medical image segmentation using transformers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Vasylechko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="78" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Torchattacks: a pytorch repository for adversarial attacks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01950</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Safety and Security</title>
		<imprint>
			<biblScope unit="page" from="99" to="112" />
			<date type="published" when="2018">2018</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MICCAI multi-atlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<meeting>MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13120</idno>
		<title level="m">Medical image segmentation using deep learning: a survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Volumetric medical image segmentation: a 3D deep coarse-to-fine framework and its adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-13969-8_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-13969-8_4" />
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Convolutional Neural Networks for Medical Imaging and Clinical Informatics. ACVPR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="69" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding adversarial attacks on deep learning based medical image analysis systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107332</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deflecting adversarial attacks with pixel deflection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Storer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">UNETR++: delving into efficient and accurate 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04497</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jorge Cardoso</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67558-9_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67558-9_28" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2017</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10553</biblScope>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
