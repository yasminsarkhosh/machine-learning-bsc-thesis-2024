<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dimitrios</forename><surname>Psychogyios</surname></persName>
							<idno type="ORCID">0000-0002-3377-530X</idno>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francisco</forename><surname>Vasconcelos</surname></persName>
							<email>f.vasconcelos@ucl.ac.uk</email>
							<idno type="ORCID">0000-0002-4609-1177</idno>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
							<email>danail.stoyanov@ucl.ac.uk</email>
							<idno type="ORCID">0000-0002-0980-3227</idno>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Realistic Endoscopic Illumination Modeling for NeRF-Based Data Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">957D6F5FC9CF22F6097D0D37F895DB5D</idno>
					<idno type="DOI">10.1007/978-3-031-43996-451.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Surgical Data Science</term>
					<term>Surgical AI</term>
					<term>Data generation</term>
					<term>Neural Rendering</term>
					<term>Colonoscopy Supplementary Information The online version contains supplementary material</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Expanding training and evaluation data is a major step towards building and deploying reliable localization and 3D reconstruction techniques during colonoscopy screenings. However, training and evaluating pose and depth models in colonoscopy is hard as available datasets are limited in size. This paper proposes a method for generating new pose and depth datasets by fitting NeRFs in already available colonoscopy datasets. Given a set of images, their associated depth maps and pose information, we train a novel light source location-conditioned NeRF to encapsulate the 3D and color information of a colon sequence. Then, we leverage the trained networks to render images from previously unobserved camera poses and simulate different camera systems, effectively expanding the source dataset. Our experiments show that our model is able to generate RGB images and depth maps of a colonoscopy sequence from previously unobserved poses with high accuracy. Code and trained networks can be accessed at https://github.com/surgical-vision/ REIM-NeRF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During colonoscopy screenings, localizing the camera and reconstructing the colon directly from the video feed could improve the detection of polyps and help with navigation. Such tasks can be either treated individually using depth <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> and pose estimation approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> or jointly, using structure from motion (SfM) and visual simultaneous localization and mapping (VSLAM) algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. However, the limited data availability present in surgery often makes evaluation and supervision of learning-based approaches difficult.</p><p>To address the lack of data in surgery, previous work has explored both synthetic pose and depth data generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>, and real data acquisition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>. Generating datasets from endoscopic sequences using game engines is scalable and noise-free but often cannot replicate the material properties and lighting conditions of the target environment. In contrast, capturing real data is a laborious process that often introduces sources of error.</p><p>Neural Radiance Field (NeRF) <ref type="bibr" target="#b10">[11]</ref> networks aim to learn an implicit 3D representation of a 3D scene from a set of images captured from known poses, enabling image synthesis from previously unseen viewpoints. NeRF models render 3D geometry and color, including view-dependent reflections, enabling the rendering of photo-realistic images and geometrically consistent depth maps. EndoNeRF <ref type="bibr" target="#b19">[20]</ref> applied NeRF techniques for the first time on surgical video. The method fits a dynamic NeRF <ref type="bibr" target="#b12">[13]</ref> on laparoscopic videos, showing tools manipulating tissue from a fixed viewpoint. After training, the video sequences were rendered again without the tools obstructing the tissue. However, directly applying similar techniques in colonoscopy, is challenging because NeRF assumes fixed illumination. As soon as the endoscope moves, changes in tissue illumination result in color ambiguities.</p><p>This paper aims to mitigate the depth and pose data scarcity in colonoscopy. Inspired by work in data generation using NeRF <ref type="bibr" target="#b17">[18]</ref>, we present an extension of NeRF which makes it more suitable for use in endoscopic scenes. Our approach aims to expand colonoscopy VSLAM datasets <ref type="bibr" target="#b3">[4]</ref> by rendering views from novel trajectories while allowing simulation of different camera models Fig. <ref type="figure" target="#fig_0">1</ref>. Our approach addresses the scalability issues of real data generation techniques while reproducing realistic images. Our main contributions are: 1)The introduction of a depth-supervised NeRF variant conditioned on the location of the endoscope's light source. This extension is important for modeling variation in tissue illumination while the endoscope moves. 2) We evaluate our model design choices on the C3VD dataset and present renditions of the dataset from previously unseen viewpoints in addition to simulating different camera systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our method requires a set of images with known intrinsic and extrinsic camera parameters and sparse depth maps of a colonoscopy sequence. This information is already available in high-quality VSLAM <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref> datasets which we wish to expand or can be extracted by running an SfM pipeline such as COLMAP <ref type="bibr" target="#b16">[17]</ref> on prerecorded endoscopic sequences. Our pipeline involves first optimizing a special version of NeRF modified to model the unique lighting conditions present in endoscopy. The resulting network is used to render views and their associated dense depth maps from user-defined camera trajectories while allowing to specify of the camera model used during rendering. Images rendered from our models closely resemble the characteristics of the training samples. Similarly, depth maps are geometrically consistent as they share a commonly learned 3D representation. Those properties make our method appealing as it makes realistic data generation easy, configurable, and scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Radiance Fields</head><p>A NeRF <ref type="bibr" target="#b10">[11]</ref> implicitly encodes the geometry and radiance of a scene as a continuous volumetric function F Θ : (x, d) → (c, σ)). The inputs of F Θ are the 3D location of a point in space x = (x, y, z) and the 2D direction from which the point is observed d = (φ, θ). The outputs are the red-green-blue (RGB) color c = (r, g, b) and opacity σ. F θ is learned and stored in the weights of two cascaded multi-layer perceptron (MLP) networks. The first, f σ , is responsible for encoding the opacity σ of a point, based only on x. The second MLP, f c , is responsible for encoding the point's corresponding c based on the output of the f σ and d. NeRFs are learned using differentiable rendering techniques given a set of images of scenes and their associated poses. Optimization is achieved by minimizing the L2 loss between the predicted and reference color of all pixels in the training images. To predict the color of a pixel, a ray r(t) = o + td is defined in space starting for the origin of the corresponding image o and heading towards d which is the direction from where light projects to the corresponding pixel. N points r(t i ), i ∈ [n, f ] are sampled along the ray between a near t n and a far t f range to query NeRF for both σ and c. The opacity values σ of all points along the r can be used to approximate the accumulated transmittance T i as defined in Eq. ( <ref type="formula">1</ref>), which describes the probability of light traveling from o to r(t i ). T i together with the color output of f σ for every point along the ray, can be used to compute the pixel color C p using alpha composition as defined in Eq. ( <ref type="formula" target="#formula_0">2</ref>)</p><formula xml:id="formula_0">T i = exp ⎛ ⎝ - i-1 j=0 σ j Δ j ⎞ ⎠ , Δ k = t k+1 -t k (1) C p = N i=1 w i c i , where w i = T i (1 -exp(-Δ i σ i ))<label>(2)</label></formula><p>Similarly, the expected ray termination distance D p can be computed from Eq.</p><p>(3), which is an estimate of how far a ray travels from the camera until it hits solid geometry. D p can be converted to z-depth by knowing the uv coordinates of the corresponding pixel and camera model.</p><formula xml:id="formula_1">D p = N i=1 w i t i (3)</formula><p>In practice, NeRF uses two pairs of MLPs. Initially, a coarse NeRF F Θc is evaluated on N c samples along a ray. The opacity output of the coarse network is used to re-sample rays with more dense samples where opacity is higher. The new N f ray samples are used to query a fine NeRF F Θf . During both training and inference, both networks are working in parallel. Lastly, to enable NeRF to encapsulate high-frequency geometry and color details, every input of F Θ is processed by a hand-crafted positional encoding module γ(•), using Fourier features <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Extending NeRF for Endoscopy</head><p>Light-Source Location Aware MLP. During a colonoscopy, the light source always moves together with the camera. Light source movement results in illumination changes on the tissue surface as a function of both viewing direction (specularities), camera location (exposure changes), and distance between the tissue and the light source (falloff) Fig. <ref type="figure" target="#fig_1">2</ref>. NeRF <ref type="bibr" target="#b10">[11]</ref>, only models radiance as a function of viewing direction as this is enough when the scene is lit uniformly from a fixed light source and the camera exposure is fixed. To model changes in illumination as a function of light source location, we extend the original NeRF formulation by conditioning f c on both the 2D ray direction γ(d) and also the location of the light source o. For simplicity, throughout this work, we assume a single light source co-located with the camera. This parameterization allows the network to learn how light decays as it travels away from the camera and adjusts the scene brightness accordingly.</p><p>Depth Supervision. NeRF achieves good 3D reconstruction of scenes using images captured from poses distributed in a hemisphere <ref type="bibr" target="#b10">[11]</ref>. This imposes geometric constraints during the optimization because consistent geometry would result in a 3D point projecting in correct pixel locations across different views.</p><p>Training a NeRF on colonoscopy sequences is hard because the camera moves along a narrow tube-like structure and the colon wall is often texture-less. Supervising depth together with color can guide NeRF to learn a good 3D representation even when pose distribution is sub-optimal <ref type="bibr" target="#b6">[7]</ref>. In this work, we compute the distance between the camera and tissue D ref from the reference depth maps and we sample K out of M pixel Eq. ( <ref type="formula">5</ref>) to optimize both color and depth as described in Eq. ( <ref type="formula" target="#formula_2">4</ref>).</p><formula xml:id="formula_2">L = 1 K K j=1 D ref j -D pj 1 + 1 M M i=1 C ref i -C pi 2 2<label>(4)</label></formula><formula xml:id="formula_3">D ref j ∼ U[D ref 1 , D ref M ], i ∈ K ≤ |M | (5)</formula><p>• 1 is the L1 loss, • 2 2 is the L2 loss, U denotes uniform sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We train and evaluate our method on C3VD <ref type="bibr" target="#b3">[4]</ref>, which provides 22 small video sequences captured from a real wide-angle colonoscopy at 1350 × 1080 resolution, moving inside 4 different colon phantoms. The videos include sequences from the colon cecum, descending, sigmoid, and transcending. Videos range from 61 to 1142 frames adding to 10.015 in total. We use the per-frame camera poses and set K/M = 0.03 in Eq. ( <ref type="formula">5</ref>). For each scene, we construct a training set using one out of every 5 frames. We further remove and allocate one out of every 5 poses from the training set for evaluation. Frames not present in either the train or evaluation set are used for testing. We choose to sample both poses and depth information to allow our networks to interpolate more easily between potentially noisy labels and also create a dataset that resembles the sparse output of SfM algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Before training, we spatially re-scale and shift the 3D scene from each video sequence such that every point is enclosed within a cube with a length of two, centered at (0,0,0). Prescaling is important for the positional encoding module γ(x) to work properly. We configure positional encoding modules to compute 10 frequencies for each component of x and 4 frequencies for each component of d and o. We train models on images of 270 × 216 resolution to ignore both depth and RGB information outside a circle with a radius of 130 pixels centered at a principal point to avoid noise due to inaccuracies of the calibration model. We used Adam optimizer <ref type="bibr" target="#b7">[8]</ref> with a batch size of 1024 for about 140K iterations for all sequences, with an initial learning rate of 5e-4 which we later multiply by 0.5 at 50% and 75% of training. We set the number of samples along the ray to N c = 64 and N f = 64. We configure positional encoding modules to compute 10 frequencies for each component of x and 4 frequencies for each component of d and o. Each model from this work is trained for around 30 min on 4 graphics cards from an NVIDIA DGX-A100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Ablation Study</head><p>We ablate our model showing the effects of conditioning NeRF on the light source location and supervising depth. To assess RGB reconstruction, we measure the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) between reconstructed and reference images at 270 × 216 resolution. We evaluate depth using the mean squared error (MSE) in the original dataset scale. For each metric, we report the average across all sequences together with the average standard deviation in Table <ref type="table" target="#tab_0">1</ref>. Conditioning NeRF based on the light source location (this work) produces better or equally good results compared to vanilla NeRF for all metrics. Both depth-supervised models, learn practically the same 3D representation but our model achieves better image quality metrics. Figure <ref type="figure" target="#fig_2">3</ref> shows renditions of each model of the ablation study for the same frame. Both non-depth-supervised models failed to capture correct geometry but were able to reconstruct accurate RGB information. Since non-depth-supervised networks were optimized only on color with weak geometric constraints, they learn floating artifacts in space which when viewed from a specific viewpoint, closely approximate the training samples. In contrast, depth-supervised networks learned a good representation of (3D) geometry while being able to reconstruct RGB images accurately. The depth-supervised NeRF model produces flare artifacts in the RGB image. That is because, during optimization, points are viewed from the same direction but at different distances from the light source. Our Full model is able to cope with illumination changes resulting in artifact-free images and accurate depth. Notably, most of the errors in depth for the depth-supervised approaches are located around sharp depth transitions. Such errors in depth may be a result of inaccuracies in calibration or imperfect camera pose information. Nevertheless, we argue that using RGB images and depth maps produced from our approach can be considered error-free because during inference the learned 3D geometry is fixed and consistent across all rendered views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Generation</head><p>We directly use our proposed model of the d4v2 C3VD scene from the ablation study to render novel views and show results in Fig. <ref type="figure" target="#fig_3">4</ref>. In the second column, we show an image rendered from a previously unseen viewpoint, radially offset from the original camera path. Geometry is consistent and the RGB image exhibits the same photo-realistic properties observed in the training set. In the third column, we render a view by rotating a pose from the training trajectory. In the rotated view, the tissue is illuminated in a realistic way even though the camera never pointed in this direction in the training set. In the fourth column, we show an image rendered using a pinhole camera model whilst only fisheye images were used during training. This is possible because NeRF has captured a good representation of the underlying scene and image formation is done by projecting rays in space based on user-defined camera parameters. All the above demonstrate the ability of our method to render images from new, user-defined, trajectories and camera systems similar to synthetic data generation while producing photo-realistic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented an approach for expanding existing VSLAM datasets by rendering RGB images and their associated depth maps from user-defined camera poses and models. To achieve this task, we propose a novel variant of NeRF, conditioned on the location of the light source in 3D space and incorporating sparse depth supervision. We evaluate the effects of our contributions on phantom datasets and show that our work effectively adapts NeRF techniques to the lighting conditions present in endoscopy. We further demonstrate the efficacy of our method by showing RGB images and their associated depth maps rendered from novel views of the target endoscopic scene. 3D information and conditioning NeRF based on the light source location made NeRF suitable for use in Endoscopy. Currently, our method assumes a static environment and requires accurate camera intrinsic and extrinsic information. Subsequent work can incorporate mechanisms to represent deformable scenes <ref type="bibr" target="#b12">[13]</ref> and refine camera parameters during training <ref type="bibr" target="#b21">[21]</ref>. Further research can investigate adopting the proposed model for data generation in other endoscopic scenes or implementing approaches to perform label propagation for categorical data <ref type="bibr" target="#b22">[22]</ref>. We hope this work will mitigate the data scarcity issue currently present in the surgical domain and inspire the community to leverage and improve neural rendering techniques for data generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (Left) The reference depth and RGB images (yellow trajectory) are used to learn an implicit representation of a scene F θ . (Right) After training, the user can define a new trajectory (white) and extent the original dataset. (Color figure online)</figDesc><graphic coords="2,41,79,54,20,340,21,131,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (left) The color of a point as seen from the colonoscope, from two different distances due to changes in illumination. (center) original NeRF formulation is not able to assign different colors for the same point and viewing direction. (right) ours is conditioned on the location of the light source and the point, modeling light decay.</figDesc><graphic coords="5,69,96,54,14,312,70,124,57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Renditions from all model ablations compared to a reference RGB image and depth map. Reference and reconstructed images (top row), depth maps (middle row), and MSE depth difference between the reference and each model prediction in different scales (bottom row).</figDesc><graphic coords="7,55,98,53,84,340,24,180,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Data generated using our work. The top row shows RGB images and the second row shows depth maps. a)Reference view from frame 60 of the d4v2 C3VD sequence. b) Translating (a) along all axis. c) Rotating (a) around both the x and y-axis d) Simulating a pinhole model in (a).</figDesc><graphic coords="8,41,79,54,02,340,24,152,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Mean and standard deviation metrics of every method aggregated across all sequences of C3VD dataset.</figDesc><table><row><cell>Model</cell><cell>PSNR ↑</cell><cell>SSIM ↑</cell><cell>Depth MSE (mm)↓</cell></row><row><cell>NeRF</cell><cell cols="3">32.097 ± (1.173) 0.811 ± (0.021) 4.263 ± (1.178)</cell></row><row><cell cols="4">NeRF + ls loc(Ours) 32.489 ± (1.128) 0.820 ± (0.018) 1.866 ± (0.594)</cell></row><row><cell>NeRF + depth</cell><cell cols="3">30.751 ± (1.163) 0.788 ± (0.022) 0.015 ± (0.016)</cell></row><row><cell>Full model (Ours)</cell><cell cols="3">31.662 ± (1.082) 0.797 ± (0.020) 0.013 ± (0.018)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning camera pose from optical colonoscopy frames through deep convolutional neural network (CNN)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Armin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grimpen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Salvado</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67543-5_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-67543-55" />
	</analytic>
	<monogr>
		<title level="m">CARE/CLIP -2017</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10550</biblScope>
			<biblScope unit="page" from="50" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Azagra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14240</idno>
		<title level="m">Endomapper dataset of complete calibrated endoscopy procedures</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photometric single-view dense 3D reconstruction in endoscopy</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Batlle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4904" to="4910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Bobrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Golhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Akshintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08903</idno>
		<title level="m">Colonoscopy 3D video dataset with paired depth from 2D-3D registration</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Bobrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00283</idno>
		<title level="m">SLAM endoscopy enhanced by adversarial depth prediction</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth estimation for colonoscopy images with self-supervised learning from videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-112" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part VI</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth-supervised NeRF: fewer views and faster training for free</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12882" to="12891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DefSLAM: tracking and mapping of deforming scenes from monocular sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lamarca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Rob</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="291" to="303" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RNNSLAM: reconstructing the 3D colon to visualize missing regions during a colonoscopy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102100</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NeRF: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-824" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Endoslam dataset and an unsupervised monocular visual odometry and depth estimation approach for endoscopic videos</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Ozyoruk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102058</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">D-NeRF: neural radiance fields for dynamic scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10318" to="10327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bimodal camera pose prediction for endoscopy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04968</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Implicit domain adaptation with conditional generative adversarial networks for depth prediction in endoscopy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rau</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-019-01962-w</idno>
		<ptr target="https://doi.org/10.1007/s11548-019-01962-w" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1167" to="1176" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the uncertain single-view depths in colonoscopies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rodriguez-Puigvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martinez-Cantin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-813" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part III</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Block-NeRF: scalable large scene neural view synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8248" to="8258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7537" to="7547" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural rendering for stereo 3D reconstruction of deformable tissues in robotic surgery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="431" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-141" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Prisacariu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07064</idno>
		<title level="m">NeRF-: neural radiance fields without known camera parameters</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">In-place scene labelling and understanding with implicit scene representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laidlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15838" to="15847" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
