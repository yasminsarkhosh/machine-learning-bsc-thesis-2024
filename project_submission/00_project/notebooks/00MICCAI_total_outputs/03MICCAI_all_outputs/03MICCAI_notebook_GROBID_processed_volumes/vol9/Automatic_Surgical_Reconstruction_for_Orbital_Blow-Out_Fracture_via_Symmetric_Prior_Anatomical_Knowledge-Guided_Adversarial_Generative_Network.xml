<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network</title>
				<funder ref="#_rwyxCAv #_4UQa6bc">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_SbywJsD">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_SGXBpGs">
					<orgName type="full">Foundation of Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
				<funder ref="#_jNy7suG">
					<orgName type="full">Funding of Xiamen Science and Technology Bureau</orgName>
				</funder>
				<funder ref="#_4tc3uJd #_aBueerg #_sdcM3ee #_N2Jc3Pg #_DSJbhrV #_T7GdCps">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_tpNNdd6">
					<orgName type="full">Cross disciplinary Research Fund of Shanghai Ninth People&apos;s Hospital, Shanghai Jiao Tong University School of Medicine</orgName>
				</funder>
				<funder ref="#_32KuMuT">
					<orgName type="full">Shanghai Jiao Tong University Foundation on Medical and Technological Joint Science Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiangchang</forename><surname>Xu</surname></persName>
							<idno type="ORCID">0000-0003-3187-888X</idno>
							<affiliation key="aff0">
								<orgName type="department">Institute of Biomedical Manufacturing and Life Quality Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yining</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">Shanghai Ninth People&apos;s Hospital</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiao Tong University School of Medicine</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huifang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">Shanghai Ninth People&apos;s Hospital</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiao Tong University School of Medicine</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Shanghai Key Laboratory of Orbital Diseases and Ocular Oncology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinwei</forename><surname>Li</surname></persName>
							<email>dr_yinwei_li@foxmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Ophthalmology</orgName>
								<orgName type="institution">Shanghai Ninth People&apos;s Hospital</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiao Tong University School of Medicine</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Shanghai Key Laboratory of Orbital Diseases and Ocular Oncology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
							<email>xiaojunchen@sjtu.edu.cn</email>
							<idno type="ORCID">0000-0002-0298-4491</idno>
							<affiliation key="aff0">
								<orgName type="department">Institute of Biomedical Manufacturing and Life Quality Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute of Medical Robotics</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Surgical Reconstruction for Orbital Blow-Out Fracture via Symmetric Prior Anatomical Knowledge-Guided Adversarial Generative Network</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="462" to="471"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">94C70CCF32834F413CBBD8B0415FBFF1</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_44</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automatic surgery planning</term>
					<term>Orbital blow-out fracture</term>
					<term>Surgical reconstruction</term>
					<term>Symmetric prior anatomical knowledge</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Orbital blow-out fracture (OBF) is a complex disease that can cause severe damage to the orbital wall. The ultimate means of treating this disease is orbital reconstruction surgery, where automatic reconstruction of the orbital wall is a crucial step. However, accurately reconstructing the orbital wall is a great challenge due to the collapse, damage, fracture, and deviation in OBF. Manual or semi-automatic reconstruction methods used in clinics also suffer from poor accuracy and low efficiency. Therefore, we propose a symmetric prior anatomical knowledge (SPAK)-guided generative adversarial network (GAN) for automatic reconstruction of the orbital wall in OBF. Above all, a spatial transformation-based SPAK generation method is proposed to generate prior anatomy that guides the reconstruction of the fractured orbital wall. Next, the generated SPAK is introduced into the GAN network, to guide the network towards automatic reconstruction of the fractured orbital wall. Additionally, a multi-function combination supervision strategy is proposed to further improve the network reconstruction performance. Our evaluation on the test set showed that the proposed network achieved a Dice similarity coefficient (DSC) of 92.35 ± 2.13% and a 95% Hausdorff distance of 0.59 ± 0.23 mm, which is significantly better than other networks. The proposed network is the first AI-based method to implement the automatic reconstruction of OBF, effectively</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Orbital fractures represent a frequent occurrence of orbital trauma, with their incidence on the rise primarily attributed to assault, falls, and vehicle collisions <ref type="bibr" target="#b0">[1]</ref>. Orbital blow-out fracture (OBF) is a frequent type of fracture where one of the orbital walls fractures due to external force, while the orbital margin remains intact <ref type="bibr" target="#b1">[2]</ref>. It is a complex disease that can result in the destruction or collapse of the orbital wall, orbital herniation, invagination of the eyeball, and even visual dysfunction or changes in appearance in severe cases <ref type="bibr" target="#b2">[3]</ref>. OBF repair surgery is the ultimate treatment for this disease and involves implanting artificial implants to repair and fill the fractured area. Automatic reconstruction of the orbital wall is a crucial step in this procedure to achieve precise preformed implants and assisted intraoperative navigation.</p><p>Orbital wall reconstruction is challenging due to the complex and diverse OBF types, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, including (a) medial wall fracture, (b) floor wall fracture, (c) fractures in both the medial and floor walls, (d) roof wall fracture, (e) and other types. The orbital walls in these cases are often collapsed, damaged, fractured, deviated, or exhibit a large number of defects in severe cases, which makes reconstruction more difficult. Furthermore, the orbital medial and floor walls are thin bones with low CT gradient values and blurred boundaries, which further increases the complexity of reconstruction. Currently, commercial software is typically used for semi-automatic reconstruction in clinical practice. However, this method is inefficient and inaccurate, requiring tedious manual adjustment and correction. As a result, doctors urgently need fast and accurate automated surgical reconstruction methods.</p><p>Several automatic segmentation methods for the orbital wall have been explored in previous studies <ref type="bibr" target="#b3">[4]</ref>, such as Kim et al.'s <ref type="bibr" target="#b4">[5]</ref> orbital wall modeling based on paranasal sinus segmentation and Lee et al.'s <ref type="bibr" target="#b5">[6]</ref> segmentation algorithm of the orbital cortical bone and thin wall with a double U-Net network structure. However, they did not explore segmentation for OBF. Taghizadeh et al. <ref type="bibr" target="#b6">[7]</ref> proposed an orbital wall segmentation method based on a statistical shape model and local template matching, but individualized differences and factors such as orbital wall deviation and collapse greatly affect its performance in fractured orbits. Deep learning-based algorithms for skull defect reconstruction have also been proposed, such as Li et al.'s <ref type="bibr" target="#b7">[8]</ref> automatic repair network for skull defects, Xiao et al.'s <ref type="bibr" target="#b8">[9]</ref> network model that estimates bone shape using normal facial photos and CT of craniomaxillofacial trauma, and Han et al.'s <ref type="bibr" target="#b9">[10]</ref> craniomaxillofacial defect reconstruction algorithm with a statistical shape model and individual features. However, these methods require the removal of the lesion area followed by reconstruction of the defect, which is different from OBF repair that directly performs orbital wall reconstruction without removal of the destroyed bone. The above methods may be less effective in cases of OBF where factors such as deviation, collapse, and fracture greatly impact the reconstruction network, reducing the reconstruction effect. As of now, there are no automated reconstruction methods reported for OBF surgery. To address the above challenges, this paper proposes a symmetric prior anatomical knowledge-guided adversarial generative network (GAN) for reconstructing orbital walls in OBF surgery. Firstly, the paper proposes an automatic generation method of symmetric prior anatomical knowledge (SPAK) based on spatial transformation, which considers that the symmetrical normal orbital anatomy can guide the reconstruction of the fractured orbital wall. Secondly, the obtained SPAK is used as a prior anatomical guidance for GAN to achieve more accurate automatic reconstruction of the orbital wall. To further improve the network's reconstruction performance, the paper adopts a supervision strategy of multi-loss function combination. Finally, experimental results demonstrate that the proposed network outperforms some state-of-the-art networks in fracture orbital wall reconstruction.</p><p>The main contributions of this paper are summarized as follows: (1) A GAN model guided by SPAK is developed for automatic reconstruction of the orbital wall in OBF surgery, which outperforms the existing methods. (2) The proposed method is the first AI-based automatic reconstruction method of the orbital wall in OBF surgery, which can enhance the repair effectiveness and shorten the surgical planning time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The structure of the proposed SPAK-guided GAN is illustrated in Fig. <ref type="figure">2</ref>, and it mainly comprises three components: automatic generation of SPAK, GAN network structure, and multiple loss function supervision. Each component is elaborated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Automatic Generation of SPAK Based on Spatial Transformation</head><p>The reconstruction of the orbital wall in OBF is a complex task, made more difficult when severe displacement or defects are present. However, since the left and right orbits are theoretically symmetrical structures, utilizing the normal orbital wall on the symmetrical side as prior anatomical guidance in the reconstruction network can aid in accuracy. Prior knowledge has been demonstrated to be effective in medical image computing <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Nonetheless, left and right orbits are not perfectly symmetrical, and the mirror plane can be challenging to locate, leading to substantial errors and unwieldy operations. To address these issues, we propose an automatic generation method for SPAK based on spatial transformations, as depicted in Fig. <ref type="figure">2</ref>. The main steps of this method include: in 3D dimensions and obtaining their respective three-dimensional models; <ref type="bibr" target="#b6">(7)</ref> using the ICP algorithm to register the normal and fractured orbital walls and acquire their deformation field; <ref type="bibr" target="#b7">(8)</ref> based on the previous step, transforming the symmetrical normal orbital wall to the side of the fractured orbital wall using the deformation field; (9) separating the transformed normal orbital wall to obtain a single orbital wall that can serve as a SPAK to guide GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SPAK-Guided GAN for Orbital Wall Reconstruction</head><p>Reconstructing the fractured orbital wall requires generative prediction of the damaged area, which is why we adopted the GAN. Our proposed SPAK-guided GAN consists of a generative network (GN) and a discriminative network (DN), as illustrated in Fig. <ref type="figure">2</ref>. The GN uses a network structure based on 3D V-Net and consists of five encoded layers and four decoded layers to achieve automatic reconstruction of the fractured orbital wall. The input of the GN is the merged block of SPAK and the original image, which guides the GAN network to make more accurate predictions of the orbital wall. To expand the receptive field, each convolution layer uses two convolution stacks and includes residual connections to reduce gradient dissipation. The activation function is ReLU, and group normalization <ref type="bibr" target="#b13">[14]</ref> is added after each convolution to prevent network overfitting. To avoid the loss of shallow features as the number of network layers increases, we added skip connections between the corresponding convolutional layers of the encoded and decoded sections. The DN identifies the authenticity of the reconstructed orbital wall and the ground truth to produce a more realistic orbital wall. It includes five encoded feature layers and one fully connected layer. Each encoded feature layer comprises stacked convolutions and a max-pooling layer with a filter kernel of 2 × 2 × 2 and a step size of 2 to compress the feature map. The input of the DN is either the merged image block of the original image and the reconstructed orbital wall area or the original image and the ground truth. The DN distinguishes between the authenticity of these inputs. After restoring the output result of the network to the original position and resampling, we perform 3D reconstruction to obtain the reconstructed orbital wall model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multiple Loss Function Supervision for GAN</head><p>The GAN network's loss function comprises two components: the loss function of the discriminative network, denoted as Loss D , and the loss function of the generative network, denoted as Loss G . To enhance the reconstruction performance, we adopt a supervision strategy that combines multiple loss functions. To ensure that the GAN network can identify the authenticity of samples, we incorporate the commonly used discriminative loss function in GAN, as presented in Eq. <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">Loss D = min G max D E x,y [log (D (x, y))] + E x [log (1 -D (x, G (x)))] ,<label>(1)</label></formula><p>where D represents the network discriminator, G represents the network generator, G(x) represents reconstruction result, x represents input image, y represents ground truth.</p><p>To improve the accuracy of the reconstructed orbital wall, a combination of multiple loss functions is utilized in Loss G . First, to better evaluate the area loss, the dice coefficient loss function Loss dice is adopted, which can calculate the loss between the reconstructed orbital wall region and the ground truth using Eq. ( <ref type="formula" target="#formula_1">2</ref>). Secondly, due to the potential occurrence of boundary fractures and orbital wall holes during GAN reconstruction, the cross-entropy loss function Loss ce is added to Loss G . Its equation is shown in <ref type="bibr" target="#b2">(3)</ref> to evaluate the reconstruction of the boundary and holes.</p><formula xml:id="formula_1">Loss dice = 1 - 2 N i G (x i ) y i N i G (x i ) 2 + N i y i 2 , (<label>2</label></formula><formula xml:id="formula_2">)</formula><formula xml:id="formula_3">Loss ce = - 1 N N i [y i log(G (x i )) + (1 -y i ) log(1 -G (x i ))] , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where N represents the total number of voxels, G(x i ) represents the voxels of reconstruction results, y i represents the voxels of ground truth. Furthermore, an adversarial loss function L adv , is also incorporated into Loss G . This function evaluates the loss of the generator's output by the discriminator, thus enabling the network to improve its performance in reconstructing the fractured orbital wall. The equation for L adv is shown in <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_5">L adv = E x [log (D (x, G (x)))] ,<label>(4)</label></formula><p>Finally, we combine the loss function Loss dice , Loss ce and L adv to obtain the final Loss G , whose equation is shown in <ref type="bibr" target="#b4">(5)</ref> and λ is the weight parameter.</p><formula xml:id="formula_6">Loss G = Loss dice + Loss ce + λ • L adv . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Set and Settings</head><p>The dataset for this study was obtained from Shanghai Ninth People's Hospital Affiliated to Shanghai Jiao Tong University School of Medicine. It included 150 cases of OBF CT data: 100 for training and 50 for testing. Each case had a blowout orbital fracture on one side and a normal orbit on the other. For normal orbit segmentation training, 70 additional cases of normal orbit CT data were used. The images were 512 × 512 in size, with resolutions ranging from 0.299 mm × 0.299 mm to 0.717 mm × 0.717 mm. The number of slices varied from 91 to 419, with thicknesses ranging from 0.330 mm to 1.0 mm. The ground truth was obtained through semi-automatic segmentation and manual repair by experienced clinicians. To focus on the orbital area, CT scans were resampled to 160 × 160 with a multiple of 32 slices after cutting out both orbits. This resulted in 240 single-orbital CT data for normal orbital segmentation training and 100 for OBF reconstruction training. The proposed networks used patches of size 32 × 160 × 160, with training data augmented using sagittal symmetry.</p><p>The proposed and comparison networks all adopted the same input patch size of 32 × 160 × 160, a batch size of 1, a learning rate of 0.001, and were trained for 30,000 iterations using TensorFlow 1.14 on an NVIDIA RTX 8000 GPU. Evaluation of the reconstruction results was based on the dice similarity coefficient (DSC), intersection over union (IOU), precision, sensitivity, average surface distance (ASD), and 95% Hausdorff distance (95HD). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Experiment Results</head><p>The proposed network is based on the GN, and an ablation experiment was conducted to verify the effectiveness of the adopted strategy. The experimental results are presented in Table <ref type="table" target="#tab_0">1</ref>. Comparing SPAK with other networks, it is evident that the accuracy of the reconstructed orbital wall is relatively poor, and it cannot be directly employed as a reconstruction network. However, it can be used as a prior guide for GAN. By comparing GN with GN+DN (GAN), it is apparent that the accuracy of GAN, except for precision, is better than GN. This finding shows that DN can promote GN to better reconstruct the orbital wall, indicating the correctness of adopting GAN as a reconstruction network for OBF. Furthermore, the addition of SPAK to GN and GAN significantly improved their reconstruction accuracy. Notably, compared with GAN, the proposed reconstruction network improved the DSC of orbital wall reconstruction accuracy by more than 3.5%, increased IOU by more than 5%, and decreased 95HD by 0.35 mm. These results indicate that SPAK guidance plays a crucial role in orbital wall reconstruction. It further demonstrates that the improved strategy can effectively achieve the precise reconstruction of the orbital wall in OBF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparative Experiment Results</head><p>To demonstrate the superior performance of the proposed reconstruction algorithm, we compared it with several state-of-the-art networks used in medical image processing, including U-Net <ref type="bibr" target="#b14">[15]</ref>, V-Net <ref type="bibr" target="#b15">[16]</ref>, Attention U-Net <ref type="bibr" target="#b16">[17]</ref>, and Attention V-Net. The accuracy of the reconstructed results from these networks is compared in Table <ref type="table" target="#tab_1">2</ref>. The comparison with U-Net, V-Net, and Attention U-Net reveals that the proposed reconstruction network outperforms them significantly in terms of reconstruction accuracy evaluation. Specifically, the DSC is improved  by more than 2.5%, the IOU is improved by more than 4.5%, the sensitivity is improved by more than 6.5%, and the 95HD distance error is reduced by more than 0.35 mm. These results indicate that the proposed network has better reconstruction performance for the orbital wall. Comparing with Attention V-Net, it is shown that the proposed reconstruction network has better reconstruction accuracy, except for precision. Although Attention V-Net has higher precision, its standard deviation is relatively large. Figure <ref type="figure">3</ref> is a comparison chart of the reconstruction results of each method, showing that other methods are difficult to accurately predict the orbital wall boundary, while the proposed method can generate the orbital wall more accurately. Figure <ref type="figure">4</ref> presents a comparison chart of surface distance errors from the reconstruction results, which shows that other methods have relatively large distance errors in the medial and floor walls of the orbit, and even lead to cracks and holes. In contrast, the proposed network significantly addresses these issues, indicating its superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In summary, this paper proposes a SPAK-guided GAN for accurately automating OBF wall reconstruction. Firstly, we propose an automatic generation method of SPAK based on spatial transformation, which maps the segmented symmetrical normal orbital wall to a fractured orbit to form an effective SPAK. On this basis, a SPAK-guided GAN network is developed for the automatic reconstruction of the fractured orbital wall through adversarial learning. Furthermore, we use the strategy of multi-loss function supervision to improve the accuracy of network reconstruction. The final experimental results demonstrate that the proposed reconstruction network achieves accurate automatic reconstruction of the fractured orbital wall, with a DSC of 92.35 ± 2.13% and a 95% Hausdorff distance of 0.59 ± 0.23 mm, which is significantly better than other networks. This network achieves the automatic reconstruction of the orbital wall in OBF, which effectively improves the accuracy and efficiency of OBF surgical planning. In the future, it will have excellent application prospects in the repair surgery of OBF.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Common types and reconstruction challenges of orbital blowout fractures.</figDesc><graphic coords="3,57,81,158,84,308,08,104,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )Fig. 2 .</head><label>12</label><figDesc>Fig. 2. Structure of the proposed symmetric prior anatomical knowledge-guided generative adversarial network for reconstructing the fractured orbital wall.</figDesc><graphic coords="4,72,48,336,32,307,12,226,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Orbital wall reconstruction results comparisons between our network and other networks. Green is the ground truth, and red is the network's reconstruction result. (Color figure online)</figDesc><graphic coords="8,79,98,322,79,292,96,128,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results from ablation experiments of our method.</figDesc><table><row><cell>Networks</cell><cell>DSC (%) ↑</cell><cell>IOU (%) ↑</cell><cell cols="3">Precision (%) ↑ Sensitivity (%) ↑ ASD (mm) ↓ 95HD (mm) ↓</cell></row><row><cell>SPAK</cell><cell cols="3">71.27 ± 4.28 55.53 ± 5.06 74.97 ± 4.98</cell><cell>68.02 ± 4.55</cell><cell>0.50 ± 0.08</cell><cell>1.80 ± 0.30</cell></row><row><cell>GN</cell><cell cols="3">87.97 ± 4.08 78.75 ± 6.14 89.40 ± 3.23</cell><cell>86.73 ± 5.74</cell><cell>0.20 ± 0.11</cell><cell>1.06 ± 0.59</cell></row><row><cell cols="4">GN+DN(GAN) 88.98 ± 3.15 80.28 ± 4.95 87.69 ± 3.95</cell><cell>90.41 ± 3.55</cell><cell>0.18 ± 0.09</cell><cell>0.96 ± 0.62</cell></row><row><cell>GN+ SPAK</cell><cell cols="3">91.87 ± 2.25 85.03 ± 3.75 92.26 ± 2.65</cell><cell>91.57 ± 3.30</cell><cell>0.12 ± 0.05</cell><cell>0.64 ± 0.26</cell></row><row><cell>Our method</cell><cell cols="3">92.35 ± 2.13 85.86 ± 3.53 92.01 ± 2.58</cell><cell>92.75 ± 2.66</cell><cell>0.11 ± 0.05 0.59 ± 0.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluation results from ablation experiments of our method. Att U-Net denotes Attention U-Net, and Att V-Ne denotes Attention V-Net. Net<ref type="bibr" target="#b14">[15]</ref> 88.01 ± 3.19 78.73 ± 5.02 88.27 ± 3.39 87.97 ± 5.23 0.20 ± 0.10 1.15 ± 0.10 V-Net [16] 89.42 ± 2.72 80.97 ± 4.39 91.04 ± 3.22 87.97 ± 3.65 0.18 ± 0.12 1.16 ± 1.78</figDesc><table><row><cell cols="2">Att U-Net [17] 87.46 ± 3.58 77.88 ± 5.37 91.05 ± 3.98</cell><cell>84.43 ± 5.95</cell><cell>0.22 ± 0.19</cell><cell>1.41 ± 2.50</cell></row><row><cell>Att V-Net</cell><cell cols="2">89.27 ± 8.81 80.77 ± 12.30 92.11 ± 12.50 86.71 ± 5.88</cell><cell>0.19 ± 0.19</cell><cell>0.90 ± 0.80</cell></row><row><cell>Our method</cell><cell>92.35 ± 2.13 85.86 ± 3.53 92.01 ± 2.58</cell><cell>92.75 ± 2.66</cell><cell cols="2">0.11 ± 0.05 0.59 ± 0.23</cell></row></table><note><p><p>Networks</p>DSC (%) ↑ IOU (%) ↑ Precision (%) ↑ Sensitivity (%) ↑ ASD (mm) ↓ 95HD (mm) ↓ U-</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by grants from the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">81971709</rs>; <rs type="grantNumber">M-0019</rs>; <rs type="grantNumber">82011530141</rs>), the <rs type="funder">China Postdoctoral Science Foundation</rs> (<rs type="grantNumber">2023M732245</rs>), the <rs type="funder">Foundation of Science and Technology Commission of Shanghai Municipality</rs> (<rs type="grantNumber">20490740700</rs>; <rs type="grantNumber">22Y11 911700</rs>), <rs type="funder">Shanghai Jiao Tong University Foundation on Medical and Technological Joint Science Research</rs> (<rs type="grantNumber">YG2021ZD21</rs>; <rs type="grantNumber">YG2021QN72</rs>; <rs type="grantNumber">YG2022QN056</rs>; <rs type="grantNumber">YG2023ZD19</rs>; <rs type="grantNumber">YG2023ZD15</rs>), the <rs type="funder">Funding of Xiamen Science and Technology Bureau</rs> (No. <rs type="grantNumber">3502Z20221012</rs>), <rs type="funder">Cross disciplinary Research Fund of Shanghai Ninth People's Hospital, Shanghai Jiao Tong University School of Medicine</rs> (<rs type="grantNumber">JYJC202115</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SbywJsD">
					<idno type="grant-number">81971709</idno>
				</org>
				<org type="funding" xml:id="_4tc3uJd">
					<idno type="grant-number">M-0019</idno>
				</org>
				<org type="funding" xml:id="_rwyxCAv">
					<idno type="grant-number">82011530141</idno>
				</org>
				<org type="funding" xml:id="_4UQa6bc">
					<idno type="grant-number">2023M732245</idno>
				</org>
				<org type="funding" xml:id="_SGXBpGs">
					<idno type="grant-number">20490740700</idno>
				</org>
				<org type="funding" xml:id="_32KuMuT">
					<idno type="grant-number">22Y11 911700</idno>
				</org>
				<org type="funding" xml:id="_aBueerg">
					<idno type="grant-number">YG2021ZD21</idno>
				</org>
				<org type="funding" xml:id="_sdcM3ee">
					<idno type="grant-number">YG2021QN72</idno>
				</org>
				<org type="funding" xml:id="_N2Jc3Pg">
					<idno type="grant-number">YG2022QN056</idno>
				</org>
				<org type="funding" xml:id="_DSJbhrV">
					<idno type="grant-number">YG2023ZD19</idno>
				</org>
				<org type="funding" xml:id="_jNy7suG">
					<idno type="grant-number">YG2023ZD15</idno>
				</org>
				<org type="funding" xml:id="_tpNNdd6">
					<idno type="grant-number">3502Z20221012</idno>
				</org>
				<org type="funding" xml:id="_T7GdCps">
					<idno type="grant-number">JYJC202115</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The incidence of ocular injuries in isolated orbital fractures</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jupiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Czerwinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Plastic Surg</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="61" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Factors associated with increased risk of serious ocular injury in the setting of orbital fracture</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Rossin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szypko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Giese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="83" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Surgical repair of orbital blow-out fractures: outcomes and complications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ozturker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Ozbilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tuncer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Beyoglu Eye J</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="199" to="206" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic segmentation of orbital wall from CT images via a thin wall region supervision-based multi-scale feature search network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Three-dimensional orbital wall modeling using paranasal sinus segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cranio-Maxillofacial Surg</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="959" to="967" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MGB-net: orbital bone segmentation from head and neck CT images using multi-graylevel-bone convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="692" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated CT bone segmentation using statistical shape modelling and local template matching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Taghizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Terrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Becce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Büchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Biomech. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1303" to="1310" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic skull defect restoration and cranial implant generation for cranioplasty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102171</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimating reference shape model for personalized surgical reconstruction of craniomaxillofacial defects</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="362" to="373" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical and individual characteristics-based reconstruction for craniomaxillofacial surgery</title>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1155" to="1165" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A review on AI-based medical image computing in head and neck surgery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A symmetric prior knowledge based deep learning model for intracerebral hemorrhage lesion segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nijiati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Physiol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">2481</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cardiac MRI segmentation with sparse annotations: ensembling deep learning uncertainty and shape priors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kuling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">102532</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuxin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="742" to="755" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention gated networks: learning to leverage salient regions in medical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
