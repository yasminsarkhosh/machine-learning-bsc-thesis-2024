<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization</title>
				<funder ref="#_uV5CcJK #_PRhDx79 #_D867rtM #_2wmSq6M">
					<orgName type="full">National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yubo</forename><surname>Fan</surname></persName>
							<email>yubo.fan@vanderbilt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37235</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianing</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Digital Technology and Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiyuan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Digital and Automation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<postCode>19355</postCode>
									<settlement>Malvern</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37235</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37235</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Otolaryngology -Head and Neck Surgery</orgName>
								<orgName type="institution">Medical University of South Carolina</orgName>
								<address>
									<postCode>29425</postCode>
									<settlement>Charleston</settlement>
									<region>SC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jack</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37235</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benoit</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37235</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Deep-Learning-Based Framework for Cochlear Implant Electrode Array Localization</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="376" to="385"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E42698236122BD7FF580062B74E1E160</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cochlear Implant</term>
					<term>Electrode Localization</term>
					<term>Object Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cochlear implants (CIs) are neuroprosthetics that can provide a sense of sound to people with severe-to-profound hearing loss. A CI contains an electrode array (EA) that is threaded into the cochlea during surgery. Recent studies have shown that hearing outcomes are correlated with EA placement. An imageguided cochlear implant programming technique is based on this correlation and utilizes the EA location with respect to the intracochlear anatomy to help audiologists adjust the CI settings to improve hearing. Automated methods to localize EA in postoperative CT images are of great interest for large-scale studies and for translation into the clinical workflow. In this work, we propose a unified deep-learning-based framework for automated EA localization. It consists of a multi-task network and a series of postprocessing algorithms to localize various types of EAs. The evaluation on a dataset with 27 cadaveric samples shows that its localization error is slightly smaller than the state-of-the-art method. Another evaluation on a large-scale clinical dataset containing 561 cases across two institutions demonstrates a significant improvement in robustness compared to the state-of-the-art method. This suggests that this technique could be integrated into the clinical workflow and provide audiologists with information that facilitates the programming of the implant leading to improved patient care.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A cochlear implant (CI) has an electrode array (EA) that is surgically inserted into the cochlea to stimulate the auditory nerves and treat patients with severe-to-profound sensorineural hearing loss. Although CIs have achieved great success, hearing outcomes among recipients vary significantly <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Recent studies have shown that factors related to electrode positioning impact on audiological outcomes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. These studies require knowing the precise electrode locations, which can be obtained by postoperative CT imaging. Another clinical application that requires precise electrode locations is imageguided cochlear implant programming <ref type="bibr" target="#b4">[5]</ref>. After surgical implantation, the CI needs to be programmed such that an optimized frequency mapping <ref type="bibr" target="#b23">[24]</ref> can be determined for each patient. Knowledge about the spatial relationship between the electrodes and the intracochlear anatomy permits to generate programming solutions which have been shown to significantly improve the hearing outcomes for both adult and pediatric CI recipients <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>To facilitate the EA localization process, automated methods have been proposed by several groups. In <ref type="bibr" target="#b7">[8]</ref> EAs are categorized as closely-spaced and distantly-spaced. EAs with an interelectrode spacing such that the intensity contrast between electrodes cannot be distinguished are considered closely-spaced, while the opposite applies to distantlyspaced EAs. Note that a given EA model could be categorized as closely-spaced or distantly-spaced, depending on the image resolution. For example, some closely-spaced EAs included in <ref type="bibr" target="#b7">[8]</ref> could be considered distantly-spaced in some images acquired at very high resolution (0.08mm isotropic) as in <ref type="bibr" target="#b8">[9]</ref>. For algorithms localizing closely-spaced EAs, extracting the centerline of an EA is often an important step <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. It is achieved by either using intensity-based features only <ref type="bibr" target="#b10">[11]</ref> or combining both intensity-based and EA shape-based features <ref type="bibr" target="#b7">[8]</ref>. For localizing distantly-spaced EAs, hand-crafted feature extractors are utilized to detect individual blobs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. To link the electrode candidates in the correct order and remove false positive candidates, graph-based pathfinding algorithms <ref type="bibr" target="#b11">[12]</ref> or Markov random field models <ref type="bibr" target="#b13">[14]</ref> have been proposed. While there has been an emergence of methods based on deep learning (DL) recently <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, they cannot be viewed as a complete solution to the automatic EA localization problem because the networks are only trained and validated on one type of EA model <ref type="bibr" target="#b16">[17]</ref> or cannot order/link the detected electrodes to form a complete array <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In this work, we present a novel DL-based framework that consists of a multi-task network and a set of postprocessing algorithms to localize the cochlear implant electrode array. Our contribution is three-fold: (1) To the best of our knowledge, it is the first unified DL-based framework designed for localizing both distantly-and closely-spaced EAs in CT and cone-beam CT (CBCT) images. <ref type="bibr" target="#b1">(2)</ref> We propose four (three detection and one segmentation) tasks for the multi-task network such that this single network can be trained on various kinds of EAs. <ref type="bibr" target="#b2">(3)</ref> We extensively evaluate this framework on datasets that significantly exceed the scale of all datasets reported in the literature to date: a heterogeneous clinical test set with CT or CBCT images of 561 implanted ears and a test set with gold standard ground truth for 27 implanted cadaveric ears. Results show that the proposed framework is significantly more robust (generates results that require manual adjustments less often) than the state-of-the-art (SOTA) techniques, while also being slightly more accurate. These findings indicate that the proposed framework could be reliably used to support large-scale quantitative studies and deployed in the clinical workflow to provide clinicians with critical information at the time and point of care. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>The data in this study consist of a large-scale clinical dataset from CI recipients (dataset #1) as well as 27 cadaveric samples (dataset #2). Dataset #1 includes 1324 implanted ears from CI recipients treated at two institutions (datasets #1A and #1B). 8 types of distantlyand closely-spaced EA from 3 manufacturers are included in these cases. Dataset #1A has 958 implanted ears of which 97% are scanned with CBCT scanners, and the remaining (3%) are scanned with conventional CT scanners. Dataset #1B includes 366 implanted ears of which most (98%) are scanned with conventional CT scanners and the remaining (2%) are scanned with CBCT scanners. Dataset #2 contains 4 types of distantly-spaced EAs, and these specimens are scanned with conventional CTs.</p><p>The training and validation sets are all from dataset #1A and constitute 60% and 20% of that dataset, respectively. The remaining 20% of dataset #1A along with dataset #1B (561 implanted ears in total) are used to test the robustness of the proposed framework. Dataset #2 is used to test its accuracy because the gold standard ground truth can be obtained using their paired micro-CT <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. Details on the datasets and EA specifications can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-task U-Net</head><p>The proposed framework is designed to localize various types of EAs in both CT and CBCT. This is challenging because the number of electrodes to be detected and the interelectrode spacing is different among EA models, and the postoperative images can have different intensity characteristics depending on the type of scanner, i.e., CT or CBCT, that is used for their acquisition. To normalize the input image in a way that enhances both the nearby anatomy, which contains contextual information, and the high intensity (usually the electrodes) component, inspired by <ref type="bibr" target="#b15">[16]</ref>, we separate the raw image into two channels. One is the original image clipped at the 99.9% of its intensity histogram. The other is the original image in the [99.9%-100%] interval of its intensity histogram. Each channel is linearly normalized to [0,1] based on its own min-max values. We refer to these as the low and high intensity bands and an example can be seen in Fig. <ref type="figure" target="#fig_0">1A</ref>.</p><p>To avoid having to train a network for each EA type, we define four tasks that a single multi-task network can learn simultaneously and whose output can be used to localize and order the electrodes in all arrays. Specifically, as shown in Fig. <ref type="figure" target="#fig_0">1B</ref>, the four tasks include (1) detection of all the electrodes on the EA by heatmap regression, (2) detection of the most apical endpoint (tip) of the EA by heatmap regression, (3) detection of the most basal endpoint (the farthest electrode from the tip) of the EA by heatmap regression, and (4) segmentation of the EA centerline that starts and ends with the two endpoints. Although the network is a simple U-Net-like encoder-decoder architecture, the innovation is more focused on the multi-task strategy such that the network can serve as a robust feature extractor that leverages a large heterogeneous dataset.</p><p>We train the multi-task network using the electrode positions obtained with the methods described in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> corrected manually when a large localization error is visually observed. For tasks (#1,2, and 3) aiming at localizing electrodes, the ground truth is a one-channel heatmap for each task with a Gaussian kernel (variance of 2 voxels) at each electrode location. Following <ref type="bibr" target="#b18">[19]</ref>, we use a penalty-reduced voxel-wise logistic regression with the focal loss <ref type="bibr" target="#b19">[20]</ref> as the training objective.</p><p>Depending on the image quality and the interelectrode spacing, EAs can appear as a whole bright (i.e., with high Hounsfield Unit (HU) values) tubular structure or distinguishable bright blobs representing individual electrodes. We define the centerline of the EA as a line connecting each electrode in sequential order (from most apical to most basal or the opposite). The motivation for segmenting the EA centerline is twofold. First, after we extract all the electrodes from the predicted heatmap (Task #1), we need to order them. We will show later in our postprocessing algorithms that these detected electrodes can be linked in the correct order using the segmented centerline and the detected two endpoints (Algorithm 1 in Fig. <ref type="figure" target="#fig_0">1C</ref>). The second reason is that it can serve as an alternative EA localization method by sampling the centerline using the known interelectrode spacing specific to a particular EA model (Algorithm 2 in Fig. <ref type="figure" target="#fig_0">1C</ref>). It is essential when the electrodes are not discernible due to the low image resolution and/or the small interelectrode spacing. Different from <ref type="bibr" target="#b7">[8]</ref>, we resort to a DL approach rather than human-crafted feature extractors. To make the training of the centerline segmentation easier, we dilate the ground truth centerline to a tubular-structured mask with a radius of 3 voxels as the learning target. In addition to the Dice loss, we adopt a clDice loss which has shown its superiority in improving the accuracy and preserving the topology of the underlying one-voxel wide centerline <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Postprocessing Algorithms for EA Localization</head><p>As said above, although the output of the multi-task network contains essential information to identify the EA, it does not provide the desired final output, i.e., the position (coordinates) of the ordered electrodes in the image. To do so, we have designed a series of postprocessing algorithms that can effectively extract the ordered electrode locations from the four output maps. As shown in Fig. <ref type="figure" target="#fig_0">1C</ref>, there are two main algorithms (Algorithm 1 and Algorithm 2) that are suitable for distantly-and closely-spaced EAs, respectively.</p><p>Algorithm 1 takes the heatmap of all the electrodes (Task #1) as input and utilizes a non-maximum suppression (NMS) algorithm, which is a common postprocessing step for object detection <ref type="bibr" target="#b21">[22]</ref>, to obtain the desired number of electrodes. Then, the centerline of the EA is extracted by skeletonizing its segmented mask. Its two endpoints are further refined by merging the detection results of the most apical and basal points. Finally, all the detected electrodes are linked with the guidance of the centerline along the direction from the most apical to the most basal. Algorithm 1 works well if there is an apparent contrast between the electrodes in the heatmap, which is the case for most distantly-spaced EAs.</p><p>However, for closely-spaced EAs, it is nearly impossible to differentiate the individual electrodes. Algorithm 2 is designed to localize EAs in such situations. After the extraction and refinement of the EA centerline, the centerline is smoothed with a cubic spline, and the final electrode positions are obtained by resampling along it using known interelectrode spacing for the EA <ref type="bibr" target="#b7">[8]</ref>. Note that for distantly-spaced EAs, Algorithm 1 can occasionally lead to abnormal localization results, such as an incorrect number of detected electrodes or the spacing between the detected electrodes being inconsistent with the known interelectrode spacing for the EA. This is most often caused by poor image quality that affects the creation of the subsequent heatmap (Task #1). We have designed simple rules to detect these anomalies. When one is detected, the framework switches to Algorithm 2 for a more reliable sampling-based localization for distantly-spaced EAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>The multi-task U-Net is trained with PyTorch 1.12 on an NVIDIA RTX 2080 Ti GPU. We use MONAI <ref type="bibr" target="#b22">[23]</ref> for data augmentation which contains an additive Gaussian noise and random affine transformations. The images are preprocessed by being rigidly registered to the left ear (mirroring is performed if one case is a right ear) of a template volume (atlas), resampled to a 0.1mm isotropic voxel size, and cropped into a region of interest (ROI) with a dimension of 320 × 320 × 320. Due to GPU memory size limit, we use a patch-based strategy (with a dimension of 256 × 256 × 192) for training. The batch size is set to 1 and we use AdamW optimizer with learning rate of 5e-4. At inference we use a sliding-window approach to merge the results. The network is trained for 250 epochs, and we select the epoch with the lowest validation loss as our final model. Note that a minority (3%) of the images in dataset #1 are reconstructed with a limited HU range, i.e., <ref type="bibr">[-1024,3071</ref>]HU. In these images bone and electrodes have similar intensity values. The remaining (97%) images have maximum intensities far larger than 3071HU. In these images electrodes have larger intensity values than bone. To address this issue, we train another DL model with the same training strategy and dataset, but the intensity of all the training images is saturated at 3071HU, i.e., every pixel with an intensity above 3071HU is assigned a value of 3071HU. All the results obtained for limited HU range images presented herein are obtained with this dedicated model. The postprocessing algorithms are implemented in Python 3.9 and NumPy. We use skimage for 3D skeletonization and the csaps package for calculating the cubic spline. The inference time for the proposed framework (from loading the image to outputting the electrode locations) ranges between 5 to 20 s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation and Results</head><p>The techniques described in <ref type="bibr" target="#b7">[8]</ref> (for closely-spaced EAs) and <ref type="bibr" target="#b11">[12]</ref> (for distantly-spaced EAs) are designed and validated with EAs and imaging protocols that are similar to those used in this study. They are also in routine use with over 2000 ears processed at the two institutions in dataset #1. They are considered to be the SOTA methods for comparison. We first evaluate the accuracy of the proposed framework on dataset #2 (27 implanted cadaveric ears) for which the localization ground truth is known. Since the EAs in dataset #2 are all distantly-spaced, <ref type="bibr" target="#b11">[12]</ref> is used as the previous SOTA method for comparison. We define the point-to-point error (P2PE) as the Euclidean distance between the predicted electrode location and the ground truth location, and we calculate five P2PE-based metrics (maximum, median, mean, standard deviation (std), and the minimum P2PE) for all the electrodes in each case. The quantitative results are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We can see from the left box plot that the results of the previous method <ref type="bibr" target="#b11">[12]</ref> contain an outlier with relatively large P2PE, which is attributed to the low quality of the image. The median values in the right box plot show that the proposed framework has slightly smaller P2PE across the five metrics, but the differences are not statistically significant.</p><p>As introduced in Sect. 2.1, the 561 clinical cases in dataset #1 used for testing are highly heterogeneous. They contain CBCT and CT images from dataset #1A and #1B. Since manual annotations are not available for these images, we ask three experts to evaluate and compare the results obtained with the previous SOTA methods ( <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b11">[12]</ref>) and those obtained with the proposed method. Specifically, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>, we generate MIP (Maximum Intensity Projection) images and mark the locations at which the electrodes have been localized. This permits a rapid visual assessment of the localization quality. Next, for each case, we present the visualization of the results generated by the previous SOTA methods and by the proposed method side-by-side but in a random order, such that the experts are blind to the methods used to localize the contacts. They are then asked to rate the localization results as acceptable, i.e., no need to adjust the localization result or "Needs Adjustment (NA)", i.e., at least one electrode location needs to be adjusted by more than half a contact size. When both results are acceptable, raters are also asked to decide which one is preferred (i.e., has a more accurate localization result) or there is no preference.</p><p>Before performing the expert evaluation, we calculate the maximum P2PE between the results from the previous SOTA and the proposed methods. For cases with maximum P2PE larger than 0.3 mm (we refer to them as the large-error subset), there is a high probability that at least one of the methods generates NA results. For cases with maximum P2PE smaller than 0.3 mm (we refer to them as the small-error subset), we presume that the probability of NA results is relatively small. To limit the demand on the experts' time, we ask three experts (R1, R2, and R3) to rate the large error subset (164 cases) and only one expert (R1) to rate the small error subset (397 cases).</p><p>The expert evaluation results are shown as radar plots in Fig. <ref type="figure" target="#fig_2">3</ref>. The left figure shows the acceptance rate for the large error cases evaluated by raters R1, R2, and R3. Except for the cases rated by R1 that contain closely-spaced EAs in CBCT ("R1-CBCT-Closely"), the localization results generated by the proposed method have a substantially higher acceptance rate than the previous SOTA methods ( <ref type="bibr" target="#b7">[8]</ref> for closely-spaced and <ref type="bibr" target="#b11">[12]</ref> for distantly-spaced). For the cases in which both methods are acceptable, the average preference rates across the three raters are: 20.2% for "SOTA preferred", 30.3% for "Proposed preferred", and 49.5% for "No preference". As can be seen in the right plot of Fig. <ref type="figure" target="#fig_2">3</ref>, the overall acceptance rate on the whole clinical test set evaluated by R1 is 80.7% for the previous SOTA methods and 89.7% for the proposed method. It is interesting to note that although the proposed method is trained mostly on CBCT images, it still generalizes well on CT images. Figure <ref type="figure" target="#fig_3">4</ref> shows two representative cases from the large-error subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we have proposed a novel DL-based framework for cochlear implant EA localization. To the best of our knowledge, it is the first unified DL-based framework designed for localizing both distantly-and closely-spaced EAs in CT and CBCT images.</p><p>Compared to the SOTA methods, the proposed framework is substantially more robust (9% less NA results) when evaluated on a large-scale clinical dataset and achieves slightly more accurate localization results on a dataset containing 27 cadaveric samples with gold standard ground truth. While it may be possible to improve our success rate further, a low percentage of NA results is unavoidable. We are thus developing quality assessment techniques to alert end users when images have poor quality and/or results are unreliable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the proposed framework. It consists of two major modules: a multi-task U-Net that is generic for various EA types (B) and a set of postprocessing algorithms that utilize the known EA geometry (C). The input to the multi-task U-Net is the two-channel image that contains low and high intensity bands extracted from the raw image (A). The output of the framework is the ordered electrode locations (D).</figDesc><graphic coords="3,43,80,100,52,336,10,247,87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Box plot of the P2PE in dataset #2 (27 implanted cadaveric ears). The median value for each metric is shown above each box in the right figure.</figDesc><graphic coords="6,55,98,385,85,340,36,126,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Acceptance rate (higher means better) of the SOTA and proposed localization results on the large-error subset (left) and the whole clinical test set (right). R1, R2, and R3 are different raters.</figDesc><graphic coords="7,41,79,57,14,340,33,151,27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Two representative cases from the large-error subset are shown with the display used to visually evaluate localization quality. Yellow circles highlight the regions that contain the NA electrode localization. Top row: a distantly-spaced EA. Bottom row: a closely-spaced EA. The Left and right images in each row represent results obtained with one of the methods; the method that is used for each is not disclosed to the evaluator.</figDesc><graphic coords="8,56,46,57,26,339,52,212,14" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research is supported by the <rs type="funder">National Institutes of Health</rs> grant <rs type="grantNumber">R01DC014037</rs>, <rs type="grantNumber">R01DC008408</rs>, and <rs type="grantNumber">R01DC014462</rs>, and <rs type="grantNumber">T32EB021937</rs>. The content is solely the responsibility of the authors and does not necessarily represent the official views of these institutes.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uV5CcJK">
					<idno type="grant-number">R01DC014037</idno>
				</org>
				<org type="funding" xml:id="_PRhDx79">
					<idno type="grant-number">R01DC008408</idno>
				</org>
				<org type="funding" xml:id="_D867rtM">
					<idno type="grant-number">R01DC014462</idno>
				</org>
				<org type="funding" xml:id="_2wmSq6M">
					<idno type="grant-number">T32EB021937</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hearing and speech benefits of cochlear implantation in children: a review of the literature</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Cushing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Papsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijporl.2020.109984</idno>
		<ptr target="https://doi.org/10.1016/j.ijporl.2020.109984" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Pediatr. Otorhinolaryngol</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">109984</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cochlear implantation outcomes in adults: a scoping review</title>
		<author>
			<persName><forename type="first">I</forename><surname>Boisvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dowell</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0232421</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0232421" />
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">232421</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Factors affecting open-set word recognition in adults with cochlear implants</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Holden</surname></persName>
		</author>
		<idno type="DOI">10.1097/AUD.0b013e3182741aa7</idno>
		<ptr target="https://doi.org/10.1097/AUD.0b013e3182741aa7" />
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Further evidence of the relationship between cochlear implant electrode positioning and hearing outcomes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakravorti</surname></persName>
		</author>
		<idno type="DOI">10.1097/MAO.0000000000002204</idno>
		<ptr target="https://doi.org/10.1097/MAO.0000000000002204" />
	</analytic>
	<monogr>
		<title level="j">Otol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">617</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image-guidance enables new methods for customizing cochlear implant stimulation strategies</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNSRE.2013.2253333</idno>
		<ptr target="https://doi.org/10.1109/TNSRE.2013.2253333" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="820" to="829" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clinical evaluation of an image-guided cochlear implant programming strategy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hedley-Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
		<idno type="DOI">10.1159/000365273</idno>
		<ptr target="https://doi.org/10.1159/000365273" />
	</analytic>
	<monogr>
		<title level="j">Audiol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="400" to="411" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Initial results with image-guided cochlear implant programming in children</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1097/MAO.0000000000000909</idno>
		<ptr target="https://doi.org/10.1097/MAO.0000000000000909" />
	</analytic>
	<monogr>
		<title level="j">Otol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic localization of closely spaced cochlear implant electrode arrays in clinical CTs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1002/mp.13185</idno>
		<ptr target="https://doi.org/10.1002/mp.13185" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5030" to="5040" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated calculation of cochlear implant electrode insertion parameters in clinical cone-beam CT</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A W</forename><surname>Andersen</surname></persName>
		</author>
		<idno type="DOI">10.1097/MAO.0000000000003432</idno>
		<ptr target="https://doi.org/10.1097/MAO.0000000000003432" />
	</analytic>
	<monogr>
		<title level="j">Otol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="199" to="205" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic localization of cochlear implant electrodes in CT</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10404-1_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10404-1_42" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Hata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Howe</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic localization of cochlear implant electrode contacts in CT</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bennink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Wendrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vonken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Van Zanten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<idno type="DOI">10.1097/AUD.0000000000000438</idno>
		<ptr target="https://doi.org/10.1097/AUD.0000000000000438" />
	</analytic>
	<monogr>
		<title level="j">Ear Hear</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">376</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic graph-based method for localization of cochlear implant electrode arrays in clinical CT with sub-voxel accuracy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakravorti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2018.11.005</idno>
		<ptr target="https://doi.org/10.1016/j.media.2018.11.005" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cochlear implant electrode localization in post-operative CT using a spherical measure</title>
		<author>
			<persName><forename type="first">B</forename><surname>Braithwaite</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI.2016.7493512</idno>
		<ptr target="https://doi.org/10.1109/ISBI.2016.7493512" />
	</analytic>
	<monogr>
		<title level="m">IEEE 13th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1329" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Localization of cochlear implant electrodes from cone beam computed tomography using particle belief propagation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nogueira</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI48211.2021.9433845</idno>
		<ptr target="https://doi.org/10.1109/ISBI48211.2021.9433845" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="593" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deep-learning-based method for the localization of cochlear implant electrodes in CT images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI.2019.8759536</idno>
		<ptr target="https://doi.org/10.1109/ISBI.2019.8759536" />
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1141" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Metal artifact reduction, intra cochlear anat-omy segmentation, and cochlear implant electrodes localization in CT images with a multi-task 3D network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2580931</idno>
		<ptr target="https://doi.org/10.1117/12.2580931" />
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2021: Image Processing</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</editor>
		<imprint>
			<publisher>SPIE, Online Only, United States</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A web-based automated image processing research platform for cochlear implantation-related studies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Margeta</surname></persName>
		</author>
		<idno type="DOI">10.3390/jcm11226640</idno>
		<ptr target="https://doi.org/10.3390/jcm11226640" />
	</analytic>
	<monogr>
		<title level="j">J. Clin. Med</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">6640</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Validation of automatic cochlear im-plant electrode localization techniques using µCTs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JMI.5.3.035001</idno>
		<ptr target="https://doi.org/10.1117/1.JMI.5.3.035001" />
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">35001</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Objects as Points</title>
		<imprint>
			<date type="published" when="2019">190407850. 2019</date>
		</imprint>
	</monogr>
	<note>Cs</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2858826</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2858826" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">clDice --a novel topology-preserving loss function for tubular structure segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shit</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR46437.2021.01629</idno>
		<ptr target="https://doi.org/10.1109/CVPR46437.2021.01629" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16555" to="16564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MONAI: An open-source framework for deep learning in healthcare</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.02701</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2211.02701" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A cochlear frequency-position function for several species-29 years later</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Greenwood</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.399052</idno>
		<ptr target="https://doi.org/10.1121/1.399052" />
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="2592" to="2605" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
