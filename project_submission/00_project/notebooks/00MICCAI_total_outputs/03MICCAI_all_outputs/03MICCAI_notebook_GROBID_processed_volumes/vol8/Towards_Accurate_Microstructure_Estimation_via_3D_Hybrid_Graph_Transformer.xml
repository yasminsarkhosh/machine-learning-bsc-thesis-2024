<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer</title>
				<funder ref="#_f6QbnZC">
					<orgName type="full">Natural Science Foundation of Heilongjiang Province</orgName>
				</funder>
				<funder ref="#_Pb2EaXh #_rP3FuUy">
					<orgName type="full">United States National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_Ndk6evJ #_fhQ7NVU">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junqing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haotian</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang University</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tewodros</forename><surname>Tassew</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiquan</forename><surname>Ma</surname></persName>
							<email>majiquan@hlju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang University</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pew-Thian</forename><surname>Yap</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">University of North Carolina</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Biomedical Research Imaging Center</orgName>
								<orgName type="institution">University of North Carolina</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Geng</forename><surname>Chen</surname></persName>
							<email>geng.chen@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="25" to="34"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">5AE6D3CDED01DD530FE755B4523CB1EC</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Microstructure Imaging</term>
					<term>Graph Neural Network</term>
					<term>Transformer</term>
					<term>3D Spatial Domain</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has drawn increasing attention in microstructure estimation with undersampled diffusion MRI (dMRI) data. A representative method is the hybrid graph transformer (HGT), which achieves promising performance by integrating q-space graph learning and x-space transformer learning into a unified framework. However, this method overlooks the 3D spatial information as it relies on training with 2D slices. To address this limitation, we propose 3D hybrid graph transformer (3D-HGT), an advanced microstructure estimation model capable of making full use of 3D spatial information and angular information. To tackle the large computation burden associated with 3D x-space learning, we propose an efficient q-space learning model based on simplified graph neural networks. Furthermore, we propose a 3D x-space learning module based on the transformer. Extensive experiments on data from the human connectome project show that our 3D-HGT outperforms stateof-the-art methods, including HGT, in both quantitative and qualitative evaluations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Diffusion microstructure imaging has drawn increasing research attention in recent years. A number of powerful microstructure models have been proposed and shown great success in both clinical and research sides. Typical examples include diffusion kurtosis imaging (DKI) <ref type="bibr" target="#b14">[15]</ref>, neurite orientation dispersion and density imaging (NODDI) <ref type="bibr" target="#b22">[23]</ref>, and spherical mean technique (SMT) <ref type="bibr" target="#b15">[16]</ref>. However, these models usually rely on diffusion MRI (dMRI) data densely sampled in q-space with a sufficient angular resolution and multiple shells, which are impractical in clinical settings.</p><p>To resolve this problem, deep learning has been introduced to learn the mapping between high-quality microstructure indices and q-space undersampled dMRI data. For instance, Golkov et al. <ref type="bibr" target="#b11">[12]</ref>, for the first time, introduced deep learning to microstructure estimation from undersampled dMRI data with a multilayer perceptron (MLP). Gibbons et al. <ref type="bibr" target="#b10">[11]</ref> trained a 2D CNN to generate NODDI and fractional anisotropy parameter maps from undersampled dMRI data. Tian et al. <ref type="bibr" target="#b17">[18]</ref> proposed DeepDTI to predict high-fidelity diffusion tensor imaging metrics using a 10-layer 3D CNN. Ye et al. designed MEDN <ref type="bibr" target="#b20">[21]</ref> and MEDN+ <ref type="bibr" target="#b21">[22]</ref> based on a dictionary-inspired framework to learn the mapping between undersampled dMRI data and high-quality microstructure indices. Zheng et al. <ref type="bibr" target="#b23">[24]</ref> proposed a three-stage microstructure estimation model that combines sparse encoding with a transformer. Chen et al. <ref type="bibr" target="#b3">[4]</ref> introduced the graph convolutional neural network (GCNN) to learn q-space information for microstructure estimation, which considers the angular information in q-space and achieves promising performance.</p><p>Recently, inspired by the fact that dMRI data live in a joint x-q space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>, a powerful hybrid model, called HGT <ref type="bibr" target="#b4">[5]</ref>, was proposed to jointly learn spatialangular information for accurate microstructure estimation. It achieves superior performance in comparison with various types of existing methods. However, a major limitation of HGT lies in the ignorance of the fact that the spatial domain of dMRI is a 3D space rather than a 2D one. A large number of studies have demonstrated that careful consideration of 3D space is the key to advancing the performance of different medical image analysis tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>To this end, we propose 3D-HGT, an advanced microstructure estimation model capable of making full use of 3D x-space information and q-space information jointly. Specifically, we design an efficient q-space learning module based on simple graph convolution (SGC) <ref type="bibr" target="#b19">[20]</ref>, which runs with less memory at high speed and is able to alleviate the large computational burden associated with 3D spatial information learning. We further propose a 3D x-space learning module based on a U-shape transformer, which is able to capture the long-range relationships in 3D spatial space for improved performance. Finally, we train two modules end-to-end for predicting microstructure index maps. Our 3D-HGT exploits 3D spatial information and angular information in an efficient manner for more accurate microstructure estimation. We perform extensive experiments on data from the human connectome project (HCP) <ref type="bibr" target="#b18">[19]</ref>. The experimental results demon-strate that our 3D-HGT outperforms cutting-edge methods, including HGT, both quantitatively and qualitatively.</p><p>Fig. <ref type="figure">1</ref>. Overview of 3D-HGT. (a) shows the overall architecture of the model, where the q-space learning module first learns the feature of dMRI data in q-space, and then the x-space learning module learns the 3D spatial information; (b) shows the architecture of the LSA/WSA component, which is a cascaded transformer group with two self-attention blocks; (c) shows the structure of the x-space learning module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">3D Hybrid Graph Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Overview</head><p>As shown in Fig. <ref type="figure">1</ref>, our 3D-HGT consists of two key modules: an efficient qspace learning module and a 3D x-space learning module. The q-space learning module is built with SGC, which can extract q-space features from voxel-wise dMRI data more efficiently. Motivated by the nnFormer <ref type="bibr" target="#b24">[25]</ref>, the x-space learning module mainly consists of a U-shaped network composed of local volume-based multi-head self-attention (LSA) and wide volume-based multi-head self-attention (WSA) components.</p><p>The input dMRI data X ∈ R L×W ×H×G has four dimensions, where L, W , H, and G denote the length, width, height, and number of gradient directions, respectively. In our model, the data is first reshaped into a two-dimensional tensor X ∈ R LW H×G and fed into the SGC network to learn the q-space features efficiently. The output X q is then reshaped to four dimensions Xq ∈ R L×W ×H×G and enters into the x-space learning module followed by convolutional layers. Finally, our 3D-HGT generates microstructure predictions Y ∈ R L×W ×H×M , where M denotes the number of microstructure indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient q-Space Learning Module</head><p>According to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>, the geometric structure of the q-space of dMRI data can be encoded as a graph G determined by a binary affinity matrix A = {a i,j }. If the angle θ i,j between two sampling points i and j in q-space is less than an angle threshold θ, then a i,j = 1; otherwise, a i,j = 0. Following such processing, GCNN can be used for q-space learning.</p><p>Different from HGT, 3D-HGT learns in q-space with SGC, which removes the non-linearity between the graph convolutional layers and collapses K graph convolutional layers into one. In this way, the complex computation of a multilayer network is reduced to a few matrix multiplications. Thus, we can use fast matrix multiplication to speed up computation, reduce network redundancy, and improve computational efficiency. Mathematically, the feature provided by our q-space learning module is as follows:</p><formula xml:id="formula_0">X q = ÂK XΘ,<label>(1)</label></formula><p>where K denotes the number of SGC layers, Â is the normalized version of A with self-loop added, and Θ is the product of the weight matrices learned by the q-space learning module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">3D x-Space Learning Module</head><p>The overall design of the x-space learning module is a U-shaped network composed of three parts: an encoder, a bottleneck, and a decoder. The three components are made up of cascaded self-attention transformer blocks. The encoder and decoder are similar in structure, with two self-attention layers. However, there are two key differences between them. Firstly, the former is composed of two local volume-based layers, while the latter has two wide volumebased layers. Secondly, before entering the network, data from the encoder passes through an embedding layer, while data from decoders pass through an extension layer for feature integration before outputting results. The bottleneck layer comprises two LSAs and one WSA, which allows skip attention to integrate shallow and deep attention between the encoder and decoder layers. Notably, we omit the pooling or un-pooling operations in the encoder-decoder architecture since pooling can lead to the loss of features, especially when dealing with small-sized 3D patches.</p><p>The Embedding Layer. The embedding layer divides the input Xq into highdimensional patches, which are then sent into the transformer block. Unlike the nnFormer, our embedding layer consists of two convolutional layers, a GELU <ref type="bibr" target="#b12">[13]</ref> layer and a normalization layer <ref type="bibr" target="#b0">[1]</ref>, where both the convolutional kernel and step size are set to one.</p><p>Local Volume-Based Multi-head Self-attention (LSA). Based on Swin Transformer <ref type="bibr" target="#b16">[17]</ref>, LSA is an offset window self-retaining module that defines the window as a local 3D volume block. The multi-head self-attention is computed in this local volume, which reduces the computational cost significantly.</p><p>Let Q, K, V ∈ R S L ×S W ×S H ×D for LSA be the query, key, and value matrices, and B ∈ R S L ×S W ×S H be the relative position matrix, where {S L , S W , S H } denotes the size of the local volume and D is the dimension of query/key. The self-attention is then computed in each 3D local volume as follows:</p><formula xml:id="formula_1">Attention(Q, K, V) = Softmax( QK T √ D + B)V.<label>(2)</label></formula><p>The LSA computational complexity can be expressed as follows:</p><formula xml:id="formula_2">Ω( LSA ) = 4N P C 2 + 2S L S W S H N P C, (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where N P is the size of a patch, and C is the length of the embedding sequence.</p><p>Wide Volume-Based Multi-head Self-attention (WSA). Although LSA is efficient, its receptive field is limited. Thus, by increasing the size of the window to {m × S L , m × S W , m × S H } with m denoting a constant (two by default), we have WSA, which improves the global context awareness ability of LSA. The computational complexity of WSA is as follows:</p><formula xml:id="formula_4">Ω( WSA ) = 4N P C 2 + 2m 3 S L S W S H N P C. (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>We extend the self-attention field of view in the bottleneck using three wide-field transformer blocks, where six WSA layers are used.</p><p>Skip Attention. This component effectively combines shallow and deep attention. A single-layer neural network decomposes the output X l at layer l of the encoder into a key matrix K l and a value matrix V l , while the output of X l at layer l of the decoder is used as a query matrix Q l . Mathematically, the self-attention is defined as:</p><formula xml:id="formula_6">Attention(Q l , K l , V l ) = Softmax( Q l K T l √ D l + B l )V l ,<label>(5)</label></formula><p>where B l is the relative position encoding matrix, and D l is the dimension of query/key.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>Experimental Settings. We use the mean square error as the loss function, Adam as the optimizer, and an initial learning rate of 9e-4. We set the number of epochs to 100, batch size to 2, and iterations to 10. The angular threshold θ for constructing graph is set at 45 • , and the number of graph convolutional layers (i.e., K) is set to 2. In the x-space learning module, the embedding dimension is set to 192, and the window sizes of the LSA and WSA layers are set to 4 and 8, respectively. The model was implemented using PyTorch 1.11 and PyTorch-Geometric 2.1.0, and trained on a server equipped with an RTX 3090 GPU.</p><p>Comparison Methods. We compare our 3D-HGT with various methods, including AMICO <ref type="bibr" target="#b7">[8]</ref>, MLP <ref type="bibr" target="#b11">[12]</ref>, GCNN <ref type="bibr" target="#b3">[4]</ref>, MEDN <ref type="bibr" target="#b20">[21]</ref>, MEDN+ <ref type="bibr" target="#b21">[22]</ref>, U-Net <ref type="bibr" target="#b9">[10]</ref>, U-Net++ <ref type="bibr" target="#b25">[26]</ref>, and HGT <ref type="bibr" target="#b4">[5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset and Evaluation Metrics</head><p>Dataset. Following <ref type="bibr" target="#b4">[5]</ref>, we randomly select 21 subjects from the HCP to construct our dataset. For each dMRI scan, q-space undersampling is performed on a single shell with b=1000 s/mm 2 to extract 30 gradients uniformly. The undersampled data is then normalized by dividing by the average of b 0 images. Finally, we extract patches from the normalized data with a dimension of 32×32×32×30.</p><p>The validation and test sets are also processed in the same way. In our experiments, the ratio between training, validation, and test sets is 10:1:10. We train our model to predict NODDI-derived indices, including intracellular volume fraction (ICVF), isotropic volume fraction (ISOVF), and orientation dispersion index (ODI). The gold standard microstructural indices are computed using the complete HCP data with 270 gradients using AMICO <ref type="bibr" target="#b7">[8]</ref>.</p><p>Evaluation Metrics. We evaluate the quality of predicted NODDI-derived indices with the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head><p>Table <ref type="table" target="#tab_0">1</ref> compares our 3D-HGT model with cutting-edge models in control experiments under the same conditions using various evaluation metrics. By taking into account the 3D x-space data from the dMRI data, our model is able to learn rich features across a broader range of spatial domains. 3D-HGT outperforms HGT in terms of PSNR and SSIM. The most notable improvements among all indices can be observed in the PSNR of the ICVF, which has increased from 21.96 to 23.60 dB, and the SSIM of the ICVF, which has increased from 0.814 to 0.837. Furthermore, compared with the deep learning models (i.e., MLP to HGT) in Table <ref type="table" target="#tab_0">1</ref>, 3D-HGT improves the PSNR of ALL by 16.3%, 15.5%, 15.2%, 14.0%, 13.3%, 12.0%, and 4.1%, respectively. Although HGT takes into account x-space and q-space, it only makes use of the 2D spatial features of the data. In contrast, 3D-HGT explicitly considers the 3D spatial domain and improves the PSNR and SSIM of ALL by 4.1% and 1.1%, respectively. Figure <ref type="figure" target="#fig_0">2</ref> depicts the visual comparison of 3D-HGT and other models' microstructure predictions. In particular, the close-up views, shown in the bottom row of Fig. <ref type="figure" target="#fig_0">2</ref>, indicate that our model provides the best result, which is much closer to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>To investigate the effectiveness of the proposed modules, we perform ablation experiments with different ablated versions. The ablation results are shown in Table <ref type="table" target="#tab_1">2</ref>. It should be noted that the ablated versions "(A)" and "(E)" correspond to HGT and the full version of 3D-HGT, respectively. Effectiveness of Efficient q-Space Learning Module. As shown in Table <ref type="table" target="#tab_1">2</ref>, "(C)" is the ablated version with only the 3D x-space learning module. It can be observed that "(E)" outperforms "(C)", demonstrating that our q-space learning module can effectively improve performance. Moreover, we perform additional experiments using the undersampled dMRI data with a higher angular resolution, i.e., 45 gradients. The results show that our q-space learning module provides a larger improvement when the dMRI data is with a higher angular resolution.</p><p>The q-space learning with SGC also shows advantages over TAGCN. Both "(B)" and "(E)" improve the performance and computational efficiency in comparison with their corresponding ablated versions equipped with TAGCN, i.e., "(A)" and "(D)". In particular, compared with "(D)", the training time cost of "(E)" is reduced by nearly 30 s for one epoch, verifying the high efficiency of our q-space learning module.</p><p>Effectiveness of 3D x-Space Learning Module. As shown in Table <ref type="table" target="#tab_1">2</ref>, two sets of comparisons, "(A)" vs. "(D)" and "(B)" vs. "(E)", indicate that modifying the x-space learning module to a 3D transformer improves performance. More specifically, compared with "(A)", "(D)" improves the PSNR of ICVF by 1.48 dB, ISOVF by 0.67 dB, ODI by 0.23 dB and ALL by 0.82 dB. Compared with "(B)", "(E)" improves the PSNR of ICVF by 1.63 dB, ISOVF by 0.77 dB, ODI by 0.10 dB and ALL by 0.82 dB. The improvement owes to the 3D x-space learning module, which is equipped with LSAs and WSAs, allowing the model to capture long-term dependencies with a large 3D receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed 3D-HGT, an improved microstructure estimation model that makes full use of 3D x-space information and q-space information. Our x-space learning is achieved with a 3D transformer architecture, allowing the model to thoroughly learn the long-term dependencies of features in the 3D spatial domain. To alleviate the large computational burden associated with 3D x-space learning, we further propose an efficient q-space learning module, which is built with a simplified graph learning architecture. Extensive experiments on the HCP demonstrate that, compared with HGT, 3D-HGT effectively improves the quality of microstructure estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparison of prediction results.</figDesc><graphic coords="7,65,52,54,29,328,48,178,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance evaluation of microstructre estimation using single-shell undersampled data with b=1000 s/mm 2 , 30 gradient directions in total. All: A combination of ICVF, ISOVF, and ODI.</figDesc><table><row><cell>Method</cell><cell></cell><cell>PSNR ↑</cell><cell>SSIM ↑</cell></row><row><cell></cell><cell cols="2">ICVF ISOVF ODI</cell><cell>All ICVF ISOVF ODI</cell><cell>All</cell></row><row><cell>AMICO [8]</cell><cell>8.09</cell><cell cols="2">14.90 12.03 10.80 0.020 0.273 0.380 0.210</cell></row><row><cell>MLP [12]</cell><cell cols="3">19.15 24.07 20.31 20.72 0.670 0.706 0.667 0.681</cell></row><row><cell>MEDN [21]</cell><cell cols="3">19.18 24.43 20.54 20.87 0.674 0.737 0.684 0.698</cell></row><row><cell>GCNN [4]</cell><cell cols="3">19.27 24.48 20.55 20.92 0.676 0.742 0.681 0.700</cell></row><row><cell cols="4">MEDN+ [22] 19.14 24.67 21.30 21.14 0.684 0.754 0.717 0.719</cell></row><row><cell>U-Net [10]</cell><cell cols="3">19.95 25.11 20.41 21.28 0.727 0.787 0.651 0.722</cell></row><row><cell cols="4">U-Net++ [26] 19.94 25.49 20.87 21.51 0.750 0.798 0.688 0.745</cell></row><row><cell>HGT [5]</cell><cell cols="3">21.96 27.13 22.12 23.16 0.814 0.850 0.757 0.807</cell></row><row><cell>3D-HGT</cell><cell cols="3">23.60 27.98 22.43 24.10 0.837 0.845 0.765 0.816</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study for 3D-HGT. "2D Trans." denotes the 2D x-space learning module of HGT. "3D Trans." denotes the 3D x-space learning module of 3D-HGT, as shown in Fig.1(c). The metric "Time" indicates the time cost (in seconds) for training the corresponding ablation version in one epoch.</figDesc><table><row><cell>Model q-Space Module</cell><cell>x-Space Module</cell><cell>PSNR ↑</cell><cell></cell><cell>Time ↓</cell></row><row><cell cols="3">TAGCN SGC 2D Trans. 3D Trans. ICVF ISOVF ODI</cell><cell>All</cell></row><row><cell>(A)</cell><cell></cell><cell cols="2">21.96 27.13 22.12 23.16</cell><cell>121</cell></row><row><cell>(B)</cell><cell></cell><cell cols="2">21.97 27.21 22.33 23.28</cell><cell>109</cell></row><row><cell>(C)</cell><cell></cell><cell cols="2">23.38 27.78 22.40 23.97</cell><cell>122</cell></row><row><cell>(D)</cell><cell></cell><cell cols="2">23.44 27.80 22.35 23.98</cell><cell>226</cell></row><row><cell>(E)</cell><cell></cell><cell cols="3">23.60 27.98 22.43 24.10 197</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>equally to this work. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> through Grants <rs type="grantNumber">62201465</rs> and <rs type="grantNumber">62171377</rs>, and the <rs type="funder">Natural Science Foundation of Heilongjiang Province</rs> through Grant <rs type="grantNumber">LH2021F046</rs>. P.-T. Yap was supported in part by the <rs type="funder">United States National Institutes of Health (NIH)</rs> through Grants <rs type="grantNumber">MH125479</rs> and <rs type="grantNumber">EB008374</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Ndk6evJ">
					<idno type="grant-number">62201465</idno>
				</org>
				<org type="funding" xml:id="_fhQ7NVU">
					<idno type="grant-number">62171377</idno>
				</org>
				<org type="funding" xml:id="_f6QbnZC">
					<idno type="grant-number">LH2021F046</idno>
				</org>
				<org type="funding" xml:id="_Pb2EaXh">
					<idno type="grant-number">MH125479</idno>
				</org>
				<org type="funding" xml:id="_rP3FuUy">
					<idno type="grant-number">EB008374</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Denoising of diffusion MRI data via graph framelet matching in x-q space</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2838" to="2848" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">XQ-SR: joint x-q space super-resolution with application to infant diffusion MRI</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating tissue microstructure with undersampled diffusion data via graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59728-3_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59728-3_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12267</biblScope>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid graph transformer for tissue microstructure estimation with undersampled diffusion MRI data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-6_11" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noise reduction in diffusion MRI using non-local self-similar information in joint x-q space</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="79" to="94" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">VoxResNet: deep voxelwise residual networks for brain segmentation from 3D MR images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="446" to="455" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accelerated microstructure imaging via convex optimization (AMICO) from diffusion MRI data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daducci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="32" to="44" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">U-net: deep learning for cell counting, detection, and morphometry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="70" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simultaneous NODDI and GFA parameter map generation from subsampled q-space imaging using deep learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Gibbons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnet. Resonan. Med</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2399" to="2411" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">q-Space deep learning: twelve-fold shorter and model-free diffusion MRI scans</title>
		<author>
			<persName><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1344" to="1351" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (GELUs)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multifold acceleration of diffusion MRI via deep learning reconstruction from slice-undersampled data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20351-1_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-20351-1_41" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2019</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C S</forename><surname>Chung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11492</biblScope>
			<biblScope unit="page" from="530" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusional kurtosis imaging: the quantification of non-gaussian water diffusion by means of magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Helpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnet. Resonan. Med</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1432" to="1440" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantitative mapping of the per-axon diffusion coefficients in brain white matter</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kaden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kruggel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnet. Resonan. Med</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1752" to="1763" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepDTI: high-fidelity six-direction diffusion tensor imaging using deep learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page">117017</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The WU-Minn human connectome project: an overview</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="62" to="79" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimation of tissue microstructure using a deep network inspired by a sparse reconstruction framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-59050-9_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-59050-9_37" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10265</biblScope>
			<biblScope unit="page" from="466" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tissue microstructure estimation using a deep network inspired by a dictionary-based framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="288" to="299" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NODDI: practical in vivo neurite orientation dispersion and density imaging of the human brain</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Wheeler-Kingshott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1000" to="1016" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A microstructure estimation transformer inspired by sparse representation for diffusion MRI</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">102788</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">nnFormer: volumetric medical image segmentation via a 3D transformer</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-5_1" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
