<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention</title>
				<funder ref="#_K5Vyh3X">
					<orgName type="full">Anhui Provincial Key R&amp;D Program</orgName>
				</funder>
				<funder ref="#_8nwbpXe">
					<orgName type="full">University Synergy Innovation Program of Anhui Province, China</orgName>
				</funder>
				<funder ref="#_HbhUewg">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hengxu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Intelligent Machines</orgName>
								<orgName type="institution" key="instit1">HFIPS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Physical Science and Information Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengpeng</forename><surname>Liang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer and Artificial Intelligence</orgName>
								<orgName type="institution">Zhengzhou University</orgName>
								<address>
									<settlement>Zhengzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Intelligent Machines</orgName>
								<orgName type="institution" key="instit1">HFIPS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Intelligent Machines</orgName>
								<orgName type="institution" key="instit1">HFIPS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Erkang</forename><surname>Cheng</surname></persName>
							<email>ekcheng@iim.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Intelligent Machines</orgName>
								<orgName type="institution" key="instit1">HFIPS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="493" to="502"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A1BBA9FC3A724BEBEE1851ECD9826761</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Circular Object Analysis</term>
					<term>Circular Queries</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Both CNN-based and Transformer-based object detection with bounding box representation have been extensively studied in computer vision and medical image analysis, but circular object detection in medical images is still underexplored. Inspired by the recent anchor free CNN-based circular object detection method (CircleNet) for ball-shape glomeruli detection in renal pathology, in this paper, we present Cir-cleFormer, a Transformer-based circular medical object detection with dynamic anchor circles. Specifically, queries with circle representation in Transformer decoder iteratively refine the circular object detection results, and a circle cross attention module is introduced to compute the similarity between circular queries and image features. A generalized circle IoU (gCIoU) is proposed to serve as a new regression loss of circular object detection as well. Moreover, our approach is easy to generalize to the segmentation task by adding a simple segmentation branch to Cir-cleFormer. We evaluate our method in circular nuclei detection and segmentation on the public MoNuSeg dataset, and the experimental results show that our method achieves promising performance compared with the state-of-the-art approaches. The effectiveness of each component is validated via ablation studies as well. Our code is released at: https:// github.com/zhanghx-iim-ahu/CircleFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nuclei detection is a highly challenging task and plays an important role in many biological applications such as cancer diagnosis and drug discovery. Rectangle object detection approaches that use CNN have made great progress in the    last decade <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">18]</ref>. These popular CNN models use boxes to represent objects that are not optimized for circular medical objects, such as detection of glomeruli in renal pathology. To address the problem, an anchor-free CNNbased circular object detection method CircleNet <ref type="bibr" target="#b15">[16]</ref> is proposed for glomeruli detection. Different from CenterNet <ref type="bibr" target="#b18">[18]</ref>, CircleNet estimates the radius rather than the box size for circular objects. But it also suffers poor detection accuracy for overlapping objects and requires additional post-processing steps to obtain the final detection results.</p><p>Recently, DETR <ref type="bibr" target="#b0">[1]</ref>, a Transformer-based object detection method reformulates object detection as a set-to-set prediction problem, and it removes both the hand-crafted anchors and the non-maximum suppression (NMS) postprocessing. Its variants <ref type="bibr">( [3,10,11,15,19]</ref>) demonstrate promising results compared with CNN-based methods and DETR by improving the design of queries for faster training convergence. Built upon Conditional-DETR, DAB-DETR <ref type="bibr" target="#b9">[10]</ref> introduces an analytic study of how query design affects rectangle object detection. Specifically, it models object query as 4D dynamic anchor boxes (x, y, w, h) and iteratively refine them by a sequence of Transformer decoders. However, recent studies on Transformer-based detection methods are designed for rectangle object detection in computer vision, which are not specifically designed for circular objects in medical images.</p><p>In this paper, we introduce CircleFormer, a Transformer-based circular object detection for medical image analysis. Inspired by DAB-DETR, we propose to use an anchor circle (x, y, r) as the query for circular object detection, where (x, y) is the center of the circle and r is the radius. We propose a novel circle cross attention module which enables us to apply circle center (x, y) to extract image features around a circle and make use of circle radius to modulate the cross attention map. In addition, a circle matching loss is adopted in the set-to-set prediction part to process circular predictions. In this way, our design of Circle-Former lends itself to circular object detection. We evaluate our CircleFormer on the public MoNuSeg dataset for nuclei detection in whole slide images. Experimental results show that our method outperforms both CNN-based methods for box detection and circular object detection. It also achieves superior results compared with recently Transformer-based box detection approaches. Meanwhile, we carry out ablation studies to demonstrate the effectiveness of each proposed component. To further study the generalization ability of our approach, we add a simple segmentation branch to CircleFormer following the recent query based instance segmentation models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">17]</ref> and verify its performance on MoNuSeg as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Our CircleFormer (Fig. <ref type="figure" target="#fig_0">1</ref>) consists of a CNN backbone, a Transformer encoder module, a Transformer decoder and a prediction head to generate circular object results. The detail of the Transformer decoder is illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Representing Query with Anchor Circle</head><p>Inspired by DAB-DETR, we represent queries in Transformer-based circular object detection with anchor circles. We denote C i = (x i , y i , r i ) as the i-th anchor, x i , y i , r i ∈ R. Its corresponding content part and positional part are Z i ∈ R D and P i ∈ R D , respectively. The positional query P i is calculated by:</p><formula xml:id="formula_0">P i = MLP(PE(C i )), PE(C i ) = PE(x i , y i , r i ) = Concat(PE(x i ), PE(y i ), PE(r i )),<label>(1)</label></formula><p>where positional encoding (PE) generates embeddings from floating point numbers, and the parameters of the MLP are shared among all layers.</p><p>In Transformer decoder, the self-attention and cross-attention are written as:</p><p>Self-Attn :</p><formula xml:id="formula_1">Q i = Z i + P i , K i = Z i + P i , V i = Z i (2)</formula><p>Cross-attn :</p><formula xml:id="formula_2">Q i = Concat(Z i , PE(x i , y i ) • MLP (csq) (Z i )) K x,y = Concat(F x,y , PE(x, y)), V x,y = F x.y ,<label>(3)</label></formula><p>where F x,y ∈ R D denote the image feature at position (x, y) and an MLP (csq) : R D → R D is used to obtain a scaled vector conditioned on content information for a query. By representing a circle query as (x, y, r), we can refine the circle query layer-by-layer in the Transformer decoder. Specifically, each Transformer decoder estimates relative circle information (Δx, Δy, Δr). In this way, the circle query representation is suitable for circular object detection and is able to accelerate the learning convergence via layer-by-layer refinement scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Circle Cross Attention</head><p>We propose circle-modulated attention and deformable circle cross attention to consider size information of circular object detection in cross attention module.</p><p>Circle-modulated Attention. The circle radius modulated positional attention map provides benefits to extract image features of objects with different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MA((x, y), (x ref , y</head><formula xml:id="formula_3">ref )) = (PE(x)•PE(x ref ) r i,ref r i +PE(y)•PE(y ref ) r i,ref r i )/ √ D,<label>(4)</label></formula><p>where r i is the radius of the circle anchor A i , and r i,ref is the reference radius calculated by</p><formula xml:id="formula_4">r i,ref = sigmoid(MLP(C i )). sigmoid is used to normalize the pre- diction r i,ref to the range [0, 1].</formula><p>Deformable Circle Cross Attention. We modify standard deformable attention to deformable circle cross attention by applying radius information as constraint. Given an input feature map F ∈ R C×H×W , let i index a query element with content feature Z i and a reference point P i , the deformable circle cross attention feature is calculated by:</p><formula xml:id="formula_5">CDA(Z i , P i , F ) = M m=1 W m K k=1 Attn mik Ẇ m F ((P ix + Δr mik ×r i,ref × cos Δθ mik , P iy + Δr mik × r i,ref × sin Δθ mik )) , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where m indexes the attention head, k indexes the sampled keys. M and K are the number of multi-heads and the total sampled key number. W m ∈ R D×d , W m ∈ R d×D are the learnable weights and d = D/M . Attn mik denotes attention weight of the k th sampling point in the m th attention head. Δr mik and Δθ mik are radius offset and angle offset, r i,ref is the reference radius. In circle deformable attention, we transform the offset in polar coordinates to Cartesian coordinates so that the reference point ends up in the circle anchor. Rather than initialize the reference points by uniformly sampling within the rectangle as does Deformable DETR, we explore two ways to initialize the reference points within a circle, random sampling (CDA-r) and uniform sampling (CDA-c) (As in Fig. <ref type="figure" target="#fig_1">3</ref>). Experiments show that CDA-c initialization of reference points outperforms others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Circle Regression</head><p>A circle is predicted from a decoder embedding as ĉi = sigmoid(FFN(f i ) + [A i ]), where f is the decoder embedding. ĉi = (x, ŷ, r) consists of the circle center and circle radius. sigmoid is used to normalize the prediction ĉ to the range [0, 1]. FFN aims to predict the unnormalized box, A i is a circle anchor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Circle Instance Segmentation</head><p>A mask is predicted from a decoder embedding by mi = FFN(FFN(f i ) + f i ), where f is the decoder embedding. mi ∈ R 28×28 is the predicted mask. We use dice and BCE as the segmentation loss:</p><formula xml:id="formula_7">L seg = λ dice L dice (m i , mi ) + λ bce L bce (m i , mi</formula><p>) between prediction mi and the groundtruth m i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Generalized Circle IoU</head><p>CircleNet extends intersection over union (IoU) of bounding boxes to circle IoU (cIoU) and shows that the cIOU is a valid overlap metric for detection of circular objects in medical images. To address the difficulty optimizing non-overlapping bounding boxes, generalized IoU (GIoU) <ref type="bibr" target="#b12">[13]</ref> is introduced as a loss for rectangle object detection tasks. We propose a generalized circle IoU (gCIoU) to compute the similarity between two circles: gCIoU</p><formula xml:id="formula_8">= CA∩CB CA∪CB -|CC -(CA∪CB )|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CC</head><p>, where C A and C B denotes two circles, and C C is the smallest circle containing these two circles. We show that gCIoU can bring consistent improvement on circular object detection. Figure <ref type="figure" target="#fig_2">4</ref> shows the different measurements between two rectangles and circles. Different from CircleNet that only uses cIoU in the evaluation, we incorporate gCIoU in the training step. Then, we define the circle loss as: L circle (c, ĉ) = λ gciou L gciou (c, ĉ) + λ c cĉ 1 , while L gciou is generalized circle IoU loss, • 1 is 1 loss, and λ gciou , λ c ∈ R are hyperparameters.</p><p>Circle Training Loss. Following DETR, i-th each element of the groundtruth set is y i = (l i , c i ), where l i is the target class label (which may be ∅) and c i = (x, y, r). We define the matching cost between the predictions and the groundtruth set as: <ref type="bibr" target="#b5">(6)</ref> where σ ∈ S N is a permutation of all prediction elements, ŷσ(i) = ( lσ(i) , ĉσ(i) ) is the prediction, λ focal ∈ R are hyperparameters, and L focal is focal loss <ref type="bibr" target="#b7">[8]</ref>.</p><formula xml:id="formula_9">L match (y i , ŷσ(i) ) = I {li =∅} λ focal L focal (l i , lσ(i) ) + I {li =∅} L circle (c i , ĉσ(i) ),</formula><p>Finally, the overall loss is:</p><formula xml:id="formula_10">L loss (y i , ŷσ(i) ) = λ focal L focal (l i , lσ(i) ) + I {li =∅} L circle (c i , ĉσ(i) ) + L seg , (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where σ(i) is the index of prediction ŷ corresponding to the i-th ground truth y after completing the match. m i is the ground truth obtained by RoI Align <ref type="bibr" target="#b4">[5]</ref> corresponding to mi .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation</head><p>MoNuSeg Dataset. MoNuSeg dataset is a public dataset from the 2018 Multi-Organ Nuclei Segmentation Challenge <ref type="bibr" target="#b5">[6]</ref>. It contains 30 training/validataion tissue images sampled from a separate whole slide image of H&amp;E stained tissue and 14 testing images of lung and brain tissue images. Following <ref type="bibr" target="#b15">[16]</ref>, we randomly sample 10 patches with size 512 ×512 from each image and create 200 training images, 100 validation images and 140 testing images.</p><p>Evaluation Metrics. We use AP for nuclei detection evaluation metrics as in CircleNet <ref type="bibr" target="#b15">[16]</ref>, and AP m for the instance segmentation evaluation metrics. S and M are used to measure the performance of small scale with area less than 32 2 and median scale with area between 32 2 and 96 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Two variants of our proposed method for nuclei detection, CircleFormer and CircleFormer-D are built with a circle cross attention module and a deformable circle cross attention module, respectively. CircleFormer-D-Joint (Ours) extends CircleFormer-D to include instance segmentation as additional output. All the models are with ResNet50 as backbone and the number of Transformer encoders and decoders is set to 6. The MLPs of the prediction heads share the same parameters. Since the maximum number of objects per image in the dataset is close to 1000, we set the number of queries to 1000. The parameter of focal loss for classification is set to α = 0.25, γ = 0.1. λ focal is set to 2.0 in the matching step and λ focal = 1.0 in the final circle loss. We use λ iou = 2.0, λ c = 5.0, λ dice = 8.0 and λ bce = 2.0 in the experiments. All the models are initialized with the COCO pre-trained model <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Main Results</head><p>In Table jointly outputs detection and segmentation results additionally boosts the detection results of CircleFormer-D. Experiments of joint nuclei detection and segmentation are listed in Table <ref type="table" target="#tab_1">2</ref>. Our method outperforms QueryInst <ref type="bibr" target="#b1">[2]</ref>, a CNN-based instance segmentation method and SOIT <ref type="bibr" target="#b17">[17]</ref>, an Transformer-based instance segmentation approach. We extend Transformer-based box detection method to provide additional segmentation output inside the detection region, denoted as Deformable-DETR-Joint. Our method with circular query representation largely improves both detection and segmentation results.</p><p>To summarize, our method with only detection head outperforms both CNNbased methods and Transformer based approaches in most evaluation metrics for circular nuclei detection task. Our CircleFormer-D-Joint provides superior results compared to CNN-based and Transformer-based instance segmentation methods. Also, our method with joint detection and segmentation outputs also improves the detection-only setting. We have provided additional visual analysis in the open source code repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Studies</head><p>We conduct ablation studies with CircleFormer on the nuclei detection task (Table <ref type="table" target="#tab_4">5</ref>).  Effects of the Proposed Components. For simplicity, we denote the two parts of Table <ref type="table" target="#tab_2">3</ref> as P1 and P2.</p><p>In CircleFormer, the proposed circle-Modulated attention (c-MA) improves the performance of box AP from 45.7% to 48.6% box AP (Row 1 and Row 2 in P1). We replaced circle IoU (CIoU) loss with generalized circle IoU (gCIoU) loss, the performance is further boosted by 2.2% (Row 2 and Row 3 in P1).</p><p>We obtain similar observations of CircleFormer-D. When using standard deformable attention (SDA), learning cIoU loss gives a 1.2% improvement on box AP compared to using box IoU (Row 1 and Row 2 in P2). Replacing CIoU with gCIoU, the performances of SDA (Row 2 and Row 5 in P2), CDA-r (Row 3 and Row 6 in P2) and CDA-c (Row 4 and Row 7 in P2) are boostd by 0.3% box AP, 0.7% box AP and 1.8% box AP, respectively. Results show that the proposed gCIoU is a favorable loss for circular object detection.</p><p>Two multi-head initialization methods, random sampling (CDA-r) and uniform sampling (CDA-c), achieve similar results (Row 3 and Row 4 in P2) and both surpass SDA by 0.3% box AP (Row 2 and Row 3 in P2). By using gCIoU, CDA-r and CDA-c initialization methods surpasses SDA 1.1% box AP (Row 5 and Row 6 in P2), and 1.8% box AP (Row 5 and Row 7 in P2), respectively.</p><p>Numbers of Multi-head and Reference Points. We discuss how the number of Multi-heads in the Decoder affects the CircleFormer-D-DETR model. We vary the number of heads for multi-head attention and the performance of the model is shown in the Table <ref type="table" target="#tab_3">4</ref>. We find that the performance increases gradually as the number of heads increases up to 8. However, the performance drops when the number of head is 16. We assume increasing the number of heads brings too many parameters and makes the model difficult to converge. Similarly, we study the impact of the number of reference points in the cross attention module. We find that 4 reference points give the best performance. Therefore, we choose to use 8 attention heads of decoder and use 4 reference points in the cross attention module through all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we introduce CircleFormer, a Transformer-based circular medical object detection method. It formulates object queries as anchor circles and refines them layer-by-layer in Transformer decoders. In addition, we also present a circle cross attention module to compute the key-to-image similarity which can not only pool image features at the circle center but also leverage scale information of a circle object. We also extend CircleFormer to achieve instance segmentation with circle detection results. To this end, our CircleFormer is specifically designed for circular object analysis with DETR scheme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Transformer-based circle detection and segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. initialization of crossattention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Different IoU evaluation metrics.</figDesc><graphic coords="2,202,17,86,63,181,27,188,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>1, for nuclei detection, we compare our CircleFormer with CNN-based box detection, CNN-based circle detection and Transformer-based box detection. DETR can not produce satisfied results due to its low convergence. CircleFormer imporoves box AP by 4.0% compared to DAB-DETR, and CircleFormer-D improves box AP by 13.4% and 3.3% compared to Deformable-DETR and DAB-Deformable-DETR. CircleFormer-D-Joint which Results of nuclei detection on Monuseg Dataset. Best and second-best results are colored red and blue, respectively. MS: Multi-Scale.</figDesc><table><row><cell>Compared to Faster-RCNN [12] with ResNet50 as backbone, CircleFormer and</cell></row><row><cell>CircleFormer-D significantly improve box AP by 8.1% and 11.3%, respectively.</cell></row><row><cell>CircleFormer and CircleFormer-D also surpass CircleNet [16] by 1.0% and 4.2%</cell></row><row><cell>box AP. In summary, our CircleFormer designed for circular object detection</cell></row><row><cell>achieves superior performance compared to both CNN-based box detection and</cell></row><row><cell>CNN-based circle detection approaches.</cell></row><row><cell>Our CircleFormer with detection head also yields better performance than</cell></row><row><cell>Transformer-based methods.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of nuclei joint detection and segmentation on Monuseg Dataset. All the methods are with ResNet50 as backbone.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Detection</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Segmentation</cell><cell></cell></row><row><cell>Methods</cell><cell cols="10">AP AP (50) AP (75) AP (S) AP (M ) AP m AP m (50) AP m (75) AP m (S) AP m (M )</cell></row><row><cell>QueryInst [2]</cell><cell>40.2</cell><cell>77.7</cell><cell>36.7</cell><cell>40.8</cell><cell>21.4</cell><cell>38.0</cell><cell>76.2</cell><cell>33.6</cell><cell>38.0</cell><cell>39.4</cell></row><row><cell>SOIT [17]</cell><cell>44.8</cell><cell>82.6</cell><cell>45.2</cell><cell>45.5</cell><cell>27.5</cell><cell>41.3</cell><cell>80.6</cell><cell>38.8</cell><cell>41.3</cell><cell>40.7</cell></row><row><cell cols="2">Deformable-DETR-Joint 45.7</cell><cell>86.7</cell><cell>43.6</cell><cell>46.2</cell><cell>28.2</cell><cell>43.5</cell><cell>84.8</cell><cell>40.5</cell><cell>43.5</cell><cell>42.3</cell></row><row><cell>CircleFormer-D-Joint</cell><cell cols="2">53.0 90.0</cell><cell>59.0</cell><cell>53.8</cell><cell>32.8</cell><cell>44.4</cell><cell>84.5</cell><cell>43.5</cell><cell>44.4</cell><cell>45.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of the ablation study analyzing the effects of proposed components in CircleFormer on Monuseg Dataset. wh-MA: wh-Modulated Attention; c-MA: circle-Modulated Attention; SDA: standard deformable attention; CDA-r: Circle Deformable Attention with random initialization; CDA-c: Circle Deformable Attention with cirle initialization. † denotes CircleFormer. ‡ denotes CircleFormer-D.</figDesc><table><row><cell>Method</cell><cell cols="9">IoU gIoU CIoU gCIoU wh-MA c-MA SDA CDA-r CDA-c AP ↑ AP (50) ↑ AP (75) ↑ AP (S) ↑ AP (M ) ↑ Cross Attention AP</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.7</cell><cell>88.9</cell><cell>41.9</cell><cell>46.3</cell><cell>34.8</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.6</cell><cell>88.3</cell><cell>48.8</cell><cell>49.6</cell><cell>30.7</cell></row><row><cell>†</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.7</cell><cell>88.8</cell><cell>50.9</cell><cell>51.1</cell><cell>35.4</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>49.6</cell><cell>89.5</cell><cell>51.5</cell><cell>50.1</cell><cell>31.9</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>50.8</cell><cell>88.2</cell><cell>54.4</cell><cell>51.7</cell><cell>26.6</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>51.1</cell><cell>87.9</cell><cell>55.8</cell><cell>52.5</cell><cell>23.2</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>51.1</cell><cell>86.8</cell><cell>56.6</cell><cell>52.7</cell><cell>30.1</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>51.1</cell><cell>87.6</cell><cell>55.6</cell><cell>52.7</cell><cell>29.1</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>51.8</cell><cell>88.4</cell><cell>57.2</cell><cell>53.1</cell><cell>30.2</cell></row><row><cell>‡</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>52.9</cell><cell>89.6</cell><cell>58.7</cell><cell>54.1</cell><cell>31.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation Study of number of Multi-Head.</figDesc><table><row><cell>1 50.3 86.9</cell><cell>54.4</cell><cell>51.6</cell><cell>30.3</cell></row><row><cell>2 50.4 87.5</cell><cell>53.9</cell><cell>51.6</cell><cell>33.3</cell></row><row><cell>4 51.3 88.6</cell><cell>54.9</cell><cell>52.2</cell><cell>31.1</cell></row><row><cell>8 52.9 89.6</cell><cell>58.7</cell><cell>54.1</cell><cell>31.7</cell></row><row><cell>16 50.4 88.2</cell><cell>53.6</cell><cell>51.6</cell><cell>28.3</cell></row></table><note><p># AP↑ AP (50) ↑ AP (75) ↑ AP (S) ↑ AP (M ) ↑</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation Study of number of reference points.</figDesc><table><row><cell>1 49.3 86.4</cell><cell>52.5</cell><cell>50.6</cell><cell>25.6</cell></row><row><cell>2 51.2 88.4</cell><cell>55.7</cell><cell>52.4</cell><cell>29.6</cell></row><row><cell>4 52.9 89.6</cell><cell>58.7</cell><cell>54.1</cell><cell>31.7</cell></row><row><cell>8 50.8 88.1</cell><cell>54.7</cell><cell>52.0</cell><cell>29.2</cell></row></table><note><p># AP↑ AP (50) ↑ AP (75) ↑ AP (S) ↑ AP (M ) ↑</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is supported in part by <rs type="funder">NSFC</rs> (<rs type="grantNumber">61973294</rs>), <rs type="funder">Anhui Provincial Key R&amp;D Program</rs> (<rs type="grantNumber">2022i01020020</rs>), and the <rs type="funder">University Synergy Innovation Program of Anhui Province, China</rs> (<rs type="grantNumber">GXXT-2021-030</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HbhUewg">
					<idno type="grant-number">61973294</idno>
				</org>
				<org type="funding" xml:id="_K5Vyh3X">
					<idno type="grant-number">2022i01020020</idno>
				</org>
				<org type="funding" xml:id="_8nwbpXe">
					<idno type="grant-number">GXXT-2021-030</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-813" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Instances as queries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6910" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3621" to="3630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multi-organ nucleus segmentation challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1380" to="1391" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CornerNet: detecting objects as paired keypoints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-148" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DAB-DETR: dynamic anchor boxes are better queries for DETR</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=oMI9PjOb9Jl" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3651" to="3660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Anchor detr: query design for transformerbased detector</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2567" to="2575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CircleNet: anchor-free glomerulus detection with circle representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-14" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Soit: segmenting objects with instance-aware transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3188" to="3196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
