<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wasserstein Distance-Preserving Vector Space of Persistent Homology</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tananun</forename><surname>Songdechakraiwut</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><forename type="middle">M</forename><surname>Krause</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">I</forename><surname>Banks</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kirill</forename><forename type="middle">V</forename><surname>Nourski</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Barry</forename><forename type="middle">D</forename><surname>Van Veen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wasserstein Distance-Preserving Vector Space of Persistent Homology</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="277" to="286"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6888E3AA2346E0BBE5EB18512EF5961B</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_27</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Persistent homology</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Analysis of large and dense networks based on topology is very difficult due to the computational challenges of extracting meaningful topological features from networks. In this paper, we present a computationally tractable approach to topological data analysis of large and dense networks. The approach utilizes principled theory from persistent homology and optimal transport to define a novel vector space representation for topological features. The feature vectors are based on persistence diagrams of connected components and cycles and are computed very efficiently. The associated vector space preserves the Wasserstein distance between persistence diagrams and fully leverages the Wasserstein stability properties. This vector space representation enables the application of a rich collection of vector-based models from statistics and machine learning to topological analyses. The effectiveness of the proposed representation is demonstrated using support vector machines to classify measured functional brain networks. Code for the topological vector space is available at https://github.com/topolearn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Networks are ubiquitous representations for describing complex, highly interconnected systems that capture intricate patterns of relationships between nodes. <ref type="bibr" target="#b2">[3]</ref>. Finding meaningful, computationally tractable characterizations of network structure is very difficult, especially for large and dense networks with node degrees ranging over multiple orders of magnitude <ref type="bibr" target="#b4">[5]</ref>.</p><p>Persistent homology <ref type="bibr" target="#b9">[10]</ref> is an emerging tool for understanding, characterizing and quantifying the topology of complex networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. Connected components and cycles are the most dominant and fundamental topological features of real networks. For example, many networks naturally organize into modules or connected components <ref type="bibr" target="#b4">[5]</ref>. Similarly, cycle structure is ubiquitous and is often interpreted in terms of information propagation, redundancy and feedback loops <ref type="bibr" target="#b13">[14]</ref>. Topological features are represented in persistent homology using descriptors called persistence diagrams <ref type="bibr" target="#b9">[10]</ref>. Effective use of such topological descriptors in machine learning requires a notion of proximity. Wasserstein distance is often used to quantify the distance between persistence diagrams, motivated by its central stability properties <ref type="bibr" target="#b17">[18]</ref>. However, the integration of persistence diagrams and Wasserstein distance with standard learning methods from statistics and machine learning has been a challenging open problem due to the differences between Wasserstein distance and standard Euclidean-based metrics <ref type="bibr" target="#b7">[8]</ref>.</p><p>Approaches that embed persistence diagrams into vector spaces <ref type="bibr" target="#b0">[1]</ref> or Hilbert spaces <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> have recently been proposed to address this challenge. None of the embedding methods proposed thus far preserve Wasserstein distance in the original space of persistence diagrams <ref type="bibr" target="#b5">[6]</ref>. Thus, these approaches do not inherit the stability properties of Wasserstein distance.</p><p>Recently, it was shown that persistence diagrams are inherently 1dimensional if the topological features of networks are limited to connected components and cycles, and that the Wasserstein distance between these diagrams has a closed form expression <ref type="bibr" target="#b18">[19]</ref>. Consequently, the work in <ref type="bibr" target="#b19">[20]</ref> provides a computationally tractable, topological clustering approach for complex networks. However, significant limitations of the result in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> are that it is unclear how this approach can be incorporated with standard Euclidean-based learning methods from statistics and machine learning, and that the approach is limited to evaluating networks with the identical number of nodes. There are many opportunities for applications of topological analysis of networks of different size, such as studies of the human brain when different subjects are sampled at different resolutions.</p><p>In this work, we present a novel topological vector space (TopVS) that embeds 1-dimensional persistence diagrams representing connected components and cycles for networks of different sizes. Thus, TopVS enables topological machine learning with networks of different sizes and greatly expands the applicability of previous work. Importantly, TopVS preserves the Wasserstein distance in the original space of persistence diagrams. Preservation of the Wasserstein distance ensures the theoretical stability properties of persistence diagrams carry over to the proposed embedding. In addition to the robustness benefits, TopVS also enables the application of a wide variety of Euclidean metric-based learning methods to topological data analysis. Particularly, the utility of TopVS is demonstrated in topology-based classification problems using support vector machines. TopVS is illustrated by classifying measured functional brain networks based on data obtained from subjects with different numbers of electrodes. The results show that TopVS performs very well compared to other competing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Wasserstein Distance-Preserving Vector Space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">One-Dimensional Persistence Diagrams</head><p>Define a network as an undirected weighted graph G = (V, w) with a set of nodes V and a weighted adjacency matrix w = (w ij ). Define a binary graph G with the identical node set V by thresholding the edge weights so that an edge between nodes i and j exists if w ij &gt; . The binary graph is viewed as a 1-skeleton <ref type="bibr" target="#b14">[15]</ref>. As increases, more and more edges are removed from the network G. Thus, we have a graph filtration:</p><formula xml:id="formula_0">G 0 ⊇ G 1 ⊇ • • • ⊇ G k , where 0 ≤ 1 ≤ • • • ≤ k are called filtration values.</formula><p>Persistent homology keeps track of the birth and death of topological features over filtration values . A topological feature that is born at a filtration b i and persists up to a filtration d i , is represented by a point (b i , d i ) in a 2D plane. A set of all the points {(b i , d i )} is called persistence diagram <ref type="bibr" target="#b9">[10]</ref>. In the 1-skeleton, the only non-trivial topological features are connected components and cycles <ref type="bibr" target="#b21">[22]</ref>. As increases, the number of connected components β 0 (G ) and cycles β 1 (G ) are monotonically increasing and decreasing, respectively <ref type="bibr" target="#b18">[19]</ref>. Thus, the representation of the connected components and cycles can be simplified to a collection of sorted birth values</p><formula xml:id="formula_1">B(G) = {b i } |V |-1 i=1 and a collection of sorted death values D(G) = {d i } 1+|V |(|V |-3)/2 i=1</formula><p>, respectively <ref type="bibr" target="#b18">[19]</ref>. B(G) comprises edge weights in the maximum spanning tree (MST) of G. Once B(G) is identified, D(G) is given as the remaining edge weights that are not in the MST. Thus B(G) and D(G) are computed very efficiently in O(n log n) operations with n number of edges in networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Closed-Form Wasserstein Distance for Different-Size Networks</head><p>The Wasserstein distance between the 1-dimensional persistence diagrams can be obtained using a closed-form solution. Let G 1 and G 2 be two given networks possibly with different node sizes, i.e., their birth and death sets may differ in size. Their underlying empirical distributions on the persistence diagrams for connected components are defined in the form of Dirac masses <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_2">f G1,B (x) := 1 |B(G1)| b∈B(G1) δ(x-b) and f G2,B (x) := 1 |B(G2)| b∈B(G2) δ(x-b),</formula><p>where δ(xb) is a Dirac delta centered at the point b. Then the empirical distribution functions are the integration of f G1,B and f G2,B as</p><formula xml:id="formula_3">F G1,B (x) = 1 |B(G1)| b∈B(G1) 1 b≤x and F G2,B (x) = 1 |B(G2)| b∈B(G2) 1 b≤x , where 1 b≤x is an indicator function taking the value 1 if b ≤ x, and 0 otherwise. A pseudoinverse of F G1,B is defined as F -1 G1,B (z) = inf{b ∈ R | F G1,B (b) ≥ z}, i.e., F -1 G1,B (z) is the smallest b for which F G1,B (b) ≥ z. Similarly, we define a pseudoinverse of F G2,B as F -1 G2,B (z) = inf{b ∈ R | F G2,B (b) ≥ z}.</formula><p>Then the empirical Wasserstein distance for connected components has a closed-form solution in terms of these pseudoinverses as</p><formula xml:id="formula_4">W p,B (G 1 , G 2 ) = 1 0 |F -1 G1,B (z) -F -1 G2,B (z)| p dz 1/p . (<label>1</label></formula><formula xml:id="formula_5">)</formula><p>Similarly, the Wasserstein distance for cycles W p,D (G 1 , G 2 ) is defined in terms of empirical distributions for death sets D(G 1 ) and D(G 2 ). The empirical Wasserstein distances W p,B and W p,D are approximated by computing the Lebesgue integration in (1) numerically as follows.</p><formula xml:id="formula_6">Let B(G 1 ) = {F -1 G1,B (1/m), F -1 G1,B (2/m), ..., F -1 G1,B (m/m)} and D(G 1 ) = {F -1 G1,D (1/n), ..., F -1</formula><p>G1,D (n/n)} be pseudoinverses for network G 1 sampled with partitions of equal intervals. Let B(G 2 ) and D(G 2 ) be sampled pseudoinverses for network G 2 with the same partitions of m and n, respectively. Then the approximated Wasserstein distances are given by</p><formula xml:id="formula_7">W p,B (G 1 , G 2 ) = 1 m p m k=1 F -1 G1,B (k/m) -F -1 G2,B (k/m) p 1/p , (<label>2</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">W p,D (G 1 , G 2 ) = 1 n p n k=1 F -1 G1,D (k/n) -F -1 G2,D (k/n) p 1/p . (<label>3</label></formula><formula xml:id="formula_10">)</formula><p>For a special case when networks G 1 and G 2 have the same number of nodes, i.e., |B(G</p><formula xml:id="formula_11">1 )| = |B(G 2 )| and |D(G 1 )| = |D(G 2 )|,</formula><p>then exact computation of the Wasserstein distance is achieved using those birth and death sets, and setting m to the cardinality of the birth sets and n to that of the death sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Vector Representation of Persistence Diagrams</head><p>A collection of 1-dimensional persistence diagrams together with the Wasserstein distance is a metric space. 1-dimensional persistence diagrams can be embedded into a vector space that preserves the Wasserstein metric on the original space of persistence diagrams as follows. Let G 1 , G 2 , ..., G N be N observed networks possibly with different node sizes. Let F -1  Gi,B be a pseudoinverse of network G i . The vector representation of a persistence diagram for connected components in network G i is defined as a vector of the pseudoinverse sampled at 1/m, 2/m, ..., m/m, i.e., v B,i</p><formula xml:id="formula_12">:= F -1 Gi,B (1/m), F -1 Gi,B (2/m), ..., F -1 Gi,B (m/m) . A collection of these vectors M B = {v B,i } N i=1 with the p-norm || • || p induces the p-norm metric d p,B given by d p,B (v B,i , v B,j ) = ||v B,i -v B,j || p = m W p,B .</formula><p>Thus, for p = 1 the proposed vector space describes Manhattan distance, p = 2 Euclidean distance, and p → ∞ the maximum metric, which in turn correspond to the earth mover's distance (W 1 ), 2-Wasserstein distance (W 2 ), and the bottleneck distance (W ∞ ), respectively, in the original space of persistence diagrams. Similarly, we can define a vector space of persistence diagrams for cycles M D = {v D,i } N i=1 with the p-norm metric d p,D . The normed vector space (M B , d p,B ) describes topological space of connected components in networks, while (M D , d p,D ) describes topological space of cycles in networks.</p><p>The topology of a network viewed as a 1-skeleton is completely characterized by connected components and cycles. Thus, we can fully describe the network topology using both M B and M D as follows. Let </p><formula xml:id="formula_13">M B × M D = {(v B,i , v D,i ) | v B,i ∈ M B , v D,i ∈ M D }</formula><formula xml:id="formula_14">d p,× (v B,i , v D,i ), (v B,j , v D,j ) = [d p,B (v B,i , v B,j )] p + [d p,D (v D,i , v D,j )] p 1/p = [m W p,B ] p + [n W p,D ] p 1/p ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_15">(v B,i , v D,i ), (v B,j , v D,j ) ∈ M B × M D , m = V -1 and n = 1 + V(V-3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>. Thus, d p,× is a weighted combination of p-Wasserstein distances, and is simply the p-norm metric between vectors constructed by concatenating v B,i and v D,i . The normed vector space (M B × M D , d p,× ) is termed topological vector space (TopVS). Note the form of d p,× given in (4) results in an unnormalized mass after multiplying m and n by their reciprocals given in ( <ref type="formula" target="#formula_7">2</ref>) and ( <ref type="formula" target="#formula_9">3</ref>). This unnormalized variant of Wasserstein distance is widely used in both theory <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> and application <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> of persistent homology. A direct consequence of the equality given in ( <ref type="formula" target="#formula_14">4</ref>) is that the mean of persistence diagrams under the approximated Wasserstein distance is equivalent to the sample mean vector in TopVS. In addition, the proposed vector representation is highly interpretable because persistence diagrams can be easily reconstructed from vectors by separating sorted births and deaths.</p><p>For a special case in which networks G 1 , G 2 , ..., G N have the same number of nodes, the vectors v B,i and v D,i are simply the original birth set B(G i ) and death set D(G i ), respectively, and the p-norm metric d p,× is expressed in terms of exact Wasserstein distances as</p><formula xml:id="formula_16">d p,× = ([mW p,B ] p + [nW p,D ] p ) 1/p .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Application to Functional Brain Networks</head><p>Dataset. We evaluate our method using functional brain networks from the anesthesia study reported by <ref type="bibr" target="#b1">[2]</ref>. The brain networks are based on alpha band (8-12 Hz) weighted phase lag index applied to 10-second segments of resting state intracranial electroencephalography recordings. These recordings were made from eleven neurosurgical patients during administration of increasing doses of the general anesthetic propofol just prior to surgery. Each segment is labeled as one of the three arousal states: pre-drug wake (W), sedated but responsive to command (S), or unresponsive (U). The number of brain networks belonging to each subject varies from 71 to 119, resulting in the total of 977 networks from all the subjects. The network size varies from 89 to 199 nodes across subjects.</p><p>Classification Performance Evaluation. We are interested in whether candidate methods 1) can differentiate arousal states within individual subjects, and 2) generalize their learned knowledge to unknown subjects afterwards. As a result, we consider two different nested cross validation (CV) tasks as follows.</p><p>1. For the first task, we classify a collection of brain networks belonging to each subject separately. Specifically, we apply a nested CV comprising an outer loop of stratified 2-fold CV and an inner loop of stratified 3-fold CV. Since we may get a different split of data folds each time, we perform the nested CV for 100 trials and report an average accuracy score and standard deviation for each subject. We also average these individual accuracy scores across subjects (11 × 100 scores) to obtain an overall accuracy. 2. For the second task, we use a different nested CV comprising both outer and inner loops with a leave-one-subject-out scheme. That is, a classifier is trained using all but one test subject. The inner loop is used to determine optimal hyperparameters, while the outer loop is used to assess generalization capacity of the candidate methods to unknown subjects in the population.</p><p>Method Comparison. Brain networks are used to compare the classification performance of the proposed TopVS relative to that of five state-of-the-art kernel methods and two well-established graph neural network methods. Three of these kernel methods are based on conventional 2-dimensional persistence diagrams for connected components and cycles: the persistence image (PI) vectorization <ref type="bibr" target="#b0">[1]</ref>, the sliced Wasserstein kernel (SWK) <ref type="bibr" target="#b6">[7]</ref> and the persistence weighted Gaussian kernel (PWGK) <ref type="bibr" target="#b12">[13]</ref>. The other two kernel methods are based on graph kernels: the propagation kernel (Prop) <ref type="bibr" target="#b15">[16]</ref> and the GraphHopper kernel (GHK) <ref type="bibr" target="#b10">[11]</ref>. The PI method embeds persistence diagrams into a vector space in which classification is performed using linear support vector machines (SVMs). The non-linear SWK, PWGK, Prop and GHK methods are combined with SVMs to perform classification. While nearly any classifier may be used with TopVS, here we illustrate results using the SVM with the linear kernel, which maximizes Wasserstein distance-based margin. When the TopVS method is applied to different-size networks, we upsample birth and death sets of smaller networks to match that of the largest network in size. Hyperparameters are tuned using grid search. SVMs have a regularization parameter C = {0.01, 1, 100}. Thus, a grid search trains TopVS and PI methods with each C ∈ C. The SWK and WGK methods have a bandwidth parameter Σ = {0.1, 1, 10}, and thus grid search trains both methods with each pair (C, σ) ∈ C × Σ. The Prop method has a maximum number of propagation iterations T max = {1, 5, 10}, and thus is trained with each pair (C, t max ) ∈ C × T max . GHK method uses the RBF kernel with a parameter Γ = {0.1, 1, 10} between node attributes, and thus is trained with each pair (C, γ) ∈ C × Γ .</p><p>In addition, we also evaluate two well-established graph neural network methods including graph convolutional networks (GCN) <ref type="bibr" target="#b11">[12]</ref> and graph isomorphism network (GIN) <ref type="bibr" target="#b24">[25]</ref>. GCN and GIN are based on configurations and choices of hyperparameter values used in <ref type="bibr" target="#b24">[25]</ref> as follows. Five graph neural network layers are applied, and the Adam optimizer with initial learning rate and weight decay of 0.01 are employed. We tune the following hyperparameters: the number of hidden units in {16, 32}, the batch size in {32, 128} and the dropout ratio in {0, 0.5}. The number of epochs is set to 100 to train both methods.</p><p>Results. Results for the first task are summarized in Fig. <ref type="figure" target="#fig_1">1</ref>, in which classification accuracy for individual subjects is shown. There is variability in individual subject performance because a different subject's network has a different number of electrodes, different electrode locations and different effective signal to noise ratio. So we expect these subjects to exhibit a diverse set of topological features across spatial resolutions. In most subjects all methods perform relatively well. The consistently poorer performance of PI, Prop and GIN is evident in the lower overall performance. On the other hand, our TopVS method is consistently among the best performing classifiers, resulting in the higher overall performance. For classification accuracy across subjects from the second task, we have 0.65±0.21 for TopVS, 0.58±0.22 for PI, 0.57±0.20 for SWK, 0.60±0.21 for WGK, 0.36 ± 0.12 for Prop, 0.43 ± 0.14 for GHK, 0.53 ± 0.20 for GCN and 0.48 ± 0.19 for GIN. TopVS is also among the best methods for classifying across subjects, while the performance of all the graph neural networks and graph kernels is significantly weaker. These results suggest that the use of computationally demanding and complex classification methods, such as GCN and GIN, does not result in significant increase in generalizability when classifying brain networks.</p><p>In addition, we compute confusion matrices to gain insights into the acrosssubject predictions for the second task, as displayed in Fig. <ref type="figure" target="#fig_0">2</ref>. The persistent homology based methods, including TopVS, PI, SWK and WGK, are generally effective for separating unresponsive (U) from the other two states, and the majority of classification errors are associated with the differentiation between wake (W) and sedated (S) states. Prior work <ref type="bibr" target="#b1">[2]</ref> demonstrated that wake and sedated brains are expected to have a great deal of similarity in comparison to the less similar unresponsive brains. However, the work in <ref type="bibr" target="#b1">[2]</ref> performed the analysis on each individual subject separately while the results presented here are based on the analysis across subjects. Thus, not only the results here are consistent with the previous work <ref type="bibr" target="#b1">[2]</ref> but also suggest that such biological expectation carries over to brains across subjects and that topology based methods can potentially derive biomarkers of changes in arousal states in the population, which underlie transitions into and out of consciousness, informing our understanding of the neural correlates of consciousness in clinical settings. TopVS shows clear advantages over all other topological baseline methods for differentiating wake and sedated states, suggesting that the proposed vector representation is an effective choice for representing subtle topological structure in brain networks. Runtime Experiment. The kernel candidate methods are evaluated for a runtime experiment based on Intel Core i7 CPU with 16 GB of RAM. Figure <ref type="figure">3</ref> displays the runtime vs input size plot. The result clearly shows that all three persistent homology based kernels (PI, SWK and WGK) are limited to dense networks with a few hundred nodes, representing the current scaling limit of persistent homology embedding methods. On the other hand, TopVS is able to compute a kernel between 2000-node networks each with approx. two million edges in about one second. The computational practicality of TopVS extends its applicability to the large-scale analyses of brain networks that cannot be analyzed using prior methods based on conventional 2-dimensional persistence diagrams. Note that the time complexity of Prop is linear while TopVS has the slightly higher complexity as linearithmic. While Prop is the most efficient among all the methods, it has the lowest average accuracy when classifying the brain network data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Potential Impact and</head><p>Limitation. An open problem in neuroscience is identifying an algorithm that reliably extracts a patient's level of consciousness from passively recorded brain signals (i.e., biomarkers) and is robust to inter-patient variability, including where the signals are recorded in the brain. Conveniently, the anesthesia dataset is labeled according to consciousness state, and electrode placement (node location) was dictated solely by clinical considerations and thus varied across patients. Importantly, the relatively robust performance across patients suggests there are reliable topological signatures of consciousness captured by TopVS. The distinction between Wake and Sedated states involves relatively nuanced differences in connectivity, yet TopVS exploits the subtle differences in topology that differentiate these states better than the com-Fig. <ref type="figure">3</ref>. Runtime experiment. We measured the runtime as the average amount of time each algorithm takes to compute its kernel between two complete graphs starting from edge weights as a given input. The runtime is plotted with respect to network size in terms of both the number of nodes and edges.</p><p>peting methods. Our results suggest that the neural correlates of consciousness can be captured in measurements of brain network topology, a longstanding problem of great significance. Additionally, TopVS is a principled framework that connects persistent homology theory with practical applications. Our versatile vector representation can be used with various vector-based statistical and machine learning models, expanding the potential for analyzing extensive and intricate networks beyond the scope of this paper. While TopVS is limited to representing connected components and cycles, assessment of higher-order topological features beyond cycles is of limited value due to their relative rarity and interpretive challenges, and consequent minimal discriminitive power <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 ,</head><label>2</label><figDesc>be the Cartesian product between M B and M D so the vectors in M B × M D are the concatenations of v B,i and v D,i . For this product space to represent meaningful topology of network G i , the vectors v B,i and v D,i must be a network decomposition, as discussed in Sect. 2.1. Thus v B,i and v D,i are constructed by sampling their psudoinverses with m = V -1 and n = 1 + V(V-3) respectively, where V is a free parameter indicating a reference network size. The metrics d p,B and d p,D can be put together to form a p-product metric d p,× on M B × M D as<ref type="bibr" target="#b8">[9]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Accuracy classifying brain networks within individual subjects. The last column displays the average accuracy obtained across all subjects. The center markers and bars depict the means and standard deviations obtained over 100 different trials.</figDesc><graphic coords="7,55,98,54,50,340,18,101,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Confusion matrices illustrating method performance for classifying across subjects. The numbers represent the fraction of brain networks in the test subjects being predicted as one of the three states: wake (W), sedated (S), and unresponsive (U). The confusion matrices are normalized with the entries in each row summing to 1.</figDesc><graphic coords="8,83,31,54,56,257,89,171,16" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Persistence images: a stable vector representation of persistent homology</title>
		<author>
			<persName><forename type="first">H</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cortical functional connectivity indexes arousal state during sleep and anesthesia</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Banks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="page">116627</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The architecture of complex weighted networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pastor-Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vespignani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3747" to="3752" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The persistence of large scale structures. Part I. Primordial non-Gaussianity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Biagetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cosmol. Astropart. Phys</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Complex brain networks: graph theoretical analysis of structural and functional systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="198" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the metric distortion of embedding persistence diagrams into separable Hilbert spaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carrière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Computational Geometry</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sliced Wasserstein kernel for persistence diagrams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carriere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oudot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lipschitz functions have LP-stable persistence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mileyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="127" to="139" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Deza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deza</surname></persName>
		</author>
		<title level="m">Encyclopedia of Distances</title>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computational Topology: An Introduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Mathematical Society</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Persistence weighted Gaussian kernel for topological data analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kusano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hiraoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of feedback loops and robustness in network evolution based on Boolean models</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Elements of Algebraic Topology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Munkres</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CRC Press</publisher>
			<pubPlace>Boca Raton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="245" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cliques and cavities in the human connectome</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Sizemore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Vettel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Betzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bassett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Neurosci</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="115" to="145" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Skraba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16824</idno>
		<title level="m">Wasserstein stability for persistence diagrams</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Topological learning for brain networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Songdechakraiwut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="403" to="433" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast topological clustering with Wasserstein distance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Songdechakraiwut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Nourski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D V</forename><surname>Veen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Topological learning and its application to multimodal brain network integration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Songdechakraiwut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-316" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="166" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Homology groups of graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sunada</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-4-431-54177-6_4</idno>
		<ptr target="https://doi.org/10.1007/978-4-431-54177-64" />
	</analytic>
	<monogr>
		<title level="s">Topological Crystallography. Surveys and Tutorials in the Applied Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="37" to="51" />
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>Tokyo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fréchet means for distributions of persistence diagrams</title>
		<author>
			<persName><forename type="first">K</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mileyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Comput. Geom</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="70" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Persistent homology analysis of protein structure, flexibility, and folding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Numer. Methods Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="814" to="844" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
