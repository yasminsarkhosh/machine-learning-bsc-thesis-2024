<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image</title>
				<funder>
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_vEdBR6z">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder>
					<orgName type="full">Bio &amp; Medical Technology Development Program of the National Research Foundation</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder>
					<orgName type="full">MSIT (Ministry of Science, ICT)</orgName>
				</funder>
				<funder ref="#_Dcjn2SK">
					<orgName type="full">Korea</orgName>
				</funder>
				<funder ref="#_hfBAJGH">
					<orgName type="full">Korean government (MSIT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Go-Eun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dankook University</orgName>
								<address>
									<settlement>Yongin</settlement>
									<region>Gyeonggi-do</region>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seon</forename><forename type="middle">Ho</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jungchan</forename><surname>Cho</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Gachon University</orgName>
								<address>
									<settlement>Seongnam</settlement>
									<region>Gyeonggi-do</region>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sang</forename><forename type="middle">Tae</forename><surname>Choi</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Sang-Il</forename><forename type="middle">Tae</forename><surname>Choi</surname></persName>
							<email>choisi@dankook.ac.kr</email>
							<affiliation key="aff3">
								<orgName type="institution">Chung-Ang University College of Medicine</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Dankook University</orgName>
								<address>
									<settlement>Yongin</settlement>
									<region>Gyeonggi-do</region>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="537" to="546"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">D811D9C94F80E019C5282A4237EDA74A</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_52</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Segmentation</term>
					<term>Multi Modal Learning</term>
					<term>Cross Position Attention</term>
					<term>Text-Guided Attention</term>
					<term>Medical Image</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel text-guided cross-position attention module which aims at applying a multi-modality of text and image to position attention in medical image segmentation. To match the dimension of the text feature to that of the image feature map, we multiply learnable parameters by text features and combine the multi-modal semantics via cross-attention. It allows a model to learn the dependency between various characteristics of text and image. Our proposed model demonstrates superior performance compared to other medical models using image-only data or image-text data. Furthermore, we utilize our module as a region of interest (RoI) generator to classify the inflammation of the sacroiliac joints. The RoIs obtained from the model contribute to improve the performance of classification models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advances in deep learning have been witnessed in many research areas over the past decade. In medical field, automatic analysis of medical image data has actively been studied. In particular, segmentation which identify region of interest (RoI) in an automatic way is an essential medical imaging process. Thus, deep learning-based segmentation has been utilized in various medical domains such as brain, breast cancers, and colon polyps. Among the popular architectures, variants of U-Net have been widely adopted due to their effective encoderdecoder structure, proficient at capturing the characteristics of cells in images. Recently, it has been demonstrated that the attention modules <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref> enable deep learning networks to better extract robust features, which can be applied in medical image segmentation to learn subtle medical features and achieve higher performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>However, as image-only training trains a model with pixels that constitute an image, there is a limit in extracting fine-grained information about a target object even if transfer learning is applied through a pre-trained model. Recently, to overcome this limitation, multi-modality studies have been conducted, aiming to enhance the expressive power of both text and image features. For instance, CLIP <ref type="bibr" target="#b11">[12]</ref> used contrastive learning based on image-text pairs to learn the similarity between the image of an object and the text describing it, achieving significant performance gains in a variety of computer vision problems.</p><p>The trend of text-image multi-modality-based research on image processing has extended to the medical field. <ref type="bibr" target="#b18">[19]</ref> proposed a semantic matching loss that learns medical knowledge to supplement the disadvantages of CLIP that cannot capture uncertain medical semantic meaning. In <ref type="bibr" target="#b1">[2]</ref>, they trained to increase the similarity between the image and text by calculating their influence on each other as a weighted feature. For the segmentation task, LViT <ref type="bibr" target="#b9">[10]</ref> generated the positional characteristics of lesions or target objects as text labels. Furthermore, it proposed a Double U-Shaped structure consisting of a U-Shaped ViT that combines image and text information and a U-Shaped CNN that produces a segmentation mask. However, when combining medical images with non-finegrained text information, noise can affect the outcome.</p><p>In this paper, we propose a new text-guided cross-position attention module (CP AM T G ) that combines text and image. In a medical image, a position attention module (PAM) effectively learns subtle differences among pixels. We utilized PAM which calculates the influence among pixels of an image to capture the association between text and image. To this end, we converted the global text representation generated from the text encoder into a form, such as an image feature map, to create keys and values. The image feature map generated from an image encoder was used as a query. Learning the association between text and image enables us to learn positional information of targets in an image more effectively than existing models that learned multi-modality from medical images. CP AM T G showed an excellent segmentation performance in our comprehensive experiments on various medical images, such as cell, chest X-ray, and magnetic resonance image (MRI). In addition, by applying the proposed technique to the automatic RoI setting module for the deep learning-based diagnosis of sacroiliac arthritis, we confirmed that the proposed method could be effective when it is used in a practical application of computer-aided diagnosis.</p><p>Our main contributions are as follows:</p><p>-We devised a text-guided cross-position attention module (CP AM T G ) that efficiently combines text information with image feature maps. -We demonstrated the effect of CP AM T G on segmentation for various types of medical images. -For a practical computer-aided diagnosis system, we confirm the effectiveness of the proposed method in a deep learning-based sacroiliac arthritis diagnosis system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we propose text-guided segmentation model that can effectively learn the multi-modality of text and images. Figure <ref type="figure" target="#fig_0">1</ref> shows the overall architecture of the proposed model, which consists of an image encoder for generating a feature map from an input image, a text encoder for embedding a text describing the image, and a cross-attention module. The cross-attention module allows the text to serve as a guide for image segmentation by using the correlation between the global text representation and the image feature map. To achieve robust text encoding, we adopt a transformer <ref type="bibr" target="#b16">[17]</ref> structure which performs well in Natural Language Processing (NLP). For image encoding and decoding, we employed U-Net, widely used as a backbone in medical image segmentation. To train our proposed model, we utilize a dataset consisting of image and text pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Configuration of Text-Image Encoder and Decoder</head><p>As Transformer has demonstrated its effectiveness in handling the long-range dependency in sequential data through self-attention <ref type="bibr" target="#b0">[1]</ref>, it performs well in various fields requiring NLP or contextual information analysis of data. We used a Transformer (Encoder T ) to encode the semantic information of the text describing a medical image into a global text representation v T ∈ R 1×2C as v T = Encoder T (T ). Here, the text semantics (T ) can be a sentence indicating the location or characteristics of an interested region in an image such as a lesion shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>To create a segmentation mask from medical images (I), we used U-Net <ref type="bibr" target="#b12">[13]</ref> which has a relatively simple yet effective structure for biomedical image segmen- tation. U-Net operates as an end-to-end fully connected network-based model consisting of a convolutional encoder and decoder connected by skip connections. This architecture is particularly suitable for our purpose because it can be successfully trained on a small amount of data. In the proposed method, we used VGG-16 <ref type="bibr" target="#b14">[15]</ref> as the encoder (Encoder I ) to obtain the image feature F I ∈ R C×H×W as F I = Encoder I (I) and the decoder (Decoder I ) that will generate the segmented image from the enhanced encoding vector obtained by the cross-position attention which will be described in the following subsection.</p><p>The weights of text and image encoders were initialized by the weights of CLIP's pre-trained transformer and VGG16 pre-trained on ImageNet, respectively, and fine-tuned by a loss function for segmentation which will be described in Sect. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text-Guided Cross Position Attention Module</head><p>We introduce a text-guided cross-position attention module (CP AM T G ) that integrates cross-attention <ref type="bibr" target="#b2">[3]</ref> with the position attention module (PAM) <ref type="bibr" target="#b4">[5]</ref> to combine the semantic information of text and image. This module utilizes not only the image feature map from the image encoder but also the global text representation from the text encoder to learn the dependency between various characteristics of text and image. PAM models rich contextual relationships for local features generated from FCNs. It effectively captures spatial dependencies among pixels by generating keys, queries, and values from feature maps. By encoding broad contextual information into local features, and then adaptively gathering spatial contexts, PAM improves representation capability. In particular, this correlation analysis among pixels can effectively analyze medical images in which objects are relatively ambiguous compared to other types of natural images.</p><p>In Fig. <ref type="figure" target="#fig_1">2</ref>, we multiply the learnable parameter (l ∈ R 1×(HW ) ) by the global text representation (v T ) to match the dimension of the text feature with that of the image feature map as The text feature map F T is used as key and value, and the image feature map F I is used as a query to perform self-attention as</p><formula xml:id="formula_0">F T = R(G(v T ) × l),</formula><formula xml:id="formula_1">Q = H Q (F I ), K = H K (F T ), V = H V (F T ),<label>(1)</label></formula><p>where H Q , H K , and H V are convolution layers with a kernel size of 1, and Q, K, and V are queries, keys, and values for self-attention.</p><formula xml:id="formula_2">Attention = sof tmax(Q K) (<label>2</label></formula><formula xml:id="formula_3">)</formula><formula xml:id="formula_4">CP AM T G = Attention V + F I (3)</formula><p>Finally, by upsampling the low-dimensional CP AM T G obtained through crossattention of text and image together with skip-connection, more accurate segmentation prediction can express the detailed information of an object.</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Medical Datasets. We evaluated CP AM T G using three datasets: MoNuSeg <ref type="bibr" target="#b7">[8]</ref> dataset, QaTa-COV19 <ref type="bibr" target="#b5">[6]</ref> dataset, and sacroiliac joint (SIJ) dataset. The first two datasets are the same benchmark datasets used in <ref type="bibr" target="#b9">[10]</ref>. MoNuSeg <ref type="bibr" target="#b7">[8]</ref> contains 30 digital microscopic tissue images of several patients and QaTa-COV19 are COVID-19 chest X-ray images. The ratio of training, validation, and test sets was the same as in <ref type="bibr" target="#b9">[10]</ref>. SIJ is the dataset privately prepared for this study which consists of 804 MRI slices of nineteen healthy subjects and sixty patients diagnosed with axial spondyloarthritis. Among all MRI slices, we selected the gadoliniumenhanced fat-suppressed T1-weighted oblique coronal images, excluding the first and last several slices in which the pelvic bones did not appear, and added the text annotations for the slices.</p><p>Training and Metrics. For a better training, data augmentation was used. We randomly rotated images by -20 • ∼ +20 • and conducted a horizontal flip with 0.5 probability for only the MoNuSeg and QaTa-COV19 datasets. The batch size and learning rate were set to 2 and 0.001, respectively. The loss function (L T ) for training is the sum of the binary cross-entropy loss (L BCE ) and the dice loss (L DICE ):</p><formula xml:id="formula_5">L T = L BCE + L DICE .</formula><p>The mDice and mIoU metrics, widely used to measure the performance of segmentation models, were used to evaluate the performance of object segmentation. For experiments, PyTorch (v1.7.0) were used on a computer with NVIDIA-V100 32 GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Segmentation Performance</head><p>Table <ref type="table">1</ref> presents the comparison of image segmentation performance among the proposed model and the U-Net <ref type="bibr" target="#b12">[13]</ref>, U-Net++ <ref type="bibr" target="#b21">[22]</ref>, Attention U-Net <ref type="bibr" target="#b10">[11]</ref>,</p><p>MedT <ref type="bibr" target="#b15">[16]</ref>, and LViT <ref type="bibr" target="#b9">[10]</ref> methods. Analyzing the results in Table <ref type="table">1</ref>, unlike natural image segmentation, the attention module-based method (Attention U-Net) and transformer-based method (MEdT) did not achieve significant performance gains compared to U-Net based methods (U-Net and U-Net++). By contrast, LViT and CP AM T G , which utilize both text and image information, significantly improved image segmentation performance because of multimodal complementarity, even for medical images with complex and ambiguous object boundaries. Furthermore, CP AM T G achieves a better performance by 1 to 3% than LViT <ref type="bibr" target="#b9">[10]</ref> on all datasets. This means that the proposed CP AM T G helps to improve segmentation performance by allowing text information to serve as a guide for feature extraction for segmentation.</p><p>Figure <ref type="figure">3</ref> shows the examples of segmentation masks obtained using each method. In Fig. <ref type="figure">3</ref>, we marked the boundary of the target object with a red box and showed the ground truth masks for these objects in the last column. Similar to the analysis that can be derived from Table <ref type="table">1</ref>, Fig. <ref type="figure">3</ref> shows that CP AM T G and LViT, which use text information together for image segmentation, create a segmentation mask with more distinctive borders than other methods. In particular, with SIJ, CP AM T G accurately predicted the boundaries of even thin bone parts compared to LViT. Figure <ref type="figure">3</ref> also shows that even on the QaTa-COV19 and MoNuSeg datasets, CP AM T G predicted the most accurate segmentation masks (see the red box areas). From these results, we conjecture that the reasons for the performance improvement of CP AM T G are as follows. CP AM T G independently encodes the input text and image and then combines semantic information via a cross-attention module. Consequently, the two types of information (text and image) do not act as noise from each other, and CP AM T G achieves an improved performance compared to LViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To validate the design of our proposed model, we perform an ablation study on position attention and CP AM T G . Specifically, for the SIJ dataset, we examined the effect of attention in extracting feature maps through comparison with backbone networks (U-Net) and PAM. In addition, we investigated whether text information about images serves as a guide in the position attention process for image segmentation by comparing it with CP AM T G . Table <ref type="table" target="#tab_0">2</ref> summarizes the result of each case. As can be observed in Table <ref type="table" target="#tab_0">2</ref>, the performance of PAM was higher than that of the backbone. This indicates that PAM improves performance by learning associations between pixels for ambiguous targets, as in medical images. In addition, the best performance results of CP AM T G show that text information provided helpful information in an image segmentation process using the proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Application: Deep-Learning Based Disease Diagnosis</head><p>In this section, we confirm the effectiveness of the proposed segmentation method through a practical bio-medical application as a deep learning-based active sacroiliitis diagnosis system. MRI is a representative means for early diagnosis of "active sacroiliitis in axSpA". As active sacroiliitis is a disease that occurs between the pelvic bone and sacral bone, when a MR slice is input, the diagnostic system first separates the area around the pelvic bone into an RoI patch and uses it as an input for the active sacroiliitis classification network <ref type="bibr" target="#b6">[7]</ref>. However, even in the same pelvis, the shape of the bone shown in MR slices varies depending on the slice position of the MRI and the texture of the tissue around the bone is complex. This makes finding an accurate RoI a challenge.</p><p>We segmented the pelvic bones in MRI slices using the proposed method to construct a fully automatic deep learning-based active sacroiliitis diagnosis system, including RoI settings from MRI input images. Figure <ref type="figure">4</ref> shows the results of generating RoI patches by dividing the pelvic bone from MRI slices using the proposed method. As presented in Table <ref type="table" target="#tab_1">3</ref>, compared to the case of using the original MRI image without the RoI setting, using the hand-crafted RoI patch <ref type="bibr" target="#b8">[9]</ref> showed an average of 7% higher performance in recall, precision, and f1. It is noticeable that the automatically set RoI patch showed similar or better performance than the manual RoI patch for each measurement. This indicates that the proposed method can be effectively utilized in practical applications of computer-aided diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we developed a new text-guided cross-attention module (CP AM T G ) that learns text and image information together. The proposed model has a composite structure of position attention and cross-attention in that the key and value are from text data, and the query is created from the image. We use a learnable parameter to convert text features into a tensor of the same dimension as the image feature map to combine text and image information effectively. By calculating the association between the reshaped global text representation and each component of the image feature map, the proposed method outperformed image segmentation performance compared to previous studies using both text and image or image-only training method. We also confirmed that it could be utilized for a deep-learning-based sacroiliac arthritis diagnosis system, one of the use cases for practical medical applications. The proposed method can be further used in various medical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our proposed segmentation model.</figDesc><graphic coords="3,67,98,54,65,316,66,195,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Text-guided cross-position attention module (CP AM T G ).</figDesc><graphic coords="4,74,79,54,08,274,90,111,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where G(•) is a fully connected layer that adjusts the 2C channel of the global text representation v T to the image feature map channel C. R(•) is a reshape operator to C × H × W .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 1 .Fig. 3 .</head><label>13</label><figDesc>Fig. 3. Qualitative results of segmentation models.</figDesc><graphic coords="7,55,98,208,04,340,15,144,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of the effectiveness of our proposed module. "PAM" means we used it instead of CP AM T G for image-only training.</figDesc><table><row><cell>Backbone PAM CP AM T G mDice mIoU</cell></row><row><cell>0.7395 0.6082</cell></row><row><cell>0.7886 0.6591</cell></row><row><cell>0.8800 0.7887</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The results of classification models.</figDesc><table><row><cell></cell><cell cols="3">Recall Precision Specificity NPV</cell><cell>F1</cell></row><row><cell>Origin</cell><cell>0.8500 0.6640</cell><cell>0.7346</cell><cell cols="2">0.8880 0.7456</cell></row><row><cell>[9]</cell><cell>0.9215 0.7343</cell><cell>0.7424</cell><cell cols="2">0.9245 0.8174</cell></row><row><cell cols="3">Proposed 0.9217 0.8281 0.8503</cell><cell cols="2">0.9328 0.8724</cell></row></table><note><p>Fig. 4. Generating RoI.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">MSIT (Ministry of Science, ICT)</rs>, <rs type="funder">Korea</rs>, under the <rs type="programName">High-Potential Individuals Global Training Program</rs> (<rs type="grantNumber">RS-2022-00155227</rs>) supervised by the <rs type="institution">IITP (Institute for Information &amp; Communications Technology Planning &amp; Evaluation)</rs>, the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Korea government (MSIT)</rs> (<rs type="grantNumber">2021R1A2B5B01001412</rs>), and the <rs type="funder">Bio &amp; Medical Technology Development Program of the National Research Foundation (NRF)</rs> funded by the <rs type="funder">Korean government (MSIT)</rs> (<rs type="grantNumber">RS-2023-00220408</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Dcjn2SK">
					<idno type="grant-number">RS-2022-00155227</idno>
					<orgName type="program" subtype="full">High-Potential Individuals Global Training Program</orgName>
				</org>
				<org type="funding" xml:id="_vEdBR6z">
					<idno type="grant-number">2021R1A2B5B01001412</idno>
				</org>
				<org type="funding" xml:id="_hfBAJGH">
					<idno type="grant-number">RS-2023-00220408</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving pneumonia localization via cross-attention on medical images and reports</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhalodia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_53" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="571" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Crossvit: cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Covid-cxnet: detecting covid-19 in frontal chest x-ray images using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haghanifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Majdabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deivalakshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="30615" to="30645" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Method for diagnosing the bone marrow edema of sacroiliac joint in patients with axial spondyloarthritis using magnetic resonance image analysis based on deep learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1156</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lvit: language meets vision transformer in medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.14718</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attention u-net: learning where to look for the pancreas</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Covid-transformer: interpretable covid-19 detection using vision transformer for healthcare</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Environ. Res. Public Health</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">11086</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Medical transformer: gated axial-attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_4" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Essert</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TransBTS: multimodal brain tumor segmentation using transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_11" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10163</idno>
		<title level="m">Medclip: contrastive learning from unpaired medical images and text</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cbam: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nestedformer: nested modality-aware transformer for brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="140" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-5_1" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
