<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis</title>
				<funder ref="#_rDR4MBs">
					<orgName type="full">Shenzhen-Hong Kong Institute of Brain Science-Shenzhen Fundamental Research Institutions</orgName>
				</funder>
				<funder ref="#_v6bB8av">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_4s49G79 #_XTrv3Nz #_ju2FKMG">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_bmjg5jT">
					<orgName type="full">Guangdong Provincial Clinical Research Center for Digestive Diseases</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoyu</forename><surname>Duan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Pathology</orgName>
								<orgName type="institution" key="instit1">The Seventh Affiliated Hospital</orgName>
								<orgName type="institution" key="instit2">Sun Yat-sen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuya</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziyin</forename><surname>Ye</surname></persName>
							<email>yeziyin@mail.sysu.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Pathology</orgName>
								<orgName type="institution" key="instit1">The First Affiliated Hospital</orgName>
								<orgName type="institution" key="instit2">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bingsheng</forename><surname>Huang</surname></persName>
							<email>huangb@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="63" to="71"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CC6B48DC6184F348BBB24764CBFC7D34</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multiple instance learning</term>
					<term>Multi-level Labels</term>
					<term>Pathology Images</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In current pathology image classification, methods mostly rely on patch-based multi-instance learning (MIL), which only considers the relationship between patches and slides. However, in clinical medicine, doctors use slide-level labels to summarize patient-level labels as a diagnostic result, indicating the involvement of three levels of patch, slide, and patient in actual pathology image analysis, which we refer to as the multi-level multi-instance learning (ML-MIL) problem. To address this issue, we propose a novel and general framework called Patients and Slides are Equal (P&amp;SrE), inspired by the doctor's diagnostic process of repeatedly confirming labels at the patient and slide level. In this framework, we treat patients and slides as instances at the same level and use transformers and attention mechanisms to build connections between them. This allows for interaction between patient-level and slide-level information and the correction of their respective features to achieve better classification performance. We evaluate our method on two datasets using two state-of-the-art MIL methods as baselines. The results show that our method improves the performance of the baselines on both slide and patient levels. Our method provides a simple and effective solution to the common problem of ML-MIL in medical clinical scenarios and has broad potential applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pathological image analysis is a vital area of research within medical image analysis, focused on utilizing computer technology to aid doctors in diagnosing and treating diseases by analyzing pathological tissue slide images <ref type="bibr" target="#b4">[5]</ref>. Advancements in pathological image analysis have been made in early cancer diagnosis, tumor localization, and grading, and treatment planning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>. Multi-instance learning <ref type="bibr" target="#b1">[2]</ref> is the primary analysis method used, which involves analyzing tasks based on slide labels and patches. Despite this, the clinical pathological analysis presents certain challenges and complexities, with the ultimate diagnosis relying on patients rather than slides.</p><p>Specifically, in clinical problems of pathological image analysis, doctors usually summarize patient-level labels based on slide labels as the diagnostic results <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. For example, for the pathological discrimination diagnosis task of intestinal tuberculosis(ITB) and Crohn's desease(CD), the categories of postoperative slides are divided into three types (normal, CD, ITB), and doctors will summarize the binary results of patients (ITB or CD) based on slide-level labels <ref type="bibr" target="#b5">[6]</ref>. Similar situations exist in other tasks, such as the classification of breast cancer metastases in lymph nodes, where slide categories may have different classifications, and the corresponding diagnosis of the same patient is whether the cancer has spread to the regional lymph nodes (N-stage) <ref type="bibr" target="#b0">[1]</ref>. Therefore, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, actual pathological image analysis involves the relationships of patches, slides, and patients, which is called a multi-level multi-instance learning (ML-MIL) problem. Among them, for patients and slides, patients are bags while slides are instances, and for slides and patches, slides are bags while patches are instances.</p><p>There are generally two methods to solve the ML-MIL problem. The first method is to directly average the prediction values of slides or take the maximum prediction value <ref type="bibr" target="#b8">[9]</ref>. This method is relatively simple, but the information exchange between slides is not fully utilized, which may lead to errors in the summary result. The second method is to treat slide-patient as a new MIL problem according to the traditional MIL thinking, where slides are regarded as instances and patient labels as bags. Although this method seems reasonable, the number of patients is usually relatively small, and deep learning models usually require a large amount of data for training. Therefore, the insufficient number of samples at the slide-patient level may make it difficult for the model to learn enough information.</p><p>To address the multi-level multi-instance learning (ML-MIL) problem in medical field, we propose a novel framework called Patients and Slides are Equal (P&amp;SrE). Inspired by the iterative labeling process in medical diagnosis, this framework treats patients and slides as instances at the same level and uses transformers and attention mechanisms to build connections between them. This simple yet effective method allows for interaction between patient-level and slidelevel information to correct their respective features and improve classification performance. Our framework consists of two steps: first, at the patch-slide level, a common MIL framework is used to train a MIL neural network and obtain slide-level feature vectors; then, at the slide-patient level, we use self-attention mechanisms to combine the slides of the same patient into patient-level feature vectors, and treat these patient-level feature vectors together with all slide-level feature vectors of the same patient as instances at the same level, which are inputted into transformers for feature interaction and prediction of patient-and slide-level labels. Our method can effectively solve the problem of difficult training due to the scarcity of samples at the highest level in ML-MIL, and can be integrated into two state-of-the-art methods to further improve performance. We conducted rigorous experiments on two datasets and demonstrated the effectiveness of our method. Our contributions include:</p><p>1) Proposing a novel general framework to address the unique "patch-slidepatient" ML-MIL problem in the medical field. Before this, no other framework had directly tackled this specific problem, making our proposal a ground-breaking step in the application of ML-MIL in healthcare; 2) Proposing a simple yet highly effective method that leverages self-attention mechanisms and transformer models to enhance the interaction between slide and patient information. This innovative approach not only improves the classification performance at the patient level but also at the slide level, showcasing its effectiveness and versatility; 3) Conducting extensive experiments on two separate datasets. Our method was seamlessly integrated with two prior state-of-the-art methods, demonstrating its compatibility and adaptability. The experiments resulted in improved performance, indicating that our method enhances the efficacy of these existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Our proposed method P&amp;SrE is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. Specifically, the framework consists of two parts. The first part is the slide-patch level MIL based on a state-of-the-art MIL method. The second part is the patient-slide level MIL, which generates patient-level features using attention mechanism and interacts the features with transformer. To enhance readability, we first provide the following symbolization for ML-MIL: For a patient X i , it has a patient-level classification label Y i . For patient X i , there may exist N i slides S i ={s j |j=1 to N i }, where the classification label for each slide s j is denoted as z j . For each slide s j , it may be divided into M j patches P j ={p k |k=1 to M j }. Here, i,j, and k are indices for patient, slide, and patch levels, respectively. Our proposed framework has strong scalability as it can be based on any attention-based MIL method. Therefore, we directly use the state-of-the-art (SOTA) MIL methods, ABMIL <ref type="bibr" target="#b7">[8]</ref> and DSMIL <ref type="bibr" target="#b8">[9]</ref> for the slide-patch stage. These two methods differ in their attention computing approach for each patch.</p><p>For ABMIL, the attention of each patch is computed by an MLP. Specifically, for M j patches p k , an encoder is applied to obtain the patch feature matrix F i , where,F i ∈ R Mj ×1024 . Then, F i is passed through an fc layer followed by a Tanh activation and another fc layer followed by a sigmoid activation to obtain two feature matrices, F i and F i , both ∈ R Mj ×128 . These matrices are elementwise multiplied and then passed through an fc layer to obtain the weight of each patch, ω k .</p><p>For DSMIL, the attention of each patch is based on the cosine distance between instances and key instances. First, an fc layer is applied to the patch feature matrix F i to obtain the importance score θ k for each patch. The patch with the highest score is selected as the key instance. Then, the feature matrix F i is mapped to a matrix Q i ∈ R Mj ×128 and the cosine similarity between all instances and the key instance is computed as the weight of each patch, ω k .</p><p>Although ABMIL and DSMIL compute attention differently, both methods compute the attention-weighted sum of patch instances features as the bag representation of the slide. Therefore, the slide feature output by both methods can be generalized as:</p><formula xml:id="formula_0">h j = Mj k=1 ω k * p k / Mj k=1 ω k (1)</formula><p>Finally, we obtain the feature vector set H i ={h j |j=1 to N i } for all slides {s j } of patientX i through patch-slide MIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Patient-Slide Level MIL</head><p>After performing patch-slide level MIL, we move on to patient-slide level MIL. In general MIL algorithms, the patient is regarded as the bag and the slide as the instance. However, considering the diagnostic process in clinical practice, we propose to treat both patients and slides as instances at the same level. Specifically, our P&amp;SrE framework for patient-slide level consists of two parts: patient-level feature generation based on self-attention and patient-slide feature interaction based on Transformer <ref type="bibr" target="#b10">[11]</ref>.</p><p>Patient-Level Feature Generation Based on Self-attention. Doctors usually select certain key slides for careful observation and information aggregation during diagnosis, similar to the self-attention mechanism. Therefore, we directly use a fully connected (FC) layer to integrate the feature-level features into patient-level features v i through attention mechanism, serving as patient instances. Specifically, given the feature vector collection {h j } from multiple slides in the previous step, we input it to the FC layer and apply the sigmoid activation function to output the weight α j for each h j . Then, we perform a weighted average of the vectors based on this weight to obtain the patient feature v i :</p><formula xml:id="formula_1">α j = F C({h j |j = 1 to N i }) (2) v j = Ni j=1 α j * h j / Ni j=1 α j (3)</formula><p>Patient-Slide Feature Interaction Based on Transformer. This process is where our framework shines. After doctors summarize the patient-level results, they typically review the slides to double-check the diagnosis results. This patient-slide feature interaction (PSFI) naturally lends itself to the construction of a Transformer, and information exchange and integration between slides and patient level are bidirectional. Thus, self-attention is more ideal for this purpose than other kinds of attention (such as cross-attention or doctors' attention). By using the self-attention-based transformer structure, each input token is treated equally (i.e., viewed as the same instance level), and tokens can interact extensively with each other, enabling mutual correction between patients and slides and even between slides. Specifically, we merge the slide feature set {h j } and the patient feature v i into the input tokens</p><formula xml:id="formula_2">T in i = {h 1 , h 2 , ..., h Ni , v i } = {t},</formula><p>and then input them into a multi-layer transformer through self-attention and feed-forward neural network layers to obtain the interaction information between slides and output tokens T out i :</p><formula xml:id="formula_3">β k,l = sof tmax(W Q t k T (W K t l )/ √ d) (4) t k = Ni+1 l=1 β k,l W V t l (<label>5</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">t k = RELU (t k W R + b 1 )W O + b 2 (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where d is the dimension of the token, and t k and t l come from T in i . β k,l is multi-head attention matrix, and W Q , W K , and W V are weight matrices of query, key, and value, respectively. W R and W O are transformation matrices. b 1 and b 2 are bias vectors. This update procedure is repeated for L layers, where the t k are fed to the successive transformer layer. Finally, we obtain the output tokens</p><formula xml:id="formula_7">T out i ={h 1 , h 2 , ..., h Ni , v i }.</formula><p>Then, all output tokens are input into a shared FC layer, and the patient's predicted logits Y i and the predicted classification logits {z j |j = 1 to N i } for each slide are output.</p><p>Training Progress and Loss Function. During training, we sampled one patient at a time and pre-extracted their batch-level features for all slides, in order to save GPU memory. Due to the issue of class imbalance in both slide level and patient level, we use the LADE <ref type="bibr" target="#b6">[7]</ref> loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation</head><p>CD-ITB Dataset. CD-ITB is a private dataset consisting of 853 slides from 163 patients, with binary patient-level labels of CD or ITB in a ratio of 103:60 and tri-class slide-level labels of CD, ITB, and normal slides in a ratio of 436:121:296, respectively. On average, there were 5 slides per patient. The slides were scanned at a magnification of 40× (0.25 µm/px), and annotations were curated by experienced pathologists. We adopted a patient-level stratification approach for 5-fold cross-validation, with 20% of the training set randomly assigned as the validation set for each fold. The dataset comprises an average of 2.3k instances per bag, with the largest bag containing over 16k instances.</p><p>Camelyon17 Dataset. Camelyon17 <ref type="bibr" target="#b0">[1]</ref> is a publicly dataset, and its training set comprises 500 slides from 100 breast cancer patients with lymph node metastases. The slides are classified into four distinct categories, namely negative, ITC, micro, and macro, in proportions of 318:36:59:87, respectively. There were 5 slides per patient on average. The patients are divided into two groups based on their pN stage, namely lymph node positive and lymph node negative, in proportions of 24:76, respectively. The data folding method is the same as the CD-ITB dataset. The average number of instances per bag is approximately 6.1k, and the largest bag contains over 23k instances.</p><p>Metrics. We report class-wise weighted accuracy (Acc), precision(Pre), Recall, and F1-score (F1). To avoid randomness, we run all experiments five times and report the averaged metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We utilized ResNet50, which was pre-trained on ImageNet1K, to extract features from patches. Each patch was of size 512 × 512 pixels. For both ABMIL and DSMIL networks, we kept the original parameters for the number of channels at each layer. Following the reference <ref type="bibr" target="#b3">[4]</ref>, we employed a transformer with 8 heads and 8 layers in the patient-slide feature interactions. All networks are implemented using PyTorch and trained on a NVIDIA RTX TITAN GPU with 24 GB memory. We employed two Adam optimizers with a maximum learning rate of 1e-4 and a cosine annealing update strategy that gradually decreased the learning rate to 1e-12 over 300 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparisons and Results</head><p>We compared our strategy with two state-of-the-art MIL methods to evaluate its performance. To investigate the impact of self-attention and transformers on slide-level and case-level results, we conducted ablation experiments: "ABMIL + P&amp;SrE (with/without PSFI)" and "DSMIL + P&amp;SrE (with/without PSFI)", respectively. For slide-level classification, we used mean pooling and max pooling to pool feature vectors of patches into a representative vector for the slide, which was then fed into a fully connected layer for classification. At the patient level, we used two approaches for prediction: MaxS, where the feature of the instance that achieves the maximum positive probability from the slide-level MIL model is selected to patient-level model, and MaxMinS, where the mean value of features of the maximum and minimum positive probability from the slide-level MIL model is selected to patient-level model.</p><p>The results of 5-fold CV at the slide and patient levels are reported in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, respectively. Our P&amp;SrE framework improves both ABMIL and DSMIL methods at both levels. ABMIL with P&amp;SrE improves the F1 score from 0.565 to 0.579 for the CD-ITB dataset and from 0.529 to 0.571 for the Camelyon17 dataset at the slide-level, and improves the F1 score from 0.522 to 0.599 for the CD-ITB dataset and from 0.842 to 0.861 for the Camelyon17 dataset at the patient-level. Therefore, the ablation experiments demonstrate the effectiveness of P&amp;SrE in enhancing the classification performance at both the slide and patient levels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Limitations</head><p>Our study has some limitations that should be addressed. For instance, we did not explore the possibility of treating patches as an equivalent level to slides and patients. The primary reason is that the vast number of patches required for analysis is significantly larger than that of slides and patients, which presents a computational challenge for training. As a result, we have not yet explored this avenue. In the future, we plan to leverage clustering and active learning methods to reduce the number of patches and enable the interaction of all three levels with the Transformer, which would further enhance the accuracy and efficiency of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This study proposes a highly scalable and versatile framework to address M-MIL problems. We first classify the process from patch to slide to the patient in medical pathology diagnosis as a multi-level MIL problem. Based on existing state-of-the-art MIL methods, we then extend the framework to P&amp;SrE, which conducts feature extraction and interaction at the slide-patient level. By introducing a transformer, the framework enables iterative interaction and correction of information between patients and slides, resulting in better performance at both the patient level and slide level compared to existing state-of-the-art algorithms on two validation datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Description and solutions for the ML-MIL problem.</figDesc><graphic coords="3,73,53,57,38,312,76,128,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed framework P&amp;SrE. This framework consists of twolevel MIL parts: Slide-patch level MIL and patient-slide level MIL</figDesc><graphic coords="4,45,30,164,00,333,76,98,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Slide-level 5-fold CV results (%)</figDesc><table><row><cell>Method</cell><cell cols="2">CD-ITB dataset</cell><cell cols="2">Camelyon17 dataset</cell></row><row><cell></cell><cell cols="2">Pre Recall Acc F1</cell><cell cols="2">Pre Recall Acc F1</cell></row><row><cell cols="2">Mean pooling (on patch-level feature vectors) 45.0 40.2</cell><cell cols="2">40.2 41.6 47.4 31.6</cell><cell>31.6 35.5</cell></row><row><cell cols="2">Max pooling (on patch-level feature vectors) 39.6 33.1</cell><cell cols="2">33.1 34.9 46.9 25.4</cell><cell>25.4 29.9</cell></row><row><cell>ABMIL [8]</cell><cell>57.6 57.2</cell><cell cols="2">57.2 55.4 55.3 63.7</cell><cell>63.7 50.9</cell></row><row><cell>(ours) ABMIL + P&amp;SrE (w/o PSFI)</cell><cell>57.0 57.3</cell><cell cols="2">57.3 56.5 58.6 65.2</cell><cell>65.2 52.9</cell></row><row><cell>(ours) ABMIL + P&amp;SrE</cell><cell cols="4">59.0 59.7 59.7 57.9 61.0 66.9 66.9 57.1</cell></row><row><cell>DSMIL [9]</cell><cell>57.0 57.3</cell><cell cols="2">57.3 56.9 55.4 63.8</cell><cell>63.8 50.5</cell></row><row><cell>(ours) DSMIL + P&amp;SrE (w/o PSFI)</cell><cell>56.7 57.4</cell><cell cols="2">57.4 56.6 55.5 64.6</cell><cell>64.6 51.7</cell></row><row><cell>(ours) DSMIL + P&amp;SrE</cell><cell cols="4">58.5 59.1 59.1 57.3 55.8 66.4 66.4 52.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Patient-level 5-fold CV results (%)</figDesc><table><row><cell>Method</cell><cell cols="2">CD-ITB dataset</cell><cell cols="2">Camelyon17 dataset</cell></row><row><cell></cell><cell cols="2">Pre Recall Acc F1</cell><cell cols="3">Pre Recall Acc F1</cell></row><row><cell>ABMIL (MaxS)</cell><cell>36.9 63.3</cell><cell cols="2">46.6 46.6 78.0 94.4</cell><cell cols="2">75.0 85.0</cell></row><row><cell>ABMIL+ (MaxMinS)</cell><cell>38.2 65.0</cell><cell cols="2">48.5 48.2 78.3 94.7</cell><cell cols="2">76.0 85.7</cell></row><row><cell cols="2">ABMIL(baseline+Mean) (on probabilities of slides) 38.2 43.3</cell><cell cols="2">53.4 40.6 98.3 75.0</cell><cell cols="2">80.0 85.1</cell></row><row><cell>(ours) ABMIL + P&amp;SrE (w/o PSFI)</cell><cell>48.3 60.0</cell><cell cols="2">59.5 52.2 81.5 87.1</cell><cell cols="2">75.2 84.2</cell></row><row><cell>(ours) ABMIL + P&amp;SrE</cell><cell cols="5">56.8 71.0 66.3 59.9 78.0 96.1 76.4 86.1</cell></row><row><cell>DSMIL + (MaxS)</cell><cell>50.8 56.7</cell><cell cols="2">63.8 53.5 78.7 97.4</cell><cell>78</cell><cell>87.1</cell></row><row><cell>DSMIL + (MaxMinS)</cell><cell>50.0 60.0</cell><cell cols="2">63.2 54.6 79.3 85.5</cell><cell>72</cell><cell>82.3</cell></row><row><cell cols="2">DSMIL(baseline)+Mean (on probabilities of slides) 43.8 57.7</cell><cell cols="2">53.3 48.1 100 73.7</cell><cell cols="2">80.0 84.9</cell></row><row><cell>(ours) DSMIL + P&amp;SrE (w/o PSFI)</cell><cell cols="5">49.7 61.7 62.8 54.9 78.0 97.9 77.4 86.8</cell></row><row><cell>(ours) DSMIL + P&amp;SrE</cell><cell>60.0 56.7</cell><cell cols="2">69.7 57.7 80.2 96.8</cell><cell cols="2">79.4 87.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This study was supported by <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (No. <rs type="grantNumber">2020A1515010571</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">82271958</rs>, <rs type="grantNumber">81971684</rs>, <rs type="grantNumber">81801761</rs>), <rs type="funder">Shenzhen-Hong Kong Institute of Brain Science-Shenzhen Fundamental Research Institutions</rs> (No. <rs type="grantNumber">2022SHIBS0003</rs>), and <rs type="funder">Guangdong Provincial Clinical Research Center for Digestive Diseases</rs> (No. <rs type="grantNumber">2020B1111170004</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_v6bB8av">
					<idno type="grant-number">2020A1515010571</idno>
				</org>
				<org type="funding" xml:id="_4s49G79">
					<idno type="grant-number">82271958</idno>
				</org>
				<org type="funding" xml:id="_XTrv3Nz">
					<idno type="grant-number">81971684</idno>
				</org>
				<org type="funding" xml:id="_ju2FKMG">
					<idno type="grant-number">81801761</idno>
				</org>
				<org type="funding" xml:id="_rDR4MBs">
					<idno type="grant-number">2022SHIBS0003</idno>
				</org>
				<org type="funding" xml:id="_bmjg5jT">
					<idno type="grant-number">2020B1111170004</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="550" to="560" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple instance learning: a survey of problem characteristics and applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gagnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="329" to="353" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Artificial intelligence and computational pathology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lab. Invest</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="412" to="422" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computational pathology: challenges and promises for tissue analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="515" to="530" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Differential diagnosis of inflammatory bowel disease: imitations and complications</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Gecse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vermeire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Gastroenterol. Hepatol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="644" to="653" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Disentangling label distribution for long-tailed visual recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6626" to="6636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Eliceiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14318" to="14328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Translational AI and deep learning in diagnostic pathology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Serag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">185</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
