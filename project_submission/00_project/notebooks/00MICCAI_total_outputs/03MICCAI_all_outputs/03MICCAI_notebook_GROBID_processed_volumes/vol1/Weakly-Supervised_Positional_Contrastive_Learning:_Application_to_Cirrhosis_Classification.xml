<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification</title>
				<funder ref="#_AbCgqv8">
					<orgName type="full">Région Ile-de-France</orgName>
				</funder>
				<funder ref="#_WCRKCVj">
					<orgName type="full">ANRT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Emma</forename><surname>Sarfati</surname></persName>
							<email>emma.sarfati@guerbet.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Guerbet Research</orgName>
								<address>
									<settlement>Villepinte</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<settlement>Télécom Paris, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Bône</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Guerbet Research</orgName>
								<address>
									<settlement>Villepinte</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc-Michel</forename><surname>Rohé</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Guerbet Research</orgName>
								<address>
									<settlement>Villepinte</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Gori</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<settlement>Télécom Paris, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Isabelle</forename><surname>Bloch</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<settlement>Télécom Paris, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Sorbonne Université</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>LIP6</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">760CB75FF5543E75A66A699317B8500F</idno>
					<idno type="DOI">10.1007/978-3-031-43907-022.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Weakly-supervised learning</term>
					<term>Contrastive learning</term>
					<term>CT</term>
					<term>Cirrhosis prediction</term>
					<term>Liver Supplementary Information The online version contains supplementary material</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations, and small strongly-labeled (i.e., high-confidence) datasets. The proposed model improves the classification AUC by 5% with respect to a baseline model on our internal dataset, and by 26% on the public LIHC dataset from the Cancer Genome Atlas. The code is available at: https://github. com/Guerbet-AI/wsp-contrastive.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the medical domain, obtaining a large amount of high-confidence labels, such as histopathological diagnoses, is arduous due to the cost and required technicality. It is however possible to obtain lower confidence assessments for a large amount of images, either by a clinical questioning, or directly by a radiological diagnosis. To take advantage of large volumes of unlabeled or weakly-labeled images, pre-training encoders with self-supervised methods showed promising results in deep learning for medical imaging <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b29">[29]</ref>. In particular, contrastive learning (CL) is a self-supervised method that learns a mapping of the input images to a representation space where similar (positive) samples are moved closer and different (negative) samples are pushed far apart. Weak discrete labels can be integrated into contrastive learning by, for instance, considering as positives only the samples having the same label, as in <ref type="bibr" target="#b12">[13]</ref>, or by directly weighting unsupervised contrastive and supervised cross entropy loss functions, as in <ref type="bibr" target="#b18">[19]</ref>. In this work, we focus on the scenario where radiological meta-data (thus, low-confidence labels) are available for a large amount of images, whereas high-confidence labels, obtained by histological analysis, are scarce.</p><p>Naive extensions of contrastive learning methods, such as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, from 2D to 3D images may be difficult due to limited GPU memory and therefore small batch size. A usual solution consists in using patch-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>. However, these methods pose two difficulties: they reduce the spatial context (limited by the size of the patch), and they require similar spatial resolution across images. This is rarely the case for abdominal CT/MRI acquisitions, which are typically strongly anisotropic and with variable resolutions. Alternatively, depth position of each 2D slice, within its corresponding volume, can be integrated in the analysis. For instance, in <ref type="bibr" target="#b3">[4]</ref>, the authors proposed to integrate depth in the sampling strategy for the batch creation. Likewise, in <ref type="bibr" target="#b25">[26]</ref>, the authors proposed to define as similar only 2D slices that have a small depth difference, using a normalized depth coordinate d ∈ [0, 1]. These works implicitly assume a certain threshold on depth to define positive and negative samples, which may be difficult to define and may be different among applications and datasets. Differently, inspired by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>, here we propose to use a degree of "positiveness" between samples by defining a kernel function w on depth positions. This allows us to consider volumetric depth information during pre-training and to use large batch sizes. Furthermore, we also propose to simultaneously leverage weak discrete attributes during pre-training by using a novel and efficient contrastive learning composite kernel loss function, denoting our global method Weakly-Supervised Positional (WSP).</p><p>We apply our method to the classification of histology-proven liver cirrhosis, with a large volume of (weakly) radiologically-annotated CT-scans and a small amount of histopathologically-confirmed cirrhosis diagnosis. We compare the proposed approach to existing self-supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Let x t be an input 2D image, usually called anchor, extracted from a 3D volume, y t a corresponding discrete weak variable and d t a related continuous variable. In this paper, y t refers to a weak radiological annotation and d t corresponds to the normalized depth position of the 2D image within its corresponding 3D volume: if V max corresponds to the maximal depth-coordinate of a volume V , we compute d t = pt Vmax with p t ∈ [0, V max ] being the original depth coordinate.</p><p>Let x - j and x + i be two semantically different (negative) and similar (positive) images with respect to x t , respectively.</p><p>The definition of similarity is crucial in CL and is the main difference between existing methods. For instance, in unsupervised CL, methods such as SimCLR <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> choose as positive samples random augmentations of the anchor x + i = t(x t ), where t ∼ T is a random transformation chosen among a user-selected family T . Negative images x - j are all other (transformed) images present in the batch. Once x - j and x + i are defined, the goal of CL is to compute a mapping function f θ : X → S d , where X is the set of images and S d the representation space, so that similar samples are mapped closer in the representation space than dissimilar samples. Mathematically, this can be defined as looking for a f θ that satisfies the condition:</p><formula xml:id="formula_0">s - tj -s + ti ≤ 0 ∀t, j, i<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">s - tj = sim(f θ (x t ), f θ (x - j )) and s + ti = sim(f θ (x t ), f θ (x + i ))</formula><p>, with sim a similarity function defined here as sim(a, b) = a T b τ with τ &gt; 0. In the presence of discrete labels y, the definition of negative (x - j ) and positive (x + i ) samples may change. For instance, in SupCon <ref type="bibr" target="#b12">[13]</ref>, the authors define as positives all images with the same discrete label y. However, when working with continuous labels d, one cannot use the same strategy since all images are somehow positive and negative at the same time. A possible solution <ref type="bibr" target="#b25">[26]</ref> would be to define a threshold γ on the distance between labels (e.g., d a , d b ) so that, if the distance is smaller than γ (i.e., ||d ad b || 2 &lt; γ), the samples (e.g., x a and x b ) are considered as positives. However, this requires a user-defined hyperparameter γ, which could be hard to find in practice. A more efficient solution, as proposed in <ref type="bibr" target="#b7">[8]</ref>, is to define a degree of "positiveness" between samples using a normalized kernel function w σ (d,</p><formula xml:id="formula_2">d i ) = K σ (d -d i )</formula><p>, where K σ is, for instance, a Gaussian kernel, with user defined hyper-parameter σ and 0 ≤ w σ ≤ 1. It is interesting to notice that, for discrete labels, one could also define a kernel as: w δ (y, y i ) = δ(yy i ), δ being the Dirac function, retrieving exactly SupCon <ref type="bibr" target="#b12">[13]</ref>.</p><p>In this work, we propose to leverage both continuous d and discrete y labels, by combining (here by multiplying) the previously defined kernels, w σ and w δ , into a composite kernel loss function. In this way, samples will be considered as similar (positive) only if they have a composite degree of "positiveness" greater than zero, namely both kernels have a value greater (or different) than 0 (w σ &gt; 0 and w δ = 0). An example of resulting representation space is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. This constraint can be defined by slightly modifying the condition introduced in Eq. 1, as:</p><formula xml:id="formula_3">w δ (y t , y i ) • w σ (d t , d i ) composite kernel wti (s tj -s ti ) ≤ 0 ∀t, i, j = i (2)</formula><p>where the indices t, i, j traverse all N images in the batch since there are no "hard" positive or negative samples, as in SimCLR or SupCon, but all images are considered as positive and negative at the same time. As commonly done in CL <ref type="bibr" target="#b2">[3]</ref>, this condition can be transformed into an optimization problem using the max operator and its smooth approximation LogSumExp:</p><p>arg min</p><formula xml:id="formula_4">f θ t,i max(0, w ti {s tj -s ti } N j=1 j =i ) = arg min f θ t,i w ti max(0, {s tj -s ti } N j=1 j =i ) ≈ arg min f θ ⎛ ⎝ - t,i w ti log exp(s ti ) N j =i exp(s tj ) ⎞ ⎠</formula><p>(3) By defining P (t) = {i : y i = y t } as the set of indices of images x i in the batch with the same discrete label y i as the anchor x t , we can rewrite our final loss function as:</p><formula xml:id="formula_5">L W SP = - N t=1 i∈P (t) w σ (d t , d i ) log exp(s ti ) N j =i exp(s tj )<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">w σ (d t , d i ) is normalized over i ∈ P (t).</formula><p>In practice, it is rather easy to find a good value of σ, as the proposed kernel method is quite robust to its variation. A robustness study is available in the supplementary material. For the experiments, we fix σ = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We compare the proposed method with different contrastive and non-contrastive methods, that either use no meta-data (SimCLR <ref type="bibr" target="#b4">[5]</ref>, BYOL <ref type="bibr" target="#b9">[10]</ref>), or leverage only discrete labels (SupCon <ref type="bibr" target="#b12">[13]</ref>), or continuous labels (depth-Aware <ref type="bibr" target="#b7">[8]</ref>). The proposed method is the only one that takes simultaneously into account both discrete and continuous labels. In all experiments, we work with 2D slices rather than 3D volumes due to the anisotropy of abdominal CT-scans in the depth direction and the limited spatial context or resolution obtained with 3D patchbased or downsampling methods, respectively, which strongly impacts the cirrhosis diagnosis that is notably based on the contours irregularity. Moreover, the large batch sizes necessary in contrastive learning can not be handled in 3D due to a limited GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Three histo . It corresponds to absent fibrosis (F0), mild fibrosis (F1), significant fibrosis (F2), severe fibrosis (F3) and cirrhosis (F4). This score is then binarized to indicate the absence or presence of advanced fibrosis <ref type="bibr" target="#b13">[14]</ref>: F0/F1/F2 (N = 28) vs. F3/F4 (N = 78). D 2 histo . This is the public LIHC dataset from the Cancer Genome Atlas <ref type="bibr" target="#b8">[9]</ref>, which presents a histological score, the Ishak score, designated as y 2 histo , that differs from the METAVIR score present in D 1  histo . This score is also distributed through five labels: No Fibrosis, Portal Fibrosis, Fibrous Speta, Nodular Formation and Incomplete Cirrhosis and Established Cirrhosis. Similarly to the METAVIR score in D 1 histo , we also binarize the Ishak score, as proposed in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>, which results in two cohorts of 34 healthy and 15 pathological patients.</p><p>In all datasets, we select the slices based on the liver segmentation of the patients. To gain in precision, we keep the top 70% most central slices with respect to liver segmentation maps obtained manually in D radio , and automatically for D 1  histo and D 2 histo using a U-Net architecture pretrained on D radio <ref type="bibr" target="#b17">[18]</ref>. For the latter pretraining dataset, it presents an average slice spacing of 3.23 mm with a standard deviation of 1.29 mm. For the x and y axis, the dimension is 0.79 mm per voxel on average, with a standard deviation of 0.10 mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture and Optimization</head><p>Backbones. We propose to work with two different backbones in this paper: TinyNet and ResNet-18 <ref type="bibr" target="#b11">[12]</ref>. TinyNet is a small encoder with 1.1M parameters, inspired by <ref type="bibr" target="#b23">[24]</ref>, with five convolutional layers, a representation space (for downstream tasks) of size 256 and a latent space (after a projection head of two dense layers) of size 64. In comparison, ResNet-18 has 11.2M parameters, a representation space of dimension 512 and a latent space of dimension 128. More details and an illustration of TinyNet are available in the supplementary material, as well as a full illustration of the algorithm flow.</p><p>Data Augmentation, Sampling and Optimization. CL methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> require strong data augmentations on input images, in order to strengthen the association between positive samples <ref type="bibr" target="#b21">[22]</ref>. In our work, we leverage three types of augmentations: rotations, crops and flips. Data augmentations are computed on the GPU, using the Kornia library <ref type="bibr" target="#b16">[17]</ref>. During inference, we remove the augmentation module to only keep the original input images.</p><p>For sampling, inspired by <ref type="bibr" target="#b3">[4]</ref>, we propose a strategy well-adapted for contrastive learning in 2D medical imaging. We first sample N patients, where N is the batch size, in a balanced way with respect to the radiological/histological classes; namely, we roughly have the same number of subjects per class. Then, we randomly select only one slice per subject. In this way, we maximize the slice heterogeneity within each batch. We use the same sampling strategy also for classification baselines. For D 2  histo , which has fewer patients than the batch size, we use a balanced sampling strategy with respect to the radiological/histological classes with no obligation of one slice per patient in the batch. As we work with 2D slices rather than 3D volumes, we compute the average probability per patient of having the pathology. The evaluation results presented later are based on the patient-level aggregated prediction.</p><p>Finally, we run our experiments on a Tesla V100 with 16GB of RAM and a 6 CPU cores, and we used the PyTorch-Lightning library to implement our models. All models share the same data augmentation module, with a batch size of B = 64 and a fixed number of epochs n epochs = 200. For all experiments, we fix a learning rate (LR) of α = 10 -4 and a weight decay of λ = 10 -4 . We add a cosine decay learning rate scheduler <ref type="bibr" target="#b14">[15]</ref> to prevent over-fitting. For BYOL, we initialize the moving average decay at 0.996. Evaluation Protocol. We first pretrain the backbone networks on D radio using all previously listed contrastive and non-contrastive methods. Then, we train a regularized logistic regression on the frozen representations of the datasets D 1 histo and D 2 histo . We use a stratified 5-fold cross-validation. As a baseline, we train a classification algorithm from scratch (supervised) for each dataset, D 1  histo and D 2 histo , using both backbone encoders and the same 5-fold crossvalidation strategy. We also train a regularized logistic regression on representations obtained with a random initialization as a second baseline (random). Finally, we report the cross-validated results for each model on the aggregated dataset  </p><formula xml:id="formula_7">D 1+2 histo = D 1 histo + D 2 histo .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>We present in Table <ref type="table" target="#tab_1">1</ref> the results of all our experiments. For each of them, we report whether the pretraining method integrates the weak label meta-data, the depth spatial encoding, or both, which is the core of our method. First, we can notice that our method outperforms all other pretraining methods in D 1 histo and D 1+2 histo , which are the two datasets with more patients. For the latter, the proposed method surpasses the second best pretraining method, depth-Aware, by 4%. For D 1  histo , it can be noticed that WSP (ours) provides the best AUC score whatever the backbone used. For the second dataset D 2 histo , our method is on par with BYOL and SupCon when using a small encoder and outperforms the other methods when using a larger backbone.</p><p>To illustrate the impact of the proposed method, we report in Fig. <ref type="figure" target="#fig_1">2</ref> the projections of the ResNet-18 representation vectors of 10 randomly selected subjects of D 1 histo onto the first two modes of a PCA. It can be noticed that the representation space of our method is the only one where the diagnostic label (not available during pretraining) and the depth position are correctly integrated. Indeed, there is a clear separation between slices of different classes (healthy at the bottom and cirrhotic cases at the top) and at the same time it seems that the depth position has been encoded in the x-axis, from left to right. SupCon performs well on the training set of D radio (figure available in the supplementary material), as well as D 2 histo with TinyNet, but it poorly generalizes to D 1 histo and D 1+2 histo . The method depth-Aware manages to correctly encode the depth position but not the diagnostic class label.</p><p>To assess the clinical performance of the pretraining methods, we also compute the balanced accuracy scores (bACC) of the trained classifiers, which is compared in Table <ref type="table" target="#tab_2">2</ref> to the bACC achieved by radiologists who were asked to visually assess the presence or absence of cirrhosis for the N=106 cases of D 1 histo . The reported bACC values correspond to the best scores among those obtained with Tiny and ResNet encoders.</p><p>Radiologists achieved a bACC of 82% with respect to the histological reference. The two bestperforming methods surpassed this score: depth-Aware and the proposed WSP approach, improving respectively the radiologists score by 2% and 3%, suggesting that including 3D information (depth) at the pretraining phase was beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a novel kernel-based contrastive learning method that leverages both continuous and discrete meta-data for pretraining. We tested it on a challenging clinical application, cirrhosis prediction, using three different datasets, including the LIHC public dataset. To the best of our knowledge, this is the first time that a pretraining strategy combining different kinds of meta-data has been proposed for such application. Our results were compared to other stateof-the-art CL methods well-adapted for cirrhosis prediction. The pretraining methods were also compared visually, using a 2D projection of the representation vectors onto the first two PCA modes. Results showed that our method has an organization in the representation space that is in line with the proposed theory, which may explain its higher performances in the experiments. As future work, it would be interesting to adapt our kernel method to non-contrastive methods, such as SimSIAM <ref type="bibr" target="#b6">[7]</ref>, BYOL <ref type="bibr" target="#b9">[10]</ref> or Barlow Twins <ref type="bibr" target="#b24">[25]</ref>, that need smaller batch sizes and have shown greater performances in computer vision tasks. In terms of application, our method could be easily translated to other medical problems, such as pancreas cancer prediction using the presence of intrapancreatic fat, diabetes mellitus or obesity as discrete meta-labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of representation space constructed by our loss function, leveraging both continuous depth coordinate d and discrete label y (i.e., radiological diagnosis y radio ). Samples from different radiological classes are well separated and, at the same time, samples are ordered within each class based on their depth coordinate d.</figDesc><graphic coords="4,62,31,54,05,295,54,170,65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Projections of the ResNet-18 representation vectors of 10 randomly selected subjects of D 1 histo onto the first two modes of a PCA. Each dot represents a 2D slice. Color gradient refers to different depth positions. Red = cirrhotic cases. Blue = healthy subjects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>datasets of abdominal CT images are used in this study. One dataset is used for contrastive pretraining, and the other two for evaluation. All images have a 512 × 512 size, and we clip the intensity values between -100 and 400. D radio . First, D radio contains 2,799 CT-scans of patients in portal venous phase with a radiological (weak) annotation, i.e. realized by a radiologist, indicating four different stages of cirrhosis: no cirrhosis, mild cirrhosis, moderate cirrhosis and severe cirrhosis (y radio ). The respective numbers are 1880, 385, 415 and 119. y radio is used as the discrete label y during pre-training. D 1 histo . It contains 106 CT-scans from different patients in portal venous phase, with an identified histopathological status (METAVIR score) obtained by a histological analysis, designated as y 1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Resulting 5-fold cross-validation AUCs. For each encoder, best results are in bold, second top results are underlined. * = We use the pretrained weights from ImageNet with ResNet-18 and run a logistic regression on the frozen representations.</figDesc><table><row><cell cols="7">Backbone Pretraining method Weak labels Depth pos. D 1 h isto (N=106) D 2 h isto (N=49) D 1+2 h isto (N=155)</cell></row><row><cell>TinyNet</cell><cell>Supervised</cell><cell>✗</cell><cell>✗</cell><cell>0.79 (±0.05)</cell><cell>0.65 (±0.25)</cell><cell>0.71 (±0.04)</cell></row><row><cell></cell><cell>None (random)</cell><cell>✗</cell><cell>✗</cell><cell>0.64 (±0.10)</cell><cell>0.75 (±0.13)</cell><cell>0.73 (±0.06)</cell></row><row><cell></cell><cell>SimCLR</cell><cell>✗</cell><cell>✗</cell><cell>0.75 (±0.08)</cell><cell>0.88 (±0.16)</cell><cell>0.76 (±0.11)</cell></row><row><cell></cell><cell>BYOL</cell><cell>✗</cell><cell>✗</cell><cell>0.75 (±0.09)</cell><cell>0.95 (±0.07)</cell><cell>0.77 (±0.08)</cell></row><row><cell></cell><cell>SupCon</cell><cell>✓</cell><cell>✗</cell><cell>0.76 (±0.09)</cell><cell>0.93 (±0.07)</cell><cell>0.72 (±0.06)</cell></row><row><cell></cell><cell>depth-Aware</cell><cell>✗</cell><cell>✓</cell><cell>0.80 (±0.13)</cell><cell>0.81 (±0.08)</cell><cell>0.77 (±0.08)</cell></row><row><cell></cell><cell>Ours</cell><cell>✓</cell><cell>✓</cell><cell>0.84 (±0.12)</cell><cell>0.91 (±0.11)</cell><cell>0.79 (±0.11)</cell></row><row><cell>ResNet-18</cell><cell>Supervised</cell><cell>✗</cell><cell>✗</cell><cell>0.77 (±0.10)</cell><cell>0.56 (±0.29)</cell><cell>0.72 (±0.08)</cell></row><row><cell></cell><cell>None (random)</cell><cell>✗</cell><cell>✗</cell><cell>0.69 (±0.19)</cell><cell>0.73 (±0.12)</cell><cell>0.68 (±0.09)</cell></row><row><cell></cell><cell>ImageNet*</cell><cell>✗</cell><cell>✗</cell><cell>0.72 (±0.17)</cell><cell>0.76 (±0.04)</cell><cell>0.66 (±0.10)</cell></row><row><cell></cell><cell>SimCLR</cell><cell>✗</cell><cell>✗</cell><cell>0.79 (±0.09)</cell><cell>0.82 (±0.14)</cell><cell>0.79 (±0.08)</cell></row><row><cell></cell><cell>BYOL</cell><cell>✗</cell><cell>✗</cell><cell>0.78 (±0.09)</cell><cell>0.77 (±0.11)</cell><cell>0.78 (±0.08)</cell></row><row><cell></cell><cell>SupCon</cell><cell>✓</cell><cell>✗</cell><cell>0.69 (±0.07)</cell><cell>0.69 (±0.13)</cell><cell>0.76 (±0.12)</cell></row><row><cell></cell><cell>depth-Aware</cell><cell>✗</cell><cell>✓</cell><cell>0.83 (±0.07)</cell><cell>0.82 (±0.11)</cell><cell>0.80 (±0.07)</cell></row><row><cell></cell><cell>Ours</cell><cell>✓</cell><cell>✓</cell><cell>0.84 (±0.07)</cell><cell>0.85 (±0.10)</cell><cell>0.84 (±0.07)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the pretraining methods with a binary radiological annotation for cirrhosis on D 1 histo . Best results are in bold, second top results are underlined.</figDesc><table><row><cell cols="3">Pretraining method bACC models bACC radiologists</cell></row><row><cell>Supervised</cell><cell>0.78 (±0.04)</cell><cell></cell></row><row><cell>None (random)</cell><cell>0.71 (±0.13)</cell><cell></cell></row><row><cell>ImageNet</cell><cell>0.74 (±0.13)</cell><cell></cell></row><row><cell>SimCLR</cell><cell>0.78 (±0.08)</cell><cell></cell></row><row><cell>BYOL</cell><cell>0.77 (±0.04)</cell><cell>0.82</cell></row><row><cell>SupCon</cell><cell>0.77 (±0.10)</cell><cell></cell></row><row><cell>depth-Aware</cell><cell>0.84 (±0.04)</cell><cell></cell></row><row><cell>Ours</cell><cell>0.85 (±0.09)</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by <rs type="funder">Région Ile-de-France</rs> (<rs type="projectName">ChoTherIA</rs> project) and <rs type="funder">ANRT</rs> (CIFRE #<rs type="grantNumber">2021/1735</rs>).</p><p>Compliance with Ethical Standards. This research study was conducted retrospectively using human data collected from various medical centers, whose Ethics Committees granted their approval. Data was de-identified and processed according to all applicable privacy laws and the <rs type="institution">Declaration of Helsinki</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_AbCgqv8">
					<orgName type="project" subtype="full">ChoTherIA</orgName>
				</org>
				<org type="funding" xml:id="_WCRKCVj">
					<idno type="grant-number">2021/1735</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Big self-supervised models advance medical image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3458" to="3468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contrastive learning for regression in multi-site brain age prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Barbano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dufumier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grangetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ISBI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unbiased Supervised Contrastive Learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Barbano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dufumier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tartaglione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grangetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contrastive learning of global and local features for medical image segmentation with limited annotations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12546" to="12558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="15745" to="15753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contrastive learning with continuous proxy meta-data for 3D MRI classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dufumier</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-36" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Radiology data from the cancer genome atlas colon adenocarcinoma [TCGA-COAD] collection</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual nets model for staging liver fibrosis on plain CT images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-020-02206-y</idno>
		<ptr target="https://doi.org/10.1007/s11548-020-02206-y" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1399" to="1406" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Histopathological study of chronic hepatitis B: a comparative study of Ishak and METAVIR scoring systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohamadnejad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Organ Transp. Med</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kornia: an open source differentiable computer vision library for PyTorch</title>
		<author>
			<persName><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to diagnose cirrhosis from radiological and histological labels with joint self and weakly-supervised pretraining strategies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sarfati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bloch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-04">Apr 2023</date>
			<publisher>IEEE ISBI. Cartagena de Indias</publisher>
			<pubPlace>Colombia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ishak versus METAVIR: Terminology, convertibility and correlation with laboratory changes in chronic hepatitis C</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shiha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zalata</surname></persName>
		</author>
		<idno>chap. 10</idno>
	</analytic>
	<monogr>
		<title level="m">Liver Biopsy</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Takahashi</surname></persName>
		</editor>
		<meeting><address><addrLine>Rijeka</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contig: Self-supervised multimodal contrastive learning for medical imaging with genetics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirchler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lippert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="20908" to="20921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Contrastive learning with stronger augmentations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<idno>CoRR abs/2104.07713</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for classification of Alzheimer&apos;s disease: overview and reproducible evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101694</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Liver fibrosis staging by deep learning: a visual-based explanation of diagnostic decisions of the model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A J O</forename><surname>Dierckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Mouridsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Kwee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>De Haas</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00330-021-08046-x</idno>
		<ptr target="https://doi.org/10.1007/s00330-021-08046-x" />
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="9620" to="9627" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Positional contrastive learning for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="221" to="230" />
			<pubPlace>Berlin; Heidelberg</pubPlace>
		</imprint>
		<respStmt>
			<orgName>MICCAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self supervised deep representation learning for fine-grained body part recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 14th International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="578" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Models genesis: generic autodidactic models for 3D medical image analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-942" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning for 3D medical images by playing a Rubik&apos;s cube</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
