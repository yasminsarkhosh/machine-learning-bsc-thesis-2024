<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medical Image Computing and Computer Assisted Intervention – MICCAI 2023</title>
				<funder ref="#_wP3BhNR">
					<orgName type="full">UK EPSRC (Engineering and Physical Research Council)</orgName>
				</funder>
				<funder ref="#_hMjCPDz #_sFngheN">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Divyanshu</forename><surname>Mishra</surname></persName>
							<email>divyanshu.mishra@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Biomedical Engineering</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">He</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Biomedical Engineering</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pramit</forename><surname>Saha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Biomedical Engineering</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aris</forename><forename type="middle">T</forename><surname>Papageorghiou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Nuffield Department of Women&apos;s and Reproductive Health</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">Alison</forename><surname>Noble</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Biomedical Engineering</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Conditioned Diffusion Models for Out-of-Distribution Detection: Application to Fetal Ultrasound Videos</title>
					</analytic>
					<monogr>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">74700FF2C47CCE17B623FB13661E953C</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> Out-of-distribution (OOD)  <p>detection is essential to improve the reliability of machine learning models by detecting samples that do not belong to the training distribution. Detecting OOD samples effectively in certain tasks can pose a challenge because of the substantial heterogeneity within the in-distribution (ID), and the high structural similarity between ID and OOD classes. For instance, when detecting heart views in fetal ultrasound videos there is a high structural similarity between the heart and other anatomies such as the abdomen, and large in-distribution variance as a heart has 5 distinct views and structural variations within each view. To detect OOD samples in this context, the resulting model should generalise to the intra-anatomy variations while rejecting similar OOD samples. In this paper, we introduce dualconditioned diffusion models (DCDM) where we condition the model on in-distribution class information and latent features of the input image for reconstruction-based OOD detection. This constrains the generative manifold of the model to generate images structurally and semantically similar to those within the in-distribution. The proposed model outperforms reference methods with a 12% improvement in accuracy, 22% higher precision, and an 8% better F1 score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Existing out-of-distribution (OOD) detection methods work well when the indistribution (ID) classes have low heterogeneity (low variance) but fail when in-distribution classes have high heterogeneity <ref type="bibr" target="#b22">[23]</ref> or high spatial similarity between ID and OOD classes <ref type="bibr" target="#b8">[9]</ref>. Fetal ultrasound (US) anatomy detection is one such application where both the challenges co-exist.</p><p>In this paper, we propose a Dual-Conditioned Diffusion Model (DCDM) to detect OOD samples when in-distribution data has high variance and test the performance by detecting heart views in fetal US videos as an example application. Specifically, an Ultrasound (US) typically comprises 13 anatomies and their views. However, analysis models are usually developed for anatomy-specific tasks. Hence, to separate heart views from other 12 anatomies (head, abdomen, femur etc.) we develop an OOD detection algorithm. Our in-distribution data comprises five structurally different heart views captured across different cardiac cycles of a beating heart during obstetric US scanning. We develop a diffusionbased model for reconstruction-based OOD detection, which extends <ref type="bibr" target="#b13">[14]</ref> with a novel dual conditioning mechanism that alleviates the influence of high interand intra-class variation within different classes by leveraging in-distribution class conditioning (IDCC) and latent image feature conditioning (LIFC). These conditioning mechanisms allow our model to generate images similar to the input image for in-distribution data. The primary contributions of our paper are summarized as follows: 1) We introduce a novel conditioned diffusion model for OOD detection and demonstrate that the dual conditioning mechanism is effective in tackling challenging scenarios where in-distribution data comprises multiple heterogeneous classes and there is a high spatial similarity between ID and OOD classes. 2) Two original conditions are proposed for the diffusion model, which are in-distribution class conditioning (IDCC) and latent image feature conditioning (LIFC). IDCC is proposed to handle high inter-class variance within in-distribution classes and high spatial similarity between ID and OOD classes. LIFC is introduced to counter the intra-class variance within each class. 3) We demonstrate in our experiments that DCDM can detect and separate heart views from other anatomies in fetal ultrasound videos without needing any labelled data for OOD classes. Extensive experiments and ablations demonstrate superior performance over existing OOD detection methods. Our approach is not fetal ultrasound specific and could be applied to other OOD applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>OOD detection <ref type="bibr" target="#b30">[31]</ref> involves identifying samples that do not belong to the training distribution. Such models can be categorized into: (a) unsupervised OOD detection <ref type="bibr" target="#b22">[23]</ref> and (b) supervised OOD detection. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>. Unsupervised OOD detection methods can again be divided into two main categories: (i) likelihoodbased approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>, and (ii) reconstruction-based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. Likelihoodbased approaches suffer from several issues, including assigning higher likelihood to OOD samples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, susceptibility to adversarial attacks <ref type="bibr" target="#b7">[8]</ref>, and calibration issues <ref type="bibr" target="#b27">[28]</ref>. Current reconstruction-based approaches are sensitive to dimensions of the bottleneck layer and require rigorous tuning specific to the dataset and task <ref type="bibr" target="#b9">[10]</ref>. Additionally, models trained using a generator-discriminator architecture and optimizing adversarial losses can be highly unstable and challenging to train <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Finally, reconstruction-based methods often rely on highly compressed latent representations, which can lead to loss of important low-level detail. This can be problematic when discriminating between classes with high spatial similarity. Recently, diffusion models have been introduced to address these limitations on tasks such as image synthesis <ref type="bibr" target="#b5">[6]</ref>, and OOD detection <ref type="bibr" target="#b9">[10]</ref>.</p><p>Denoising Diffusion Probabilistic Models (DDPMs) <ref type="bibr" target="#b13">[14]</ref> are generative models that work by gradually adding noise to an input image through a forward diffusion process followed by gradually removing noise using a trained neural network in the backward diffusion process <ref type="bibr" target="#b31">[32]</ref>. To guide the generative process of a diffusion model (DM), previous work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> condition the DDPMs on taskspecific conditioning. In image-to-image translation tasks like super-resolution, colourization, etc., previous papers <ref type="bibr" target="#b23">[24]</ref> condition the model by concatenating a resized or grayscale version of the input image to the noised image. This concatenation is unsuitable for reconstruction-based OOD detection as the model will generate similar images for ID and OOD samples. In the context of OOD detection using DMs, previous works <ref type="bibr" target="#b9">[10]</ref> have trained unconditional DDPMs and, during inference, sampled using a Pseudo Linear Multi Step (PLMS) <ref type="bibr" target="#b15">[16]</ref> sampler for varying noise levels. However, their approach generates 5500 samples to detect OOD samples for each input image which is time-consuming and impractical for settings where shorter inference times are needed. AnoDDPM <ref type="bibr" target="#b28">[29]</ref> utilises simplex noise rather than Gaussian noise to corrupt the image (t=250 rather than t=1000) for anomaly detection. However, this approach requires data specific tuning, and is outperformed by <ref type="bibr" target="#b9">[10]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dual Conditioned Diffusion Models</head><p>Diffusion models are generative models that rely on two Markov processes known as forward and backward diffusion <ref type="bibr" target="#b13">[14]</ref>. To improve efficiency during training and inference, forward and backward diffusion is applied to the latent space <ref type="bibr" target="#b21">[22]</ref>. Autoencoder (AE = E + D) is pretrained separately on ID heart data and can successfully reconstruct the input heart images (SSIM=0.956). The latent variable z 0 is obtained by passing an input image x 0 through a pretrained encoder E. Given the latent vector z 0 and a fixed variance schedule <ref type="bibr" target="#b13">[14]</ref> {β t ∈ (0, 1)} T t=1 , the forward diffusion process, defined by Eq. 1, gradually adds Gaussian noise to z 0 to give a noised latent vector z t where α t = 1β t and ᾱt = t i=1 α i :</p><formula xml:id="formula_0">q(z t |z 0 ) = N (z t ; √ ᾱt z 0 , (1 -ᾱt )I)<label>(1)</label></formula><p>In backward diffusion, we aim to reverse the forward diffusion process and predict z t-1 given z t . To predict (z t-1 |z t ), we train a denoising U-Net <ref type="bibr" target="#b13">[14]</ref> denoted as θ (z t , t, d 0 ) that takes the current timestep t, noised latent vector z t and the dual conditioning embedding vector d 0 as input and predicts the noise at timestep t as shown in Eq. 2.</p><formula xml:id="formula_1">z t-1 = N (z t-1 ; 1 √ α t z t - 1 -α t √ 1 -ᾱt θ (z t , t, d 0 ) , (1 -ᾱt )I)<label>(2)</label></formula><p>The dual embedding vector d 0 is obtained by combining IDCC (f cls ) and LIFC (f img ) vectors, which we explain in Sect. 3.2. The output z t-1 is again input to θ . This process is repeated until z 0 is obtained. The final model optimisation objective is given by Eq. 3 where is the original noise added during the forward diffusion process.</p><formula xml:id="formula_2">L DCDM := E E(x), ∼N (0,1),t -θ (z t , t, d 0 )<label>2 2</label></formula><p>(3)</p><p>Once we obtain z 0 from the backward diffusion process, it is passed on to the decoder D and mapped back to the pixel space to give generated image x 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual Conditioning Mechanism</head><p>Image features and in-distribution class information are utilised in our proposed dual conditioning mechanism. This guides the DCDM to generate images that are spatially and semantically similar to the input image for in-distribution samples and dissimilar for OOD samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Image Feature Conditioning (LIFC):</head><p>The image conditioning dictates the desired appearance of generated images in terms of shape and texture.</p><p>In our model, we use the features extracted by a pretrained encoder for conditioning. Empirically, we use the same encoder E as our feature extractor to obtain latent feature vector z 0 as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, the input image of dimension 224 × 224 × 3 is passed through the encoder E and a feature map with the size of 7 × 7 × 128 is obtained which is followed by global average pooling (GAP) resulting in a feature vector (f img ) with dimension 128. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Attention Guidance:</head><p>To integrate the dual-conditioning guidance into the diffusion model, we use a cross-attention <ref type="bibr" target="#b26">[27]</ref> mechanism inside the denoising U-Net rather than just concatenation <ref type="bibr" target="#b23">[24]</ref> as it is more effective <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref> and allows condition diffusion models on various input modalities <ref type="bibr" target="#b21">[22]</ref>. Our LIFC and IDCC are first concatenated to give a feature vector with a dimension of 256. This acts as a side input to each UNet block. The features from the UNet block and the conditional features are fused by cross-attention and serve as input to the following UNet block as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. For more details,regarding crossattention block refer to Rombach et al. <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">In-Distribution Classifier</head><p>The in-distribution classifier (CFR) serves two main functions. First, it provides labels for the class conditioning during inference; second, it is utilized as a feature extractor for calculating the OOD score.</p><p>Inference Class Guidance. IDCC requires in-distribution class information to generate the class conditional embedding. However, class information is only available during training. To obtain class information during inference, we separately train a ConvNext CNN based classifier (accuracy = 88%) on the indistribution data and use its predictions as the class information. During inference, the input image x 0 is passed through the classifier, and the predicted label is used to generate the class embedding by feeding to the label encoder as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Moreover, as the classifier is only trained on in-distribution data, it classifies an OOD sample to an in-distribution class. The classifier's prediction is utilised by the DCDM and it tries to generate an image belonging to in-distribution class for the OOD samples. This reduces the structural and semantic similarity between the input and the generated image, as demonstrated by our qualitative results (Fig. <ref type="figure" target="#fig_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature-Based OOD Detection</head><p>To evaluate the performance of the DCDM, the cosine similarity between features of the input image x 0 and the generated image x 0 from the in-distribution classifier is calculated and is referred as an OOD score where f 0 and f 0 are the features of x 0 and x 0 , respectively:</p><formula xml:id="formula_3">OOD score = sim(f 0 , f 0 ) = f 0 • f 0 f 0 2 f 0 2 , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>An input image x 0 is classified as in-distribution (ID) or OOD based on Eq. 5 where τ is a pre-defined threshold and y pred is the prediction of our feature-based OOD detection algorithm.</p><formula xml:id="formula_5">y pred = 0(ID) if OOD score &gt; τ 1 (OOD) otherwise (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Dataset and Implementation. For our experiments, we utilized a fetal ultrasound dataset of 359 subject videos that were collected as part of the PULSE project <ref type="bibr" target="#b6">[7]</ref>. The in-distribution dataset consisted of 5 standard heart views (3VT, 3VV, LVOT, RVOT, and 4CH), while the out-of-distribution dataset comprised of three non-heart anatomies -fetal head, abdomen, and femur. The original images were of size 1008 × 784 pixels and were resized to 224 × 224 pixels.</p><p>To train the models, we randomly sampled 5000 fetal heart images and used 500 images for evaluating image generation performance. To test the performance of our final model and compare it with other methods, we used an held-out dataset of 7471 images, comprising 4309 images of different heart views and 3162 images (about 1000 for each anatomy) of out-of-distribution classes. Further details about the dataset are given in Supp. Fig. <ref type="figure" target="#fig_2">2</ref> and<ref type="figure" target="#fig_3">3</ref>.</p><p>All models were trained using PyTorch version 1.12 with a Tesla V100 32 GB GPU. During training, we used T=1000 for noising the input image and a linearly increasing noise schedule that varied from 0.0015 to 0.0195. To generate samples from our trained model, we used DDIM <ref type="bibr" target="#b25">[26]</ref> sampling with T=100. All baseline models were trained and evaluated using the original implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>We evaluated the performance of the dual-conditioned diffusion models (DCDMs) for OOD detection by comparing them with two current state-ofthe-art unsupervised reconstruction-based approaches and one likelihood-based approach. The first baseline is Deep-MCDD <ref type="bibr" target="#b14">[15]</ref>, a likelihood-based OOD detection method that proposes a Gaussian discriminant-based objective to learn class conditional distributions. The second baseline is ALOCC <ref type="bibr" target="#b22">[23]</ref> a GAN-based model that uses the confidence of the discriminator on reconstructed samples to detect OOD samples. The third baseline is the method of Graham et al. <ref type="bibr" target="#b9">[10]</ref>, where they use DDPM <ref type="bibr" target="#b13">[14]</ref> to generate multiple images at varying noise levels for each input. They then compute the MSE and LPIPS metrics for each image compared to the input, convert them to Z-scores, and finally average them to obtain the OOD score.</p><p>Quantitative Results. The performance of the DCDM, along with comparisons with the other approaches, are shown in the input image and belonging to the same heart view for ID samples while structurally diverse heart views for OOD samples. In Fig. <ref type="figure" target="#fig_2">2 (c</ref>) for OOD sample, even-though the confidence is high (0.68), the gap between ID and OOD classes is wide enough to separate the two.Additional qualitative results can be observed in Supp. Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Ablation experiments were performed to study the impact of various conditioning mechanisms on the model performance both qualitatively and quantitatively. When analyzed quantitatively, as shown in    separately, improves performance with an AUC of 75.27% and 77.40%, respectively. The best results are achieved when both mechanisms are used (DCDM), resulting in an 11% improvement in the AUC score relative to the unconditional model. Although there is a small margin of performance improvement between the combined model (DCDM) and the LIFC model in terms of AUC, the precision improves by 3%, demonstrating the combined model is more precise and hence the best model for OOD detection.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, the unconditional diffusion model generates a random heart view for a given input for both in-distribution and OOD samples. The IDCC guides the model to generate a heart view according to the in-distribution classifier (CFR) prediction which leads to the generation of similar samples for in-distribution input while dissimilar samples for OOD input. On the other hand, LIFC generates an image with similar spatial information. However, heart views are still generated for OOD samples as the model was only trained on them. When dual-conditioning (DC) is used, the model generates images that are closer aligned to the input image for in-distribution input and high-fidelity heart views for OOD than those generated by a model conditioned on either IDCC or LIFC alone. Supp. Fig. <ref type="figure" target="#fig_0">1</ref> presents further qualitative ablations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce novel dual-conditioned diffusion model for OOD detection in fetal ultrasound videos and demonstrate how the proposed dual-conditioning mechanisms can manipulate the generative space of a diffusion model. Specifically, we show how our dual-conditioning mechanism can tackle scenarios where the in-distribution data has high inter-(using IDCC) and intra-(using LIFC) class variations and guide a diffusion model to generate similar images to the input for in-distribution input and dissimilar images for OOD input images. Our approach does not require labelled data for OOD classes and is especially applicable to challenging scenarios where the in-distribution data comprises more than one class and there is high similarity between the in-distribution and OOD classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. DCDM architecture where (a) the input image x0 is mapped to the latent vector z0 using a pretrained encoder E and forward diffusion is applied, (b) the backward diffusion process denoises the latent vector zt and the final denoised latent vector z0 is mapped to pixel space by the decoder D (c) the dual-conditioning mechanism. We obtain fimg by passing the input image x0 through the encoder E. f cls is obtained using the true label during training or predicted class label during testing.</figDesc><graphic coords="3,86,22,292,94,246,64,181,42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>-Distribution Class Conditioning (IDCC): Given an in-distribution dataset comprising n heterogeneous classes, conditioning the model only on image-level features is insufficient. Therefore we introduce an in-distribution class conditioning (IDCC) that informs the DCDM of the class of the input image and enables it to generate samples belonging to the same class for ID. A label encoder generates a unique class conditional embedding (f cls ) of dimension 128 for each class label. The class label is assigned based on the ground truth label during the training phase and to the classifier's prediction during inference, as depicted in Fig.1. In practice, we train a CNN classifier, freeze its weight and use it as our in-distribution classifier (CFR), as discussed in Sect.3.3.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative comparison of our method with (a) ALOCC generates similar images to the input for ID and OOD samples (b) Graham et al. generates any random heart view for a given input image (c) Our model generates images that are similar to the input image for ID and dissimilar for OOD samples. Classes predicted by CFR and the OOD score (τ = 0.73) are mentioned in brackets.</figDesc><graphic coords="8,119,94,56,45,218,56,137,26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative ablation study showing the effect of (a) IDCC, (b) LIFC and, (c) DC on generative results of DM. Brackets in IDCC, DC show labels predicted by CFR.</figDesc><graphic coords="8,89,52,386,54,276,79,133,63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of our model (DCDM) with reference methods</figDesc><table><row><cell>Method</cell><cell cols="4">AUC(%) F1-Score(%) Accuracy(%) Precision(%)</cell></row><row><cell cols="2">Deep-MCDD [15] 64.58</cell><cell>66.23</cell><cell>60.41</cell><cell>51.82</cell></row><row><cell>ALOCC [23]</cell><cell>57.22</cell><cell>59.34</cell><cell>52.28</cell><cell>45.63</cell></row><row><cell cols="2">Graham et al. [10] 63.86</cell><cell>63.55</cell><cell>60.15</cell><cell>50.89</cell></row><row><cell>DCDM(Ours)</cell><cell>77.60</cell><cell>74.29</cell><cell>77.95</cell><cell>73.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The GAN-based method ALOCC<ref type="bibr" target="#b22">[23]</ref> has the lowest AUC of 57.22%, which is improved to 63.86% by the method of Graham et al. and further improved to 64.58% by likelihoodbased Deep-MCDD. DCDM outperforms all the reference methods by 20%, 14% and 13%, respectively and has an AUC of 77.60%. High precision is essential for OOD detection as this can reduce false positives and increase trust in the model. DCDM exhibits a precision that is 22% higher than the reference methods while still having an 8% improvement in F1-Score. Qualitative results are shown in Fig.2. Visual comparisons show ALOCC generates images structurally similar to input images for indistribution and OOD samples. This makes it harder for the ALOCC model to detect OOD samples. The model of Graham et al. generates any random heart view for a given image as a DDPM is unconditional, and our in-distribution data contains multiple heart views. For example, given a 4CH view as input, the model generates an entirely different heart view. However, unlike ALOCC, the Graham et al. model generates heart views for OOD samples, improving OOD detection performance. DCDM generates images with high spatial similarity to</figDesc><table><row><cell>Qualitative Results.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note><p>, the unconditional model has the lowest AUC of 69.61%. Incorporating the IDCC guidance or LIFC</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of different conditioning mechanisms of DCDM.</figDesc><table><row><cell>Unconditional</cell><cell>68.16</cell><cell>58.44</cell><cell>69.61</cell></row><row><cell cols="2">In-Distribution Class Conditioning 74.39</cell><cell>66.12</cell><cell>75.27</cell></row><row><cell cols="2">Latent Image Feature Conditioning 77.02</cell><cell>70.02</cell><cell>77.40</cell></row><row><cell>Dual Conditioning</cell><cell>77.95</cell><cell>73.34</cell><cell>77.60</cell></row></table><note><p><p>Method</p>Accuracy (%) Precision (%) AUC (%)</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by the <rs type="grantName">InnoHK-funded Hong Kong Centre for Cerebro-cardiovascular Health Engineering</rs> (COCHE) Project 2.1 (Cardiovascular risks in early life and fetal echocardiography), the <rs type="funder">UK EPSRC (Engineering and Physical Research Council)</rs> <rs type="grantName">Programme Grant</rs> <rs type="grantNumber">EP/T028572/1</rs> (VisualAI), and a <rs type="grantName">UK EPSRC Doctoral Training Partnership award</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wP3BhNR">
					<orgName type="grant-name">InnoHK-funded Hong Kong Centre for Cerebro-cardiovascular Health Engineering</orgName>
				</org>
				<org type="funding" xml:id="_hMjCPDz">
					<idno type="grant-number">EP/T028572/1</idno>
					<orgName type="grant-name">Programme Grant</orgName>
				</org>
				<org type="funding" xml:id="_sFngheN">
					<orgName type="grant-name">UK EPSRC Doctoral Training Partnership award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeing what a GAN cannot generate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4502" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised detection of lesions in brain MRI using constrained adversarial auto-encoders</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04972</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<title level="m">Waic, but why? Generative ensembles for robust anomaly detection</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning confidence for out-of-distribution detection in neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04865</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transforming obstetric ultrasound into data science using eye tracking, voice recording, transducer motion and ultrasound video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drukker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14109</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial vulnerability of powerful near out-of-distribution detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07012</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring the limits of out-of-distribution detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7068" to="7081" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Denoising diffusion models for out-of-distribution detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Pinaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Tudosiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07740</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Guénais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vamvourellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06096</idno>
		<title level="m">Bacoun: Bayesian classifers with out-of-distribution uncertainty</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-ofdistribution examples in neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Prompt-to-prompt image editing with cross attention control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mokady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.01626</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-class data description for out-of-distribution detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1362" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09778</idno>
		<title level="m">Pseudo numerical methods for diffusion models on manifolds</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Attention-based conditioning methods for external knowledge integration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Margatina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03674</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sdedit: guided image synthesis and editing with stochastic differential equations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Do deep generative models know what they don</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09136</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">t know? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Rebain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lagun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10684</idno>
		<title level="m">Attention beats concatenation for conditioning neural fields</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Palette: Image-to-image diffusion models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2022 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-59050-9_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-59050-912" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Neithammer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10265</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On calibration and out-of-domain generalization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Greenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2215" to="2227" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Anoddpm: anomaly detection with denoising diffusion probabilistic models using simplex noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Schmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="650" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection via variational auto-encoder for seasonal KPIs in web applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generalized out-of-distribution detection: a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11334</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.00796</idno>
		<title level="m">Diffusion models: a comprehensive survey of methods and applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking reconstruction autoencoder-based out-of-distribution detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7379" to="7387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Step: out-of-distribution detection in the presence of limited in-distribution labeled data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="29168" to="29180" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
