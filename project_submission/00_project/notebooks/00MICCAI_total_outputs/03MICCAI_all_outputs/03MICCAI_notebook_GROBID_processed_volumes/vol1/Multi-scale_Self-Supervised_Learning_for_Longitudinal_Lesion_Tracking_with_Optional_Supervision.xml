<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision</title>
				<funder>
					<orgName type="full">National Lung Screening Trial</orgName>
					<orgName type="abbreviated">NLST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Anamaria</forename><surname>Vizitiu</surname></persName>
							<email>anamaria.vizitiu@siemens.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Siemens SRL</orgName>
								<address>
									<settlement>Advanta, Brasov</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antonia</forename><forename type="middle">T</forename><surname>Mohaiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Siemens SRL</orgName>
								<address>
									<settlement>Advanta, Brasov</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioan</forename><forename type="middle">M</forename><surname>Popdan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Siemens SRL</orgName>
								<address>
									<settlement>Advanta, Brasov</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abishek</forename><surname>Balachandran</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Digital Technology and Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florin</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Digital Technology and Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Digital Technology and Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">96BB797D0171C89E6C8D81254AFE5B23</idno>
					<idno type="DOI">10.1007/978-3-031-43907-055.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Self-supervised learning</term>
					<term>Multi-scale</term>
					<term>Longitudinal lesion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Longitudinal lesion or tumor tracking is an essential task in different clinical workflows, including treatment monitoring with followup imaging or planning of re-treatments for radiation therapy. Accurately establishing correspondence between lesions at different timepoints, recognizing new lesions or lesions that have disappeared is a tedious task that only grows in complexity as the number of lesions or timepoints increase. To address this task, we propose a generic approach based on multi-scale self-supervised learning. The multi-scale approach allows the efficient and robust learning of a similarity map between multi-timepoint image acquisitions to derive correspondence, while the self-supervised learning formulation enables the generic application to different types of lesions and image modalities. In addition, we impose optional supervision during training by leveraging tens of anatomical landmarks that can be extracted automatically. We train our approach at large scale with more than 50,000 computed tomography (CT) scans and validate it on two different applications: 1) Tracking of generic lesions based on the DeepLesion dataset, including liver tumors, lung nodules, enlarged lymph-nodes, for which we report highest matching accuracy of 92%, with localization accuracy that is nearly 10% higher than the state-ofthe-art; and 2) Tracking of lung nodules based on the NLST dataset for which we achieve similarly high performance. In addition, we include an error analysis based on expert radiologist feedback, and discuss next steps as we plan to scale our system across more applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Longitudinal lesion or tumor tracking is a fundamental task in treatment monitoring workflows, and for planning of re-treatments in radiation therapy. Based on longitudinal imaging for a given patient it requires establishing which lesions are corresponding (i.e., same lesion, observed at different timepoints), which lesions have disappeared and which are new compared to prior scanning. This information can be leveraged to assess treatment response, e.g., by analyzing the evolution of size and morphology for a given tumor <ref type="bibr" target="#b0">[1]</ref>, but also for adaptation of (re-)treatment radiotherapy plans that take into account new tumors.</p><p>In practice, the development of automatic and reliable lesion tracking solutions is hindered by the complexity of the data (over different modalities), the absence of large, annotated datasets, and the difficulties associated with lesion identification (i.e., varying sizes, poses, shapes, and sparsely distributed locations). In this work, we present a multi-scale self-supervised learning solution for lesion tracking in longitudinal studies using the capabilities of contrastive learning <ref type="bibr" target="#b8">[9]</ref>. Inspired by the pixel-wise contrastive learning strategy introduced in <ref type="bibr" target="#b4">[5]</ref>, we choose to learn pixel-wise feature representations that embed consistent anatomical information from unlabeled (i.e., without lesion-related annotations) and unpaired (i.e., without the use of longitudinal scans) data, overcoming barriers to data collection. To increase the system robustness and emulate the clinician's reading strategies, we propose to use multi-scale embeddings to enable the system to progressively refine the fine-grained location. In addition, as imaging offers contextual information about the human body that is naturally consistent, we design the model to benefit from biologically-meaningful points (i.e., anatomical landmarks). The reasoning behind this strategy is that simple data augmentation methods cannot faithfully model inter-subject variability or possible organ deformations. Hence, we ensure the spatial coherence of the tracked lesion location using well-defined anatomical landmarks.</p><p>Our proposed method brings two elements of novelty from a technical point of view: (1) the multi-scale approach for the anatomical embedding learning and (2) a positive sampling approach that incorporates anatomically significant landmarks across different subjects. With these two strategies, the goal is to ensure a high degree of robustness in the computation of the lesion matching across different lesion sizes and varying anatomies. Furthermore, a significant focus and contribution of our research is the experimental study at a very large scale: we (1) train a pixel-wise self-supervised system using a very large and diverse dataset of 52,487 CT volumes and (2) evaluate on two publicly available datasets. Notably, one of the datasets, NLST, presents challenging cases with 68% of lesions being very small (i.e., radius &lt; 5 mm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>The problem of lesion tracking in longitudinal data is typically divided into two steps: (1) detection of lesions and (2) tracking the same lesion over multiple time points. Classical methods to solve this problem rely on image registration, where tracking is performed via image alignment and rule-based correspondence matching <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>. These approaches are difficult to optimize, especially when scaling across different body regions and fields of view. Appearance-based trackers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> adopt a different strategy by projecting lesions detected beforehand with dedicated detectors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> onto a representation space and employing nearest neighbor analysis. One recent approach, Deep Lesion Tracker (DLT) <ref type="bibr" target="#b7">[8]</ref>, integrates both strategies to perform appearance-based recognition under anatomical constraints. As a more direct matching approach, Yan et al. <ref type="bibr" target="#b4">[5]</ref> uses a self-supervised anatomical embedding model (SAM) to create semantic embeddings for each image pixel, avoiding the detection step. Training exclusively on augmented paired data prevents SAM from accurately representing anatomical changes and deformations that occur over time. This can influence the contextual information of a pixel, which in turn impacts the pixel-wise embeddings on which the similarity-based tracker depends. To overcome this, we propose to train a pixel-wise multi-scale embedding model that accounts for anatomical similarity among different subjects, making the embeddings more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Let I 1 (i.e., template or baseline image) and I 2 (i.e., query or follow-up image) be two 3D-CT scans acquired at time t 1 and t 2 , respectively, Additionally, let p 1 and p 2 denote the point of interest (i.e., the lesion center) in both images. The problem of lesion tracking can be formulated as finding the optimal transformation that maps p 1 to its corresponding location, p 2 , in I 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Stage</head><p>Let D = {X 1 , X 2 , ..., X N } be a set of N unpaired and unlabeled 3D-CT volumes.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, given an image X ∈ R d×h×w from the training dataset D, we randomly select two overlapping 3D patches (anchor and query), namely X a and X q . To create synthetic paired data that mimics appearance changes across different images, we apply random data augmentation (i.e., random spatial and intensity-related transformations) to the content of X a and X q . We implement a similar augmentation strategy to that described in <ref type="bibr" target="#b4">[5]</ref>. Given X a and X q , we use an embedding extraction model to construct a hierarchy of multi-scale semantic embeddings for each image pixel, labeled F a and F q respectively. The embedding at ith scale, 1 ≤ i ≤ s, is denoted as F i a and F i q and is represented as a 4D feature map, with an embedding vector of length L associated with each pixel.</p><p>Given the nature of contrastive learning, the sampling strategy (extracting negative and positive pixel pairs from augmented 3D paired patches) is essential to achieving discriminative pixel-wise embeddings. We arbitrary sample n pos positive pixel pairs from the overlapping area of X a and X q , denoted by a + = {a + 1 , ..., a + j ..., a + npos }, q + = {q + 1 , ..., q + j , ..., q + npos }, 1 ≤ j ≤ n pos . To further enhance embeddings, 10% of the positive pixel pairs are derived from biologically-meaningful points across different volumes in the batch. We use data-driven models <ref type="bibr" target="#b13">[14]</ref> to extract 37 anatomical landmarks, such as the top right lung, suprasternal notch, tracheal bifurcation, etc. Similar to <ref type="bibr" target="#b4">[5]</ref>, for each positive pixel pair (a + j , q + j ), we select n neg hard and diverse negative pixels, denoted by</p><formula xml:id="formula_0">h -= {h - 1 , ..., h - k , ..., h - nneg }, 1 ≤ k ≤ n neg .</formula><p>Next, at each scale, we extract the embedding vectors for positive and negative pixel pairs from F i a , F i q , guided by the corresponding locations, a + , q + , h -, which are downsampled to match the scale. We denote the positive embeddings at ith scale at pixel location a + j , q + j as f a i j , f q i j ∈ R L . Similarly, we denote the negative embeddings at pixel location h - k associated to a positive positive pixel pair (a + j , q + j ) as f i jk ∈ R L . We use L2-norm to normalize the embedding vectors before the loss computation. We use pixel-wise InfoNCE loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref> to enhance the similarity among similar pixels (i.e., positive pairs of pixels) and decrease the similarity among dissimilar pixels (i.e., negative pairs of pixels). Correspondingly, we set the contrastive loss at the ith scale:</p><formula xml:id="formula_1">L i = - npos j=1 log exp(f a i j • f q i j /τ ) exp(f a i j • f q i j /τ ) + nneg k=1 exp(f a i j • f i jk /τ ) , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where τ = 0.5 is a temperature parameter. The final loss is then calculated as the average of all these individual losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference Stage</head><p>Let X a be a 3D-CT volume template with an input point of interest p a ∈ X a , and X q a corresponding query 3D-CT volume. The first step is to project the image X a into a multi-scale feature space, creating a hierarchy of multi-scale semantic embeddings F a for each pixel in the image (i.e., a 4D feature map).</p><p>Next, we follow a similar process for the query image X a and acquire the pixellevel embeddings F q .</p><p>To measure the similarity between the embeddings of the input X a at the point of interest p a and the query embeddings F q , we compute cosine similarity maps at each scale:</p><formula xml:id="formula_3">S i = F i a (p a ) • F i q F i a (p a ) 2 • F i q 2</formula><p>(2)</p><p>Finally, we combine the multi-scale similarity maps through summation and select the voxel with the highest similarity as the matching point in the query volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Setup</head><p>Datasets: We train the universal and fine-grained anatomical point matching model using an in-house CT dataset (VariousCT). The training dataset contains 52,487 unlabeled 3D CT volumes capturing various anatomies, including chest, head, abdomen, pelvis, and more.</p><p>The evaluation is based on two datasets, the publicly released Deep Longitudinal Study (DLS) dataset <ref type="bibr" target="#b7">[8]</ref> and the National Lung Screening Trial (NLST) dataset <ref type="bibr" target="#b11">[12]</ref>. The DLS dataset is a subset of the DeepLesion <ref type="bibr" target="#b10">[11]</ref> medical imaging dataset, containing 3891 pairs of lesions with information on their location and size. The dataset covers various types of lesions across different organs. We follow the official data split for DLS dataset and perform evaluation on the testing dataset which comprises 480 lesion pairs. For NLST, we randomly selected a subset of 1045 test images coming from 420 patients with up to 3 studies. A certified radiologist annotated the testing data by identifying the location and size of the pulmonary nodules, resulting in a total of 825 paired annotations. We evaluate lesion tracking in both directions, from baseline to follow-up and from follow-up to baseline <ref type="bibr" target="#b7">[8]</ref>. This results in a total of 960 and 1650 testing lesion pairs in DLS and NLST test sets, respectively. The isotropic resolution of all CT volumes is adjusted to 2mm through bilinear interpolation.</p><p>System Training: Our learning model is implemented in PyTorch and uses the TorchIO library <ref type="bibr" target="#b12">[13]</ref> for medical data manipulation and augmentation.</p><p>We employ a U-Net-based encoder-decoder architecture <ref type="bibr" target="#b1">[2]</ref> that utilizes an inflated 3D ResNet-18 <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> as its encoder, which extends all 2D convolutions Table <ref type="table">1</ref>. Comparison between the proposed solution and several state-of-the-art approaches (reference results are from <ref type="bibr" target="#b7">[8]</ref>). The exact same test set was used to compute the performance of each approach listed in the table; however, we retrained only SAM. in the standard ResNet to 3D convolutions and allows the use of pre-trained ImageNet weights. The multi-scale embedding model employs s = 5 scales, and the embedding length is fixed at L = 128 for each scale. Convolution with a stride of (2, 2, 2) is used to reduce the feature map size at the first and fifth levels, while a stride of (1, 2, 2) is employed for intermediary levels 2 to 4. The U-Net decoder uses a convolution layer with a 3 × 3 × 3 kernel after every up-sampling layer to generate the final cascade of feature embeddings. The model is trained with AdamW optimizer <ref type="bibr" target="#b5">[6]</ref> for 64 epochs using an early stopping strategy with a patience of 5 epochs, a batch size of 8 augmented 3D paired patches of 32 × 96 × 96, and a learning rate of 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>For data augmentation, we apply random cropping, scaling, rotation, and Gaussian noise injections. A windowing approach that covers the intensity ranges of lungs and soft tissues is used to scale CT intensity values to [-1, 1]. The sampling hyperparameters consist of 100 positive pixel pairs (n pos = 100), 100 hard negative pixel pairs, and 200 diverse negative pixel pairs (n neg = 300).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics:</head><p>We use mean Euclidean distance (MED) to measure the distance between predicted lesion center and ground truth, and the center point matching accuracy (i.e., percentage of accurately matched lesions given the annotated lesion radius), denoted with CPM@Radius. For lesions of large sizes, we set a maximum distance limit of 10 mm as acceptance criteria <ref type="bibr" target="#b7">[8]</ref>, denoted with CPM@10 mm. The NLST testing dataset has a distinctive feature wherein nodules are relatively small, 68% of annotated lesions have a radius of less than 5 mm (compared to 6% in DLS dataset). To ensure that such small nodules are not missed during evaluation, we relax the minimum distance requirement and consider a distance of 6 mm as a permissible matching error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>For the lesion tracking task on DLS dataset, we quantitatively compare our system against existing trackers in Table <ref type="table">1</ref>. These include the Deep Lesion Tracker (DLT) and its variants <ref type="bibr" target="#b7">[8]</ref>, as well as registration-based trackers <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref> and appearance-based trackers via detector learning <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Given the clear superiority of approach <ref type="bibr" target="#b4">[5]</ref> compared to all reference solutions, we focus on achieving a direct comparison against SAM <ref type="bibr" target="#b4">[5]</ref>. Hence, for performance comparison against self-supervised anatomical embedding tracker, we retrain SAM <ref type="bibr" target="#b4">[5]</ref> with images from VariousCT dataset. Our method achieves a matching accuracy of 91.87%, that is 1.84% higher than SAM and 5.74% higher than DLT. To confirm the significance of the improvement achieved by our method compared to SAM <ref type="bibr" target="#b4">[5]</ref>, we conduct a paired t-test for statistical analysis and show that the improvement is statistically significant (p-value &lt; 10 -6 ). Compared to the self-supervised version of DLT, the difference in performance is significantly greater, the proposed systems outperforms DLT-SSL by more than 10%. When imposing a maximum distance limit of 10 mm between the ground truth and prediction, our method increases performance by 1.46%, showing the importance of the multi-scale approach in lesion On the NLST dataset, our proposed method obtains a center point matching accuracy of 92.12% (Table <ref type="table" target="#tab_1">2</ref>). In the case of longitudinal lung nodule tracking (Fig. <ref type="figure" target="#fig_2">3</ref>), it is more frequent to observe significant changes in size and density. As our system relies on the concept of anatomical embedding matching, the most substantial errors in lesion matching for our system occur when there are significant pathological distortions that deviate greatly from one timepoint to another. Examples of such cases are depicted in Fig. <ref type="figure" target="#fig_3">4</ref>, based on expert radiologist feedback.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In conclusion, this paper presents an effective method for longitudinal lesion tracking based on multi-scale self-supervised learning. The method is generic, it does not require expert annotations or longitudinal data for training and can generalize to different types of tumors/organs/modalities. The multi-scale approach ensures a high degree of robustness and accuracy for small lesions. Through large-scale experiments and validation on two longitudinal datasets, we highlight the superiority of the proposed method in comparison to state-of-theart. We found that adopting a multi-scale approach (instead of the global/local approach as proposed in <ref type="bibr" target="#b4">[5]</ref>) can lead to embeddings that better capture the anatomical location and are able to handle lesions that vary in size or appearance at different scales. Moreover, the changes proposed in this work help to alleviate the confusion caused by left-right body symmetries (e.g., the apices of the lungs). This effect challenged the tracking of small nodules in the lungs using <ref type="bibr" target="#b4">[5]</ref>. Our future work aims to enhance the matching accuracy by examining the implications of correlation magnitude, conducting robustness studies on slight variations in tracking initialization, and implementing a more advanced fusion strategy for the multi-scale similarity maps. In addition, we aim to expand to more applications, e.g., treatment monitoring for brain cancer using MRI.</p><p>Disclaimer: The concepts and information presented in this paper/presentation are based on research results that are not commercially available. Future commercial availability cannot be guaranteed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overview of the proposed multi-scale self-supervised learning system. During training, we randomly extract positive samples (optionally, include same anatomical landmarks from different volumes), hard-negative samples, and diverse negative samples of pixels from augmented 3D paired patches. During inference, the extracted embeddings are used to generate a cascade of cosine similarity maps that initially locate the corresponding location in a follow-up image within a larger area and subsequently improve the matching accuracy through gradual refinement.</figDesc><graphic coords="3,61,59,54,53,271,72,117,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.Examples of lesion matching results on the DLS testing dataset. We denote the ground-truth points using green markers in both the baseline and follow-up images, whereas the predicted points are indicated by red markers. To illustrate the extent of the lesions, we also display the annotated bounding boxes on the follow-up images. For more clarity, we show only the axial view. (Color figure online)</figDesc><graphic coords="7,58,98,406,73,334,84,123,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.Example case from the test set, highlighting the progression of a lung nodule over three timepoints: t0, t1 and t3. Our system was robust to the axial rotations of the scans and the increasing size of the nodule and correctly established the correspondence.</figDesc><graphic coords="8,45,30,362,30,333,67,165,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of lesion matching results on clinically challenging cases from NLST testing dataset: (a) bronchiectasis with mucus plugging adjacent to the nodule, (b) spiculated nodule in a setting of interstitial lung disease, (c), (d) small nodule progressed and increased significantly in size. The green and red markers denote the ground-truth and predicted lesion location. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on the NLST dataset related to the tracking of lung nodules.</figDesc><table><row><cell cols="2">Method CPM@</cell><cell>CMP@</cell><cell>MEDx</cell><cell>MEDy</cell><cell>MEDz</cell><cell>MED(mm)</cell></row><row><cell></cell><cell>10 mm</cell><cell>Radius</cell><cell>(mm)</cell><cell>(mm)</cell><cell>(mm)</cell><cell></cell></row><row><cell>Ours</cell><cell>90.05</cell><cell>92.12</cell><cell>2.0 ± 2.6</cell><cell>2.2 ± 3.8</cell><cell>2.7 ± 4.8</cell><cell>4.9 ± 6.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The authors thank the <rs type="institution">National Cancer Institute</rs> for access to NCI's data collected by the <rs type="funder">National Lung Screening Trial (NLST)</rs>. The statements contained herein are solely those of the authors and do not represent or imply concurrence or endorsement by <rs type="institution">NCI</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">New response evaluation criteria in solid tumours: revised RECIST guideline (version 1.1)</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Eisenhauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Cancer</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="247" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SAM: self-supervised learning of pixel-wise anatomical embeddings in radiological images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2658" to="2669" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv: Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep lesion tracker: monitoring lesions in 4D longitudinal imaging studies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="15154" to="15164" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DeepLesion: automated deep mining, categorization and detection of significant radiology image findings using largescale clinical lesion annotations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The National Lung Screening Trial: overview and study design</title>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="page" from="243" to="253" />
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>National Lung Screening Trial Research Team</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TorchIO: a python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Prog. Biomed</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page">106236</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-scale deep reinforcement learning for real-time 3Dlandmark detection in CT scans</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="176" to="189" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SimpleElastix: a user-friendly, multi-lingual library for medical image registration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Marstal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="574" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An unsupervised learning model for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9252" to="9260" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from multiple datasets with heterogeneous and partial labels for universal lesion detection in CT</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2759" to="2770" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep volumetric universal lesion detection using light-weight pseudo 3D convolution and surface point regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep lesion graphs in the wild: relationship learning and organization of significant radiology image findings in a diverse large-scale lesion database</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="9261" to="9270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Holistic and comprehensive annotation of clinically significant findings on diverse CT images: learning from radiology reports and label ontology</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sandfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8515" to="8524" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MRF-based deformable registration and ventilation estimation of lung CT</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1239" to="1248" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
