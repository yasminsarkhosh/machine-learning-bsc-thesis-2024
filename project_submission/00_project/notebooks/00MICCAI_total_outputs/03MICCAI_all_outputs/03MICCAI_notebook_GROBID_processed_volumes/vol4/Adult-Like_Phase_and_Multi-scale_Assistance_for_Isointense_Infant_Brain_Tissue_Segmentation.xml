<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation</title>
				<funder ref="#_6DbSn4y">
					<orgName type="full">The Key R&amp;D Program of Guangdong Province, China</orgName>
				</funder>
				<funder ref="#_ktqxSpX #_U8TZDrR">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_bYs4NfA">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
					<orgName type="abbreviated">STCSM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiameng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feihong</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Northwest University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaicong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mianxin</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuyan</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute of Artificial Intelligence and Robotics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Shanghai United Imaging Intelligence Co. Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Shanghai Clinical Research and Trial Center</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="56" to="66"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E46BAEA7AF9035D33D3DAFF723A9B00C</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Isointense infant tissue segmentation</term>
					<term>Semantic-preserved GAN</term>
					<term>Anatomical guidance segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Precise brain tissue segmentation is crucial for infant development tracking and early brain disorder diagnosis. However, it remains challenging to automatically segment the brain tissues of a 6-month-old infant (isointense phase), even for manual labeling, due to inherent ongoing myelination during the first postnatal year. The intensity contrast between gray matter and white matter is extremely low in isointense MRI data. To resolve this problem, in this study, we propose a novel network with multi-phase data and multi-scale assistance to accurately segment the brain tissues of the isointense phase. Specifically, our framework consists of two main modules, i.e., semantics-preserved generative adversarial network (SPGAN) and Transformer-based multi-scale segmentation network (TMSN). SPAGN bi-directionally transfers the brain appearance between the isointense phase and the adult-like phase. On the one hand, the synthesized isointense phase data augments the isointense dataset. On the other hand, the synthesized adult-like images provide prior knowledge to the ambiguous tissue boundaries in the paired isointense phase data. TMSN integrates features of multi-phase image pairs in a multi-scale manner, which exploits both the adult-like phase data, with much clearer boundaries as structural prior, and the surrounding tissues, with a larger receptive field to assist the isointense data tissue segmentation. Extensive experiments on the public dataset show that our proposed framework achieves significant improvement over the state-ofthe-art methods quantitatively and qualitatively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Charting brain development during the first postnatal year is crucial for identifying typical and atypical changes of brain tissues, i.e., grey matter (GM), white matter (WM), and cerebrospinal fluid (CSF), which can be utilized for diagnosis of autism and other brain disorders <ref type="bibr" target="#b15">[16]</ref>. Early identification of autism is very important for effective intervention, and currently, an accurate diagnosis can be achieved as early as 12 months old <ref type="bibr" target="#b20">[21]</ref>. Advancing diagnosis at an earlier stage could provide more time for greatly improving intervention <ref type="bibr" target="#b16">[17]</ref>. Unfortunately, it remains challenging in obtaining a precise tissue map from the early infancy structural MRI (sMRI) data, such as T1 or (and) T2.</p><p>The acquisition of a mass of tissue maps relies on automatic segmentation techniques. However, the accuracy cannot be guaranteed, and the most difficult case is the segmentation of the isointense phase (6-9 months) data <ref type="bibr" target="#b17">[18]</ref>, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, where the intensity distributions of GM and WM are highly overlapped compared with the adult-like phase (≥ 9 months) <ref type="bibr" target="#b17">[18]</ref> (clear boundaries between GM and WM). Such a peculiarity incurs two challenges to conventional segmentation methods: i ) lack of well-annotated data for training automatic segmentation algorithms; and ii ) GM and WM demonstrate a low contrast result in limited anatomical information for accurately distinguishing GM and WM, even with enough high-quality annotations as training data.</p><p>Conventional methods can mainly be classified into two categories: 1) the registration-based methods and 2) the learning-based methods, as introduced below. The registration-based methods usually utilize a single previous-defined atlas, for cross-sectional data, or a sequence of atlases, for longitudinal dataguided methods, to indirectly obtain the tissue maps <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Those methods require a substantial number of atlases and follow-up adult-like images to guide the segmentation process, which is known for low accuracy in segmenting infant tissues due to the rapid developmental changes in early life. The learning-based methods have gained significant prominence for individualized segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>, which is also exploited to segment isointense brain tissues. For example, Nie et al. <ref type="bibr" target="#b8">[9]</ref> used fully convolutional networks (FCNs) to directly segment tissue from MR images, and Wang et al. <ref type="bibr" target="#b22">[23]</ref> employed an attention mechanism for better isointense infant brain segmentation. Also, Bui et al. proposed a 3D CycleGAN <ref type="bibr" target="#b1">[2]</ref> to utilize the isointense data to synthesize adult-like data, which can be employed to make the segmentation of isointense data more accurate.</p><p>In this work, we propose a novel Transformer-based framework for isointense tissue segmentation via T1-weighted MR images, which is composed of two stages: i.e., i ) the semantics-preserved GAN (SPGAN), and ii ) Transformerbased multi-scale segmentation network (TMSN). Specifically, SPGAN is a bidirectional synthesis model that enables both the isointense data synthesis using adult-like data and vice verse. The isointense structural MRI data is paired with the segmented tissue maps for extending the training dataset. Additionally, the synthesized adult-like data from isointense infant data are adopted to assist segmentation in TMSN. TMSN incorporates a Transformer-based cross-branch fusing (TCF) module which exploits supplementary tissue information from a patch with a larger receptive field to guide the local segmentation. Extensive experiments are conducted on the public dataset, National Database for Autism Research (NDAR) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>, and the results demonstrate that our proposed framework outperforms other state-of-the-art methods, particularly in accurately segmenting ambiguous tissue boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We propose a Transformer-based framework for isointense tissue segmentation, which includes two main modules: i ) the semantic-preserved GAN (SPGAN), and ii ) the Transformer-based multi-scale segmentation network (TMSN). An overview of the framework is provided in Fig. <ref type="figure" target="#fig_1">2</ref>, SPGAN is designed to synthesize isointense or adult-like data to augment the training dataset and guide segmentation with anatomical information (Sect. 2.1). TMSN is designed for isointense tissue segmentation by utilizing the tissue information from the synthesized adult-like data (Sect. 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantics-Preserved Multi-phase Synthesis</head><p>Motivated by the effectiveness of classical unpaired image-to-image translation models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, we propose a novel bi-directional generative model that synthesizes both isointense and adult-like data while introducing essential anatomical constraints to maintain structural consistency between the input images and the corresponding synthesized images. As shown in Fig. <ref type="figure" target="#fig_1">2</ref> (b.1), the SPGAN consists of two generators, G A2I and G I2A , along with their corresponding discriminators D A2I and D I2A . The adult-like infant images are aligned to the isointense infant images using affine registration methods <ref type="bibr" target="#b0">[1]</ref>, before training SPGAN. Isointense Phase Synthesis. The generator G A2I is designed to synthesize isointense data from adult-like ones, which is constructed based on the 3D UNet <ref type="bibr" target="#b2">[3]</ref> model, consisting of 4 encoder layers and 4 decoder layers. Inspired by GauGAN <ref type="bibr" target="#b9">[10]</ref>, we incorporate the spatial adaptive denormalization (SPADE) into G A2I to maintain structural consistency between the synthesized isointense brain images and corresponding adult-like brain images, as shown in Fig. <ref type="figure" target="#fig_1">2 (b.</ref>2). SPADE module takes the adult-like tissue maps to perform feature normalization during synthesis stage of isointense phase data. Thus, the generated isointense brain images can obtain consistent tissue structures with adult-like brain images. Specifically, X in with a spatial resolution of (H × W × D) represents the input feature map to each layer, consisting of a batch of N samples and C channels. For an arbitrary n-th sample at the c-th channel, the activation value associated with location (i,j,k ) is denoted as x n,c,i,j,k . The SPADE operator adopts two convolution blocks to learn the modulation maps α(M ) and β(M ) for a given tissue map M , where the first convolution block makes the number of channels in the tissue map consistent with the number of channels in the feature map. The modulated value at (i,j,k ) in each feature map is formulated as:</p><formula xml:id="formula_0">x n,c,i,j,k = α(M ) n,c,i,j,k x n,c,i,j,k -μ c σ c + β(M ) n,c,i,j,k<label>(1)</label></formula><p>where μ c and σ c denote the mean and standard deviation, respectively. Additionally, we adopt PatchGAN <ref type="bibr" target="#b24">[25]</ref> as the discriminator D A2I to provide adversary loss to train the generator G A2I .</p><p>Adult-Like Phase Synthesis. The generator G I2A is designed to synthesize adult-like infant brain images from isointense infant brain images, which is employed to provide clear structural information for identifying the ambiguous tissue boundaries in isointense brain images. To ensure the synthesized adult-like infant brain images can provide tissue information as realistic and accurate as the real images, we utilize a pre-trained adult-like brain tissue segmentation network S A (3D UNet) to preserve the structural similarity between the synthesized adult-like data and the real adult-like data and promote more reasonable anatomical structures in the synthesized images. To achieve that goal, during the training of SPGAN, we freeze the parameters of S A and adopt the mean square error (MSE) to penalize the dissimilarity between the tissue probability maps of the real and synthesized brain images (extracted by the pre-trained segmentation model S A ). The MSE loss is formulated as</p><formula xml:id="formula_1">L MSE = MSE(S A (I RAB ), S A (I CAB ))</formula><p>where I RAB and I CAB denote the real adult-like brain images and synthesized adult-like brain images synthesized by G I2A . The overall loss function of SPGAN is defined as:</p><formula xml:id="formula_2">L SP GAN = γ 1 L GI2A + γ 2 L GA2I + γ 3 L MSE + γ 4 L cycle (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where L cycle denotes the cycle consistency loss between two generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer-Based Multi-scale Segmentation</head><p>The overview of our proposed segmentation network (TMSN) is shown in Fig. <ref type="figure" target="#fig_1">2</ref> (c). In order to guide the network with anatomical prior, TMSN takes the pair of isointense and the corresponding adult-like images as input. The isointense and adult-like images are concatenated and cropped to image patches and then, they are fed into the top branch of TMSN. Due to the fact that neighboring tissues can provide additional information and greatly improve segmentation performance, hence, we also employ a multi-scale strategy by taking an image pair of the larger receptive field as the input of the bottom branch. It is important to note that the image pairs of the top and bottom branches should have the same sizes. Moreover, we fuse the features of the two branches at the same block using a novel Transformer-based cross-branch fusion (TCF) module, where the Transformer can better capture relationships across two different branches (with local and global features). Specifically, F m s and F m b denote the learned features of the two branches at the m-th stage, respectively. The TCF module treats the F m s and F m b as the input of the encoder and decoder in the multi-head Transformer to learn the relationships between the centroid tissues in the top branch and the corresponding surrounding tissues in the bottom branch and then fuses the bottom branch features with the ones of the top branch by utilizing the learned relationships. The proposed TCF module can be formulated as:</p><formula xml:id="formula_4">F m s = F m s + [Q(F m s ).(K(F m b )) ] * V (F m b )<label>(3)</label></formula><p>where Q, K, and V denote the query, key, and value in the Transformer. We take a hybrid Dice and focal loss to supervise the two segmentation branches as follows:</p><formula xml:id="formula_5">L Seg = λ 1 L s (I s , GT s ) + (1 -λ 1 )L b (I b , GT b ) (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where I s and GT s , respectively, denote the input and ground truth (GT) of the top branch, and I b and GT b represent the ones of the bottom branch. The final tissue segmentation results are obtained from the top branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metrics</head><p>We evaluated our proposed framework for isointense infant brain tissue segmentation on the public dataset NDAR <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>. The NDAR dataset comprises T1weighted brain images of 331 cases at the isointense phase and 368 cases at the adult-like phase (12-month-old), where 180 cases contain both time points. The GT annotations for GM, WM, and CSF were manually labeled by experienced radiologists. The data was aligned to MNI space and normalized to standard distribution using z-score normalization with dimensions of 182, 218, and 182 in the x, y, and z axis, respectively. The dataset was randomly divided into three subsets, i.e., 70% for training, 10% for validation, and 20% for testing. For quantitative evaluation, we employed three assessment metrics including Dice score, Hausdorff distance (HD), and average surface distance (ASD) <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Our proposed framework is implemented based on PyTorch 1.7.1 <ref type="bibr" target="#b10">[11]</ref> and trained on a workstation equipped with two NVIDIA V100s GPUs. We employ the Adam optimizer <ref type="bibr" target="#b6">[7]</ref> with momentum of 0.99 and weight decay of 0.001. The learning </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation and Discussion</head><p>We conduct extensive experiments to evaluate the effectiveness of our infant brain tissue segmentation framework. Table <ref type="table" target="#tab_0">1</ref> summarizes the results of the ablation study and demonstrates the contribution of each component in our framework. Particularly, SegNet represents the baseline model using a single scale, based on which, SegNetMS involves an additional branch that captures a larger receptive field and employs channel concatenation to fuse the features of these two branches. SegNetMSAtt replaces feature concatenation with the Transformer-based TCF module. Each of the above three configurations contains four different settings, as listed in Table <ref type="table" target="#tab_0">1</ref>. The results demonstrate the benefits of using both isointense phase data augmentation and adult-like phase structural enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Multi-phase Synthesis.</head><p>In Table <ref type="table" target="#tab_0">1</ref>, we demonstrate the effectiveness of the synthesis of isointense and adult-like phase images by the proposed synthesis network SPGAN. When comparing with the methods based on limited samples and augmented samples, we witness that data augmentation (DA) effectively improves segmentation performance. Furthermore, by comparing with the methods with or without structural enhancement (SE), we observe that the generated adult-like infant brain images provide richer and more reliable structural information to guide the identification of ambiguous tissue boundaries at the isointense phase. When combining DA and SE, an improvement of the segmentation performance on all evaluating metrics emerge, which demonstrates that the DA and SE can be effectively integrated to bring significant benefits for improving tissue segmentation performance.</p><p>Effectiveness of Multi-scale Assistance. In order to demonstrate the effectiveness of the additional branch in TMSN, which contains a larger receptive field, we show the corresponding results as one part of the ablation study in Table <ref type="table" target="#tab_0">1</ref>. Comparing the multi-scale SegNetMS with the single-scale variant Seg-Net, we can see that SegNetMS improves the Dice score from 94.32% to 96.01% and achieves a decrease in HD from 7.95 mm to 6.99 mm on average. This indicates that the multi-scale strategy enables the segmentation model to make use of the intrinsic relations between the centroid tissues and the corresponding counterparts with a larger receptive field, which leads to great improvement in the segmentation performance. In our proposed model SegNetMSAtt, we replace the channel concatenation in SegNetMS with TCF. We can find that the TCF module can more effectively build up the correspondence between top and bottom branches such that the surrounding tissues can well guide the segmentation.</p><p>Comparison with State-of-the-Arts (SOTAs). We conduct a comparative evaluation of our framework against several SOTA learning-based tissue segmentation networks, including 1) the 3D UNet segmentation network <ref type="bibr" target="#b2">[3]</ref>, 2) the DualAtt network <ref type="bibr" target="#b3">[4]</ref> utilizing both spatial and channel attention, and 3) the SwinUNETR network <ref type="bibr" target="#b5">[6]</ref> employing a 3D Swin Transformer. From the results listed in Table <ref type="table" target="#tab_1">2</ref>, we can find that our framework achieves the highest average Dice score and lower HD and ASD compared with the SOTA methods.</p><p>To further illustrate the advanced performance of our framework, we provide a visual comparison of two typical cases in Fig. <ref type="figure" target="#fig_2">3</ref>. It can be seen that the predicted tissue maps (the last column) obtained by our method are much closer to the GT (the second column), especially in the subcortical regions. We can draw the same conclusion from the error maps (the second and the fourth rows in Fig. <ref type="figure" target="#fig_2">3</ref>) between the segmented tissue maps obtained by each method and GT. Both the quantitative improvement and the superiority from the qualitative results performance consistently show the effectiveness of our proposed segmentation framework, as well as the two main components, i.e., SPGAN, and TMSN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we have presented a novel Transformer-based framework to segment tissues from isointense T1-weighted MR images. We designed two modules, i.e., i ) semantic-preserved GAN (SPGAN), and ii ) Transformer-based multiscale segmentation network (TMSN). SPGAN is designed to synthesize both isointense and adult-like data, which augments the dataset and provides supplementary tissue constraint for assisting isointense tissue segmentation. TMSN is used to segment tissues from isointense data under the guidance of the synthesized adult-like data. Their advantages are distinctive. For example, SPGAN overcomes the lack of training samples and synthesizes adult-like infant brain images with clear structural information for enhancing ambiguous tissue boundaries. TMSN exploits the pair of isointense and adult-like phase images, as well as the multi-scale scheme, to provide more information for achieving accurate segmentation performance. Extensive experiments demonstrate that our proposed framework outperforms the SOTA approaches, which shows the potential in early brain development or abnormal brain development studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. T1-weighted images (the first row) and different tissue intensity distributions (the second row) of two typical infant brains scanned in isointense and adult-like phases, respectively</figDesc><graphic coords="3,90,81,54,59,242,29,111,10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) An overview of our proposed framework including: i) the semanticspreserved GAN (SPGAN), and ii) Transformer-based multi-scale segmentation (TMSN) network; (b) Detailed SPGAN architecture with the discriminator omitted for making the figure concise; (c) Detailed architecture of TMSN.</figDesc><graphic coords="4,55,98,53,75,340,30,172,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative comparison with SOTA methods: two representative cases with corresponding error maps.</figDesc><graphic coords="8,98,97,54,38,254,14,163,03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation analysis of our proposed framework. DA: data augmentation; SE: structural enhancement; SegNet: single-branch segmentation network; SegNetMS: multi-branch segmentation network based on channel concatenation; SegNetMSAtt: proposed Transformer-based multi-branch segmentation network.</figDesc><table><row><cell>Method</cell><cell>DA SE</cell><cell>Dice (%) ↑</cell><cell></cell><cell></cell><cell>HD (mm) ↓</cell><cell></cell><cell></cell><cell>ASD ×10 (mm) ↓</cell></row><row><cell></cell><cell>WM</cell><cell>GM</cell><cell>CSF</cell><cell>WM</cell><cell>GM</cell><cell>CSF</cell><cell>WM</cell><cell>GM</cell><cell>CSF</cell></row><row><cell>SegNet</cell><cell cols="9">93.68 ± 0.55 92.45 ± 0.62 93.31 ± 0.76 5.44 ± 1.56 11.19 ± 1.86 8.69 ± 1.19 1.44 ± 0.17 1.17 ± 0.10 0.88 ± 0.09</cell></row><row><cell>SegNet</cell><cell cols="9">94.31 ± 0.54 93.35 ± 0.60 94.50 ± 0.64 5.34 ± 1.27 11.26 ± 2.01 10.60 ± 2.40 1.30 ± 0.16 1.01 ± 0.09 0.70 ± 0.07</cell></row><row><cell>SegNet</cell><cell cols="9">94.01 ± 0.61 93.28 ± 0.61 94.92 ± 0.58 7.40 ± 2.11 8.19 ± 2.62 10.15 ± 2.29 1.43 ± 0.19 1.00 ± 0.10 0.65 ± 0.06</cell></row><row><cell>SegNet</cell><cell cols="9">94.40 ± 0.56 93.83 ± 0.57 94.74 ± 0.62 5.44 ± 1.05 8.38 ± 2.51 10.04 ± 2.36 1.25 ± 0.15 0.93 ± 0.09 0.67 ± 0.06</cell></row><row><cell>SegNetMS</cell><cell cols="9">95.02 ± 0.41 94.50 ± 0.46 95.44 ± 0.48 6.51 ± 1.94 8.29 ± 2.36 7.95 ± 1.16 1.12 ± 0.13 0.84 ± 0.09 0.57 ± 0.05</cell></row><row><cell>SegNetMS</cell><cell cols="9">95.38 ± 0.37 94.99 ± 0.44 95.79 ± 0.51 4.96 ± 1.17 8.87 ± 2.32 8.20 ± 1.75 1.04 ± 0.11 0.77 ± 0.08 0.53 ± 0.05</cell></row><row><cell>SegNetMS</cell><cell cols="9">95.88 ± 0.41 95.40 ± 0.42 96.18 ± 0.43 4.86 ± 0.94 8.11 ± 2.79 7.87 ± 1.72 0.94 ± 0.12 0.70 ± 0.08 0.48 ± 0.04</cell></row><row><cell>SegNetMS</cell><cell cols="9">96.06 ± 0.40 95.62 ± 0.45 96.35 ± 0.48 5.14 ± 1.18 7.61 ± 2.37 8.22 ± 1.99 0.90 ± 0.11 0.66 ± 0.07 0.45 ± 0.05</cell></row><row><cell>SegNetMSAtt</cell><cell cols="9">95.68 ± 0.36 95.10 ± 0.47 95.85 ± 0.55 4.90 ± 0.97 8.44 ± 2.77 8.33 ± 1.35 1.00 ± 0.12 0.74 ± 0.07 0.53 ± 0.06</cell></row><row><cell>SegNetMSAtt</cell><cell cols="9">95.82 ± 0.36 95.40 ± 0.46 96.23 ± 0.48 4.82 ± 0.78 7.59 ± 2.53 8.32 ± 1.63 0.95 ± 0.12 0.68 ± 0.07 0.47 ± 0.05</cell></row><row><cell>SegNetMSAtt</cell><cell cols="9">96.21 ± 0.38 95.79 ± 0.42 96.50 ± 0.46 5.24 ± 0.95 8.39 ± 2.87 7.44 ± 1.22 0.86 ± 0.11 0.64 ± 0.07 0.43 ± 0.05</cell></row><row><cell>SegNetMSAtt</cell><cell cols="9">96.31 ± 0.36 95.89 ± 0.41 96.60 ± 0.42 4.71 ± 0.99 7.89 ± 2.65 7.30 ± 1.24 0.84 ± 0.11 0.62 ± 0.07 0.42 ± 0.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison with the state-of-the-art (SOTA) methods. ± 0.55 92.45 ± 0.62 93.32 ± 0.76 5.44 ± 1.57 11.19 ± 1.86 8.69 ± 1.19 1.44 ± 0.17 1.17 ± 0.10 0.88 ± 0.09 DualAtt [4] 95.21 ± 0.34 94.61 ± 0.41 95.51 ± 0.47 5.24 ± 1.22 10.96 ± 1.89 7.94 ± 1.42 1.07 ± 0.11 0.85 ± 0.08 0.56 ± 0.04 SwinUNETR [6] 94.99 ± 0.40 94.17 ± 0.48 94.88 ± 0.54 4.88 ± 1.14 7.60 ± 2.58 8.47 ± 1.82 1.13 ± 0.14 0.86 ± 0.08 0.64 ± 0.05rate is set as 0.001 and reduced to 90% of the current learning rate every 50 epochs for both SPGAN and TMSN. For SPGAN the hyperparameters γ 1 , γ 2 , γ 2 , γ 4 are set to 10, 10, 1 and 1 in our experiments, respectively. For TMSN, the patch sizes of the top and bottom branches are set as 128 × 128 × 128 and 160×160×160, respectively. The total parameter number of SPGAN and TMSN is 44.51M. The hyperparameter λ 1 in TMSN is set to 0.6. The inference for one single image took about 8.82 s. Code is available at this link.</figDesc><table><row><cell>Method</cell><cell>Dice (%) ↑</cell><cell></cell><cell></cell><cell>HD (mm) ↓</cell><cell></cell><cell></cell><cell>ASD ×10 (mm) ↓</cell><cell></cell></row><row><cell>WM</cell><cell>GM</cell><cell>CSF</cell><cell>WM</cell><cell>GM</cell><cell>CSF</cell><cell>WM</cell><cell>GM</cell><cell>CSF</cell></row><row><cell cols="9">3D UNet [3] 93.68 Ours 96.31 ± 0.36 95.89 ± 0.41 96.60 ± 0.42 4.71 ± 0.99 7.89 ± 2.65 7.30 ± 1.23 0.84 ± 0.11 0.62 ± 0.07 0.42 ± 0.04</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62131015</rs> and <rs type="grantNumber">62203355</rs>), and <rs type="funder">Science and Technology Commission of Shanghai Municipality (STCSM)</rs> (No. <rs type="grantNumber">21010502600</rs>), and <rs type="funder">The Key R&amp;D Program of Guangdong Province, China</rs> (No. <rs type="grantNumber">2021B0101420006</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ktqxSpX">
					<idno type="grant-number">62131015</idno>
				</org>
				<org type="funding" xml:id="_U8TZDrR">
					<idno type="grant-number">62203355</idno>
				</org>
				<org type="funding" xml:id="_bYs4NfA">
					<idno type="grant-number">21010502600</idno>
				</org>
				<org type="funding" xml:id="_6DbSn4y">
					<idno type="grant-number">2021B0101420006</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Advanced normalization tools (ants)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Insight J</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">365</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">6-month infant brain MRI segmentation guided by 24-month data using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="359" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sharing heterogeneous data: the national database for autism research</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Huerta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Farber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="331" to="339" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Swin UNETR: swin transformers for semantic segmentation of brain tumors in MRI images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08999-2_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08999-222" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">12962</biblScope>
		</imprint>
	</monogr>
	<note>Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-scale segmentation network for Rib fracture classification from CT images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87589-3_56</idno>
		<idno>978-3-030-87589-3 56</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12966</biblScope>
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multimodality isointense infant brain image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 13Th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1342" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GauGAN: semantic image synthesis with spatially adaptive normalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH 2019 Real-Time Live!</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">National Database for Autism Research (NDAR): big data opportunities for health services research and health technology assessment</title>
		<author>
			<persName><forename type="first">N</forename><surname>Payakachat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tilford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pharmacoeconomics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic segmentation of MR images of the developing newborn brain</title>
		<author>
			<persName><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Construction of multiregion-multi-reference atlases for neonatal brain MRI segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="693" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Brain development and the role of experience in the early years</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zero Three</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anatomy-guided joint tissue segmentation and topological correction for 6-month infant brain MRI with risk of autism</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2609" to="2623" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Benchmark on automatic six-month-old infant brain segmentation algorithms: the iSeg-2017 challenge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2219" to="2230" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integration of sparse multi-modality representation and anatomical constraint for isointense infant brain MR image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="152" to="164" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmentation of neonatal brain MR images using patch-driven level sets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">multi-modality tissue segmentation of serial infant images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Medical image segmentation using deep learning: a survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Proc</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1243" to="1267" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local U-Nets for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6315" to="6322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unpaired brain MR-to-CT synthesis using a structure-constrained CycleGAN</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-520" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="174" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
