<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality</title>
				<funder ref="#_Pdme9v3">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_byPd5UC #_sjFQnkN">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Washington University Center for High Performance Computing</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Peijie</forename><surname>Qiu</surname></persName>
							<email>peijie.qiu@wustl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Mallinckrodt Institute of Radiology</orgName>
								<orgName type="institution" key="instit2">Washington University School of Medicine</orgName>
								<address>
									<settlement>St. Louis</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Satrajit</forename><surname>Chakrabarty</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Systems Engineering</orgName>
								<orgName type="institution">Washington University in St. Louis</orgName>
								<address>
									<settlement>St. Louis</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">University of Cincinnati</orgName>
								<address>
									<settlement>Cincinnati</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soumyendu</forename><forename type="middle">Sekhar</forename><surname>Ghosh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Systems Engineering</orgName>
								<orgName type="institution">Washington University in St. Louis</orgName>
								<address>
									<settlement>St. Louis</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aristeidis</forename><surname>Sotiras</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Mallinckrodt Institute of Radiology</orgName>
								<orgName type="institution" key="instit2">Washington University School of Medicine</orgName>
								<address>
									<settlement>St. Louis</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Informatics, Data Science and Biostatistics</orgName>
								<orgName type="institution">Washington University School of Medicine</orgName>
								<address>
									<settlement>St. Louis</settlement>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="173" to="182"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B00D2A8F0EE8C4907485F88BF0F6BA79</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_17</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-01T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automatic quality control</term>
					<term>Brain tumor segmentation</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has achieved state-of-the-art performance in automated brain tumor segmentation from magnetic resonance imaging (MRI) scans. However, the unexpected occurrence of poor-quality outliers, especially in out-of-distribution samples, hinders their translation into patient-centered clinical practice. Therefore, it is important to develop automated tools for large-scale segmentation quality control (QC). However, most existing QC methods targeted cardiac MRI segmentation which involves a single modality and a single tissue type. Importantly, these methods only provide a subject-level segmentationquality prediction, which cannot inform clinicians where the segmentation needs to be refined. To address this gap, we proposed a novel network architecture called QCResUNet that simultaneously produces segmentation-quality measures as well as voxel-level segmentation error maps for brain tumor segmentation QC. To train the proposed model, we created a wide variety of segmentation-quality results by using i) models that have been trained for a varying number of epochs with different modalities; and ii) a newly devised segmentation-generation method called SegGen. The proposed method was validated on a large public brain tumor dataset with segmentations generated by different methods, achieving high performance on the prediction of segmentationquality metric as well as voxel-wise localization of segmentation errors. The implementation will be publicly available at https://github.com/ peijie-chiu/QC-ResUNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gliomas are the most commonly seen central nervous system malignancies with aggressive growth and low survival rates <ref type="bibr" target="#b19">[19]</ref>. Accurate multi-class segmentation of gliomas in multimodal magnetic resonance imaging (MRI) plays an indispensable role in quantitative analysis, treatment planning, and monitoring of progression and treatment. Although deep learning-based methods have achieved state-of-the-art performance in automated brain tumor segmentation <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b13">14]</ref>, their performance often drops when tasked with segmenting out-of-distribution samples and poor-quality artifactual images. However, segmentations of desired quality are required to reliably drive treatment decisions and facilitate clinical management of gliomas. Therefore, tools for automated quality control (QC) are essential for the clinical translation of automated segmentation methods. Such tools can enable a streamlined clinical workflow by identifying catastrophic segmentation failures, informing clinical experts where the segmentations need to be refined, and providing a quantitative measure of quality that can be taken into account in downstream analyses.</p><p>Most previous studies of segmentation QC only provide subject-level quality assessment by either directly predicting segmentation-quality metrics or their surrogates. Specifically, Wang et al. <ref type="bibr" target="#b17">[18]</ref> leveraged a variational autoencoder to learn the latent representation of good-quality image-segmentation pairs in the context of cardiac MRI segmentation. During the inference, an iterative search scheme was performed in the latent space to find a surrogate segmentation. This segmentation is assumed to be a good proxy of the (unknown) ground-truth segmentation of the query image, and can thus be compared to the at-hand predicted segmentation to estimate its quality. Another approach that takes advantage of the pairs of images and ground-truth segmentation is the reverse classification accuracy (RCA) framework <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>. In this framework, the test image is registered to a preselected reference dataset with known ground-truth segmentation. The quality of a query segmentation is assessed by warping the query image to the reference dataset. However, these methods primarily targeted QC of cardiac MRI segmentation, which involves a single imaging modality and a single tissue type with a welch-characterized location and appearance. In contrast, brain tumor segmentation involves the delineation of heterogeneous tumor regions, which are manifested through intensity changes relative to the surrounding healthy tissue across multiple modalities. Importantly, there is significant variability in brain tumor appearances, including multifocal masses and complex shapes with heterogeneous textures. Consequently, adapting approaches for automated QC of cardiac segmentation to brain tumor segmentation is challenging. Additionally, iterative search or registration during inference makes the existing methods computationally expensive and time-consuming, which limits their applicability in large-scale segmentation QC.</p><p>Multiple studies have also explored regression-based methods to directly predict segmentation-quality metrics, e.g., Dice Similarity Coefficient (DSC). For example, Kohlberger et al. <ref type="bibr" target="#b9">[10]</ref> used Support Vector Machine (SVM) with handcrafted features to detect cardiac MRI segmentation failures. Robinson et al. <ref type="bibr" target="#b11">[12]</ref>  proposed a convolutional neural network (CNN) to automatically extract features from segmentations generated by a series of Random Forest segmenters to predict DSC for cardiac MRI segmentation. Kofler et al. <ref type="bibr" target="#b8">[9]</ref> proposed a CNN to predict holistic ratings of segmentations, which were annotated by neuroradiologists, with the goal of better emulating how human experts. Though these regression-based methods are advantageous for fast inference, they do not provide voxel-level localization of segmentation failures, which can be crucial for both auditing purposes and guiding manual refinements.</p><p>In summary, while numerous efforts have been devoted to segmentation QC, most works were in the context of cardiac MRI segmentation with few works tackling segmentation QC of brain tumors, which have more complex and heterogeneous appearances than the heart. Furthermore, most of the existing methods do not localize segmentation errors, which is meaningful for both auditing purposes and guiding manual refinement. To address these challenges, we propose a novel framework for joint subject-level and voxel-level prediction of segmentation quality from multimodal MRI. The contribution of this work is four-fold. First, we proposed a predictive model (QCResUNet) that simultaneously predicts DSC and localizes segmentation errors at the voxel level. Second, we devised a datageneration approach, called SegGen, that generates a wide range of segmentations of varying quality, ensuring unbiased model training and testing. Third, our end-to-end predictive model yields fast inference. Fourth, the proposed method achieved a good performance in predicting subject-level segmentation quality and identifying voxel-level segmentation failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Given four imaging modalities denoted as [X 1 , X 2 , X 3 , X 4 ] and a predicted multiclass brain tumor segmentation mask (S pred ), the goal of our approach is to automatically assess the tumor segmentation quality by simultaneously predicting DSC and identifying segmentation errors as a binary mask (S err ). Toward this end, we proposed a 3D encoder-decoder architecture termed QCResUNet (see Fig. <ref type="figure" target="#fig_0">1(a)</ref>) for simultaneously predicting DSC and localizing segmentation errors. QCResUNet has two parts trained in an end-to-end fashion: i) a ResNet-34 <ref type="bibr" target="#b3">[4]</ref> encoder for DSC prediction; and ii) a decoder architecture for segmentation error map prediction (i.e., the difference between predicted segmentation and ground-truth segmentation).</p><p>The ResNet-34 encoder enables the extraction of semantically rich features that are useful for characterizing the quality of the segmentation. We maintained the main structure of the vanilla 2D ResNet-34 <ref type="bibr" target="#b3">[4]</ref> but made the following modifications, which were necessary to account for the 3D nature of the input data (see Fig. <ref type="figure" target="#fig_0">1(b)</ref>). First, all the 2D convolutional layers and pooling layers in the vanilla ResNet were changed to 3D. Second, the batch normalization <ref type="bibr" target="#b4">[5]</ref> was replaced by instance normalization <ref type="bibr" target="#b15">[16]</ref> to accommodate the small batch size in 3D model training. Third, spatial dropout <ref type="bibr" target="#b14">[15]</ref> with a probability of 0.3 was added to each residual block to prevent overfitting.</p><p>The building block of the decoder consisted of an upsampling by a factor of two, which was implemented by a nearest neighbor interpolation in the feature map, followed by two convolutional blocks that halve the number of feature maps. Each convolutional block comprised a 3 × 3 × 3 convolutional layer followed by an instance normalization layer and a leaky ReLU activation <ref type="bibr" target="#b10">[11]</ref> (see Fig. <ref type="figure" target="#fig_0">1(c)</ref>). The output of each decoder block was concatenated with features from the corresponding encoder level to facilitate information flow from the encoder to the decoder. Compared to the encoder, we used a shallower decoder with fewer parameters to prevent overfitting and reduce computational complexity.</p><p>The objective function for training QCResUNet consists of two parts. The first part corresponds to the DSC regression task. It consists of a mean absolute error (MAE) loss (L MAE ) term that penalizes differences between ground truth (DSC gt ) and predicted DSC (DSC pred ):</p><formula xml:id="formula_0">L MAE = 1 N N n=1 |DSC (n) gt -DSC (n) pred | 1 ,<label>(1)</label></formula><p>where N denotes the number of samples in a batch. The second part of the objective function corresponds to the segmentation error prediction. It consists of a dice loss <ref type="bibr" target="#b2">[3]</ref> and a binary cross-entropy loss, given by:</p><formula xml:id="formula_1">L dice = - 2 • I i=1 S (i) errgt • S (i) err pred I i=1 S (i) errgt + I i=1 S (i) err pred L BCE = - 1 I I i=1 S (i) errgt log S (i) err pred + (1 -S (i) errgt ) log(1 -S (i) err pred ),<label>(2)</label></formula><p>where S errgt , S err pred denote the binary ground-truth segmentation error map and the predicted error segmentation map from the sigmoid output of the decoder, respectively. The dice loss and cross-entropy loss were averaged across the number of pixels I in a batch. The two parts are combined using a weight parameter λ to balance the different loss components:</p><formula xml:id="formula_2">L total = L MAE + λ (L dice + L BCE ).<label>(3)</label></formula><p>3 Experiments</p><p>For this study, pre-operative multimodal MRI scans of varying grades of glioma were obtained from the 2021 Brain Tumor Segmentation (BraTS) challenge <ref type="bibr" target="#b0">[1]</ref> training dataset (n = 1251). For each subject, four modalities viz. pre-contrast T1-weighted (T1), T2-weighted (T2), post-contrast T1-weighted (T1c), and Fluid attenuated inversion recovery (FLAIR) are included in the dataset. It also included expert-annotated multi-class tumor segmentation masks comprising enhancing tumor (ET), necrotic tumor core (NCR), and edema (ED) classes. All data were already registered to a standard anatomical atlas and skull-stripped. The skull-stripped scans were then z-scored to zero mean and unit variance. All the data was first cropped to non-zero value regions, and then zero-padded to a size of 160 × 192 × 160 to be fed into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Generation</head><p>The initial dataset was expanded by producing segmentation results at different levels of quality to provide an unbiased estimation of segmentation quality. To this end, we adopted a three-step approach. First, a nnUNet framework <ref type="bibr" target="#b5">[6]</ref> was adopted and trained five times separately using different modalities as input (i.e., T1-only, T1c-only, T2-only, FLAIR-only, and all four modalities). As only certain tissue-types are captured in each modality (e.g., enhancing tumor is captured well in T1c but not in FLAIR), this allowed us to generate segmentations of a wide range of qualities. nnUNet was selected for this purpose due to its wide success in brain tumor segmentation tasks. Second, to further enrich our dataset with segmentations of diverse quality, we sampled segmentations along the training routines at different iterations. A small learning rate (1 × 10 -6 ) was chosen in training all the models to slower their convergence in order to sample segmentations gradually sweeping from poor quality to high quality. Third, we devised a method called SegGen that applied image transformations, including random rotation (angle = [-15 • , 15 • ]), random scaling (scale = [0.85, 1.25]), random translation (moves = [-20, 20]), and random elastic deformation (displacement = [0, 20]), to the ground-truth segmentations with a probability of 0.5, resulting in three segmentations for each subject.</p><p>The original BraTS 2021 training dataset was split into training (n = 800), validation (n = 200), and testing (n = 251) sets. After applying the three-step approach, it resulted in 48000, 12000, and 15060 samples for the three sets, respectively. However, this generated dataset suffered from imbalance (Fig. <ref type="figure" target="#fig_1">2(a),  (b),</ref> and<ref type="figure">(c</ref>)) because the CNN models could segment most of the cases correctly. Training using such an imbalanced dataset is prone to producing biased models that do not generalize well. To mitigate this issue, we proposed a resampling strategy during the training to make the DSC more uniformly distributed. Specifically, we used the Quantile transform to map the distribution of a variable to a target distribution by randomly smoothing out the samples unrelated to the target distribution. Using the Quantile transform, the data generator first transformed the distribution of the generated DSC to a uniform distribution. Next, the generated samples closest to the transformed uniform distribution in terms of Euclidean distance were chosen to form the resampled dataset. After applying our proposed resampling strategy, the DSC in the training and validation set approached a uniform distribution (Fig. <ref type="figure" target="#fig_1">2</ref>(a), (b), and (c)). The total number of samples before and after resampling remained the same with repeating samples. We kept the resampling stochastic at each iteration during training to make all the generated samples seen by the model. The generated testing set was also resampled to perform an unbiased estimation of the quality at different levels resulting in 4895 samples.</p><p>In addition to the segmentations generated by the nnUNet framework and the SegGen method, we also generated out-of-distribution segmentation samples for the testing set to validate the generalizability of our proposed model. For this purpose, five models were trained on the training set using the DeepMedic framework <ref type="bibr" target="#b7">[8]</ref> with different input modalities (i.e., T1-only, T1c-only, T2-only, FLAIR-only, and all four modalities). This resulted in 251 × 5 = 1255 out-ofdistribution samples in the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Design</head><p>Baseline Methods: In this study, we compared the performance of the proposed model to three baseline models: (i) a UNet model <ref type="bibr" target="#b13">[14]</ref>, (ii) a ResNet-34 <ref type="bibr" target="#b3">[4]</ref>, and (iii) the ReNet-50 model used by Robinson et al. <ref type="bibr" target="#b11">[12]</ref>. For a fair comparison, the residual blocks in the ResNet-34 and ResNet-50 were the same as that in the QCResUNet. We added an average pooling followed by a fully-connected layer to the last feature map of the UNet to predict a single DSC value. The evaluation was conducted on in-sample (nnUNet and SegGen) and out-of-sample segmentations generated by DeepMedic.</p><p>Table <ref type="table">1</ref>. The QC performance of three baseline methods and the proposed method was evaluated on in-sample (nnUNet and SegGen) and out-of-sample (DeepMedic) segmentations. The best metrics in each column are highlighted in bold. DSCerr denotes the median DSC between Serr gt and Serr pred across all samples. Training Procedure: All models were trained for 150 epochs using an Adam optimizer with a L 2 weight decay of 5 × 10 -4 . The batch size was set to 4. Data augmentation, including random rotation, random scaling, random mirroring, random Gaussian noise, and Gamma intensity correction, was applied to prevent overfitting during training. We performed a random search <ref type="bibr" target="#b1">[2]</ref> to determine the optimal hyperparameters (i.e., initial learning rate and loss weight balance parameter λ) on the training and validation set. The hyperparameters that yielded the best results were λ = 1 and an initial learning rate of 1 × 10 -4 . The learning rate was exponentially decayed by a factor of 0.9 at each epoch until 1 × 10 -6 . Model training was performed on four NVIDIA Tesla A100 and V100S GPUs. The proposed method was implemented in PyTorch v1.12.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Analysis:</head><p>We assessed the performance of the subject-level segmentation quality prediction in terms of Pearson coefficient r and MAE between the predicted DSC and the ground-truth DSC. The performance of the segmentation error localization was assessed by the DSC err between the predicted segmenta- tion error map and the ground-truth segmentation error map. P-values were computed using a paired t-test between DSC predicted by QCResUNet versus ones predicted by corresponding baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The proposed QCResUNet achieved good performance in predicting subjectlevel segmentation quality for in-sample (MAE = 0.0570, r = 0.964) and out-ofsample (MAE = 0.0606, r = 0.966) segmentations. The proposed method also showed statistically significant improvement against all three baselines (Table <ref type="table">1</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>). We found that the DSC prediction error (MAE) of the proposed method was distributed more evenly across different levels of quality than all baselines (see Fig. <ref type="figure" target="#fig_2">3</ref>) with a smaller standard deviation of 0.050 for in-sample segmentations and 0.049 for out-of-sample segmentations. A possible explanation is that the joint training of predicting subject-level and voxel-level quality enabled the QCResUNet to learn deep features that better characterize the segmentation quality. For the voxel-level segmentation error localization task, the model achieved a median DSC of 0.834 for in-sample segmentations and 0.867 for out-of-sample segmentations. This error localization is not provided by any of the baselines and enables QCResUnet to track segmentation failures at different levels of segmentation quality (Fig. <ref type="figure" target="#fig_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a novel CNN architecture called QCResUNet to perform automatic brain tumor segmentation QC in multimodal MRI scans. QCRe-sUNet simultaneously provides subject-level segmentation-quality prediction and localizes segmentation failures at the voxel level. It achieved superior DSC prediction performance compared to all baselines. In addition, the ability to localize segmentation errors has the potential to guide the refinement of predicted segmentations in a clinical setting. This can significantly expedite clinical workflows, thus improving the overall clinical management of gliomas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) The proposed QCResUNet model adopts an encoder-decoder neural network architecture that takes four modalities and the segmentation to be evaluated. Given this input, QCResUNet predicts the DSC and segmentation error map. (b) The residual block in the encoder. (c) The convolutional block in the decoder.</figDesc><graphic coords="3,62,46,54,11,327,85,127,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. DSC distribution of the generated dataset before and after resampling. (a), (b) and (c) are the DSC distribution for the training, validation, and testing set.</figDesc><graphic coords="5,55,98,54,11,340,18,85,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of QC performance between three baseline methods (UNet, ResNet-34, ResNet-50) and the proposed method (QCResUNet) for segmentations generated using nnUNet and SegGen (top row) as well as DeepMedic (bottom row).</figDesc><graphic coords="7,61,98,202,82,328,57,147,19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples showcasing the performance of the proposed methods. The last column denotes the QC performance of different methods. The penultimate column denotes the predicted segmentation error.</figDesc><graphic coords="8,56,31,54,41,311,26,231,28" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. All computations were supported by the <rs type="funder">Washington University Center for High Performance Computing</rs>, which was partially funded by <rs type="funder">NIH</rs> grants <rs type="grantNumber">S10OD025200</rs>, <rs type="grantNumber">1S10RR022984-01A1</rs>, and <rs type="grantNumber">1S10OD018091-01</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Pdme9v3">
					<idno type="grant-number">S10OD025200</idno>
				</org>
				<org type="funding" xml:id="_byPd5UC">
					<idno type="grant-number">1S10RR022984-01A1</idno>
				</org>
				<org type="funding" xml:id="_sjFQnkN">
					<idno type="grant-number">1S10OD018091-01</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The RSNA-ASNR-MICCAI BraTS 2021 benchmark on brain tumor segmentation and radiogenomic classification</title>
		<author>
			<persName><forename type="first">U</forename><surname>Baid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46976-8_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46976-8_19" />
	</analytic>
	<monogr>
		<title level="m">LABELS/DLMIA -2016</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">10008</biblScope>
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation and radiomics survival prediction: contribution to the BRATS 2017 challenge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kickingereder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-75238-9_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-75238-9_25" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2017</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10670</biblScope>
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Kofler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10355</idno>
		<title level="m">Deep quality estimation: creating surrogate models for human quality ratings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating segmentation error without ground truth</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alvino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bahlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33415-3_65</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-33415-3_65" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2012</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Delingette</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7510</biblScope>
			<biblScope unit="page" from="528" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time prediction of segmentation quality</title>
		<author>
			<persName><forename type="first">R</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_66" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="578" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated quality control in image segmentation: application to the UK Biobank cardiovascular magnetic resonance imaging study</title>
		<author>
			<persName><forename type="first">R</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cardiovasc. Magn. Reson</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Instance normalization: the missing ingredient for fast stylization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reverse classification accuracy: predicting segmentation performance in the absence of ground truth</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Valindria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1597" to="1606" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep generative model-based quality control for cardiac MRI segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-1_9" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using holistically nested neural networks in MRI images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5234" to="5243" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
