Paper Title,Header Number,Header Title,Text
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,1,Introduction,"Recent years have seen a significant increase in the use of artificial intelligence (AI) in medical imaging, as evidenced by the rising number of academic publications and the accelerated approval of commercial AI applications for clinical use [1,12,14,19,21]. Despite the growing availability of market-ready AI products and clinical enthusiasm to adopt these solutions [20], the translation of AI into real-world clinical practice remains limited. The reasons for this gap are multifaceted and include technical challenges, restricted IT resources, and a deficiency of clear data-driven clinical utility analyses. Efforts are underway to address these many barriers through existing and emerging solutions [4,6,13,22]. However, a major concern remains: What happens to the AI after it is put into production?Monitoring the performance of AI models in production systems is crucial to ensure safety and effectiveness in healthcare, particularly for medical imaging applications. The unrealistic expectation that input data and model performance will remain static indefinitely runs counter to decades of machine learning operations research, as outlined by extensive experience in AI model deployment for other verticals [11,17]. Traditional drift detection methods require real-time feedback and lack the ability to guard against performance drift crucial for safe AI deployment in healthcare and, as such, the absence of solutions for monitoring AI model performance in medical imaging is a significant barrier to widespread adoption of AI in healthcare [7].In healthcare, the availability of real-time ground truth data is often limited, presenting a significant challenge to accurate and timely performance monitoring. This limitation renders many existing monitoring strategies inadequate, as they require access to contemporaneous ground truth labels. Moreover, existing solutions do not tackle the distinct challenges posed by monitoring medical imaging data, including both pixel and non-pixel data, as they are primarily designed for structured tabular data. Our challenge is then to develop a systematic approach to real-time monitoring of medical imaging AI models without contemporaneous ground truth labels. This gap in the current landscape of monitoring strategies is what our method aims to fill.In this manuscript, we present a solution that relies on only statistics of input data, deep-learning based pixel data representations, and output predictions. Our innovative approach goes beyond traditional methods and addresses this gap by not necessitating the use of up-to-date ground truth labels. Our framework is coupled with a novel multi-modal integration methodology for realtime monitoring of medical imaging AI systems for conditions which will likely have an adverse effect on performance. Through the solution proposed in this paper, we make a meaningful contribution to the medical imaging AI monitoring landscape, offering an approach specifically tailored to navigate the inherent constraints and challenges in the field."
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,2,Materials and Methods,
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,2.1,Data and Deep Learning Model,"We test our medical imaging AI drift workflow using the CheXpert [9] and Pad-Chest [2] datasets. CheXpert comprises 224, 316 images of 65, 240 patients who Unlike other datasets, PadChest keeps the chronology of scans making it valuable for drift experiments. We categorized PadChest into three sets using examination dates: training, validation, and test. Our experimentation period spans the first year of the test set. To align with the labels in CheXpert, we consolidate relevant labels from the PadChest dataset into a set of ten unified labels. See Table 1 for details (Fig. 1).Our approach utilizes a Densely Connected Convolutional Neural Network [8], pretrained on frontal-only CheXpert data, then we fine-tuned only the final classifier layers of the model using PadChest frontal training data. To assess the performance of our classifier over a simulated production timeframe, we employ AUROC as an evaluation metric. This approach offers a definitive indication of any potential model drift, but it necessitates real-time, domain expert-labeled ground truth labels."
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,2.2,Data Stream Drift and Concordance,"Our approach differs from others in that we monitor for similarity or concordance of the datastream with respect to a reference dataset rather than highlighting differences. When the concordance metric decreases the degree to which the data has drifted has increased. Our method summarizes each exam in the datasteam into an embedding consisting of DICOM metadata, pixel features and model output which is sampled into temporal detection windows in order to compare distributions of individual features to a reference set using statistical tests. Our framework, though extensible, uses two statistical tests: 1) the Kolmogorov-Smirnov (K-S) test and 2) the chi-square (χ 2 ) goodness of fit test. The K-S test is a non-parametric test which measures distribution shift in a real-valued sample without assuming any specific distribution [5]. The chi-square goodnessof-fit test compares observed frequencies in categorical data to expected values and calculates the likelihood they are obtained from the reference distribution [16]. Both these tests provide a p-value and statistical similarity (distance) value. We found that statistical ""distance"" provided a smoother and more consistent metric which we use exclusively in our experiments, ignoring p-values.Multi-Modal Embedding. To calculate statistics, each image must be embedded into a compressed representation suitable for our statistical tests. Our embedding is comprised of three categories: 1) DICOM metadata, 2) image appearance, and 2) model output (Fig. 2). AI performance in medical imaging can be affected by shifts in imaging data, due to factors like hardware changes or disease presentation variations. To address this, we employ a Variational Autoencoder (VAE)-an auto-encoder variant that models underlying parametric probability distributions of input data for fine-grained, explainable analysis [3,18,23]. Our approach uses a VAE to encode images and apply statistical tests for drift detection, representing, to our knowledge, the first VAE use case for medical imaging drift analysis. The VAE is trained on PadChest's frontal and lateral images.The aim of live medical data stream monitoring is to ensure consistency, detect changes impacting model performance, and identify shifts in class distribution or visual representations. We utilize soft predictions (model raw score/activation) to monitor model output and detect subtle distribution changes that hard predictions may overlook, enhancing early detection capabilities."
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,,Metric Measurement and Unification,"Our framework constructs detection windows using a sliding window approach, where the temporal parameters dictate the duration and step size of each window. Specifically, the duration defines the length of time that each window covers, while the step size determines the amount of time between the start of one window and the start of the next. We use ψi (ω t ) = mt i to denote individual metrics calculated at time t from a ω t , and m[a,b] i to represent the collection of individual metric values from time a to b.To mitigate sample size sensitivity, we employ a bootstrap method. This involves repeatedly calculating metrics on fixed-size samples drawn with replacement from the detection window. We then average these repeated measures to yield a final, more robust metric value. Formally:where θ K collects K samples from ω with replacement and ψ i is the metric function calculated on the sample. There remains three main challenges to metric unification: 1) fluctuation normalization, 2) scale standardization, and 3) metric weighting. Fluctuation normalization and scale standardization are necessary to ensure that the metrics are compatible and can be meaningfully compared and aggregated. Without these steps, comparing or combining metrics could lead to misleading results due to the variations in the scale and distribution of different metrics. We address the first two challenges by utilizing a standardization function, Γ , which normalizes each individual metric into a numerical space with consistent upper and lower bounds across all metrics. This function serves to align the metric values so that they fall within a standard range, thereby eliminating the influence of extreme values or discrepancies in the original scales of the metrics. In our experiments, we apply a simple normalization function using scale (η) and offset factors (ζ), specifically: Γ (m) = m-ζ η . Metric weighting is used to reflect the relative importance or reliability of each metric in the final unified metric. The weights are determined through a separate process which takes into account factors such as the sensitivity and specificity of each metric. We then calculate our unified multi-modal concordance metric, MMC , on a detection window ω by aggregating individual metric values across L metrics using predefined weights, α i , for each metric, as follows:where ψi (ω) represents the ith metric calculated on detection window ω, Γ i represents the standardization function, and α i represents the weight used for the ith metric value. Each metric value is derived by a function that measures a specific property or characteristic of the detection window. For instance, one metric could measure the average intensity of the window, while another could measure the variability of intensities. By calculating MMC on a time-indexed detection window set Ω [a,b] , we obtain a robust multi-modal concordance measure that can monitor drift over the given time period from a to b, denoted as MMC [a,b] . This unified metric is advantageous as it provides a single, comprehensive measurement that takes into account multiple aspects of the data, making it easier to track and understand changes over time."
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,3,Experimentation,"Our framework is evaluated through three simulations, inspired by clinical scenarios, each involving a datastream modification to induce noticeable drift. All experiments share settings of a 30-day detection window, one-day stride, and parameters K = 2500 and N = 20 in Eq. 1. Windows with less than 150 exams are skipped. We use the reference set for generating Ω r and calculating η and ζ. Weights α i are calculated by augmenting Ω r with poor-performing samples.Scenario 1: Performance Degradation. We investigate if performance changes are detectable by inducing degradation through hard data mining. We compile a pool of difficult exams for the AI to classify by selecting exams with low model scores but positive ground truth for their label as well as high scoring negatives. Exams are chosen based on per-label quantiles of scores, with Q = 0.25 indicating the lowest 25% positives and highest 25% negatives are included. These difficult exams replace all other exams in each detection window at a given point in the datastream. Scenario 2: Metadata Filter Failure. In this scenario, we simulate a workflow failure, resulting in processing out-of-spec data, specifically lateral images, in contrast to model training on frontal images only. The datastream is modified at two points to include and then limit to lateral images.Scenario 3: No Metadata Available. The final experiment involves a nometadata scenario using the Pediatric Pneumonia Chest X-ray dataset [10], comprising of 5, 856 pediatric Chest X-rays. This simulates a drift scenario with a compliance boundary, relying solely on the input image. The stream is altered at two points to first include and then limit to out-of-spec data."
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,4,Results and Discussion,"The performance and concordance metrics of each experiment are visualized in Fig. 3. Each sub-figure the top panel depicts performance as well as calculated MMC as calculated in each detection window. Each sub-figure has vertical lines which represent points indicating where the datastream was modified.We start by discussing results in Fig. 3a from the first experiment. The top panel shows micro-averaged AUROC, the middle panel shows MMC w , and the bottom panel shows MMC 0 . As Q decreases, performance drops and the concordance metric drops as well. Both the weighted and unweighted versions of our metric are shown, with the weighted version providing clearer separation between performance profiles showing that our weighting methodology emphasizes relevant metrics for consistent performance proxy.Next, Fig. 3b shows results of our second experiment. The top panel shows performance, and the bottom panel shows our metric MMC w . Two trials are depicted, a baseline (original data stream, blue) and a second trial (red) where drift is induced. Two vertical lines denote points in time where data stream is modified: at point A, lateral images are introduced, and at point B, indistribution (frontal) data is removed, leaving only laterals. The figure shows a correlation between performance and MMC w ; at point A, performance drops from above 0.9 to approx. 0.85 and MMC w drops to approx. -4. At point B, performance drops to around 0.75 and MMC w drops to and hovers around -8. This demonstrates the robustness of our method to changes in data composition detectable by metadata tags and visual appearance.Results of our final scenario, appear in Fig. 3c. In this experiment, we measure Pneumonia AUROC, as the pediatric data includes only pneumonia labels. We observe a drop in performance and concordance at both points where we modify the data stream, show that our approach remains robust without metadata and can still detect drift. We also notice a larger drop in concordance compared to performance, indicating that concordance may be more sensitive to data stream changes, which could be desirable for detecting this type of drift when the AI model is not cleared for use on pediatric patients.We demonstrate model monitoring for a medical imaging with CheXStray can achieve real-time drift metrics in the absence of contemporaneous ground truth in a chest X-ray model use case to inform potential change in model performance. This work will inform further development of automated medical imaging AI monitoring tools to ensure ongoing safety and quality in production to enable safe and effective AI adoption in medical practice. The important contributions include the use of VAE in reconstructing medical images for the purpose of detecting input data changes in the absence of ground truth labels, data-driven unsupervised drift detection statistical metrics that correlate with supervised drift detection approaches and ground truth performance, and open source code and datasets to optimize validation and reproducibility for the broader community."
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,,Fig. 1 .,
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,,Fig. 2 .,
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,,Fig. 3 .,
CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI,,Table 1 .,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,1,Introduction,"Coronary computed tomographic angiography (CCTA) is a well-established noninvasive imaging modality to diagnose coronary artery disease (CAD) which is a common disease with increasing prevalence worldwide [15]. Accurate extraction of coronary arteries from CCTA can assist doctors in diagnosis and treatment of CAD [16].Manual segmentation of coronary arteries is time-consuming and expensive. As a result, multiple traditional methods based on image processing techniques have been developed over the years to automatically or semi-automatically segment coronary arteries [1,2]. With the rapid development of deep learning, more methods have been proposed to automatically extract coronary arteries which show better scalability and higher accuracy [18][19][20]23]. Wolterink et al. [19] used graph convolutional networks to predict vertices in the luminal surface mesh. Zhu et al. [23] designed a multi-scale CNN for thin artery segmentation. Wang et al. [18] aggregated local features on point clouds to remove irrelevant vessels. Zhang et al. [20] captured anotamical dependence and hierarchical topology representations for accurate segmentation. Recent studies on coronary artery segmentation gradually pay attention to extracting thin structures [19,23] or removing false vessels [18], but few of them [20] focus on both two aspects.In spite of reasonably good performance achieved by deep learning-based methods, there are still often some disconnected segments in segmentation results, which may be the mispredicted vessels, or may be caused by the presence of artifacts, stenosis, plaques, and occlusions [9]. Vascular connectivity has an important impact on the screening of vascular lesions. Several studies have been carried out to reconnect the broken vessels in 2D [7,11,13] and 3D [3,6], including coronary arteries. All these studies integrate both local vesselness details and geometric priors in vessel reconnection process. But there is no strategy which is specially designed for 3D coronary artery reconnection and show capability to handle complex disconnections and obtain the complete coronary tree to our knowledge.In this paper, we propose a topology-preserving scheme for the extraction of fully-connected coronary arteries, integrating image segmentation, centerline reconnection, and geometry reconstruction. Our major contributions are as follows: 1) We design a new centerline enhanced loss for coronary artery segmentation; 2) We propose the distance probability cosine (DP C) regularized walk algorithm to reconnect the broken centerlines; 3) We use a reconstruction method based on 2D level-set model and 3D implicit modeling technique to reconstruct vascular model along the stitched centerlines."
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,2,Method,"As shown in Fig. 1, the proposed CorSegRec consists of three stages: vascular segmentation stage, vascular reconnection stage and vascular reconstruction stage. Firstly, CCTA images are fed into a vascular segmentation network (e.g. nnU-Net [8]) and trained with the proposed centerline enhanced loss to obtain the initial segmentation result. Then, we reconnect the broken centerlines with the proposed distance, probability, and cosine similarity (DP C) regularized walk algorithm. Finally, we reconstruct the broken vascular model along the stitched cenerlines by integrating level-set segmentation and implicit modeling techniques, and obtain the fully-connected coronary tree."
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,2.1,Vascular Segmentation Based on NSDT Soft-ClDice Loss,"Shit et al. [17] proposed Soft-ClDice for tubular structure segmentation, in which vessel voxels on skeleton and near wall are assigned with the same weight. However, the centerlines of vessels contain significant topology information. Therefore, we calculate the normalized distance from foreground voxels in masks to skeletons to increase the weights of vessel skeletons. In addition, the skeletons of vessels with different radii are increased to the same weight, which will be beneficial for detecting thin vascular structures. As a result, the network will be more sensitive to vascular topology, and thus detect more hard-to-segment vessels and improve the potential of reconnection. The proposed Normalized Skeleton Distance Transform (NSDT) Soft-ClDice loss is denoted as L dscl and calculated as follows:where V L and V P represent true mask and real-valued probabilistic prediction, while S L , S P represent their skeletons, and NSDT L , NSDT P represent their normalized skeleton distance transform masks. • is the Hadamard product. The first and second parts of numerator in Eq. ( 2) and Eq. ( 3) represent skeletons and masks with NSDT (and probability) values respectively. In Eq. ( 4), in represents foreground, x -y 2 calculates the Euclidean distance between voxel x and y. R is the amplification parameter and set to the maximum skeleton distance in one case. We combine Soft-Dice loss (L dc ) and L dscl in training, and get the final loss:where γ is the balancing weight and set to 0.5 empirically."
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,2.2,Vascular Reconnection Based on DPC Walk,"To improve the connectivity of the segmented coronary arteries, we propose the DP C walk algorithm to reconnect the disconnected vascular centerlines. First, centerlines are extracted from predictions. We refer to the centerlines of the two biggest connected components as V i (i = 1, 2, ...) and other broken centerlines as CL j (j = 1, 2, ...). V i consists of a sequence of points (p 1 , p 2 , ..., p m ) where p 1 represents the head and p m represents the tail, while CL j is represented by (q 1 , q 2 , ..., q n ). Then we select candidate branches for CL j by distance and direction.To connect CL j to candidate V i , we iteratively make locally optimal decisions based on distance (D), centerline probability (P ) and cosine similarity (C). The goal is to identify a potential path from the head of CL j to the tail of V i . In general, if the current point of walker is A, then its 26-connected neighbors are denoted as Θ = {A k |k = 1, 2, ..26}, which form a cube sized 3 × 3 × 3 (see Fig. 2). The D, P and C for each neighbor point A k are calculated as follows:where D is the inverse distance between A k and p m , P is the probability that A k is on the centerline, and C is the cosine similarities between A k 's offset vector (o k ) and the offset vectors of the last two stitched points (o -1 , o -2 ). To obtain P , we first train a centerline binary classifier on mini patch level using cascade forest classifier (CFC) [22] due to its few hyper parameters and high training efficiency. While predicting P for A k , we create a patch of size l × l × l centered at A k . We then flatten the patch and input it into CFC. D, P and C are added up, with P of a weight denoted as ω, which is generally set to 5 but magnified ten-fold in environment of extremely low P . When the direction of o -1 and o -2 is too close, we avoid calculating C to prevent a straight line in walk. The calculation of DP C is defined as follows:The global direction is a vector from q 1 to p m , denoted as v. And the cosine similarity of candidate o k and v should be greater than 0. We then select the next point (A next ) with the largest DP C from the current point A (see Fig. 2):The reconnection process ends abnormally by steps beyond the limit or continuous low probabilities. If A arrives at p m , we then analyse the probability sequences and filter out unstable reconnections. Finally, we remove the vascular segments reconnected unsuccessfully and obtain reconnected centerlines."
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,2.3,Vascular Reconstruction Based on Level-Set Segmentation and Implicit Modeling,"The task of coronary reconstruction stage is to reconstruct the vascular model along the stitched centerlines and obtain a coronary artery tree with full connectivity.Firstly, we construct cross-section profiles perpendicular to the stitched centerlines, and extract the corresponding vessel contours using level-set model. We propose a Self-adaptive Local Hybrid level-set model, which can automatically calculate the dynamic threshold using the local region information to act as the lower bound of target object. The energy function is defined as follows: (11) where α and β are pre-set weights, I is image, φ is the contour represented by the level-set method, g( * ) is monotone decreasing function of [0, ∞] → R+, and H(φ) is Heaviside function. μ(x, φ) is the automatically calculated local threshold, and calculated as follows:where K σ (x) is a truncated Gaussian window sized (4k + 1) × (4k + 1). k is the largest integer smaller than the standard deviation. Secondly, the extracted vessel contours are represented as 2D Partial Shape-Preserving Spline (PSPS) functions [10], and different cross-section profiles are weighted and extruded into 3D vessel models along the centerline using the 1D PSPS basis functions (Fig. 3). Finally, these reconstructed 3D vessel models are merged with the original vessel branches to ultimately obtain the fully-connected coronary artery tree."
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,3,Experiments,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,3.1,Setup,"Dataset. We validate our approach on two datasets. The first dataset is public available from the MICCAI 2020 Automated Segmentation of Coronary Arteries (ASOCA) challenge1  [4,5]. ASOCA includes 60 CCTA images, 40 for training and 20 for testing, half of which are patients with CAD. The second dataset named PDSCA is from reference [20], including 50 CCTAs, which is evaluated by five-fold cross-validation for fair comparison.Evaluation Metric. The quantitative results are reported using Dice similarity coefficient (Dice), Hausdorff Distance (HD) and Overlap (OV). Besides, Reconnection Accuracy (RecAcc) is defined to evaluate how successful DPC Walk is to reconnect the broken vessels by calculating directly on centerlines involved in reconnection and removal:where b and s represent the broken and stitched centerlines respectively. For the points on these centerlines, we label them by comparing the reconnected centerlines with ground truth. Similarly, we obtain Reconnection Sensitivity (RecSen) and Reconnection Specificity (RecSpe):"
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,3.2,Implementation Details,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,,1),"We trained 3d_fullres version of nnU-Net [8] as baseline in environment of NVIDIA GETFORCE GTX 1080Ti, Python 3.7.11 and PyTorch 1.7.1, only loss function modified. 2) We used CascadeForestClassifier [22] implemented in Python deepforest library as centerline classifier. The length parameter l was set to 7, while for thick vessels it was 15 and finally unified to 7 by maxpool. We ramdomly sampled mini image patches centered at voxels on training set with a ratio 1:4 for positive and negative samples. The validation accuracy reached 86% and 94% for two categories respectively."
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,3.3,Comparison with State-of-the-Art Methods,"We performed quantitative comparisons with some advanced deep learning-based segmentation methods on ASOCA and PDSCA as presented in Table 1 and Table 2.For ASOCA, our CorSegRec has better performance in Dice and HD compared with other methods using vanilla 2D or 3D U-Net, and nnU-Net [8] with pre-processing, network improvement or post-processing. It proves CorSegRec has stronger ability in extracting coronary arteries compared with some methods of vascular enhancing and removal.For PDSCA, our approach shows significant improvements in Dice and HD compared with ResU-Net [12] and MPSPNet [23]. The rest of methods pay attention to the structures of objects in segmentation [14,17,20]. Differently, we focus on learning useful information from probability and direction to reconnect the broken vessels and get the best results."
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,3.4,Ablation Study,"Ablation Study on Components of DPC Walk. We conducted several experiments by combining different subsets of D, P , and C. As shown in Table 3, DPC Walk outperforms the others in terms of RecAcc, RecSen and OV. Our focus is to extract a complete coronary tree by emphasizing connection rather than removal, hence the RecSpe is relatively low. The walk without P yielded the worst results, indicating that P is the critical component of the DPC Walk algorithm. The ranking of P C and DP is close, as both have the crucial P . P C Walk lacks guidance to the destination and may encounter difficulties in longdistance reconnections, whereas DP Walk is unable to handle situations when D or P dominates. Overall, DPC Walk is the optimal choice. Ablation Study on Vascular Reconstruction. Figure 4 presents the evaluation of initial segmentation and final results using Dice (orange lines with a left y-axis) and HD (green bars with a right y-axis). The complete CorSegRec scheme achieves superior results. As shown in Fig. 5, most broken segments are correctly connected to the two largest components after reconstruction. Moreover, the reconstructed vessel models have similar appearance to the ground truth. "
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,4,Conclusion,"In this paper, we present a new topology-preserving scheme called CorSegRec to extract fully-connected coronary arteries from CCTA. Our approach combines coronary artery segmentation, centerline reconnection, and coronary model reconstruction. Notably, the proposed DPC Walk algorithm can remove most false positive segments and effectively track and connect some arteries hardto-segment. Experiment results on two CCTA datasets demonstrate that our method can achieve better connected and more accurate coronary artery extraction compared to other methods."
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,,Fig. 1 .,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,,Fig. 2 .,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,,Fig. 3 .,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,,Fig. 4 .Fig. 5 .,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,,Table 1 .,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,,Table 2 .,
CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography,,Table 3 .,
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,1,Introduction,"Semantic segmentation is a fundamental task in medical imaging used to delineate regions of interest, and has been applied extensively in diagnostic radiology. Recently, deep learning methods that use a dense probability map to classify each pixel such as UNet [2], R-CNN [3], FCN [4] have advanced the state-of-the-art in this area. Despite overall excellent performance, dense-based approaches learn using a loss defined at the pixel-level which can lead to implausible segmentation boundaries such as unexpected interior holes or disconnected blobs [1]. This is a particular problem in medical image analysis where information-poor, occluded or artefact-affected areas are common and often limit a network's ability to predict reasonable boundaries. Furthermore, minimising the largest error (Hausdorff distance (HD)) is often prioritised over general segmentation metrics such as Dice Similarity (DS) or Jaccard Coefficient (JC) in medical imaging, as stable and trustworthy predictions are more desirable.To address this problem in segmentation networks, Gaggion et al. proposed HybridGNet [1] that replaces the convolutional decoder in UNet with a graph convolutional network (GCN), where images are segmented using a polygon generated from learned points. Due to the relational inductive bias of graph networks where features are shared between neighbouring nodes in the decoder, there is a natural smoothing effect in predictions leading to stable segmentation and vastly reduced HD. In addition this approach is robust to domain shift and can make reasonable predictions on unseen datasets sourced from different medical centres, whereas dense-based methods fail due to domain memorization [5]. In HybridGNet, improved stability and HD comes at the cost of reduced contour detail conveyed by sub-optimal DS and JC metrics when compared to dense-based approaches such as UNet. Many methods have addressed this problem by rasterizing polygon points predicted by a decoder to a dense mask and then training the network using typical pixel-level losses such as Dice or crossentropy [7,9,10]. These approaches have merit but are often limited by their computational requirements. For example, in CurveGCN [7], the rasterization process uses OpenGL polygon triangulation which is not differentiable, and the gradients need to be approximated using Taylor expansion which is computationally expensive and can therefore only be applied at the fine-tuning stage [8]. While in ACDRNet [10], rasterization is differentiable, however the triangulation process is applicable only to convex polygons, and therefore limits application to more complicated polygon shapes. Rasterization is extended to non-convex polygons in BoundaryFormer [9] by bypassing the triangulation step and instead approximating the unsigned distance field. This method gives excellent results on MS-COCO dataset [11], however is computationally expensive (see Sect. 3.3).With this in mind, we return to HybridGNet which efficiently optimises points directly and theorise about the causes of the performance gap relative to dense segmentation models. We identify that describing segmentation contours using points is a sub-optimal approach because (1) points are an incomplete representation of the segmentation map; (2) the supervisory signal is usually weaker (n distances are calculated from n pairs of points, versus, h x w distances for pairs of dense probability maps); (3) the distance from the contour is more meaningful than the distance from the points representing the contour, hence minimising the point-wise distance can lead to predictions which fall on the contour being penalised."
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,,Contributions:,"We propose a novel joint architecture and contour loss to address this problem that leverages the benefits of both point and dense approaches. First, we combine image features from an encoder trained using a point-wise distance with image features from a decoder trained using a pixel-level objective. Our motivation is that contrasting training strategies enable diverse image features to be encoded which are highly detailed, discriminative and semantically rich when combined. Our joint learning strategy benefits from the segmentation accuracy of dense-based approaches, but without topological errors that regularly afflict models trained using a pixel-level loss. Second, we propose a novel hybrid contour distance (HCD) loss which biases the distance field towards pre-dictions that fall on the contour boundary using a sampled unsigned distance function which is fully differentiable and computationally efficient. To our knowledge this is the first time unsigned distance fields have been applied to graph segmentation tasks in this way. Our approach is able to generate highly plausible and accurate contour predictions with lower HD and higher DS/JC scores than a variety of dense and graph-based segmentation baselines."
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,2,Methods,
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,2.1,Network Design,"We implement an architecture consisting of two networks, a Dense-Graph (DG) network and a Dense-Dense (DD) network, as shown in Fig. 1. Each network takes the same image input X of height H and width W with skip connections passing information from the decoder of DD to the encoder of DG. For DG, we use a HybridGNet-style architecture containing a convolutional encoder to learn image features at multiple resolutions, and a graph convolutional decoder to regress the 2D coordinates of each point. In DG, node features are initialised in a variational autoencoder (VAE) bottleneck where the final convolutional output is flattened to a low dimensional latent space vector z. We sample z from a distribution Normal(μ, σ) using the reparameterization trick [12], where μ and σ are learnt parameters of the encoder. Image-to-Graph Skip Connections (IGSC) [1] are used to sample dense feature maps F I ∈ R H×W ×C from DG's encoder using node position predictions P ∈ R N ×2 from DG's graph decoder and concatenate these with previous node features F G ∈ R N ×f to give new node features F G ∈ R N ×(f +C+2) . Here, N is the number of nodes in the graph and f is the dimension of the node embedding. We implement IGSC at every encoder-decoder level and pass node predictions as output, resulting in seven node predictions. For DD, we use a standard UNet using the same number of layers and dimensions as the DG encoder with a dense segmentation prediction at the final decoder layer. "
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,2.2,Graph Convolutional Network,"Our graph decoder passes features initialised from the VAE bottleneck through six Chebyshev spectral graph convolutional [13] (ChebConv) layers using K-order polynomial filters. Briefly, this is defined bywhere Θ (k) ∈ R fin × fout are learnable weights and σ is a ReLU activation function. Z (k)  is computed recursively such that Z (1) = X, Z (2) = L•Z (1) , 2) where X ∈ R N × fin are graph features, and L represents the scaled and normalized graph Laplacian [14]. In practice, this allows for node features to be aggregated within a K-hop neighbourhood, eventually regressing the 2D location of each node using additional ChebConv prediction layers (f out = 2). As in [1], our graph network also includes an unpooling layer after ChebConv block 3 to upsample the number of points by adding a new point in between existing ones. "
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,2.3,Joint Dense-Point Learning,"As typical DG networks are trained with a point-wise distance loss and not a pixel-level loss, the image encoder is not directly optimised to learn clear and well-defined boundary features. This misalignment problem results in the DG encoder learning features pertinent to segmentation which are distinctively different from those learnt in DD encoders. This is characterised by activation peaks in different image regions such as the background and other non-boundary areas (see Fig. 2). To leverage this observation, we enrich the DG encoder feature maps at multiple scales by fusing them with image features learnt by a DD decoder using a pixel-level loss. These diverse and highly discriminative features are concatenated before being passed through the convolutional block at each level. Current GCN feature learning paradigms aim at combining feature maps from neighbouring or adjacent levels so as to aggregate similar information. This results in a ""coarse-to-fine"" approach by first passing high level features to early graph decoder blocks, followed by low level features to late graph decoder blocks. Our joint learning approach is similar to this strategy but also supplements each DG encoder level with both semantically rich and highly detailed contour features learnt by the DD network. "
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,2.4,Hybrid Contour Distance,"Mean squared error (MSE) is a spatially symmetric loss which is agnostic to true contour borders. We alleviate this pitfall by designing an additional contouraware loss term that is sensitive to the border. To achieve this we precompute a 2D unsigned distance map S from the dense segmentation map for each class c (i.e. lungs, heart), where each position represents the normalised distance to the closest contour border of that class. Specifically, for a dense segmentation map M we use a Canny filter [15] to find the contour boundary δM and then determine the minimum distance between a point x ∈ c and any point p on the boundary δM c . This function is positive for both the interior and exterior regions, and zero on the boundary. Our method is visualised in Fig. 3 (first row) and formalised below:During training, we sample S c as an additional supervisory signal using the predicted 2D point coordinates ŷi ∈ c, and combine with MSE with weight β.The effect of β is illustrated in Fig. 3 (second row) and full HCD loss function is defined below, where N is the number of points and y i ∈ c is the ground truth point coordinate.3 Experiments and Results"
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,3.1,Datasets,"We obtain four publicly available Chest X-ray segmentation datasets (JSRT [16], Padchest [17], Montgomery [18] "
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,3.2,Model Implementation and Training,"We implement our model in PyTorch and use PyTorch-Geometric for the graph layer. All models were trained for 2500 epochs using a NVIDIA A100 GPU from Queen Mary's Andrena HPC facility. For reliable performance estimates, all models and baselines were trained from scratch three times, the mean scores obtained for quantitative analysis and the median model used for qualitative analysis. Hyperparameters for all experiments were unchanged from [1]. To impose a unit Gaussian prior on the VAE bottleneck we train the network with an additional KL-divergence loss term with weight 1e -5 , and use β = 2.5e -2 for the HCD weight. For joint models we pretrain the first UNet model separately using the recipe from [1] and freeze its weights when training the full model. This is done to reduce complexity in our training procedure."
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,3.3,Comparison to Existing Methods and Ablation Study,"We compare our approach to a variety of different dense-and point-based segmentation methods. First we validate our joint DD-DG learning approach by comparing to a DD-only segmentation network (UNet [2]) and DG-only segmentation networks (HybridGNet [6], HybridGNet+ISGC [1]). Next, we explore five alternative configurations of our joint architecture to demonstrate that our design choices are superior. These are: (1) UNet Joint: a network that uses our joint learning strategy but with two DD (UNet) networks, (2) Hourglass: joint learning but with no sharing between DD decoder and DG encoder, only the output of DD is passed to the input of DG, similar to the stacked hourglass network [21,22], (3) Hourglass Concat: as above, but the output of DD is concatenated with the input and both are passed to DG, (4) Multi-task: a single dense encoder is shared between a dense and graph decoder, similar to [23], (5) No Joint: our network with no joint learning strategy.To demonstrate the effectiveness of our HCD loss, we compare to our joint network trained with the contour term removed (MSE only). Our HCD loss is similar to differentiable polygon rasterization in BoundaryFormer [9], as they both use the distance field to represent points with respect to the true boundary. However, our method precomputes the distance field for each example and samples it during training, while BoundaryFormer approximates it on the fly. Hence we also compare to a single DG network (HybridGNet+IGSC) where each point output is rendered to a dense 1028px × 1028px segmentation map using rasterization and the full model is trained using a pixel-level loss.Tables 1 and2 demonstrate that our methodology outperforms all pointand dense-based segmentation baselines on both datasets. As seen in Fig. 4, the performance increase from networks that combine image features from dense and point trained networks (column 7,9) is superior to when image features from two dense trained networks are combined (column 5). Furthermore, concatenating features at each encoder-decoder level (Tables 1 and2, row 11) instead of at the input-output level (row 5-6) shows improved performance. The addition of HCD supervision to a DG model (Tables 1 and2, row 8) gives similar improvements in segmentation when compared to using a differentiable rasterization pipeline (row 10), yet is far more computationally efficient (Table 2, column 7). "
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,4,Conclusion,"We proposed a novel segmentation architecture which leverage the benefits of both dense-and point-based algorithms to improve accuracy while reducing topological errors. Extensive experiments support our hypothesis that networks that utilise joint dense-point representations can encode more discriminative features which are both semantically rich and highly detailed. Limitations in segmentation methods using a point-wise distance were identified, and remedied with a new contour-aware loss function that offers an efficient alternative to differentiable rasterization methods. Our methodology can be applied to any graph segmentation network with a convolutional encoder that is optimised using a point-wise loss, and our experiments across four datasets demonstrate that our approach is generalizable to new data."
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,,Fig. 1 .,
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,,Fig. 2 .,
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,,Fig. 3 .,
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,,Fig. 4 .,
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,,,"Each combined dataset is randomly split into 70% train, 15% validation and 15% test examples, each with a 1024px × 1024px resolution X-ray image and ground truth point coordinates for organ contours obtained from[5]."
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,,Table 1 .,
Joint Dense-Point Representation for Contour-Aware Graph Segmentation,,Table 2 .,
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,1,Introduction,"In digital pathology, nuclear segmentation and classification are crucial tasks in disease diagnosis. Because of the diverse nature (e.g., shape, size, and color) and large numbers of nuclei, nuclei analysis in whole-slide images (WSIs) is a challenging task for which computerized processing has become a de facto standard [11]. With the advent of deep learning, challenges in nuclei analysis, such as color inconsistency, overlapping nuclei, and clustered nuclei, have been effectively addressed through data-driven approaches [6,8,15,23]. Recent studies have addressed nuclear segmentation and classification simultaneously.For example, HoVer-Net [8] and SONNET [6] performed nuclei segmentation using a distance map to identify nucleus instances and then assigned an appropriate class to each of them. Although such deep-learning-based algorithms have shown promising performance and have overcome various challenges in nuclei analysis, data imbalance among nuclei types in training data has become a major performance bottleneck [5].Data augmentation [2,16] can be an effective solution for compensating data imbalance and generalizing DNN by expanding the learnable training distribution using virtual training data. Several studies have been conducted on image classification tasks. Mixup [22] interpolates pairs of images and labels to generate virtual training data. CutOut [3] randomly masks square regions of the input during training. CutMix [21] cuts patches from the original images and pastes them onto other training images. Recently, a generative adversarial network(GAN) [7,10,12,18] has been actively studied for pathology data augmentation. However, training a GAN is challenging owing to its instability and the need for hyper parameter tuning [4]. Moreover, most previous studies mainly focused on nuclei segmentation without considering nuclei classification. Recently, Doan et al. [5] proposed a data regularization scheme that addresses the data imbalance problem in pathological images. The main concept is to cut the nuclei from a scarce class image and paste them onto the nuclei from an abundant class image. Because the source and target nuclei differ in size and shape, a distance-based blending scheme is proposed. This method slightly reduces the data-imbalance problem; however, it only considers pixel values for blending and some unrealistic blending, artifacts can be observed, which is the main limitation of this method.The main motivation for this study stems from the recent advances in generative models. Recently, the denoising diffusion probabilistic model(DDPM) [9] has gained considerable attention owing to its superior performance, which surpasses that of conventional GANs, and it has been successfully adopted in conditional environments [4,13,20]. Among these, we were inspired by Wang et al. [19], semantic diffusion model(SDM) which can synthesize a semantic image conditioned on the semantic label map. Because data augmentation for nuclei segmentation and classification requires accurate semantic images and label map pairs, we believe that SDM effectively fits the data augmentation scenario of our unbalanced nuclei data while allowing more realistic pathology image generation compared to pixel-blending or GAN-based prior work.In this study, we proposed a novel data augmentation technique using a conditioned diffusion model, DiffMix, for imbalanced nuclear pathology datasets. DiffMix consists of the following steps. First, we trained the SDM with semantic map guidance, which consists of instance and class-type maps. Next, we built custom label maps by modifying the existing imbalanced label maps. We changed the nuclei labels and randomly shifted the locations of the nuclei mask to ensure that the number of each class label was balanced and the data distribution expanded. Finally, we synthesized more diverse, semantically realistic, and well-balanced new pathology nuclei images using SDM conditioned on custom label maps. The main contributions of this study are summarized as follows:-We introduce a data augmentation framework for imbalanced pathology image datasets that can generate realistic samples using semantic diffusion model conditioned on two custom label maps, which can enlarge the data distribution. -We demonstrate the efficacy and generalization ability of our scheme with experimental results on two imbalanced pathology nuclei datasets, GLySAC [6] and CoNSeP [8], improving the performance of state-of-the-art networks. -Our experiments demonstrated that the optimal approach for data augmentation depends on the level of imbalance, balancing sample numbers, and enlarging the training data distribution, which are critical factors for consideration."
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2,Method,"In this section, the proposed method is described in detail. DiffMix operates through several steps. First, we trained the SDM first on the training data. Balancing label maps comprise several rare class labels, and enlarging label maps are composed of randomly shifted nuclei. Finally, using the pre-trained SDM and custom label maps, we synthesized realistic data to train on imbalanced datasets. Before discussing DiffMix, a brief introduction of SDM is presented. An overview of the proposed method is presented in Fig. 1."
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2.1,Preliminaries,"The SDM is a conditional denoising diffusion probabilistic model (CDPM) conditioned on semantic label maps. Based on the CDPM, SDM follows two fundamental diffusion processes i.e., forward and reverse process. The reverse process was a Markov chain with Gaussian transitions. When the added noise is sufficiently large, the reverse process is approximated by a random variable y T ∼ N (0, I), defined as follows:The forward process implements Gaussian noise addition for T timesteps based on variance schedule {β 1 , ...β T } as follows:For α t := 1β t and ᾱt := t s=1 α s , we can write the marginal distribution as follows, q(y t |y 0 ) = N (y t ;The conditional DDPM was optimized to minimize the negative log-likelihood of the data for the particular input and condition information. If noise in the data follows Gaussian distribution with a diagonal covariance matrix Σ θ (y t , x, t) = σ t I, denoising can be the optimization target by removing the noise assumed to be present in data as follows,"
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2.2,Semantic Diffusion Model (SDM),"The SDM is a U-Net-based network that estimates noise from a noisy input image. Contrary to other conditional DDPMs, the denoising network of the SDM processes the semantic label map x and noisy input y t independently. When y t is fed into the encoder, x is injected into the decoder to fully leverage semantic information [19]. For training, the SDM is trained similarly to the improved DDPM [14] to ensure that it predicts the noise involved in reconstructing the input image as well as variances to enhance the log-likelihood of the generated images. To improve the sample quality, SDM utilizes classifier-free guidance for inference. SDM replaces the semantic label map x with an empty (null) map ∅ to separate the noise estimated under the label map guidance by θ (y t |x), from the noise estimated in an unconditioned case θ (y t |∅). This strategy allows the inference of the gradient of log probability, expressed as follows:During the sampling process, the disentangled component s is increased to improve the samples from the conditional diffusion models, formulated as follows:"
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2.3,Custom Semantic Label Maps Generation,"Figure 1 illustrates the process of creating custom label maps to condition the semantic diffusion model for synthesizing the desired data based on the original input image label y 0 . We prepared custom semantic label maps to condition the SDM and used it to synthesize data for our imbalanced datasets. Therefore, we considered two types of semantic label maps to improve imbalanced datasets. First, balancing maps to balance the number of nuclei among different nuclei types. GradMix [5] increased the number of fewest type nuclei in datasets by cutting, pasting, and smoothing both images and labels. However, we used only mixed labels in our experiments. Second, enlarging maps perturbs the positions of the nuclei instances to diversify the datasets. We randomly moved nuclei positions on semantic maps to synthesize diverse image patches with SDM by conditioning them with unfamiliar semantic maps to allow the diffusion model to generate significant patches."
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,2.4,Image Synthesis,"We synthesized the virtual data with a pretrained SDM conditioned on custom semantic label maps x. Figure 1 depicts the data sampling process in the SDM. Before inputting the original image y 0 into diffusion net f , we added noise to y 0 . The semantic label and noisy image y t were used simultaneously. To synthesize virtual images, we built two label maps and trained a semantic diffusion model. Before data synthesis, we added noise to the input image y 0 . Subsequently, we input y t and x to the pretrained denoising network f , y t for the encoder, and x for the decoder. As SDM generates samples, it uses an empty label ∅ to generate the unconditioned output. The image was sampled from an existing noisy patch, depending on the predefined time steps. Therefore, we generated patches conditioned on custom semantic label maps. In this process, we added noise to the input image y 0 ; thus, we input the noisy input y t into the pretrained denoising network f , and following [19], we input the custom semantic label maps to the decoder parts. Thereafter, the semantic label maps condition the SDM to synthesize image data that satisfy the label maps. We sampled the new data using the DDIM [17] process, which reduces the number of sampling steps, but with high-quality data.3 Experiments"
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,3.1,Datasets,"In this study, we used two imbalanced nuclear segmentation and classification datasets. First, GLySAC [6] "
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,3.2,Implementation Details,"We used an NVIDIA RTX A6000 to train the SDM for 10000epochs. For data synthesis, we implemented a DDIM-based diffusion process from 1000 to 100 and added noise to the input image to the SDM, setting T as 55. In our scheme, ∅ is defined as the all-zero vector, as in [19] and s = 1.5 when sampling both datasets. We conducted experiments on two baseline networks, SONNET [6] and HoVer-Net [8]. SONNET was implemented using Tensorflow version 1.15 [1] as the software framework with two NVIDIA GeForce 2080 Ti GPUs. HoVer-Net was trained using PyTorch 1.11.0, as the software framework, with one NVIDIA GeForce 3090 Ti GPU. We implemented 4-fold cross validation for SONNET and 5-fold cross validation for HoVer-Net.  epochs for DiffMix-B and DiffMix-E, and trained 50 epochs for GradMix and DiffMix."
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,3.3,Results,"Figure 2 shows a qualitative comparison of the synthesized patches. The original patch is shown on the left, followed by the enlarging, balancing, and GradMix patches. The two types of patches were well harmonized with the surrounding structure compared to GradMix. Moreover, using our scheme, many patches can be synthesized using a semantic diffusion model. For a quantitative evaluation, we implemented two state-of-the-art networks, HoVer-Net and SONNET, on two publicly imbalanced nuclei-type datasets, GLySAC and CoNSeP. Table 1 lists the results of the five experiments per network for each dataset. Furthermore, we conducted ablation studies on balanced (DiffMix-B) and enlarged (DiffMix-E) patch datasets. Before analyzing the experimental results, we computed the proportion of the least common nuclei type in each dataset. We found that miscellaneous nuclei accounted for approximately 19% of the nuclei in GLySAC, but only 2.4% in CoNSeP. This indicates that GLySAC is more balanced in terms of nuclei type than CoNSeP. Considering this information, we analyzed the experimental results. DiffMix-E exhibited the highest classification performance on the GLySAC dataset. This result indicates that enlarging semantic map-based data synthesis, such as DiffMix-E, provides sufficient opportunities to enlarge the learning distribution for GLySAC. However, for the CoNSeP dataset, DiffMix-E performed lower than the other methods, suggesting that the dataset is slightly balanced; this, it is important to expand the available data distribution. DiffMix showed the highest performance in most metrics, with 4% and 9% margins from the second-highest result in classifying miscellaneous data, successfully diminishing the classification performance variability among class types. Furthermore, DiffMix improved the segmentation and classification performances of the two state-of-the-art networks, even when compared with GradMix."
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,4,Conclusion,"In this study, we introduced DiffMix, a semantic diffusion model-based data augmentation framework for imbalanced pathology nuclei datasets. We experimentally demonstrated that our method can synthesize virtual data that can balance and enlarge imbalanced nuclear pathology datasets. Our method also outperformed the state-of-the-art GradMix in terms of qualitative and quantitative comparisons. Moreover, DiffMix enhances the segmentation and classification performance of two state-of-the-art networks, HoVer-Net and SONNET, even in imbalanced datasets, such as CoNSeP. Our results suggest that DiffMix can be used to improve the performance of medical image processing tasks in various applications. In the future, we plan to improve the performance of the diffusion model to generate various pathological tissue types."
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,,Fig. 1 .,
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,,,
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,,Fig. 2 .,
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,,Table 1 .,"lial, and 6507 miscellaneous. Second, CoNSeP[8], comprising 41 H&E images of size 1000×1000 pixels, was divided into 27 training images and 14 test images. CoNSeP comprised 24319 nuclei, and four nuclei classes: 5537 epithelial nuclei, 3941 inflammatory, 5700 spindle, and 371 miscellaneous nuclei."
DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_33.
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,1,Introduction,"For the past several years, in skin disease diagnosis, deep learning (DL) techniques have been extensively studied, due to its effectiveness and outstanding diagnostic performance [8,16,19]. For example, Wu et al. used a custom dataset to develop an EfficientNet-b4-based DL model and successfully diagnosed skin diseases [19]. Srinivasu et al. designed an advanced DL model, by combining long short-term memory (LSTM) with MobileNet and achieved improved performance as well as fast prediction time in skin disease diagnosis [16]. For the development of DL models, a large number of datasets are needed for accurate model fitting at the training stage. Acquiring enormous skin disease datasets, however, at a single medical site is challenging. As such, the performance of DL models is often limited, due to a small number of datasets [2,16,20]. To overcome the limitations mentioned above, federated learning (FL) can be a viable solution in the context of digital healthcare, especially in the COVID-19 pandemic era [15]. In particular, FL enables that the edge devices only share the gradient of DL models without sharing data, such that FL improves the data privacy and security. Moreover, FL allows to acquire many heterogeneous images from edge devices at multiple medical sites [1,6,7,10,15]. For example, B. McMahan et al. developed a protocol to average the gradients from decentralized clients, without data sharing [10]. K. Bonawitz et al. built high-level FL systems and architectures in a mobile environment [1]. However, DL models in the FL environment were optimized to deal with datasets from multiple clients; therefore, while the DL models yielded a generalized prediction capability across all of the domains involved, the DL models cannot efficiently perform personalized diagnosis, which is deemed a weakness of FL. To alleviate this, personalized FL methodologies have been emerged [12,14,17].We propose to develop a personalized diagnosis network, called APD-Net in the FL framework to target personalized diagnostics. Our APD-Net comprises two novel techniques, including (1) a genetic algorithm (GA)-based fine-tuning method and (2) a dual-pipeline (DP) architecture for the DL models. The GAbased fine-tuning method improves APD-Net on each edge device by adaptively customizing the optimized DL model. We validated our framework on three public datasets, including 7pt [4], Human Against Machine (HAM) [18], and International Skin Imaging Collaboration (ISIC) [13], as well as our own datasets.Experimental results demonstrated that the APD-Net yielded outstanding performance, compared with other comparison methods, and achieved adaptively personalized diagnosis. The contributions of this paper are three-fold:• We developed a mobile-and FL-based learning (APD-Net) for skin disease diagnosis and achieved superior performance on skin disease diagnosis for public and custom datasets. • We introduce a customized GA for APD-Net, combined with a corresponding network architecture, resulting in improved personalized diagnostic performance as well as faster prediction time.  This work aims to develop a mobile-based FL system that can provide a personalized and customized diagnosis to patients across different clients. The APD-Net framework, shown in Fig. 1, includes common procedures (1)-( 5) that are typically used in a general FL system, as well as unique sequences ( 6) and (7) for adaptively personalized diagnosis in the proposed system. In particular, as depicted in Fig. 1 (6), the parameters transferred from the server are fine-tuned to be suitable for each domain."
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,2,Methodology,"The detailed procedure to fine-tune APD-Net is described in Fig. 2(a). Here, P g represents a set of generalized parameters in the FL server, and P * p k represents a set of optimally fine-tuned parameters in the k th client. Before diagnosis, a set of newly fine-tuned parameters (P f ) is generated by GA. By jointly using both P g and P * p k , the initial population of a set of personalized parameters (Pp k , here chromosome) is achieved, where i = 1, 2, 3, ..., N , and N is the number of populations. The evolutionary operations, including crossover and mutation, then offer a new 4N number of chromosomes. Here, the fitness scores are compared for all individual chromosomes, and the N number of chromosomes that achieve a high fitness score is selected to form a new population. Subsequently, the fitness score is calculated using the DP architecture, as illustrated in Fig. 2(d). After several generations of fine-tuned parameters, the chromosome, evaluated as the highest fitness score, replaces the personalized parameters, P * p k . Finally, the optimally fine-tuned parameters are utilized to diagnose patients in each client."
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,2.1,Dual-Pipeline (DP) Architecture for APD-Net,"Since the proposed FL system is implemented in a mobile-based environment, MobileNetV3 is employed as a baseline network of APD-Net. In particular, to achieve faster fine-tuning time, MobileNetV3-small is utilized. Here, the novel architecture of APD-Net is differentiated by the use of the DP architecture that allows (1) to diagnose patients adaptively, and (2) to evaluate the fitness function. Therefore, in the DP architecture, one pipeline employs the generalized parameters (P g ) from the FL server, whereas the other pipeline employs the personalized parameters (P * p k ) in the client."
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,2.2,Customized Genetic Algorithm,"In the FL environment, data privacy is achieved by transferring gradients without sharing data. Therefore, since the domain of one client is not recognizable by another client, the domain gap between two clients is not computable. Therefore, in the proposed FL system, to adaptively fine-tune the parameters transferred from the FL server to be personalized concerning the domain of one client, the GA is employed. The GA is the optimal solution for adaptively personalized diagnosis in the FL environment, since it heuristically searches for another local minimum point regardless of the domain gaps. The detailed procedures of the GA are illustrated in Algorithm I (Appendix)."
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Gene and Chromosome.,"A gene and a chromosome are modeled to represent fine-tuned parameters (P (i) f ) by jointly using P g and P * f . The gene indicates the internal division between P g and P * p . Figure 2(b) represents the mathematical modeling of a gene and a chromosome. In particular, let gk ∈ [-1, 1] be a k th gene in the i th chromosome, and then the k th convolution weight (Pwhere P g | k and P * p | k are the k th convolution parameter in P g and P * f , respectively. Here,is calculated by the internal division between P g and P * p .Crossover. Let crossover (P (i) , P (j) ) be a crossover function by jointly using two chromosomes, and then the k th genes of P (i) | k and P (j) | k are changed in the 50% probability when the constraint ofHere, since the convolution parameters in a deep depth are rarely fine-tuned due to a gradient vanishing problem, l k exhibits a relatively larger value when k becomes larger. In addition, since we experimentally demonstrated that l k ≥ 0.15 provides a much longer time to fine-tune APD-Net, we constrained l k ≤ 0.15, and the fixed values of l k are randomly determined for each experiment.Mutation. mutation (P (i) ) represents a mutation function onto a chromosome, of which the k th gene is P (i) | k , such that it is defined as follows:(where η is the randomly selected value in the constraint range for each individual gene. While training the DL model, we experimentally verified that the convolution weights are changed within the range of the maximum 0.2%. Therefore, here, μ is initially determined as 2e-3 (0.2%), but it depends on the variance of convolution weights in every epoch.Selection. As illustrated in Algorithm I, the newly generated chromosomes, which yield a large value of the fitness score, are contained in a new population.In APD-Net, the fitness score is evaluated by the fitness function that is jointly utilized in the architecture of APD-Net as illustrated in Fig. 2(d). Let C(I; P ) be the output of Part I using the parameter P with an input image (I). As illustrated in Fig. 2(d), we can then calculate three outputs of C(I; P g ), C(I; P * p ), and C(I; P (i) f ), which use a generalized parameter, a personalized parameter, and a candidate parameter for a fine-tuned parameter, respectively. To achieve personalized diagnosis, the fine-tuned parameter should be similar to the personalized parameter (client) rather than the generalized parameter (server). Therefore, we define the fitness score as the ratio of the similarity between P * p and P (i) f to the similarity between P g and P (i) f . Here, we use a softmax function to convert the ratio into probability, as formulated below:where sim(x, y) is cosine similarity between x and y, and exp(x) is the exponential function. Note that APD-Net provides the fitness function related to its architecture, and the fitness function is more reliable than other fitness functions used in accuracy-based GA. Therefore, the APD-Net with our GA offers high accuracy in both the conventional diagnosis for overall patients and the personalized diagnosis for each patient at a specific client."
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,2.3,Training and Fine-Tuning APD-Net,"To summarize, (1) APD-Net is initially trained in the FL server using gradients from many clients. The cross-entropy loss function is utilized for training APD-Net, and the generally optimized parameter (P g ) is achieved in the initial training.(2) P g is then transferred to each client, and (3) the fine-tuned procedures are processed by jointly using P g and a personalized parameter (P * p ). By using the proposed GA, the new population of fine-tuned parameters (P (i) f where i = 1, 2, ..., 4N ) is generated, and the chromosome with the highest fitness score becomes a new personalized parameter. (4) After the diagnosis, the gradients are shared with the FL server, and P g is newly optimized.  3 Experimental Results"
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,3.1,Experimental Setup,"Dataset. To evaluate the performance and feasibility of APD-Net, we used three public datasets, including 7pt, ISIC, and HAM, and detailed descriptions for datasets are illustrated in Table 1. Furthermore, in this work, we collected skin images through the tertiary referral hospital under the approval of the institutional review board (IRB No. 1908-161-1059) and obtained images with the consent of the subjects according to the principles of the Declaration of Helsinki from 51 patients and subjects. The dataset included four categories,   including eczema, dermatitis, rosacea, and normal skin, with 258, 294, 738, and 1,200 images, respectively, as illustrated in Table 2(Left).To compensate for the limited number of images in the test set, a 4-fold cross-validation approach was employed. To assess the performance of the proposed network as well as compared networks, two distinct FL environments were considered: (1) an FL simulation environment to evaluate the performance of APD-Net and (2) a realistic FL environment to analyze the feasibility of APD-Net. For the FL simulation environment in Experiment I, public datasets were employed, and the distribution of samples was re-sampled using t-Distributed Stochastic Neighbor Embedding (t-SNE) [9]. The images in all skin datasets were subsequently re-grouped, as illustrated in Fig. 3. In contrast, for Experiment II, we utilized a custom dataset for a realistic FL environment. Six DL models that have shown exceptional performances in DA, FL, and skin disease diagnosis were used as comparison methods to evaluate the FL and DL performance of APD-Net. The salient characteristics of these models are summarized in Table 2(Right)."
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,3.2,Experiment I. FL Simulation,"Ablation Study. An ablation study was conducted to evaluate the impact of GA and DP on diagnostic performance. APD-GA and APD-GA-DP indicate APD-Net without GA and without GA and DP, respectively. APD-DP was not evaluated since GA could not be realized without the DP architecture. As illustrated in Table 3(Left), the APD-Net with GA and DP yielded the best performance. Here, it is important to note that the DP architecture also improved the performance of the models for adaptively personalized diagnosis, similar to GA, by jointly using personalized and generalized parameters in the DP architecture.Comparison Analysis. Performance of APD-Net was compared against those of the other DL models for adaptively personalized diagnosis. Table 3(Right) shows the performances of the DL models in every client (group). APD-Net yielded an accuracy of 9.11%, which was higher than the other DL models for adaptively personalized diagnosis. Furthermore, Fig. 4 demonstrates that the prediction with images from other clients yielded lower accuracy.  "
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,3.3,Experiment II. Realistic FL Environment,"To verify the feasibility of APD-Net, the performance of APD-Net was evaluated using the customized datasets acquired from our devices for adaptively personalized diagnosis. The performance of APD-Nets was compared against the other DL models. Since the prediction time is critical in the mobile-based environment, the prediction times of the DL models were compared in addition to the accuracy. As illustrated in Fig. 5(Left), APD-Net (Ours) yielded an outstanding performance as well as a shorter prediction time compared with the other DL models for adaptively personalized diagnosis. In addition, Fig. 5(Right) shows the performances of APD-Nets for adaptively personalized diagnosis in every client. APD-Net achieved an improved accuracy of 9.9%, compared with the DL models in adaptively personalized diagnosis. Furthermore, Fig. 6(Left) illustrates the performance of APD-Net. Since the number of images is relatively small, the images were divided into three clients. In this work, 3-fold cross-validation was applied to evaluate our APD-Net. The results showed that our APD-Net has the potential to be used in the FL environment with an accuracy of 88.51%.In addition, to verify the similarity as a fitness score, we examined the correlation between the similarity score and the prediction accuracy. The fitness score and accuracy were calculated corresponding to many input images and various versions of the fine-tuned parameters. Figure 6 shows that the classification accuracy was improved when the parameters with high similarity scores were used. Since the GA requires significantly longer fine-tuning time, the fine-tuning step was detached from the training and prediction steps at the application level. In particular, in the synchronization step where the generalized parameters were transferred from the FL server, the GA-based fine-tuning was realized, and a new optimal personalized parameter was generated. Therefore, the prediction time without the fine-tuning step was significantly reduced. It is important to note that, at the application level, a slightly longer synchronization time is generally considered more acceptable than a longer prediction time."
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,4,Conclusion,"In this work, we first carried out the adaptively personalized diagnosis task in the FL system, and developed a novel DL model, termed APD-Net, with the FL system. Our APD-Net yielded outstanding performance, by jointly using a novel DP architecture and a GA-based fine-tuning technique for adaptively personalized diagnosis. The DP architecture enabled extracting feature maps using generalized and personalized parameters, thereby providing high diagnostic accuracy. In addition, GA heuristically generated the best performance for the personalized parameters. Using the public skin datasets, including 7pt, ISIC, and HAM, our APD-Net was able to achieve an improved accuracy of 9.9% compared with other state-of-the-art DL models for the adaptively personalized diagnosis. The ablation study also demonstrated that the partial fine-tuning technique achieved higher accuracy as well as a faster fine-tuning time. Furthermore, the feasibility of APD-Net in FL was demonstrated by using the customized datasets acquired from our system, thus suggesting that the proposed system with APD-Net can be applied to the multiclass classification task of various skin diseases. However, achieving even faster prediction speeds is possible by incorporating computing performance enhancement methodologies from related fields or employing lightweight techniques like quantization, and it remains a future work."
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Fig. 1 .,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Fig. 2 .,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Fig. 3 .,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Fig. 4 .,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Fig. 5 .,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Fig. 6 .,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,•,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Table 1 .,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Table 2,. (Left) The number of skin images in the customized dataset for Experiment II. (Right) Summary of key features of the classification models.
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Table 3 .,
Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_37.
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,1,Introduction,"Domain shift (see Fig. 1) may cause deep classifiers to struggle in making plausible predictions during testing [15]. This risk seriously limits the reliable deployment of these deep models in real-world scenarios, especially for clinical analysis. Collecting data from the target domain to retrain from scratch or fine-tune the trained model is the potential solution to handle the domain shift risks. However, obtaining adequate testing images with manual annotations is laborious and impracticable in clinical practice. Thus, different solutions have been proposed to conquer the problem and improve the model robustness. From left to right: 1) four-chamber views of heart from Vendor A&B, 2) abdomen planes from Vendor C&D, 3) fundus images with diabetic retinopathy of grade 3 from Center E-G. Appearance and distribution differences can be seen in each group.Unsupervised Domain Adaptation (UDA) refers to training the model with labeled source data and adapting it with target data without annotation [6,18,22]. Recently, Fourier domain adaptation was proposed in [25,26], with the core idea of achieving domain transfer by replacing the low-frequency spectrum of source data with that of the target one. Although effective, they require obtaining sufficient target data in advance, which is challenging for clinical practice. Domain Generalization (DG) aims to generalize models to the unseen domain not presented during training. Adversarial learning-based DG is one of the most popular choices that require multi-domain information for learning domain-invariant representations [11,12]. Recently, Liu et al. [14] proposed to construct a continuous frequency space to enhance the connection between different domains. Atwany et al. [1] imposed a regularization to reduce gradient variance from different domains for diabetic retinopathy classification. One drawback is that they require multiple types of source data for extracting rich features. Other alternatives proposed using only one source domain to perform DG [4,27]. However, they still heavily rely on simulating new domains via various data augmentations, which can be challenging to control.Test-Time Adaptation (TTA) adapts the target data or pre-trained models during testing [8,9]. Test-time Training (TTT) [21] and TTT++ [15] proposed to minimize a self-supervised auxiliary loss. Wang et al. [23] proposed the TENT framework that focused on minimizing the entropy of its predictions by modulating features via normalization statistics and transformation parameters estimation. Instead of batch input like the above-mentioned methods, Single Image TTA (SITA) [10] was proposed with the definition that having access to only one given test image once. Recently, different mechanisms were developed to optimize the TTA including distribution calibration [16], dynamic learning rate [24], and normalizing flow [17]. Most recently, Gao et al. [5] proposed projecting the test image back to the source via the source-trained diffusion models. Although effective, these methods often suffer from the problems of unstable parameter estimation, inaccurate proxy tasks/pseudo labels, difficult training, etc. Thus, a simple yet flexible approach is highly desired to fully mine and combine information from test data for online adaptation.In this study, we propose a novel framework called Fourier TTA (FTTA) to enhance the model robustness. We believe that this is the first exploration of dual-adaptation design in TTA that jointly updates input and model for online refinement. Here, one assumption is that a well-adapted model will get consistent outputs for different transformations of the same image. Our contribution is twofold. First, we align the high-level features and attention regions of transformed paired images for complementary consistency at global and local dimensions. We adopt the Fourier -based input adaptation as the transformation strategy, which can reduce the distances between unseen testing images and the source domain, thus facilitating the model learning. We further propose to smooth the hard consistency via the weighted integration of features, thus reducing the adaptation difficulties of the model. Second, we employ self-consistency of frequency-based style interpolation to regularize the output logits. It can provide direct and effective hints to improve model robustness. Validated on three classification datasets, we demonstrate that FTTA is general in improving classification robustness, and achieves state-of-the-art results compared to other strong TTA methods. "
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,2,Methodology,"Figure 3 shows the pipeline of FTTA. Given a trained classifier G, FTTA first conducts F ourier-based input adaptation to transfer each unseen testing image x t into two source-like images (x t1 and x t2 ). Then, using linear style interpolation, two groups of images will be obtained for subsequent smooth consistency measurement at global features (L f ) and local visual attention (L c ). Furthermore, regularization in the logit space can be computed following the style interpolation consistency in the frequency space (L s ). Finally, FTTA updates once based on the multi-consistency losses to output the final average prediction.Fourier-Based Input Adaptation for Domain Transfer. Transferring unseen images to the known domain plays an important role in handling domain shift risks. In this study, instead of learning on multiple domains, we only have access to one single domain of data during training. Therefore, we need to utilize the limited information and find an effective way to realize the fast transfer from the unseen domain to the source domain. Inspired by [20,25,26], we adopt the Fast Fourier Transform (FFT) based strategy to transfer the domain information and achieve input adaptation during testing. Specifically, we transfer the domain information from one image to another by low-frequency amplitude (A) swapping while keeping the phase components (see Fig. 2). This is because in Fourier space, the low-frequency A encodes the style information, and semantic contents are preserved in P [25]. Domain transfer via amplitude swapping between image x s to x t can be defined as:where M is the circular low-pass filtering with radius r to obtain the radialsymmetrical amplitude [26]. λ aims to control the degree of style interpolation [14], and it can make the transfer process continues (see Fig. 2). After inverse FFT (IFFT, F -1 ), we can obtain an image x t by F -1 (A x t , P xt ).Since one low-level amplitude represents one style, we have n style choices. n is the number of training data. The chosen styles for input adaptation should be representative of the source domain while having significant differences from each other. Hence, we use the validation set to select the styles by first turning the whole validation data into the n styles and calculating n accuracy. Then, styles for achieving top-k performance are considered representative, and L2 distances between the C 2 K pairs are computed to reflect the differences. Smooth Consistency for Global and Local Constraints. Building a reliable consistency measurement of paired inputs is the key to achieving TTA. In this study, we propose global and local alignments to provide a comprehensive consistency signal for tuning the model toward robustness. For global consistency, we compare the similarity between high-level features of paired inputs. These features encode rich semantic information and are therefore well-suited for assessing global consistency. Specifically, we utilize hard and soft feature alignments via pixel-level L2 loss and distribution-level cosine similarity loss, to accurately compute the global feature loss L f . To ensure local consistency, we compute the distances between the classification activation maps (CAMs) of the paired inputs. It is because CAMs (e.g., Grad-CAM [19]) can reflect the local region the model focuses on when making predictions. Forcing CAMs of paired inputs to be close can guide the model to optimize the attention maps and predict using the correct local region for refining the prediction and improving model robustness (see Fig. 3, c t1 is encouraged to be closer with c t2 for local visual consistency). Finally, the distances between two CAMs can be computed by the combination of L2 and JS-divergence losses.Despite global and local consistency using single paired images can provide effective self-supervised signals for TTA in most cases, they may be difficult or even fail in aligning the features with a serious gap during testing. This is because the representation ability of single-paired images is limited, and the hard consistency between them may cause learning and convergence difficulties. For example, the left-upper CAMs of c1 and c2 in Fig. 3 are with no overlap. Measuring the local consistency between them is meaningless since JS divergence will always output a constant in that case. Thus, we first generate two groups of images, each with four samples, by style interpolation using different λ. Then, we fed them into the model for obtaining two groups of features. Last, we propose learnable integration with parameters u and v to linearly integrate the global and local features. This can enhance the feature representation ability, thus smoothing the consistency evaluation to accelerate the adaptation convergence.Style Consistency for Regularization on Logit Space. As described in the first half of Eq. 1, two low-level amplitudes (i.e., styles) can be linearly combined into a new one. We propose to use this frequency-based style consistency to regularize the model outputs in logit space, which is defined as the layer before softmax. Thus, it is directly related to the model prediction. A total of 8 logit pairs can be obtained (see Fig. 3), and the loss can be defined as:where x t and x ti,i∈1,2 are the testing image and two transformed images after input adaptation. x ij represents style-interpolated images controlled by λ j . y log (•) outputs the logits of the model."
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,3,Experimental Results,"Materials and Implementations. We validated the FTTA framework on three classification tasks, including one private dataset and two public datasets (see Fig. 1). Approved by the local IRB, the in-house Fetal-17 US dataset containing 8727 standard planes with gestational age (GA) ranging from 20 to 24 +6 weeks was collected. It contains 17 categories of planes with different parts, including limbs (4), heart (4), brain (3), abdomen (3), face (2), and spine (1). Four 10-year experienced sonographers annotated one classification tag for each image using the Pair annotation software package [13]. Fetal-17 consists of two vendors (A&B) and we conducted bidirectional experiments (A2B and B2A) for method evaluation. The Maternal-fetal US dataset named Fetal-8 (GA: 18-40 weeks) [2] 1 contains 8 types of anatomical planes including brain (3), abdomen (1), femur (1), thorax (1), maternal cervix (1), and others (1). Specifically, 10850 images from vendors ALOKA and Voluson (C&D) were used for bidirectional validation (C2D and D2C). Another public dataset is a fundus dataset named Messidor, which contains 1200 images from 0-3 stage of diabetic retinopathy [3]2 . It was collected from three ophthalmologic centers (E, F&G) with each of them can treated as a source domain, allowing us to conduct three groups of experiments (E2FG, F2EG and G2EF). Dataset split information is listed in Table 1. We implemented FTTA in Pytorch, using an NVIDIA A40 GPU. All images were resized to 256 × 256, and normalized before input to the model. For the fetal datasets, we used a 1-channel input, whereas, for the fundus dataset, 3-channel input was utilized. During training, we augmented the data using common strategies including rotation, flipping and contrast transformation. We selected ImageNet-pretrained ResNet-18 [7] as our classifier backbone and optimized it using the AdamW optimizer in 100 epochs. For offline training, with batch size = 196, the learning rate (lr) is initialized to 1e-3 and multiplied by 0.1 per 30 epochs. Cross-entropy loss is the basic loss for training. We selected models with the best performance on validation sets to work with FTTA. For online testing, we set the lr equal to 5e-3, and λ j,j=1,2,3,4 for style interpolation was set as 0.2, 0.4, 0.6, and 0.8, respectively. We only updated the network parameters and learnable weights once based on the multi-level consistency losses function before obtaining the final predictions. Quantitative and Qualitative Analysis. We evaluated the classification performance using four metrics including Accuracy (Acc, %), Precision (Pre, %), Recall (Rec, %), and F1-score (F1, %). Table 2 compares the FTTA (Ours) with seven competitors including the Baseline without any adaptation and six stateof-the-art TTA methods. Upper-bound represents the performance when training and testing on the target domain. It can be seen from Upper-bound and Baseline that all the metrics have serious drops due to the domain shift. Ours achieves significant improvements on Baseline, and outperforms all the strong competitors in terms of all the evaluation metrics, except for the Pre in Group B2A. It is also noted that the results of Ours are approaching the Upper-bound, with only 5.31% and 4.40% gaps in Acc.We also perform ablation studies on the Fetal-17 dataset in the last 7 rows of Table 2. Table 3 reports the results of FTTA on two public datasets. We only perform methods including Upper-bound, Baseline, and Ours with evaluation metrics Acc and F1. Huge domain gaps can be observed by comparing Upper-bound and Baseline. All five experimental groups prove that our proposed FTTA can boost the classification performance over baseline, and significantly narrow the gaps between upper-bound. Note that MESSDIOR is a challenging dataset, with all the groups having low Upper-bounds. Even for the multi-source DG method, Messidor only achieves 66.70% accuracy [1]. For the worst group (F2EG), Acc drops 35.13% in the testing sets. However, the proposed FTTA can perform a good adaptation and improve 26.30% and 5.96% in Acc and F1.Figure 4 shows the CAM results obtained by Ours. The red boxes denote the key regions, like the eyes in (a), which were annotated by sonographers and indicate the region-of-interest (ROI) with discriminant information. We consider that if one model can focus on the region having a high overlap with the ROI box, it has a high possibility to be predicted correctly. The second columns visualize the misclassified results before adaptation. It can be observed via the CAMs that the focus of the model is inaccurate. Specifically, they spread dispersed on the whole image, overlap little with the ROI, or with low prediction confidence. After TTA, the CAMs can be refined and close to the ROI, with prediction corrected."
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,4,Conclusion,"In this study, we proposed a novel and general FTTA framework to improve classification robustness. Based on Fourier-based input adaptation, FTTA is driven by the proposed multi-level consistency, including smooth global and local constraints, and also the self-consistency on logit space. Extensive experiments on three large datasets validate that FTTA is effective and efficient, achieving stateof-the-art results over strong TTA competitors. In the future, we will extend the FTTA to segmentation or object detection tasks."
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,Fig. 1 .,
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,Fig. 2 .,
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,Fig. 3 .,
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,Fig. 4 .,
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,,
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,Table 1 .,
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,Table 2 .,
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,Table 3 .,
Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_22.
Performance Metrics for Probabilistic Ordinal Classifiers,1,Introduction and Related Work,"The output of predictive machine learning models is often presented as categorical values, i.e. ""hard"" class membership decisions. Nonetheless, understanding the faithfulness of the underlying probabilistic predictions giving rise to such hard class decisions can be essential in some critical applications. Meaningful probabilities enable not only high model accuracy, but also more reliable decisions: a doctor may choose to order further diagnostic tests if a binary classifier gives a p = 45% probability of disease, even if the hard prediction is ""healthy"" [2]. This is particularly true for ordinal classification problems, e.g. disease severity staging [6,7] or medical image grading [14,21]. In these problems, predictions should be as close as possible to the actual category; further away predictions must incur in heavier penalties, as they have increasingly worse consequences.There is a large body of research around performance metrics for medical image analysis [20]. Most existing measures, like accuracy or the F1-score, focus on assessing hard predictions in specific ways that capture different aspects of a problem. In ordinal classification, the recommended metrics are Quadratic-Weighted Kappa and the Expected Cost [5,16]. In recent years, measuring the performance of ""soft"" probabilistic predictions has attracted an increasing research interest [12,19]. For this purpose, the current consensus is to employ Calibration Errors like the ECE and Proper Scoring Rules like the Brier score [16]. We will show that other metrics can instead be a better choice for assessing probabilistic predictions in the particular case of ordinal classification problems.How to measure the correctness of probabilistic predictions is a decades-old question, naturally connected to forecasting, i.e. predicting the future state of a complex system [9]. A key aspect of forecasting is that, contrary to classifiers, forecasters do not output hard decisions, but probability distributions over possible outcomes. Weather forecasts do not tell us whether it will rain tomorrow or not, they give us a probability estimate about the likelihood of raining, leaving to us the decision of taking or not an umbrella, considering the personal cost of making such decision. The same applies for financial investments or sports betting, where it is also the final user who judges risks and makes decisions based on probabilistic forecasts. In this context, Proper Scoring Rules (PSRs) have been long used by the forecasting community to measure predictive performance [10]. PSRs are the focus of this paper, and will be formally defined in Sect. 2.1."
Performance Metrics for Probabilistic Ordinal Classifiers,,Relation to Calibration:,"A popular approach to assess the quality of probabilistic predictions is measuring calibration. A model is well calibrated if its probabilistic predictions are aligned with its accuracy on average. PSRs and calibration are intertwined concepts: PSRs can be decomposed into a calibration and a resolution component [8]. Therefore, a model needs to be both calibrated and resolved (i.e. having sharp, or concentrated probabilities) in order to have a good PSR value. For example, if a disease appears in 60% of the population, and our model is just ""return p=0.6"", in the long run the model is correct 60% of the time, and it is perfectly calibrated, as its confidence is fully aligned with its accuracy, despite having zero predictive ability. If the model predicted in a ""resolved"" manner with p = 0.99 the presence of the disease, but being correct only 70% of the time, then it would be overconfident, which is a form of miscalibration. Only when the model is simultaneously confident and correct can it attain a good PSR value.The two most widely adopted PSRs are the Brier and the Logarithmic Score [1,11]. Unfortunately, none of these is appropriate for the assessment of ordinal classification probabilities [3]. A third PSR, long used by forecasting researchers in this scenario, the Ranked Probability Score (RPS, [4]), appears to have been neglected so far in biomedical image grading applications. This paper first covers the definition and basic properties of PSRs, and then motivates the use the RPS for ordinal classifiers. We also illustrate a counter-intuitive behavior of the RPS, and propose a simple modification to solve it. Our experiments cover two relevant biomedical image grading problems and illustrate how the RPS can better assess probabilistic predictions of ordinal classification models."
Performance Metrics for Probabilistic Ordinal Classifiers,2,Methods,
Performance Metrics for Probabilistic Ordinal Classifiers,2.1,"Scoring Rules -Notation, Properties, Examples","We consider a K-class classification problem, and a classifier that takes an image x and maps it into a vector of probabilities p ∈ [0, 1] K . Typically, p is the result of applying a softmax operation on the output of a neural network. Suppose x belongs to class y ∈ {1, ..., K}, and denote by y its one-hot representation. A Scoring Rule (SR) S is any function taking the probabilistic prediction p and the label y and producing a number S(p, y) ∈ R (a score). Here we consider negatively oriented SRs, which assign lower values to better predictions.Of course, the above is an extremely generic definition, to which we must now attach additional properties in order to encode our understanding of what better predictions means for a particular problem.Property 1: A Scoring Rule (SR) is proper if its value is minimal when the probabilistic prediction coincides with the ground-truth in expectation.Example: The Brier Score [1] is defined as the sum of the squared differences between probabilities and labels:Since its value is always non-negative, and it decreases to 0 when p = y, we conclude that the Brier Score is indeed proper.Property 2: A Proper Scoring Rule (PSR) is local if its value only depends on the probability assigned to the correct category.Example: The Brier Score is non-local, as its value depends on the probability placed by the model on all classes. The Logarithmic Score [11], given by:where c is the correct category of x, rewards the model by placing as much probability mass as possible in c, regardless of how the remaining probability is distributed. It is, therefore, a local PSR. The Logarithmic Score is also known, when taken on average over a dataset, as the Negative Log-Likelihood.Property 3: A PSR is sensitive to distance if its value takes into account the order of the categories, in such a way that probability placed in categories further away from the correct class is more heavily penalized. Example: Both the Brier and the Logarithmic scores are insensitive to distance (shuffling p and y won't affect the score). Sensitivity to distance is essential for assessing ordinal classifiers. Below we define the Ranked Probability Score (RPS) [4,18], which has this property, and is therefore more suitable for our purposes."
Performance Metrics for Probabilistic Ordinal Classifiers,2.2,The Ranked Probability Score for Ordinal Classification,"Consider a test sample (x, y) in a 3-class classification problem, with label y and two probabilistic predictions p 1 , p 2 :In this scenario, both the Brier and the Logarithmic scores produce the same penalty for each prediction, whereas a user might prefer p 1 over p 2 due to the latter assigning more probability to the second category. Indeed, if we use the arg-max operator to generate a hard-decision for this sample, we will obtain a prediction of class 2 and class 3 respectively, which could result in the second model declaring a patient as severely unhealthy with serious consequences. In this context, we would like to have a PSR that takes into account distance to the true category, such as the Ranked Probability Score (RPS, [4]), given by:The RPS is the squared 2 distance between the cumulative distributions Y of the target label y and P of the probabilistic prediction p, discounting their last component (as they are both always one) and normalizing so that it varies in the unit interval. In the above example, the RPS would give for each prediction a penalty of RPS(p 1 , y) = 1 /8, RPS(p 2 , y) = 1 /4, as shown in Fig. 1. Among many interesting properties, one can show that the RPS is proper [17], and reduces to the Brier score for K = 2. Despite the RPS dating back more than 50 years [4], and enjoying great popularity in the weather forecasting community, it appears to be much less known in the image analysis and computer vision areas, where we could not find any trace of it. The first goal of this paper is to bring to the attention of computer vision researchers this tool for measuring the performance of probabilistic predictions in ordinal classification."
Performance Metrics for Probabilistic Ordinal Classifiers,2.3,The Squared Absolute RPS,"Our second goal in this paper is to identify and then fix certain failure modes of the RPS that might lead to counter-intuitive behaviors. First, in disease grading and other ordinal classification problems it is customary to assign penalties to mistakes that grow quadratically with the distance to the correct category. This is the reason why most works utilize the Quadratic-Weighted Kappa Score (QWK) instead of the linearly weighted version of this metric. However, the RPS increases the penalty linearly, as can be quickly seen with a simple 3-class problem and an example (x 1 , y 1 ) of class 1 (y 1 = [ 1, 0, 0 ]):Also, the RPS has a hidden preference for symmetric predictions. To see this, consider a second example (x 2 , y 2 ) in which the correct category is now the middle one (y 2 = [ 0, 1, 0 ]), and two probabilistic predictions: p sym = [ 3/10, 4/10, 3/10 ], p asym = [ 1/10, 5/10, 9/10 ]. In principle, there is no reason to prefer p sym over p asym , unless certain prior/domain knowledge tells us that symmetry is a desirable property. In this particular case, p asym is actually more confident on the correct class than p sym , which is however the preferred prediction for the RPS: RPS([ 0.30, 0.40, 0.30 ], y 2 ) = 0.09 < 0.1025 = RPS([ 0.45, 0.50, 0.05 ], y 2 ). (6) Fig. 2. The Ranked Probability Score displays some counter-intuitive behavior that the proposed sa-RPS can fix. Here, p2 places more probability on the correct class but p1 is preferred due to its symmetry.In order to address these aspects of the conventional RPS, we propose to implement instead the Squared Absolute RPS (sa-RPS), given by:Replacing the inner square in Eq. ( 4) by an absolute value, we manage to break the preference for symmetry of the RPS, and squaring the overall result we build a metric that still varies in [0,1] but gives a quadratic penalty to further away predictions. This is illustrated in Fig. 2 above."
Performance Metrics for Probabilistic Ordinal Classifiers,2.4,Evaluating Evaluation Metrics,"Our third goal is to demonstrate how the (sa-)RPS is useful for evaluating probabilistic ordinal predictions. In the next section we will show some illustrative examples that qualitatively demonstrate its superiority over the Brier and logarithmic score. However, it is hard to quantitatively make the case for one performance metric over another, since metrics themselves are what quantify modeling success. We proceed as follows: we first train a neural network to solve a biomedical image grading problem. We generate probabilistic predictions on the test set and apply distance sensitive metrics to (arg-maxed) hard predictions (QWK and EC, as recommended in [16]), verifying model convergence.Here it is important to stress that, contrary to conventional metrics (like accuracy, QWK, or ECE) PSRs can act on an individual datum, without averaging over sets of samples. We exploit this property to design the following experiment: we sort the probabilistic predictions of the test set according to a score S, and then progressively remove samples that are of worst quality according to S. We take the arg-max on the remaining probabilistic predictions and compute QWK and EC. If S prefers better ordinal predictions, we must see a performance increase on that subset. We repeat this process, each time removing more of the worse samples, and graph the evolution of QWK and EC for different scores S: a better score should result in a faster QWK/EC-improving trend.Lastly, in order to derive a single number to measure performance, we compute the area under the remaining samples vs QWK/EC curve, which we call Area under the Retained Samples Curve (AURSC). In summary:What we expect to see: As we remove test set samples considered as worse classified by RPS, we expect to more quickly improve QWK/EC on the resulting subsets. We measure this with the Area under the Retained Samples Curve (AURSC)"
Performance Metrics for Probabilistic Ordinal Classifiers,3,Experimental Results,"We now give a description of the data we used for experimentation, analyze performance for each considered problem, and close with a discussion of results."
Performance Metrics for Probabilistic Ordinal Classifiers,3.1,Datasets and Architecture,"Our experiments are on two different medical image grading tasks: 1) the TMED-v2 dataset ( [13], link) contains 17,270 images from 577 patients, with an aortic stenosis (AS) diagnostic label from three categories (none, early AS, or significant AS). The authors provide an official train/test distribution of the data that we use here. 2) Eyepacs (link) contains retinal images and labels for grading Diabetic Retinopathy (DR) stage into five categories, ranging from healthy to proliferative DR. Ithas 35,126 images for training and 53,576 in the test set.We train a ConvNeXt [15], minimizing the CE loss with the adam algorithm for 10 epochs starting with a learning rate of l = 1e-4, decaying to zero over the training. We report average Area under the Retained Samples Curve (AURSC) for 50 bootstrap iterations in each dataset below, and also plot the evolution of performance as we remove more samples considered to be worse by four PSRs: the Brier score, the Logarithmic score (Neg-Log), RPS and sa-RPS."
Performance Metrics for Probabilistic Ordinal Classifiers,3.2,How is RPS Useful? Qualitative Error Analysis,"The obvious application of RPS would be to train better ordinal classification models. But beyond this, RPS also enables improved, fine-grained error analysis. Let us see this through a simple experiment. Since PSRs assess samples individually, we can sort our test set using RPS, NLL, and Brier score. The worst-scored items are what the model considers the wrongest probabilistic predictions. The result of sorting predictions on the Eyepacs test set with the Brier, Neg-Log and RPS rules is show on Fig. 3. We can see that the prediction identified as worst by  For a visual analysis, Fig. 4 shows the full Sample Retention Curves from which AURSC-QWK values in Table 1 were computed. These curves show how PSRs can indeed take a single probabilistic prediction and return a score that is correlated to QWK, which is computed over sets of samples. This is because as we remove samples according to any PSR, performance in the remaining test set improves in all cases. The curves in Fig. 4 also tell a more complete story of how the two distance-sensitive scores outperform the Brier and Neg-Log scores, particularly for TMED and Eyepacs. Just by removing a 5%-6% of samples with worse (higher) RPS, we manage to improve QWL and EC to a greater extent. Fig. 4. We sort probabilistic predictions in each test set using several PSRs: Brier, Neg-Log, RPS, sa-RPS. We progressively discard worse-scored samples, improving the metric of interest (only QWK shown). Removing worse samples according to RPS and sa-RPS leads to better QWK, implying that they both capture better ordinal classification performance at the probabilistic level."
Performance Metrics for Probabilistic Ordinal Classifiers,4,Conclusion and Future Work,"We have shown that Proper Scoring Rules are useful tools for diagnosing probabilistic predictions, but the standard Brier and Logarithmic scores should not be preferred in ordinal classification problems like medical image grading. Instead, the Ranked Probability Score, popular in the forecasting community, should be favoured. We have also proposed sa-RPS, an extension of the RPS that can better handle some pathological cases. Future work will involve using the RPS to learn ordinal classifiers, and investigating its impact in calibration problems."
Performance Metrics for Probabilistic Ordinal Classifiers,,Fig. 1 .,
Performance Metrics for Probabilistic Ordinal Classifiers,,Fig. 3 .,
Performance Metrics for Probabilistic Ordinal Classifiers,,Table 1 .,RPS 14.76 ± 0.28 2.68 ± 0.14 17.81 ± 0.03 1.99 ± 0.04 sa-RPS 14.95 ± 0.25 2.53 ± 0.12 17.86 ± 0.03 1.88± 0.04 the
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,1,Introduction,"Recent years have witnessed the remarkable success of deep learning in medical image segmentation. However, although the performance of deep learning models even surpasses the accuracy of human exports on some segmentation tasks, two challenges still persist. (1) Different segmentation tasks are usually tackled separately by specialized networks (see Fig. 1(a)), leading to distributed research efforts. (2) Most segmentation tasks face the limitation of a small labeled dataset, especially for 3D segmentation tasks, since pixel-wise 3D image annotation is labor-intensive, time-consuming, and susceptible to operator bias. Train one model on n datasets using task-specific prompts. We use purple to highlight where to add the task-related information.Several strategies have been attempted to address both challenges. First, multi-head networks (see Fig. 1(b)) were designed for multiple segmentation tasks [4,7,21]. A typical example is Med3D [4], which contains a shared encoder and multiple task-specific decoders. Although they benefit from the encoder parameter-sharing scheme and the rich information provided by multiple training datasets, multi-head networks are less-suitable for multi-task co-training, due to the structural redundancy caused by the requirement of preparing a separate decoder for each task. The second strategy is the multi-class model, which formulates multiple segmentation tasks into a multi-class problem and performs it simultaneously. To achieve this, the CLIP-driven universal model [16] (see Fig. 1(c)) introduces the text embedding of all labels as external knowledge, obtained by feeding medical prompts to CLIP [5]. However, CLIP has limited ability to generalize in medical scenarios due to the differences between natural and medical texts. It is concluded that the discriminative ability of text prompts is weak in different tasks, and it is difficult to help learn task-specific semantic information. The third strategy is dynamic convolution. DoDNet [29] and its variants [6,17,25] present universal models, which can perform different segmentation tasks based on using task encoding and a controller to generate dynamic convolutions (see Fig. 1(d)). The limitations of these models are two-fold. (1) Different tasks are encoded as one-hot vectors, which are mutually orthogonal, ignoring the correlations among tasks. (2) The task-related information (i.e., dynamic convolution parameters) is introduced at the end of the decoder. It may be too late for the model to be 'aware' of the ongoing task, making it difficult to decode complex targets.In this paper, we propose a prompt-driven Universal Segmentation model (UniSeg) to segment multiple organs, tumors, and vertebrae on 3D medical images with diverse modalities and domains. UniSeg contains a vision encoder, a fusion and selection (FUSE) module, and a prompt-driven decoder. The FUSE module is devised to generate the task-specific prompt, which enables the model to be 'aware' of the ongoing task (see Fig. 1(e)). Specifically, since prompt learning has a proven ability to represent both task-specific and task-invariant knowledge [24], a learnable universal prompt is designed to describe the correlations among tasks. Then, the universal prompt and the features extracted by the vision encoder are fed to the FUSE module to generate task prompts for all tasks. The task-specific prompt is selected according to the ongoing task. Moreover, to introduce the prompt information to the model early, we move the task-specific prompt from the end of the decoder to the start of the decoder (see Fig. 2). Thanks to both designs, we can use a single decoder and a segmentation head to predict various targets under the supervision of the corresponding ground truths. We collected 3237 volumetric data with three modalities (CT, MR, and PET) and various targets (eight organs, vertebrae, and tumors) from 11 datasets as the upstream dataset. On this dataset, we evaluated our UniSeg model against other universal models, such as DoDNet and the CLIP-driven universal model. We also compared UniSeg to seven advanced single-task models, such as CoTr [26], nnFormer [30], and nnUNet [12], which are trained independently on each dataset. Furthermore, to verify its generalization ability on downstream tasks, we applied the trained UniSeg to two downstream datasets and compared it to other pre-trained models, such as MG [31], DeSD [28], and UniMiSS [27]. Our results indicate that UniSeg outperforms all competing methods on 11 upstream tasks and two downstream tasks.Our contributions are three-fold: (1) We design a universal prompt to describe the correlations among different tasks and use it to generate task prompts for all tasks. (2) We utilize the task-related prompt information as the input of the decoder, facilitating the training of the whole decoder, instead of just the last few layers. (3) The proposed UniSeg can be trained on and applied to various 3D medical image tasks with diverse modalities and domains, providing a highquality pre-trained 3D medical image segmentation model for the community."
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2,Method,
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.1,Problem Definition,"Let {D 1 , D 2 , ..., D N } be N datasets. Here,j=1 represents that the i-th dataset has a total of n i image-label pairs, and X ij and Y ij are the image and the corresponding ground truth, respectively. Straightforwardly, N tasks can be completed by training N models on N datasets, respectively. This solution faces the issues of ( 1) designing an architecture for each task, (2) distributing research effort, and (3) dropping the benefit of rich information from other tasks. Therefore, we propose a universal framework called UniSeg to solve multiple tasks with a single model, whose architecture was shown in Fig. 2."
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.2,Encoder-Decoder Backbone,"The main architecture of UniSeg is based on nnUNet [12], which consists of an encoder and a decoder shared by different tasks. The encoder has six stages, each containing two convolutional blocks, to extract features and gradually reduce the feature resolution. The convolutional block includes a convolutional layer followed by instance normalization and a ReakyReLU activation, and the first convolution layer of each stage is usually set to reduce the resolution with a stride of 2, except for the first stage. To accept the multi-modality inputs, we reform the first convolution layer and set up three different convolution layers to handle the input with one, two, or four channels, respectively. After the encoder process, we obtain the sample-specific features32 , where C is the number of channels and D, H, and W are the depth, height, and width of the input, respectively. Symmetrically, in each stage of the decoder, the upsampling operation implemented by a transposed convolution layer is applied to the input feature map to improve its resolution and reduce its channel number. The upsampled feature map is concatenated with the output of the corresponding encoder stage and then fed to a convolutional block. After the decoder process, the output of each decoder stage is passed through a segmentation head to predict segmentation maps for deep supervision, which is governed by the sum of the Dice loss and cross-entropy loss. Note that the channel number of multi-scale segmentation maps is set to the maximum number of classes among all tasks."
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.3,Universal Prompt,"Following the simple idea that everything is correlated, we believe that the correlations among different segmentation tasks must exist undoubtedly, though they are ignored by DoDNet which uses a set of orthogonal and one-hot task codes. Considering the correlations among tasks are extremely hard to handcraft, we propose a learnable prompt called universal prompt to describe them and use that prompt to generate task prompts for all tasks, aiming to encourage interaction and fusion among different task prompts. We define the shape of the universal prompt as32 , where N is the number of tasks."
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.4,Dynamic Task Prompt,"Before building a universal network, figuring out a way to make the model 'aware' of the ongoing task is a must. DoDNet adopts a one-hot vector to encode each task, and the CLIP-driven universal model [16] uses masked back-propagation to optionally optimize the task-related segmentation maps. By contrast, we first obtain N features by passing the concatenation of F uni and F through three convolutional blocks, shown as followswhere F taski denotes the prompt features belonging to the i-th task, cat(, ) is a concatenation operation, f (•) denotes the feed forward process, and Split(•) N means splitting features along the channel to obtain N features with the same shape. Then, we select the target features, called task-specific prompt F tp , from {F task1 , F task2 , ..., F taskN } according to the ongoing task. Finally, we concatenate F and selected F tp as the decoder input. In this way, we introduce task-related prior information into the model, aiming to boost the training of the whole decoder rather than only the last few convolution layers."
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,2.5,Transfer Learning,"After training UniSeg on upstream datasets, we transfer the pre-trained encoderdecoder and randomly initialized segmentation heads to downstream tasks. The model is fine-tuned in a fully supervised manner to minimize the sum of the Dice loss and cross-entropy loss.3 Experiments and Results"
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,3.1,Datasets and Evaluation Metric,"Datasets. For this study, we collected 11 medical image segmentation datasets as the upstream dataset to train our UniSeg and single-task models. The Liver and Kidney datasets are from LiTS [3] and KiTS [11], respectively. The Hepatic Vessel (HepaV), Pancreas, Colon, Lung, and Spleen datasets are from Medical Segmentation Decathlon (MSD) [1]. VerSe20 [19], Prostate [18], BraTS21 [2], and AutoPET [8] datasets have annotations of the vertebrae, prostate, brain tumors, and whole-body tumors, respectively. We used the binary version of the VerSe20 dataset, where all foreground classes are regarded as one class. Moreover, we dropped the samples without tumors in the AutoPET dataset. Meanwhile, We use BTCV [14] and VS datasets [20] as downstream datasets to verify the ability of UniSeg to generalize to other medical image segmentation tasks. BTCV contains the annotations of 13 abdominal organs, including the spleen (Sp), right kidney (RKi), left kidney (LKi), gallbladder (Gb), esophagus (Es), liver (Li), stomach (St), aorta(Ao), inferior vena cava (IVC), portal vein and splenic vein (PSV), pancreas (Pa), right adrenal gland (RAG), and left adrenal gland (LAG). The VS dataset contains the annotations of the vestibular schwannoma. More details are shown in Table 1.Evaluation Metric. The Dice similarity coefficient (Dice) that measures the overlap region of the segmentation prediction and ground truth is employed to evaluate the segmentation performance. "
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,3.2,Implementation Details,"Both pre-training on eleven upstream datasets and fine-tuning on two downstream datasets were implemented based on the nnUNet framework [12]. During pre-training, we adopted the SGD optimizer and set the batch size to 2, the initial learning rate to 0.01, the default patch size to 64 × 192 × 192, and the maximum training epoch to 1000 with a total of 550,000 iterations. Moreover, we adopted a uniform sampling strategy to sample training data from upstream datasets. In the inference stage, we employed the sliding window strategy, in which the shape of the window is the same as the training patch size, to obtain the whole average segmentation map. During fine-tuning, We set the batch size to 2, the initial learning rate to 0.01, the default patch size to 48 × 192 × 192, and the maximum training iterations to 25,000 for all downstream datasets. The sliding window strategy was also employed when inference on downstream tasks."
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,3.3,Results,"Comparing to Single-Task and Universal Models. Our UniSeg was compared with advanced single-task models and universal models. The former includes UNETR [9], nnFormer [30], PVTv2-B1 [23], CoTr [26], UXNet [15], Swin UNETR [22], and nnUNet [12]. The latter includes DoDNet [29], CLIP  DoDNet, which replaces the one-hot vectors with CLIP embeddings obtained by following [16], and CLIP-driven universal model [16]. For a fair comparison, the maximum training iterations of single-task models on each task are 50,000, and the patch size is 64 × 192 × 192, except for Swin UNETR, whose patch size is 64 × 160 × 160 due to the limitation of GPU memory. The backbones of the competing universal models and our UniSeg are the same. As shown in Table 2, Our UniSeg achieves the highest Dice on eight datasets, beating the second-best models by 1.9%, 0.7%, 0.8%, 0.4%, 0.4%, 1.0%, 0.3%, 1.2% on the Liver, Kidney, HepaV, Pancreas, Colon, Lung, Prostate, and AutoPET datasets, respectively. Moreover, UniSeg also presents superior performance with an average margin of 1.0% and 1.6% on eleven datasets compared to the second-best universal model and single-task model, respectively, demonstrating its superior performance.Comparing to Other Pre-trained Models. We compared our UniSeg with advanced unsupervised pre-trained models, such as MG [31], SMIT [13], UniMiSS [27], DeSD [28], and GVSL [10], and supervised pre-trained models, such as AutoPET and DoDNet [29]. The former are officially released with different backbones while the latter are trained using the datasets and backbone used in our UniSeg. To verify the benefit of training on multiple datasets, we also report the performance of the models per-trained on AutoPET and BraTS21, respectively. The results in Table 3 reveal that almost all pre-trained models achieve performance gains over their baselines, which were trained from scratch. More important, thanks to the powerful baseline and small gap between the pretext and downstream tasks, UniSeg achieves the best performance and competitive performance gains on downstream datasets, demonstrating that it has learned a strong representations ability. Furthermore, another advantage of UniSeg against other unsupervised pre-trained models is that it is more resource-friendly, requiring only one GPU of 11 GB memory for implementation, while unsupervised pre-trained models usually require tremendous computational resources, such as eight and four V100 for UniMiSS and SMIT, respectively.Comparison of Different Variants. We attempted three UniSeg variants, including Fixed Prompt, Multiple Prompts, and UniSeg-T, as shown in Fig. 3. The results in Table 4 suggest that (1) learnable universal prompt is helpful for building valuable prompt features; (2) using one universal prompt instead of multiple task-independent prompts boosts the interaction and fusion among all tasks, resulting in better performance; (3) adding task-related information in advance facilitates handling complex prediction situations."
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,4,Conclusion,"This study proposes a universal model called UniSeg (a single model) to perform multiple organs, tumors, and vertebrae segmentation on images with multiple modalities and domains. To solve two limitations existing in preview universal models, we design the universal prompt to describe correlations among all tasks and make the model 'aware' of the ongoing task early, boosting the training of the whole decoder instead of just the last few layers. Thanks to both designs, our UniSeg achieves superior performance on 11 upstream datasets and two downstream datasets, setting a new record. In our future work, we plan to design a universal model that can effectively process multiple dimensional data."
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,,Fig. 1 .,
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,,Fig. 2 .,
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,,Fig. 3 .,
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,,Table 1 .,
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,,Table 2 .,of single-task models and universal models on eleven datasets. We use Dice (%) on each dataset and Mean Dice (%) on all datasets as metrics. The best results on each dataset are in bold.
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,,Table 3 .,
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,,Table 4 .,
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_49.
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,1,Introduction,"Deep learning (DL) [19] has positively contributed to the development of diagnostic algorithms for the detection and classification of lung diseases in recent years [11,20]. This progress can largely be attributed to the availability of public chest X-ray datasets [4,5,14,15,27]. However, as chest radiographs inherently contain biometric information (similar to a fingerprint), the public release of such data bears the risk of automated patient re-identification by DL-based linkage attacks [22]. This would allow patient-related information, e. g., age, gender, or disease findings to be revealed. Therefore, there is an urgent need for stronger privacy mechanisms for chest X-ray data to alleviate the risk of linkage attacks.Perturbation-based anonymization approaches [16] -such as differential privacy (DP) [7,8] -have become the gold standard to obfuscate biometric identifiers from sensitive data. Such approaches are based on the postulate that the global statistical distribution of a dataset is retained while personal information is reduced. This can be realized by applying randomized modifications, i. e. noise, to either the inputs [2,6,8], the computational results, or to algorithms [1,28]. Although originally proposed for statistical data, DP has been extended to image data with the differentially private pixelization method (DP-Pix) [9,10]. This method involves pixelizing an image by averaging the pixel values of each b × b grid cell, followed by adding Laplace noise with 0 mean andscale. Parameter is used to determine the privacy budget (smaller values indicate greater privacy), while the m-neighborhood represents a sensitivity factor. However, one major drawback of perturbation-based anonymization is the potential degradation of image quality and an associated reduction in data utility.In recent years, DL has emerged as a prominent tool for anonymizing medical images. In this context, synthetic image generation with privacy guarantees is currently actively explored, aimed at creating fully anonymous medical image datasets [21,24]. Furthermore, adversarial approaches -such as Privacy-Net [17] -have been proposed, which focus on concealing biometric information while maintaining data utility. Privacy-Net is composed of a U-Net encoder that predicts an anonymized image, an identity discriminator, and a task-specific segmentation network. Through the dual process of deceiving the discriminator and optimizing the segmentation network, the encoder acquires the ability to obfuscate patient-specific patterns, while preserving those necessary for the downstream task. However, while originally designed for utility preservation on MRI segmentation tasks, the direct applicability and transferability of Privacy-Net to other image modalities and downstream tasks (e. g. chest X-ray classification) is limited. As we will experimentally demonstrate in our study, more sophisticated constraints are required for the utility-preserving anonymization of chest radiographs in order to successfully maintain fine-grained abnormality details that are crucial for reliable classification tasks.In this work, we aim to resolve the privacy-utility trade-off by proposing theto the best of our knowledge -first adversarial image anonymization approach for chest radiography data. Our proposed model architecture (PriCheXy-Net) is a composition of three independent neural networks that collectively allow for the learning of targeted image deformations to deceive a well-trained patient verification model. We apply our method to the publicly available ChestX-ray14 dataset [27] and evaluate the impact of different deformation degrees on anonymization capability and utility preservation. To evaluate the effectiveness of image anonymization, we perform linkage attacks on anonymized data and analyze the respective success rates. Furthermore, to quantify the extent of data utility preservation despite the induced image deformations, we compare the performance of a trained thoracic abnormality classification system on anonymized images versus the performance on real data. Throughout our study, we utilize Privacy-Net [17] and DP-Pix [9,10] as baseline obfuscation methods."
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,2,Methods,
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,2.1,Data,"We use X-ray data from the ChestX-ray14 dataset [27], a large-scale collection of 112,120 frontal-view chest radiographs from 30,805 unique patients. The 8bit gray-scale images are provided with a resolution of 1024×1024 pixels. We resize the images to a size of 256×256 pixels for further processing. On average, the dataset contains ≈ 3.6 images per patient. The corresponding 14 abnormality labels include Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis, Hernia, Infiltration, Mass, Nodule, Pleural Thickening, Pneumonia, and Pneumothorax. A patient-wise splitting strategy is applied to roughly divide the data into a training, validation, and test set by ratio 70:10:20, respectively. Based on this split, we utilize available follow-up scans to randomly construct positive image pairs (two unique images from the same patient) and negative image pairs (two unique images from two different patients) -10,000 for training, 2,000 for validation, and 5,000 for testing. The resulting subsets are balanced with respect to the number of positive and negative samples."
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,2.2,PriCheXy-Net: Adversarial Image Anonymization,"The proposed adversarial image anonymization approach is depicted in Fig. 1. It is composed of three trainable components: (1) A U-Net generator G that predicts a flow field F used to deform the original image x 1 of abnormality class y c , (2) an auxiliary classifier that takes the modified image F (x 1 ) resulting in corresponding class predictions ŷc , and (3) a siamese neural network (SNN) that receives the deformed image F (x 1 ) as well as another real image x 2 of either the same or a different patient and yields the similarity score ŷv for patient verification. The U-Net serves as an anonymization tool aiming to obfuscate biometric information through targeted image deformations, while the auxiliary classifier and the patient verification model contribute as guidance to optimize the flow field generator. [26] architecture is implemented according to Buda et al. [3]. Its last sigmoid activation function is replaced by a hyperbolic tangent activation function to predict a 2-channel flow field F , bounded by [-1, 1]. During training, especially in early stages, the raw output of the U-Net may lead to random deformations that destroy the content of the original images, thus revoking the diagnostic utility. To circumvent this issue, the following constraints are imposed for F . First, to ensure that the learned flow field F does not substantially deviate from the identity F id , it is weighted with factor μ and subsequently subtracted from the identity according to"
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,U-Net. The U-Net,"(Factor μ controls the degree of deformation, with larger values allowing for more deformation. Note that the exclusive use of F id would result in the original image, i. e., x = F (x), assuming the deformation factor is being set to μ = 0. The resulting flow field F is Gaussian filtered (kernel size 9, σ = 2) to ensure smooth deformations in the final image. The corresponding parameters were selected manually in preliminary experiments.Auxiliary Classifier. To ensure the preservation of underlying abnormality patterns and image utility during deformation, PriCheXy-Net integrates an auxiliary classifier using CheXNet [25], a densely connected convolutional network (DenseNet) [13] consisting of 121 layers. It outputs a 14-dimensional probability vector ŷc indicating the presence or absence of each abnormality appearing in the ChestX-ray14 dataset. Its parameters θ aux are initialized using a pre-trained model that achieves a mean AUC of 80.5%."
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Patient Verification Network.,"The incorporated patient verification model is represented by the SNN architecture presented by Packhäuser et al. [22], consisting of two ResNet-50 [12] branches that are merged using the absolute difference of their resulting 128-dimensional feature vectors. A fully-connected layer with a sigmoid activation function produces the final verification score ŷv in the value range of [0, 1], indicating the probability of whether or not the two input images belong to the same patient. For initializing the network parameters θ ver , we employ a pre-trained network that has been created according to [22] yielding an AUC value of 99.4% for a patient verification task."
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,2.3,Objective Functions,"Similar to most adversarial models, our system undergoes training through the use of dual loss functions that guide the model towards opposing directions. To enforce the U-Net not to eliminate important class information while deforming a chest radiograph, we introduce the auxiliary classifier loss L aux (θ G , θ aux ) realized by the class-wise binary cross entropy (BCE) loss according to Eq. 2where i represents one out of 14 abnormality classes. Conversely, to guide the U-Net with deceiving the incorporated patient verification model, we utilize its output as an additional verification loss term L ver (θ G , θ ver ) (see Eq. 3):The total loss to be minimized (see Eq. 4) results from the sum of the two partial losses L aux (θ G , θ aux ) and L ver (θ G , θ ver ):Lastly, both the auxiliary classifier and the verification model are updated by minimizing the loss terms in Eq. 5 and Eq. 6, respectively. Note that the similarity labels for positive and negative pairs are encoded using y v = 1 and y v = 0. 3 Experiments and Results"
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,3.1,Experimental Setup,"For all experiments, we used PyTorch (1.10.2) [23] and Python (3.9.5). We followed a multi-part experimental setup consisting of the following steps."
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Pre-training of the Flow Field,"Generator. The incorporated U-Net architecture was pre-trained on an autoencoder-like reconstruction task for 200 epochs using the mean squared error (MSE) loss, Adam [18], a batch size of 64 and a learning rate of 10 -4 to enable faster convergence. The model that performed best on the validation set was then used for weight initialization in step 2.Training of PriCheXy-Net. After pre-training, PriCheXy-Net was trained in an end-to-end fashion for 250 epochs using the Adam optimizer [18], a batch size of 64 and a learning rate of 10 -4 using the objective functions presented above. To evaluate the effect of the deformation degree μ on anonymization capability and image utility, we performed multiple training runs with various values μ ∈ {0.001, 0.005, 0.01}. For each configuration, the U-Net that performed best on the validation set was then used for further evaluations in steps 3 and 4."
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Re-training and Evaluation of the Verification Model.,"To assess the anonymization capability of PriCheXy-Net and to determine if the anonymized images can reliably deceive the verification model, we re-trained the incorporated SNN for each model configuration by using deformed images only. We then simulated multiple linkage attacks by comparing deformed images with real ones.Training was conducted until early stopping (patience p = 5) using the BCE loss, the Adam optimizer [18], a batch size of 32 and a learning rate of 10 -4 . For each model configuration, we performed 10 independent training and testing runs. We report the means and standard deviations of the resulting AUC values.Evaluation of the Classification Model on Anonymized Data. To assess the extent to which underlying abnormalities, and thus data utility, were preserved during the anonymization process, each individually trained anonymization network was used to perturb the images of our test set. Then, the pre-trained auxiliary classifier was evaluated using the resulting images. We report the mean of the 14 class-wise AUC values. To quantify the uncertainty, the 95% confidence intervals (CIs) from 1,000 bootstrap runs were computed.Comparison with Other Obfuscation-Based Methods. To compare our proposed system with other obfuscation-based methods, we additionally analyzed the anonymization capability and utility preservation of Privacy-Net [17] and DP-Pix [9,10]. Since Privacy-Net was originally proposed for segmentation tasks, we replaced its segmentation component with the auxiliary classifier. Then, the network was trained and evaluated in the exact same setting as PriCheXy-Net. For DP-Pix, we investigated the effect of different cell sizes b ∈ {1, 2, 4, 8} at a common privacy budget = 0.1. The m-neighborhood was set to the smallest possible value (m = 1) to prevent the added Laplace noise (mean: 0; scale: 255m b 2 ) from destroying the complete content of the images.    "
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,3.2,Results,"Baseline and Comparison Methods. The results of all conducted experiments are shown in Table 1. Compared to real (non-anonymized) data, which enables a successful re-identification with an AUC of 81.8%, the patient verification performance desirably decreases after applying DP-Pix (50.0%-52.5%) and Privacy-Net (49.8%). However, while the classification performance on real data indicates a high data utility with a mean AUC of 80.5%, we observe a sharp drop for images that have been modified with DP-Pix (50.0%-52.9%) and Privacy-Net (57.5%). This suggests that relevant class information and specific abnormality patterns are destroyed during the obfuscation process. Resulting example images for both comparison methods are provided in Suppl. Fig. 1."
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,PriCheXy-Net.,"The results of our proposed PriCheXy-Net (see Table 1) show more promising behavior. As can be seen, increasing deformation degrees μ lead to a successive decline in patient verification performance. Compared to the baseline, the AUC decreases to 74.7% (μ = 0.001), to 64.3% (μ = 0.005), and to 57.7% (μ = 0.01), indicating a positive effect on patient privacy and the obfuscation of biometric information. In addition, PriCheXy-Net hardly results in a loss of data utility, as characterized by the constantly high classification performance with a mean AUC of 80.4% (μ = 0.001), 79.3% (μ = 0.005), and 76.2% (μ = 0.01). These findings are further visualized in the privacy-utility trade-off plot in Fig. 2. In contrast to the examined comparison methods, the data point corresponding to our best experiment with PriCheXy-Net (green) lies near the top right corner, highlighting the capability to closely satisfy both objectives in the privacy-utility trade-off. Examples of deformed chest radiographs resulting from a trained model of PriCheXy-Net are shown in Fig. 3. More examples are given in Suppl. Fig. 1. Difference maps are provided in Suppl. Fig. 2."
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,4,Discussion and Conclusion,"To the best of our knowledge, we presented the first adversarial approach to anonymize thoracic images while preserving data utility for diagnostic purposes. Our proposed anonymization approach -PriCheXy-Net -is a composition of three independent neural networks consisting of (1) a flow field generator, (2) an auxiliary classifier, and (3) a patient verification network. In this work, we were able to show that collective utilization of these three components enables learning of a flow field that targetedly deforms chest radiographs and thus reliably deceives a patient verification model, even after re-training was performed. For the best hyper-parameter configuration of PriCheXy-Net, the re-identification performance drops from 81.8% to 57.7% in AUC for a simulated linkage attack, whereas the abnormality classification performance only decreases from 80.5% to 76.2%, which indicates the effectiveness of the proposed approach. We strongly hypothesize that the promising performance of PriCheXy-Net can be largely attributed to the constraints imposed on the learned flow field F . The limited deviation of the flow field from the identity (cf. Eq. 1) ensures a realistic appearance of the resulting deformed image to a considerable extent, thereby avoiding its content from being completely destroyed. This idea has a positive impact on preserving relevant abnormality patterns in chest radiographs, while allowing adequate scope to obfuscate biometric information. Such domain-specific constraints are not integrated in examined comparison methods such as Privacy-Net (which directly predicts an anonymized image without ensuring realism) and DP-Pix (which does not contain any mechanism to maintain data utility). This is, as we hypothesize, the primary reason for their limited ability to preserve data utility and the overall superiority of our proposed system. Interestingly, PriCheXy-Net's deformation fields primarily focus on anatomical structures, including lungs and ribs, as demonstrated in Fig. 3, Suppl. 1, and Suppl. Fig. 2. This observation aligns with Packhäuser et al.'s previous findings [22], which revealed that these structures contain the principal biometric information in chest radiographs.In future work, we aim to further improve the performance of PriCheXy-Net by incorporating additional components into its current architecture. For instance, we plan to integrate a discriminator loss into the model, which may positively contribute to achieving perceptual realism. Furthermore, we also consider implementing a region of interest segmentation step into the pipeline to ensure not to perturb diagnostically relevant image areas. Lastly, we hypothesize that our method is robust to variations in image size or compression rate, and posit its applicability beyond chest X-rays to other imaging modalities as well. However, confirmation of these hypotheses requires further exploration to be conducted in forthcoming studies."
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Fig. 1 .,
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,,
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Fig. 2 .,
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Fig. 3 .,
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Table 1 .,
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Data Use,Declaration. This research study was conducted retrospectively using human subject data made available in open access by the National Institutes of Health (NIH) Clinical Center [27]. Ethical approval was not required as confirmed by the license attached with the open-access data.Code Availability. The source code of this study has been made available at https://github.com/kaipackhaeuser/PriCheXy-Net.
Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 26.
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,1,Introduction,"Histopathological image analysis is an important step towards cancer diagnosis. However, shortage of pathologists worldwide along with the complexity of histopathological data make this task time consuming and challenging. Therefore, developing automatic and accurate histopathological image analysis methods that leverage recent progress in deep learning has received significant attention in recent years. In this work, we investigate the problem of diagnosing colorectal cancer, which is one of the most common reason for cancer deaths around the world and particularly in Europe and America [23].Existing deep learning-based colorectal tissue classification methods [18,21,22] typically require large amounts of annotated histopathological training data for all tissue types to be categorized. However, obtaining large amount of training data is challenging, especially for rare cancer tissues. To this end, it is desirable to develop a few-shot colorectal tissue classification method, which can learn from seen tissue classes having sufficient training data, and be able to transfer this knowledge to unseen (novel) tissue classes having only a few exemplar training images.While generative adversarial networks (GANs) [6] have been utilized to synthesize images, they typically need to be trained using large amount of real images of the respective classes, which is not feasible in aforementioned few-shot setting. Therefore, we propose a few-shot (FS) image generation approach for generating high-quality and diverse colorectal tissue images of novel classes using limited exemplars. Moreover, we demonstrate the applicability of these generated images for the challenging problem of FS colorectal tissue classification."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,,Contributions:,"We propose a few-shot colorectal tissue image generation framework, named XM-GAN, which simultaneously focuses on generating highquality yet diverse images. Within our tissue image generation framework, we introduce a novel controllable fusion block (CFB) that enables a dense aggregation of local regions of the reference tissue images based on their congruence to those in the base tissue image. Our CFB employs a cross-attention based feature aggregation between the base (query) and reference (keys, values) tissue image features. Such a cross-attention mechanism enables the aggregation of reference features from a global receptive field, resulting in locally consistent features. Consequently, colorectal tissue images are generated with reduced artifacts.To further enhance the diversity and quality of the generated tissue images, we introduce a mapping network along with a controllable cross-modulated layer normalization (cLN) within our CFB. Our mapping network generates 'metaweights' that are a function of the global-level features of the reference tissue image and the control parameters. These meta-weights are then used to compute the modulation weights for feature re-weighting in our cLN. This enables the cross-attended tissue image features to be re-weighted and enriched in a controllable manner, based on the reference tissue image features and associated control parameters. Consequently, it results in improved diversity of the tissue images generated by our transformer-based framework (see Fig. 3).We validate our XM-GAN on the FS colorectral tissue image generation task by performing extensive qualitative, quantitative and subject specialist (pathologist) based evaluations. Our XM-GAN generates realistic and diverse colorectal tissue images (see Fig. 3). In our subject specialist (pathologist) based evaluation, pathologists could differentiate between our XM-GAN generated colorectral tissue images and real images only 55% time. Furthermore, we evaluate the effectiveness of our generated tissue images by using them as data augmentation during training of FS colorectal tissue image classifier, leading to an absolute gain of 4.4% in terms of mean classification accuracy over the vanilla FS classifier."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,2,Related Work,"The ability of generative models [6,15] to fit to a variety of data distributions has enabled great strides of advancement in tasks, such as image generation [3,12,13,19], and so on. Despite their success, these generative models typically require large amount of data to train and avoid overfitting. In contrast, fewshot (FS) image generation approaches [2,4,7,9,16] strive to generate natural images from disjoint novel categories from the same domain as in the training. Existing FS natural image generation approaches can be broadly divided into three categories based on transformation [1], optimization [4,16] and fusion [7,9,10]. The transformation-based approach learns to perform generalized data augmentations to generate intra-class images from a single conditional image. On the other hand, optimization-based approaches typically utilize meta-learning techniques to adapt to a different image generation task by optimizing on a few reference images from the novel domain. Different from these two paradigms that are better suited for simple image generation task, fusion-based approaches first aggregate latent features of reference images and then employ a decoder to generate same class images from these aggregated features.Our Approach: While the aforementioned works explore FS generation in natural images, to the best of our knowledge, we are the first to investigate FS generation in colorectal tissue images. In this work, we look into multi-class colorectal tissue analysis problem, with low and high-grade tumors included in the set. The corresponding dataset [14] used in this study is widely employed for multi-class texture classification in colorectal cancer histology and comprises eight types of tissue: tumor epithelium, simple stroma, complex stroma, immune cells, debris, normal mucosal glands, adipose tissue and background (no tissue). Generating colorectal tissue images of these diverse categories is a challenging task, especially in the FS setting. Generating realistic and diverse tissue images require ensuring both global and local texture consistency (patterns). Our XM-GAN densely aggregates features [5,20] from all relevant local regions of the reference tissue images at a global-receptive field along with a controllable mechanism for modulating the tissue image features by utilizing meta-weights computed from the input reference tissue image features. As a result, this leads to high-quality yet diverse colorectal tissue image generation in FS setting."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,3,Method,"Problem Formulation: In our few-shot colorectal tissue image generation framework, the goal is to generate diverse set of images from K input examples X of a unseen (novel) tissue classes. Let D s and D u be the set of seen and unseen classes, respectively, where D s ∩ D u = ∅. In the training stage, we sample images from D s and train the model to learn transferable generation ability to produce new tissue images for unseen classes. During inference, given K images from an unseen class in D u , the trained model strives to produce diverse yet plausible images for this unseen class without any further fine-tuning. Overall Architecture: Figure 1 shows the overall architecture of our proposed framework, XM-GAN. Here, we randomly assign a tissue image from X as a base image x b , and denote the remaining K-1 tissue images as reference {x ref i } K-1 i=1 . Given the input images X, we obtain feature representation of the base tissue image and each reference tissue image by passing them through the shared encoder F E . Next, the encoded feature representations h are input to a controllable fusion block (CFB), where cross-attention [20] is performed between the base and reference features, h b and h ref i , respectively. Within our CFB, we introduce a mapping network along with a controllable cross-modulated layer normalization (cLN) to compute meta-weights w i , which are then used to generate the modulation weights used for re-weighting in our cLN. The resulting fused representation f is input to a decoder F D to generate tissue image x. The whole framework is trained following the GAN [17] paradigm. In addition to L adv and L cl , we propose to use a guided perceptual loss term L p , utilizing the control parameters α i . Next, we describe our CFB in detail."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,3.1,Controllable Fusion Block,"Figure 2 shows the architecture of our proposed CFB, comprises of a shared crosstransformer followed by a feature fusion mechanism. Here, the cross-transformer is based on multi-headed cross-attention mechanism that densely aggregates relevant input image features, based on pairwise attention scores between each position in the base tissue image with every region of the reference tissue image. The query embeddings q m ∈ R n×d are computed from the base features h b ∈ R n×D , while keys k m i ∈ R n×d and values v m i ∈ R n×d are obtained from the reference features h ref i ∈ R n×D , where d = D /M with M as the number of attention heads. Next, a cross-attention function maps the queries to outputs r m i using the keyvalue pairs. Finally, the outputs r m i from all M heads are concatenated and processed by a learnable weight matrix W ∈ R D×D to generate cross-attended features c i ∈ R n×D given byFig. 2. Cross-attending the base and reference tissue image features using controllable cross-modulated layer norm (cLN) in our CFB.Here, a reference feature h ref i , noise z and control parameter αi are input to a mapping network for generating meta-weights wi. The resulting wi modulates the features via λ(wi) and β(wi) in our cLN. As a result of this controllable feature modulation, the output features fi enable the generation of tissue images that are diverse yet aligned with the semantics of the input tissue images.Next, we introduce a controllable feature modulation mechanism in our crosstransformer to further enhance the diversity and quality of generated images.Controllable Feature Modulation: The standard cross-attention mechanism, described above, computes locally consistent features that generate images with reduced artifacts. However, given the deterministic nature of the cross-attention and the limited set of reference images, simultaneously generating diverse and high-quality images in the few-shot setting is still a challenge. To this end, we introduce a controllable feature modulation mechanism within our CFB that aims at improving the diversity and quality of generated images. The proposed modulation incorporates stochasticity as well as enhanced control in the feature aggregation and refinement steps. This is achieved by utilizing the output of a mapping network for modulating the visual features in the layer normalization modules in our crosstransformer.Mapping Network: The meta-weights w i ∈ R D are obtained by the mapping network as,where ψ α (•) and ψ z (•) are linear transformations, z ∼ N (0, 1) is a Gaussian noise vector, and α i is control parameter. g ref i is global-level feature computed from the reference features h ref i through a linear transformation and a global average pooling operation. The meta-weights w i are then used for modulating the features in our cross-modulated layer normalization, as described below."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,,Controllable Cross-modulated Layer Normalization (cLN):,"Our cLN learns sample-dependent modulation weights for normalizing features since it is desired to generate images that are similar to the few-shot samples. Such a dynamic modulation of features enables our framework to generate images of high-quality and diversity. To this end, we utilize the meta-weights w i for computing the modulation parameters λ and β in our layer normalization modules. With the cross-attended feature c i as input, our cLN modulates the input to produce an output feature o i ∈ R n×D , given bywhere μ and σ 2 are the estimated mean and variance of the input c i . Here, λ(w i ) is computed as the element-wise multiplication between meta-weights w i and sample-independent learnable weights λ ∈ R D , as λ w i . A similar computation is performed for β(w i ). Consequently, our proposed normalization mechanism achieves a controllable modulation of the input features based on the reference image inputs and enables enhanced diversity and quality in the generated images. The resulting features o i are then passed through a feed-forward network (FFN) followed by another cLN for preforming point-wise feature refinement, as shown in Fig. 2. Afterwards, the cross-attended features f i are aggregated using control parameters α i to obtain the fused feature representationFinally, the decoder F D generates the final image x."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,3.2,Training and Inference,"Training: The whole framework is trained end-to-end following the hinge version GAN [17] formulation. With generator F G denoting our encoder, CFB and decoder together, and discriminator F Dis , the adversarial loss L adv is given byandAdditionally, to encourage the generated image x to be perceptually similar to the reference images based on the specified control parameters α, we use a parameterized formulation of the standard perceptual loss [11], given bywhereMoreover, a classification loss L cl enforces that the images generated by the decoder are classified into the corresponding class of the input few-shot samples.Our XM-GAN is then trained using the formulation:where η p and η cl are hyperparameters for weighting the loss terms.Inference: During inference, multiple high-quality and diverse images x are generated by varying the control parameter α i for a set of fixed K-shot samples. While a base image x b and α i can be randomly selected, our framework enables a user to have control over the generation based on the choice of α i values."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4,Experiments,"We conduct experiments on human colorectal cancer dataset [14]. The dataset consist of 8 categories of colorectal tissues, Tumor, Stroma, Lymph, Complex, Debris, Mucosa, Adipose, and Empty with 625 per categories. To enable few-shot setting, we split the 8 categories into 5 seen (for training) and 3 unseen categories (for evaluation) with 40 images per category. We evaluate our approach using two metrics: Frèchet Inception Distance (FID) [8] and Learned Perceptual Image Patch Similarity (LPIPS) [24]. Our encoder F E and decoder F D both have five convolutional blocks with batch normalization and Leaky-ReLU activation, as in [7]. The input and generated image size is 128 × 128. The linear transformation ψ(•) is implemented as a 1 × 1 convolution with input and output channels set to D. The weights η p and η cl are set to 50 and 1. We set K = 3 in all the experiments, unless specified otherwise. Our XM-GAN is trained with a batchsize of 8 using the Adam optimizer and a fixed learning rate of 10 -4 ."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4.1,State-of-the-Art Comparison,"FS Tissue Image Generation: In Tab. 1, we compare our XM-GAN approach for FS tissue image generation with state-of-the-art LoFGAN [7] on [14] dataset.Our proposed XM-GAN that utilizes dense aggregation of relevant local information at a global receptive field along with controllable feature modulation outperforms LoFGAN with a significant margin of 30.1, achieving FID score of 55.8. Furthermore, our XM-GAN achieves a better LPIPS score. In Fig. 3, we present a qualitative comparison of our XM-GAN with LoFGAN [7].    "
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4.2,Ablation Study,"Here, we present our ablation study to validate the merits of the proposed contributions. Table 3 shows the baseline comparison on the [14] dataset. Our Baseline comprises an encoder, a standard cross-transformer with standard Layer normalization (LN) layers and a decoder. This is denoted as Baseline.Baseline+PL refers to extending the Baseline by also integrating the standard perceptual loss. We conduct an additional experiment using random values of α i s.t. i α i = 1 for computing the fused feature f and parameterized perceptual loss (Eq. 5). We refer to this as Baseline+PPL. Our final XM-GAN referred here as Baseline+PPL+cLN contains the novel CFB. Within our CFB, we also validate the impact of the reference features for feature modulation by computing the meta-weights w i using only the Gaussian noise z in Eq. 2. This is denoted here as Baseline+PPL+cLN † . Our approach based on the novel CFB achieves the best performance amongst all baselines."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4.3,Human Evaluation Study,"We conducted a study with a group of ten pathologists having an average subject experience of 8.5 years. Each pathologist is shown a random set of 20 images (10 real and 10 XM-GAN generated) and asked to identify whether they are real or generated. The study shows that pathologists could differentiate between the AI-generated and real images only 55% time, which is comparable with a random prediction in a binary classification problem, indicating the ability of our proposed generative framework to generate realistic colorectal images."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,5,Conclusions,"We proposed a few-shot colorectal tissue image generation approach that comprises a controllable fusion block (CFB) which generates locally consistent features by performing a dense aggregation of local regions from reference tissue images based on their similarity to those in the base tissue image. We introduced a mapping network together with a cross-modulated layer normalization, within our CFB, to enhance the quality and diversity of generated images. We extensively validated our XM-GAN by performing quantitative, qualitative and human-based evaluations, achieving state-of-the-art results."
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,,Fig. 1 .,
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,,Fig. 3 .,
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,,Table 1 .,
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,,Table 2 .,
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,,Table 3 .,
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,1,Introduction,"The COVID-19 pneumonia pandemic has posed an unprecedented global health crisis, with lung imaging as a crucial tool for identifying and managing affected individuals [16]. The commonly used imaging modalities for COVID-19 diagnosis are chest X-rays and chest computerized tomography (CT). The latter has been the preferred method for detecting acute lung manifestations of the virus due to its exceptional imaging quality and ability to produce a 3D view of the lungs. Effective segmentation of COVID-19 infections using CT can provide valuable insights into the disease's development, prediction of the pathological stage, and treatment response beyond just screening for COVID-19 cases. However, the current method of visual inspection by radiologists for segmentation is time-consuming, requires specialized skills, and is unsuitable for large-scale screening. Automated segmentation is crucial, but it is also challenging due to three factors: the infected regions often vary in shape, size, and location, appear similar to surrounding tissues, and can disperse within the lung cavity. The success of deep convolutional neural networks (DCNNs) in image segmentation has led researchers to apply this approach to COVID-19 segmentation using CT scans [7,14,17]. However, DCNNs require large-scale annotated data to explore feature representations effectively. Unfortunately, publicly available CT scans with pixel-wise annotations are relatively limited due to high imaging and annotation costs and data privacy concerns. This limited data scale currently constrains the potential of DCNNs for COVID-19 segmentation using CT scans.In comparison to CT scans, 2D chest X-rays are a more accessible and costeffective option due to their fast imaging speed, low radiation, and low cost, especially during the early stages of the pandemic [21]. For example, the ChestXray dataset [18] contains about 112,120 chest X-rays used to classify common thoracic diseases. ChestXR dataset [1] contains 17,955 chest X-rays used for COVID-19 recognition. We advocate using chest X-ray datasets such as ChestXray and ChestXR may benefit COVID-19 segmentation using CT scans because of three reasons: (1) supplement limited CT data and contribute to training a more accurate segmentation model; (2) provide large-scale chest X-rays with labeled features, including pneumonia, thus can help the segmentation model to recognize patterns and features specific to COVID-19 infections; and (3) help improve the generalization of the segmentation model by enabling it to learn from different populations and imaging facilities. Inspired by this, in this study, we propose a new learning paradigm for COVID-19 segmentation using CT scans, involving training the segmentation model using limited CT scans with pixelwise annotations and unpaired chest X-ray images with image-level labels.To achieve this, an intuitive solution is building independent networks to learn features from each modality initially. Afterward, late feature fusion, coattention or cross-attention modules are incorporated to transfer knowledge between CT and X-ray [12,13,22,23]. However, this solution faces two limitations. First, building modality-specific networks may cause insufficient interaction between CT and X-ray, limiting the model's ability to integrate information effectively. Although ""Chilopod""-shaped multi-modal learning [6] has been proposed to share all CNN kernels across modalities, it is still limited when the different modalities have a significant dimension gap. Second, the presence of unpaired data, specifically CT and X-ray data, in the feature fusion/crossattention interaction can potentially cause the model to learn incorrect or irrelevant information due to the possible differences in their image distributions and objectives, leading to reduced COVID-19 segmentation accuracy. It's worth noting that the method using paired multimodal data [2] is not suitable for our application scenario, and the latest unpaired cross-modal [3] requires pixel-level annotations for both modalities, while our method can use X-ray images with image-level labels for training.This paper proposes a novel Unpaired Cross-modal Interaction (UCI) learning framework for COVID-19 segmentation, which aims to learn strong representations from limited dense annotated CT scans and abundant image-level annotated X-ray images. The UCI framework learns representations from both segmentation and classification tasks. It includes three main components: a multimodal encoder for image representations, a knowledge condensation and interaction module for unpaired cross-modal data, and task-specific networks. The encoder contains modality-specific patch embeddings and shared Transformer layers. This design enables the network to capture optimal feature representations for both CT and X-ray images while maintaining the ability to learn shared representations between the two modalities despite dimensional differences. To address the challenge of information interaction between unpaired cross-modal data, we introduce a momentum-updated prototype learning strategy to condense modality-specific knowledge. This strategy groups similar representations into the same prototype and iteratively updates the prototypes with a momentum term to capture essential information in each modality. Therewith, a knowledge-guided interaction module is developed that accepts the learned prototypes, enabling the UCI to better capture critical features and relationships between the two modalities. Finally, the task-specific networks, including the segmentation decoder and classification head, are presented to learn from all available labels. The proposed UCI framework has significantly improved performance on the public COVID-19 segmentation benchmark [15], thanks to the inclusion of chest X-rays.The main contributions of this paper are three-fold: (1) we are the first to employ abundant X-ray images with image-level annotations to improve COVID-19 segmentation on limited CT scans, where the CT and X-ray data are unpaired and have potential distributional differences; (2) we introduce the knowledge condensation and interaction module, in which the momentum-updated prototype learning is offered to concentrate modality-specific knowledge, and a knowledgeguided interaction module is proposed to harness the learned knowledge for boosting the representations of each modality; and (3) our experimental results demonstrate our UCI learning method's effectiveness and strong generalizability in COVID-19 segmentation and the potential for related disease screening. This suggests that the proposed framework can be a valuable tool for medical practitioners in detecting and identifying COVID-19 and other associated diseases. "
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2,Approach,"The proposed UCI aims to explore effective representations for COVID-19 segmentation by leveraging both limited dense annotated CT scans and abundant image-level annotated X-rays. Figure 1 illustrates the three primary components of the UCI framework: a multi-modal encoder used to extract features from each modality, the knowledge condensation and interaction module used to model unpaired cross-modal dependencies, and task-specific heads designed for segmentation and classification purposes."
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.1,Multi-modal Encoder,"The multi-modal encoder F(•) consists of three stages of blocks, with modalityspecific patch embedding layers and shared Transformer layers in each block, capturing modality-specific and shared patterns, which can be more robust and discriminative across modalities. Notice that due to the dimensional gap between CT and X-ray, we use the 2D convolution block as patch embedding for X-rays and the 3D convolution block as patch embedding for CTs. In each stage, the patch embedding layers down-sample the inputs and generate the sequence of modality-specific embedded tokens. The resultant tokens, combined with the learnable positional embedding, are fed into the shared Transformer layers for long-term dependency modeling and learning the common patterns. More details about architecture can be found in the Appendix.Given a CT volume x ct , and a chest X-ray image x cxr , we denote the output feature sequence of the multi-modal encoder aswhere C ct and C cxr represent the channels of CT and X-ray feature sequence.N ct and N cxr means the length of CT and X-ray feature sequence."
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.2,Knowledge Condensation and Interaction,"Knowledge Condensation. It is difficult to directly learn cross-modal dependencies using the features obtained by the encoder because CT and X-ray data were collected from different patients. This means that the data may not have a direct correspondence between two modalities, making it challenging to capture their relationship. As shown in Fig. 1(a), we design a knowledge condensation (KC) module by introducing a momentum-updated prototype learning strategy to condensate valuable knowledge in each modality from the learned features. For the X-ray modality, given its prototypes P cxr = {p cxr 1 , p cxr 2 , ..., p cxr k } initialized randomly and the feature sequence f cxr , KC module first reduces the spatial resolution of f cxr and groups the reduced f cxr into k prototypes by calculating the distance between each feature point and prototypes, shown as followswhere C cxr i suggests the feature points closing to the i-th prototype. σ(•) represents a linear projection to reduce the feature sequence length to relieve the computational burden. Then we introduce a momentum learning function to update the prototypes with C cxr i , which means that the updates at each iteration not only depend on the current C cxr i but also consider the direction and magnitude of the previous updates, defined aswhere λ is the momentum factor, which controls the influence of the previous update on the current update. Similarly, the prototypes P ct for CT modality can be calculated and updated with the feature set f ct . The prototypes effectively integrate the informative features of each modality and can be considered modality-specific knowledge to improve the subsequent cross-modal interaction learning. The momentum term allows prototypes to move more smoothly and consistently towards the optimal position, even in the presence of noise or other factors that might cause the prototypes to fluctuate. This can result in a more stable learning process and more accurate prototypes, thus contributing to condensate the knowledge of each modality better.Knowledge-Guided Interaction. The knowledge-guided interaction (KI) module is proposed for unpaired cross-modality learning, which accepts the learned prototypes from one modality and features from another modality as inputs. As shown in Fig. 1(b), the KI module contains two multi-head attention (MHA) blocks. Take CT features f ct and X-ray prototypes P cxr as input example, the first block considers P cxr as the query and reduced f ct as the key and value of the attention. It embeds the X-ray prototypes through the calculated affinity map between f ct and P cxr , resulting in the adapted prototype P cxr . The first block can be seen as a warm-up to make the prototype adapt better to the features from another modality. The second block treats f ct as the query and the concatenation of reduced f ct and P cxr as the key and value, improving the f ct through the adapted prototypes. Similarly, for the f cxr and P ct as inputs, the KI module is also used to boost the X-ray representations. Inspired by the knowledge prototypes, KI modules boost the interaction between the two modalities and allow for the learning of strong representations for COVID-19 segmentation and X-ray classification tasks."
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.3,Task-Specific Networks,"The outputs of the KI module are fed into two multi-task heads -one decoder for segmentation and one prediction head for classification respectively. The segmentation decoder has a symmetric structure with the encoder, consisting of three stages. In each stage, the input feature map is first up-sampled by the 3D patch embedding layer, and then refined by the stacked Transformer layers. Besides, we also add skip connections between the encoder and decoder to keep more low-level but high-resolution information. The decoder includes a segmentation head for final prediction. This head includes a transposed convolutional layer, a Conv-IN-LeakyReLU, and a convolutional layer with a kernel size of 1 and the output channel as the number of classes. The classification head contains a linear layer with the output channel as the number of classes for prediction. We use the deep supervision strategy by adding auxiliary segmentation losses (i.e., the sum of the Dice loss and cross-entropy loss) to the decoder at different scales. The cross-entropy loss is used to optimize the classification task."
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3,Experiment,
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.1,Materials,"We used the public COVID-19 segmentation benchmark [15] to verify the proposed UCI. It is collected from two public resources [5,8] on chest CT images available on The Cancer Imaging Archive (TCIA) [4]. All CT images were acquired without intravenous contrast enhancement from patients with positive Reverse Transcription Polymerase Chain Reaction (RT-PCR) for SARS-CoV-2. In total, we used 199 CT images including 149 training images and 50 test images. We also used two chest x-ray-based classification datasets including ChestX-ray14 [18] and ChestXR [1] to assist the UCI training. The ChestX-ray14 dataset comprises 112,120 X-ray images showing positive cases from 30,805 patients, encompassing 14 disease image labels pertaining to thoracic and lung ailments. An image may contain multiple or no labels. The ChestXR dataset consists of 21,390 samples, with each sample classified as healthy, pneumonia, or COVID-19."
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.2,Implementation Details,"For CT data, we first truncated the HU values of each scan using the range of [-958, 327] to filter irrelevant regions, and then normalized truncated voxel values by subtracting 82.92 and dividing by 136.97. We randomly cropped subvolumes of size 32 × 256 × 256 as the input and employed the online data augmentation like [10] to diversify the CT training set. For chest X-ray data, we set the size of input patches to 224 × 224. We employ the online data argumentation, including random cropping and zooming, random rotation, and horizontal/vertical flip, to enlarge the X-ray training dataset. We follow the extension of [20] for weight initialization and use the AdamW optimizer [11] and empirically set the initial learning rate to 0.0001, batch size to 2 and 32 for segmentation and classification, maximum iterations to 25w, momentum factor λ to 0.99, and the number of prototypes k to 256.To evaluate the COVID-19 segmentation performance, we utilized six metrics, including the Dice similarity coefficient (DSC), intersection over union (IoU), sensitivity (SEN), specificity (SPE), Hausdorff distance (HD), and average surface distance (ASD). These metrics provide a comprehensive assessment of the segmentation quality. The overlap-based metrics, namely DSC, IoU, SEN, and SPE, range from 0 to 1, with a higher score indicating better performance. On the other hand, HD and ASD are shape distance-based metrics that measure the dissimilarity between the surfaces or boundaries of the segmentation output and the ground truth. For HD and ASD, a lower value indicates better segmentation results."
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.3,Compared with Advanced Segmentation Approaches,"Table 1 gives the performance of our models and four advanced competing ones, including nnUNet [10], CoTr [19], nnformer [24], and Swin UNETR [9] in COVID-19 lesion segmentation. The results demonstrate that our UCI, which utilizes inexpensive chest X-rays, outperforms all other methods consistently and significantly, as evidenced by higher Dice and IoU scores. This suggests that the segmentation outcomes generated by our models are in good agreement with the ground truth. Notably, despite ChestXR being more focused on COVID-19 recognition, the UCI model aided by the ChestX-ray14 dataset containing 80k images performs better than the UCI model using the ChestXR dataset with only 16k images. This suggests that having a larger auxiliary dataset can improve the segmentation performance even if it is not directly related to the target task. The results also further prove the effectiveness of using a wealth of chest X-rays to assist the COVID-19 segmentation under limited CTs. Finally, our UCI significantly reduces HD and ASD values compared to competing approaches. This reduction demonstrates that our segmentation results provide highly accurate boundaries that closely match the ground-truth boundaries. "
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.4,Discussions,"Ablations. We perform ablation studies over each component of UCI, including the multi-modal encoder, Knowledge Condensation (KC) and Knowledge Interaction (KI) models, as listed in Fig. 2. We set the maximum iterations to 8w and use ChestX-ray14 as auxiliary data for all ablation experiments. We compare five variants of our UCI: (1) baseline: trained solely on densely annotated CT images;(2) w/o shared encoder: replacing the multi-modal encoder with two independent encoders, each designed to learn features from a separate modality; (3) w/o KC: removing the prototype and using the features before KC for interaction; (4) w/o KC & KI: only with encoder to share multi-modal information; and (5) w/o warm-up: removing the prototype warm-up in KI. Figure 2 reveals several noteworthy conclusions. Firstly, our UCI model, which jointly uses Chest X-rays, outperforms the baseline segmentation results by up to 1.69%, highlighting the effectiveness of using cheap large-scale auxiliary images. Secondly, using only a shared encoder for multi-modal learning (UCI w/o KC & KI) can still bring a segmentation gain of 0.96%, and the multi-modal encoder outperforms building independent modality-specific networks (UCI w/o shared encoder), underscoring the importance of shared networks. Finally, our results demonstrate the effectiveness of the prototype learning and prototype warm-up steps.Hyper-Parameter Settings. To evaluate the impact of hyper-parameter settings on COVID-19 segmentation, we conducted an investigation of the number of prototypes (k) and the number of momentum factors (λ). Figure 3 illustrates the Dice scores obtained on the test set for different values of k and λ, providing insights into the optimal settings for these hyper-parameters."
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,4,Conclusion,"Our study introduces UCI, a novel method for improving COVID-19 segmentation under limited CT images by leveraging unpaired X-ray images with imagelevel annotations. Especially, UCI includes a multi-modal shared encoder to capture optimal feature representations for CT and X-ray images while also learning shared representations between the two modalities. To address the challenge of information interaction between unpaired cross-modal data, UCI further develops a KC and KI module to condense modality-specific knowledge and facilitates cross-modal interaction, thereby enhancing segmentation training. Our experiments demonstrate that the UCI method outperforms existing segmentation models for COVID-19 segmentation."
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,,Fig. 1 .,
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,,Fig. 3 .,
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,,Table 1 .,Fig. 2. Effectiveness of each module in UCI.
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 58.
The Role of Subgroup Separability in Group-Fair Medical Image Classification,1,Introduction,"Medical image computing has seen great progress with the development of deep image classifiers, which can be trained to perform diagnostic tasks to the level of skilled professionals [19]. Recently, it was shown that these models might rely on sensitive information when making their predictions [7,8] and that they exhibit performance disparities across protected population subgroups [20]. Although many methods exist for mitigating bias in image classifiers, they often fail unexpectedly and may even be harmful in some situations [26]. Today, no bias mitigation methods consistently outperform the baseline approach of empirical risk minimisation (ERM) [22,27], and none are suitable for real-world deployment. If we wish to deploy appropriate and fair automated systems, we must first understand the underlying mechanisms causing ERM models to become biased.An often overlooked aspect of this problem is subgroup separability: the ease with which individuals can be identified as subgroup members. Some medical images encode sensitive information that models may leverage to classify individuals into subgroups [7]. However, this property is unlikely to hold for all modalities and protected characteristics. A more realistic premise is that subgroup separability varies across characteristics and modalities. We may expect groups with intrinsic physiological differences to be highly separable for deep image classifiers (e.g. biological sex from chest X-ray can be predicted with > 0.98 AUC). In contrast, groups with more subtle differences (e.g. due to 'social constructs') may be harder for a model to classify. This is especially relevant in medical imaging, where attributes such as age, biological sex, self-reported race, socioeconomic status, and geographic location are often considered sensitive for various clinical, ethical, and societal reasons.We highlight how the separability of protected groups interacts in non-trivial ways with the training of deep neural networks. We show that the ability of models to detect which group an individual belongs to varies across modalities and groups in medical imaging and that this property has profound consequences for the performance and fairness of deep classifiers. To the best of our knowledge, ours is the first work which analyses group-fair image classification through the lens of subgroup separability. Our contributions are threefold:-We demonstrate empirically that subgroup separability varies across realworld modalities and protected characteristics. -We show theoretically that such differences in subgroup separability affect model bias in learned classifiers and that group fairness metrics may be inappropriate for datasets with low subgroup separability. -We corroborate our analysis with extensive testing on real-world medical datasets, finding that performance degradation and subgroup disparities are functions of subgroup separability when data is biased."
The Role of Subgroup Separability in Group-Fair Medical Image Classification,2,Related Work,"Group-fair image analysis seeks to mitigate performance disparities caused by models exploiting sensitive information. In medical imaging, Seyyed-Kalantari et al. [20] highlighted that classification models trained through ERM underdiagnose historically underserved population subgroups. Follow-up work has additionally shown that these models may use sensitive information to bias their predictions [7,8]. Unfortunately, standard bias mitigation methods from computer vision, such as adversarial training [1,14] and domain-independent training [24], are unlikely to be suitable solutions. Indeed, recent benchmarking on the MEDFAIR suite [27] found that no method consistently outperforms ERM. On natural images, Zietlow et al. [26] showed that bias mitigation methods worsen performance for all groups compared to ERM, giving a stark warning that blindly applying methods and metrics leads to a dangerous 'levelling down' effect [16].One step towards overcoming these challenges and developing fair and performant methods is understanding the circumstances under which deep classifiers learn to exploit sensitive information inappropriately. Today, our understanding of this topic is limited. Closely related to our work is Oakden-Rayner et al., who consider how 'hidden stratification' may affect learned classifiers [18]; similarly, Jabbour et al. use preprocessing filters to inject spurious correlations into chest X-ray data, finding that ERM-trained models are more biased when the correlations are easier to learn [12]. Outside of fairness, our work may have broader impact in the fields of distribution shift and shortcut learning [6,25], where many examples exist of models learning to exploit inappropriate spurious correlations [3,5,17], yet tools for detecting and mitigating the problem remain immature."
The Role of Subgroup Separability in Group-Fair Medical Image Classification,3,The Role of Subgroup Separability,"Consider a binary disease classification problem where, for each image x ∈ X, we wish to predict a class label y ∈ Y : {y + , y -}. We denote P : [Y |X] → [0, 1] the underlying mapping between images and class labels. Suppose we have access to a (biased) training dataset, where P tr is the conditional distribution between training images and training labels; we say that such a dataset is biased if P tr = P . We focus on group fairness, where each individual belongs to a subgroup a ∈ A and aim to learn a fair model that maximises performance for all groups when deployed on an unbiased test dataset drawn from P . We assume that the groups are consistent across both datasets. The bias we consider in this work is underdiagnosis, a form of label noise [4] where some truly positive individuals x + are mislabeled as negative. We are particularly concerned with cases where underdiagnosis manifests in specific subgroups due to historic disparities in healthcare provision or discriminatory diagnosis policy. Formally, group A = a * is said to be underdiagnosed if it satisfies Eq. ( 1):We may now use the law of total probability to express the overall mapping from image to label in terms of the subgroup-wise mappings in Eq. ( 2). Together with Eq. ( 1), this implies Eq. ( 3) -the probability of a truly positive individual being assigned a positive label is lower in the biased training dataset than for the unbiased test set.P tr (y|x) = a∈A P tr (y|x, a)P tr (a|x)(2)At training time, supervised learning with empirical risk minimisation aims to obtain a model p, mapping images to predicted labels ŷ = argmax y∈Y p(y|x) such that p(y|x) ≈ P tr (y|x), ∀(x, y). Since this model approximates the biased training distribution, we may expect underdiagnosis from the training data to be reflected by the learned model when evaluated on the unbiased test set. However, the distribution of errors from the learned model depends on subgroup separability. Revisiting Eq. ( 2), notice that the prediction for any individual is a linear combination of the mappings for each subgroup, weighted by the probability the individual belongs to each group. When subgroup separability is high due to the presence of sensitive information, the model will learn a different mapping for each subgroup, shown in Eq. ( 4) and Eq. ( 5). This model underdiagnoses group A = a * whilst recovering the unbiased mapping for other groups. p(y|x + , a * ) ≈ P tr (y|x + , a * ) ≤ P (y|x + , a * ) (and ∀a = a * , p(y|x + , a) ≈ P tr (y|x + , a) = P (y|x + , a) (Equation ( 4) and Eq. (5) show that, at test-time, our model will demonstrate worse performance for the underdiagnosed subgroup than the other subgroups. Indeed, consider True Positive Rate (TPR) as a performance metric. The groupwise TPR of an unbiased model, TPR (u)  a , is expressed in Eq. (6).Here, N +,a denotes the number of positive samples belonging to group a in the test set. Remember, in practice, we must train our model on the biased training distribution P tr . We thus derive test-time TPR for such a model, TPR (b)  a , from Eq. ( 4) and Eq. ( 5), giving Eq. ( 7) and Eq. (8).andIn the case of high subgroup separability, Eq. ( 7) and Eq. ( 8) demonstrate that TPR of the underdiagnosed group is directly affected by bias from the training set while other groups are mainly unaffected. Given this difference across groups, an appropriately selected group fairness metric may be able to identify the bias, in some cases even without access to an unbiased test set [23]. On the other hand, when subgroup separability is low, this property does not hold. With nonseparable groups (i.e. P (a|x) ≈ 1  |A| , ∀a ∈ A), a trained model will be unable to learn separate subgroup mappings, shown in Eq. (9). p(y|x + , a) ≈ P tr (y|x + ), ∀a ∈ A (9)Equations ( 3) and ( 9) imply that the performance of the trained model degrades for all groups. Returning to the example of TPR, Eq. ( 10) represents performance degradation for all groups when separability is poor. In such situations, we expect performance degradation to be uniform across groups and thus not be detected by group fairness metrics. The severity of the degradation depends on both the proportion of corrupted labels in the underdiagnosed subgroup and the size of the underdiagnosed subgroup in the dataset.We have derived the effect of underdiagnosis bias on classifier performance for the two extreme cases of high and low subgroup separability. In practice, subgroup separability for real-world datasets may vary continuously between these extremes. In Sect. 4, we empirically investigate (i) how subgroup separability varies in the wild, (ii) how separability impacts performance for each group when underdiagnosis bias is added to the datasets, (iii) how models encode sensitive information in their representations."
The Role of Subgroup Separability in Group-Fair Medical Image Classification,4,Experiments and Results,"We support our analysis with experiments on five datasets adapted from a subset of the MEDFAIR benchmark [27]. We treat each dataset as a binary classification task (no-disease vs disease) with a binary subgroup label. For datasets with multiple sensitive attributes available, we investigate each individually, giving eleven dataset-attribute combinations. The datasets cover the modalities of skin dermatology [9,10,21], fundus images [15], and chest X-ray [11,13]. We record summary statistics for the datasets used in the supplementary material (Table A1), where we also provide access links (Table A2). Our architecture and hyperparameters are listed in Table A3, adapted from the experiments in MEDFAIR."
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Subgroup Separability in the Real World,"We begin by testing the premise of this article: subgroup separability varies across medical imaging settings. To measure subgroup separability, we train binary subgroup classifiers for each dataset-attribute combination. We use testset area under receiver operating characteristic curve (AUC) as a proxy for separability, reporting results over ten random seeds in Table 1. Some patterns are immediately noticeable from Table 1. All attributes can be predicted from chest X-ray scans with > 0.9 AUC, implying that the modality encodes substantial information about patient identity. Age is consistently well predicted across all modalities, whereas separability of biological sex varies, with prediction of sex from fundus images being especially weak. Importantly, the wide range of AUC results [0.642 → 0.986] across the dataset-attribute combinations confirms our premise that subgroup separability varies substantially across medical imaging applications."
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Performance Degradation Under Label Bias,"We now test our theoretical finding: models are affected by underdiagnosis differently depending on subgroup separability. We inject underdiagnosis bias into each training dataset by randomly mislabelling 25% of positive individuals in Group 1 (see Table 1) as negative. For each dataset-attribute combination, we train ten disease classification models with the biased training data and ten models with the original clean labels; we test all models on clean data. We assess how the test-time performance of the models trained on biased data degrades relative to models trained on clean data. We illustrate the mean percentage point accuracy degradation for each group in Fig. 1 and use the Mann-Whitney U test (with the Holm-Bonferroni adjustment for multiple hypothesis testing) to determine if the performance degradation is statistically significant at p critical = 0.05. We include an ablation experiment over varying label noise intensity in Fig. A1. Our results in Fig. 1 are consistent with our analysis in Sect. 3. We report no statistically significant performance degradation for dataset-attribute combinations with low subgroup separability (<0.9 AUC). In these experiments, the proportion of mislabelled images is small relative to the total population; thus, the underdiagnosed subgroups mostly recover from label bias by sharing the correct mapping with the uncorrupted group. While we see surprising improvements in performance for PAPILA, note that this is the smallest dataset, and these improvements are not significant at p critical = 0.05. As subgroup separability increases, performance degrades more for the underdiagnosed group (Group 1), whilst performance for the uncorrupted group (Group 0) remains somewhat unharmed. We see a statistically significant performance drop for Group 0 in the MIMIC-Sex experiment -we believe this is because the model learns separate group-wise mappings, shrinking the effective size of the dataset for Group 0."
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Use of Sensitive Information in Biased Models,"Finally, we investigate how biased models use sensitive information. We apply the post hoc Supervised Prediction Layer Information Test (SPLIT) [7,8] to all models trained for the previous experiment, involving freezing the trained backbone and re-training the final layer to predict the sensitive attribute. We report test-set SPLIT AUC in Fig. 2, plotting it against subgroup separability AUC from Table 1 and using Kendall's τ statistic to test for a monotonic association between the results (p critical = 0.05). We find that models trained on biased data learn to encode sensitive information in their representations and see a statistically significant association between the amount of information available and the amount encoded in the representations. Models trained on unbiased data have no significant association, so do not appear to exploit sensitive information. "
The Role of Subgroup Separability in Group-Fair Medical Image Classification,5,Discussion,"We investigated how subgroup separability affects the performance of deep neural networks for disease classification. We discuss four takeaways from our study: Subgroup Separability Varies Substantially in Medical Imaging. In fairness literature, data is often assumed to contain sufficient information to identify individuals as subgroup members. But what if this information is only partially encoded in the data? By testing eleven dataset-attribute combinations across three medical modalities, we found that the ability of classifiers to predict sensitive attributes varies substantially. Our results are not exhaustive -there are many modalities and sensitive attributes we did not consider -however, by demonstrating a wide range of separability results across different attributes and modalities, we highlight a rarely considered property of medical image datasets.Performance Degradation is a Function of Subgroup Separability. We showed, theoretically and empirically, that the performance and fairness of models trained on biased data depends on subgroup separability. When separability is high, models learn to exploit the sensitive information and the bias is reflected by stark subgroup differences. When separability is low, models cannot exploit sensitive information, so they perform similarly for all groups. This indicates that group fairness metrics may be insufficient for detecting bias when separability is low. Our analysis centred on bias in classifiers trained with the standard approach of empirical risk minimisation -future work may wish to investigate whether subgroup separability is a factor in the failure of bias mitigation methods and whether it remains relevant in further image analysis tasks (e.g. segmentation)."
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Sources of Bias Matter.,"In our experiments, we injected underdiagnosis bias into the training set and treated the uncorrupted test set as an unbiased ground truth. However, this is not an endorsement of the quality of the data. At least some of the datasets may already contain an unknown amount of underdiagnosis bias (among other sources of bias) [2,20]. This pre-existing bias will likely have a smaller effect size than our artificial bias, so it should not play a significant role in our results. Still, the unmeasured bias may explain some variation in results across datasets. Future work should investigate how subgroup separability interacts with other sources of bias. We renew the call for future datasets to be released with patient metadata and multiple annotations to enable analysis of different sources and causes of bias.Reproducibility and Impact. This work tackles social and technical problems in machine learning for medical imaging and is of interest to researchers and practitioners seeking to develop and deploy medical AI. Given the sensitive nature of this topic, and its potential impact, we have made considerable efforts to ensure full reproducibility of our results. All datasets used in this study are publicly available, with access links in Table A2. We provide a complete implementation of our preprocessing, experimentation, and analysis of results at https://github. com/biomedia-mira/subgroup-separability."
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Fig. 1 .,
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Fig. 2 .,
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Table 1 .,
The Role of Subgroup Separability in Group-Fair Medical Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 18.
Self-adaptive Adversarial Training for Robust Medical Segmentation,1,Introduction,"Medical image segmentation is a fundamental task in medical image analysis [14,23], where deep neural network based model shave achieved revolutionary progress [13,26]. Although these cutting-edge models can achieve near-human level performance on medical tasks [17] and can play a crucial role in medical diagnosis, treatment planning, and monitoring of various diseases, they are vulnerable to adversarial attacks like other deep learning models [14,18,31]. The vulnerability of medical segmentation models to adversarial attacks could have severe consequences in clinical scenarios, leading to incorrect diagnoses and inappropriate or even harmful treatments that risk the patient's safety. Hence, improving the adversarial robustness of medical segmentation models is crucial.Recent studies on the natural image domains show that adversarial training is one of the most successful strategies against adversarial attacks [3,4,12,32]. The concept behind adversarial training is to utilise the adversarially perturbed examples as training data to improve the trained models' robustness [11,19,30]. Although a large amount of efforts has been made to adopt adversarial training techniques as data argumentation to mitigate the shortage of data [24,25,33,36], how to effectively deploy adversarial training to improve adversarial robustness has been discussed less by the medical image community. Along this direction, Daza et al. [6] proposed to adversarially fine-tune pre-trained models on the Medical Segmentation Decathlon (MSD) datasets [1] to improve their robustness, which empirically demonstrated the effectiveness of adversarial training. However, the theoretical foundations for developing more effective adversarial training techniques for 3D segmentation tasks are still lacking.In this paper, we consider how to effectively improve the adversarial robustness in 3D medical image segmentation tasks. Taking inspiration from the PAC-Bayes generalisation bounds on standard training [22] and adversarial training [8], we show that reducing the Lipschitz constant of the trained model (defined in Eq. ( 5)) can narrow down the generalisation gap and improve the effect of adversarial training. Nevertheless, existing approaches served for such a purpose, e.g., spectral normalisation [28] and penalising gradient norm [21], are impractical due to the complexity of model architecture and the large volume of examples in 3D segmentation tasks. To overcome these difficulties, as shown in Fig. 1, we empirically demonstrate that conducting adversarial training with an appropriate number of adversarial iterations during training can induce a regularisation effect on the trained models' gradient norm. This motivates us to design an adversarial training strategy that dynamically changes adversarial iterations during training. As shown in Fig. 2 and Tab. 2, the proposed adversarial strategy can train robust segmentation models under both adversarial training and fine-tuning scenarios. Compared with FREE adversarial training with a fixed number of adversarial iterations, our self-adaptive adversarial training strategy conducts much fewer backpropagation, leading to a considerable boost in training efficiency.In summary, our contribution comes from three parts: i) Based on the PAC-Bayes generalisation framework, we show that the adversarial training effect on 3D segmentation tasks can be improved by reducing the norm of the trained models' gradient; ii) As existing methods do not work on 3D tasks, our empirical investigation demonstrates that dynamically adjusting the adversarial iteration can achieve a better regularising effect on the gradient norm than fixing the iteration; iii) We design a SElf-adaptive Adversarial Training strategy, SEAT for short, and empirically prove its effectiveness on the MSD dataset."
Self-adaptive Adversarial Training for Robust Medical Segmentation,2,Related Works,"The goal of adversarial attacks is to add malicious perturbations to the input examples, aiming to fool or deceive target neural networks while maintaining imperceptible to human or detection mechanisms [30]. In previous studies [2,34], extensive empirical analyses have been conducted on the adversarial robustness of 2D segmentation tasks. These studies showed that segmentation models were 'inherently' more robust to adversarial examples than classification models, thanks to components such as residual connections and multiscale processing that can enhance the models' robustness. However, similar to the 'arms race' between adversarial attack and defence developed for classification tasks [3], new attack methods like [9,38] have been developed to break the natural robustness of segmentation models, revealing their vulnerability to malicious perturbations. To achieve adversarial robustness in segmentation models, Xu et al. [35] introduced adversarial training, one of the most effective defence mechanisms against strong adversarial attacks [3]. Later, Gu et al. [10] proposed SegPGD, an efficient segmentation attack method that can be used to evaluate or adversarially train 2D segmentation models.In the field of 3D medical imaging, due to the large volume of 3D examples and the shortage of training data, medical segmentation models are often prone to overfitting, resulting in poor generalisation and increased vulnerability to adversarial attacks [14]. While approaches such as preprocessing [16] and robust detection [15] have been proposed to defend against adversarial attacks, they operate as additional protection for the deployed model rather than improving its robustness. In contrast, adversarial training methods can produce models with intrinsic robustness [3,10], but research on effectively applying them to train robust medical segmentation models just commences. Daza et al. [6] proposed a lightweight segmentation model called ROG and adopted FREE adversarial training [29] to fine-tune models pretrained on MSD datasets [1]. They also extended AutoAttack [5], a combination of four attacks, to evaluate the adversarial robustness."
Self-adaptive Adversarial Training for Robust Medical Segmentation,3,Methodology,
Self-adaptive Adversarial Training for Robust Medical Segmentation,,Notations. Considering a segmentation task with input domain X,", where • is a norm constrain and C is the number of classes. We let D be a dataset containing N pairs of example and segmentation mask drawn i.i.d from the unknown distribution D. Denoted by f w : R n → R n , the segmentation results can be computed via a neural network parameterised over w = vecwhere d is the number of blocks."
Self-adaptive Adversarial Training for Robust Medical Segmentation,3.1,PAC-Bayes Generalisation Bounds,"Previous works, e.g., [8,11,22], propose to utilise the PAC-Bayes framework [20] to study the generalisation on both benign and adversarial examples of classification models. As the whole example corresponds to one label, the expected margin loss [22] for classification models is defined aswhere γ > 0 is the margin term. Similarly, we can extend the expected margin loss for segmentation models as(Based on Eq. ( 2), we can then adopt the PAC-Bayes bounds to formulate the generalisability of segmentation models. Specifically, letting L 0 be the expected risk, i.e., γ = 0, and L γ be the empirical margin loss, the following bound holds for any δ, γ > 0 with probability ≥ 1δ on benign training set [22].where h is number of hidden units in each block and Φ is the complexity score given by. Farnia et al. [8] extended the generalisation bound in Eq. ( 3) to adversarial training scenario and gave the following adversarial generalisation bound,where ε is the perturbation ratio, and Φ adv is proportion to Φ, while the exact form of it depends on the adversarial attack method. The complexity scores Φ and Φ adv are both proportional to d i=1 W i 2 , which is the product of the spectral norm of all blocks [8] that can be viewed as an estimation of the Lipschitz constant of the trained model [27]. As other factors become constants when a specific training task and the model architecture are given, the above analysis implies that narrowing down the generalisation gap can be achieved by reducing the complexity scores Φ and Φ adv through decreasing the Lipschitz constant of the model defined as follows.Definition 1 (Lipschitz constant). Let f w : R n → R n be a segmentation model, δ be a perturbation, and L be a Lipschitz continued loss function, K > 0 is said to be a Lipschitz constant of model f w if, for any x, x + δ ∈ X , we have(5)"
Self-adaptive Adversarial Training for Robust Medical Segmentation,3.2,Narrowing down the Generalisation Gap,"Decreasing the expected risk requires reducing the Lipschitz constant of the trained model while maintaining satisfactory training performance. One approach to control the Lipschitz constant is regularising the gradient during training, where Farnia et al. [8] accomplished this by applying spectral normalisation [28] to 2D convolution and other linear operations. However, 3D segmentation models cannot directly benefit from such an approach because spectral Fig. 1. A comparison was made on three tasks from MSD, namely tasks 3, 7, and 9, to investigate the regularising effect on the gradient norm induced by different approaches. These approaches we implemented are denoted as random, FREE-5/3, and SEAT, which respectively represent randomised noise, FREE adversarial training with 5 and 3 adversarial iterations, and the proposed self-adaptive adversarial training strategy.normalisation is theoretically inapplicable for high-dimensional tensors. Note that the Lipschitz constant is an upper bound on how fast the loss value changes when small perturbations are added to the network's input [30], i.e., K ≥ max x∈X ∇ x L(f w (x)). Therefore, one can utilise ∇ x L(f w (x)) as a penalty during training to reduce the generalisation gap, but directly minimising the gradient norm through gradient descent can lead to to an unacceptable computational cost [7].On a small classification dataset, Moosavi-Dezfooli et al. [21] found that regularising the gradient norm can train robustness models while conducting adversarial training could reduce the gradient norm. Therefore, taking three datasets within MSD as examples, we compared the regularisation effect on the trained models' gradient norm induced by FREE adversarial training and randomised noise. These models have been trained in 50 epochs. We record the gradient norm at the end of each epoch and report the averaged value throughout the training. It can be seen from Fig. 1 that adversarial training indeed has notably reduced the gradient norm. However, more adversarial iterations did not always result in the lowest averaged gradient norm. This observation aligns with findings from previous works [29,32], which showed that repeatedly training the model too many rounds on the same batch could lead to 'catastrophic forgetting'. Hence, conducting appropriate numbers of adversarial iterations at appropriate timing could be critical to regularising the gradient norm."
Self-adaptive Adversarial Training for Robust Medical Segmentation,3.3,Self-adaptive Adversarial Training Schedule,"Motivated by the empirical investigation in Fig. 1, we design an adversarial training schedule that can automatically adjust the number of adversarial iterations during training. As described in Algorithm 1 and Algorithm 2, we allow the algorithm to compute and monitor the accumulation of the gradient norm K throughout training. Given the update frequency q, the model is initially trained on clean examples, while adversarial training starts at the q-th epoch by only performing one adversarial iteration. After another q training epochs, a threshold is initialised based on the K at that epoch. From there, the algorithm checks whether the current K is larger or smaller than the threshold every q epochs, and if so, the number of adversarial iterations will be increased or decreased accordingly unless reaching the minimum or minimum values. As illustrated in Fig. 1, SEAT showed the best regularisation effect on the gradient norm in this preliminary investigation. We will conduct a comprehensive evaluation of its training performance in the next section. "
Self-adaptive Adversarial Training for Robust Medical Segmentation,4,Experiment,"This section evaluates the proposed adversarial training strategy under both adversarial training and adversarial fine-tuning scenarios on the MSD datasets [1].Implementation Details. Following the benchmark on the MSD dataset built by Daza et al. [6], we adopted the ROG model [6] but applied the SGD optimiser with a fixed number of epochs. In Fig. 2, we trained the model for 300 epochs using a two-step learning rate schedule. The initial learning rate was set to 0.01 and was decreased by a factor of 10 twice during the training process. The training adversarial perturbation budget is set to be 8/255, and we re-scale the perturbation according to the value range of examples when performing adversarial training and attack. We set the number of adversarial iterations in FREE to 5 and allow SEAT to perform up to 5 adversarial iterations as well. The number of iterations is updated every 3 epochs in SEAT. Besides, our implementation is built with the PyTorch framework, and experiments are carried out on a workstation with an Intel i7-10700KF processor, a GeForce RTX 3090 graphics card, and 64 GB memory. Regarding the test methods, Daza et al. [6] introduced two gradient-based white-box adversarial attack methods, i.e., APGD-CE and APGD-DLR, and two query-based black-box attack, FAB and Square [5]. However, as reported in their paper, the FAB attack can barely reduce the target segmentation models' performance, while APGD-DLR needs at least three classes (C > 2) when computing the loss, which is inapplicable for some training tasks in MSD. We adopted APGD-CE, PGD, and Square to evaluate the trained models' robustness. However, as evident from Table 2, the Square black-box attack also performed poorly in our evaluation.Robustness Performance. We first evaluate the adversarially trained models by using attacks with different ratios. The averaged dice scores over categories and attacks on each task are reported in Fig. 2, and the computational cost given by the number of backpropagations is summarised in Table 1. As all adversarial training methods are carried out with ε = 8/255, through this experiment, we can see the trained models' robustness is generalisable to adversarial perturbations with smaller or larger ratios. Although the fine-tuned models generally demonstrate better robustness than their counterparts that underwent only adversarial training across a majority of the tasks, we observe that these performance gaps appear to correlate with the number of available training examples specific to each task. The widest performance gap is revealed in Task 9, which only has 41 training examples [1]. Conversely, it's interesting to note that in Task 3, which includes 210 training examples [1], the adversarially trained models actually surpass the performance of their fine-tuned counterparts. From a methodological perspective, models trained using SEAT often show robustness performance that's on par with, and occasionally superior to, those trained using FREE. Because backpropagation is the most computationally expensive opera- Adversarial Trade-Off. In classification tasks, adversarial training suffers from the trade-off between the adversarial robustness and the accuracy of clean examples [37]. An increase in the robustness of trained models often results in a decrease in performance on clean data [29], which, however, is not the case in the 3D segmentation tasks. As shown in Table 2, increasing the training epochs generally led to enhanced robustness while maintaining an appreciable Dice score on clean examples. This is likely caused by the significantly increased difficulties of the training tasks and the lack of training data."
Self-adaptive Adversarial Training for Robust Medical Segmentation,5,Conclusion,"In this paper, we first introduce the PAC-Bayes generalisation bounds by defining the expected margin loss for the segmentation task and show that the generalisation gap can be narrowed down by reducing the Lipschitz constant of the trained model. While existing techniques like spectral normalisation and penalising the gradient norm are impractical for 3D segmentation models, we empirically show that dynamically adjusting the adversarial iterations can achieve a better regularisation of the model's Lipschitz constant. Accordingly, we developed a self-adaptive adversarial training method, namely SEAT, and evaluated its performance on the MSD dataset. Our experiments demonstrate that SEAT can train segmentation models with considerable robustness and is much more efficient than its opponents. Please note that the observation in this paper is only made on the ROG model, and we plan to extend our investigation to other state-of-the-art segmentation models in the future."
Self-adaptive Adversarial Training for Robust Medical Segmentation,,Fig. 2 .,
Self-adaptive Adversarial Training for Robust Medical Segmentation,,Table 1 .,
Self-adaptive Adversarial Training for Robust Medical Segmentation,,Table 2 .,tion
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,1,Introduction,"Medical image segmentation is ruled by large machine learning models which require substantial infrastructure to be executed. These are variations of UNetstyle [17] architectures that win numerous grand challenges [9]. This emerging trend raises concerns, as the utilization of such models is limited to scenarios with abundant resources, posing barriers to adoption in resource-limited settings. For example, conflict zones [10], low-income countries [3], and primary care facilities in rural areas [1] often lack the necessary infrastructure to support the deployment of these models, impeding access to critical medical services. Even when the infrastructure is in place, shifts in domains can cause the performance of deployed models to deteriorate, posing a risk to patient treatment decisions. To address this risk, automated quality control is essential [6], but it can be difficult and computationally expensive.Neural Cellular Automata (NCA) [5] diverges strongly from most deep learning architectures. Inspired by cell communication, NCAs are one-cell models that communicate only with their direct neighbours. By iterating over each cell of an image, these relatively simple models, with often sizes of less than 13k parameters, can reach complex global targets. By contrast, UNet-style models quickly reach 30m parameters [11], limiting their area of application. Though several minimal UNet-style architectures with backbones such as EfficientNet [22], MobileNetV2 [18], ResNet18 [7] or VGG11 [20] exist, their performance is generally restricted by their limited size and still require several million parameters.With Med-NCA, Kalkhof et al. [11] have shown that by iterating over two scales of the same image, high-resolution 2D medical image segmentation using NCAs is possible while reaching similar performance to UNet-style architectures. While this is a step in the right direction, the limitation to two-dimensional data and the fixed number of downscaling layers make this method inapplicable for many medical imaging scenarios and ultimately restricts its potential.Naively adapting Med-NCA for three-dimensional inputs exponentially increases VRAM usage and convergence becomes unstable. We address these challenges with M3D-NCA, which takes NCA medical image segmentation to the third dimension and is illustrated in Fig. 1. Our n-level architecture addresses VRAM limitations by training on patches that are precisely adaptable to the dataset requirements. Due to the one-cell architecture of NCAs the inference can be performed on the full-frame image. Our batch duplication scheme stabilizes the loss across segmentation levels, enabling segmentation of highresolution 3D volumes with NCAs. In addition, we propose a pseudo-ensemble technique that exploits the stochasticity of NCAs to generate multiple valid segmentations masks that, when averaged, improve performance by 0.5-1.3%. Moreover, by calculating the variance of these segmentations we obtain a quality assessment of the derived segmentation mask. Our NCA quality metric (NQM) detects between 50% (prostate) and 94.6% (hippocampus) of failure cases. M3D-NCA is lightweigth enough to be run on a Raspberry Pi 4 Model B (2 GB RAM).We compare our proposed M3D-NCA against the UNet [17], minimal variations of UNet, Seg-NCA [19] and Med-NCA [11] on the medical segmentation decathlon [2] datasets for hippocampus and prostate. M3D-NCA consistently outperforms minimal UNet-style and other NCA architectures by at least 2.2% and 1.1% on the hippocampus and prostate, respectively, while being at least two magnitudes smaller than UNet-style models. However, the performance is still lower than the nnUNet by 0.6% and 6.3% Dice, the state-of-the-art auto ML pipeline for many medical image segmentation tasks. This could be due to the additional pre-and post-processing steps of the pipeline, as well as the extensive augmentation operations. We make our complete framework available under github.com/MECLabTU DA/M3D-NCA, including the trained M3D-NCA models for both anatomies as they are only 56 KB in size."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,2,Methodology,"Cellular Automata (CA) are sets of typically hand-designed rules that are iteratively applied to each cell of a grid. They have been actively researched for decades, Game Of Life [4] being the most prominent example of them. Recently, this idea has been adapted by Gilpin et al. [5] to use neural networks as a representation of the update rule. These Neural Cellular Automata (NCA) are minimal and interact only locally (illustration of a 2D example can be found in the supplementary). Recent research has demonstrated the applicability of NCAs to many different domains, including image generation tasks [12], selfclassification [16], and even 2D medical image segmentation [11].NCA segmentation in medical images faces the problem of high VRAM consumption during training. Our proposed M3D-NCA described in Sect. 2.1 solves this problem by performing segmentation on different scales of the image and using patches during training. In Sect. 2.3 we introduce a score that indicates segmentation quality by utilizing the variance of NCAs during inference."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,2.1,M3D-NCA Training Pipeline,"Our core design principle for M3D-NCA is to minimize the VRAM requirements. Images larger than 100 × 100, can quickly exceed 40 GB of VRAM, using a naive implementation of NCA, especially for three-dimensional configurations.The training of M3D-NCA operates on different scales of the input image where the same model architecture m is applied, as illustrated in Fig. 2. The input image is first downscaled by the factor d multiplied by the number of layers n. If we consider a setup with an input size of 320 × 320 × 24, a downscale factor of d = 2, and n = 3, the image is downscaled to 40 × 40 × 3. As d and n exponentially decrease the image size, big images become manageable. On this smallest scale, our first NCA model m 1 , which is constructed from our core architecture (Sect. 2.2), is iterated over for s steps, initializing the segmentation on the smallest scale. The output of this model gets upscaled by factor d and appended with the according higher resolution image patch. Then, a random patch is selected of size 40 × 40 × 3, which the next model m 2 iterates over another s times. We repeat this patchification step n -1 times until we reach the level with the highest resolution. We then perform the dice focal loss over the last remaining patch and the according ground truth patch. Changing the downscaling factor d and the number of layers n allows us to precisely control the VRAM required for training."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Batch Duplication:,"Training NCA models is inherently more unstable than classical machine learning models like the UNet, due to two main factors. First, stochastic cell activation can result in significant jumps in the loss trajectory, especially in the beginning of the training. Second, patchification in M3D-NCA can cause serious fluctuations in the loss function, especially with three or more layers, thus it may never converge properly.The solution to this problem is to duplicate the batch input, meaning that the same input images are multiple times in each batch. While this limits the number of images per stack, it greatly improves convergence stability."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Pseudo Ensemble:,"The stochasticity of NCAs, caused by the random activation of cells gives them an inherent way of predicting multiple valid segmentation masks. We utilize this property by executing the trained model 10 times on the same data sample and then averaging over the outputs. We visualize the variance between several predictions in Fig. 3. Once the model is trained, inference can be performed directly on the fullscale image. This is possible due to the one-cell architecture of NCAs, which allows them to be replicated across any image size, even after training."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,2.2,M3D-NCA Core Architecture,"The core architecture of M3D-NCA is optimized for simplicity. First, a convolution with a kernel size k is performed, which is appended with the identity of the current cell state of depth c resulting in state vector v of length 2 * c. v thus contains information about the surrounding cells and the knowledge stored in the cell. v is then passed into a dense layer of size h, followed by a 3D BatchNorm layer and a ReLU. In the last step, another Dense layer is applied, which has the output size c, resulting in the output being of the same size as the input. Now the cell update can be performed, which adds the model's output to the previous state. Performing a full execution of the model requires it to be applied s times. In the standard configuration, the core NCA sets the hyperparameters to k = 7 for the first layer, and k = 3 for all the following ones. c = 16 and h = 64 results in a model size of 12480 parameters. The bigger k in the first level allows the model to detect low-frequency features, and c and h are chosen to limit VRAM requirements. The steps s are determined per level by s = max(width, height, depth)/((k -1)/2), allowing the model to communicate once across the whole image."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,2.3,Inherent Quality Control,"The variance observed in the derived segmentation masks serves as a quantifiable indicator of the predicted segmentation. We expect that a higher variance value indicates data that is further away from our training domain and consequently may lead to poorer segmentation accuracy. Nevertheless, relying solely on this number is problematic, as the score obtained is affected by the size of the segmentation mask. To address this issue, we normalize the metric by dividing the sum of the standard deviation by the number of segmentation pixels.The NCA quality metric (NQM) where v is an image volume and v i are N = 10 different predictions of M3D-NCA for v is defined as follows: We calculate the relation between Dice and NQM by running a linear regression on the training dataset, which has been enriched with spike artifacts to extend the variance range. Using the regression, we derive the detection threshold for a given Dice value (e.g., Dice > 0.8). In clinical practice, this value would be based on the task and utility."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,3,Experimental Results,"The evaluation of the proposed M3D-NCA and baselines is performed on hippocampus (198 patients, ∼ 35×50×35) and prostate (32 patients, ∼ 320×320× 20) datasets from the medical segmentation decathlon (medicaldecathlon.com) [2,21]. All experiments use the same 70% training, and 30% test split and are trained on an Nvidia RTX 3090Ti and an Intel Core i7-12700. We use the standard configuration of the UNet [14], Segmentation Models Pytorch [8] and nnUNet [9] packages for the implementation in PyTorch [13]."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,3.1,Comparison and Ablation,"Our results in Fig. 4 show that despite their compactness, M3D-NCA performs comparably to much larger UNet models. UNet-style models instead tend to underperform when parameter constraints are imposed. While an advanced training strategy, such as the auto ML pipeline nnUNet, can alleviate this problem, it involves millions of parameters and requires a minimum of 4 GB of VRAM [9].In contrast, our proposed M3D-NCA uses two orders of magnitude fewer parameters, reaching 90.5% and 82.9% Dice for hippocampus and prostate respectively. M3D-NCA outperforms all basic UNet-style models, falling short of the nnUNet by only 0.6% for hippocampus and 6.3% for prostate segmentation. Utilizing the 3D patient data enables M3D-NCA to outperform the 2D segmentation model Med-NCA in both cases by 2.4% and 1.1% Dice. The Seg-NCA [19] is due to its one-level architecture limited to small input images of the size 64 × 64, which for prostate results in a performance difference of 12.8% to our proposed M3D-NCA and 5.4% for hippocampus. We execute M3D-NCA on a Raspberry Pi 4 Model B (2 GB RAM) to demonstrate its suitability on resource-constrained systems, as shown in Fig. 5. Although our complete setup can be run on the Raspberry Pi 4, considerably larger images that exceed the device's 2 GB memory limit require further optimizations within the inference process. By asynchronously updating patches of the full image with an overlapping margin of (k -1)/2 we can circumvent this limitation while ensuring identical inference. The ablation study of M3D-NCA in Table 1 shows the importance of batch duplication during training, especially for larger numbers of layers. Without batch duplication, performance drops by 1.8-7.9% Dice. Increasing the number of layers reduces VRAM requirements for larger datasets, but comes with a trade-off where each additional layer reduces segmentation performance by 2.7-5.5% (with the 4-layer setup, a kernel size of 5 is used on the first level, otherwise the downscaled image would be too small). The pseudo-ensemble setup improves the performance of our models by 0.5-1.3% and makes the results more stable. The qualitative evaluation of M3D-NCA, illustrated in Fig. 6, shows that M3D-NCA produces accurate segmentations characterized by well-defined boundaries with no gaps or random pixels within the segmentation volume."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,3.2,Automatic Quality Control,"To evaluate how well M3D-NCA identifies failure cases through the NQM metric, we degrade the test data with artifacts using the TorchIO package [15]. More precisely, we use noise (std = 0.5), spike (intensity = 5) and ghosting (num ghosts = 6 and intensity = 2.5) artifacts to force the model to collapse (prediction/metric pairs can be found in the supplementary). We effectively identify 94.6% and 50% of failure cases (below 80% Dice) for hippocampus and prostate segmentation, respectively, as shown in Fig. 7. Although not all failure cases are identified for prostate, most false positives fall close to the threshold. Furthermore, the false negative rates of 4.6% (hippocampus) and 8.3% (prostate), highlights its value in identifying particularly poor segmentations. "
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,4,Conclusion,"We introduce M3D-NCA, a Neural Cellular Automata-based training pipeline for achieving high-quality 3D segmentation. Due to the small model size with under 13k parameters, M3D-NCA can be run on a Raspberry Pi 4 Model B (2 GB RAM). M3D-NCA solves the VRAM requirements for 3D inputs and the training instability issues that come along. In addition, we propose an NCA quality metric (NQM) that leverages the stochasticity of M3D-NCA to detect 50-94.6% of failure cases without additional overhead. Despite its small size, M3D-NCA outperforms UNet-style models and the 2D Med-NCA by 2% Dice on both datasets. This highlights the potential of M3D-NCAs for utilization in primary care facilities and conflict zones as a viable lightweight alternative."
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Fig. 1 .,
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Fig. 2 .,
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Fig. 3 .,
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Fig. 4 .,
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Fig. 5 .,
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Fig. 6 .,
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Fig. 7 .,
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Table 1 .,
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,.051 0.811 ± 0.045 0.824 ± 0.051,
M3D-NCA: Robust 3D Segmentation with Built-In Quality Control,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 17.
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,1,Introduction,"Instances with complex shapes arise in many biomedical domains, and their morphology carries critical information. For example, the structure of gland tissues in microscopy images is essential in accessing the pathological stages for cancer diagnosis and treatment. These instances, however, are usually closely in touch with each other and have non-convex structures with parts of varying widths (Fig. 1a), posing significant challenges for existing segmentation methods.In the biomedical domain, most methods [3,4,13,14,22] first learns intermediate representations and then convert them into masks with standard segmentation algorithms like connected-component labeling and watershed transform. These representations are not only efficient to predict in one model forward pass but also able to capture object geometry (i.e., precise instance boundary), which are hard for top-down methods using low-resolution features for mask generation. However, existing representations have several restrictions. For example, boundary map is usually learned as a pixel-wise binary classification task, which makes the model conduct relatively local predictions and consequently become vulnerable to small errors that break the connectivity between adjacent instances (Fig. 1b). To improve the boundary map, Deep Watershed Transform (DWT) [1] predicts the Euclidean distance transform (DT) of each pixel to the instance boundary. This representation is more aware of the structure for convex objects, as the energy value for centers is significantly different from pixels close to the boundary. However, for objects with non-convex morphology, the boundary-based distance transform produces multiple local optima in the energy landscape (Fig. 1c), which tends to break the intra-instance connectivity when applying thresholding and results in over-segmentation.To preserve the connectivity of instances while keeping the precise instance boundary, in this paper, we propose a novel representation named skeleton-aware distance transform (SDT). Our SDT incorporate object skeleton, a concise and connectivity-preserving representation of object structure, into the traditional boundary-based distance transform (DT) (Fig. 1d). In quantitative evaluations, we show that our proposed SDT achieves leading performance on histopathology image segmentation for instances with various sizes and complex structures. Specifically, under the Hausdorff distance for evaluating shape similarity, our approach improves the previous state-of-the-art method by relatively 10.6%."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,1.1,Related Work,"Instance Segmentation. Bottom-up instance segmentation approaches have become de facto for many biomedical applications due to the advantage in segmenting objects with arbitrary geometry. U-Net [14] and DCAN [4] use fully convolutional models to predict the boundary map of instances. Since the boundary map is not robust to small errors that can significantly change instance structure, shape-preserving loss [22] adds a curve fitting step in the loss function to enforce boundary connectivity. In order to further distinguish closely touching instances, deep watershed transform (DWT) [1] predicts the distance transform (DT) that represents each pixel as its distance to the closest boundary. However, for complex structure with parts of varying width, the boundary-based DT tends to produce relatively low values for thin connections and consequently causes oversegmentation. Compared to DWT, our SDT incorporates object skeleton (also known as medial axis) [2,8,24] that concisely captures the topological connectivity into standard DT to enforce both the geometry and connectivity.Object Skeletonization. Object skeleton [15] is a one-pixel wide representation of object masks that can be calculated by topological thinning [8,12,24] or medial axis transform [2]. The vision community has been working on direct object skeletonization from images [7,9,16,19]. Among the works, only Shen et al. [16] shows the application of the skeleton on segmenting single-object images. We instead focus on the more challenging instance segmentation task with multiple objects closely touching each other. Object skeletons are also used to correct errors in pre-computed segmentation masks [11]. Our SDT framework instead use the skeleton in the direct segmentation from images."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,2,Skeleton-Aware Distance Transform,
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,2.1,SDT Energy Function,"Given an image, we aim to design a new representation E for a model to learn, which is later decoded into instances with simple post-processing. Specifically, a good representation for capturing complex-structure masks should have two desired properties: precise geometric boundary and robust topological connectivity.Let Ω denote an instance mask, and Γ b be the boundary of the instance (pixels with other object indices in a small local neighborhood). The boundary (or affinity) map is a binary representation where E| Γ b = 0 and E| Ω\Γ b = 1. Taking the merits of DT in modeling the geometric arrangement and skeleton in preserving connectivity, we propose a new representation E that satisfies:Here E| Ω\Γs < E| Γs = 1 indicates that there is only one global maximum for each instance, and the value is assigned to a pixel if and only if the pixel is on the skeleton. This property avoids ambiguity in defining the object interior and preserve connectivity. Besides, E| Ω\Γ b > E| Γ b = 0 ensures that boundary is distinguishable as the standard DT, which produces precise geometric boundary.For the realization of E, let x be a pixel in the input image, and d be the metric, e.g., Euclidean distance. The energy function for distance transform (DT) is defined as E DT (x) = d(x, Γ b ), which starts from 0 at object boundary and increases monotonically when x is away from the boundary. Similarly, we can define an energy function d(x, Γ s ) representing the distance from the skeleton. It vanishes to 0 when the pixel approaches the object skeleton. Formally, we define the energy function of the skeleton-aware distance transform (Fig. 2) aswhere α controls the curvature of the energy surface1 . When 0 < α < 1, the function is concave and decreases faster when being close to the boundary, and vice versa when α > 1. In the ablation studies, we demonstrate various patterns of the model predictions given different α.Besides, since common skeletonization algorithms can be sensitive to small perturbations on the object boundary and produce unwanted branches, we smooth the masks before computing the object skeleton by Gaussian filtering and thresholding to avoid complex branches.Learning Strategy. Given the ground-truth SDT energy map, there are two ways to learn it using a CNN model. The first way is to regress the energy using L 1 or L 2 loss. In the regression mode, the output is a single-channel image. The second way is to quantize the [0, 1] energy space into K bins and rephrase the regression task into a classification task [1,18], which makes the model robust to small perturbations in the energy landscape. For the classification mode, the model output has (K +1) channels with one channel representing the background region. We fix the bin size to 0.1 without tweaking, making K = 10. Softmax is applied before calculating the cross-entropy loss. We test both learning strategies in the experiments to illustrate the optimal setting for SDT."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,2.2,SDT Network,"Network Architecture. Directly learning the energy function with a fully convolutional network (FCN) can be challenging. Previous approaches either first regress an easier direction field representation and then use additional layers to predict the desired target [1], or take the multi-task learning approach to predict additional targets at the same time [16,21,22]. Fortunately, with recent progress in FCN architectures, it becomes feasible to learn the target energy map in an end-to-end fashion. Specifically, in all the experiments, we use a DeepLabV3 model [5] with a ResNet [6] backbone to directly learn the SDT energy without additional targets (Fig. 3, Training Phase). We also add a CoordConv [10] layer before the 3rd stage in the backbone network to introduce spatial information into the segmentation model."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Target SDT Generation.,"There is an inconsistency problem in object skeleton generation: part of the complete instance skeleton can be different from the skeleton of the instance part (Fig. 4). Some objects may touch the image border due to either a restricted field of view (FoV) of the imaging devices or spatial data augmentation like the random crop. If pre-computing the skeleton, we will get local skeleton (Fig. 4c) for objects with missing masks due to imaging restrictions, and partial skeleton (Fig. 4b) due to spatial data augmentation, which causes ambiguity. Therefore we calculate the local skeleton for SDT on-the-fly after all spatial transformations instead of pre-computing to prevent the model from hallucinating the structure of parts outside of the currently visible region. In inference, we always run predictions on the whole images to avoid inconsistent predictions. We use the skeletonization algorithm in Lee et al. [8], which is less sensitive to small perturbations and produces skeletons with fewer branches.Instance Extraction from SDT. In the SDT energy map, all boundary pixels share the same energy value and can be processed into segments by direct thresholding and connected component labeling, similar to DWT [1]. However, since the prediction is never perfect, the energy values along closely touching boundaries are usually not sharp and cause split-errors when applying a higher threshold or merge-errors when applying a lower threshold. Therefore we utilize a skeleton-aware instance extraction (Fig. 3, Inference Phase) for SDT. Specifically, we set a threshold θ = 0.7 so that all pixels with the predicted energy bigger than θ are labeled as skeleton pixels. We first perform connected component labeling of the skeleton pixels to generate seeds and run the watershed algorithm on the reversed energy map using the seeds as basins (local optima) to generate the final segmentation. We also follow previous works [4,22] and refine the segmentation by hole-filling and removing small spurious objects."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,3,Experiments,
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,3.1,Histopathology Instance Segmentation,"Accurate instance segmentation of gland tissues in histopathology images is essential for clinical analysis, especially cancer diagnosis. The diversity of object appearance, size, and shape makes the task challenging.Dataset and Evaluation Metric. We use the gland segmentation challenge dataset [17] that contains colored light microscopy images of tissues with a wide range of histological levels from benign to malignant. There are 85 and 80 images in the training and test set, respectively, with ground truth annotations provided by pathologists. According to the challenge protocol, the test set is further divided into two splits with 60 images of normal and 20 images of abnormal tissues for evaluation. Three evaluation criteria used in the challenge include instance-level F1 score, Dice index, and Hausdorff distance, which measure the performance of object detection, segmentation, and shape similarity, respectively. For the instance-level F1 score, an IoU threshold of 0.5 is used to decide the correctness of a prediction."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Methods in Comparison.,"We compare SDT with previous state-of-the-art segmentation methods, including DCAN [4], multi-channel network (MCN) [21], shape-preserving loss (SPL) [22] and FullNet [13]. We also compare with suggestive annotation (SA) [23], and SA with model quantization (QSA) [20], which use multiple FCN models to select informative training samples from the dataset. With the same training settings as our SDT, we also report the performance of skeleton with scales (SS) and traditional distance transform (DT). Training and Inference. Since the training data is relatively limited due to the challenges in collecting medical images, we apply pixel-level and spatial-level augmentations, including random brightness, contrast, rotation, crop, and elastic transformation, to alleviate overfitting. We set α = 0.8 for our SDT in Eq. 2. We use the classification learning strategy and optimize a model with 11 output channels (10 channels for energy quantized into ten bins and one channel for Table 1. Comparison with existing methods on the gland segmentation. Our SDT achieves better or on par F1 score and Dice Index, and significantly better Hausdorff distance for evaluating shape similarity. DT and SS represent distance transform and skeleton with scales. [4] 0.912 0.716 0.897 0.781 45.42 160.35 MCN [21] 0.893 0.843 0.908 0.833 44.13 116.82 SPL [22] 0.924 0.844 0.902 0.840 49.88 106.08 SA [23] 0.921 0.855 0.904 0.858 44.74 96.98 FullNet [13]  Specifically for SS, we set the number of output channels to two, with one channel predicting skeleton probability and the other predicting scales. Since the scales are non-negative, we add a ReLU activation for the second channel and calculate the regression loss. Masks are generated by morphological dilation. We do not quantize the scales as DT and SDT since even ground-truth scales can yield masks unaligned with the instance boundary with quantization."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Results.,"Our SDT framework achieves state-of-the-art performance on 5 out of 6 evaluation metrics on the gland segmentation dataset (Table 1). With the better distinguishability of object interior and boundary, SDT can unambiguously separate closely touching instances (Fig. 5, first two rows), performs better than previous methods using object boundary representations [4,22]. Besides, under the Hausdorff distance for evaluating shape-similarity between ground-truth and predicted masks, our SDT reports an average score of 44.82 across two test splits, which improves the previous state-of-the-art approach (i.e., FullNet with an average score of 50.15) by 10.6%. We also notice the different sensitivities of the three evaluation metrics. Taking the instance D (Fig. 5, 3rd row) as an example: both SDT and FullNet [13] have 1.0 F1-score (IoU threshold 0.5) for the correct detection; SDT has a slightly higher Dice Index (0.956 vs. 0.931) for better pixel-level classification; and our SDT has significantly lower Hausdorff distance (24.41 vs. 48.81) as SDT yields a mask with much more accurate morphology."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,3.2,Ablation Studies,"Loss Function. We compare the regression mode using L1 and L2 losses with the classification mode using cross-entropy loss. There is a separate channel for background under the classification mode where the energy values are quantized into bins. However, for regression mode, if the background value is 0, we need to use a threshold τ > 0 to decide the foreground region, which results in shrank masks. To separate the background region from the foreground objects, we assign an energy value of -b to the background pixels (b ≥ 0). To facilitate the regression, given the predicted value ŷi for pixel i, we apply a sigmoid function (σ) and affine transformation so that ŷ i = (1+b) • σ(ŷ i )b has a range of (-b, 1). We set b = 0.1 for the experiments. We show that under the same settings, the model trained with quantized energy reports the best results (Table 2). We also notice that the model trained with L 1 loss produces a much sharper energy surface than the model trained with L 2 loss, which is expected.Curvature. We also compare different α in Eq. 2 that controls the curvature of the energy landscape. Table 2 shows that α = 0.8 achieves the best overall performance, which is slightly better than α = 1.0. Decreasing α to 0.6 introduces more merges and make the results worse.Global/Local Skeleton. In Sect. 2.2 we show the inconsistency problem of global and local skeletons. In this study, we set α = 0.8 and let the model learn the pre-computed SDT energy for the training set. The results show that pre-computed SDT significantly degrades performance (Table 2). We argue this is because pre-computed energy not only introduces inconsistency for instances touching the image border but also restricts the diversity of SDT energy maps."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,4,Conclusion,"In this paper, we introduce the skeleton-aware distance transform (SDT) to capture both the geometry and topological connectivity of instance masks with complex shapes. For multi-class problems, we can use class-aware semantic segmentation to mask the SDT energy trained for all objects that is agnostic to their classes. We hope this work can inspire more research on not only better representations of object masks but also novel models that can better predict those representations with shape encoding. We will also explore the application of SDT in the more challenging 3D instance segmentation setting."
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Fig. 1 .,
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Fig. 2 .,
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Fig. 3 .,
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Fig. 4 .,
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Fig. 5 .,
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,,Table 2 .,
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,1,Introduction,"The human brain comprises millions of neurons that interconnect via intricate neural synapses, enabling efficient information exchange and transient, self-organized functional fluctuations. Regrettably, this rapid and efficient transport mechanism also facilitates the dissemination of toxic pathological proteins, including amyloid-beta (Aβ) plaques and neurofibrillary tangles (tau), which spread rapidly throughout the brain, exacerbating the progression of Alzheimer's disease (AD) [3]. Although tremendous efforts have been made to comprehend the principles and mechanisms that underlie complex brain cognition and behavior as a complex information-exchanging system [1], AD presents a distinct clinical course characterized cognitive decline as the earliest symptom, indicating an obstacle in the information-exchanging system in the brain. The studies so far indicate that the cause of such phenomenon is well-correlated with a progressive pattern of intracellular aggregates of tau (neurofibrillary tangles) [27]. However, other research suggests that amyloidosis precedes the spread of pathologic tau, ultimately leading to neurodegeneration and cognitive decline [21]. As such, the role of Aβ and tau in the pathogenesis of AD remains an open question, particularly with respect to the interaction between these two factors (Aβ-tau).Aβ and tau represent pathological hallmarks of AD, which can be measured through PET (positron emission tomography) scan [6]. With the rapid advancement of neuroimaging technologies such as magnetic resonance imaging (MRI) and diffusion-weighted imaging (DWI), it has become feasible to investigate the wiring of neuronal fibers (aka. structural connectomes) of the human brain invivo [10,28]. As evidence suggests that AD is characterized by the propagation of tau aggregates triggered by the Aβ build-up [3,11], tremendous machine learning efforts have been made to predict the spreading of tau pathology in the progression of AD from longitudinal PET scans [26,30]. With the prevalence of public neuroimaging data cohorts such as ADNI [23], the research focus of computational neuroscience shifts to the realm of deep learning.A plethora of deep learning models [14,15,24,25] have been proposed to predict clinical outcomes by combining network topology heuristics and pathology measurements, including Aβ and tau, at each brain region. In spite of various machine learning backbones such as graph neural network (GNN) networks [16], most of the methods are formulated as a graph embedding representation learning problem, that is, aggregating the node features with a graph neighborhood in a ""blackbox"" such that the diffused graph embedding vectors are aligned with the one-hot vectors of outcome variables (i.e., healthy or disease condition). Although the graph attention technique [20,29] allows us to quantify the contribution of each node/link in predicting outcome, its power is limited in dissecting the mechanistic role of Aβ-tau interactions, which drives the dynamic prion-like pattern of tau propagation throughout the brain network.Fortunately, the partial differential equation (PDE)-based systems biology approach studies biological pathways and interactions between Aβ and tau from the mathematical perspective, allowing us to uncover the intrinsic mechanism that steers the spatiotemporal dynamics of tau propagation throughout the brain. In this context, reaction-diffusion model (RDM) [17] has shown promising results by explicitly modeling the Aβ-tau interaction and the prion-like propagation of tau aggregates using PDEs [12,31]. Neuro-RDM [7], a recent study, has demonstrated the potential of using neural networks to predict the state of brain activities underlying the RDM mechanism. By treating the brain as a complex system, Neuro-RDM designs an equivalent deep model of RDM that addresses the issue of a-priori choice of basis function in the conventional PDE-based model and it unveils the brain dynamic. The model takes observation signals as input and characterizes the reaction of massive neuronal synapses at each brain region as the reaction process, while considering the diffusion process as the information exchange process between regions. Eventually, solving the PDE enables the inference of the evolutionary states of the brain.Following this cue, we sought to integrate the principle of systems biology and the power of machine learning in a unified mathematical framework, with a focus on an RDM-based deep model for uncovering the novel biological mechanism. Specifically, we introduce a novel framework for producing fresh PDE-based solutions from an application-specific constrained functional, known as Aβ influence. We formulate the evolving biological process of Aβ cascade and tau propagation into a closed-loop feedback system where the system dynamics are constrained by region-to-region white matter fiber tracts in the brain. This approach enables accurate prediction of the progression of the underlying neurobiological process, namely tau propagation. Additionally, we develop an explainable deep learning model that is based on the newly formulated RDM. The neural network is trained to clarify the Aβ-tau interaction while adhering to the principles of mathematics. We demonstrate promising results in both predicting AD progression and diagnosing the disease on the ADNI dataset."
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,2,Method,
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,,Suppose we have a brain network,", and the output is the clinical outcome η T . From the perspective of brain dynamics, we introduce an evolution state v i (t) for each brain region, which can be regarded as the intrinsic interaction trajectory of the features of each brain node. Herein, we investigate two prominent features on the basis of the brain region, namely tau-x i (t) and Aβ-u i (t), we then explore the interaction between tau propagation and amyloid cascade, which is believed to play a crucial role in the evolution dynamicsi=1 of AD progression. In particular, we investigate how Aβ influences the spreading of tau in AD progression. Our study aims to shed light on the complex mechanisms underlying the progression of AD, a critical area of research in the field of neuroscience."
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,2.1,Reaction-Diffusion Model for Neuro-Dynamics,"RDM, a mathematical model with the capability to capture various dynamic evolutionary phenomena, is often employed to describe the reaction-diffusion process [7,17] that governs the evolution of the dynamic state v(t) of the brain.A = -∇ • (∇) expresses the Laplacian operator, which is represented by the divergence ∇• of gradient ∇. In this context, the first term of Eq. ( 1) denotes the diffusion process (i.e., the information exchange between nodes) constrained by the network topology A. R(x, v, t) denotes the reaction process that encapsulates the nonlinear interaction between the observation x and the evolution state v.In the deep neural network chiché, the nonlinear interaction is often defined as"
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,2.2,Construction on the Interaction Between Tau and Amyloid,"Conventionally the connection between tau and amyloid has been established by treating them as an embedding (z i {x i , u i } ∈ R M ) on the graph in a cuttingedge graph-based learning approach [14,24,25], such manner cannot capture the interaction between the features of embedding on the node. Herein, we propose a novel solution to model the interaction between tau x i and amyloid u i . Upon the RDM, we introduce an interaction term that describes how amyloid has influenced the spreading of tau during the evaluation of tau accumulation. Following this clue, we propose a new PDE as follows:where the designed last term characterizes the interaction between the evolution state v and u with B denoting the interaction matrix. To reasonably and appropriately establish the interaction, we incorporate an interaction constraint that ensures a desirable evolution state. In the spirit of the linear quadratic regulator (LQR) in control theory [2], the interaction constraint is formulated by:where the requirement that L ≥ 0 implies that both P and Q are positive definite. Minimizing the objective function Eq. ( 3) can yield the stable and highperformance design of the new PDE. To achieve this, we can minimize Eq. ( 3) through an optimal control constraint problem, as outlined in [4]. Such optimal control constraint is regarded as a closed-loop feedback system, u acts as the control term to yield the optimal feedback in control theory [4]. Moreover, to account for the known clinical outcome (i.e., ground truth η T ), we incorporate the clinical outcome as a terminal cost in the optimization process. In doing so, we formulate the problem as a supervised evolution process, thereby enhancing the accuracy of the inference process in reflecting the actual disease progression.where the terminal cost ψ(v T , η T ) = KL(v T , η T ) is measured by Kullback-Leibler (KL) divergence [18] between predicted final status v T and the ground truth η T . The second term is the LQR constraint described in Eq. (3). To solve this optimization problem, we follow the approach outlined in [5], where we first construct a novel PDE (blue box in Fig. 1) to model the evolution of brain dynamics, introduce an interaction constraint on the basis of LQR (cyan box Fig. 1), followed by optimizing the equation with the aid of ground truth (orange box Fig. 1) to guide the learning. Figure 1 illustrates an overview of our framework."
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,2.3,Neural Network Landscape of RDM-Based Dynamic Model,"In this section, we further design the explainable deep model based on the new PDE, and the designed deep model is trained to learn the mechanism of neurodynamics, i.e., tau propagation, which can predict disease progression and diagnosis accuracy.The overall network architecture of our physics-informed model is shown in Fig. 2, in which the backbone is the reaction-diffusion model. Specifically, we first define the reaction process (R(x, v) in Eq. ( 2)) by a deep neural network (DNN, green shadow), thereby yielding the reacted state ṽ0 by the initial state v 0 and the observed tau level x 0 . A graph diffusion process is conducted by vanilla GNN (red shadow), and then we can obtain a desirable feature representation v0 by the tailored reaction-diffusion model. Inspired by the insights gained from the closed-loop feedback system, the LQR is implemented to accommodate problem constraints (cyan shadow) to produce the optimal interaction constraint û under a supervised manner (orange shadow). Upon the v0 and û, the new PDE equation can be built according to Eq. ( 2), then we use the PDE solver with time-constant [13] (gray shadow) to recurrently seek the future evolutionary state trajectory v 1 to v T . Eventually, the predicted xT is derived by a mapping function formulated "
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,3,Experiments,
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,3.1,Data Description and Experimental Setting,"We evaluate the performance of the proposed PDE-informed deep mode of neurodynamics on ADNI dataset [23]. We select 126 cohorts, in which the involved longitudinal observations of whole brain tau and Aβ SUVR (standard update value ratio) levels through PET scans have two time-series data of each subject at least. For Aβ data, we only retain the baseline as the interaction constraint reference. Each subject includes multiple diffusion-weighted imaging (DWI) scans, we merely extract the baseline to act as the structural connectome. We divided the involved cohorts into four groups based on the diagnostic labels of each scan, including the cognitive normal (CN) group, early-stage mild cognitive impairment (EMCI) group, late-stage mild cognitive impairment (LMCI) group and AD group. The precise description for the binary classification is the prediction of conversion of AD. Since the clinical symptom is not onset until converting from EMCI to LMCI, we consider CN+EMCI as 'non-convert' and LMCI+AD as 'converted' group. For each neuroimaging scan, we parcellate the whole brain into a cortical surface including 148 Destrieux regions [8] and 12 sub-cortical regions. The 148 cortical regions are separated into six lobes commonly identified in structural brain networks: frontal lobe, insula lobe, temporal lobe, occipital lobe, parietal lobe, and limbic lobe. Since the clinical diagnostic label is available at each visit time, we split long time series (≥ 3-time points) into a collection of 2-time-point temporal segments to reach data augmentation. By doing so, we augment the sample pool to a magnitude of 1.6 times larger.We (1) validate the performance of disease progression prediction (i.e., the future tau accumulation) of our proposed PDE-informed deep model, a PDEbased liquid time-constant network (LTC-Net) [13], a PDE-based neural network (Neuro-RDM) [7] and graph conventional network (GCN) [16] on ADNI dataset.(2) predict the diagnosis accuracy of AD (i.e., the recognition of 'non-convert' vs. 'converted'), and further uncover the interaction between tau and Aβ. To assess fairness, we perform one solely utilizing tau as input, and the other incorporating both amyloid and tau as input. In the latter scenario, we conduct a concatenation operation for LTC-Net and Neuro-RDM, with tau and Aβ serving as the graph embeddings based on GCN. To further verify the effectiveness of the proposed components of our deep model, we conduct an ablation study in terms of the presence/absence LQR constraint. Note, we report the testing results using 5fold cross-validation, the evaluation metrics involve (1) the mean absolute error (MAE) for predicting the level of tau burden (2) the prediction accuracy for recognizing the clinical outcomes."
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,3.2,Ablation Study in Prediction Disease Progression,"We design the ablation study in the scene of predicting the future tau burden x T using the baseline tau level x 0 , where we model the influence of amyloid build-up (u 0 ...u T ) in the time course of tau propagation dv dt . As shown in the first column of Fig. 3, the prediction error (MAE) by our method, denoted by ""OURS "" in blue, shows a reduction compared to our (down-graded) method without LQR constraint, denoted by ""OURS (w/o LQR)"", and our full method but without the supervised LQR constraint, denoted by ""OURS (w LQR)"", where '*' indicates the performance improvement is statistically significant (p < 0.0001). We also display the prediction MAE results by LTC-Net in green, Neuro-RDM in red, and GCN in purple, using 'tau' only and 'tau+Aβ' as the input, respectively.It is clear that (1) the prediction error by the counterpart methods is much less reliable than our PDE-informed method since the machine learning model does not fully capture neurobiological mechanism, and (2) adding additional information (Aβ) does not contribute to the prediction, partially due to the lack of modeling Aβ-tau interaction, implying that the Aβ interacts with the propagation of tau to a certain extent.  "
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,3.3,Prognosis Accuracies on Forecasting AD Risk,"First, suppose we have the baseline amyloid and tau scans, we evaluate the prediction accuracy in forecasting the risk of developing AD by LTC-Net (in green), Neuro-RDM (in red), GCN (in purple), and OURS (in blue) in Fig. 4(a). At the significance level of 0.001, our method outperforms all other counterpart methods in terms of prediction accuracy (indicated by '*'). Second, we sought to uncover the Aβ-tau interaction through the explainable deep model, by answering the following scientific questions.(In what mechanism that local or remote Aβ-tau interaction promotes the spreading of tau aggregates? Our explainable deep model aims to uncover the answer from the interaction matrix B in Eq. (2), which is the primary motivation behind our research. We visualize the interaction matrix B and the corresponding brain mapping (node size and link bandwidth are in proportion to the strength of local and remote interaction, respectively) in Fig. 4(b). Our analysis reveals that Aβ plaques primarily contribute to the local cascade of tau aggregates (Fig. 4(c)). However, we observe a few significant remote interactions in the middle-temporal lobe, where the high activity of tau pathology has been frequently reported [19,30]. (2) Which nodes are most vulnerable in the progression of AD? We retrain our method using AD subjects only. Then, following the notion (Gramian matrix K = T t=0 (A) t BB T (A T ) t ) of control theory [22], we calculate the node-wise T race(K ξi ) that projects the amount of effort (amyloid build-up in the scenario of AD progression) needed to reach the terminal state v T (i.e., developing AD). Note, a small degree of T race(•) indicates that the underlying node is vulnerable to the intervention of amyloid plaques. In this context, we display the top 10 most vulnerable brain regions in Fig. 4(d). It is apparent that most brain regions are located in temporal and limbic lobes, and sub-cortical areas, which is in line with the current clinical findings [19]."
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,4,Conclusion,"In this endeavor, we have embarked on an explainable machine learning initiative to unearth the intrinsic mechanism of Aβ-tau interaction from the unprecedented amount of spatiotemporal data. Since RDM has been well studied in the neuroscience field, we formulate optimal constraint in vanilla RDM and dissect it into the deep model. We have applied our RDM-based deep model to investigate the prion-like propagation mechanism of tau aggregates as well as the downstream association with clinical manifestations in AD, our tailored deep model not only achieves significant improvement in the prediction accuracy of developing AD, but also sheds the new light to discover the latent pathophysiological mechanism of disease progression using a data-driven approach."
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,,Fig. 1 .,
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,,Fig. 2 .,
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,,Fig. 3 .,
Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation,,Fig. 4 .,
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,1,Introduction,"Image segmentation is one of the most fundamental and popular tasks in medical image analysis. It is widely applied to parse organs, bones, soft tissues, or lesions in N -D medical images. Conventional methods rely on statistics of image intensities or object shapes to infer the boundaries of the target regions [8]. Recently, the convolutional neural networks (CNN) demonstrated superior performance in multiple tasks. CNNs are inherently translation-invariant with inneighborhood computation, which makes training efficient and deployment effective. For instance, the U-shaped encoder-decoder CNN is greatly favored among segmentation models due to its simplicity and effectiveness [16]. Despite the success, adding new components to existing models in pursuit of better performance and efficiency is always an ongoing effort in different research fields.Inspired by the advancement from related domains, e.g., natural language processing (NLP), transformers [24] have been successfully introduced to image processing and computer vision [7]. The transformer block is crafted with longrange dependency inside sequences with marginal inductive bias. To incorporate a transformer into image analysis models, images are divided into patches with equal size and serialized as a sequence of tokens, so that the transformer based models can treat N -D images in the same way as 1-D sentences. Such operation explicitly destroys neighborhood relationships in the images, and instead learnable position embeddings are added to encourage the learning of flexible patch interaction. In addition to supervised tasks, transformer-based models have also been shown to achieve superior performance in pre-training with largescale (labeled/unlabeled) data sets [11].Most existing neural architectures are designed with strong human heuristics. Neural architecture search (NAS) has been proposed in an attempt to reduce dependency on such heuristics while optimizing model performance for given tasks. Given target constraints, it is capable of optimizing multiple objectives (e.g., accuracy, memory consumption, latency, etc.) of the neural network models at the same time. Nowadays, NAS has been widely applied for many applications in medical imaging including image classification and segmentation.Existing NAS works have been focusing on optimization with convolutional deep learning components [18,31]. Our proposed NAS method, named DAST, on the other hand learns the relationship between convolutions and transformers within the search space of segmentation networks. During architecture searching, those two operations can be placed at different scale resolutions and levels for performance optimization. Intrinsically, it shall benefit from inductive biases of these two popular deep learning ingredients. Meanwhile, DAST is also equipped with capacities of optimizing memory consumption of the searched architecture, so that the input shape of the neural network can be properly adjusted according to the available computing resource, and long-range dependency of transformers can be visualized through attention matrices. We evaluated our proposed algorithm on two public data sets with excellent performance."
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,2,Related Work,"Neural architecture search tries to find optimal global model structures and local operations from large search spaces for different applications. Searching algorithms, including reinforcement learning and genetic algorithms [2,27,31], have been proposed for different search spaces. These approaches usually require large-scale computing resources to train a large number of independent neural networks, which makes them less practical when applied to large-scale data sets. On the contrary, differentiable neural architecture search (DARTS) aims to boost search efficiency and reduce computation budgets via continuous relaxation in the optimization [6,9,12,17,18,26,30]. It defines a large super-net containing all network candidates with learnable intermediate path weights, such that optimizing model architecture is equivalent to optimizing and binarizing those path weights. However, most existing NAS algorithms in medical imaging rely heavily on fully convolution-based search spaces, which may limit its receptive field.Transformer based neural network has been recently introduced to medical imaging domain for various applications following the success of vision transformer (ViT) in computer vision [7]. The direct extension applies ViT to 2D medical image analysis [21,23]. Some works have adopted ViT for 3D medical image segmentation [3,4,10,22,25,29] via serializing 3D images as sequences of patches/cubes. Most works rely on conventional designs of neural architectures and replace the convolution operations with transformers. For instance, the segmentation networks are always in ""symmetric"" encoder-decoder structure. However, from a network design's perspective, it is not trivial to find the right balance between convolutions and transformers inside the architecture. "
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,3,Method,"Our NAS algorithm, DAST, is an intuitive extension of DiNTS [12] for 3D medical image segmentation. Like other DARTS type of algorithms, it requires continuous relaxation of the super-net search space for gradient descent optimization. Unlike other NAS algorithms, it searches for a multi-path neural network and optimizes the searched architecture with additional memory constraints. Large image inputs can fit the searched architecture with low memory consumption, which helps to build long-range dependencies for transformers. Differentiable Search Space. Following DiNTS [12], the main search space is defined as R × L grids with R resolution scales and L levels from input image to output segmentation shown in Fig. 1. The grid node n i,j (at resolution i and level j) has directed connections towards neighboring nodes n i-1,j+1 , n i,j+1and n i+1,j+1 . Each edge e of connection is a weighted combination of outputs from operations o, and the pool of o includes skip-connection, convolution and transformer. For neighboring nodes at different levels, additional ×2 up-sampling or down-sampling is added in e, as well as convolutions to match feature channel numbers. There is no additional operation when passing features between nodes at the same level. To optimize the architecture, two different types of weights are introduced. Each edge has weight w e , and each operation has weight w o . Then the searched architecture can be defined when all w e and w o are binarized.The stem cells are concatenated at input and output of the search space. The input stem cells down-samples image toward different resolution scales and fit image features to the search space. The output stem cells up-samples multiscale features out of search space with necessary concatenation to produce multichannel probability maps. Transformer. We introduce transformer [24] into the search space as a candidate of the operation pool as shown in Fig. 1. The input and output of transformer are with dimension C × N (C is feature dimension and N is length of the sequence). However, this dimension is not suitable for networks with high dimensional image features. Therefore, we add a convolutional projector P [5] before the transformer, aiming to resize and project features X c×h×w×d to a smaller size with the number of channels matching the number of tokens required by the transformer (h, w, d denotes height, width, depth of 3D patches). Another input of projector is a learnable 3D positional encoding P shown in Eq. 1.The positional embedding P in Eq. 2 is initialized as a normalized 3D position map. It is efficient to compute with only 3 channels.The full transformer with an encoder and a decoder is adopted to process the output of the projector. The decoder takes additional learnable query embedding as input. The output of the transformer shares the same dimension/shape with the input. Next, we need to add another reversed projector to map the 1-D feature map back to the 3D shape of input. Segmentation Attention. To further understand the self-attention scheme, we embed an additional multi-head self-attention layer A after the transformer.A uses the feature maps from the transformer as the semantic query q, and the features from the transformer encoder as key k. Unlike multi-head self-attention layers inside the transformer, A does not have residual connection. Thus, A is enforced to learn meaningful attention weights for segmentation tasks. The attention weights are directly multiplied with intermediate features maps, which can be reshaped from 1-D to 3-D for visual interpretation. Memory Estimation. Like DiNTS [12], the memory budget constraints were proposed as part of loss functions to optimize memory usage at training and inference. It requires to estimate peak memory usage in each operation given the fixed input shape. Since the token is designed to have the same dimension C as the channel dimension at different resolution scales, the memory consumption estimation of entire transformer is shown in Eq. 3:In practice, the number of token l is fixed as 512 to avoid potential memory explosion. Eight heads are used in each multi-head self-attention layer. Number of operations N is approximately estimated as 15 including convolutions, batch normalization, linear operations, layer normalization, and multi-head selfattention."
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4,Experiments,
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.1,Data Sets and Implementation Details,"We adopted large-scale data sets Task07 Pancreas from Medical Segmentation Decathlon (MSD) [1] as used in [12] for architecture searching, and KiTS'19 [13,14] to validate the searched architectures. Both are very challenging applications involving various fields of view of CT volumes and different types of pathology. The pancreas data set has 3-class segmentation labels (background, pancreas and tumor) for 282 CT volumes. We adopt entire labeled set for NAS with the same data split as [12]: 114 volumes for model training, 114 volumes for architecture search, and 54 volumes for model validation. A different 4 : 1 data split is used for experiments of training from scratch. The KiTS'19 data set has 3-class segmentation labels (background, kidney and tumor) for 210 CT volumes, and an additional standalone 90 test volumes (with hidden ground truth) for the public leaderboard. We train the searched models with 5-fold data split, and verify the model performance on the test set using the public leaderboard. All data sets are re-sampled into the isotropic voxel spacing 1.0 mm for both images and labels. For both CT data sets, the voxel intensities of the images are normalized to the range [0, 1] according to the 5 th and 95 th percentile of overall foreground intensities.A combination of dice loss and cross-entropy loss is adopted to minimize both global and pixel-wise distance between ground truth and predictions. The NAS is conducted through conventional bi-level optimization following implementation 1 . We use the same definition of search space with 4 resolution scales and 12 levels. For cell level searching, we only use three different operations: skip-connection, convolution and transformer. For model training, we use a large input patch shape 160 3 , and the batch size at each GPU is 2. The training settings (like data augmentation, optimizer, etc.) are very similar to the model searching. We keep a constant learning rate 1e -3 to train the model from scratch for 40, 000 iterations. Our experiments are conducted using MONAI and trained on eight NVIDIA V100 GPUs with 32 GB of memory. For searching or training, the time cost is ∼ 15 hours including training and validation on-the-fly (similar as [12])."
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.2,Comparison with DiNTS,"Since DiNTS is the closest work to DAST, we directly compare the performance on DiNTS's searching tasks with the same data split. Then we re-train the searched architectures from both methods from scratch. As shown in Fig. 2, DAST has better training convergence and validation accuracy compared to DiNTS. The default model input shapes of DAST and DiNTS are different, so we experiment with various combinations. With input shape 160 3 , DAST converges faster than DiNS-160 with a better validation curve. The same conclusion can be made with input shape 96 3 . Based on the results, training with smaller input shape would make the training process harder. DAST consistently has better performance than DiNTS under different settings."
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.3,KiTS'19 Experiments,"To verify the effectiveness and generalization of our searched architectures from DAST, we validate the searched architecture (from pancreas data set) on this challenging task. Metrics for kidneys and tumors are the average Dice score per case. Finally, we evaluate our single-fold model as well as the ensemble from 5 cross-validation models on the public test leaderboard2 .Table 1. KiTS'19 challenge test-set performance evaluation for kidney and tumor segmentation in terms of the average Dice score per case. The evaluation results of our method are copied directly from the public leaderboard. Based on the results from the public leaderboard in Table . 1, our single-fold model and ensemble of five models achieve excellent performance compared to all other entries in the challenge shown in the Table . 1. The nnU-Net [16] is the best among all other entries, but the method utilized 20 U-Net models with training strategies to achieve the ensemble result for the challenge. Some other entries rely on cascaded models, which use more complex and intensive training mechanisms. On the contrary, DAST shows great simplicity when transferring a searched architecture to a new task. It is important to point out that the performance of our models is not only the best of all entries with publications, but also the best of all public entries (around 2, 000) on the test leaderboard."
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,,Method,
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.4,Ablation Studies,"Memory Constraints. We provide the option to change the parameter controlling memory consumption budget in the loss function with different values shown in Fig. 3. From the results, we can observe that given different values, the searched models have a clear trend: for the model with the highest memory (λ = 0.8), transformers are distributed at different resolution scales. As memory constraints increase (where λ is reduced), transformers are more towards lower Fig. 4. Four attention weights visualized with CT images and model predictions. The first row is from a transformer layer (r = 4, l = 8), and the second row is from another transformer layer at higher resolution (r = 3, l = 10). In each case, the left side is the original image, the middle one is the overlaid display with the attention weights, and the right side is the overlaid display of the attention weights and segmentation masks. resolution scales. It agrees with our expectation since transformers normally consume more GPU memory than convolutions due to several linear operations in large token dimension. On the other hand, more convolutions are chosen than transformers, which implicitly suggests that this balance between convolutions and transformers is better for feature learning in segmentation. Another benefit shown in Fig. 2 is that re-training architectures with lower memory constraints would not hurt the model performance. It is encouraging to see that such combination of convolution and transformer retains the same-level performance with lower GPU memory costs and receptive field of the entire model input. Attention Mechanism. We visualize the attention weights computed by a dedicated self-attention operation in the transformer. The attention weight is with shape 512 × 4096 = 512 × 16 3 . Then we take the average from the channel dimension and resize it to a volume with shape 160 3 by trilinear interpolation (for visualization). We can see that the self-attention weights of the kidney segmentation consistently focus on the lower spine or pelvis areas at different transformer layers as evidence of long-range dependency (Fig. 4). One potential explanation could be that kidneys are located around those areas and both kidneys are on the opposite sides of the spine. So the information over there can help roughly identify the kidneys from the whole-body CT. Especially specific bones are good bio-markers with high intensity values in CT. Furthermore, to the best of our knowledge, it is the first time that the multi-head self-attention is visualized for 3D medical image segmentation."
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,5,Discussion and Conclusion,"In this study, we observe that DAST is able to find effective and concise relationships between convolutions and transformers in a single neural network model. The optimized connections between operations improves the model effectiveness in various applications. Such models benefit from the different inductive biases introduced by these two operations. Adding a memory constraint loss as an additional objective can lower memory consumption for the searched architecture. Transformers will then benefit more from long-range dependencies of larger input patches. We hope this perspective will be helpful for different applications in medical imaging."
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,,Fig. 1 .,
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,,Fig. 2 .,
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,,Fig. 3 .,
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,1,Introduction,"Recent advancements in applying machine learning techniques to MRI-based brain imaging studies have shown substantial progress in predicting neurodegenerative diseases (e.g., Alzheimer's Disease or AD) and clinical phenotypes (e.g., behavior measures), and in uncovering novel biomarkers that are closely related to them [4]. Different MRI techniques can be used to depict different aspects of the brain organization or dynamics [8,19,23]. In general, diffusion MRI can derive brain structural networks that depict the connectivity of white matter tracks among brain regions, which gains system-level insights into the brain structural changes related to brain diseases and those phenotypes [29]. However, the structural networks may not inform us about whether this tract or the regions it connects are ""activated"" or ""not activated"" in a specific state. As a complementary counterpart, the functional MRI provides measures of BOLD (blood-oxygenlevel-dependent) signals to present activities of brain regions over time [3], but no clue on whether those regions are physically connected or not. Therefore, different brain imaging data provide distinct but complementary information, and separately analyzing the data of each modality will always be suboptimal. In this context, multimodal approaches are being explored to improve prediction accuracy by integrating multiple information sources [9,13,27,[31][32][33]. For example, it has been shown that combining different modalities of data (e.g., image and text) can enhance performance in image classification and clustering tasks [27,33]. In the healthcare field, multimodal machine learning has shown its potential in disease detection and diagnosis [13]. In brain imaging studies, many studies aim to explore multimodal MRI data representations by modeling the communications between functional MRI and its structural counterpart. Most of these studies primarily focus on establishing a unidirectional mapping between these two imaging modalities (i.e., mapping from structural MRI data to the functional counterpart [24,32], or the inverse [16,31]). However, for the same biological brain, these two mappings generate distinct results, which highlights the likelihood of bias in the unidirectional mapping approach.To address this, we propose a novel bidirectional mapping framework, where the mapping from structural MRI data (i.e., diffusion MRI-derived brain structural network) to the functional counterpart (i.e., BOLD signals) and the inverse mapping are implemented simultaneously. Unlike previous studies [6,15,22,28,32] that employ unidirectional mappings, our approach leverages bidirectional mapping, minimizing the discrepancies in the latent space of each one-way mapping through contrastive learning at the brain region-of-interest level (ROI level). This method subsequently unveils the inherent unity across both imaging modalities. Moreover, our framework is interpretable, where we employ integrated gradients [20] to generate brain saliency maps for interpreting the outcomes of our model. Specifically, the identified top key brain ROIs in the brain saliency maps are closely related to the predicted diseases and clinical phenotypes. Extensive experiments have been conducted to demonstrate the effectiveness and superiority of our proposed method on two publicly available datasets (i.e., the Human Connectome Project (HCP), and Open Access Series of Imaging Studies (OASIS)). In summary, the contributions of this paper can be outlined as follows:-We propose a novel bidirectional framework to yield multimodal brain MRI representations by modeling the interactions between brain structure and the functional counterpart. -We use contrastive learning to extract the intrinsic unity of both modalities. -The experimental results on two publicly available datasets demonstrate the superiority of our proposed method in predicting neurodegenerative diseases and clinical phenotypes. Furthermore, the interpretability analysis highlights that our method provides biologically meaningful insights. Afterward, ROI-level contrastive learning is applied to these extracted representations, facilitating their alignment in a common space. These derived representations are then utilized for downstream prediction tasks."
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,2,Method,"The proposed bidirectional mapping framework (Fig. 1) comprises two encoderdecoder structures. One constructs BOLD signals from structural networks, while the other performs the inverse mapping. A ROI-level's contrastive learning is utilized between the encoder and decoder to minimize the distinction of the latent spaces within two reconstruction mappings. Finally, a multilayer perceptron (MLP) is utilized for task predictions. It's worth mentioning that instead of using the functional connectivity matrix, we directly utilize BOLD signals for bidirectional mapping. We believe this approach is reasonable as it allows us to capture the dynamic nature of the brain through the BOLD time sequence. Using the functional connectivity matrix may potentially disrupt this dynamic information due to the calculations of correlations. Furthermore, our experiments indicate that our encoder can directly model the temporal relations between different brain regions from the BOLD signals, eliminating the need to construct functional networks."
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,,Preliminaries.,"A structural brain network is an attributed and weighted graph G = (A, H) with N nodes, where H ∈ R N ×d is the node feature matrix, and A ∈ R N ×N is the adjacency matrix where a i,j ∈ R represents the edge weight between node i and node j. Meanwhile, we utilize X B ∈ R N ×T to represent the BOLD signal matrix derived from functional MRI data of each subject, where each brain ROI has a time series BOLD signal with T points.Reconstruction. For the reconstruction task, we deploy an encoder-decoder architecture and utilize the L 1 loss function. Particularly, we use a multi-layer feed-forward neural network as the encoder and decoder. Our method differs from previous studies [21,32], where the encoder and decoder do not necessitate a GNN-based framework, allowing us to directly utilize the adjacency matrix A of structural networks as the inputs. Previous studies randomly initialize the node features (i.e., H) for the GNN input, since it is difficult to find informative brain node features that provide valuable information from the HCP and OASIS datasets. Hence, we propose a reconstruction framework that detours using the node feature matrix. Our framework is bidirectional, where we simultaneously conduct structural network and BOLD signal reconstruction. Here, we have latent representations Z B = Encoder B (X B ) and Z S = Encoder S (A) for BOLD signals and structural networks, respectively."
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,,ROI-Level's Contrastive Representation Learning.,"With latent representation Z B ∈ R N ×dB generated from BOLD signal and Z S ∈ R N ×dS from structural networks, we then conduct ROI-level's contrastive learning to associate the static structural and dynamic functional patterns of multimodal brain measurements. The contrastive learning loss aims to minimize the distinctions between latent representations from two modalities. To this end, we first utilize linear layers to project Z B and Z S to the common space, where we obtainto denote representations from the same ROI, where z B i and z S i are elements of Z B and Z S , respectively. For the same brain ROI, the static structural representation and the dynamic functional counterpart are expected to share a maximum similarity. Conversely, for the pairs that do not match, represented as (z B i , z S j ) i =j , these are drawn from different ROIs and should share a minimum similarity.To formally build up the ROI-level's contrastive loss, it is intuitive to construct positive samples and negative ones based on the match of ROIs. Specifically, we construct (z B i , z S i ) i=1•••N as positive sample pair, and (z B i , z S j ) i =j as negative sample pair. And our contrastive loss can be formulated as follow:where Similarity(•) is substantiated as cosine similarity.Loss Functions. The loss functions within our proposed framework are summarized here. Besides the reconstruction loss (L rec ) and the ROI-level's contrastive loss (L contrast ), we utilize cross-entropy loss (L supervised = L cross-entropy ) for classification tasks, and L 1 loss (L supervised = L mean-absolute-error ) for regression tasks, respectively. In summary, the loss function can be described as:where η 1 , η 2 and η 3 are loss weights. "
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3,Experiments,
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.1,Data Description and Preprocessing,"Two publicly available datasets were used to evaluate our framework. The first includes data from 1206 young healthy subjects (mean age 28.19 ± 7.15, 657 women) from the Human Connectome Project [25] (HCP). The second includes 1326 subjects (mean age = 70.42 ± 8.95, 738 women) from the Open Access Series of Imaging Studies (OASIS) dataset [12]. Details of each dataset may be found on their official websites. CONN [26] and FSL [10] were used to reconstruct the functional and structural networks, respectively. For the HCP data, both networks have a dimension of 82 × 82 based on 82 ROIs defined using FreeSurfer (V6.0) [7]. For the OASIS data, both networks have a dimension of 132 × 132 based on the Harvard-Oxford Atlas and AAL Atlas. We deliberately chose different network resolutions for HCP and OASIS, to evaluate whether the performance of our new framework is affected by the network dimension or atlas. The source code is available at: https://github.com/FlynnYe/BMCL."
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.2,Experimental Setup and Evaluation Metrics,"We randomly split each dataset into 5 disjoint sets for 5-fold cross-validations, and all the results are reported in mean (s.t.d.) across 5 folds. To evaluate the performance of each model, we utilize accuracy, precision score, and F 1 score for classification tasks, and mean absolute error (MAE) for regression tasks. The learning rate is set as 1 × 10 -4 and 1 × 10 -3 for classification and regression tasks, respectively. The loss weights (i.e., η 1 ,η 2 , and η 3 ) are set equally as 1/3. To demonstrate the superiority of our method in cross-modal learning, bidirectional mapping, and ROI-level's contrastive learning, we select four baselines including 2 single-modal graph learning methods (i.e., DIFFPOOL [30] and SAGPOOL [14]), as well as 2 multimodal methods (i.e., VGAE [11] and DSBGM [22]) for all tasks. We use both functional brain networks, in which edge weights are defined as the Pearson Correlation between BOLD signals, and brain structural networks as input for baseline methods. The functional brain networks are signed graphs including positive and negative edge weights, however, the DIFFPOOL, SAGPOOL, and VGAE can only take unsigned graphs (i.e., graphs only include positive edges) as input. Therefore, we convert the functional brain networks to unsigned graphs by using the absolute values of the edge weights."
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.3,BOLD Signal and Structural Network Reconstruction,"We train the model in a task-free manner where no task-specific supervised loss is involved. The MAE values between the edge weights in the ground-truth and reconstructed structural networks are 0.0413 ± 0.0009 and 0.0309 ± 0.0015 under 5-fold cross-validation on the HCP and OASIS, respectively. The MAE values between ground-truth and reconstructed BOLD signals are 0.0049 ± 0.0001 and 0.0734 ± 0.0016 on the HCP and OASIS, respectively. The reconstruction results on HCP are visualized in Fig. 2. "
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.4,Disease and Sex Classification,"We conduct Alzheimer's disease (AD) classification on the OASIS dataset, and sex classification on the HCP dataset. As shown in Table 1, our proposed BMCL can achieve the best results in accuracy, precision, and F 1 score for both tasks among all methods. For example, in the AD classification, our model outperforms the baselines with at least 4.2%, 5.8% and 4.0% increases in accuracy, precision and F 1 scores, respectively. In general, multimodal methods can outperform single-model methods. The superiority of our bidirectional BMCL model, compared to the unidirectional methods, attributes to the fact that our BMCL reduces the distinction between the latent spaces generated by two unidirectional mappings through ROI-level's contrastive learning."
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.5,ASR and MMSE Regression,"Mini-Mental State Exam (MMSE) is a quantitative measure of cognitive status in adults, and Adult Self-Report scale (ASR) [1] is to measure the adult's behavior. As shown in Table 2, our proposed BMCL model outperforms all baselines in terms of MAE values. The regression results also demonstrate the superiority of bidirectional mapping and the importance of ROI-level's contrastive learning, which is consistent with the results in the classification tasks."
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.6,Ablation Study,"To demonstrate the significance of bidirectional mapping, we remove a part of our proposed BMCL model to yield two unidirectional mappings (i.e., either mapping from structural network to BOLD signal, or mapping inversely). As shown in the bottom three rows in Table 1 and Table 2, the prediction results are declined when we remove each directional mapping, which clearly demonstrates the importance of bidirectional mapping.  "
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,3.7,Interpretability,"The 10 key brain regions (Fig. 3) associated with AD (from OASIS) and with each sex (from HCP) are identified using the brain saliency map. The salient regions for AD are concentrated in cerebelum (i.e., cerebelum 3 right and left, cerebelum 8 left, cerebelum crus2 right) and middle Temporal gyrus (i.e., the posterior division left and right, as well as the temporooccipital right of middle temporal gyrus), which have been verified as core AD biomarkers in literature [2,18]. Similarly, 10 key regions (Fig. 3) are identified for regression tasks (i.e., 3 ASR from HCP and MMSE from OASIS). Interestingly, several brain regions (including left and right accumbens areas, cortex left hemisphere cuneus and insula, as well as cortex right hemisphere posteriorcingulate and parahippocampal) are consistently identified across 3 ASR scales (i.e., aggression, rule-break, and intrusive). This finding is supported by [21], which suggests that similar ASR exhibits common or similar biomarkers. Also, these regions have been reported as important biomarkers for aggressive-related behaviors in literature [5,17]."
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,4,Conclusions,"We propose a new multimodal data mining framework, named BMCL, to learn the representation from two modality data through bidirectional mapping between them. The elaborated ROI-level contrastive learning in BMCL can reduce the distinction and eliminate biases between two one-way mappings.Our results on two publicly available datasets show that BMCL outperforms all baselines, which demonstrates the superiority of bidirectional mapping with ROI-level contrastive learning. Beyond these, our model can identify key brain regions highly related to different clinical phenotypes and brain diseases, which demonstrates that our framework is interpretable and the results are biologically meaningful. The contrastive learning method, while emphasizing the alignment of features from different modalities, may inadvertently neglect the unique characteristics inherent to each modality. Moving forward, we intend to refine our method by aiming for a balance between the alignment of modalities and the preservation of modality-specific information. Additionally, the pre-selection of important features or the consideration of subnetworks holds promising for further research."
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,,Fig. 1 .,
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,,Fig. 2 .,
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,,Fig. 3 .,
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,,Table 1 .,
Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data,,Table 2 .,
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,1,Introduction,"Deep learning models have shown high prediction performance on many medical imaging tasks (e.g., [3,15,21,24]). However, deep learning models can indeed make errors, leading to distrust and hesitation by clinicians to integrate them into their workflows. In particular, models that show a tendency for overconfident incorrect predictions present real risk to patient care if deployed in real clinical practice. One way to improve the trustworthiness of a model is to ensure that it is well-calibrated, in that the predicted probabilities of the outcomes align with the probability of making a correct prediction [8]. While several methods have been shown to successfully improve calibration on the overall population [8,16], they cannot guarantee a small calibration error on sub-populations. This can lead to a lack of fairness and equity in the resulting diagnostic decisions for a subset of the population. Figure 1(a) illustrates how a deep learning model can achieve good calibration for the overall population and for younger patients, but produces significantly overconfident and incorrect predictions for older patients. Although various methods have been shown to successfully mitigate biases by improving prediction performance (e.g. accuracy) in the worst-performing subgroup [1,13,18,27,28], improved prediction performance does not necessarily imply better calibration. As such, this paper focuses on the open problem of mitigating calibration bias in medical image analysis. Moreover, our method does not require subgroup attributes during the training, which permits the flexibility to mitigate biases for different choices of sensitive attributes without re-training. This paper proposes a novel two-stage method: Cluster-Focal. In the first stage, a model f id is trained to identify poorly calibrated samples. The samples are then clustered according to their calibration gap. In the next stage, a prediction model f pred is trained via group-wise focal loss. Extensive experiments are performed on (a) skin lesion classification, based on the public HAM10000 dataset [3], and (b) on predicting future new lesional activity for multiple sclerosis (MS) patients on a proprietary, federated dataset of MRI acquired during different clinical trials [2,7,26]. At test time, calibration bias mitigation is examined on subgroups based on sensitive demographic attributes (e.g. age, sex). In addition, we consider subgroups with different image-derived attributes, such as lesion load. We further compare Cluster-Focal with recent debiasing methods that do not need subgroup annotations, such as EIIL (Environment Inference for Invariant Learning) [4], ARL (Adversarially Reweighted Learning) [10], and JTT (Just Train Twice) [14]. Results demonstrate that Cluster-Focal can effectively reduce calibration error in the worst-performing subgroup, while preserving good prediction performance, when split into different subgroups based on a variety of attributes (Fig. 2). "
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,2,Methodology,"We propose a two-stage training strategy, Cluster-Focal. The first stage consists of identifying different levels of poorly calibrated samples. In the second stage, we introduce a group-wise focal loss to mitigate the calibration bias. At test time, our model can mitigate biases for a variety of relevant subgroups of interest.We denote D = {(x i , y i )} N i=1 as a dataset, where x i represents multi-modal medical images and y i ∈ {1, 2, . . . } are the corresponding ground-truth class label. A neural network f produces pi,y = f (y|x i ), the predicted probability for a class y given x i . The predicted class for an x i is defined as ŷi = argmax y pi,y , with the corresponding prediction confidence pi = pi,ŷi ."
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,2.1,Training Procedure: Two-Stage Method,
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Stage 1: Identifying Poorly Calibrated Samples (Clustering).,"In this stage, we first train a model f id via ERM [25], which implies training a model by minimizing the average training cross entropy loss, without any fairness considerations. f id is then used to identify samples that have potentially different calibration properties. Concretely, we compute the gap between prediction confidence pi and correctness via f id :where pi is the confidence score of the predicted class. whereIntuitively, the focal loss penalizes confident predictions with an exponential term (1 -f pred (y i |x i )) γ , thereby reducing the chances of poor calibration [16].Additionally, due to clustering based on gap(x i ), poorly calibrated samples will end up in the same cluster. The number of samples in this cluster will be small compared to other clusters for any model with good overall performance. As such, doing focal loss separately on each cluster instead of on all samples will implicitly increase the weight of poorly calibrated samples and help reduce bias."
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,2.2,Test Time Evaluation on Subgroups of Interest,"At test time, we aim to mitigate the calibration error for the worst-performing subgroup for various subgroups of interest [6]. For example, if we consider sex (M/F) as the sensitive attribute and denote ECE A=M as the expected calibration error (ECE) on male patients, then the worst-performing subgroup ECE is denoted as max(ECE A=F , ECE A=M ). Following the strategy proposed in [17,19], we use Q(uantile)-ECE to estimate the calibration error, an improved estimator for ECE that partitions prediction confidence into discrete bins with an equal number of instances and computes the average difference between each bin's accuracy and confidence.In practice, calibration performance cannot be considered in isolation, as there always exists a shortcut model that can mitigate calibration bias but have poor prediction performance, e.g., consider a purely random (under-confident) prediction with low accuracy. As such, there is an inherent trade-off between calibration bias and prediction error. When measuring the effectiveness of the proposed method, the objective is to ensure that calibration bias is mitigated without a substantial increase in the prediction error. "
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,3,Experiments and Results,"Experiments are performed on two different medical image analysis tasks. We evaluate the performance of the proposed method against popular debiasing methods. We examine whether these methods can mitigate calibration bias without severely sacrificing performance on the worst-performing subgroups.Task 1: Skin lesion multi-class (n = 7) classification. HAM10000 is a public skin lesion classification dataset containing 10,000 photographic 2D images of skin lesions. We utilize a recent MedFair pipeline [27] to pre-process the dataset into train (80%), validation (10%) and test (10%) sets. Based on the dataset and evaluation protocol in [27], we test two demographic subgroups of interest: age (age ≤ 60, age > 60), and sex (male, female)."
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Task 2: Future new multiple sclerosis (MS) lesional activity prediction (binary classification).,"We leverage a large multi-centre, multi-scanner proprietary dataset comprised of MRI scans from 602 RRMS (Relapsing-Remitting MS) patients during clinical trials for new treatments [2,7,26]. The task is to predict the (binary) presence of new or enlarging T2 lesions or Gadoliniumenhancing lesions two years from their current MRI. The dataset was divided as follows: training (70%) and test (30%) sets, validation is conducted through 4-fold cross validation in training set. We test model performance on four different subgroups established in the MS literature [5,11,12,22,23]. This includes: age (age < 50, age ≥ 50), sex (male, female), T2 lesion volume (vol ≤ 2.0 ml, vol > 2.0 ml) and Gad lesion count (count = 0, count > 0). Age and sex are sensitive demographic attributes that are common for subgroup analysis. The image-derived attributes were chosen because high T2 lesion volume, or the presence of Gad-enhancing lesions, in baseline MRI is generally predictive of the appearance of new and enlarging lesions in future images. However, given the heterogeneity of the population with MS, subgroups without these predictive markers can still show future lesional activity. That being said, these patients can form a subgroup with poorer calibration performance."
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Implementation Details:,We adopt 2D/3D ResNet-18 [9] for Task 1 and Task 2 respectively. All models are trained with Adam optimizer. Stage 1 model f id is trained for 10 (Task 1) and 300 (Task 2) epochs and Stage 2 prediction model f pred for 60 (Task 1) and 600 (Task 2) epochs. We set the number of clusters to 4 and γ = 3 in group-wise focal loss. Averaged results across 5 runs are reported.
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Comparisons and Evaluations:,"Macro-F1 is used to measure the performance for Task 1 (7 class), and F1-score is used for Task 2 (binary). Q-ECE [16] is used to measure the calibration performance for both tasks. The performance of the proposed method is compared against several recent bias mitigation methods that do not require training with subgroup annotations: ARL [10], which applies a min-max objective to reweigh poorly performing samples; EIIL [4], which pro- poses an adversarial approach to learn invariant representations, and JTT [14], which up-weights challenging samples. Comparisons are also made against ERM, which trains model without any bias mitigation strategy. For all methods, we evaluate the trade-off between the prediction performance and the reduction in Q-ECE error for the worst-performing subgroups on both datasets."
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,3.1,"Results, Ablations, and Analysis","Results: The resulting performance vs. Q-ECE errors tradeoff plots for worstperforming subgroups are shown in Figs. 3 and4. The proposed method (Cluster-Focal) consistently outperforms the other methods on Q-ECE while having minimal loss in performance, if any. For instance, when testing on sex (male/female) for the MS dataset, (Cluster-Focal) loses around 2% prediction performance relative to (ERM) but has around 8% improvement in calibration error. When testing on sex in the HAM10000 dataset, we only observe a 2% performance degradation with a 4% improvement in Q-ECE.In addition to subgroups based on sensitive demographic attributes, we investigate how the methods perform on subgroups defined on medical image-derived features. In the context of MS, results based on subgroups, lesion load or Gadenhancing lesion count are shown in Fig. 4(c-d). The proposed method performs best, with results that are consistent with demographic based subgroups. For Gad-enhancing lesion count, when compared with JTT, Cluster-Focal improves Q-ECE by 20%+ with a reduction in the prediction performance on the worstperforming subgroup of 2%. Detailed numeric values for the results can be found in the Supplemental Materials."
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Ablation Experiments:,"Further experiments are performed to analyze the different components of our method. The following variant methods are considered: (1) Focal: Removing stage 1 and using regular focal loss for the entire training set; (2) Cluster-ERM: Group-wise focal loss in stage 2 is replaced by standard cross entropy; (3) Cluster-GroupDRO: Group-wise focal loss in stage 2 is replaced by GroupDRO [20]; (4) Oracle-Focal: In stage 1, the identified cluster is replaced by the true subgroups evaluated on at test time (oracle); (5) Oracle-GroupDRO: We use GroupDRO with the true subgroups used at test time. Results for MS, shown in Fig. 5, illustrate that each stage of our proposed model is required to ensure improved calibration while avoiding performance degradation for the worst-performing subgroups.Calibration Curves: Figure 6 shows the reliability diagram for competing methods on Task 2: predicting future new MS lesional activity, with age being the chosen subgroup of interest (also see Fig. 1(a) for ERM results). Results indicate that popular fairness mitigation methods are not able to correct for the calibration bias in older patients (i.e. the worst-performing subgroup). With ARL, for example, most of the predictions were over-confident, resulting in a large calibration error. In contrast, our proposed method (Cluster-Focal) could effectively mitigate the calibration error in the worst-performing subgroup. "
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,4,Conclusions,"In this paper, we present a novel two stage calibration bias mitigation framework (Cluster-Focal) for medical image analysis that (1) successfully controls the trade-off between calibration error and prediction performance, and (2) flexibly overcomes calibration bias at test time without requiring pre-labeled subgroups during training. We further compared our proposed approach against different debiasing methods and under different subgroup splittings such as demographic subgroups and image-derived attributes. Our proposed framework demonstrates smaller calibration error in the worst-performing subgroups without a severe degradation in prediction performance."
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Fig. 1 .,
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Fig. 2 .,
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Fig. 3 .,
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Fig. 4 .,
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Fig. 5 .,
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Fig. 6 .,
Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 19.
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,1,Introduction,"The U-Net [16] is widely used for medical image segmentation, but its results can deteriorate when changing the image acquisition device [7], even when the resulting differences in image characteristics are so subtle that a human would not be confused by them [19]. This is particularly critical when failure is silent [10], i.e., incorrect results are produced with high confidence [11].It has been proposed that anomalous activation patterns within the network, which differ from those that were observed during training, indicate problematic inputs [15]. In the context of medical image segmentation, one such approach was recently shown to provide high accuracy for the detection of images that come from a different source [10].Our work introduces segmentation distortion, a novel and more specific measure of segmentation uncertainty under domain shift. It is motivated by the observation that latent space distances reliably detect images from a different scanner, but do not correlate strongly with segmentation errors within a given domain, as illustrated in Fig. 3. This suggests that not all anomalies have an equal effect on the final output. Our main idea is to better assess their actual effect by making anomalous activations more similar to those that were observed during training, propagating the result through the remainder of the network, and observing how strongly this distorts the segmentation.This yields a novel image-level uncertainty score, which is a better indicator of segmentation errors in out-of-distribution data than activation space distances or mean entropy. At the same time, it can be added to any existing U-Net, since it neither requires modification of its architecture nor its training."
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,2,Related Work,"The core of our method is to modify activation maps so that they become more similar to those that were observed during training, and to observe the effect of this after propagating the result through the remainder of the network. We use autoencoders for this, based on the observation that the difference r(x) -x between the reconstruction r(x) of a regularized autoencoder and its input x points towards regions of high density in the training data [1].This has previously motivated the use of autoencoders for unsupervised anomaly segmentation [2,4,8]. In contrast to these works, the autoencoder in our work acts on activation maps, not on the original image, and the anomalies we are looking for are irregular activation patterns that arise due to the domain shift, not pathological abnormalities in the image.Conditional variational autoencoders have been integrated into the U-Net to quantify uncertainty that arises from ambiguous labels [3,14]. Their architecture and goal differ from ours, since we assume non-ambiguous training data, and aim to quantify uncertainty from domain shifts. Merging their idea with ours to account for both sources of uncertainty remains a topic for future investigation."
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,3,Methodology,
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,3.1,"Autoencoder Architecture, Placement, and Loss","We adapted a U-shaped autoencoder architecture which was successfully used in a recent comparative study [4] to the higher number of channels and lower resolution of activation maps as compared to images. Specifically, our encoder uses two blocks of four 3 × 3 kernels each with stride one, LayerNorm, and a LeakyReLU activation function. At the end of each block, we reduce spatial resolution with a stride of two. After passing through a dense bottleneck, spatial resolution is restored with a mirrored set of convolutional and upsampling layers.Using autoencoders to make activations more similar to those observed in the training data requires regularization [1]. We tried denoising autoencoders, as well as variational autoencoders [13], but they provided slightly worse results than a standard autoencoder in our experiments. We believe that the narrow bottleneck in our architecture provides sufficient regularization by itself.Since we want to use the difference between propagating the reconstruction r(x) instead of x through the remainder of the network as an indicator of segmentation uncertainty due to domain shift, the autoencoder should reconstruct activations from the training set accurately enough so that it has a negligible effect on the segmentation. However, the autoencoder involves spatial subsampling and thus introduces a certain amount of blurring. This proved problematic when applying it to the activations that get passed through the U-Net's skip connections, whose purpose it is to preserve resolution. Therefore, we only place an autoencoder at the lowest resolution level, as indicated in Fig. 1. This agrees with recent work on OOD detection in U-Nets [10].While autoencoders are often trained with an 1 or 2 (MSE) loss, we more reliably met our goal of preserving the segmentation on the training data by introducing a loss that explicitly accounts for it. Specifically, let U (I) denote the logits (class scores before softmax) obtained by applying the U-Net U to an input image I without the involvement of the autoencoder, while U d • r(x) indicates that we apply the U-Net's decoder U d after replacing bottleneck activations x with the reconstruction r(x). We define the segmentation preservation loss asand complement it with the established 2 lossto induce a degree of consistency with the underlying activation space. Since in our experiments, the optimization did not benefit from an additional balancing factor, we aggregate both terms into our training objective"
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,3.2,Segmentation Distortion,"We train the autoencoder so that, on in distribution (ID) images, it has almost no effect on the segmentation. Out of distribution (OOD), reconstructions diverge from the original activation. It is the goal of our segmentation distortion measure to quantify how much this affects the segmentation. Therefore, we define segmentation distortion (SD) by averaging the squared differences of class probabilities P (C p |U ) that are estimated by the U-Net U at pixel p ∈ P with and without the autoencoder, over pixels and classes c ∈ C:SD is defined similarly as the multi-class Brier score [6]. However, while the Brier score measures the agreement between probabilistic predictions and actual outcomes, SD measures the agreement between two predictions, with or without the autoencoder. In either case, a zero score indicates a perfect match."
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,3.3,Implementation Details,"Optimizing the autoencoder with respect to L seg requires gradient flow through the U-Net's decoder. Our implementation makes use of PyTorch's pre-forward hook functionality to compute it while keeping the weights of the U-Net intact. The U-Nets and the corresponding autoencoders were trained on identical training sets, with Adam and default parameters, until the loss converged on a respective validation set. We crop images to uniform shape to accommodate our AEs with fixed-size latent dimension. This facilitated some of our ablations, but is not a requirement of our method itself, and might be avoided by fully convolutional AE architectures in future work. Our AEs were trained on single TITAN X GPUs for approximately three hours and exhausted the 11GB of VRAM through appropriate batching. Our code is publicly available on github. 4 Experiments"
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,4.1,Experimental Setup,"We show results for two segmentation tasks, which are illustrated in Fig. 2. The first one, Calgary-Campinas-359 (CC-359), is brain extraction in head MRI. It uses a publicly available multi-vendor, multi-field strength brain imaging dataset [18], containing T1 weighted MR scans of 359 subjects. Images are from three scanner manufacturers (GE, Philips, Siemens), each with field strengths of 1.5T and 3T. For training and evaluation, we used the brain masks that are provided with the dataset. The second task, ACDC/M&MS, is the segmentation of left and right ventricle cavities, and left ventricle myocardium, in cardiac MRI. Here, we train on data from the Automated Cardiac Diagnosis Challenge (ACDC) that was held at MICCAI 2017 [5]. It contains images from a 1.5T and a 3T Siemens scanner. We test on data from the multi-center, multi-vendor and multi-disease cardiac segmentation (M&MS) challenge [7], which was held at MICCAI 2020. It contains MR scans from four different vendors with scans taken at different field strengths. We again use segmentation masks provided with the data. In addition to the differences between MRI scanners, images in M&MS include pathologies, which makes this dataset much more challenging than CC-359. Datasets for each task are publicly available (download links: CC-359, ACDC, M&MS).For both tasks, we train U-Nets on one of the domains, with an architecture similar to previous work on domain shift in image segmentation [17] (Fig. 1). We use the Adam optimizer with default parameters and a learning rate of 1e-3, until convergence on a held-out validation set from the same domain. Similar to previous work [17], we study the effects of domain shift both with and without data augmentation during training. Specifically, results on the easier CC-359 dataset are without augmentation, while we use the same augmentations as the nnU-Net [12] when training on ACDC. For CC-359, a bottleneck dimension of 64 in our autoencoders was sufficient, while we used 128 for M&MS.Figure 2 shows example segmentations, with errors highlighted in red, and reports the mean Dice scores across the whole dataset below the examples. To average out potential artefacts of individual training runs, we report the standard deviation of mean Dice after repeating the training 10 times. These 10 runs also underly the following results."
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,4.2,Correlation with Segmentation Errors,"The goal of our proposed segmentation distortion (SD) is to identify images in which segmentation errors arise due to a domain shift. To quantify whether this goal has been met, we report rank correlations with (1-Dice), so that positive correlations will indicate a successful detection of errors. The use of rank correlation eliminates effects from any monotonic normalization or re-calibration of our uncertainty score.Figure 3 compares segmentation distortion to a recently proposed distancebased method for uncertainty quantification under domain shift [10]. It is based on the same activations that are fed into our autoencoder, but pools them into low-dimensional vectors and computes the Mahalanobis distance with respect to the training distribution to quantify divergence from in-distribution activations. We label it Pooling Mahalanobis (PM). To better understand the difference between that approach and ours, we also introduce the Latent Mahalanobis (LM) method that is in between the two: It also uses the Mahalanobis distance, but computes it in the latent space (on the bottleneck vectors) of our autoencoder.On both segmentation tasks, SD correlates much more strongly with segmentation errors than PM. The correlation of LM is usually in between the two, indicating that the benefit from our method is not just due to replacing the simpler pooling strategy with an autoencoder, but that passing its reconstruction through the remainder of the U-Net is crucial for our method's effectiveness.As another widely used uncertainty measure, Fig. 3 includes the entropy in the model output. We compute it based on the per-pixel class distributions, and average the result to obtain a per-image uncertainty score. We evaluate the entropy for single U-Nets, as well as for ensembles of five. In almost all cases, SD showed a stronger correlation with segmentation error than both entropy based approaches, which do not specifically account for domain shift.We note that ensembling affects not just the uncertainty estimates, but also the underlying segmentations, which are now obtained by averaging over all ensemble members. This leads to slight increases in Dice, and makes the results from ensembling less directly comparable to the others.We also investigated the effects of our autoencoder on downstream segmentation accuracy in out-of-distribution data, but found that it led to a slight reduction in Dice. Therefore, we keep the segmentation masks from the unmodified U-Net, and only use the autoencoder for uncertainty quantification."
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,4.3,Out-of-Distribution Detection,"The distance-based method PM was initially introduced for out-of-distribution (OOD) detection, i.e., detecting whether a given image has been taken with the same device as the images that were used for training [9]. To put the weak correlation with segmentation errors that was observed in Fig. 3 into perspective, we will demonstrate that, compared to the above-described alternatives, it is highly successful at this task.For this purpose, we report the AUROC for the five uncertainty scores based on their classification of images as whether they were drawn from a target domain or an in-domain validation set. As before, we evaluate each target domain separately for all independently trained U-Nets. For the M&MS dataset, results are displayed in Fig. 4 (left). Since all methods achieved near-perfect AUROC on CC-359, those results are not presented as a figure.This experiment confirms the excellent results for OOD detection that were reported previously for the PM method [9]. In contrast, our SD has not been designed for OOD detection, and is not as effective for that task. Similarly, mean entropy in the segmentation map is not a reliable indicator for OOD inputs.Of course, a method that successfully solves OOD detection can be used to reject OOD inputs, and thereby avoid silent failures that arise due to domain shifts. However, it can be seen from Fig. 4 (right) that this comes at the cost of filtering out many images that would be segmented sufficiently well. This figure shows the distributions of Dice scores on all domains. It illustrates that, even though scanner changes go along with an increased risk for inaccurate segmentation, many images from other scanners are still segmented as well as those from the one that was used for training. Note that results for Siemens (ACDC) are from a separate validation subset, but from the same scanner as the training data. Siemens (M&MS) is a different scanner.It is a known limitation of the PM method, which our Segmentation Distortion seeks to overcome, that ""many OOD cases for which the model did produce adequate segmentation were deemed highly uncertain"" [10]."
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,5,Discussion and Conclusion,"In this work, we introduced Segmentation Distortion as a novel approach for the quantification of segmentation uncertainty under domain shift. It is based on using an autoencoder to modify activations in a U-Net so that they become more similar to activations observed during training, and quantifying the effect of this on the final segmentation result.Experiments on two different datasets, which we re-ran multiple times to assess the variability in our results, confirm that our method more specifically detects erroneous segmentations than anomaly scores that are based on latent space distances [10,15]. They also indicate a benefit compared to mean entropy, which does not explicitly account for domain shift. This was achieved on pretrained U-Nets, without constraining their architecture or having to interfere with their training, and held whether or not data augmentation had been used.Finally, we observed that different techniques for uncertainty quantification under domain shift have different strengths, and we argue that they map to different use cases. If safety is a primary concern, reliable OOD detection should provide the strongest protection against the risk of silent failure, at the cost of excluding inputs that would be adequately processed. On the other hand, a stronger correlation with segmentation errors, as it is afforded by our approach, could be helpful to prioritize cases for proofreading, or to select cases that should be annotated to prepare training data for supervised domain adaptation."
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,,Fig. 1 .,
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,,Fig. 2 .,
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,,Fig. 3 .,
Segmentation Distortion: Quantifying Segmentation Uncertainty Under Domain Shift via the Effects of Anomalous Activations,,Fig. 4 .,
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,1,Introduction,"The success of deep neural networks heavily relies on the availability of large and diverse annotated datasets across a range of computer vision tasks. To learn a strong data representation for robust and performant medical image segmentation, huge datasets with either many thousands of annotated data structures or less specific self-supervised pretraining objectives with unlabeled data are needed [29,33]. The annotation of 3D medical images is a difficult and laborious task. Thus, depending on the task, only a bare minimum of images and target structures is usually annotated. This results in a situation where a zoo of partially labeled datasets is available to the community. Recent efforts have resulted in a large dataset of >1000 CT images with >100 annotated classes each, thus providing more than 100,000 manual annotations which can be used for pre-training [30]. Focusing on such a dataset prevents leveraging the potentially precious additional information of the above mentioned other datasets that are only partially annotated. Integrating information across different datasets potentially yields a higher variety in image acquisition protocols, more anatomical target structures or details about them as well as information on different kinds of pathologies. Consequently, recent advances in the field allowed utilizing partially labeled datasets to train one integrated model [21]. Early approaches handled annotations that are present in one dataset but missing in another by considering them as background [5,27] and penalizing overlapping predictions by taking advantage of the fact that organs are mutually exclusive [7,28]. Some other methods only predicted one structure of interest for each forward pass by incorporating the class information at different stages of the network [4,22,31]. Chen et al. trained one network with a shared encoder and separate decoders for each dataset to generate a generalized encoder for transfer learning [2]. However, most approaches are primarily geared towards multi-organ segmentation as they do not support overlapping target structures, like vessels or cancer classes within an organ [6,8,12,23]. So far, all previous methods do not convincingly leverage cross-dataset synergies. As Liu et al. pointed out, one common caveat is that many methods force the resulting model to average between distinct annotation protocol characteristics [22] by combining labels from different datasets for the same target structure (visualized in Fig. 1 b)). Hence, they all fail to reach segmentation performance on par with cutting-edge single dataset segmentation methods. To this end, we introduce MultiTalent (MULTI daTAset LEarNing and pre-Training), a new, flexible, multi-dataset training method: 1) MultiTalent can handle classes that are absent in one dataset but annotated in another during training. 2) It retains different annotation protocol characteristics for the same target structure and 3) allows for overlapping target structures with different level of detail such as liver, liver vessel and liver tumor. Overall, Mul-tiTalent can include all kinds of new datasets irrespective of their annotated target structures.MultiTalent can be used in two scenarios: First, in a combined multi-dataset (MD) training to generate one foundation segmentation model that is able to predict all classes that are present in any of the utilized partially annotated datasets, and second, for pre-training to leverage the learned representation of this foundation model for a new task. In experiments with a large collection of abdominal CT datasets, the proposed model outperformed state-of-the-art segmentation networks that were trained on each dataset individually as well as all previous methods that incorporated multiple datasets for training. Interestingly, the benefits of MultiTalent are particularly notable for more difficult classes and pathologies. In comparison to an ensemble of single dataset solutions, MultiTalent comes with shorter training and inference times. Additionally, at the example of three challenging datasets, we demonstrate that fine-tuning MultiTalent yields higher segmentation performance than training from scratch or initializing the model parameters using unsupervised pretraining strategies [29,33]. It also surpasses supervised pretrained and fine-tuned state-of-the art models on most tasks, despite requiring orders of magnitude less annotations during pre-training."
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,2,Methods,"We introduce MultiTalent, a multi dataset learning and pre-training method, to train a foundation medical image segmentation model. It comes with a novel dataset and class adaptive loss function. The proposed network architecture enables the preservation of all label properties, learning overlapping classes and the simultaneous prediction of all classes. Furthermore, we introduce a training schedule and dataset preprocessing which balances varying dataset size and class characteristics."
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,2.1,Problem Definition,"We begin with a dataset collection of K datasets, where C (k) ⊆ C is the label set associated to dataset D (k) . Even if classes from different datasets refer to the same target structure we consider them as unique, since the exact annotation protocols and labeling characteristics of the annotations are unknown and can vary between datasets: C (k) ∩ C (j) = ∅, ∀k = j. This implies that the network must be capable of predicting multiple classes for one voxel to account for the inconsistent class definitions."
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,2.2,MultiTalent,"Network Modifications. We employ three different network architectures, which are further described below, to demonstrate that our approach is applicable to any network topology. To solve the label contradiction problem we decouple the segmentation outputs for each class by applying a Sigmoid activation function instead of the commonly used Softmax activation function across the dataset. The network shares the same backbone parameters Θ but it has independent segmentation head parameters Θ c for each class. The Sigmoid probabilities for each class are defined as ŷc = f (x, Θ, Θ c ). This modification allows the network to assign multiple classes to one pixel and thus enables overlapping classes and the conservation of all label properties from each dataset. Consequently, the segmentation of each class can be thought of as a binary segmentation task. While the regular dice loss is calculated for each image within a batch, we calculate the dice loss jointly for all images of the input batch. This regularizes the loss if only a few voxels of one class are present in one image and a larger area is present in another image of the same batch. Thus, an inaccurate prediction of a few pixels in the first image has a limited effect on the loss. In the following, we unite the sum over the image voxels i and the batch b to z . We modify the loss function to be calculated only for classes that were annotated in the corresponding partially labeled dataset [5,27], in the following indicated by 1and 0 otherwise. Instead of averaging, we add up the loss over the classes. Hence, the loss signal for each class prediction does not depend on the number of other classes within the batch. This compensates for the varying number of annotated classes in each dataset. Otherwise, the magnitude of the loss e.g. for the liver head from D1 (2 classes) would be much larger as for D7 (13 classes). Gradient clipping captures any potential instability that might arise from a higher loss magnitude:Network Architectures. To demonstrate the general applicability of this approach, we applied it to three segmentation networks. We employed a 3D U-Net [24], an extension with additional residual blocks in the encoder (Resenc U-Net), that demonstrated highly competitive results in previous medical image segmentation challenges [14,15] and a recently proposed transformer based architecture (SwinUNETR [29]). We implemented our approach in the nnU-Net framework [13]. However, the automatic pipeline configuration from nnU-net was not used in favor of a manually defined configuration that aims to reflect the peculiarities of each of the datasets, irrespective of the number of training cases they contain. We manually selected a patch size of [96,192,192] and image spacing of 1mm in plane and 1.5mm for the axial slice thickness, which nnU-Net used to automatically create the two CNN network topologies. For the SwinUNETR, we adopted the default network topology.Multi-dataset Training Setup. We trained MultiTalent with 13 public abdominal CT datasets with a total of 1477 3D images, including 47 classes (Multi-dataset (MD) collection) [1,3,9,11,[18][19][20]25,26]. Detailed information about the datasets, can be found in the appendix in Table 3 and Fig. 3, including the corresponding annotated classes. We increased the batch size to 4 and the number of training epochs to 2000 to account for the high number of training images. To compensate for the varying number of training images in each dataset, we choose a sampling probability per case that is inversely proportional to √ n, where n is the number of training cases in the corresponding source dataset. Apart from that, we have adopted all established design choices from nnU-Net to ensure reproducibility and comparability.Transfer Learning Setup. We used the BTCV (small multi organ dataset [19]), AMOS (large multi organ dataset [16]) and KiTS19 (pathology dataset [11]) datasets to evaluate the generalizability of the MultiTalent features in a pre-training and fine tuning setting. Naturally, the target datasets were excluded from the respective pre-training. Fine tuning was performed with identical configuration as the source training, except for the batch size which was set to 2. We followed the fine-tuning schedule proposed by Kumar et al. [17]. First, the segmentation heads were warmed up over 10 epochs with linearly increasing learning rate, followed by a whole-network warm-up over 50 epochs. Finally, we continued with the standard nnU-Net training schedule."
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,2.3,Baselines,"As a baseline for the MultiTalent, we applied the 3D U-Net generated by the nnU-Net without manual intervention to each dataset individually. Furthermore, we trained a 3D U-Net, a Resenc U-Net and a SwinUNETR with the same network topology, patch and batch size as our MultiTalent for each dataset. All baseline networks were also implemented within the nnU-Net framework and follow the default training procedure. Additionally, we compare MultiTalent with related work on the public BTCV leaderboard in Table 1. Furthermore, the utility of features generated by MultiTalent is compared to supervised and unsupervised pre-training baselines. As supervised baseline, we used the weights resulting from training the three model architectures on the TotalSegmentator dataset, which consists of 1204 images and 104 classes [30], resulting in more than 10 5 annotated target structures. In contrast, Mul-tiTalent is only trained with about 3600 annotations. We used the same patch size, image spacing, batch size and number of epochs as for the MultiTalent training. As unsupervised baseline for the CNNs, we pre-trained the networks on the Multi-dataset collection based on the work of Zhou et al. (Model Genesis [33]). Finally, for the SwinUNETR architecture, we compared the utility of the weights from our MultiTalent with the ones provided by Tan et al. who performed self-supervised pre-training on 5050 CT images. This necessitated the use of the original (org.) implementation of SwinUNETR because the recommended settings for fine tuning were used. This should serve as additional external validation of our model. To ensure fair comparability, we did not scale up any models. Despite using gradient checkpointing, the SwinUNETR models requires roughly 30 GB of GPU memory, compared to less than 17 GB for the CNNs."
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,3,Results,"Multi-dataset training results are presented in Fig. 2. In general, the convolutional architectures clearly outperform the transformer-inspired SwinUNETR. MultiTalent improves the performance of the purely convolutional architectures (U-Net and Resenc U-Net) and outperforms the corresponding baseline models that were trained on each dataset individually. Since a simple average over all classes would introduce a biased perception due to the highly varying numbers of images and classes, we additionally report an average over all datasets. For example, dataset 7 consists of only 30 training images but has 13 classes, whereas   4 in the appendix provides all results for all classes. Averaged over all datasets, the MultiTalent gains 1.26 Dice points for the Resenc U-Net architecture and 1.05 Dice points for the U-Net architecture. Compared to the default nnU-Net, configured without manual intervention for each dataset, the improvements are 1.56 and 0.84 Dice points. Additionally, in Fig. 2 we analyzed two subgroups of classes. The first group includes all ""difficult"" classes for which the default nnU-Net has a Dice smaller than 75 (labeled by a ""d"" in Table 4 in the appendix). The second group includes all cancer classes because of their clinical relevance. Both class groups, but especially the cancer classes, experience notable performance improvements from MultiTalent. For the official BTCV test set in Table 1, MultiTalent outperforms all related work that have also incorporated multiple datasets during training, proving that MultiTalent is substantially superior to related approaches. The advantages of MultiTalent include not only better segmentation results, but also considerable time savings for training and inference due to the simultaneous prediction of all classes. The training is 6.5 times faster and the inference is around 13 times faster than an ensemble of models trained on 13 datasets.Transfer learning results are found in Table 2, which compares the finetuned 5-fold cross-validation results of different pre-training strategies for three different models on three datasets. The MultiTalent pre-training is highly beneficial for the convolutional models and outperforms all unsupervised baselines. Although MultiTalent was trained with a substantially lower amount of manually annotated structures ( ˜3600 vs. ˜10 5 annotations), it also exceeds the supervised pre-training baseline. Especially for the small multi-organ dataset, which only has 30 training images (BTCV), and for the kidney tumor (KiTs19), the Multi-Talent pre-training boosts the segmentation results. In general, the results show that supervised pre-training can be beneficial for the SwinUNETR as well, but pre-training on the large TotalSegmentator dataset works better than the MD pre-training. For the AMOS dataset, no pre-training scheme has a substantial impact on the performance. We suspect that it is a result of the dataset being saturated due to its large number of training cases. The Resenc U-Net pretrained with MultiTalent, sets a new state-of-the-art on the BTCV leaderboard1 (Table 1).  allows including any publicly available datasets (e.g. AMOS and TotalSegmentator). This paves the way towards holistic whole body segmentation model that is even capable of handling pathologies."
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,,Fig. 1 .,
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,,,
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,,Fig. 2 .,
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,,dataset 6,
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,,Table 1 .,
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,,Table 2 .,
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_62.
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,1,Introduction,"Supervised learning of deep neural networks is susceptible to overfitting when labelled training datasets are small, as is often the case in medical image analysis. Data augmentation (DA) tackles this issue by transforming (informed by knowledge of task-specific invariances and equivariances) labelled input-output pairs, thus simulating new input-output pairs to expand the training dataset. This idea is used in semi-supervised learning [6,15] via an unsupervised loss function that promotes the desired invariance and equivariance properties in predictions for unlabelled images. We refer to this as consistency regularization (CR).While previous work has employed CR to leverage unlabelled images, we show that even in the absence of any additional unlabelled images, CR improves calibration [9], and sometimes, even segmentation accuracy of neural networks over those trained with DA. This is surprising at first sight. Compared to DA, when employed in the supervised setting, CR does not have access to additional data. What are then the causes of this benefit?To answer this question, we note that boundaries between anatomical regions are often ambiguous in medical images due to absence of sufficient contrast or presence of image noise or partial volume effects. Annotations in labelled segmentation datasets, however, typically comprise of hard class assignments for each pixel, devoid of information regarding such ambiguity. Supervised learning approaches then insist on perfect agreement at every pixel between predictions and ground truth labels, which can be achieved by over-parameterized neural networks. For instance, using the cross-entropy loss function for training maximizes logit differences between the ground truth class and other classes for each pixel [22]. This bias for low-entropy predictions caused by supervised learning loss functions coupled with inherent ambiguity in the true underlying labels leads to over-confident predictions and miscalibrated models.This viewpoint suggests that reduced logit differences across classes for pixels with ambiguous labels may help counter such miscalibration. Based on this idea, we make two main contributions in this paper. First, we show that CR can automatically discover such pixels and prevent overfitting to their noisy labels. In doing so, CR induces a spatially varying pixel-wise regularization effect, leading to improved calibration. In contrast to previous use of CR in medical image segmentation [6,15], these new benefits are independent of additional unlabelled images. Second, based on this understanding of the mechanism underlying the calibration benefits of CR, we propose a spatially-varying weighing strategy for the CR loss relative to the supervised loss. This strategy emphasizes regularization in pixels near tissue boundaries, as these pixels are more likely to suffer from label ambiguity. We illustrate the calibration benefits of our approach on segmentation tasks in prostate and heart MRI."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,2,Related Work,"Label ambiguity in medical image segmentation is tackled either by generating multiple plausible segmentations for each image [3,13], or by predicting a single well-calibrated segmentation [10,12,14,18,22]. In the latter group, predictions of multiple models are averaged to produce the final segmentation [10,18]. Alternatively, the training loss of a single model is modified to prevent low-entropy predictions at all pixels [22], at pixels with high errors [14] or pixels near boundaries [12,21]. Smoothing ground truth labels of boundary pixels [12] disregards image intensities that cause label ambiguity. In contrast, the boundary-weighted variant of our approach emphasizes regularization in those regions but allows consistency across stochastic transformations to differentiate sub-regions with varying label ambiguity. Related, boundary-weighted supervised losses have been proposed in different contexts [1,11]. Aleatoric uncertainty estimation in medical images [19,29] is closely related to the problem of pixel-wise label ambiguity due to uncertainty in the underlying image intensities. In particular, employing stochastic transformations during inference has been shown to produce estimates of aleatoric uncertainty [29], while we use them during training to automatically identify regions with ambiguous labels and prevent low-entropy segmentation predictions in such regions. In semi-supervised medical image segmentation, CR is widely used as a means to leverage unlabelled images to improve segmentation accuracy [6,7,15,17,30]. In contrast, we investigate the capability of CR to improve calibration without using any unlabelled images. Finally, for image classification, CR can help mitigating label noise [8] and label smoothing has been shown to improve calibration [20]. To our knowledge, this paper is the first to investigate the role of CR as a means to improve calibration of segmentation models."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,3,Methods,"Using a labelled dataset {(X i , Y i )}, i = 1, 2, . . . n, we wish to learn a function that maps images X ∈ R H×W to segmentation labelmaps Y ∈ {1, 2, ..., C} H×W , where C is the number of classes. Let f θ be a convolutional neural network that predicts Ŷ = σ(f θ (X)), where f θ (X) ∈ R H×W ×C are logits and σ is the softmax function. In supervised learning, optimal parameter values are obtained by minimizing an appropriate supervised loss, θ = argmin θ E X,Y L s (σ(f θ (X)), Y ). Data augmentation (DA) leverages knowledge that the segmentation function is invariant to intensity transformations S φ (e.g., contrast and brightness modifications, blurring, sharpening, Gaussian noise addition) and equivariant to geometric transformations T ψ (e.g., affine and elastic deformations)."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,The optimization becomes θ = argmin,"In order to achieve equivariance with respect to T ψ , the loss is computed after applying the inverse transformation to the logits. Consistency regularization (CR) additionally constrains the logits predicted for similar images to be similar. This is achieved by minimizing a consistency loss L c between logits predicted for two transformed versions of the same image: θThe exact strategy for choosing arguments of L c can vary: as above, we use predictions of the same network θ for different transformations (φ, ψ) and (φ , ψ ) [6]; alternatives include setting φ = φ, ψ = ψ, and using two variants of the model θ and θ [26,27] or different combinations of these approaches [7,15]."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,3.1,Consistency Regularization at Pixel-Level,"Here, we show how understanding the relative behaviours of the supervised and unsupervised losses used in CR help to improve calibration. Common choices for L s and L c are pixel-wise cross-entropy loss and pixel-wise sum-of-squares loss, respectively. For these choices, the total loss for pixel j can be written as follows:where z j and z j are C-dimensional logit vectors at pixel j in g(X; θ, φ, ψ) and g(X; θ, φ , ψ ) respectively, and the subscript c indexes classes. L s drives the predicted probability of the ground truth label class to 1, and those of all other classes to 0. Such low-entropy predictions are preferred by the loss function even for pixels whose predictions should be ambiguous due to insufficient image contrast, partial volume effect or annotator mistakes.Consistency loss L c encourages solutions with consistent logit predictions across stochastic transformations. This includes, but is not restricted to, the low-entropy solution preferred by L s . In fact, it turns out that due to the chosen formulation of L s in the probability space and L c in the logit space, deviations  from logit consistency are penalized more strongly than deviations from lowentropy predictions. Thus, L c permits high confidence predictions only for pixels where logit consistency across stochastic transformations can be achieved.Furthermore, variability in predictions across stochastic transformations has been shown to be indicative of aleatoric image uncertainty [29]. This suggests that inconsistencies in logit predictions are likely to occur at pixels with high label ambiguity, causing high values of L c and preventing high confidence predictions at pixels with latent ambiguity in labels."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Special Case of Binary Segmentation:,"To illustrate the pixel-wise regularization effect more clearly, let us consider binary segmentation. Here, we can fix z 1 = 0 and let z 2 = z, as only logit differences matter in the softmax function. Further, let us consider only one pixel, drop the pixel index and assume that its ground truth label is c = 2. Thus, y 1 = 0 and y 2 = 1. With these simplifications, L s =log(σ(z)) and L c = (zz ) 2 . Figure 1 shows that L s favours high z values, regardless of z , while L c prefers the z = z line, and heavily penalizes deviations from it. The behaviour of these losses is similar for multi-label segmentation."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,3.2,Spatially Varying Weight for Consistency Regularization,"Understanding consistency regularization as mitigation against overfitting to hard labels in ambiguous pixels points to a straightforward improvement of the method. Specifically, the regularization term in the overall loss should be weighed higher when higher pixel ambiguity, and thus, higher label noise, is expected. Natural candidates for higher ambiguity are pixels near label boundaries. Accordingly, we propose boundary-weighted consistency regularization (BWCR):where r j is the distance to the closest boundary from pixel j, λ(r j ) drops away from the label boundaries, and R is the width of the boundary region affected by the regularization. We compute r j = argmin c r j c , where r j c is the absolute value of the euclidean distance transform [24] at pixel j of the binarized segmentation for foreground label c. Figure 2 shows examples of r j and λ(r j ) maps."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,4,Experiments and Results,
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Datasets:,"We investigate the effect of CR on two public datasets. The NCI [5] dataset includes T2-weighted prostate MRI scans of N = 70 subjects (30 acquired with a 3T scanner and a surface coil, and 40 acquired with a 1.5T scanner and an endo-rectal coil). In-plane resolution is 0.4 -0.75 mm 2 , throughplane resolution is 3 -4mm. Expert annotations are available for central gland (CG) and peripheral zone (PZ). The ACDC [4] dataset consists of cardiac cine MRI scans of N = 150 subjects (evenly distributed over 4 pathological types and healthy subjects, and acquired using 1.5T and 3T scanners). In-plane resolution is 1.37 -1.68 mm 2 , through-plane resolution is 5 -10 mm. Expert annotations are provided for right ventricle (RV), left ventricle (LV) and myocardium (MY). Two 3D volumes that capture the end-systolic and end-diastolic stages of the cine acquisition respectively are annotated for each subject.Data Splits: From the N subjects in each dataset, we select N ts test, N vl validation and N tr training subjects. {N ts , N vl } are set to {30, 4} for NCI and {50, 5} for ACDC. We have 3 settings for N tr : small, medium and large, with N tr as 6, 12 and 36 for NCI, and 5, 10 and 95 for ACDC, in the three settings, respectively. All experiments are run thrice, with test subjects fixed across runs, and training and validation subjects randomly sampled from remaining subjects. In each dataset, subjects in all subsets are evenly distributed over different scanners."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Pre-processing:,"We correct bias fields using the N4 [28] algorithm, linearly rescale intensities of each image volume using its 2nd and 98th intensity percentile, followed by clipping at 0.0 and 1.0, resample (linearly for images and with nearest-neighbours for labels) NCI and ACDC volumes to 0.625 mm 2 and 1.33 mm 2 in-plane resolution, while leaving the through-plane resolution unchanged, and crop or pad with zeros to set the in-plane size to 192 x 192 pixels.Training Details: We use a 2D U-net [25] architecture for f θ , and use crossentropy loss as L s and squared difference between logits as L c . For S φ , we employ gamma transformations, linear intensity scaling and shifts, blurring, sharpening and additive Gaussian noise. For T ψ , we use affine transformations. For both, we use the same parameter ranges as in [31]. For every 2D image in a batch, we apply each transformation with probability 0.5. We set the batch size to 16, train for 50000 iterations with Adam optimizer, and linearly decay the learning rate from 10 -4 to 10 -7 . After the training is completed, we set θ to its exponential moving average at the iteration with the best validation Dice score [2].Evaluation Criteria: We evaluate segmentation accuracy using Dice similarity coefficient and calibration using Expected Calibration Error (ECE) [9] and Thresholded Adaptive Calibration Error (TACE) [23] (computed using 15 bins and threshold of 0.01). ECE measures the average difference of accuracy and mean confidence of binned predicted probabilities, while TACE employs an adaptive binning scheme such that all bins contain an equal number of predictions."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,4.1,Effect of CR,"First, we check if CR improves calibration of segmentation models. We perform this experiment in the small training dataset setting, and present results in Table 1. It can be seen that as λ (Eq. 1) increases from 0.01 to 1.0, CR improves calibration in both datasets while retaining similar segmentation accuracy to DA (λ = 0.0). These results validate the discussion presented in Sect. 3.1. However, increasing λ to 10.0 leads to accuracy degradation. This motivates us to propose the boundary-weighted extension to CR in order to further improve calibration while preserving or improving segmentation accuracy.Table 1. Effect of CR (λ > 0) and DA (λ = 0) on segmentation accuracy and calibration. Results are reported as % average ± % standard deviation values of over test volumes and three experiment runs. For brevity, TACE values are scaled by 10. Increasing λ from 0.01 to 1.0 improves calibration, but further increasing λ leads to degradation in segmentation accuracy."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,4.2,Effect of BWCR,"We compare CR and BWCR with the following baseline methods: (1) supervised learning without DA (Baseline), (2) data augmentation (DA) [31], (3) spatially varying label smoothing (SVLS) [12] and (4) margin-based label smoothing (MLS) [16,22]. For CR, we set λ = 1.0. For BWCR, we set λ min = 0.01, λ max = 1.0 and R = 10 pixels. These values were set heuristically; performance may be further improved by tuning them using a validation set. For SVLS and MLS, we use the recommended hyper-parameters, setting the size of the blurring kernel to 3 × 3 and its standard deviation to 1.0 in SVLS, and margin to 10.0 and regularization term weight to 0.1 in MLS. To understand the behaviour of these methods under different training dataset sizes, we carry out these comparisons in the small, medium and large settings explained above. The following observations can be made from Table 2: 1. As training data increases, both accuracy and calibration of the supervised learning baseline improve. Along this axis, reduced segmentation errors improve calibration metrics despite low-entropy predictions. 2. Similar trends exist for DA along the data axis. For fixed training set size, DA improves both accuracy and calibration due to the same reasoning as above. This indicates that strong DA should be used as a baseline method when developing new calibration methods. 3. Among the calibration methods, CR provides better calibration than SVLS and MLS. BWCR improves calibration even further. For all except the ACDC large training size setting, BWCR's improvements in ECE and TACE over all other methods are statistically significant (p < 0.001) according to paired permutation tests. Further, while CR causes slight accuracy degradation compared to DA, BWCR improves or retains accuracy in most cases. 4. Subject-wise calibration errors (Fig. 3) show that improvements in calibration statistics stem from consistent improvements across all subjects. 5. Figure 4 shows that predictions of CR and BWCR are less confident around boundaries. BWCR also shows different uncertainty in pixels with similar distance to object boundaries but different levels of image uncertainty.   6. Figure 4 also reveals an intriguing side-effect of the proposed method: CR, and to a lesser extent BWCR, exhibit confidence leakage along object boundaries of other foreground classes. For instance, in row 1 (3), CR assigns probability mass along PZ (MY) edges in the CG (RV) probability map. We defer analysis of this behaviour to future work. 7. While CR and BWCR effectively prevent over-fitting to hard ground truth labels in ambiguous pixels, they fail (in most cases) to improve segmentation accuracy as compared to DA. 8. In the large training set experiments for ACDC, CR and BWCR exhibit worse calibration than other methods. The segmentation accuracy is very high for all methods, but CR and BWCR still provide soft probabilities near boundaries thus causing poorer calibration."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,5,Conclusion,"We developed a method for improving calibration of segmentation neural networks by noting that consistency regularization mitigates overfitting to ambiguous labels, and building on this understanding to emphasize this regularization in pixels most likely to face label noise. Future work can extend this approach for lesion segmentation and/or 3D models, explore the effect of other consistency loss functions (e.g. cosine similarity or Jensen-Shannon divergence), develop other strategies to identify pixels that are more prone to ambiguity, or study the behaviour of improved calibration on out-of-distribution samples."
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Fig. 1 .,
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Fig. 2 .,
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Fig. 3 .,
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Fig. 4 .,
Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks,,Table 2 .,"results reported as % average ± % standard deviation over test volumes and 3 experiment runs. For brevity, TACE values are scaled by 10. The best values in each column are highlighted, with the winner for tied averages decided by lower standard deviations. Paired permutation tests (n = 10000) show that ECE and TACE improvements of BWCR over all other methods are statistically significant with p < 0.001, for all except the ACDC large training size setting. Dice ↑ ECE ↓ TACE ↓ Dice ↑ ECE ↓ TACE ↓ Dice ↑ ECE ↓ TACE ↓ Dice ↑ ECE ↓ TACE ↓ Dice ↑ ECE ↓ TACE ↓ Dice ↑ ECE ↓ TACE ↓"
Co-assistant Networks for Label Correction,1,Introduction,"The success of deep neural networks (DNNs) mainly depends on the large number of samples and the high-quality labels. However, either of them is very difficult to be obtained for conducting medical image analysis with DNNs. In particular, obtaining high-quality labels needs professional experience so that corrupted labels can often be found in medical datasets, which can seriously degrade the effectiveness of medical image analysis. Moreover, sample annotation needs expensive cost. Hence, correcting corrupted labels might be one of effective solutions to solve the issues of high-quality labels.Numerous works have been proposed to tackle the issue of corrupted labels. Based on whether correcting corrupted labels, previous methods can be roughly divided into two categories, i.e., robustness-based methods [12,17] and label correction methods [14,22]. Robustness-based methods are designed to utilize various techniques, such as dropout, augmentation and loss regularization, to avoid the adverse impact of corrupted labels, thereby outputting a robust model. Label correction methods are proposed to first detect corrupted labels and then correct them. For example, co-correction [9] simultaneously trains two models and corrects labels for medical image analysis, and LCC [5] first regards the outputs of DNN as the class probability of the training samples and then changes the labels of samples with low class probability. Label correction methods are significant for disease diagnosis, because physicians can double check the probably mislabeled samples to improve diagnosis accuracy. However, current label correction methods still have limitations to be addressed. First, they cannot detect and correct all corrupted labels, and meanwhile they usually fail to consider boosting the robustness of the model itself, so that the effectiveness of DNNs is possibly degraded. Second, existing label correction methods often ignore to take into account the relationship among the samples so that influencing the effectiveness of label correction.To address the aforementioned issues, in this paper, we propose a new co-assistant framework, namely Co-assistant Networks for Label Correction (CNLC) (shown in Fig. 1), which consists of two modules, i.e., noise detector and noise cleaner. Specifically, the noise detector first adopts a convolutional neural network (CNN [6,20]) to predict the class probability of samples, and then the loss is used to partition all the training samples for each class into three subgroups, i.e., clean samples, uncertain samples and corrupted samples. Moreover, we design a robust loss (i.e., a resistance loss) into the CNN framework to avoid model overfitting on corrupted labels and thus exploring the first issue in previous label correction methods. The noise cleaner constructs a graph convolutional network (GCN [18,19]) model for each class to correct the corrupted labels. During the process of noise cleaner, we consider the relationship among samples (i.e., the local topology structure preservation by GCN) to touch the second issue in previous methods. In particular, our proposed CNLC iteratively updates the noise detector and the noise cleaner, which results in a bi-level optimization problem [4,10] Compared to previous methods, the contributions of our method is two-fold. First, we propose a new label correction method (i.e., a co-assistant framework) to boost the model robustness for medical image analysis by two sequential modules. Either of them adaptively adjusts the other, and thus guaranteeing to output a robust label correction model. Second, two sequential modules in our framework results in a bi-level optimization problem. We thus design a bi-level optimization algorithm to solve our proposed objective function."
Co-assistant Networks for Label Correction,2,Methodology,"In this section, our proposed method first designs a noise detector to discriminate corrupted samples from all samples, and then investigates a noise cleaner to correct the detected corrupted labels."
Co-assistant Networks for Label Correction,2.1,Noise Detector,"Noise detector is used to distinguish corrupted samples from clean samples. The prevailing detection method is designed to first calculate the loss of DNNs on all training samples and then distinguish corrupted samples from clean ones based on their losses. Specifically, the samples with small losses are regarded as clean samples while the samples with large losses are regarded as corrupted samples.Different from previous literature [6,7], our noise detector involves two steps, i.e., CNN and label partition, to partition all training samples for each class into three subgroups, i.e., clean samples, uncertain samples and corrupted samples. Specifically, we first employ CNN with the cross-entropy loss as the backbone to obtain the loss of all training samples. Since the cross-entropy loss is easy to overfit on corrupted labels without extra noise-tolerant term [1,21], we change it to the following resistant loss in CNN:where b is the number of samples in each batch, p t i [j] represents the j-th class prediction of the i-th sample in the t-th epoch, ỹi ∈ {0, 1, ..., C -1} denotes the corrupted label of the i-th sample x i , C denotes the number of classes and λ(t) is a time-related hyper-parameter. In Eq. ( 1), the first term is the cross-entropy loss. The second term is the resistance loss which is proposed to smooth the update of model parameters so that preventing model overfitting on corrupted labels to some extent [12].In label partition, based on the resistant loss in Eq. ( 1), the training samples for each class are divided into three subgroups, i.e., clean samples, uncertain Fig. 2. Cross-entropy loss distribution and GMM probability on different noise rates on BreakHis [13]. ""clean"" denotes the samples with the ground-truth labels, while ""corrupted"" denotes the samples with the corrupted labels.samples, and corrupted samples. Specifically, the samples with n 1 smallest losses are regarded as clean samples and the samples with n 1 largest loss values are regarded as corrupted samples, where n 1 is experimentally set as 5.0% of all training sample for each class. The rest of the training samples for each class are regarded as uncertain samples.In noise detector, our goal is to identify the real clean samples and real corrupted samples, which are corresponded to set as positive samples and negative samples in noise cleaner. If we select a large number of either clean samples or corrupted samples (e.g., larger than 5.0% of all training samples), they may contain false positive samples or false negative samples, so that the effectiveness of the noise cleaner will be influenced. As a result, our noise detector partitions all training samples for each class into three subgroups, including a small proportion of clean samples and corrupted samples, as well as uncertain samples."
Co-assistant Networks for Label Correction,2.2,Noise Cleaner,"Noise cleaner is designed to correct labels of samples with corrupted labels. Recent works often employ DNNs (such as CNN [8] and MLP [15]) to correct the corrupted labels. First, these methods ignore to take into account the relationship among the samples, such as local topology structure preservation, i.e., one of popular techniques in computer vision and machine learning, which ensures that nearby samples have similar labels and dissimilar samples have different labels. In particular, based on the partition mentioned in the above section, the clean samples within the same class should have the same label and the corrupted samples should have different labels from clean samples within the same class. This indicates that it is necessary to preserve the local topology structure of samples within the same class. Second, in noise detector, we only select a small proportion of clean samples and corrupted samples for the construction of noise cleaner. Limited number of samples cannot guarantee to build robust noise cleaner. In this paper, we address the above issues by employing semi-supervised learning, i.e., a GCN for each class, which keeps the local topology structure of samples on both labeled samples and unlabeled samples. Specifically, our noise cleaner includes three components, i.e., noise rate estimation, class-based GCNs, and corrupted label correction.The inputs of each class-based GCN include labeled samples and unlabeled samples. The labeled samples consist of positive samples (i.e., the clean samples of this class with the new label z ic = 1 for the i-th sample in the c-th class) and negative samples (i.e., the corrupted samples of this class with the new label z ic = 0). The unlabeled samples include a subset of the uncertain samples from all classes and corrupted samples of other classes. We follow the principle to select uncertain samples for each class, i.e., the higher the resistant loss in Eq. ( 1), the higher the probability of the sample belonging to corrupted samples. Moreover, the number of uncertain samples is determined by noise rate estimation.Given the resistant loss in Eq. ( 1), in noise rate estimation, we estimate the noise rate of the training samples by employing a Gaussian mixed model (GMM) composed of two Gaussian models. As shown in Fig. 2, we observe that the mean value of Gaussian model for corrupted samples is greater than that of Gaussian model for clean samples. Thus, the Gaussian model with a large mean value is probably the curve of corrupted labels. Based on this, given two outputs of the GMM model for the i-th sample, its output with a larger mean value and the output with a smaller mean value, respectively, are denoted as M i,1 and M i,2 , so the following definition v i is used to determine if the i-th samples is noise:Hence, the noise rate r of training samples is calculated by:where n represents the total number of samples in training dataset. Supposing the number of samples in the c-th class is s c , the number of uncertain samples of each class is s c × r -n 1 . Hence, the total number of unlabeled samples for each class is n × r -n 1 in noise cleaner. Given 2 × n 1 labeled samples and n × r -n 1 unlabeled samples, the classbased GCN for each class conducts semi-supervised learning to predict n × r samples, including n × r -n 1 unlabeled samples and n 1 corrupted samples for this class. The semi-supervised loss L ssl includes a binary cross-entropy loss L bce for labeled samples and an unsupervised loss L mse [8] for unlabeled samples, i.e., L ssl = L bce + L mse , where L bce and L mse are defined as:where q t ic denotes the prediction of the i-th sample in the t-th epoch for the class c, qt ic is updated by qt ic =, where (t) is related to time [8].In corrupted label correction, given C well-trained GCNs and the similarity scores on each class for a subset of uncertain samples and all corrupted samples, their labels can be determined by: ỹi = argmax 0≤c≤C-1 (q ic ) .(6)"
Co-assistant Networks for Label Correction,2.3,Objective Function,"The optimization of the noise detector is associated with the corrupted label set ỹ, which is determined by noise cleaner. Similarly, the embedding of all samples E is an essential input of the noise cleaner, which is generated by the noise detector. As the optimizations of two modules are nested, the objective function of our proposed method is the following bi-level optimization problem: In this paper, we construct a bi-level optimization algorithm to search optimal network parameters of the above objective function. Specifically, we optimize the noise detector to output an optimal feature matrix E * , which is used for the construction of the noise cleaner. Furthermore, the output ỹ * of the noise cleaner is used to optimize the noise detector. This optimization process alternatively optimize two modules until the noise cleaner converges. We list the optimization details of our proposed algorithm in the supplemental materials."
Co-assistant Networks for Label Correction,3,Experiments,
Co-assistant Networks for Label Correction,3.1,Experimental Settings,"The used datasets are BreakHis [13], ISIC [3], and NIHCC [16]. BreakHis consists of 7,909 breast cancer histopathological images including 2,480 benigns and 5,429 malignants. We randomly select 5,537 images for training and 2,372 ones for testing. ISIC has 12,000 digital skin images where 6,000 are normal and 6,000 are with melanoma. We randomly choose 9,600 samples for training and the remaining ones for testing. NIHCC has 10,280 frontal-view X-ray images, where 5,110 are normal and 5,170 are with lung diseases. We randomly select 8,574 images for training and the rest of images for testing. In particular, the random selection in our experiments guarantees that three datasets (i.e., the training set, the testing set, and the whole set) have the same ratio for each class. Moreover, we assume that all labels in the used raw datasets are clean, so we add corrupted labels with different noise rates = {0, 0.2, 0.4} into these datasests, where = 0 means that all labels in the training set are clean.We compare our proposed method with six popular methods, including one fundamental baseline (i.e., Cross-Entropy (CE)), three robustness-based methods (i.e., Co-teaching (CT) [6], Nested Co-teaching (NCT) [2] and Self-Paced Resistance Learning (SPRL) [12]), and two label correction methods (i.e., Co-Correcting (CC) [9] and Self-Ensemble Label Correction (SELC) [11]). For fairness, in our experiments, we adopt the same neural network for all comparison methods based on their public codes and default parameter settings. We evaluate the effectiveness of all methods in terms of four evaluation metrics, i.e., classification accuracy (ACC), specificity (SPE), sensitivity (SEN) and area under the ROC curve (AUC)."
Co-assistant Networks for Label Correction,3.2,Results and Analysis,"Table 1 presents the classification results of all methods on three datasets. Due to the space limitation, we present the results at = 0.0 of all methods in the supplemental materials. First, our method obtains the best results, followed by CT, NCT, SPRL, CELC, CC, and CE, on all datasets in terms of four evalua- This might be because our proposed method not only utilizes a robust method to train a CNN for distinguishing corrupted labels from clean labels, but also corrects them by considering their relationship among the samples within the same class. Second, all methods outperform the fundamental baseline (i.e., CE) on all cases. For example, the accuracy of CC improves by 4.8% and 28.2% compared with CE at = 0.2 and = 0.4, respectively, on ISIC. The reason is that the cross-entropy loss easily results in the overfitting issue on corrupted labels."
Co-assistant Networks for Label Correction,3.3,Ablation Study,"To verify the effectiveness of the noise cleaner, we compare our method with the following comparison methods: 1) W/O NC: without noise cleaner, and 2) MLP: replace GCN with Multi-Layer Perceptron, i.e., without considering the relationship among samples. Due to the space limitation, we only show results on ISIC, which is listed in the first and second rows of Table 2. The methods with noise cleaner (i.e., MLP and CNLC) outperform the method without noise cleaner W/O NC. For example, CNLC improves by 4.2% compared with W/O NC at = 0.4. Thus, the noise cleaner plays an critical role in CNLC. Additionally, CNLC obtains better performance than MLP because it considers the relationship among samples. Both of the above observations verify the conclusion mentioned in the last section again.To verify the effectiveness of the resistance loss in Eq. ( 1), we remove the second term in Eq. ( 1) to have a new comparison method CNLC-RL and list the results in the third row of Table 2. Obviously, CNLC outperforms CNLC-RL. For example, CNLC improves by 1.0% and 2.3%, respectively, compared to CNLC-RL, in terms of four evaluation metrics at = 0.2 and = 0.4. The reason is that the robustness loss can prevent the model from overfitting on corrupted labels, and thus boosting the model robustness. This verifies the effectiveness of the resistance loss defined in Eq. (1) for medical image analysis, which has been theoretically and experimentally verified in the application of natural images [12]."
Co-assistant Networks for Label Correction,4,Conclusion,"In this paper, we proposed a novel co-assistant framework, to solve the problem of DNNs with corrupted labels for medical image analysis. Experiments on three medical image datasets demonstrate the effectiveness of the proposed framework. Although our method has achieved promising performance, its accuracy might be further boosted by using more powerful feature extractors, like pre-train models on large-scale public datasets or some self-supervised methods, e.g., contrastive learning. In the future, we will integrate these feature extractors into the proposed framework to further improve its effectiveness."
Co-assistant Networks for Label Correction,,Fig. 1 .,
Co-assistant Networks for Label Correction,,,
Co-assistant Networks for Label Correction,,Table 1 .,
Co-assistant Networks for Label Correction,,Table 2 .,
Co-assistant Networks for Label Correction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_16.
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,1,Introduction,"Retinal fundus images are widely used in diabetic retinopathy (DR) detection through a data-driven approach [4]. Specifically, lesions in fundus images play a major role in DR recognition. Some of most prominent lesions associated with DR include microaneurysms, hard exudates, soft exudates, hemorrhages, intraretinal microvascular abnormalities, neovascularization [15]. Hard exudates and soft exudates [3,21,23] are commonly observed in the early stages of diabetic retinopathy. Therefore, it is important using data augmentation on the lesions of DR images in AI-based models, especially exudates. However, the size and quantity of exudates can potentially serve as discriminatory indicators of the severity of diabetic retinopathy [18]. As a result, the most crucial issue is that we need augment the lesions without changing the severity level of DR, especially in the region of exudates.Classic image processing methods for data augmentation include random rotation, vertical and horizontal flipping, cropping, random erasing and so on. In some cases, standard data augmentation methods can increase the size of dataset and prevent overfitting of the neural network [8,22]. But in medical field, conventional data augmentation may damage the semantic content of the image. In ophthalmology diagnosis, the eyes sometimes need to be divided into left and right, and each eye has its own features that are important for image recognition. In this case, we cannot augment the image by flipping method. The same is true for random clipping as the clipped area may not contain any pathological information, which may cause labeling errors. Therefore, the standard methods may damage the semantics of the medical image and also cannot add pathological diversity in the lesion region. Synthetic images generated by GANs can handle the problem of pathological diversity [7,11], but the generated images are unlabeled. Even if a GAN model is trained separately for different categories of images [2,5], there is still a possibility of labeling errors in the generated images. Because the generated images are only a sample of the distribution of a given dataset, and the label of generated images are not controlled. Moreover, it is time consuming to train a new model for each category separately. Therefore, it is important to find a way to use the GAN model to increase the size and diversity of the data while keeping the labels in data augmentation.For the aforementioned problems, this paper proposes a data augmentation method for dealing with the class imbalanced problem by manipulating the lesions in DR images to increase the size and diversity of the pathologies while preserving the labels of the data. In this paper, we train a DR image generator based on StyleGAN3 [14] to perform manipulation on lesions in latent space. To make the fundus images after manipulation more realistic in terms of DR pathologies, we add an LPIPS [26] loss based on DR detection to the network. In a well-disentangled latent space, one latent code can control the generation of a specific attribute [6]. In this paper, we mainly focus on exudates augmentation, our purpose is to find the latent code that individually controls the generation of exudates region. Different from StyleSpace [25], we do not need to detect the position of latent code in latent space by calculating the jacobian matrix of the generated image with respect to latent space. Instead, we set the mask of exudates as the contribution map of generated image to DR pathologies, and apply backpropagation to obtain the contribution map of DR lesion with respect to latent space. Then we analyze the contribution score of each latent code, the position of latent code that controls the exudates region is determined by highest contribution score.The major contributions of this work can be summarized as follows. Firstly, we introduce LPIPS loss based on DR detection in the training phase of Style-GAN3 to make the reconstructed images have more realistic lesions. Secondly we apply channel locating method with gradient and backpropagation algorithms to identify the position of latent code with the highest contribution score to lesions in DR images. Finally, we perform manipulation on lesions for data augmentation. We also show the performance of proposed method for DR recognition, and explore the correlation between label preservation and editing strength."
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,2,Methods,"Figure 1 illustrates our overall structure of the data augmentation method. In part A, StyleGAN3 model is trained to generate DR images. The network maps the noise vector Z to W and W + space through two linear connection layers, and then maps it to S space via affine transformation layer A. LPIPS loss based on DR detection was added in training phase. In part B, we use projector P to embed real image to S space, then decoder D trained in part A is used for reconstruction. We also define y as the importance score of DR pathology (exudates), and then the lesion map can be used as the contribution map of y with respect to output image. The meaning of lesion map is the contribution score matrix of each pixel to DR pathology in output image, because 0 in the lesion map means non-lesion, 1 means lesion. If we want to get the contribution score of each latent code to DR pathology in S space, we can compute the gradient of y with respect to S space by applying back-propagation. The gradient list of all latent codes is denoted as R which can be used as a list of contribution score for latent space because of the meaning of lesion map. In part C, we perform manipulation on real image by fusing the latent code of position (l, c) in S space and average space μ with editing strength α."
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,2.1,Part A: Training Phase of StyleGAN3,"StyleGAN3 has the ability to finely control the style of generated images. It achieves style variations by manipulating different directions in the latent vector space. In latent space of StyleGAN3, the latent code z ∈ Z is a stochastic normally distributed vector, always called Z space. After the fully connected mapping layers, z is transformed into a new latent space W [1]. In affine transformation layers of StyleGAN3, the input vectors are denoted as W + space, and the output vectors are denoted as S space. In [25], the experiments demonstrate that the style code s ∈ S can better reflect the disentangled nature of the learned distribution than W and W + spaces.LPIPS Loss Based on DR Detection. Due to the good performance of LPIPS in generation tasks [26], we use additional LPIPS loss in generator of StyleGAN3. But the original LPIPS loss is based on the Vgg and Alex network trained on ImageNet, which don't include fundus images. As we want to make the fundus images after manipulation have more realistic DR pathology, we customized the LPIPS loss based on DR detection tasks. Firstly, we train a DR detection network based on VGG16 [24] and select the 5 feature maps after max pooling layers as input for LPIPS. In this way, the loss function will make the generator focus more on lesion of fundus images. At last, we add original StyleGAN loss and LPIPS loss together when training."
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,2.2,Part B: Detecting the Channel of Style Code Having Highest Contribution to Exudates,"As shown in Fig. 1, we define y = f (x) as the function to compute the importance score of DR pathology for x, so y represents the importance score. The meaning of lesion map is the contribution score matrix of each pixel to DR pathology in output image. In the paper, because the binary segmentation mask only values 1 and 0. the value 1 can give the meaning of contribution, 0 means no contribution. So we can regard binary semantic segmentation mask as a contribution matrix. Due to these prerequisites, the binary semantic mask has same meaning with gradient map. Because of these characteristic that we can consider set the mask as the contribution matrix of y with respect to the output image.When we get output image from the generator of a well-trained StyleGAN3, we apply a pre-trained exudates segmentation network based on Unet [20] to obtain the mask of the exudates. As Eq. 3 shown, we can compute the importance score y by integral calculus. But images and vectors in middle of StyleGAN3 are not continuous. We also do not know how function f works, the things we know are x is the output image, and lesion mask means the contribution matrix. And if a pixel value in gradient map of output image is 0, then the pixel in output image will also contribute 0 to the lesion because of forward propagation in network. So we can treat lesion map as gradient matrix of y . Since the lesion map has the property that 1 means contribution and 0 means no contribution, and we want to compute the importance score of DR pathology for x. We can rewrite the integral calculus as computing the sum of all pixels in the image based on the mask. Here, denote the pixel-wise dot product. M is the number of pixels in image.After y, the importance score of DR pathology is computed. We can calculate the contribution of each latent code to DR pathology in S space by computing the gradient of y with respect to each latent code in S space, based on backpropagation algorithm in neural network as shown in the Eq. 4.where R is the contribution score list of latent codes in S space, k ∈ K is the dimension of S space, R k is the contribution score of kth-dimension of S space.In our paper, S space has K dimensions in total including 6048 dimensions of feature maps and 3040 dimensions of tRGB blocks. So, in StyleGAN3, depending on the structure of the network, the K dimensions can be correspond to different layers l and channels c depending on the number of channels in each layers. L is the number of layers in StyleGAN3. Since we want to do image editing without affecting other features of the image as much as possible to avoid changes in image label. From the experiments in [6], a well disentanglement latent space, one latent code can control the generation of a specific attribute. So when editing the image, it is not necessary to manipulate on whole S space. We can only modify the style code with highest contribution score.To ensure that the collected style codes are not impacted by individual noise and are consistent across all images, we calculate the average of 1K images. Afterwards, the collected contribution list R are sorted from high to low based on contribution score of each latent code. To make it easier to understand we can convert k to (l, c) based on the layers and channels in StyleGAN3 network structure."
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,2.3,Part C: Manipulation on Real Images in Inference Phase,"We project the real images to style spaces using projector provided by Karras et al. [14]. Based on the position (l, c) of contribution score in R, we perform manipulation on the layer and channel of highest contribution score when the data flow forward in decoder of StyleGAN3 D(•).where S new denotes the S space after manipulation, I aug denotes the generated image after manipulation. α is the editing strength. μ is the mean vector of style space, which is obtained by projecting lots of real images to S space. m is a special vector with same dimension with S, the value of all elements in the vector is 0 except for the element at the (l, c) position, which is equal to 1. It means which layer and channel is selected."
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,3,Experiments,
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,3.1,Setup Details,"Dataset. We carry out all experiments on publicly available datasets. We train StyleGAN3 on EyePACS [9] dataset. We use EyeQ [10] to grade the quality of EyePACS and select 12K images for training and 8K images for testing from the dataset that has been classified as ""Good"" or ""Usable"". For evaluation of our data augmentation method. ODIR-5K (Ocular Disease Intelligent Recognition) [17] dataset was used for DR classification tasks. It include 6392 images with different ocular diseases of which 4000 are training set and 2392 are testing set. APTOS 2019 [13] dataset was used for grading evaluation of diabetic retinopathy as no DR, mild, moderate, severe and proliferative DR. There are 3662 image with labels in total, we split 2000 as training set and 1662 as testing set.Evaluation Metric. We employ Frechet Inception Distance (FID) [12] and Learned Perceptual Image Patch Similarity (LPIPS) [26] to measure the distance between synthetic and real data distribution. FID and LPIPS can evaluate the quality of images created by a generative model. To measure the performance of human perception score, we apply Precision, Recall, Accuracy and F1 score as the evaluation metric."
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,,Experimental Settings.,"All experiments are conducted on a single NVIDIA RTX A5000 GPU with 24GB memory using PyTorch implementation. For Style-GAN3, we initialize the learning rate with 0.0025 for G(•)and 0.002 for D(•). For optimization, we use the Adam optimizer with β 1 = 0.0, β 2 = 0.99. We empirically set λ lpips = 0.6, λ gan = 0.4. For evaluation of classification and detection tasks, we both initialize the learning rate with 0.001 for the training of VGG16. "
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,3.2,Evaluation and Results,"Results of Synthetic Images. For visual evaluation of generation of Style-GAN3, we show some synthetic images in Fig. 2. Generated images in Fig. 2a show the image without LPIPS loss in training phase. The exudates are sometimes not in the reasonable region, or even not generated. But the synthetic images are visually comparable to real fundus images, in Fig. 2b. For subjective quality evaluation, we also conduct a user study by randomly choosing 200 images which consists of 100 synthetic images and 100 real images, then professional ophthalmologists are asked to determine which images are synthetic. We evaluate the human perception score by Accuracy, Precision, Recall and F1 score in Table 1. Evidently, the proposed loss function based on LPIPS significantly improves the quality of generated images. And from the human perception score, we can also know that even for professional doctors there is only a 50% possibility to predict which one is real and which one is synthetic. This means that synthetic images might be highly confusing even for experienced clinicians. In ODIR-5k dataset, we perform the manipulation on DR image in training dataset to make DR images have same number with NDR images [16]. And then we train a classification network based on VGG16 with training dataset. We also perform manipulation on APTOS-2019 dataset for level 2-4. It is important to note that, level 1 in APTOS-2019 means mild severity of NPDR, the main lesion is microaneurysms in this stage. Therefore we will not edit the image at this stage because the exudation has not yet occurred. We compare our data augmentation method with standard method in these two tasks. Quantitative evaluation results are shown in Table 2. The results show that standard data augmentation method will not improve the accuracy too much, even the rotation(-30 • , 30 • ) will reduce the accuracy. Patho-GAN [19] can not perform augmentation in detection task, because it is not label-preserving method. In detection task, we calculate the accuracy for each level of severity. In level 2 and 3, the accuracy improvement is significant, especially level 3 is most imbalanced in original dataset. In level 4, the accuracy also have been improved, even though the main lesion in this stage is the growing of new abnormal blood vessels, augmenting the dataset can still improve the accuracy of recognition after changing the exudation area. The accuracy of level 0 and 1 have not changed much, the accuracy of level 1 has even decreased, and the reason for this phenomenon is most likely due to the imbalance of data categories caused by not performing augmentation on level 1. This also shows that data imbalance does have an impact on accuracy. Ablation Study. Since our approach is label-preserving data augmentation, it is more concerned with whether our augmentation method affects the label of the image. In order to investigate the correlation between label preservation and editing strength, we conduct comprehensive ablation studies to examine the effect of different editing strength α on the recognition accuracy. As we know, if we change exduates too much, the labels of images will be changed. Such as, level 1 will change to level 2, level 2 will change to level 3. If such a thing happens, our method will not only have no improvement on the accuracy rate, but will also have a negative impact. The Fig. 3a show the manipulation results with different editing strength. We also conduct experiments of effect on the accuracy with different editing strength as shown in Fig. 3b. Results show that when editing strength are in the range of (-0.6 0.6), the accuracy will be improved, otherwise the accuracy is significantly reduced. "
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,4,Conclusions,"In this paper we propose a label-preserving data augmentation method to deal with the classes imbalanced problem in DR detection. The proposed approach computes the contribution score of latent code, and perform manipulation on the lesion of real images. In this way, the method can augment the dataset in a label-preserving manner. It is a targeted and effective label-preserving data augmentation approach. Although the paper mainly discusses exudates, the proposed method can be applied to other types of lesions as long as we have the lesion mask and the GAN model can generate realistic lesions.Data Declaration. Data underlying the results presented in this paper are available in APTOS [13], ODIR-5K [17], EyePACS [9]."
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,,Fig. 1 .,
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,,Fig. 2 .,
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,,Fig. 3 .,
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,,Table 1 .,
Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition,,Table 2 .,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,1,Introduction,"Positron emission tomography (PET) image synthesis [1][2][3][4][5][6][7][8][9][10] aims at recovering high-quality full-dose PET images from low-dose ones. Despite great success, most algorithms [1,2,4,5,[8][9][10] are specialized for PET data from a single center with a fixed imaging system/protocol. This poses a significant problem for practical applications, which are not usually restricted to any one of the centers. Towards filling this gap, in this paper, we focus on multi-center PET image synthesis, aiming at processing data from multiple different centers.However, the generalizability of existing models can still be suboptimal for a multi-center study due to domain shift, which results from non-identical data distribution among centers with different imaging systems/protocols (see Fig. 1 (a)). Though some studies have shown that a specialized model (i.e. a convolutional neural network (CNN) [3,6] or Transformer [9] trained on a single center) exhibits certain robustness to different tracer types [9], different tracer doses [3], or even different centers [6], such generalizability of a center-specific knowledge is only applicable to small domain shifts. It will suffer a severe performance drop when exposed to new centers with large domain shifts [11]. There are also some federated learning (FL) based [7,11,12] medical image synthesis methods that improve generalizability by collaboratively learning a shared global model across centers. Especially, federated transfer learning (FTL) [7] first successfully applies FL to PET image synthesis in a multiple-dose setting. Since the resultant shared model of the basic FL method [12] ignores center specificity and thus cannot handle centers with large domain shifts, FTL addresses this by finetuning the shared model for each center/dose. However, FTL only focuses on different doses and does not really address the multi-center problem. Furthermore, it still requires a specialized model for each center/dose, which ignores potentially transferable shared knowledge across centers and scales up the overall model size.A recent trend, known as generalist models, is to request that a single unified model works for multiple tasks/domains, and even express generalizability to novel tasks/domains. By sharing architecture and parameters, generalist models can better utilize shared transferable knowledge across tasks/domains. Some pioneers [13][14][15][16][17] have realized competitive performance on various high-level vision tasks like classification [13,16], object detection [14], etc.Nonetheless, recent studies [16,18] report that conventional generalist [15] models may suffer from the interference issue, i.e. different tasks with shared parameters potentially conflict with each other in the update directions of the gradient. Specific to PET image synthesis, due to the non-identical data distribution across centers, we also observe the center interference issue that the gradient directions of different centers may be inconsistent or even opposite (see Fig. 1). This will lead to an uncertain update direction that deviates from the optimal, resulting in sub-optimal performance of the model. To address the interference issue, recent generalist models [14,16] have introduced dynamic routing [19] which learns to activate experts (i.e. sub-networks) dynamically. The input feature will be routed to different selected experts accordingly so as to avoid interference. Meanwhile, different inputs can share some experts, thus maintaining collaboration across domains. In the inference time, the model can reasonably generalize to different domains, even unknown domains, by utilizing the knowledge of existing experts. In spite of great success, the study of generalist models rarely targets the problem of multi-center PET image synthesis.In this paper, inspired by the aforementioned studies, we innovatively propose a generalist model with Dynamic Routing for Multi-Center PET image synthesis, termed DRMC. To mitigate the center interference issue, we propose a novel dynamic routing strategy to route data from different centers to different experts. Compared with existing routing strategies, our strategy makes an improvement by building cross-layer connections for more accurate expert decisions. Extensive experiments show that DRMC achieves the best generalizability on both known and unknown centers. Our contribution can be summarized as:  "
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,2,Method,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,2.1,Center Interference Issue,"Due to the non-identical data distribution across centers, different centers with shared parameters may conflict with each other in the optimization process.To verify this hypothesis, we train a baseline Transformer with 15 base blocks (Fig. 2 (b)) over four centers. Following the paper [16], we calculate the gradient direction interference metric I i,j of the j-th center C j on the i-th center C i . As shown in Fig. 1 (b), interference is observed between different centers at different layers. This will lead to inconsistent optimization and inevitably degrade the model performance. Details of I i,j [16] are shown in the supplement."
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,2.2,Network Architecture,"The overall architecture of our DRMC is shown in Fig. 2 (a). DRMC firstly applies a 3×3×3 convolutional layer for shallow feature extraction. Next, the shallow feature is fed into N blocks with dynamic routing (DRBs), which are expected to handle the interference between centers and adaptively extract the deep feature with high-frequency information. The deep feature then passes through another 3×3×3 convolutional layer for final image synthesis. In order to alleviate the burden of feature learning and stabilize training, DRMC adopts global residual learning as suggested in the paper [20] to estimate the image residual from different centers. In the subsequent subsection, we will expatiate the dynamic routing strategy as well as the design of the DRB. "
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,2.3,Dynamic Routing Strategy,"We aim at alleviating the center interference issue in deep feature extraction.Inspired by prior generalist models [13,14,16], we specifically propose a novel dynamic routing strategy for multi-center PET image synthesis. The proposed dynamic routing strategy can be flexibly adapted to various network architectures, such as CNN and Transformer. To utilize the recent advance in capturing global contexts using Transformers [9], without loss of generality, we explore the application of the dynamic routing strategy to a Transformer block, termed dynamic routing block (DRB, see Fig. 2 (c)). We will introduce our dynamic routing strategy in detail from four parts: base expert foundation, expert number scaling, expert dynamic routing, and expert sparse fusion.Base Expert Foundation. As shown in Fig. 2 (b), we first introduce an efficient base Transformer block (base block) consisting of an attention expert and a feedforward network (FFN) expert. Both experts are for basic feature extraction and transformation. To reduce the complexity burden of the attention expert, we follow the paper [9] to perform global channel attention with linear complexity instead of spatial attention [21]. Notably, as the global channel attention may ignore the local spatial information, we introduce depth-wise convolutions to emphasize the local context after applying attention. As for the FFN expert, we make no modifications to it compared with the standard Transformer block [21].It consists of a 2-layer MLP with GELU activation in between.Expert Number Scaling. Center interference is observed on both attention experts and FFN experts at different layers (see Fig. 1 (b)). This indicates that a single expert can not be simply shared by all centers. Thus, we increase the number of experts in the base block to M to serve as expert candidates for different centers. Specifically, each Transformer block has an attention expert bank, both of which have M base experts. However, it does not mean that we prepare specific experts for each center. Although using center-specific experts can address the interference problem, it is hard for the model to exploit the shared knowledge across centers, and it is also difficult to generalize to new centers that did not emerge in the training stage [16]. To address this, we turn to different combinations of experts.Expert Dynamic Routing. Given a bank of experts, we route data from different centers to different experts so as to avoid interference. Prior generalist models [13,14,16] in high-level vision tasks have introduced various routing strategies to weigh and select experts. Most of them are independently conditioned on the information of the current layer feature, failing to take into account the connectivity of neighboring layers. Nevertheless, PET image synthesis is a dense prediction task that requires a tight connection of adjacent layers for accurate voxel-wise intensity regression. To mitigate the potential discontinuity [13], we propose a dynamic routing module (DRM, see Fig. 2 (c)) that builds cross-layer connection for expert decisions. The mechanism can be formulated as:where X denotes the input; GAP(•) represents the global average pooling operation to aggregate global context information of the current layer; H is the hidden representation of the previous MLP layer. ReLU activation generates sparsity by setting the negative weight to zero. W is a sparse weight used to assign weights to different experts.In short, DRM sparsely activates the model and selectively routes the input to different subsets of experts. This process maximizes collaboration and meanwhile mitigates the interference problem. On the one hand, the interference across centers can be alleviated by sparsely routing X to different experts (with positive weights). The combinations of selected experts can be thoroughly different across centers if violent conflicts appear. On the other hand, experts in the same bank still cooperate with each other, allowing the network to best utilize the shared knowledge across centers."
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Expert Sparse Fusion.,"The final output is a weighted sum of each expert's knowledge using the sparse weight W = [W 1 , W 2 , ..., W M ] generated by DRM. Given an input feature X, the output X of an expert bank can be obtained as:where E m (•) represents an operator of E m AT T (•) or E m F F N (•). "
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,2.4,Loss Function,We utilize the Charbonnier loss [23] with hyper-parameter as 10 -3 to penalize pixel-wise differences between the full-dose (Y ) and estimated ( Ŷ ) PET images:3 Experiments and Results
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,3.1,Dataset and Evaluation,"Full-dose PET images are collected from 6 different centers (C 1 -C 6 ) at 6 different institutions 1 . The data of C 3 and C 4 [22] are borrowed from the Ultra-low Dose PET Imaging Challenge 2 , while the data from other centers were privately collected. The key information of the whole dataset is shown in Table 1. Note that C 1 -C 4 are for both training and testing. We denote them as C kn as these centers are known to the generalist model. C 5 and C 6 are unknown centers (denote as C ukn ) that are only for testing the model generalizability. The low-dose PET data is generated by randomly selecting a portion of the raw scans based on the dose reduction factor (DRF), such as 25% when DRF=4. Then we reconstruct low-dose PET images using the standard OSEM method [24]. Since the voxel size differs across centers, we uniformly resample the images of different centers so that their voxel size becomes 2×2×2 mm 3 . In the training phase, we unfold images into small patches (uniformly sampling 1024 patches from 20 patients per center) with a shape of 64×64×64. In the testing phase, the whole estimated PET image is acquired by merging patches together.To evaluate the model performance, we choose the PSNR metric for image quantitative evaluation. For clinical evaluation, to address the accuracy of the standard uptake value (SUV) that most radiologists care about, we follow the paper [3] to calculate the bias of SU V mean and SU V max (denoted as B mean and B max , respectively) between low-dose and full-dose images in lesion regions. "
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,3.2,Implementation,"Unless specified otherwise, the intermediate channel number, expert number in a bank, and Transformer block number are 64, 3, and 5, respectively. We employ Adam optimizer with a learning rate of 10 -4 . We implement our method with Pytorch using a workstation with 4 NVIDIA A100 GPUs with 40GB memory (1 GPU per center). In each training iteration, each GPU independently samples data from a single center. After the loss calculation and the gradient backpropagation, the gradients of different GPUs are then synchronized. We train our model for 200 epochs in total as no significant improvement afterward."
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,3.3,Comparative Experiments,"We compare our method with five methods of two types. (i) 3D-cGAN [1] and 3D CVT-GAN [10] are two state-of-the-art methods for single center PET image synthesis. (ii) FedAVG [11,12], FL-MRCM [11], and FTL [7] are three federated learning methods for privacy-preserving multi-center medical image synthesis.All methods are trained using data from C kn and tested over both C kn and C ukn .For methods in (i), we regard C kn as a single center and mix all data together for training. For federated learning methods in (ii), we follow the ""Mix"" mode (upper bound of FL-based methods) in the paper [11] to remove the privacy constraint and keep the problem setting consistent with our multi-center study.Comparison Results for Known Centers. As can be seen in Table 2, in comparison with the second-best results, DRMC boosts the performance by 0.77 dB PSNR, 0.0078 B mean , and 0.0135 B max . This is because our DRMC not only leverages shared knowledge by sharing some experts but also preserves center-specific information with the help of the sparse routing strategy. Further evaluation can be found in the supplement.Comparison Results for Unknown Centers. We also test the model generalization ability to unknown centers C 5 and C 6 . C 5 consists of normal brain data (without lesion) that is challenging for generalization. As the brain region only occupies a small portion of the whole-body data in the training dataset but has more sophisticated structure information. C 6 is a similar center to C 1 but has different working locations and imaging preferences. The quantitative results are shown in Table 3 and the visual results are shown in Fig. 1 (a). DRMC achieves the best results by dynamically utilizing existing experts' knowledge for generalization. On the contrary, most comparison methods process data in a static pattern and unavoidably produce mishandling of out-of-distribution data. Furthermore, we investigate model's robustness to various DRF data, and the results are available in the supplement."
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,3.4,Ablation Study,"Specialized Model vs. Generalist Model. As can be seen in Table 5, the baseline model (using 15 base blocks) individually trained for each center acquires good performance on its source center. But it suffers performance drop on other centers. The baseline model trained over multiple centers greatly enhances the overall results. But due to the center interference issue, its performance on a specific center is still far from the corresponding specialized model. DRMC mitigates the interference with dynamic routing and achieves comparable performance to the specialized model of each center. Ablation Study of Routing Strategy. To investigate the roles of major components in our routing strategy, we conduct ablation studies through (i) removing the condition of hidden representation H that builds cross-layer connection, and replacing ReLU activation with (ii) softmax activation [14] and (iii) top-2 gating [13]. The results are shown in Table 4. We also analyze the interpretability of the routing by showing the distribution of different layers' top-1 weighted experts using the testing data. As shown in Fig. 3 (b), different centers show similarities and differences in the expert distribution. For example, C 6 shows the same distribution with C 1 as their data show many similarities, while C 5 presents a very unique way since brain data differs a lot from whole-body data.Ablation Study of Hyperparameters. In Fig. 3 (c) and (d), we show ablation results on expert number (M ) and block number (N ). We set M =3 and N =5, as it has realized good performance with acceptable computational complexity."
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,4,Conclusion,"In this paper, we innovatively propose a generalist model with dynamic routing (DRMC) for multi-center PET image synthesis. To address the center interference issue, DRMC sparsely routes data from different centers to different experts. Experiments show that DRMC achieves excellent generalizability."
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,-,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Fig. 1 .,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Fig. 2 .,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Fig. 3 .,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Table 1 .,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Table 2 .,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Table 3 .,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,54 48.26 -0.1814 -0.1483Table 4 .,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,88 0.0752 0.1155 38.40 0.1814 0.1483Table 5 .,
DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 4.
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,1,Introduction,"Computed Tomography (CT) and Magnetic Resonance (MR) are two widely used imaging techniques in clinical practice. CT imaging uses X-rays to produce detailed, cross-sectional images of the body, which is particularly useful for imaging bones and detecting certain types of cancers with fast imaging speed. However, CT imaging has relatively high radiation doses that can pose a risk of radiation exposure to patients. Low-dose CT techniques have been developed to address this concern by using lower doses of radiation, but the image quality is degraded with increased noise, which may compromise diagnostic accuracy [9].MR imaging, on the other hand, uses a strong magnetic field and radio waves to create detailed images of the body's internal structures, which can produce high-contrast images for soft tissues and does not involve ionizing radiation. This makes MR imaging safer for patients, particularly for those who require frequent or repeated scans. However, MR imaging typically has a lower resolution than CT [18], which limits its ability to visualize small structures or abnormalities.Motivated by the aforementioned, there is a pressing need to improve the quality of low-dose CT images and low-resolution MR images to ensure that they provide the necessary diagnostic information. Numerous algorithms have been developed for CT and MR image enhancement, with deep learning-based methods emerging as a prominent trend [5,14], such as using the conditional generative adversarial network for CT image denoising [32] and convolutional neural network for MR image Super-Resolution (SR) [4].These algorithms are capable of improving image quality, but they have two significant limitations. First, paired images are required for training, e.g., low-dose and full-dose CT images; low-resolution and high-resolution MR images). However, acquiring such paired data is challenging in real clinical scenarios. Although it is possible to simulate low-quality images from high-quality images, the models derived from such data may have limited generalization ability when applied to real data [9,14]. Second, customized models are required for each task. For example, for MR super-resolution tasks with different degradation levels (i.e., 4x and 8x downsampling), one may need to train a customized model for each degradation level and the trained model cannot generalize to other degradation levels. Addressing these limitations is crucial for widespread adoption in clinical practice.Recently, pre-trained diffusion models [8,11,21] have shown great promise in the context of unsupervised natural image reconstruction [6,7,12,28]. However, their applicability to medical images has not been fully explored due to the absence of publicly available pre-trained diffusion models tailored for the medical imaging community. The training of diffusion models requires a significant amount of computational resources and training images. For example, openai's improved diffusion models [21] took 1600-16000 A100 hours to be trained on the ImageNet dataset with one million images, which is prohibitively expensive. Several studies have used diffusion models for low-dose CT denoising [30] and MR image reconstruction [22,31], but they still rely on paired images.In this paper, we aim at addressing the limitations of existing image enhancement methods and the scarcity of pre-trained diffusion models for medical images. Specifically, we provide two well-trained diffusion models on full-dose CT images and high-resolution heart MR images, suitable for a range of applications including image generation, denoising, and super-resolution. Motivated by the existing plug-and-play image restoration methods [26,34,35] and denoising diffusion restoration and null-space models (DDNM) [12,28], we further introduce a paradigm for plug-and-play CT and MR image denoising and super-resolution as shown in Fig. 1. Notably, it eliminates the need for paired data, enabling greater scalability and wider applicability than existing paired-image dependent methods. Moreover, it eliminates the need to train a customized model for each task. Our method does not need additional training on specific tasks and can directly use the single pre-trained diffusion model on multiple medical image enhancement tasks. The pre-trained diffusion models and PyTorch code of the present method are publicly available at https://github.com/bowang-lab/ DPM-MedImgEnhance."
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2,Method,This section begins with a brief overview of diffusion models for image generation and the mathematical model and algorithm for general image enhancement. We then introduce a plug-and-play framework that harnesses the strengths of both approaches to enable unsupervised medical image enhancement.
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2.1,Denoising Diffusion Probabilistic Models (DDPM) for Unconditional Image Generation,"Image generation models aim to capture the intrinsic data distribution from a set of training images and generate new images from the model itself. We use DDPM [11] for unconditional medical image generation, which contains a diffusion (or forward) process and a sampling (or reverse) process. The diffusion process gradually adds random Gaussian noise to the input image x 0 , following a Markov Chain with transition kernel q(xwhere t ∈ {1, • • • , T } represents the current timestep, x t and x t-1 are adjacent image status, andis a predefined noise schedule. Furthermore, we can directly obtain x t based on x 0 at any timestep t by:where α t := 1-β t , ᾱt := Π t s=1 α s , and ∼ N (0, I). This property enables simple model training where the input is the noisy image x t and the timestep t and the output is the predicted noise θ (θ denotes model parameters). Intuitively, a denoising network is trained with the mean square loss E t,x ||θ (x t , t)|| 2 . The sampling process aims to generate a clean image from Gaussian noise x T ∼ N (0, I), and each reverse step is defined by:"
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2.2,Image Enhancement with Denoising Algorithm,"In general, image enhancement tasks can be formulated by:where y is the degraded image, H is a degradation matrix, x is the unknown original image, and n is the independent random noise. This model can represent various image restoration tasks. For instance, in the image denoising task, H is the identity matrix, and in the image super-resolution task, H is the downsampling operator. The main objective is to recover x by solving the minimization problem:x * = arg minwhere the first data-fidelity term keeps the data consistency and the second dataregularization term R(x) imposes prior knowledge constraints on the solution. This problem can be solved by the Iterative Denoising and Backward Projections (IDBP) algorithm [26], which optimizes the revised equivalent problem:whereis the pseudo inverse of the degradation matrix H and f H T H := f T H T Hf. Specifically, x * and ŷ * can be alternatively estimated by solving min x ŷx 2 2 + R(x) and min ŷ ŷx 2 2 s.t. H ŷ = y. Estimating x * is essentially a denoising problem that can be solved by a denoising operator and ŷ * has a closed-form solution:Intuitively, IDBP iteratively estimates the original image from the current degraded image and makes a projection by constraining it with prior knowledge.Although IDBP offers a flexible way to solve image enhancement problems, it still requires paired images to train the denoising operator [26]."
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,2.3,Pre-Trained Diffusion Models for Plug-and-play Medical Image Enhancement,"We introduce a plug-and-play framework by leveraging the benefits of the diffusion model and IDBP algorithm. Here we highlight two benefits: (1) it removes the need for paired images; and (2) it can simply apply the single pre-trained diffusion model across multiple medical image enhancement tasks. First, we reformulate the DDPM sampling process [11] x t-1 ∼ p θ (x t-1 |x t ) = N (x t-1 ; μ θ (x t , t), β t I) into:andIntuitively, each sampling iteration has two steps. The first step estimates the denoised image x 0|t based on the current noisy image x t and the trained denoising network θ (x t , t). The second step generates a rectified image x t-1 by taking a weighted sum of x 0|t and x t and adding a Gaussian noise perturbation.As mentioned in Eq. ( 3), our goal is to restore an unknown original image x 0 from a degraded image y. Thus, the degraded image y needs to be involved in the sampling process. Inspired by the iteration loop in IDBP, we project the estimated x 0|t on the hyperplane y = Hx:It can be easily proved that H x0|t = HH † y + Hx 0|t -HH † Hx 0|t = y. By replacing x 0|t with x0|t in Eq. ( 8), we have:Algorithm 1 shows the complete steps for image enhancement, which inherit the denoising operator from DDPM and the projection operator from IDBP. The former employs the strong denoising capability in the diffusion model and the latter can make sure that the generated results match the input image. Notably, the final algorithm is equivalent to DDNM [28], but it is derived from different perspectives."
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,,Algorithm 1. Pre-trained DDPM for plug-and-play medical image enhancement,"Require: Pre-trained DDPM θ , low-quality image y, degradation operator H 1: Initialize xT ∼ N (0, I). 2: for t = T to 1 do 3: "
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,3,Experiments,"Dataset. We conducted experiments on two common image enhancement tasks: denoising and SR. To mimic the real-world setting, the diffusion models were trained on a diverse dataset, including images from different centers and scanners. The testing set (e.g., MR images) is from a new medical center that has not appeared in the training set. Experiments show that our model can generalize to these unseen images. Specifically, the denoising task is based on the AAPM Low Dose CT Grand Challenge abdominal dataset [19], which can be also used for SR [33]. The heart MR SR task is based on three datasets: ACDC [1], M&Ms1-2 [3], and CMRxMotion [27]. Notably, the presented framework eliminates the requirement of paired data. For the CT image enhancement task, we trained a diffusion model [21] based on the full-dose dataset that contains 5351 images, and the hold-out quarter-dose images were used for testing. For the MR enhancement task, we used the whole ACDC [1] and M&Ms1-2 [3] for training the diffusion model and the CMRxMotion [27] dataset for testing. The testing images were downsampled by operator H with factors of 4× and 8× to produce low-resolution images, and the original images served as the ground truth.Evaluation Metrics. The image quality was quantitatively evaluated by the Peak Signal-to-Noise Ratio (PSNR), Structural SIMilarity index (SSIM) [29], and Visual Information Fidelity (VIF) [24], which are widely used measures in medical image enhancement tasks [9,17]. Implementation Details. We followed the standard configuration in [21] to train the diffusion model from scratch. Specifically, the diffusion step used a linear noise schedule β ∈ [1e -4, 0.02] and the number of diffusion timesteps was T = 1000. The input image size was normalized to 256 × 256 and the 2D U-Net [23] was optimized by Adam [13] with a batch size of 16 and a learning rate of 10 -4 , and an Exponential Moving Average (EMA) over model parameters with a rate of 0.9999. All the models were trained on A100 GPU and the total training time was 16 d. The implementation was based on DDNM [28]. For an efficient sampling, we used DDIM [25] with 100 diffusion steps. We followed the degradation operator settings in DDNM. Specifically, we used the identity matrix I as the degradation operator for the denoising task and scaled the projection difference H † (Hx 0|t -y) with coefficient Σ to balance the information from measurement y and denoising output x 0|t . The downsampling operator implemented with torch.nn.AdaptiveAvgPool2d for the super-resolution task. The pseudo-inverse operator H † is I for the denoising task and upsampling operator for the SR task.Comparison with Other Methods. The pseudo-inverse operator H † was used as the baseline method, namely, x * = H † y. We also compared the present method with one commonly used image enhancement method DIP [10] and two recent diffusion model-based methods: IVLR [6], which adopted low-frequency information from measurement y to guide the generation process towards a narrow data manifold, and DPS [7], which addressed the intractability of posterior sampling through Laplacian approximation. Notably, DPS used 1000 sampling steps while we only used 100 sampling steps. "
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,4,Results and Discussion,"Low-Dose CT Image Enhancement. The presented method outperformed all other methods on the denoising task in all metrics, as shown in Table 1, with average PSNR, SSIM, and VIF of 28.3, 0.803, and 0.510, respectively. Supplementary Fig. 1 (a) visually compares the denoising results, showing that the presented method effectively removes the noise and preserves the anatomical details, while other methods either fail to suppress the noise or result in loss of tissue information. We also used the same pre-trained diffusion model for simultaneously denoising and SR by setting H as the downsampling operator. Our method still achieves better performance across all metrics as shown in Table 2. By visually comparing the enhancement results in Fig. 1 (b) and (c), our results can reconstruct more anatomical details even in the challenging noisy 8× SR task. In contrast, DIP tends to smooth tissues and ILVR and DPS fail to recover the tiny structures such as liver vessels in the zoom-in regions. MR Image Enhancement. To demonstrate the generality of the presented method, we also applied it for the heart MR image 4× and 8× SR tasks, and the quantitative results are presented in Table 3. Our results still outperformed IVLR and DPS in all metrics. DIP obtains slightly better scores in PSNR for the 4× SR task and PSNR and SSIM for the 8× SR tasks, but visualized image quality is significantly worse than our results as shown in Supplementary Fig. 2, e.g., many anatomical structures are smoothed. This is because perceptual and distortion qualities are in opposition to each other as theoretically proven in [2]. DIP mainly prioritizes the distortion measures for the noise-free SR tasks while our results achieve a better trade-off between the perceptual and distortion quality."
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,5,Conclusion,"In summary, we have provided two well-trained diffusion models for abdomen CT and heart MR, and introduced a plug-and-play framework for image enhancement. Our experiments have demonstrated that a single pre-trained diffusion model could address different degradation levels without customized models. However, there are still some limitations to be solved. The degradation operator and its pseudo-inverse should be explicitly given, which limits its application in tasks such as heart MR motion deblurring. Although the present method is in general applicable for 3D images, training the 3D diffusion model still remains prohibitively expensive. Moreover, the sampling process currently requires multiple network inferences, but it could be solved with recent advances in one-step generation models [15] and faster algorithms [16]. Despite these limitations, the versatile and scalable nature of the presented paradigm has great potential to revolutionize medical image enhancement tasks. Future work could focus on developing more efficient algorithms for 3D diffusion models and expanding this paradigm to more clinical applications such as low-dose PET denoising."
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,,Fig. 1 .,
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,,,
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,,Table 1 .,
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,,Table 2 .,
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,,Table 3 .,
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_1.
Robust T-Loss for Medical Image Segmentation,1,Introduction,"Convolutional Neural Networks (CNNs) and Visual Transformers (ViTs) have become the standard in semantic segmentation, achieving state-of-the-art results in many applications [1,16,24]. However, supervised training of CNNs and ViTs requires large amounts of annotated data, where each pixel in the image is labeled with the category it belongs to. In the medical domain, obtaining these annotations can be costly and time-consuming as it requires expertise and domain knowledge that is often scarcely available [6]. In addition, medical image annotations can be affected by human bias and poor inter-annotator agreement [23], further complicating the process. Despite efforts to obtain labels through automated mining [31] and crowd-sourcing methods [11], the quality of datasets gathered using these methods remains challenging due to often high levels of label noise.For instance, the Fitzpatrick 17k dataset, commonly used in dermatology research, contains non-skin images and noisy annotations. In a random sample of 504 images, 5.4% were labeled incorrectly or as other classes [10]. The dataset was scraped from online atlases, which makes it vulnerable to inaccuracies and noise [10]. Noisy labels are and will continue to be, a problem in medical datasets. This is a concern as label noise has been shown to decrease the accuracy of supervised models [20,22,35], making it a key area of focus for both research and practical applications.Previous literature has explored many methods to mitigate the problem of noisy labels in deep learning. These methods can be broadly categorized into label correction [27,28,32], loss function correction based on an estimated noise transition matrix [21,29,33], and robust loss functions [2,18,30,34]. Compared to the first two approaches, which may suffer from inaccurate estimates of the noise transition matrix, robust loss functions enable joint optimization of model parameters and variables related to the noise model and have shown promising results in classification tasks [8,34]. Despite these advances, semantic segmentation with noisy labels is relatively understudied. Existing research in this area has focused on the development of noise-resistant network architectures [15], the incorporation of domain-specific prior knowledge [29], or more recent strategies that update the noisy masks before memorization [17].Although previous methods have shown robustness in semantic segmentation, they often have limitations, such as more hyper-parameters, modifications to the network architecture, or complex training procedures. In contrast, robust loss functions offer a much simpler solution as they could be incorporated with a simple change in a single modeling component. However, their effectiveness has not been thoroughly investigated.In this work, we show that several traditional robust loss functions are vulnerable to memorizing noisy labels. To overcome this problem, we introduce a novel robust loss function, the T-Loss, which is inspired by the negative loglikelihood of the Student-t distribution. The T-Loss, whose simplest formulation features a single parameter, can adaptively learn an optimal tolerance level to label noise directly during backpropagation, eliminating the need for additional computations such as the Expectation Maximization (EM) steps.To evaluate the effectiveness of the T-Loss as a robust loss function for medical semantic segmentation, we conducted experiments on two widely-used benchmark datasets in the field: one for skin lesion segmentation and the other for lung segmentation. We injected different levels of noise into these datasets that simulate typical human labeling errors and trained deep learning models using various robust loss functions. Our experiments demonstrate that the T-Loss outperforms these robust state-of-the-art loss functions in terms of segmentation accuracy and robustness, particularly under conditions of high noise contamination. We also observed that the T-Loss could adaptively learn the optimal tolerance level to label noise which significantly reduces the risk of memorizing noisy labels.This research is divided as follows: Sect. 2 introduces the motivation behind our T-Loss and provides its mathematical derivation. Section 3 covers the datasets used in our experiments, the implementation and training details of T-Loss, and the metrics used for comparison. Section 4 presents the main findings of our study, including the results of the T-Loss and the baselines on both datasets and an ablation study on the parameter of T-Loss. Finally, in Sect. 5, we summarize our contributions and the significance of our study for the field."
Robust T-Loss for Medical Image Segmentation,2,Methodology,"Let x i ∈ R c×w×h be an input image and y i ∈ {0, 1} w×h be its noisy annotated binary segmentation mask, where c represents the number of channels, w the image's width, and h its height. Given a set of images {x 1 , . . . , x N } and corresponding masks {y 1 , . . . , y N }, our goal is to train a model f w with parameters w such that f w (x) approximates the accurate binary segmentation mask for any given image x.To this end we note that, heuristically, assuming error terms to follow a Student-t distribution (as suggested e.g. in [19]) allows for significantly larger noise tolerance with respect to the usual gaussian form. Recall that the Student-t distribution for a D-dimensional variable y is defined by the Probability Density Function (PDF)where µ and Σ are respectively the mean and the covariance matrix of the associated multivariate normal distribution, ν is the number of degrees of freedom, and | • | indicates the determinant (see e.g. [3]). From this expression, we see that the tails of the Student-t distribution follow a power law that is indeed heavier compared to the usual negative quadratic exponential. For this reason, it is well known to be robust to outliers [7,26].Since the common Mean Squared Error (MSE) loss is derived by minimizing the negative log-likelihood of the normal distribution, we choose to apply the same transformation and getThe functional form of our loss function for one image is then obtained with the identification y = y i and the approximation µ = f w (x i ), and aggregated withEquation ( 2) has D(D +1)/2 free parameters in the covariance matrix, which should be estimated from the data. In the case of images, this can easily be in the order of 10 4 or larger, which makes a general computation highly non-trivial and may deteriorate the generalization capabilities of the model. For these reasons, we take Σ to be the identity matrix I D , despite knowing that pixel annotations in an image are not independent. The loss term for one image simplifies toTo clarify the relation with known loss functions, let δ = |y i -f w (x i )|, and fix the value of ν. For δ → 0, the functional dependence from δ reduces to a linear function of δ 2 , i.e. MSE. For large values of δ, though, Eq. ( 4) is equivalent to log δ, thus penalizing large deviations even less than the much-advocated robust Mean Absolute Error (MAE). The scale of this transition, the sensitivity to outliers, is regulated by the parameter ν.We optimize the parameter ν jointly with w using gradient descent. To this end, we reparametrize ν = e ν + where is a safeguard for numerical stability. Loss functions with similar dynamic tolerance parameters were also studied in [2] in the context of regression, where using the Student-t distribution is only mentioned in passing."
Robust T-Loss for Medical Image Segmentation,3,Experiments,"In this section, we demonstrate the robustness of the T-Loss for segmentation tasks on two public image collections from different medical modalities, namely ISIC [5] and Shenzhen [4,13,25]. In line with the literature, we use simulated label noise in our tests, as no public benchmark with real label noise exists [15]."
Robust T-Loss for Medical Image Segmentation,3.1,Datasets,"The ISIC 2017 dataset [5] is a well-known public benchmark of dermoscopy images for skin cancer detection. It contains 2000 training and 600 test images with corresponding lesion boundary masks. The images are annotated with lesion type, diagnosis, and anatomical location metadata. The dataset also includes a list of lesion attributes, such as size, shape, and color. We resized the images to 256 × 256 pixels for our experiments.Shenzhen [4,13,25] is a public dataset containing 566 frontal chest radiographs with corresponding lung segmentation masks for tuberculosis detection. Since there is not a predefined split for Shenzhen as in ISIC, to ensure representative training and testing sets, we stratified the images by their tuberculosis and normal lung labels, with 70% of the data for training and the remaining 30% for testing. Resulting in 296 training images and 170 test images. All images were resized to 256 × 256 pixels.Without a public benchmark with real noisy and clean segmentation masks, we artificially inject additional mask noise in these two datasets to test the model's robustness to low annotation quality. This simulates the real risk of errors due to factors like annotator fatigue and difficulty in annotating certain images. In particular, we follow [15], randomly sample a portion of the training data with probability α ∈ {0.3, 0.5, 0.7}, and apply morphological transformations with noise levels controlled by β ∈ {0.5, 0.7}1 . The morphological transformations included erosion, dilation, and affine transformations, which respectively reduced, enlarged, and displaced the annotated area."
Robust T-Loss for Medical Image Segmentation,3.2,Setup,"We train a nnU-Net [12] as a segmentation network from scratch. To increase variations in the training data, we augment them with random mirroring, flipping, and gamma transformations. The T-loss was initialized with ν = 0 and = 10 -8 . The nnU-Net was trained for 100 epochs using the Adam optimizer with a learning rate of 10 -3 and a batch size of 16 for the ISIC dataset and 8 for the Shenzhen dataset. The network was trained on a single NVIDIA Tesla V100 with 32 GB of memory.The model is trained using noisy masks. However, by using the ground truth for the corresponding noisy mask, we can evaluate the robustness of the model and measure noisy-label memorization. This is done by analyzing the dice score of the model's prediction compared to the actual ground truth.In addition to the T-Loss, we train several other losses for comparison. Our analysis includes some traditional robust losses, such as Mean Absolute Error (MAE), Reverse Cross Entropy (RCE), Normalized Cross Entropy (NCE), and Normalized Generalized Cross Entropy (NGCE), as well as more recent methods, such as Generalized Cross Entropy (GCE) [34], Symmetrical Cross Entropy (SCE) [30], and Active-Passive Loss (APL) [18]. For APL, in particular, we consider three combinations: 1) NCE+RCE, 2) NGCE+MAE, and 3) NGCE+RCE. We consider the mean of the predictions for the last 10 epochs with a fixed number of epochs and report its mean and standard deviation over 3 different random seeds.Finally, we complete our evaluation with statistical significance tests. We use the ANOVA test [9] to compare the differences between the means of the dice scores and obtain a p-value. In addition, if the difference is significant, we perform the Tukey post-hoc test [14] to determine which means are different. We assume statistical significance for p-values of less than p = 0.05 and denote this with a ."
Robust T-Loss for Medical Image Segmentation,4,Results,
Robust T-Loss for Medical Image Segmentation,4.1,Results on the ISIC Dataset,"We present experimental results for the skin lesion segmentation task on the ISIC dataset in Table 1. Our results show that conventional losses perform well with no noise or under low noise levels, but their performance decreases significantly with increasing noise levels due to the memorization of noisy labels. This can be observed from the training dice scores in Fig. 1, where traditional robust losses overfit data in later stages of learning while metrics for the T-Loss do not deteriorate. Our method achieves a dice score of 0.788 ± 0.007 even for the most extreme noise scenario under exam. Examples of the obtained masks can be seen in the supplementary material.  T-Loss (Ours) 0.825(5) 0.809(6) 0.804(5) 0.800(11) 0.790(5) 0.788(7) 0.761( 6)  "
Robust T-Loss for Medical Image Segmentation,4.2,Results on the Shenzhen Dataset,"The results of lung segmentation for the Shenzhen test set are reported in Table 2. Similar to the ISIC dataset, all considered robust losses perform well at low noise levels. However, as the noise level increases, their dice scores deteriorate. On the other hand, the T-Loss stands out by consistently achieving the highest dice score, even in the most challenging scenarios. The statistical test results also support this claim, with the T-Loss being significantly superior to the other methods."
Robust T-Loss for Medical Image Segmentation,4.3,Dynamic Tolerance to Noise,"The value of ν is crucial for the model's performance, as it controls the sensitivity to label noise. To shed light on this mechanism, we study the behavior of ν during training for different label noise levels and initializations on the ISIC dataset. As seen in Fig. 2, ν dynamically adjusts annotation noise tolerance in the early stages of training, independently of its initial value. The plots demonstrate that T-Loss (Ours) 0.949(1) 0.948(1) 0.939(1) 0.914(5) 0.904(8) 0.896(7) 0.870 (31) ν clearly converges to a stable solution during training, with initializations far from this solution only mildly prolonging the time needed for convergence and having no significant effect on the final dice score."
Robust T-Loss for Medical Image Segmentation,5,Conclusions,"In this contribution, we introduced the T-Loss, a loss function based on the negative log-likelihood of the Student-t distribution. The T-Loss offers the great advantage of controlling sensitivity to outliers through a single parameter that is dynamically optimized. Our evaluation on public medical datasets for skin lesion and lung segmentation demonstrates that the T-Loss outperforms other robust losses by a statistically significant margin. While other robust losses are vulnerable to noise memorization for high noise levels, the T-Loss can reabsorb this form of overfitting into the tolerance level ν. Our loss function also features remarkable independence to different noise types and levels.It should be noted that other methods, such as [15] offer better performance for segmentation on the ISIC dataset with the same synthetic noisy labels, while the T-Loss offers a simple alternative. The trade-off in terms of performance, computational cost, and ease of adaption to different scenarios remains to be investigated. Similarly, combinations of the T-Loss with superpixels and/or iterative label refinement procedures are still to be explored.The T-Loss provides a robust solution for binary segmentation of medical images in the presence of high levels of annotation noise, as frequently met in practice e.g. due to annotator fatigue or inter-annotator disagreements. This may be a key feature in achieving good generalization in many medical image segmentation applications, such as clinical decision support systems. Our evaluations and analyses provide evidence that the T-Loss is a reliable and valuable tool in the field of medical image analysis, with the potential for broader application in other domains."
Robust T-Loss for Medical Image Segmentation,6,Data Use Declaration and Acknowledgment,"We declare that we have used the ISIC dataset [5] under the Apache License 2.0, publicly available, and the Shenzhen dataset [4,13,25] public available under the CC BY-NC-SA 4.0 License."
Robust T-Loss for Medical Image Segmentation,,Table 1 .,
Robust T-Loss for Medical Image Segmentation,,Fig. 1 .,
Robust T-Loss for Medical Image Segmentation,,Fig. 2 .,
Robust T-Loss for Medical Image Segmentation,,Table 2 .,
Robust T-Loss for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 68.
Multi-Head Multi-Loss Model Calibration,1,Introduction and Related Work,"When training supervised computer vision models, we typically focus on improving their predictive performance, yet equally important for safety-critical tasks is their ability to express meaningful uncertainties about their own predictions [4]. In the context of machine learning, we often distinguish two types of uncertainties: epistemic and aleatoric [13]. Briefly speaking, epistemic uncertainty arises from imperfect knowledge of the model about the problem it is trained to solve, whereas aleatoric uncertainty describes ignorance regarding the data used for learning and making predictions. For example, if a classifier has learned to predict the presence of cancerous tissue on a colon histopathology, and it is tasked with making a prediction on a breast biopsy it may display epistemic uncertainty, as it was never trained for this problem [21]. Nonetheless, if we ask the model about a colon biopsy with ambiguous visual content, i.e. a hard-todiagnose image, then it could express aleatoric uncertainty, as it may not know how to solve the problem, but the ambiguity comes from the data. This distinction between epistemic and aleatoric is often blurry, because the presence of one of them does not imply the absence of the other [12]. Also, under strong epistemic uncertainty, aleatoric uncertainty estimates can become unreliable [31].Producing good uncertainty estimates can be useful, e.g. to identify test samples where the model predicts with little confidence and which should be reviewed [1]. A straightforward way to report uncertainty estimates is by interpreting the output of a model (maximum of its softmax probabilities) as its predictive confidence. When this confidence aligns with the actual accuracy we say that the model is calibrated [8]. Model calibration has been studied for a long time, with roots going back to the weather forecasting field [3]. Initially applied mostly for binary classification systems [7], the realization that modern neural networks tend to predict over-confidently [10] has led to a surge of interest in recent years [8]. Broadly speaking, one can attempt to promote calibration during training, by means of a post-processing stage, or by model ensembling.Training-Time Calibration. Popular training-time approaches consist of reducing the predictive entropy by means of regularization [11], e.g. Label Smoothing [25] or MixUp [30], or loss functions that smooth predictions [26]. These techniques often rely on correctly tuning a hyper-parameter controlling the trade-off between discrimination ability and confidence, and can easily achieve better calibration at the expense of decreasing predictive performance [22]. Examples of medical image analysis works adopting this approach are Difference between Confidence and Accuracy regularization [20] for medical image diagnosis, or Spatially-Varying and Margin-Based Label Smoothing [14,27], which extend and improve Label Smoothing for biomedical image segmentation tasks.Post-Hoc Calibration. Post-hoc calibration techniques like Temperature Scaling [10] and its variants [6,15] have been proposed to correct over or underconfident predictions by applying simple monotone mappings (fitted on a heldout subset of the training data) on the output probabilities of the model. Their greatest shortcoming is the dependence on the i.i.d. assumption implicitly made when using validation data to learn the mapping: these approaches suffer to generalize to unseen data [28]. Other than that, these techniques can be combined with training-time methods and return compounded performance improvements.Model Ensembling. A third approach to improve calibration is to aggregate the output of several models, which are trained beforehand so that they have some diversity in their predictions [5]. In deep learning, model ensembles are considered to be the most successful method to generate meaningful uncertainty estimates [16]. An obvious weakness of deep ensembles is the requirement of training and then keeping for inference purposes a set of models, which results in a computational overhead that can be considerable for larger architectures. Examples of applying ensembling in medical image computing include [17,24].In this work we achieve model calibration by means of multi-head models trained with diverse loss functions. In this sense, our approach is closest to some recent works on multi-output architectures like [21], where a multi-branch CNN is trained on histopathological data, enforcing specialization of the different heads by backpropagating gradients through branches with the lowest loss. Compared to our approach, ensuring correct gradient flow to avoid dead heads requires ad-hoc computational tricks [21]; in addition, no analysis on model calibration on in-domain data or aleatoric uncertainty was developed, focusing instead on anomaly detection. Our main contribution is a multi-head model that I) exploits multi-loss diversity to achieve greater confidence calibration than other learning-based methods, while II) avoiding the use of training data to learn post-processing mappings as most post-hoc calibration methods do, and III) sidesteping the computation overhead of deep ensembles."
Multi-Head Multi-Loss Model Calibration,2,Calibrated Multi-Head Models,"In this section we formally introduce multi-head models [19], and justify the need for enforcing diversity on them. Detailed derivations of all the results below are provided in the online supplementary materials."
Multi-Head Multi-Loss Model Calibration,2.1,Multi-Head Ensemble Diversity,"Consider a K-class classification problem, and a neural network U θ taking an image x and mapping it onto a representation U θ (x) ∈ R N , which is linearly transformed by f into a logits vector z = f (U θ (x)) ∈ R K . This is then mapped into a vector of probabilities p ∈ [0, 1] K by a softmax operation p = σ(z), where p j = e zj / i e zi . If the label of x was y ∈ {1, ..., K}, we can measure the error associated to prediction p with the cross-entropy loss L CE (p, y) = -log(p y ).We now wish to implement a multi-head ensemble model like the one shown in Fig. 1. For this, we replace f by M different branches f 1 , ..., f M , each of them still taking the same input but mapping it to different logits z m = f m (U θ (x)). The resulting probability vectors p m = σ(z m ) are then averaged to obtain a final prediction p μ = (1/M ) m p m . We are interested in backpropagating the loss L CE (p μ , y) = -log(p μ y ) to find the gradient at each branch, ∇ z m L CE (p μ , y). Property 1: For the M-head classifier in Fig. 1, the derivative of the crossentropy loss at head f m with respect to z m is given by where y is a one-hot representation of the label y.From Eq. (1) we see that the gradient in branch m will be scaled depending on how much probability mass p m y is placed by f m on the correct class relative to the total mass placed by all heads. In other words, if every head learned to produce a similar prediction (not necessarily correct) for a particular sample, then the optimization process of this network would result in the same updates for all of them. As a consequence, diversity in the predictions that make up the output p μ of the network would be damaged."
Multi-Head Multi-Loss Model Calibration,2.2,Multi-Head Multi Loss Models,"In view of the above, one way to obtain more diverse gradient updates in a multihead model during training could be to supervise each head with a different loss function. To this end, we will apply the weighted cross-entropy loss, given by L ω -CE (p, y) = -ω y log(p μ y ), where ω ∈ R K is a weight vector. In our case, we assign to each head a different weight vector ω m (as detailed below), in such a way that a different loss function L ω m -CE will supervise the intermediate output of each branch f m , similar to deep supervision strategies [18] but enforcing diversity. The total loss of the complete model is the addition of the per-head losses and the overall loss acting on the average prediction:where p = (p 1 , ..., p M ) is an array collecting all the predictions the network makes. Since L ω -CE results from just multiplying by a constant factor the conventional CE loss, we can readily calculate the gradient of L MH at each branch.Property 2: For the Multi-Loss Multi-Head classifier shown in Fig. 1, the gradient of the Multi-Head loss L MH at branch f m is given by:Note that having equal weight vectors in all branches fails to break the symmetry in the scenario of all heads making similar predictions. Indeed, if for any two given heads f mi , f mj we have ω mi = ω mj and p mi ≈ p mj , i.e. p m ≈ p μ ∀m, then the difference in norm of the gradients of two heads would be:It follows that we indeed require a different weight in each branch. In this work, we design a weighting scheme to enforce the specialization of each head into a particular subset of the categories {c 1 , ..., c K } in the training set. We first assume that the multi-head model has less branches than the number of classes in our problem, i.e. M ≤ K, as otherwise we would need to have different branches specializing in the same category. In order to construct the weight vector ω m , we associate to branch f m a subset of N/K categories, randomly selected, for specialization, and these are weighed with ω m j = K. Then, the remaining categories in ω m receive a weight of ω m j = 1/K. For example, in a problem with 4 categories and 2 branches, we could haveIf N is not divisible by K, the reminder categories are assigned for specialization to random branches."
Multi-Head Multi-Loss Model Calibration,2.3,Model Evaluation,"When measuring model calibration, the standard approach relies on observing the test set accuracy at different confidence bands B. For example, taking all test samples that are predicted with a confidence around c = 0.8, a well-calibrated classifier would show an accuracy of approximately 80% in this test subset. This can be quantified by the Expected Calibration Error (ECE), given by:where s B s form a uniform partition of the unit interval, and acc(B s ), conf(B s ) are accuracy and average confidence (maximum softmax value) for test samples predicted with confidence in B s .In practice, the ECE alone is not a good measure in terms of practical usability, as one can have a perfectly ECE-calibrated model with no predictive power [29]. A binary classifier in a balanced dataset, randomly predicting always one class with c = 0.5 + confidence, has a perfect calibration and 50% accuracy. Proper Scoring Rules like Negative Log-Likelihood (NLL) or the Brier score are alternative uncertainty quality metrics [9] that capture both discrimination ability and calibration: a model must be both accurate and calibrated to achieve a low PSR value. We report NLL, and also standard Accuracy, which contrary to ECE can be high even for badly-calibrated models. Finally, we show as summary metric the average rank when aggregating rankings of ECE, NLL, and accuracy."
Multi-Head Multi-Loss Model Calibration,3,Experimental Results,"We now describe the data we used for experimentation, carefully analyze performance for each dataset, and end up with a discussion of our findings."
Multi-Head Multi-Loss Model Calibration,3.1,Datasets and Architectures,"We conducted experiments on two datasets: 1) the Chaoyang dataset1 , which contains colon histopathology images. It has 6,160 images unevenly distributed in 4 classes (29%, 19%, 37%, 15%), with some amount of label ambiguity, reflecting high aleatoric uncertainty. As a consequence, the best model in the original reference [32], applying specific techniques to deal with label noise, achieved an accuracy of 83.4%. 2) Kvasir2 , a dataset for the task of endoscopic image classification. The annotated part of this dataset contains 10,662 images, and it represents a challenging classification problem due a high amount of classes (23) and highly imbalanced class frequencies [2]. For the sake of readability we do not show measures of dispersion, but we add them to the supplementary material (Appendix B), together with further experiments on other datasets.We implement the proposed approach by optimizing several popular neural network architectures, namely a common ResNet50 and two more recent models: a ConvNeXt [23] and a Swin-Transformer [23]. All models are trained for 50 epochs, which was observed enough for convergence, using Stochastic Gradient Descent with a learning rate of l = 1e-2. Code to reproduce our results and hyperparameter specifications are shared at https://github.com/agaldran/ mhml_calibration."
Multi-Head Multi-Loss Model Calibration,3.2,Performance Analysis,"Notation: We train three different multi-head classifiers: 1) a 2-head model where each head optimizes for standard (unweighted) CE, referred to as 2HSL (2 Heads-Single Loss); 2) a 2-head model but with each head minimizing a differently weighed CE loss as described in Sect. 2.2. We call this model 2HML (2 Heads-Multi Loss)); 3) Finally, we increase the number of heads to four, and we refer to this model as 4HML. For comparison, we include a standard singleloss one-head classifier (SL1H), plus models trained with Label Smoothing (LS [25]), Margin-based Label Smoothing (MbLS [22]), MixUp [30], and using the DCA loss [20]. We also show the performance of Deep Ensembles (D-Ens [16]). We analyze the impact of Temperature Scaling [10] in Appendix A.What we expect to see: Multi-Head Multi-Loss models should achieve a better calibration (low ECE) than other learning-based methods, ideally approaching Deep Ensembles calibration. We also expect to achieve good calibration without sacrificing predictive performance (high accuracy). Both goals would be reflected jointly by a low NLL value, and by a better aggregated ranking. Finally we would ideally observe improved performance as we increase the diversity (comparing 2HSL to 2HML) and as we add heads (comparing 2HML to 4HML).Chaoyang: In Table 1 we report the results on the Chaoyang dataset. Overall, accuracy is relatively low, since this dataset is challenging due to label ambiguity, and therefore calibration analysis of aleatoric uncertainty becomes meaningful here. As expected, we see how Deep Ensembles are the most accurate method, also with the lowest NLL, for two out of the three considered networks. However, we also observe noticeable differences between other learning-based calibration techniques and multi-head architectures. Namely, all other calibration methods achieve lower ECE than the baseline (SL1H) model, but at the cost of a reduced accuracy. This is actually captured by NLL and rank, which become much higher for these approaches. In contrast, 4HML achieves the second rank in two architectures, only behind Deep Ensembles when using a ResNet50 and a Swin-Transformer, and above any other 2HML with a ConvNeXt, even outperforming Deep Ensembles in this case. Overall, we can see a pattern: multi-loss multi-head models appear to be extremely well-calibrated (low ECE and NLL values) without sacrificing accuracy, and as we diversify the losses and increase the number of heads we tend to improve calibration. Kvasir: Next, we show in Table 2 results for the Kvasir dataset. Deep Ensembles again reach the highest accuracy and excellent calibration. Interestingly, methods that smooth labels (LS, MbLS, MixUP) show a strong degradation in calibration and their ECE is often twice the ECE of the baseline SL1H model. We attribute this to class imbalance and the large number of categories: smoothing labels might be ineffective in this scenario. Note that models minimizing the DCA loss do manage to bring the ECE down, although by giving up accuracy. In contrast, all multi-head models improve calibration while maintaining accuracy. Remarkably, 4HML obtains lower ECE than Deep Ensembles in all cases. Also, for two out of the three architectures 4HML ranks as the best method, and for the other one 2HML reaches the best ranking."
Multi-Head Multi-Loss Model Calibration,4,Conclusion,"Multi-Head Multi-Loss networks are classifiers with enhanced calibration and no degradation of predictive performance when compared to their single-head counterparts. This is achieved by simultaneously optimizing several output branches, each one minimizing a differently weighted Cross-Entropy loss. Weights are complementary, ensuring that each branch is rewarded for becoming specialized in a subset of the original data categories. Comprehensive experiments on two challenging datasets with three different neural networks show that Multi-Head Multi-Loss models consistently outperform other learning-based calibration techniques, matching and sometimes surpassing the calibration of Deep Ensembles."
Multi-Head Multi-Loss Model Calibration,,Fig. 1 .,
Multi-Head Multi-Loss Model Calibration,,Table 1 .,
Multi-Head Multi-Loss Model Calibration,,Table 2 .,
Multi-Head Multi-Loss Model Calibration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_11.
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,1,Introduction,"Federated learning (FL) [15] is an emerging decentralized learning paradigm that enables multiple parties to collaboratively train a model without sharing private data. FL was initially developed for edge devices, and it has been extended to medical image analysis to protect clinical data [2,6,12,21]. Non-identically independently distributed (Non-IID) data among clients is one of the most frequently stated problems with FL [5,[7][8][9]. However, most studies of non-IID FL assumed that each client owns an identical label set, which does not reflect real-world scenarios where classes of interest could vary among clients. In the medical field, for instance, datasets from different centers (e.g., hospitals) are generally annotated based on their respective domains or interests. As a result, the label sets of different centers can be non-identical, which we refer to as label set mismatch.To this end, we propose to solve this challenging yet common scenario where each client holds different or even disjoint annotated label sets. We consider not only single-label classification but also multi-label classification where partial labels exist, making this problem setting more general and challenging. Specifically, each client has data of locally identified classes and locally unknown classes. Although locally identified classes differ among clients, the union of identified classes in all clients covers locally unknown classes in each client (Fig. 1). There are few studies directly related to the label set mismatch scenario. The previous attempts were either limited in scope or have not achieved satisfactory results. For instance, FedRS [10] assumed that each client only owns locally identified classes. FedPU [11] assumed that each client owns labels of locally identified classes and unlabeled data of all classes but it was not applicable to multi-label classification. FPSL [2] was designed for federated partially supervised learning which only targets multi-label classification. Over and above that, FedRS and FedPU tried to solve this problem only through local updating and ignored the server aggregation process in FL, leading to unsatisfactory performance. FPSL used bi-level optimization in the local training, which is only effective when the data is very limited. Federated semi-supervised learning (FedSemi) [1,4,6,12,22] is another related field, but almost all of them assumed that some annotated clients [1,4,12] or a server [4,6,22] own labels of all classes. However, in real-world scenarios, especially in the healthcare domain, each client may only annotate data of specific classes within their domains and interests.In this paper, we present FedLSM, a framework that aims to solve Label Set Mismatch and is designed for both single-label and multi-label classification tasks. FedLSM relies on pseudo labeling on unlabeled or partially labeled samples, but pseudo labeling methods could lead to incorrect pseudo labels and ignorance of samples with relatively lower confidence. To address these issues, we evaluate the uncertainty of each sample using entropy and conduct pseudo labeling only on data with relatively lower uncertainty. We also apply MixUp [23] between data with low and high uncertainty and propose an adaptive weighted averaging for the classification layer that considers the client class-wise data numbers. We validated our propose method on two real-world tasks, including Chest X-ray (CXR) [13,14] diagnosis (multi-label classification) and skin lesion diagnosis [1] (single-label classification). Extensive experiments demonstrate that our method outperforms a number of state-of-the-art FL methods, holding promise in tackling the label set mismatch problem under federated learning. We followed the common FL scenario, where there are K clients and one central server. Each client owns a locally-identified class set I k and a locally-unknown class set U k . Although I k and U k can vary among clients and may even be disjoint, all clients share an identical class set as:"
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,2,Methodology,
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,2.1,Problem Setting,"(Despite the fact that each client only identifies a subset of the class set C, the union of locally identified class sets equals C, which can be formulated as:(The local dataset of client k is denoted as, where n k denotes the number of data, x i is the i -th input image, andFor subsequent illustration, we denote the backbone model as, where f θ (•) refers to the feature extractor with parameters θ and f Ψ refers to the last classification layer with parameters Ψ = {ψ c } M c=1 . Adopting the terminology from previous studies [10,17], we refer to Ψ as proxies and ψ c as c-th proxy."
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,2.2,Overview of FedLSM,"Our proposed framework is presented in Fig. 2. As depicted in Fig. 2(a), we evaluate the uncertainty of each sample in the local dataset D k using the global model f (•) and split it into three subsets based on the uncertainty level. The low and medium uncertainty subsets are used for pseudo labeling-based training, while the low and high uncertainty subsets are combined by MixUp [23] to efficiently utilize uncertain data that might be ignored in pseudo labeling. The estimated disease distribution q k c on client k is calculated using the existing labels and pseudo labels. After local training, each client sends its estimated disease distribution q k and model weight f k (•) to the central server. The feature extractors Θ are aggregated using FedAvg [15] while proxies Ψ are aggregated using our proposed adaptive weighted averaging with the help of q k ."
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,2.3,Local Model Training Uncertainty Estimation (UE).,"We use the global model to evaluate the uncertainty of each sample in the local dataset by calculating its entropy [16]. The calculation of entropy in single-label classification is defined as:where P (y = c|x i ) refers to the predicted probability for a given class c and input x i . The calculation of entropy in multi-label classification is similar to . We use the weakly-augmented version (i.e., slightly modified via rotations, shifts, or flips) of x i to generate pseudo labels on locally unknown classes by the teacher model. The loss L I k applied on locally identified class set I k is cross-entropy, and the loss applied on the locally unknown class set U k of k -th client in single-label classification can be formulated as:where where τ p and τ n are the confidence threshold for positive and negative labels, N C is the number of data."
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Uncertain Data Enhancing (UDE).,"The pseudo label filtering mechanism makes it difficult to acquire pseudo-labels for uncertain data, which results in their inability to contribute to the training process. To overcome this limitation, we propose to MixUp [23] dataset with lowest entropy (confident) D l k and dataset with highest entropy as (uncertain) D h k to generate softer label ỹ and input x as, where x l ∈ D l k and x h ∈ D h k , and y l and y h are their corresponding labels or pseudo labels, respectively. We generate pseudo labels for uncertain data (x h , y h ) with a relatively smaller confidence threshold. The UDE loss function L UDE is cross-entropy loss."
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Overall Loss Function. The complete loss function is defined as:,where λ is a hyperparameter to balance different objectives.
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,2.4,Server Model Aggregation,"After local training, the server will collect all the client models and aggregate them into a global model. In the r -th round, the aggregation of the feature extractors {θ k } K k=1 is given by:Adaptive Weighted Proxy Aggregation (AWPA). As analyzed in [10], due to the missing labels of locally unknown class set U k on k -th client, the corresponding proxies {ψ k,c } c∈U k are inaccurate and will further cause error accumulation during model aggregation. FedRS [10] and FedPU [11] both seek to solve this problem only through local training while we use pseudo labels and the existing labels to indicate the contribution of aggregation of proxies as: (7) where q k c refers to the number of training data of the c-th class on the k -th client. During training, if c ∈ U k , q k c is estimated by the number of pseudo labels asThe weighting number of each client is modulated in an adaptive way through the pseudo labeling process in each round."
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,3,Experiments,
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,3.1,Datasets,"We evaluated our method on two real-world medical image classification tasks, including the CXR diagnosis (multi-label) and skin lesion diagnosis (singlelabel).Task 1) NIH CXR Diagnosis. We conducted CXR diagnosis with NIH ChestX-ray14 dataset [20], which contains 112,120 frontal-view CXR images from 32,717 patients. NIH CXR diagnosis is a multi-label classification task and each image is annotated with 14 possible abnormalities (positive or negative).Task 2) ISIC2018 Skin Lesion Diagnosis. We conducted skin lesion diagnosis with HAM10000 [19], which contains 10,015 dermoscopy images. ISIC2018 skin lesion diagnosis is a single-label multi-class classification task where seven exclusive skin lesion sub-types are considered.Training, validation and testing sets for both datasets were divided into 7:1:2."
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,3.2,Experiment Setup,"FL Setting. We randomly divided the training set into k client training sets and randomly select s classes as locally identified classes on each client. We set the number of clients k = 8, the number of classes s = 3 for Task 1 and k = 5, s = 3 for Task 2. Please find the detail of the datasets in the supplementary materials. Data Augmentation and Preprocessing. The images in Task 1 were resized to 320×320, while in Task 2, they were resized to 224×224. For all experiments, weak augmentation refered to horizontal flip, and strong augmentation included a combination of random flip, rotation, translation, scaling and one of the blur transformations in gaussian blur, gaussian noise and median blur. Evaluation Metrics. For Task 1, we adopted AUC to evaluate the performance of each disease. For task 2, we reported macro average of AUC, Accuracy, F1, Precision and Recall of each disease. All the results are averaged over 3 runs. Implementation Details. We used DenseNet121 [3] as the backbone for all the tasks. The network was optimized by Adam optimizer where the momentum terms were set to 0.9 and 0.99. The total batch size was 64 with 4 generated samples using UDE. In task 1, we used the weighted binary cross-entropy as in FPSL [2]. The local training iterations were 200 and 30 for Task 1 and Task 2, respectively, while the total communication rounds were 50 for both tasks. Please find more detailed hyperparameter settings in the supplementary material. "
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,3.3,Comparison with State-of-the-Arts and Ablation Study,"We compared our method with recent state-of-the-art (SOTA) non-IID FL methods, including FedProx [8], which applied L 2 regularization, and MOON [7],which introduced contrastive learning. We also compared with other SOTA non-IID FL methods that shared a similar setting with ours, including FedRS [10], which restricted proxy updates of missing classes, FedPU [11], which added a misclassification loss, and FPSL [2], which adopted task-dependent model aggregation. Additionally, we compared with FedSemi methods that can be easily translated into the label set mismatch scenario including FSSL [22] and FedAvg with FixMatch [18]. For our evaluation, we used FedAvg with 100% labeled data as the benchmark and FedAvg trained with the same setting as the lower bound. The quantitative results for the two tasks are presented in Table 1 andTable 2. To ensure a fair comparison, we adopted the task-dependent model aggregation proposed in FPSL [2] in most of the compared FL methods, with the exception of FedRS and FedPU which are specifically designed for the similar scenario with us. Our proposed FedLSM achieves the best performance on almost all metrics. Notably, the improvement over the second-best method is 1.5% for average AUC on Task 1 and 6.1% for F1-score on Task 2. Ablation Study. We conducted ablation studies to assess the effectiveness of the primary components of our FedLSM framework. As depicted in Table 3, the performance drops significantly without UE or UDE. On the other hand, the adoption of AWPA boosts the performance by 0.7% in AUC for Task 1, 3.7% in F1-score, and 5.3% in recall for Task 2. "
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Methods,
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,AUC Accuracy F1 Precision Recall,FedAvg with 100% labeled data 0.977 0.889 0.809 0.743 0.800 FedAvg [15] 0.927 0.810 0.576 0.721 0.622 FedAvg* [15] 0.949 0.817 0.620 0.753 0.597 FedProx* [8] 0.952 0.820 0.630 0.768 0.612 MOON* [7] 0.948 0.826 0.652 0.755 0.620 FedPU [11] 0.927 0.796 0.550 0.699 0.570 FedRS [10] 0.926 0.800 0.577 0.716 0.597 FPSL [2] 0.952 0.825 0.638 0.728 0.613 FedAvg* + FixMatch [18] 0.940 0.789 0.564 0.681 0.541 FSSL* [22] 0 
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,4,Conclusion,"We present an effective framework FedLSM to tackle the issue of label set mismatch in FL. To alleviate the impact of missing labels, we leverage uncertainty estimation to partition the data into distinct uncertainty levels. Then, we apply pseudo labeling to confident data and uncertain data enhancing to uncertain data. In the server aggregation phase, we use adaptive weighted proxies averaging on the classification layer, where averaging weights are dynamically adjusted every round. Our FedLSM demonstrates notable effectiveness in both CXR diagnosis (multilabel classification) and ISIC2018 skin lesion diagnosis (single-label classification) tasks, holding promise in tackling the label set mismatch problem under federated learning."
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Fig. 1 .,
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Fig. 2 .,
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,,
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,,
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Table 1 .,with state-of-the-art methods on NIH CXR diagnosis. (a) FedAvg with 100% labeled data (b) FedAvg[15] (c) FedAvg*[15] (d) FedProx*[8] (e) MOON*[7] (f ) FedRS[10] (g) FPSL[2] (h) FedAvg-FixMatch*[18] (i) FSSL*[22]. * denotes the use of task-dependent model aggregation in FPSL[2].
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Table 2 .,
Scale Federated Learning for Label Set Mismatch in Medical Image Classification,,Table 3 .,
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,1,Introduction,"Medical image segmentation is greatly helpful to diagnosis and auxiliary treatment of diseases. Recently, deep learning methods [5,15] has largely improved the performance of segmentation. However, the success of deep learning methods typically relies on large densely annotated datasets, which require great efforts from domain experts and thus are hard to obtain in clinical applications.To this end, many weakly-supervised segmentation (WSS) methods are developed to alleviate the annotation burden, including image level [9,10], bounding box [7,16,20], scribble [13,23] and even points [1,12]. These methods utilize weak label as supervision signal to train the model and produce segmentation results. Unfortunately, the performance gap between these methods and its corresponding upper bound (i.e., the result of fully-supervised methods) is still large. The main reason is that these annotation methods do not provide the information of object boundaries, which are crucial for segmentation task.A new annotation strategy has been proposed and investigated recently. It is typically referred as sparse annotation and it only requires a few slice of each volume to be labeled. With this annotation way, the exact boundaries of different classes are precisely kept. It shows great potential in reducing the amount of annotation. And its advantage over traditional weak annotations has been validated in previous work [2]. To enlarge the slice difference, we annotate slices from two different planes instead of from a single plane.Most existing methods solve the problem by generating pseudo label through registration. [2] trains the segmentation model through an iterative step between propagating pseudo label and updating segmentation model. [11] adopts meanteacher framework as segmentation model and utilizes registration module to produce pseudo label. [3] proposes a co-training framework to leverage the dense pseudo label and sparse orthogonal annotation. Though achieving remarkable results, the limitation of these methods cannot be ignored. These methods rely heavily on the quality of registration result. When the registration suffers due to many reasons (e.g., small and intricate objects, large variance between adjacent slices), the performance of segmentation models will be largely degraded.Thus, we suggest to view this problem from the perspective of semisupervised segmentation (SSS). Traditional setting of 3D SSS is that there are several volumes with dense annotation and a large number of volumes without any annotation. And now there are voxels with annotation and voxels without annotation in every volume. This actually complies with the idea of SSS, as long as we view the labeled and unlabeled voxels as labeled and unlabeled samples, respectively.SSS methods [19,24] mostly fall into two categories, 1) entropy minimization and 2) consistency regularization. And one of the most popular paradigms is co-training [4,22,26]. Inspired by these co-training methods, we propose our method based on the idea of cross-teaching. As co-training theory conveys, the success of co-training largely lies on the view-difference of different networks [17]. Some works encourage the difference by applying different transformation to each network. [14] directly uses two type of networks (i.e., CNN and transformer) to guarantee the difference. Here we further extend it by adopting networks of different dimensions (i.e., 2D CNN and 3D CNN). 3D network and 2D network work largely differently for 3D network involves the inter-slice information while 2D network only utilize inner-slice information. The 3D network is trained on volume with sparse annotation and we use two 2D networks to learn from slices of two different planes. Thus, the view difference can be well-preserved. However, it is still hard to directly train with the sparse annotation due to limited supervision signal. So we utilize 3D and 2D networks to produce pseudo label to each other. In order to select more credible pseudo label, we specially propose two strategies for the pseudo label selection of 3D network and 2D networks, respectively. For 3D network, simply setting a prediction probability threshold can exclude those voxels with less confidence, which are more likely to be false prediction. However, Some predictions with high quality but low confidence are also excluded. Thus, we estimate the quality of each prediction, and design hard-soft thresholds. If the prediction is of high quality, the voxels that overpass the soft threshold are selected as pseudo label. Otherwise, only the voxels overpassing the hard threshold can be used to supervise 2D networks. For 2D networks, compared with calculating uncertainty which introduces extra computation cost, we simply use the consistent prediction of two 2D networks. As the two networks are trained on slices of different planes, thus their consistent predictions are very likely to be correct. We validate our method on the MMWHS [27,28] dataset, and the results show that our method is superior to SOTA semi-supervised segmentation methods in solving sparse annotation problem. Also, our method only uses 16% of labeled slices but achieves comparable results to the fully supervised method.To sum up, our contributions are three folds:-A new perspective of solving sparse annotation problem, which is more versatile compared with recent methods using registration. -A novel cross-teaching paradigm which imposes consistency on the prediction of 3D and 2D networks. Our method enlarges the view difference of networks and boosts the performance. -A pseudo label selection strategy discriminating between reliable and unreliable predictions, which excludes error-prone voxels while keeping credible voxels though with low confidence. 2 Method"
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,2.1,Cross Annotation,"Recent sparse annotation methods [2,11] only label one slice for each volume, however, this annotation has many limitations. 1) The segmentation object must be visible on the labeled slice. Unfortunately, in most cases, the segmentation classes cannot be all visible in a single slice, especially in multi-class segmentation tasks. 2) Even though there is only one class and is visible in the labeled slice, the variance between slices might be large, and thus the information provided by a single slice is not enough to train a well-performed segmentation model. Based on these two observations, we label multiple slices for each volume. Empirically, the selection of slices should follow the rule that they should be as variant as possible in order to provide more information and have a broader coverage of the whole data distribution. Thus, we label slices from two planes (e.g., transverse plane and coronal plane) because the difference involved by planes is larger than that involved by slice position on a single plane. The annotation looks like crosses from the third plane, so we name it Cross Annotation. The illustration of cross annotation is shown in Fig. 1. Furthermore, in order to make the slices as variant as possible, we simply select those slices with a same distance. And the distance is set according to the dataset. For example, the distance can be large for easy segmentation task with lots of volumes. Otherwise, the distance should be closer for difficult task or with less volumes. And here we provide a simple strategy to determine the distance (Fig. 2). First label one slice for each plane in a volume, and train the model to monitor its performance on validation set, which has ground truth dense annotation. Then halve the distance (i.e., double the labeling slice), and test the trained model on validation set again. The performance gain can be calculated. Then repeat the procedure until the performance gain is less than half of the previous gain. The current distance is the final distance. The performance gain is low by labeling more slices.The aim of the task is to train a segmentation model on dataset D consists of L volumes X 1 , X 2 , ..., X L with cross annotation Y 1 , Y 2 , ..., Y L ."
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,2.2,3D-2D Cross Teaching,"Our framework consists of three networks, which are a 3D networks and two 2D networks. We leverage the unlabeled voxels through the cross teaching between 3D network and 2D networks. Specifically, the 3D network is trained on volumes and the 2D networks are trained with slices on transverse plane and coronal plane, respectively. The difference between 3D and 2D network is inherently in their network structure, and the difference between 2D networks comes from the different plane slices used to train the networks.For each sample, 3D network directly use it as input. Then it is cut into slices from two directions, resulting in transverse and coronal plane slices, which are used to train the 2D networks. And the prediction of each network, which is denoted as P , is used as pseudo label for the other network after selection. The selection strategy is detailedly introduced in the following part.To increase supervision signal for each training sample, we mix the selected pseudo label and ground truth sparse annotation together for supervision. And it is formulated as:where MIX(•, •) is a function that replaces the label in P with the label in Y for those voxels with ground truth annotation.Considering that the performance of 3D network is typically superior to 2D networks, we further introduce a label correction strategy. If the prediction of 3D network and the pseudo label from 2D networks differ, no loss on that particular voxel should be calculated as long as the confidence of 3D networks is higher than both 2D networks. We use M to indicate how much a voxel contribute to the loss calculation, and the value of position i is 0 if the loss of voxel i should not be calculated, otherwise 1 for ground truth annotation and w for pseudo label, where w is a value increasing from 0 to 0.1 according to ramp-up from [8].The total loss consists of cross-entropy loss and dice loss:andwhere p i , y i is the output and the label in Ŷ of voxel i, respectively. m i is the value of M at position i. H, W, D denote the height, width and depth of the input volume, respectively. And the total loss is denoted as:"
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,2.3,Pseudo Label Selection,"Hard-Soft Confidence Threshold. Due to the limitation of supervision signal, the prediction of 3D model has lots of noisy label. If it is directly used as pseudo label for 2D networks, it will cause a performance degradation on 2D networks. So we set a confidence threshold to select voxels which are more likely to be correct. However, we find that this may also filter out correct prediction with lower confidence, which causes the waste of useful information. If we know the quality of the prediction, we can set a lower confidence threshold for the voxels in the prediction of high-quality in order to utilize more voxels. However, the real accuracy R acc of prediction is unknown, for the dense annotation is unavailable during training. What we can obtain is the pseudo accuracy P acc calculated with the prediction and the sparse annotation. And we find that R acc and P acc are completely related on the training samples. Thus, it is reasonable to estimate R acc using P acc :where I(•) is the indicator function and pi is the one-hot prediction of voxel i. Now we introduce our hard-soft confidence threshold strategy to select from 3D prediction. We divide all prediction into reliable prediction (i.e., with higher P acc ) and unreliable prediction (i.e., with lower P acc ) according to threshold t q . And we set different confidence thresholds for these two types of prediction, which are soft threshold t s with lower value and hard threshold t h with higher value. In reliable prediction, voxels with confidence higher than soft threshold can be selected as pseudo label. The soft threshold aims to keep the less confident voxels in reliable prediction and filter out those extremely uncertain voxels to reduce the influence of false supervision. And in unreliable prediction, only those voxels with confidence higher than hard threshold can be selected as pseudo label. The hard threshold is set to choose high-quality voxels from unreliable prediction. The hard-soft confidence threshold strategy achieves a balance between increasing supervision signals and reducing label noise.Consistent Prediction Fusion. Considering that 2D networks are not able to utilize inter-slice information, their performance is typically inferior to that of 3D network. Simply setting threshold or calculating uncertainty is either of limited use or involving large extra calculation cost. To this end, we provide a selection strategy which is useful and introduces no additional calculation. The 2D networks are trained on slices from different planes and they learn different  patterns to distinguish foreground from background. So they will produce predictions with large diversity for a same input sample and the consensus of the two networks are quite possible to be correct. Thus, we use the consistent part of prediction from the two networks as pseudo label for 3D network.3 Experiments"
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,3.1,Dataset and Implementation Details,"MMWHS Dataset. [27,28] is from the MICCAI 2017 challenge, which consists of 20 cardiac CT images with publicly accessible annotations that cover seven whole heart substructures. We split the 20 volumes into 12 for training, 4 for validation and 4 for testing. And we normalize all volumes through z-score normalization. All volumes are reshaped to [192,192,96] with linear interpolation.Implementation Details. We adopt Adam [6] with a base learning rate of 0.001 as optimizer and the weight decay is 0.0001. Batch size is 1 and training iteration is 6000. We adopt random crop as data augmentation strategy and the patch size is [176,176,96]. And the hyper-parameters are t q = 0.98, t h = 0.9, t s = 0.7 according to experiments on validation set. For 3D and 2D networks, we use V-Net [15] and U-Net [18] as backbone, respectively. All experiments are conducted using PyTorch and 3 NVIDIA GeForce RTX 3090 GPUs. "
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,3.2,Comparison with SOTA Methods,"As previous sparse annotation works [2,11] cannot leverage sparse annotation where there are more than one labeled slice in a volume, to verify the effectiveness of our method, we compare it with SOTA semi-supervised segmentation methods, including Mean Teacher (MT) [21], Uncertainty-aware Mean-Teacher (UAMT) [25], Cross-Pseudo Supervision (CPS) [4] and Cross Teaching Between CNN and Transformer (CTBCT) [14]. The transformer network in CTBCT is implemented as UNETR [5]. Our method uses the prediction of 3D network as result. For fairer comparisons, all experiments are implemented in 3D manners with the same setting. For the evaluation and comparisons of our method and other methods, we use Dice coefficient, Jaccard coefficient, 95% Hausdorff Distance (HD) and Average Surface Distance (ASD) as quantitative evaluation metrics. The results are required through three runs with different random dataset split and they are reported as mean value ± standard deviation.The quantitative results and qualitative results are shown in Table 1 and Fig. 3."
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,3.3,Ablation Study,"We also investigate how hyper-parameters t q , t h and t s affect the performance of the method. We conduct quantitative ablation study on the validation set.The results are shown in Table 2. Bold font presents best results and underline presents the second best. Both {1,2,3} and {4,5,6} show that t s = 0.7 obtain the best performance. The result complies with our previous analysis. When t s is too high, correctly predicted voxels in reliable prediction are wasted. And when t s is low, predictions with extreme low confidence are selected as pseudo label, which introduces much noise to the cross-teaching. Setting t q = 0.98 performs better than setting t q = 0.95, and it indicates the criterion of selecting reliable prediction cannot be too loose. The result of hyper-parameters set 7 shows that when we set hard and soft thresholds equally low, the performance is largely degraded, and it validates the effectiveness of our hard-soft threshold strategy."
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,4,Conclusion,"In this paper, we extend sparse annotation to cross annotation to suit more general real clinical scenario. We label slices from two planes and it enlarges the diversity of annotation. To better leverage the cross annotation, we view the problem from the perspective of semi-supervised segmentation and we propose a novel cross-teaching paradigm which imposes consistency on the prediction of 3D and 2D networks. Furthermore, to achieve robust cross-supervision, we propose new strategies to select credible pseudo label, which are hard-soft threshold for 3D network and consistent prediction fusion for 2D networks. And the result on MMWHS dataset validates the effectiveness of our method."
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,,Fig. 1 .,
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,,Fig. 2 .,
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,,Fig. 3 .,
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,,Table 1 .,
3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching Between 3D and 2D Networks,,Table 2 .,
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,1,Introduction,"Deep neural networks (DNN) have demonstrated state-of-the-art performance in medical image segmentation in recent years [1]. In practice, the discrepancies in the distributions between the target domain, where the test data come from, and the source domain that provides the training data, often lead to reduced test-time performance (Fig. 1). This phenomenon, known as the domain shift [5], is common in medical imaging [2], thus necessitates model re-training across institutes, resulting in a waste of resources and precluding the use of DNNs in budget-challenged scenarios.Many studies have attempted to address the domain shift. Earlier works adapt models to the target domain with access to the source domain [6][7][8], restricting their applications due to privacy concerns. In response, methods utilizing prior or anatomical information to remove the need for source data are proposed [9,10], yet their flexibility is limited. Lately, test-time domain adaptation (TTA), continual test-time adaptation (CTTA), and domain generalization (DG) methods have been gaining popularity [2,3,19]. TTA methods train a model on a labeled source domain and adapt it to an unlabeled target domain with access to target data only. Adaptation is usually performed via feature alignment through generative models [12], domain adversarial learning and paired consistency [13], image/feature translation via adaptor networks [14], and entropy minimization which fine-tunes the parameters of the batch normalization (BN) layers [15] on test data [2,16]. CTTA is an emerging approach aiming to improve the robustness of TTA methods during long-term continual adaptation, a scenario where TTA methods are susceptible to catastrophic forgetting and become overfitted to later test samples. Examples include stochastic parameter restoration [3] and normalization correction and data resampling [4]. DG methods aim to produce a more generalizable model from one or more source domains without updating parameters at test time. Popular methods involve data augmentations to enhance domain robustness [17] and learn domain-invariant features [18,19].CTTA could be a useful technique to segment patient data acquired at different time points of longitudinal studies. However, we note that adaptation is possible only when the source model, typically trained with empirical risk minimization (ERM) [2][3][4]12,[14][15][16], already demonstrates reasonable target-domain performance as the starting point. ERM models may struggle to provide adequate performance for further adaptation in severe domain shifts (see panels (g)-(l), Fig. 1). Domain knowledge can be utilized to design a preprocessing procedure that reduces the domain gap [20] and enables ERM models to perform adequately on the target domain. However, the effort to design preprocessing procedures significantly increases when a trained source model is shared with multiple end-users to account for different test-time data distributions.To address those issues, we propose a generalizable CTTA framework for the cross-domain segmentation task of medical images. We first incorporate shape-aware feature learning into existing models and train them on the source domain with DG techniques. This removes the need for carefully preprocessed target domain data and allows the source model to perform reasonably in most target domains regardless of the severity of the domain shift. Then, we use an uncertainty-weighted multi-task mean teacher network inspired by semisupervised literature to perform adaptation, producing results with improved accuracy and refined contours. In addition, a small portion of the model weight is stochastically reset to its initial, domain-generalized state at each adaptation step to prevent the model from overfitting to later test samples. We show the proposed framework works with ERM and DG-trained source models and (1) outperforms several state-of-the-art methods on three challenging cross-domain segmentation tasks and (2) is better suited for CTTA than its peers in various scenarios."
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,2,Methodology,"Overview. The proposed framework is a synergy of three components (Fig. 2): (1) shape-aware model training, (2) shape and uncertainty-aware mean teacher network for the model update, and (3) domain-generalized stochastic weight restoration for continual adaptation. Component ( 1) is used for model training in the source domain, while (2) and (3) are used simultaneously for CTTA. We describe each component in detail below. Shape-Aware Model Training. Motivated by recent studies [18,19] suggesting that shape information enables generalizable performance due to their consistent and invariable presence across different domains, we propose integrating shape awareness into the model training in the source domain. We first model the shape information with the signed distance field (SDF [22], ∈ [-1, 1]) which measures the distance between any pixel to the nearest object boundary and the position of the pixel relative to the boundary: positive if outside, zero if on the boundary, and negative if inside. Then, an SDF head is appended to the source model to share features with the existing segmentation head to encode shape information into the model. Finally, the source training is performed with DG techniques to ensure reasonable performance even in extremely drifted target domains (such as T 1 -weighted ↔ mDixon magnetic resonance images, Table 1), allowing for further adaptation to take place.Specifically, we train the modified model on the source domain (X S , Y S , Z S ) ∈ S, where X S denotes the input image, Y S the manual annotations, and Z S the ground-truth SDF calculated from Y S using [22], by minimizing a multi-task lossHere, seg and sdf represent loss functions used for optimizing the segmentation and SDF prediction tasks, respectively, N is the number of images in each batch, and) indicate the predicted segmentation probability and SDF maps produced by the source model f from the augmented input g(x). We implement g with causality-inspired DG (CiDG) [19], a shallow randomly-weighted neural network that imposes domain-generalized shape-based feature learning through constant resampling of appearances of potentially correlated objects in the image.Uncertainty and Shape-Aware Adaptation With Mean Teacher. The mean teacher network trains a student model and uses the exponential moving averages (EMA) of its weights to update an identical teacher model whose predictions further regularize the student model. Inspired by their rising popularity in semi-supervised studies [23], we use a mean teacher network to adapt all parameters of the trained shape-aware source model to the unlabeled target domain T . The overall architecture follows [21] except for the absence of the reconstruction task: both models predict SDF maps on top of segmentation labels, allowing for the utilization of shape information, and uncertainties are estimated from the teacher's outputs, avoiding misleading supervision during the adaptation phase.Specifically, both models are initialized with the weights of the source model. Then, at each time step t, the student model first predicts segmentation probability maps Ỹ T t and SDF predictions ZT t for the current test data x T t . Next, the teacher model performs K forward passes, producing K segmentation probability maps { Ŷ T tk } K k=1 and SDF predictions { ẐT tk } K k=1 from a set of noisy input images constructed by adding K random Gaussian noise vectors to x T t . The final segmentation map of the teacher model at time step t is obtained by aggregating all K segmentation probability maps through their uncertainties. The pixel-wise uncertainty of each of the K segmentation probability maps is measured as the entropy, where the log function has a base of C, the number of segmentation classes. Next, the confidence map of kth probability map is calculated as 1 -U tk , as higher values in U tk ∈ [0, 1] denote areas with higher uncertainties. Then, all confidence maps are stacked in the first dimension where we apply the softmax function, i.e., {W tk } K k=1 = softmax({1 -U tk } K k=1 ), to normalize the confidence value to [0, 1]. Lastly, the final segmentation probability map is constructed as a confidence-weighted combination of all K intermediate probability maps asThe entropy of the final segmentation represents its uncertaintyEntropy cannot be calculated on real-valued outputs such as SDF maps. As such, the final SDF prediction is obtained by averaging all K SDF maps, i.e., ẐTẐT tk , and we follow [24] to estimate the uncertainty using the varianceThe student model is therefore guided by the teacher model by minimizing four loss terms:where sdf and seg are the MSE and the Dice loss [26], Ȳ T n is the one-hot encoded pseudo-labels calculated from Ŷ T n with the argmax function, and N denotes the number of images in each test batch.2 also penalize inconsistencies between the student and teacher models, but are weighted by the calculated uncertainty maps to encourage learning of confident predictions from the teacher model. The student model also performs self-regularization comprising two loss terms:where σ is the sigmoid function and κ is a multiplying factor approximating the inverse transformation from segmentation labels to SDF maps. The first loss term converts SDF maps into approximations of their corresponding segmentation labels and enforces a cross-task consistency [25], and the second term e = c Ỹ T c log Ỹ T c reduces the entropy in the predicted segmentation maps. The final objective function is therefore formulated as a weighted sum as = t +α s . Domain Generalized Stochastic Restore. Continual and unsupervised model adaptation to T would likely result in performance degradation due to accumulations of errors, leading to catastrophic forgetting of earlier samples. Therefore, we combine DG source training and a stochastic weight restoration mechanism [3] to reset small portions of the model to its initial domaingeneralized weights, stopping the model from 'diving too deep' into specific target data while providing a decent baseline performance for the model to roll back.Let W t+1 denote the weights of a trainable conv layer after the gradient update at time step t. A small portion of W t+1 is reset to its initial weights as, where M ∼ Bernoulli(p) is a binary mask tensor, and W 0 denotes the initial domain-generalized weights of the conv layer."
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,3,Experiments,"Setup. We implemented our method with PyTorch 1.10.0 and trained it on one Nvidia Tesla V100 GPU. We evaluated our method and other benchmarking methods on three cross-domain datasets with varying degrees of domain shifts: (1) cross-site binary prostate segmentation from T 2 -weighted MRI scans collected from six different sites (12-30 scans/site) [29][30][31], (2) cross-site and cross-modality multi-class (liver, left and right kidneys, and spleen) abdominal segmentation between 30 CT and 20 MRI T 2 -SPIR scans [32,33], and (3) same-site cross-modality muscle segmentation of 13 lower-leg muscles and bones between 30 MRI T 1 and 30 mDixon scans [34]. All scans were collected from healthy and diseased individuals and normalized to zero mean and unit variance before being reformatted to 2D. The prostate and abdominal scans were resized to 192 × 192 pixels while the muscle scans were spatially resized to 128 × 128 pixels. Lastly, a window of [-275, 125] in Houndsfield units was applied to CT scans and the top 0.5% of the histogram of MRI scans were clipped as per [3].We treated each site as the source domain and adapted to all other sites in the first experiment. For other experiments, we first performed adaptation from modality A to B, then from B to A. All experiments were performed in an online manner: each test scan arrived randomly and was broken down into multiple batches if needed. The model adapted itself to each batch before making a prediction. U-Net with an EfficientNet-b2 backbone was used as the source model for all our experiments. The Adam optimizer [35] was used with a learning rate of 0.001 and a batch size of 32. α was set to 1, κ to -1500, and p to 0.01. The model was empirically updated for two steps per test batch for prostate and muscle segmentation and 10 steps for abdominal segmentation. In addition, we calculated the final performance of each model by using each model to re-predict the segmentation labels of all test samples after the adaptation was completed. We then compared the final performance of each model against their running Results. We compared our method against several state-of-the-art general and medical TTA and CTTA methods that require no additional clinical or anatomical information about either domain. General methods include BN Stats [27], Tent [16], and CoTTA [3], and medical methods involve the combination of ATTA [14] and DLTTA [28]. DLTTA was also combined with Tent and CoTTA for a more comprehensive comparison.The proposed method substantially outperformed other methods on all three tasks and could consistently improve the CiDG-trained source model even in scenarios where other peer methods could not (Table 1). Surprisingly, the CiDGtrained source model outperformed all TTA methods except ours in numerous experiments with its decent performance. We attribute this to the fact that most TTA methods rely on (1) image/feature translation and reconstruction or (2) BN statistics re-estimation. However, DG methods often employ extensive augmentations, which may continuously change the contrast of the source data to allow domain-invariant feature learning. A constantly changing source domain may impede methods such as ATTA performing image or feature-level translation or reconstruction at the adaptation phase. Furthermore, we hypothesize that the running BN statistics of DG-trained models help to stabilize domaininvariant feature extraction at test time. As such, discarding and re-estimating them from test data, as was done by Tent, may be detrimental to the targetdomain performance. To test our hypothesis, we disabled the BN statistics reestimation in Tent and had a 3% improvement of Dice and 0.4 mm improvement on ASSD in the abdominal segmentation task. DLTTA consistently improved Tent and ATTA through dynamic learning rates but failed to improve CoTTA at the same rate. CoTTA is a CTTA method highly relevant to ours, and its inconsistency in performance improvement suggests that geometric augmentations may be too strong for test-time learning and highlights the efficacy of the proposed uncertainty and shape-aware mean teacher setup. For adaptation, the uncertainty-aware module ensured only trustworthy predictions from the teacher model were used, and the shape-aware regularization further enhanced the target-domain performance by refining the smoothness of the predicted labels and ensuring the integrity of the anatomical structure of the predicted objects (Fig. 3). A brief ablation study demonstrated the effectiveness of each proposed component (see bottom of Table 1). Our framework also outperformed other methods by a larger margin on the prostate and abdominal segmentation tasks, where an ERM-trained source model was used for adaptation, further showcasing the generalizability of each proposed component (Supplementary Table 1).The proposed model also demonstrated an equal or higher final performance (in comparison to its running performance) in all experiments, whereas many of its peers demonstrated the opposite (Supplementary Table 2). Equal final performance suggests that the model remembered earlier test data, and a higher final performance indicates its capability to utilize later test samples to improve its earlier performance. On the other hand, a lower final performance suggests that the model forgot about earlier test data and overfitted to later test data. The proposed DG stochastic restore prevented the model from drifting towards later test samples, and the teacher model reduced the likelihood of error accumulation through uncertainty estimation. Together they enabled reliable CTTA for medical images."
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,4,Conclusion,"We proposed a generalizable framework for continual test-time adaption of medical images. Our approach first trains a model on the source domain with domain-invariant shape features before adapting it to the target domain with uncertainty-weighted pseudo-labels and SDF maps. Our method can work with ERM or DG-trained source models and outperformed its peers on three crosssite/cross-domain segmentation tasks without showing performance degradation as the adaptation progressed. Our framework can continuously adapt the source model to unknown test data online for the segmentation task, significantly reducing the cost and bias associated with manual labeling."
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,,Fig. 1 .,
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,,Fig. 2 .,
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,,Fig. 3 .,
Uncertainty and Shape-Aware Continual Test-Time Adaptation for Cross-Domain Segmentation of Medical Images,,Table 1 .,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,1,Introduction,"Deep learning models have achieved remarkable success in segmenting anatomy and lesions from medical images but often rely on large-scale manually annotated datasets [1][2][3]. This is challenging when working with volumetric medical data as voxelwise labeling requires a lot of time and expertise. Interactive segmentation models address this issue by utilizing weak labels, such as clicks, instead of voxelwise annotations [5][6][7]. The clicks are transformed into guidance signals, e.g., Gaussian heatmaps or Euclidean/Geodesic distance maps, and used together with the image as a joint input for the interactive model. Annotators can make additional clicks in missegmented areas to iteratively refine the segmentation mask, which often significantly improves the prediction compared to non-interactive models [4,13]. However, prior research on choosing guidance signals for interactive models is limited to small ablation studies [5,8,9]. There is also no systematic framework for comparing guidance signals, which includes not only accuracy but also efficiency and the ability to iteratively improve predictions with new clicks, which are all important aspects of interactive models [7]. We address these challenges with the following contributions:1. We compare 5 existing guidance signals on the AutoPET [1] and MSD Spleen [2] datasets and vary various hyperparameters. We show which parameters are essential to tune for each guidance and suggest default values. 2. We introduce 5 guidance evaluation metrics (M1)-(M5), which evaluate the performance, efficiency, and ability to improve with new clicks. This provides a systematic framework for comparing guidance signals in future research. 3. Based on our insights from 1., we propose novel adaptive Gaussian heatmaps, which use geodesic distance values around each click to set the radius of each heatmap. Our adaptive heatmaps mitigate the weaknesses of the 5 guidances and achieve the best performance on AutoPET [1] and MSD Spleen [2]."
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,Related Work.,"Previous work comparing guidance signals has mostly been limited to small ablation studies. Sofiiuk et al. [9] and Benenson et al. [8] both compare Euclidean distance maps with solid disks and find that disks perform better. However, neither of them explore different parameter settings for each guidance and both work with natural 2D images. Dupont et al. [12] note that a comprehensive comparison of existing guidance signals would be helpful in designing interactive models. The closest work to ours is MIDeepSeg [5], which proposes a user guidance based on exponentialized geodesic distances and compare it to existing guidance signals. However, they use only initial clicks and do not add iterative corrective clicks to refine the segmentation. In contrast to previous work, our research evaluates the influence of hyperparameters for guidance signals and assesses the guidances' efficiency and ability to improve with new clicks, in addition to accuracy. While some previous works [20][21][22] propose using a larger radius for the first click's heatmap, our adaptive heatmaps offer a greater flexibility by adjusting the radius at each new click dynamically."
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2,Methods,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.1,Guidance Signals,"We define the five guidance signals over a set of N clicks C = {c 1 , c 2 , ..., c N } where c i = (x i , y i , z i ) is the i th click. As disks and heatmaps can be computed independently for each click, they are defined for a single click c i over 3D voxels v = (x, y, z) in the volume. The disk signal fills spheres with a radius σ centered around each click c i , which is represented by the equation in Eq. (1).The Gaussian heatmap applies Gaussian filters centered around each click to create softer edges with an exponential decrease away from the click (Eq. ( 2)).The Euclidean distance transform (EDT) is defined in Eq. ( 3) as the minimum Euclidean distance between a voxel v and the set of clicks C. It is similar to the disk signal in Eq. ( 1), but instead of filling the sphere with a constant value it computes the distance of each voxel to the closest click point.The Geodesic distance transform (GDT) is defined in Eq. (4) as the shortest path distance between each voxel in the volume and the closest click in the set C [14]. The shortest path in GDT also takes into account intensity differences between voxels along the path. We use the method of Asad et al. [10] to compute the shortest path which is denoted as Φ in Eq. (4).We also examine the exponentialized Geodesic distance (exp-GDT) proposed in MIDeepSeg [5] that is defined in Eq. ( 5) as an exponentiation of GDT:Note: We normalize signals to [0, 1] and invert intensity values for Euclidean and Geodesic distances d(x) by 1d(x) for better highlighting of small distances. We define our adaptive Gaussian heatmaps ad-heatmap(v, c i , σ i ) via:Here, N ci is the 9-neighborhood of c i , a = 13 limits the maximum radius to 13, and b = 0.15 is set empirically 1 (details in supplementary). The radius σ i is smaller for higher x, i.e., when the mean geodesic distance in the neighboring voxels is high, indicating large intensity changes such as edges. This leads to a more precise guidance with a smaller radius σ i near edges and a larger radius in homogeneous areas such as clicks in the center of the object of interest. An example of this process and each guidance signal can be seen in Fig. 1a)."
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.2,Model Backbone and Datasets,"We use the DeepEdit [11] model with a U-Net backbone [15] and simulate a fixed number of clicks N during training and evaluation. For each volume, N clicks are iteratively sampled from over-and undersegmented predictions of the model as in [16] and represented as foreground and background guidance signals. We implemented our experiments with MONAI Label [23] and will release our code.We trained and evaluated all of our models on the openly available AutoPET [1] and MSD Spleen [2] datasets. MSD Spleen [2] contains 41 CT volumes with voxel size 0.79×0.79×5.00mm 3 and average resolution of 512×512×89 voxels with dense annotations of the spleen. AutoPET [1] consists of 1014 PET/CT volumes with annotated tumor lesions of melanoma, lung cancer, or lymphoma. We discard the 513 tumor-free patients, leaving us with 501 volumes. We also only use PET data for our experiments. The PET volumes have a voxel size of 2.0 × 2.0 × 2.0mm 3 and an average resolution of 400 × 400 × 352 voxels."
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.3,Hyperparameters: Experiments,"We keep these parameters constant for all models: learning rate = 10 -5 , #clicks N = 10, Dice Cross-Entropy Loss [24], and a fixed 80-20 training-validation split (D train /D val ). We apply the same data augmentation transforms to all models and simulate clicks as proposed in Sakinis et al. [16]. We train using one A100 GPU for 20 and 100 epochs on AutoPET [1] and MSD Spleen [2] respectively.We vary the following four hyperparameters (H1)-(H4): (H1) Sigma. We vary the radius σ of disks and heatmaps in Eq. ( 1) and ( 2) and also explore how this parameter influences the performance of the distancebased signals in Eq. ( 3)- (5). Instead of initializing the seed clicks C as individual voxels c i , we initialize the set of seed clicks C as all voxels within a radius σ centered at each c i and then compute the distance transform as in Eq. ( 3)-( 5).(H2) Theta. We explore how truncating the values of distance-based signals in Eq. ( 3)-( 5) affects the performance. We discard the top θ ∈ {10%, 30%, 50%} of the distance values and keep only smaller distances closer to the clicks making the guidance more precise. Unlike MIDeepSeg [5], we compute the θ threshold for each image individually, as fixed thresholds may not be suitable for all images.(H3) Input Adaptor. We test three methods for combining guidance signals with input volumes proposed by Sofiuuk et al. [9] -Concat, Distance Maps Fusion (DMF), and Conv1S. Concat combines input and guidance by concatenating their channels. DMF additionally includes 1 × 1 conv. layers to adjust the channels to match the original size in the backbone. Conv1S has two branches for the guidance and volume, which are summed and fed to the backbone. (H4) Probability of Interaction. We randomly decide for each volume whether to add the N clicks or not, with a probability of p, in order to make the model more independent of interactions and improve its initial segmentation. All the hyperparameters we vary are summarized in Table 1. Each combination of hyperparameters corresponds to a separately trained DeepEdit [11] model."
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.4,Additional Evaluation Metrics,"We use 5 metrics (M1)-(M5) (Table 2) to evaluate the validation performance.  Overlap of the guidance G with the ground-truth mask M : |M ∩G| |G| . This estimates the guidance precision as corrective clicks are often near boundaries, and if guidances are too large, such as disks with a large σ, there is a large overlap with the background outside the boundary"
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,3,Results,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,3.1,Hyperparameters: Results,"We first train a DeepEdit [11] model for each (σ, θ) pair and set p = 100% and the input adaptor to Concat to constrain the parameter space.(H1) Sigma. Results in Fig. 1b) show that on MSD Spleen [2], the highest Dice scores are at σ = 5, with a slight improvement for two samples at σ = 1, but performance decreases for higher values σ > 5. On AutoPET [1], σ = 5 and two samples with σ = 0 show the best performance, while higher values again demonstrate a significant performance drop. Figure 1c) shows that the best final  and initial Dice for disks and heatmaps are with σ = 1 and σ = 0. Geodesic maps exhibit lower Dice scores for small σ < 5 and achieve the best performance for σ = 5 on both datasets. Larger σ values lead to a worse initial Dice for all guidance signals. Differences in results for different σ values are more pronounced in AutoPET [1] as it is a more challenging dataset [17][18][19]25].(H2) Theta. We examine the impact of truncating large distance values for the EDT and GDT guidances from Eq. ( 3) and ( 4). Figure 1a) shows that the highest final Dice scores are achieved with θ = 10 for MSD Spleen [2]. On AutoPET [1], the scores are relatively similar when varying θ with a slight improvement at θ = 10. The results in Fig. 1d) also confirm that θ = 10 is the optimal parameter for both datasets and that not truncating values on MSD Spleen [2], i.e. θ = 0, leads to a sharp drop in performance.For our next experiments, we fix the optimal (σ, θ) pair for each of the five guidances (see Table 3) and train a DeepEdit [11] model for all combinations of input adaptors and probability of interaction.(H3) Input Adaptor. We look into different ways of combining guidance signals with input volumes using the input adaptors proposed by Sofiuuk et al. [9]. The results in Fig. 1e) indicate that the best performance is achieved by simply concatenating the guidance signal with the input volume. This holds true for both datasets and the difference in performance is substantial.(H4) Probability of Interaction. Figure 1e) shows that p ∈ {75%, 100%} results in the best performance on MSD Spleen [2], with a faster convergence rate for p = 75%. However, with p = 50%, the performance is worse than the non-interactive baseline (p = 0%). On AutoPET [1], the results for all p values are similar, but the highest Dice is achieved with p = 100%. Note that p = 100% results in lower initial Dice scores and requires more interactions to converge, indicating that the models depend more on the interactions. For the rest of our experiments, we use the optimal hyperparameters for each guidance in Table 1."
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,3.2,Additional Evaluation Metrics: Results,"The comparison of the guidance signals using our five metrics (M1)-(M5) can be seen in Fig. 2. Although the concrete values for MSD Spleen [2]  AutoPET [1] are different, the five metrics follow the same trend on both datasets.(M1) Initial and (M2) Final Dice. Overall, all guidance signals improve their initial-to-final Dice scores after N clicks, with AutoPET [1] showing a gap between disks/heatmaps and distance-based signals. Moreover, geodesic-based signals have lower initial scores on both datasets and require more interactions.(M3) Consistent Improvement. The consistent improvement is ≈ 65% for both datasets, but it is slightly worse for AutoPET [1] as it is more challenging. Heatmaps and disks achieve the most consistent improvement, which means they are more precise in correcting errors. In contrast, geodesic distances change globally with new clicks as the whole guidance must be recomputed. These changes may confuse the model and lead to inconsistent improvement.(M4) Overlap with Ground Truth. Heatmaps, disks, and EDT have a significantly higher overlap with the ground truth compared to geodesicbased signals, particularly on AutoPET [1]. GDT incorporates the changes in voxel intensity, which is not a strong signal for lesions with weak boundaries in AutoPET [1], resulting in a smaller overlap with the ground truth. The guidances are ranked in the same order in (M3) and in (M4) for both datasets. Thus, a good overlap with the ground truth can be associated with precise corrections.(M5) Efficiency. Efficiency is much higher on MSD Spleen [2] compared to AutoPET [1], as AutoPET has a ×2.4 larger mean volume size. The time also includes the sampling of new clicks for each simulated interaction. Disks are the most efficient signal, filling up spheres with constant values, while heatmaps are slightly slower due to applying a Gaussian filter over the disks. Distance transform-based guidances are the slowest on both datasets due to their complexity, but all guidance signals are computed in a reasonable time (<1 s).Adaptive Heatmaps: Results. Varying (H1)-(H4) and examining (M1)-(M5), we find disks/heatmaps as the best signals, but with inflexibility near edges due to their fixed radius (Fig. 1a)). Using GDT as a proxy signal to adapt σ i for each click c i mitigates this weakness by imposing large σ i in homogeneous areas and small, precise σ i near edges (Fig. 1a)). This results in substantially higher consistent improvement and overlap with ground truth and the best initial and final Dice (Table 3). Thus, our comparative study has led to the creation of a more consistent and flexible signal with a slight performance boost, albeit with an efficiency cost due to the need to compute both GDT and heatmaps."
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,4,Conclusion,"Our comparative experiments yield insights into tuning existing guiding signals and designing new ones. We find that smaller radiuses (σ ≤ 5), a small threshold (θ = 10%), more iterations with interactions (p ≥ 75%), and traditional concatenation should be used. Weaknesses in existing signals include overly large radiuses near edges and inconsistent improvement for geodesic-based signals that change with each click. This analysis inspires our adaptive heatmaps, which adapt the radiuses of the heatmaps according to the geodesic values around the clicks, mitigating the inflexibility and inconsistency of existing guidances. We emphasize the importance of guidance representation in clinical applications, where a consistent and robust model is critical. Our study provides an overview of potential pitfalls, important parameters to tune, and how to design future guidance signals, along with proposed metrics for systematic comparison."
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,Fig. 1 .,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,Table 1 .,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,Table 2 .,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,Table 3 .,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,,
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 61.
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,1,Introduction,"The recent advancements in Transformer-based models have revolutionized the field of natural language processing and have also shown great promise in a wide range of computer vision tasks [5]. As a notable example, the Vision Transformer (ViT) model utilizes Multi-head Self-Attention (MSA) blocks to globally model the interactions between semantic tokens created by treating local image patches as individual elements [7]. This approach stands in contrast to CNNs, which hierarchically increase their receptive field from local to global to capture a global semantic representation. Nevertheless, recent studies [3,20] have shown that ViT models struggle to capture high-frequency components of images, which can limit their ability to detect local textures and it is vital for many diagnostic and prognostic tasks. This weakness in local representation can be attributed to the way in which ViT models process images. ViT models split an image into a sequence of patches and model their dependencies using a self-attention mechanism, which may not be as effective as the convolution operation used in CNN models in extracting local features within receptive fields. This difference in how ViT and CNN process images may explain the superior performance of CNN models in local feature extraction [1,8]. Innovative approaches have been proposed in recent years to address the insufficient local texture representation within Transformer models. One such approach is the integration of CNN and ViT features through complementary methods, aimed at seamlessly blending the strengths of both in order to compensate for any shortcomings in local representation [5]. [5] is one of the earliest approaches incorporating the Transformer layers into the CNN bottleneck to model both local and global dependency using the combination of CNN and ViT models. Heidari et al. [11] proposed a novel solution called HiFormer, which leverages a Swin Transformer module and a CNN-based encoder to generate two multi-scale feature representations, which are then integrated via a Double-Level Fusion module. UNETR [10] used a Transformer to create a powerful encoder with a CNN decoder for 3D medical image segmentation. By bridging the CNNbased encoder and decoder with the Transformer, CoTr [26], and TransBTS [22], the segmentation performance in low-resolution stages was improved. Despite these advances, there remain some limitations in these methods such as computationally inefficiency (e.g., TransUNet model), the requirement of a heavy CNN backbone (e.g., HiFormer), and the lack of consideration for multi-scale information. These limitations have resulted in less effective network learning results in the field of medical image segmentation."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Transformers as a Complement to CNNs: TransUNet,
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,New Attention Models:,"The redesign of the self-attention mechanism within pure Transformer models is another method aiming to augment feature repre-sentation to enhance the local feature representation ultimately. In this direction, Swin-Unet [4] utilizes a linear computational complexity Swin Transformer [14] block in a U-shaped structure as a multi-scale backbone. MISSFormer [12] besides exploring the Efficient Transformer [25] counterpart to diminish the parameter overflow of vision transformers, applies a non-invertible downsampling operation on input blocks transformer to reduce the parameters. D-Former [24] is a pure transformer-based pipeline that comprises a double attention module to capture locally fine-grained attention and interaction with different units in a dilated manner through its mechanism."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Drawbacks of Transformers:,"Recent research has revealed that traditional self-attention mechanisms, while effective in addressing local feature discrepancies, have a tendency to overlook important high-frequency information such as texture and edge details [21]. This is especially problematic for tasks like tumor detection, cancer-type identification through radiomics analysis, as well as treatment response assessment, where abnormalities often manifest in texture. Moreover, self-attention mechanisms have a quadratic computational complexity and may produce redundant features [18].Our Contributions: ➊ We propose Laplacian-Former, a novel approach that includes new efficient attention (EF-ATT) consisting of two sub-attention mechanisms: efficient attention and frequency attention. The efficient attention mechanism reduces the complexity of self-attention to linear while producing the same output. The frequency attention mechanism is modeled using a Laplacian pyramid to emphasize each frequency information's contribution selectively. Then, a parametric frequency attention fusion strategy to balance the importance of shape and texture features by recalibrating the frequency features. These two attention mechanisms work in parallel. ➋ We also introduce a novel efficient enhancement multi-scale bridge that effectively transfers spatial information from the encoder to the decoder while preserving the fundamental features. ➌ Our method not only alleviates the problem of the traditional self-attention mechanism mentioned above, but also it surpasses all its counterparts in terms of different evaluation metrics for the tasks of medical image segmentation."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,2,Methods,"In our proposed network, illustrated in Fig. 1, taking an input image X ∈ R H×W ×C with spatial dimensions H and W , and C channels, it is first passed through a patch embedding module to obtain overlapping patch tokens of size 4 × 4 from the input image. The proposed model comprises four encoder blocks, each containing two efficient enhancement Transformer layers and a patch merging layer that downsamples the features by merging 2 × 2 patch tokens and increasing the channel dimension. The decoder is composed of three efficient enhancement Transformer blocks and four patch-expanding blocks, followed by a segmentation head to retrieve the final segmentation map. Laplacian-Former then employs a novel efficient enhancement multi-scale bridge to capture local and global correlations of different scale features and effectively transfer the underlying features from the encoder to the decoder."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,2.1,Efficient Enhancement Transformer Block,"In medical imaging, it is important to distinguish different structures and tissues, especially when tissue boundaries are ill-defined. This is often the case for accurate segmentation of small abnormalities, where high-frequency information plays a critical role in defining boundaries by capturing both textures and edges. Inspired by this, we propose an Efficient Enhancement Transformer Block that incorporates an Efficient Frequency Attention (EF-ATT) mechanism to capture contextual information of an image while recalibrating the representation space within an attention mechanism and recovering high-frequency details.Our efficient enhancement Transformer block first takes a LayerNorm (LN) from the input x. Then it applies the EF-ATT mechanism to capture contextual information and selectively include various types of frequency information while using the Laplacian pyramid to balance the importance of shape and texture features. Next, x and diversity-enhanced shortcuts are added to the output of the attention mechanism to increase the diversity of features. It is proved in [19] that as Transformers become deeper, their features become less varied, which restrains their representation capacity and prevents them from attaining optimal performance. To address this issue, we have implemented an augmented short- cut method from [9], a Diversity-Enhanced Shortcut (DES), employing a Kronecker decomposition-based projection. This approach involves inserting additional paths with trainable parameters alongside the original shortcut x, which enhances feature diversity and improves performance while requiring minimal hardware resources. Finally, we apply LayerNorm and MiX-FFN [25] to the resulting feature representation to enhance its power. This final step completes our efficient enhancement Transformer block, as illustrated in Fig. 2."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,2.2,Efficient Frequency Attention (EF-ATT),"The traditional self-attention block computes the attention score S using query (Q) and key (K) values, normalizes the result using Softmax, and then multiplies the normalized attention map with value (V):where d k is the embedding dimension. One of the main limitations of the dotproduct mechanism is that it generates redundant information, resulting in unnecessary computational complexity. Shen et al. [18] proposed to represent the context more effectively by reducing the computational burden from O(n 2 ) to linear form O(d 2 n):Their approach involves applying the Softmax function (ρ) to the key and query vectors to obtain normalized scores and formulating the global context by multiplying the key and value matrix. They demonstrate that efficient attention E can provide an equivalent representation of self-attention while being computationally efficient. By adopting this approach, we can alleviate the issues of feature redundancy and computational complexity associated with self-attention.Wang et al. [21] explored another major limitation of the self-attention mechanism, where they demonstrated through theoretical analysis that self-attention operates as a low-pass filter that erases high-frequency information, leading to a loss of feature expressiveness in the model's deep layers. Authors found that the Softmax operation causes self-attention to keep low-frequency information and loses its fine details. Motivated by this, we propose a new frequency recalibration technique to address the limitations of self-attention, which only focuses on low-frequency information (which contains shape information) while ignoring the higher frequencies that carry texture and edge information. First, we construct a Laplacian pyramid to determine the different frequency levels of the feature maps. The process begins by extracting (L + 1) Gaussian representations from the encoded feature using different variance values of the Gaussian function:where X refers to the input feature map, (i, j) corresponds to the spatial location within the encoded feature map, the variable σ l denotes the variance of the Gaussian function for the l-th scale, and the symbol * represents the convolution operator. The pyramid is then built by subtracting the l-th Gaussian function (G l ) output from the (l + 1)-th output (G l -G l+1 ) to encode frequency information at different scales. The Laplacian pyramid is composed of multiple levels, each level containing distinct types of information. To ensure a balanced distribution of low and high-frequency information in the model, it is necessary to efficiently aggregate the features from all levels of the frequency domain. Hence, we present frequency attention that involves multiplying the key and value of each level (X l ) to calculate the attention score and then fuses the resulting attention scores of all levels using a fusion module, which performs summation. The resulting attention score is multiplied by Query (Q) to obtain the final frequency attention result, which subsequently concatenates with the efficient attention result and applies the depth-wise convolution with the kernel size of 2×1×1 in order to aggregate both information and recalibrate the feature map, thus allowing for the retrieval of high-frequency information."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,2.3,Efficient Enhancement Multi-scale Bridge,"It is widely known that effectively integrating multi-scale information can lead to improved performance [12]. Thus, we introduce the Efficient Enhancement Multi-scale Bridge as an alternative to simply concatenating the features from the encoder and decoder layers. The proposed bridge, depicted in Fig. 1, delivers spatial information to each decoder layer, enabling the recovery of intricate details while generating output segmentation masks. In this approach, we aim to calculate the efficient attention mechanism for each level and fuse the multiscale information in their context; thus, it is important that all levels' embedding dimension is of the same size. Therefore, in order to calculate the global context (G i ), we parametrize the query and value of each level using a convolution 1 × 1 where it gets the size of mC and outputs C, where m equals 1, 2, 5, and 8 for the first to fourth levels, respectively. We multiply the new key and value to each other to attain the global context. We then use a summation module to aggregate the global context of all levels and reshape the query for matrix multiplication with the augmented global context. Taking the second level with the dimension of H 8 × W 8 × 2C, the key and value are mapped to ( H 8 × W 8 × 2C and feed it through an LN and MiX-FFN module with a skip connection to empower the feature representations. The resulting output is combined with the expanded feature map, and then projected using a linear layer onto the same size as the encoder block corresponding to that level."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,3,Results,"Our proposed technique was developed using the PyTorch library and executed on a single RTX 3090 GPU. A batch size of 24 and a stochastic gradient descent algorithm with a base learning rate of 0.05, a momentum of 0.9, and a weight decay of 0.0001 was utilized during the training process, which was carried out for 400 epochs. For the loss function, we used both cross-entropy and Dice losses (Loss = γ • L dice + (1γ) • L ce ), γ set to 0.6 empirically."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Datasets:,"We tested our model using the Synapse dataset [13], which comprises 30 cases of contrast-enhanced abdominal clinical CT scans (a total of 3,779 axial slices). Each CT scan consists of 85 ∼ 198 slices of the in-plane size of 512 × 512 and has annotations for eight different organs. We followed the same preferences for data preparation analogous to [5]. We also followed [2] experiments to evaluate our method on the ISIC 2018 skin lesion dataset [6] with 2,694 images.  Synapse Multi-organ Segmentation: Table 1 presents a comparison of our proposal with previous SOTA methods using the DSC and HD metrics across eight abdominal organs. Laplacian-Former clearly outperforms SOTA CNNbased methods. We extensively evaluated EfficientFormer (EffFormer) plus another drift of Laplacian-Former without utilizing the bridge connections to endorse the superiority of Laplacian-Former. Laplacian-Former exhibits superior learning ability on the Dice score metric compared to other transformer-based models, achieving an increase of +1.59% and +2.77% in Dice scores compared to HiFormer and Swin-Unet, respectively. Figure 3 illustrates a qualitative result of our method for different organ segmentation, specifically we can observe that the LalacianFormer produces a precise boundary segmentation on Gallbladder, Liver, and Stomach organs. It is noteworthy to mention that our pipeline, as a pure transformer-based architecture trained from scratch without pretraining weights, outperforms all previously presented network architectures.Skin Lesion Segmentation: Table 2a shows the comparison results of our proposed method, Laplacian-Former, against leading methods on the skin lesion segmentation benchmark. Our approach outperforms other competitors across most evaluation metrics, indicating its excellent generalization ability across different datasets. In particular, our approach performs better than hybrid methods such as TMU-Net [15] and pure transformer-based methods such as Swin-Unet [4].Our method achieves superior performance by utilizing the frequency attention in a pyramid scale to model local textures. Specifically, our frequency attention emphasizes the fine details and texture characteristics that are indicative of skin lesion structures and amplifies regions with significant intensity variations, thus accentuating the texture patterns present in the image and resulting in better performance. In addition, we provided the spectral response of LaplacianFormer vs. Standard Transformer in identical layers in Table 2b. It is evident Standard design frequency response in deep layers of structure attenuates more than the LaplacianFormer, which is a visual endorsement of the capability of Laplacian- Former for its ability to preserve high-frequency details. The supplementary provides more visualization results."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,4,Conclusion,"In this paper, we introduce Laplacian-Former, a novel standalone transformerbased U-shaped architecture for medical image analysis. Specifically, we address the transformer's inability to capture local context as high-frequency details, e.g., edges and boundaries, by developing a new design within a scaled dot attention block. Our pipeline benefits the multi-resolution Laplacian module to compensate for the lack of frequency attention in transformers. Moreover, while our design takes advantage of the efficiency of transformer architectures, it keeps the parameter numbers low."
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Fig. 1 .,
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Fig. 2 .,
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,8 W 8 ) 8 W 8 ) 8 W 8 ),
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Fig. 3 .,
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Table 1 .,
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,,Table 2 .,
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,1,Introduction,"Despite raising concerns about the environmental impact of artificial intelligence [35,37,38], the question of resource efficiency has not yet really reached medical imaging studies. The issue has multiple dimensions and the lack of clear metrics for a fair assessment of algorithms, in terms of resource and energy consumption, contrasts with the obvious healthcare benefits of the ever growing performance of machine and statistical learning solutions.In this work, we investigate the case of subtle abnormality detection in medical images, in an unsupervised context usually referred to as Unsupervised Anomaly Detection (UAD). This formalism requires only the identification of normal data to construct a normative model. Anomalies are then detected as outliers, i.e. as samples deviating from this normative model. Artificial neural networks (ANN) have been extensively used for UAD [21]. Either based on standard autoencoder (AE) architectures [3] or on more advanced architectures, e.g. combining a vector quantized AE with autoregressive transformers [33], ANN do not generally achieve an optimal trade-off between accuracy and computational demand. As an alternative, we show that more frugal approaches can be reached with traditional statistical models provided their cost in terms of memory usage can be addressed. Frugal solutions usually refer to strategies that can run with limited resources such as that of a single laptop. Frugal learning has been studied from several angles, in the form of constraints on the data acquired, on the algorithm deployed and on the nature of the proposed solution [9]. The angle we adopt is that of online or incremental learning, which refers to approaches that handle data in a sequential manner resulting in more efficient solutions in terms of memory usage and overall energy consumption. For UAD, we propose to investigate mixtures of probability distributions whose interpretability and versatility have been widely recognized for a variety of data and tasks, while not requiring excessive design effort or tuning. In particular, the use of multivariate Gaussian or generalized Student mixtures has been already demonstrated in many anomaly detection tasks, see [1,26,31] and references therein or [21] for a more general recent review. However, in their standard batch setting, mixtures are difficult to use with huge datasets due to the dramatic increase of time and memory consumption required by their estimation traditionally performed with an Expectation-Maximization (EM) algorithm [25]. Online more tractable versions of EM have been proposed and theoretically studied in the literature, e.g. [6,12], but with some restrictions on the class of mixtures that can be handled this way. A first natural approach is to consider Gaussian mixtures that belong to this class. We thus, present improvements regarding the implementation of an online EM for Gaussian mixtures. We then consider more general mixtures based on multiple scale t-distributions (MST) specifically adapted to outlier detection [10]. We show that these mixtures can be cast into the online EM framework and describe the resulting algorithm.Our approach is illustrated with the MR imaging exploration of de novo (just diagnosed) Parkinson's Disease (PD) patients, where brain anomalies are subtle and hardly visible in standard T1-weighted or diffusion MR images.The anomalies detected by our method are consistent with the Hoehn and Yahr (HY) scale [16], which describes how the symptoms of Parkinson's disease progress. The results provide additional interesting clinical insights by pointing out the most impacted subcortical structures at both HY stages 1 and 2. The use of such an external scale appears to be an original and relevant indirect validation, in the absence of ground truth at the voxel level. Energy and memory consumptions are also reported for batch and online EM to confirm the interesting performance/cost trade-off achieved. The code is available at https://github.com/geoffroyO/onlineEM."
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,2,UAD with Mixture Models,"Recent studies have shown that, on subtle lesion detection tasks with limited data, alternative approaches to ANN, such as one class support vector machine or mixture models [1,26], were performing similarly [31,34]. We further investigate mixture-based models and show how the main UAD steps, i.e. the construction of a reference model and of a decision rule, can be designed.Learning a Reference Model. We consider a set Y H of voxel-based features for a number of control (e.g. healthy) subjects, Y H = {y v , v ∈ V H } where V H represents the voxels of all control subjects and y v ∈ IR M is typically deduced from image modality maps at voxel v or from abstract representation features provided by some ANN performing a pre-text task [22]. To account for the distribution of such normal feature vectors, we consider two types of mixture models, mixtures of Gaussian distributions with high tractability in multiple dimensions and mixtures of multiple scale t-distributions (MST) that are more appropriate when the data present elongated and strongly non-elliptical subgroups [1,10,26]. By fitting such a mixture model to the control data Y H , we build a reference model density f H that depends on some parameterwith π k ∈ [0, 1], k=1:KH π k = 1 and K H the number of components, each characterized by a distribution f (•; θ k ). The EM algorithm is usually used to estimate Θ H that best fits Y H while K H can be estimated using the slope heuristic [2].Designing a Proximity Measure. Given a reference model (1), a measure of proximity r(y v ; Θ H ) of voxel v (with value y v ) to f H needs to be chosen. To make use of the mixture structure, we propose to consider distances to the respective mixture components through some weights acting as inverse Mahalanobis distances. We specify below this new proximity measure for MST mixtures. MST distributions are generalizations of the multivariate t-distribution that extend its Gaussian scale mixture representation [19]. The standard t-distribution univariate scale (weight) variable is replaced by a M -dimensional scale (weight)where G(•, νm  2 ) denotes the gamma density with parameter ( νm 2 , νm 2 ) ∈ R 2 and N M the multivariate normal distribution with mean parameter μ ∈ R M and covariance matrix DΔ w AD T showing the scaling by the W m 's through a diagonal matrix Δ w = diag(w -1 1 , . . . , w -1 M ). The MST parametrization uses the spectral decomposition of the scaling matrixThe scale variable W m for dimension m can be interpreted as accounting for the weight of this dimension and can be used to derive a measure of proximity. After fitting a mixture (1) with MST components to Y H , we set r(The proximity r is typically larger when at least one dimension of y v is well explained by the model. A similar proximity measure can also be derived for Gaussian mixtures, see details in the Supplementary Material Sect. 1.Decision Rule. For an effective detection, a threshold τ α on proximity scores can be computed in a data-driven way by deciding on an acceptable false positive rate (FPR) α; τ α is the value such that P (r(Y; Θ H ) < τ α ) = α, when Y follows the f H reference distribution. All voxels v whose proximity r(y v ; Θ H ) is below τ α are then labeled as abnormal. In practice, while f H is known explicitly, the probability distribution of r(Y; Θ H ) is not. However, it is easy to simulate this distribution or to estimate τ α as an empirical α-quantile [1]. Unfortunately, learning f H on huge datasets may not be possible due to the dramatic increase in time, memory and energy required by the EM algorithm. This issue often arises in medical imaging with the increased availability of multiple 3D modalities as well as the emergence of image-derived parametric maps such as radiomics [14] that should be analysed jointly, at the voxel level, and for a large number of subjects. A possible solution consists of employing powerful computers with graphics cards or grid-architectures in cloud computing. Here, we show that a more resource-friendly solution is possible using an online version of EM detailed in the next section."
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,3,Online Mixture Learning for Large Data Volumes,"Online learning refers to procedures able to deal with data acquired sequentially. Online variants of EM, among others, are described in [6,11,17,18,20,23,30]. As an archetype of such algorithms, we consider the online EM of [6] which belongs to the family of stochastic approximation algorithms [4]. This algorithm has been well theoretically studied and extended. However, it is designed only for distributions that admit a data augmentation scheme yielding a complete likelihood of the exponential family form, see (3) below. This case is already very broad, including Gaussian, gamma, t-distributions, etc. and mixtures of those. We recall below the main assumptions required and the online EM iteration.Assume (Y i ) n i=1 is a sequence of n independent and identically distributed replicates of a random variable Y ∈ Y ⊂ IR M , observed one at a time. Extension to successive mini-batches of observations is straightforward [30]. In addition, Y is assumed to be the visible part of the pair X = Y , Z ∈ X, where Z ∈ IR l is a latent variable, e.g. the unknown component label in a mixture model, and l ∈ IN. That is, each Y i is the visible part of a pair X i = Y i , Z i . Suppose Y arises from some data generating process (DGP) characterised by a probability density function f (y; θ 0 ), with unknown parameters θ 0 ∈ T ⊆ IR p , for p ∈ IN.Using the sequence (Y i ) n i=1 , the method of [6] sequentially estimates θ 0 provided the following assumptions are met:(A1) The complete-data likelihood for X is of the exponential family form: Let (γ i ) n i=1 be a sequence of learning rates in (0, 1) and let θ (0) ∈ T be an initial estimate of θ 0 . For each i = 1 : n, the online EM of [6] proceeds by computingandwhere s (0) = s(y 1 ; θ (0) ). It is shown in Thm. 1 of [6] that when n tends to infinity, the sequence (θ (i) ) i=1:n of estimators of θ 0 satisfies a convergence result to stationary points of the likelihood (cf. [6] for a more precise statement).In practice, the algorithm implementation requires two quantities, s in (4) and θ in (5). They are necessary to define the updating of sequences (s (i) ) i=1:∞ and (θ (i) ) i=1:∞ . We detail below these quantities for a MST mixture.Online MST Mixture EM. As shown in [29], the mixture case can be deduced from a single component case. The exponential form for a MST (2) writes:with s(y, w) = w 1 y, w 1 vec(yy ), w 1 , logw 1 , . . . , w M y, w M vec(yy ), w M , logw M , φ(μ, D, A, ν ) = [φ 1 , . . . , φ M ] T with φ m equal to:where d m denotes the m th column of D and vec(•) the vectorisation operator, which converts a matrix to a column vector. The exact form of h is not important for the algorithm. It follows that θ(s) is defined as the unique maximizer of function Q(s, θ) = s T φ(θ)ψ(θ) where s is a vector that matches the definition and dimension of φ(θ) and can be conveniently written as s = [s 11 , vec(S 21 ), s 31 , s 41 , . . . , s 1M , vec(S 2M ), s 3M , s 4M ] T , with for each m, s 1m is a M -dimensional vector, S 2m is a M ×M matrix, s 3m and s 4m are scalars. Solving for the roots of the Q gradients leads to θ(s) = (μ(s), Ā(s), D(s), ν(s)) whose expressions are detailed in Supplementary Material Sect.  6), these expectations need to be computed for y = y i the observation at iteration i. We therefore denote these expectations respectively by, where. The update of s (i) in ( 6) follows from the update for each m. From this single MST iteration, the mixture case is easily derived, see [29] or Supplementary Material Sect. 2.Online Gaussian Mixture EM. This case can be found in previous work e.g. [6,30] but to our knowledge, implementation optimizations are never really addressed. We propose an original version that saves computations, especially in a multivariate case where θ (s) involves large matrix inverses and determinants. Such inversions are avoided using results detailed in Supplementary Sect. 3."
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,4,Brain Abnormality Exploration in de novo PD Patients,"Data Description and Preprocessing. The Parkinson's Progression Markers Initiative (PPMI) [24] is an open-access database dedicated to PD. It includes MR images of de novo PD patients, as well as of healthy subjects (HC), all acquired on the same 3T Siemens Trio Tim scanner. For our illustration, we use 108 HC and 419 PD samples, each composed of a 3D T1-weighted image (T1w), Fractional Anisotropy (FA) and Mean Diffusivity (MD) volumes. The two latter are extracted from diffusion imaging using the DiPy package [13], registered onto T1w and interpolated to the same spatial resolution with SPM12. Standard T1w preprocessing steps, comprising non-local mean denoising, skull stripping and tissue segmentation are also performed with SPM12. HC and PD groups are age-matched (median age: 64 y.) with the male-female ratio equal to 6:4. We focus on some subcortical structures, which are mostly impacted at the early stage of the disease [7], Globus Pallidus external and internal (GPe and GPi), Nucleus Accumbens (NAC), Substantia Nigra reticulata (SNr), Putamen (Pu), Caudate (Ca) and Extended Amygdala (EXA). Their position is determined by projecting the CIT168 atlas [32] onto each individual image."
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,,Pipeline and Results,". We follow Sects. 2 and 3 using T1w, FA and MD volumes as features (M = 3) and a FPR α = 0.02. The pipeline is repeated 10 times for cross-validation. Each fold is composed of 64 randomly selected HC images for training (about 70M voxels), the remaining 44 HC and all the PD samples for testing. For the reference model, we test Gaussian and MST mixtures, with respectively K H = 14 and K H = 8, estimated with the slope heuristic. Abnormal voxels are then detected for all test subjects, on the basis of their proximity to the learned reference model, as detailed in Sect. 2. The PPMI does not provide ground truth information at the voxel level. This is a recurring issue in UAD, which limits validations to mainly qualitative ones. For a more quantitative evaluation, we propose to resort to an auxiliary task whose success is likely to be correlated with a good anomaly detection. We consider the classification of test subjects into healthy and Parkinsonian subjects based on their global (over all brain) percentages of abnormal voxels. We exploit the availability of HY values to divide the patients into two HY = 1 and HY = 2 groups, representing the two early stages of the disease's progression. Classification results yield a median g-mean, for stage 1 vs stage 2, respectively of 0.59 vs 0.63 for the Gaussian mixtures model and 0.63 vs 0.65 for the MST mixture. The ability of both mixtures to better differentiate stage 2 than stage 1 patients from HC is consistent with the progression of the disease. Note that the structural differences between these two PD stages remain subtle and difficult to detect, demonstrating the efficiency of the models. The MST mixture model appears better in identifying stage 2 PD patients based on their abnormal voxels.To gain further insights, we report, in Fig. 1, the percentages of anomalies detected in each subcortical structure, for control, stage 1 and stage 2 groups. For each structure and both mixture models, the number of anomalies increases from control to stage 1 and stage 2 groups. As expected the MST mixture shows a better ability to detect outliers with significant differences between HC and PD groups, while for the Gaussian model, percentages do not depart much from that in the control group. Overall, in line with the know pathophysiology [7], MST results suggest clearly that all structures are potential good markers of the disease progression at these early stages, with GPe, GPi, EXA and SNr showing the largest impact.Regarding efficiency, energy consumption in kilojoules (kJ) is measured using the PowerAPI library [5]. In Table 1, we report the energy consumption for the training and testing of one random fold, comparing our online mixtures with AEsupported methods for UAD [3], namely the patch-based reconstruction error [3] and FastFlow [39]. We implemented both methods with two different AE architectures: a lightweight AE already used for de novo PD detection [34], and a larger one, ResNet-18 [15]. The global g-mean (not taking HY stages into account) is also reported for the chosen fold. The experiments were run on a CPU with Intel Cascade Lake 6248@2.5 GHz (20 cores), and a GPU Nvidia V100-32 GB. Online mixtures exhibit significantly lower energy consumption, both for training and inference. In terms of memory cost, DRAM peak results, as measured by the tracemalloc Python library, also show lower costs for online mixtures, which by design deal with batches of voxels of smaller sizes than the batches of patches used in AE solutions. These results highlight the advantage of online mixtures, which compared to other hardware-demanding methods, can be run on a minimal configuration while maintaining good performance."
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,5,Conclusion and Perspectives,"Despite a challenging medical problematic of PD progression at early stages, we have observed that energy and memory efficient methods could yield interesting and comparable results with other studies performed on the same database [28,34] and with similar MR modalities [8,26,27,36]. An interesting future work would be to investigate the possibility to use more structured observations, such as patch-based features [28] or latent representations from a preliminary pretext task, provided the task cost is reasonable. Overall, we have illustrated that the constraints of Green AI [35] could be considered in medical imaging by producing innovative results without increasing computational cost or even reducing it.We have investigated statistical mixture models for an UAD task and shown that their expressivity could account for multivariate reference models, and their much simpler structure made them more amenable to efficient learning than most ANN solutions. Although very preliminary, we hope this attempt will open the way to the development of more methods that can balance the environmental impact of growing energy cost with the obtained healthcare benefits."
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,,,
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,,,
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,,Fig. 1 .,
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,,Table 1 .,
Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 40.
Understanding Silent Failures in Medical Image Classification,1,Introduction,"Although machine learning-based classification systems have achieved significant breakthroughs in various research and practical areas, their clinical application is still lacking. A primary reason is the lack of reliability, i.e. failure cases produced by the system, which predominantly occur when deployment data differs from the data it was trained on, a phenomenon known as distribution shifts. In medical applications, these shifts can be caused by image corruption (""corruption shift""), unseen variants of pathologies (""manifestation shift""), or deployment in new clinical sites with different scanners and protocols (""acquisition shift"") [4]. The robustness of a classifier, i.e. its ability to generalize across these shifts, is extensively studied in the computer vision community with a variety of recent benchmarks covering nuanced realistic distribution shifts [13,15,19,27], and is also studied in isolated cases in the biomedical community [2,3,33]. Despite these efforts, perfect classifiers are not to be expected, thus a second mitigation strategy is to detect and defer the remaining failures, thus preventing failures to be silent. This is done by means of confidence scoring functions (CSF) of different types as studied in the fields of misclassification detection (MisD) [5,11,23], Out-of-Distribution detection (OoD-D) [6,7,11,20,21,32], selective classification (SC) [9,10,22], and predictive uncertainty quantification (PUQ) [18,25].We argue, that silent failures, which occur when test cases break both the classifier and the CSF, are a significant bottleneck in the clinical translation of ML systems and require further attention in the medical community.Note that the task of silent failure prevention is orthogonal to calibration, as, for example, a perfectly calibrated classifier can still yield substantial amounts of silent failures and vice versa [15].Bernhardt et al. [3] studied failure detection on several biomedical datasets, but only assessed the performance of CSFs in isolation without considering the classifier's ability to prevent failures. Moreover, their study did not include distribution shifts thus lacking a wide range of realistic failure sources. Jaeger et al. [15], on the other hand, recently discussed various shortcomings in current research on silent failures including the common lack of distribution shifts and the lack of assessing the classifier and CSF as a joint system. However, their study did not cover tasks from the biomedical domain.In this work, our contribution is twofold: 1) Building on the work of Jaeger et al. [15], we present the first comprehensive study of silent failure prevention in the biomedical field. We compare various CSFs under a wide range of distribution shifts on four biomedical datasets. Our study provides valuable insights and the underlying framework is made openly available to catalyze future research in the community. 2) Since the benchmark reveals that none of the predominant CSFs can reliably prevent silent failures in biomedical tasks, we argue that a deeper understanding of the root causes in the data itself is required. To this end, we present SF-Visuals, a visualization tool that facilitates identifying silent failures in a dataset and investigating their causes (see Fig. 1). Our approach contributes to recent research on visual analysis of failures [13], which has not focused on silent failures and distribution shifts before."
Understanding Silent Failures in Medical Image Classification,2,Methods,"Benchmark for Silent Failure Prevention under Distribution Shifts. We follow the spirit of recent robustness benchmarks, where existing datasets have been enhanced by various distribution shifts to evaluate methods under a wide range of failure sources and thus simulate real-world application [19,27]. To our knowledge, no such comprehensive benchmark currently exists in the biomedical domain. Specifically, we introduce corruptions of various intensity levels to the images in four datasets in the form of brightness, motion blur, elastic transformations and Gaussian noise. We further simulate acquisition shifts and manifestation shifts by splitting the data into ""source domain"" (development data) and ""target domain"" (deployment data) according to sub-class information from the meta-data such as lesion subtypes or clinical sites. Dermoscopy dataset: We combine data from ISIC 2020 [26], derma 7 point [17], PH2 [24] and HAM10000 [30] and map all lesion sub-types to the super-classes ""benign"" or ""malignant"". We emulate two acquisition shifts by defining either images from the Memorial Sloan Kettering Cancer Center (MSKCC) or Hospital Clinic Barcelona (HCB) as the target domain and the remaining images as the source domain. Further, a manifestation shift is designed by defining the lesion subtypes ""keratosis-like"" (benign) and ""actinic keratosis"" (malignant) as the target domain. Chest X-ray dataset: We pool the data from CheXpert [14], NIH14 [31] and MIMIC [16], while only retaining the classes common to all three. Next, we emulate two acquisition shifts by defining either the NIH14 or the CheXpert data as the target domain. FC-Microscopy dataset: The RxRx1 dataset [28] represents the fluorescence cell microscopy domain. Since the images were acquired in 51 deviating acquisition steps, we define 10 of these batches as target-domain to emulate an acquisition shift. Lung Nodule CT dataset: We create a simple 2D binary nodule classification task based on the 3D LIDC-IDRI data [1] by selecting the slice with the largest annotation per nodule (±two slices resulting in 5 slices per nodule). Average malignancy ratings (four raters per nodule, scores between 1 and 5) > 2 are considered malignant and all others as benign. We emulate two manifestation shifts by defining nodules with high spiculation (rating > 2), and low texture (rating < 3) as target domains.The datasets consist only of publicly available data, our benchmark provides scripts to automatically generate the combined datasets and distribution shifts.The SF-Visuals Tool: Visualizing Silent Failures. The proposed tool is based on three simple operations, that enable effective and intuitive analysis of silent failures in datasets across various CSFs: 1) Interactive Scatter Plots: See example in Fig. 1b. We first reduce the dimensionality of the classifier's latent space to 50 using principal component analysis and use t-SNE to obtain the final 3-dimensional embedding. Interactive functionality includes coloring dots via pre-defined schemes such as classes, distribution shifts, classifier confusion matrix, or CSF confusion matrix. The associated images are displayed upon selection of a dot to establish a direct visual link between input space and embedding. 2) Concept Cluster Plots: See examples in Fig. 1c. To abstract away from individual points in the scatter plot, concepts of interest, such as classes or distribution shifts can be defined and visualized to identify conceptual commonalities and differences in the data as perceived by the model. Therefore, k-means clustering is applied to the 3-dimensional embedding. Nine clusters are identified per concept and the resulting plots show the closest-to-center image per cluster as a visual representation of the concept. 3) Silent Failure Visualization: See examples in Fig. 2. We sort all failures by the classifier confidence and by default show the images associated with the top-two most confident failures. For corruption shifts, we further allow investigating the predictions on a fixed input image over varying intensity levels.Based on these visualizations, the functionality of SF-Visuals is three-fold: 1) Visual analysis of the dataset including distribution shifts. 2) Visual analysis of the general behavior of various CSFs on a given task 3) Visual analysis of individual silent failures in the dataset for various CSFs."
Understanding Silent Failures in Medical Image Classification,3,Experimental Setup,"Evaluating Silent Failure Prevention: We follow Jaeger et al. [15] in evaluating silent failure prevention as a joint task of the classifier and the CSF. The area under the risk-coverage curve AURC reflects this task, since it considers both the classifier's accuracy as well as the CSF's ability to detect failures by assigning low confidence scores. Thus, it can be interpreted as a silent failure rate or the error rate averaged over steps of filtering cases one by one according to their rank of confidence score (low to high). Exemplary risk-coverage curves are shown in Appendix Fig. 3. Compared Confidence Scoring Functions: We compare the following CSFs: The maximum softmax response (MSR) and the predictive entropy computed from the classifier's softmax output, three predictive uncertainty measures based on Monte-Carlo Dropout (MCD) [8], namely mean softmax (MCD-MSR), predictive entropy (MCD-PE) and expected entropy (MCD-EE), ConfidNet [5], which is trained as an extension to the classifier, DeepGamblers (DG) that learns a confidence like reservation score (DG-Res) [22] and the work of DeVries et al. [6]. Training Settings: On each dataset, we employ the classifier behind the respective leading results in literature: For chest Xray data we use DenseNet121 [12], for dermoscopy data we use EfficientNet-B4 [29] and for fluorescence cell microscopy and lung nodule CT data we us DenseNet161 [12]. We select the initial learning rate between 10 -3 and 10 -5 and weight decay between 0 and 10 -5 via grid search and optimize for validation accuracy. All models were trained with dropout. All hyperparameters can be found in Appendix Table 3."
Understanding Silent Failures in Medical Image Classification,4,Results,
Understanding Silent Failures in Medical Image Classification,4.1,Silent Failure Prevention Benchmark,Table 1 shows the results of our benchmark for silent failure prevention in the biomedical domain and provides the first overview of the current state of the reliability of classification systems in high-stake biomedical applications.
Understanding Silent Failures in Medical Image Classification,,None of the Evaluated Methods from the Literature Beats the Maximum Softmax Response Baseline Across a Realistic Range of Failure,"Sources. This result is generally consistent with previous findings in Bernhard et al. [3] and Jaeger et al. [15], but is shown for the first time for a diverse range of realistic biomedical failure sources. Previously proposed methods do not outperform MSR baselines even in the settings they have been proposed for, e.g. Devries et al. under distribution shifts, or ConfidNet and DG-RES for i.i.d. testing. MCD and Loss Attenuation are Able to Improve the MSR. MCD-MSR is the overall best performing method indicating that MCD generally improves the confidence scoring ability of softmax outputs on these tasks. Interestingly, the DG loss attenuation applied to MCD-MSR, DG-MCD-MSR, which has not been part of the original DG publication but was first tested in Jaeger et al. [15], shows the best results on i.i.d. testing on 3 out of 4 tasks. However, the method is not reliable across all settings, falling short on manifestation shifts and corruptions on the lung nodule CT dataset. Effects of Particular Shifts on the Reliability of a CSF Might Be Interdependent. When looking beyond the averages displayed in Table 1 and analyzing the results of individual clinical centers, corruptions and manifestation shifts, one remarkable pattern can be observed: In various cases, the same CSF showed opposing behavior between two variants of the same shift on the same dataset. For instance, Devries et al. outperforms all other CSFs for one clinical site (MSKCC) as target domain, but falls short on the other one (HCB).On the Chest X-ray dataset, MCD worsens the performance for darkening corruptions across all CSFs and intensity levels, whereas the opposite is observed for brightening corruptions. Further, on the lung nodule CT dataset, DG-MCD-RES performs best on bright/dark corruptions and the spiculation manifestation shift, but worst on noise corruption and falls behind on the texture manifestation shift. These observations indicate trade-offs, where, within one distribution shift, reliability against one domain might induce susceptibility to other domains. Current Systems are Not Generally Reliable Enough for Clinical Application. Although CSFs can mitigate the rate of silent failures (see Appendix Fig. 3), the reliability of the resulting classification systems is not sufficient for high-stake applications in the biomedical domain, with substantial rates of silent failure in three out of four tasks. Therefore, a deeper understanding of the root causes of these failures is needed. "
Understanding Silent Failures in Medical Image Classification,4.2,Investigation of Silent Failure Sources,"SF-Visuals Enables Comprehensive Analysis of Silent Failures. Figure 1 vividly demonstrates the added benefit of the proposed tool. First, an Interactive Scatter Plot (Fig. 1b, left) provides an overview of the MSKCC acquisition shift on the dermoscopy dataset and reveals a severe change of the data distribution. For instance, some malignant lesions of the target domain (purple dots) are located deep within the ""benign"" cluster. Figure 1c provides a Concept Cluster Plot that visually confirms how some of these lesions (purple dot) share characteristics of the benign cluster of the source domain (turquoise dot), such as being smaller, brighter, and rounder compared to malignant source-lesions (blue dot). The right-hand plot of Fig. 1b reveals that these cases have in fact caused silent failures (red crosses) and visual inspection (see arrow and Fig. 1a) confirms the hypothesis that these failures have been caused by the fact that the acquisition shift introduced malignant target-lesions that exhibit benign characteristics. Figure 1b  In both examples, the brightening of the image leads to a malignant lesion taking on benign characteristics (brighter and smoother skin on the dermoscopy data, decreased contrast between lesion and background on the Lung Nodule CT data). Acquisition shift: Additionally to the example in Fig. 1, Fig. 2e shows how the proposed tool visualizes an acquisition shift on the chest X-ray data. While this reveals an increased blurriness in the target domain, it is difficult to derive further insights involving specific pathologies without a clinical expert. Figure 2h shows a classification failure from a target clinical center together with the model's confidence as measured by MSR and DG. While MSR assigns the prediction low confidence thereby catching the failure, DG assigns high confidence for the same model and prediction, causing a silent failure. This example shows how the tool allows the comparison of CSFs and can help to identify failure modes specific to each CSF. Manifestation shift: On the dermoscopy data (Fig. 2g), we see how a manifestation shift can cause silent failures. The benign lesions in the target domain are similar to the malignant lesions in the source domain (rough skin, irregular shapes), and indeed the two failures in the target domain seem to fall into this trap. On the lung nodule CT data (Fig. 2f), we observe a visual distinction between the spiculated target domain (spiked surface) and the non-spiculated source domain (smooth surface)."
Understanding Silent Failures in Medical Image Classification,5,Conclusion,"We see two major opportunities for this work to make an impact on the community. 1) We hope the revealed shortcomings of current systems on biomedical tasks in combination with the deeper understanding of CSF behaviors granted by SF-Visuals will catalyze research towards a new generation of more reliable CSFs. 2) This study shows that in order to progress towards reliable ML systems, a deeper understanding of the data itself is required. SF-Visuals can help to bridge this gap and equip researchers with a better intuition of when and how to employ ML systems for a particular task."
Understanding Silent Failures in Medical Image Classification,,Fig. 1 .,
Understanding Silent Failures in Medical Image Classification,,Fig. 2 .,
Understanding Silent Failures in Medical Image Classification,,"Table 1 . Silent failure prevention benchmark results measured in AURC[%] (score range: [0, 100], lower is better).",
Understanding Silent Failures in Medical Image Classification,,,"SF-Visuals Generates Insights Across Tasks and Distribution Shifts. i.i.d. (No Shift):This analysis reveals how simple class clustering (no distribution shifts involved) can help to gain intuition on the most severe silent failures (examples selected as the two highest-confidence failures). On the lung nodule CT data (Fig.2a), we see how the classifier and CSF break down when a malignant sample (typically: small bright, round) exhibits characteristics typical to benign lesions (larger, less cohesive contour, darker) and vice versa. This pattern of contrary class characteristics is also observed on the dermoscopy dataset (2c). The failure example at the top is particularly severe, and localization in the scatter plot reveals a position deep inside the 'benign' cluster indicating either a severe sampling error in the dataset (e.g. underrepresented lesion subtype) or simply a wrong label. Corruption shift: Figs.2b and 2dshow for the Lung Nodule CT data and the dermoscopy data, respectively, how corruptions can lead to silent failures in low-confident predictions."
Understanding Silent Failures in Medical Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 39.
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,1,Introduction,"Detecting out-of-distribution (OOD) samples is crucial in real-world applications of machine learning, especially in medical imaging analysis where misdiagnosis can pose significant risks [7]. Recently, deep neural networks, particularly ResNets [9] and U-Nets [15], have been widely used in various medical imaging applications such as classification and segmentation tasks, achieving state-ofthe-art performance. However, due to the typical overconfidence seen in neural networks [8,18], deep learning with uncertainty estimation is becoming increasingly important in OOD detection.Deep learning-based OOD detection methods with uncertainty estimation, such as Evidential Deep Learning (EDL) [10,17] and its variants [2,[11][12][13]24], have shown their superiority in terms of computational performance, efficiency, and extensibility. However, most of these methods consider identifying outliers that significantly differ from training samples(e.g. natural images collected from ImageNet [5]) as OOD samples [1]. These approaches overlook the inherent near OOD problem in medical images, in which instances belong to categories or classes that are not present in the training set [21] due to the differences in morbidities. Failing to detect such near OOD samples poses a high risk in medical application, as it can lead to inaccurate diagnoses and treatments. Some recent works have been proposed for near OOD detection based on density models [20], preprocessing [14], and outlier exposure [16]. Nevertheless, all of these approaches are susceptible to the quality of the training set, which cannot always be guaranteed in clinical applications.To address this limitation, we propose an Evidence Reconciled Neural Network (ERNN), which aims to reliably detect those samples that are similar to the training data but still with different distributions (near OOD), while maintain accuracy for In-Distribution (ID) classification. Concretely, we introduce a module named Evidence Reconcile Block (ERB) based on evidence offset. This module cancels out the conflict evidences obtained from the evidential head, maximizes the uncertainty of derived opinions, thus minimizes the error of uncertainty calibration in OOD detection. With the proposed method, the decision boundary of the model is restricted, the capability of medical outlier detection is improved and the risk of misdiagnosis in medical images is mitigated. Extensive experiments on both ISIC2019 dataset and in-house pancreas tumor dataset demonstrate that the proposed ERNN significantly improves the reliability and accuracy of OOD detection for clinical applications. Code for ERNN can be found at https://github.com/KellaDoe/ERNN."
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,2,Method,"In this section, we introduce our proposed Evidence Reconciled Neural Network (ERNN) and analyze its theoretical effectiveness in near OOD detection. In our approach, the evidential head firstly generates the original evidence to support the classification of each sample into the corresponding class. And then, the proposed Evidence Reconcile Block (ERB) is introduced, which reforms the derived evidence representation to maximize the uncertainty in its relevant opinion and better restrict the model decision boundary. More details and theorical analysis of the model are described below."
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,2.1,Deep Evidence Generation,"Traditional classifiers typically employ a softmax layer on top of feature extractor to calculate a point estimation of the classification result. However, the point estimates of softmax only ensure the accuracy of the prediction but ignore the confidence of results. To address this problem, EDL utilizes the Dirichlet distribution as the conjugate prior of the categorical distribution and replaces the softmax layer with an evidential head which produces a non-negative output as evidence and formalizes an opinion based on evidence theory to explicitly express the uncertainty of generated evidence.  whereBased on the fact that the parameters of the categorical distribution should obey Dirichlet distribution, the model prediction ŷ and the expected cross entropy loss L ece on Dirichlet distributioncan be inferred as:(2)"
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,2.2,Evidence Reconcile Block,"In case of OOD detection, since the outliers are absent in the training set, the detection is a non-frequentist situation. Referring to the subjective logic [10], when a variable is not governed by a frequentist process, the statical accumulation of supporting evidence would lead to a reduction in uncertainty mass. Therefore, traditional evidence generated on the basis accumulation is inapplicable and would lead to bad uncertainty calibration in OOD detection. Moreover, the higher the similarity between samples, the greater impact of evidence accumulation, which results in a dramatic performance degradation in medical near OOD detection.To tackle the problem mentioned above, we propose an Evidence Reconcile Block (ERB) that reformulates the representation of original evidence and minimizes the deviation of uncertainty in evidence generation. In the proposed ERB, different pieces of evidence that support different classes are canceled out by transforming them from subjective opinion to epistemic opinion and the theoretical maximum uncertainty mass is obtained.As shown in Fig. 1, the simplex corresponding to K-class opinions has K dimensions corresponding to each category and an additional dimension representing the uncertainty in the evidence, i.e., vacuity in EDL. For a given opinion ω, its projected predictive probability is shown as p with the direction determined by prior a. To ensure the consistency of projection probabilities, epistemic opinion ω should also lie on the direction of projection and satisfy that at least one belief mass of ω is zero, corresponding to a point on a side of the simplex. Let ü denotes the maximum uncertainty, it should satisfy:Since a is a uniform distribution defined earlier, the transformed belief mass can be calculated as: b = bb min , where b min is the minimum value in the original belief mass b. Similarly, the evidence representation ë in our ERB, based on epistemic opinion ω, can be formulated as:]a = e -min i e, f or i ∈ {1, . . . , K}.(After the transformation by ERB, the parameters α = ë + 1 of Dirichlet distribution associate with the reconciled evidence can be determined, and the reconciled evidential cross entropy loss L rece can be inferred as (6), in which S = K i=1 αi .By reconciling the evidence through the transformation of epistemic opinion in subjective logic, this model can effectively reduce errors in evidence generation caused by statistical accumulation. As a result, it can mitigate the poor uncertainty calibration in EDL, leading to better error correction and lower empirical loss in near OOD detection, as analysized in Sect. 2.3. As shown in Fig. 2, we utilize samples from three Gaussian distributions to simulate a 3-classification task and generate evidences based on the probability density of each class. When using the traditional CNN to measure the uncertainty of the output with predictive entropy, the model is unable to distinguish far OOD due to the normalization of softmax. While the introduction of evidence representation in the vacuity of EDL allows effective far OOD detections. However, due to the aforementioned impact of evidence accumulation, we observe that the EDL has a tendency to produce small uncertainties for outliers close to in-distribution (ID) samples, thus leading to failures in detecting near OOD samples. Our proposed method combines the benefits of both approaches, the evidence transformed by ERB can output appropriate uncertainty for both near and far OOD samples, leading to better identification performance for both types of outliers."
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,2.3,Theorical Analysis of ERB,"To further analyze the constraint of the proposed model in OOD detection, we theoretically analyze the difference between the loss functions before and after the evidence transformation, as well as why it can improve the ability of near OOD detection. Detailed provements of following propositons are provided in Supplements.Proposition 1. For a given sample in K-classification with the label c andThe misclassified ID samples with p c = α c /S ≤ 1/K are often located at the decision boundary of the corresponding categories. Based on Proposition 1, the reconciled evidence can generate a larger loss, which helps the model focus more on the difficult-to-distinguish samples. Therefore, the module can help optimize the decision boundary of ID samples, and promote the ability to detect near OOD.Proposition 2. For a given sample in K-classification with the label c and K i=1 α i = S, for any α c > S K , L rece < L ece is satisfied. Due to the lower loss derived from the proposed method, we achieve better classification accuracy and reduce empirical loss, thus the decision boundary can be better represented for detecting outliers. Proposition 3. For a given sample in K-classification and Dirichlet distribution parameter α, when all values of α equal to const α, L rece ≥ L ece is satisfied.During the training process, if the prediction p of ID samples is identical to the ideal OOD outputs, the proposed method generates a greater loss to prevent such evidence from occurring. This increases the difference in predictions between ID and OOD samples, thereby enhancing the ability to detect OOD samples using prediction entropy.In summary, the proposed Evidence Reconciled Neural Network (ERNN) optimizes the decision boundary and enhances the ability to detect near OOD samples. Specifically, our method improves the error-correcting ability when the probability output of the true label is no more than 1/K, and reduces the empirical loss when the probability output of the true label is greater than 1/K. Furthermore, the proposed method prevents model from generating same evidence for each classes thus amplifying the difference between ID and OOD samples, resulting in a more effective near OOD detection."
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3,Experiments,
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3.1,Experimental Setup,"Datasets. We conduct experiments on ISIC 2019 dataset [3,4,19] and an inhouse dataset. ISIC 2019 consists of skin lesion images in JPEG format, which are categorized into NV (12875), MEL (4522), BCC (3323), BKL (2624), AK (867), SCC (628), DF (239) and VASC (253), with a long-tailed distribution of classes. In line with the settings presented in [14,16], we define DF and VASC, for which samples are relatively scarce as the near-OOD classes. The in-house pancreas tumor dataset collected from a cooperative hospital is composed of eight classes: PDAC (302), IPMN (71), NET (43), SCN (37), ASC (33), CP (6), MCN (3), and PanIN (1). For each sequence, CT slices with the largest tumor area are picked for experiment. Similarly, PDAC, IPMN and NET are chosen as ID classes, while the remaining classes are reserved as OOD categories.Implementations and Evaluation Metrics. To ensure fairness, we used pretrained Resnet34 [9] as backbone for all methods. During our training process, the images were first resized to 224 × 224 pixels and normalized, then horizontal and vertical flips were applied for augmentation. The training was performed using one GeForce RTX 3090 with a batch size of 256 for 100 epochs using the AdamW optimizer with an initial learning rate of 1e-4 along with exponential decay. Note that we employed five-fold cross-validation on all methods, without using any additional OOD samples during training. Furthermore, we selected the precision(pre), recall(rec), and f1-score(f1) as the evaluation metrics for ID samples, and used the Area Under Receiver Operator Characteristic (AUROC) as OOD evaluation metric, in line with the work of [6]."
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3.2,Comparison with the Methods,"In the experiment, we compare the OOD detection performance of ERNN to several uncertainty-based approaches:• Prototype Network described in [22], where the prototypes of classes are introduced and the distance is utilized for uncertainty estimation. • Prior Network described in [12], in which the second order dirichlet distribution is utlized to estimate uncertainty. • Evidential Deep Learning described in [17], introduces evidence representation and estimates uncertainty through subjective logic. • Posterior Network described in [2], where density estimators are used for generating the parameters of dirichlet distributions.Inspired by [14], we further compare the proposed method with Mixup-based methods:• Mixup: As described in [23], mix up is applied to all samples.• MT-mixup: Mix up is only applied to mid-class and tail-class samples.• MTMX-Prototype: On the basis of MT mixup, prototype network is also applied to estimate uncertainty.The results on two datasets are shown in Table 1. We can clearly observe that ERNN consistently achieves better OOD detection performance than other uncertainty-based methods without additional data augmentation. Even with using Mixup, ERNN exhibits near performance with the best method (MTMX-Prototype) on ISIC 2019 and outperforms the other methods on in-house datasets. All of the experimental results verify that our ERNN method improves OOD detection performance while maintaining the results of ID classification even without any changes to the existing architecture.  "
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3.3,Ablation Study,"In this section, we conduct a detailed ablation study to clearly demonstrate the effectiveness of our major technical components, which consist of evaluation of evidential head, evaluation of the proposed Evidence Reconcile Block on both ISIC 2019 dataset and our in-house pancreas tumor dataset. Since the Evidence Reconcile Block is based on the evidential head, thus there are four combinations, but only three experimental results were obtained. As shown in Table 2, It is clear that a network with an evidential head can improve the OOD detection capability by 6% and 1% on ISIC dataset and in-house pancreas tumor dataset respectively. Furthermore, introducing ERB further improves the OOD detection performance of ERNN by 1% on ISIC dataset. And on the more challenging inhouse dataset, which has more similarities in samples, the proposed method improves the AUROC by 2.3%, demonstrating the effectiveness and robustness of our model on more challenging tasks."
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,4,Conclusion,"In this work, we propose a simple and effective network named Evidence Reconciled Nueral Network for medical OOD detection with uncertainty estimation, which can measure the confidence in model prediction. Our method addresses the failure in uncertainty calibration of existing methods due to the similarity of near OOD with ID samples. With the evidence reformation in the proposed Evidence Reconcile Block, the error brought by accumulative evidence generation can be mitigated. Compared to existing state-of-the-art methods, our method can achieve competitive performance in near OOD detection with less loss of accuracy in ID classification. Furthermore, the proposed plug-and-play method can be easily applied without any changes of network, resulting in less computation cost in identifying outliers. The experimental results validate the effectiveness and robustness of our method in the medical near OOD detection problem."
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,,Fig. 1 .,
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,,"Formally, the evidence",
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,,Fig. 2 .,
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,,Table 1 .,
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,,Table 2 .,
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_30.
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,1,Introduction,"Lung cancer is the main cause of cancer death worldwide [18]. Pulmonary nodules and masses are both features present in computed tomography images that aid in the diagnosis of lung cancer. The primary difference is that a nodule is smaller than 30 mm in diameter, while a mass is larger than 30 mm [22]. Early detection of these features is crucial to aid physicians in making a diagnosis of Z. Li and J. Yang-Equal contributions. Visualization on results of four large-scale mass segmentation given by nnU-Net baseline [7]. Compared with the ground-truth segmentation, the recall rate for these four samples is 46.29%, 58.34%, 79.51%, and 68.51%, respectively. This is significantly lower than the mean value of 81.68%. (b): Statistics of the number of nodules at different scales in three datasets. The range of nodule diameter corresponding to Micro, Small, Medium, and Mass is (0, 10], (10,20], (20,30], [30, ∞), respectively. (c) : The distribution of recall rate with respect to the nodule size. Existing methods have low recall rates for the segmentation of large scale nodules and masses.benign or malignant tumors [27] and determining follow-up treatment. Lesion segmentation can be utilized to evaluate two important factors: the volume of the lesion and its growth rate [5,6,8,12]. Furthermore, obtaining accurate information regarding the nodule can assist in determining the appropriate resection method and surgical margin required to preserve as much lung function as possible. [14,17].Segmenting nodules is a tedious task that requires significant human labor. Computer aided diagnosis (CAD) systems can significantly reduce such heavy workloads. The accuracy of the existing nodule detection model reaches 96.1% [9] accuracy. However, the accuracy of the 3D nodule segmentation model is prone to significantly decline in the application, regardless of whether its structure is based on CNN or Transformer [2]. As shown in Fig. 1(a-c), the recall rate of the large-scale nodule and mass is usually lower than the average level. The main reason is that the lesion scale in the two public datasets are relatively small, which matches the fact few patients have very large nodule or mass. This makes the pulmonary nodule and mass segmentation task resemble a long-tail problem rather than a mere large scale span problem. This leads to unsatisfactory results when segmenting large lesions that require more accurate delineation [26].Several studies have proposed solutions to tackle the large scale span challenges at both the input and feature level. For instance, some approaches adopt multi-scale inputs [4], where the input images are resized to different resolu-tion ratios. Some other methods leverage multi-scale feature maps to capture information from different scales, such as cross-scale feature fusion [19] or using multi-scale convolutional filters [3]. Furthermore, the attention mechanisms [23] has also been utilized to emphasize the features that are more relevant for segmentation. Though these methods have achieved impressive performance, they still struggle to accurately segment the extremely imbalanced multi-scale lesions.Recently, some click-based lesion segmentation methods [19][20][21] introduce the click at the input or feature level and modify the network accordingly, resulting in higher accuracy results. Yet, the click input does not provide the scale information of lesions for the network.In this paper, we propose a scale-aware test-time click adaptation (SaTTCA) method, which simply utilizes easily obtainable lesion click (i.e., the center detected nodule) to adjust the parameters of the network normalization layers [24] during testing. Note that we do not need to exploit any data from the training set. Specifically, we expand the click into an ellipsoid mask, which supervises the test-time adaptation. This helps to improve the segmentation performance of large-scale nodules and masses. Additionally, we also propose a multi-scale input encoder to further address the problem of imbalanced lesion scales. Experimental results on two public datasets and one in-house dataset demonstrate that the proposed method outperforms existing methods with different backbones."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,2,Method,
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,2.1,Restatement of Image Segmentation Based on Click,"For pulmonary nodule and mass segmentation, existing methods mostly rely on regions of interest (ROI) obtained by lesion detection networks. A set of 3D ROI inputs I can be represented as I ∈ R D×H×W with size (D, H, W ), along with its corresponding segmentation ground truth of nodules and masses represented by S ∈ (0, 1)D×H×W . Typically, a neural network with weighted parameters θ is trained to predict the lesion area Ŝ = θ(I), with the goal of minimizing the loss function L (S, Ŝ). The stochastic gradient descent (SGD) and the automatic data acquisition module weight decay (AdamW) optimizers are usually used to optimize the weighted parameters.For each ROI input, the center point C of the lesion, which is represented as2 ) in Cartesian coordinate system, can be used as a reference point to assist the network in improving segmentation performance. This can be achieved either through an artificial or automatic approach, for instance, by adding click channels directly to the input or by adding a prior encoder to the network as demonstrated by the methods [19,20]. However, incorporating clicks in this way does not focus on addressing the extremely imbalanced lesion scales."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,2.2,Network Architecture,"The network structure of the proposed method, as shown in Fig. 2, is enhanced with a multi-scale (MS) input encoder to address the issue of multi-scale lesions. We first get the predicted segmentation Ŝi and compute its minimum 3D bounding box B from the trained model. Then we generate an ellipsoid mask Mi (around the center Ci of the detected nodule) whose size is proportional to the size of B to supervise the parameter updating during test-time adaptation. Our SaTTCA method is applicable to backbones on CNN and Transformer. We also adopt a multiscale input encoder to further improve the segmentation performance of nodules and masses with different scales.To achieve this, we employ a clipping strategy to adjust the proportion of foreground and background in the input image, producing a group of input images with dimensions of 64 × 96 × 96, 32 × 48 × 48, and 16 × 24 × 24. These images are then passed through three convolution paths. The feature maps are concatenated as they are down-sampled to the same scale. The subsequent modules can be based on either CNN or transformer structures. The multi-scale input encoder allows the network to capture more scale information of the nodules and masses, thus mitigating the problem of large lesion scale span."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,2.3,Scale-Aware Test-Time Click Adaptation,"In clinical scenarios, the neural network for assisted diagnosis is generally a pretraining model. Due to differences in the statistical distribution of pulmonary nodule scale in image data from different medical centers, the segmentation results of some images, especially for large nodules, are worse than expected. For such scenarios, we propose the Scale-aware Test-time Click Adaptation method, which can improve the performance of segmentation results for largescale nodules and masses by adjusting some of the network parameters during testing. The pipeline of the proposed method is shown in Fig. 2. First, we use the pre-trained network to pre-segment the input CT from the test set, getting Ŝi = θwhere n is the number of samples in the test set. Then we make a projection on the main connected region of Ŝi along three coordinate axes to obtain the size of the bounding box B i = (d, w, h) of the pre-segmentation result, and generate an ellipsoid M i with three axes length proportional to the corresponding side length of the bounding box B i . More formally, the coordinates of any foreground voxel point V : (x, y, z) in M i meets the following requirement:where R represents the mapping function between the axis length of the ellipsoid and the side length of the bounding box B i . Taking the x-axis as an example, R(d) is given by: R (d) = min (0.02To account for the introduction of error information at some voxels during adaptive click adjustment, we develop a mapping function to generate M i adaptively based on the size of nodules and masses. If the nodule's length and diameter are less than 7 mm, M i degenerates into a voxel. When the predicted nodule size ranges from 7 mm to 40 mm, the axial length of B i and the side length of the bounding box follow a quadratic nonlinear relationship. If the predicted nodule size is greater than 40 mm, the axial length of M i has a linear relationship with the side length. To determine the super parametric values for the mapping function R, we perform cross-validation on three datasets."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,2.4,Training Objective of SaTTCA,"We use the foreground range of adaptively adjusted ellipsoid M i to mask Ŝi to obtain a masked segmentation ŜM i . Then we use M i to adjust the normalization layer parameters in the network during testing [24]. The test-time loss function L tt is the weighted sum of the binary cross-entropy loss L BCE and the Dice loss with sigmoid L Dice of M i and ŜM i , and the information entropy loss L ent of Ŝi . Formally, L tt is given by:where σ and γ are hyper-parameters set to 0.5 and 1 in all experiments, respectively. The sum of the first two equations is referred to as click loss L Click ."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,3,Experiments,
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,3.1,Datasets and Evaluation Protocols,"We experiment on two public datasets and one in-house dataset. All three datasets are divided into training, validation, and test sets using a 7:1:2 ratio. "
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,LIDC [1]:,"The LIDC dataset is a publicly available lung CT image database containing 1018 scans, developed by the Lung Image Database Consortium (LIDC). All pulmonary nodules and masses in the dataset have been annotated by multiple raters. To generate the ground truth for each nodule and mass, we combined the segmentation annotations from different raters. Overall, we selected a total of 1625 nodules and masses that were annotated by more than three raters from the LIDC dataset for the experiment."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,LNDb [16]:,"The LNDb dataset published in 2019, comprises 294 CT scans collected between 2016 and 2018. Each CT scan in the dataset has been segmented by at least one radiologist. The nodules included in this dataset are larger than 3 mm. The mean scale of the lesion in LNDb dataset is the shortest among the three datasets. We adopt 1968 nodules and masses from the LNDb dataset."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,In-House Data (ours):,"The in-house data (ours) contains 4055 CT scans and 6864 nodules and masses. Every CT scans are annotated with voxel-level nodule masks by radiologists. We exclude nodules and masses with diameters larger than 64 mm or smaller than 2 mm, as the diameter of the largest mass in the public dataset is no more than 64 mm."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,Evaluation Metrics:,"The performance of the nodule segmentation is evaluated by three metrics: volume-based Dice Similarity Coefficient (DSC), surface-based Normalized Surface Dice (NSD) [13], and recall rate, which calculates the shape similarity between predictions and ground truth. "
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,3.2,Implementation Details,"The ROI of the lesion is a patch cropped around nodules or masses from the original CT scans with shape 64 × 96 × 96. During pre-processing, Hounsfield Units (HU) values in all patches are first clipped to the range of [-1350, 150]. Min-max normalization is then applied, scaling HU values into the range of [0, 1]. All models are trained using AdamW [11] optimizer, cosine annealing learning rate schedule [10]  "
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,3.3,Results,"We adopt nnUNet [7] and TransBTS [25] as the backbone to evaluate the proposed method on pulmonary nodule and mass segmentation. nnUNet is a robust baseline with a complete CNN structure. Its adaptive framework makes it wellsuited for pulmonary nodule segmentation. TransBTS is a 3D medical image segmentation network with a hybrid architecture of transformer and CNN. It incorporates long-range dependencies into the traditional CNN structure to achieve a larger receptive field. The experimental results presented in Table 1, consistently demonstrate that the CNN-based network can achieve better results in multiscale pulmonary nodule and mass segmentation tasks across all three datasets. This is mainly due to the fact that large receptive fields may involve background features that are not conducive to segmentation inference for micro or small nodules. In datasets such as LIDC and In-House, where the number imbalance of multi-scale lesion phenomena is more notable, the multi-input method consistently outperforms the other two baselines. We also implemented comparative experiments with other click methods [20] and [19]. As depicted in Table 1, the experimental results show that using a point and a fixed range of gaussian intensity expansion in the case of large fluctuations in the size of the nodules does not take effect in improving the segmentation performance. The inferior segmentation results of [19] can be attributed to the fact that when it fuses features of different depths and scales, the number of channels in the feature map remains the same, and some of the up-sampling or down-sampling strides are too large, leading to redundancy in shallow features and a lack of deep features. Moreover, the SaTTCA improves the Dice coefficient and surface-based Normalized Surface Dice of segmentation results in both networks. In particular, as demonstrated in Fig. 3, the recall rate of large nodule segmentation is significantly improved.We further analyze the performance of SaTTCA. Firstly, we present the quantitative comparison in Table 2, where we group the nodules and masses in each dataset at 10 mm intervals and calculate the average segmentation performance differences of the nodules in each scale group. The statistical results show that the proposed SaTTCA significantly improves the recall rate of the segmentation on large nodules and masses. As shown in Fig. 3(a), for nodules smaller than 20 mm, both TTCA and SaTTCA effectively increase the recall rate of predicted segmentation. For the medium nodule and mass, our SaTTCA proves to be more effective in improving segmentation performance. Fig. 3(b) shows the mean recall rate for lesions at every scale. The difference between the two scatter diagrams indicates that the proposed SaTTCA effectively alleviates the issue of extremely imbalanced lesion scales, and improves the segmentation performance for large lesions. In addition, for ten epochs of TTA, the inference time of each sample will increase approximately one second comparing with baseline."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,4,Conclusion,"This paper introduces a novel approach called the Scale-aware Test-time Click Adaptation for nodule and mass segmentation, which aims to address the issue of extremely imbalanced lesion scale and poor segmentation performance on largescale nodules and masses. The network parameters are adapted at the instance level according to the scale-aware click during testing without altering the model architecture. This allows the network to achieve high recall for large-scale lesions. Then, a multi-scale input encoder is also proposed to enhance the segmentation performance of multi-scale nodules and masses. Extensive experiments on two public datasets and one in-house dataset demonstrate that though SATTCA increases inference time for each sample by about one second, it outperforms the corresponding baseline and click-based methods with different backbones."
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,Fig. 1 .,
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,Fig. 2 .,
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,,
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,Fig. 3 .,
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,Table 1 .,
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,,Table 2 .,
Chest X-ray Image Classification: A Causal Perspective,1,Introduction,"Chest X-ray (CXR) is a non-invasive diagnostic test frequently utilized by medical practitioners to identify thoracic diseases. In clinical practice, the interpretation of CXR results is typically performed by expert radiologists, which can be time-consuming and subject to individual medical abilities [1]. Consequently, researchers have sought automated and accurate CXR classification technologies based on machine learning, aiming to assist physicians in achieving more precise diagnoses [6,7,17,18,21]. However, there are some inherent problems with CXR images that are difficult to solve, such as high interclass similarity [16], dirty atypical data, complex symbiotic relationships between diseases [21], and long-tailed or imbalanced data distribution [26].Some examples are shown in Fig. 1 from the NIH dataset, we can find previous methods performed not stable when dealing with some tough cases. For example, the label of Fig. 1(d) is cardiomegaly but the predicting results generated by a traditional CNN-based model is infiltration, which fits the statistical pattern of symbiosis between these two pathologies [21]. The black-box nature of deep learning poses challenges in determining whether the learned representations truly capture causality, even when the proposed models demonstrate satisfactory performance. Unfortunately, some recent efforts such as [9,17] already notice part of the above problems but only try to solve it by data pre-processing or designing complicated model, these approaches have not succeeded in enabling deep models to effectively capture genuine causality.To effectively address the aforementioned challenges, we approach the task of CXR image classification from a causal perspective. Our approach involves elucidating the relationships among causal features, confounding features, and the classification outcomes. In essence, our fundamental idea revolves around the concept of ""borrowing from others."" To illustrate this concept, let us consider an example involving letters in an image. Suppose a portion of the image contains marked letters, which can impact the classification of the unmarked portion. We perceive these letters as confounders. By borrowing the mark from the marked portion and adding it to the unmarked part, we effectively eliminate the confounding effect: ""If everyone has it, it's as if no one has it."" The same principle applies to other confounding assumptions we have mentioned.Towards this end, we utilize causal inference to minimize the confounding effect and maximize the causal effect to achieve a stable and decent performance. Specifically, we utilize CNN-based modules to extract the feature from the input CXR images, and then apply Transformer based cross-attention mechanism [20] to produce the estimations of the causal and confounding features from the feature maps. After that, we parameterize the backdoor adjustment by causal theory [14], which combines every causal estimation with different confounding estimations and encourages these combinations to remain a stable classification performance via the idea of ""borrowing from others"". It tends to facilitate the invariance between the causal patterns and the classification results.We evaluate our method on multiple datasets and the experimental results consistently demonstrate the superior performance of our approach. The contributions of our work can be summarized as follows:-We take a casual look at the chest X-ray multi-label classification problem and model the disordered or easily-confused part as the confounder. -We propose a framework based on the guideline of backdoor adjustment and presented a novel strategy for chest X-ray image classification. It allows our properly designed model to exploit real and stable causal features while removing the effects of filtrable confounding patterns. -Extensive experiments on two large-scale public datasets justify the effectiveness of our proposed method. More visualizations with detailed analysis demonstrate the interpretability and rationalization of our proposed method."
Chest X-ray Image Classification: A Causal Perspective,2,Methodology,"In this section, we first define the causal model, then identify the strategies to eliminate confounding effects."
Chest X-ray Image Classification: A Causal Perspective,2.1,A Causal View on CXR Images,"From the above discussion, we construct a Structural Causal Model (SCM) [2] in Fig. 2 to solve the spurious correlation problems in CXR. It contains the causalities about four elements: Input CXR image D, confounding feature C, causal feature X, and prediction Y , where the arrows between elements stand for cause and effect: cause → effect. We have the following explanations:• C ← D → X: X denotes the causal feature which really contributes to the diagnosis, C denotes the confounding feature which may mislead the diagnosis and is usually caused by data bias and other complex situations mentioned above. The arrows denote feature extraction process, C and X usually coexist in the medical data D, these causal effects are built naturally.Fig. 2. SCM for CXR image classification. ""D"" is the input data, ""C"" denotes the confounding features, ""X"" is the causal features and ""Y"" is the prediction results. Confounding factors can block backdoor path between causal variables, so after adjustment, the path is blocked, shown in right part.• C → Y ← X: We denote Y as the classification result which should have been caused only by X but inevitably disturbed by confounding features. The two arrows can be implemented by classifiers.The goal of the model should capture the true causality between X and Y , avoiding the influence of C. However, the conventional correlation P (Y |X) fails to achieve that because of the backdoor path [13] Therefore, we apply the causal intervention to cut off the backdoor path and use P (Y |do(X)) to replace P (Y |X), so the model is able to exploit causal features."
Chest X-ray Image Classification: A Causal Perspective,2.2,Causal Intervention via Backdoor Adjustment,"Here, we propose to use the backdoor adjustment [2] to implement P (Y |do(X)) and eliminate the backdoor path, which is shown on the right of Fig. 2. The backdoor adjustment assumes that we can observe and stratify the confounders, i.e., C = {c 1 , c 2 , ..., c n }, where each c is a stratification of the confounder feature. We can then exploit the powerful do-calculus on causal feature X by estimating P b (Y |X) = P (Y |do(X)), where the subscript b denotes the backdoor adjustment on the SCM. Causal theory [14] provides us with three key conclusions:• P (c) = P b (c): the marginal probability is invariant under the intervention, because C will remain unchanged when cutting the link between D and X. Based on the conclusions, the backdoor adjustment for the SCM in Fig. 2 is:where C is the confounder set, P (c) is the prior probability of c. We approximate the formula by a random sample operation which will be detailed next.  3. Overview of our network. Firstly, we apply CNN with modified attention to extract the image feature, where the n in Convn denotes the kernel size of the convolutional operation, ""+"", ""×"", and ""C"" denote add, multiply, and concatenate operations, respectively. ""GAP"" means global average pooling, ""RS"" is the random sample operation, and ""Cls"" denotes the classifier. The cross-attention module inside the transformer decoder disentangles the causal and confounding feature, then we can apply parameterized backdoor adjustment to achieve causal inference."
Chest X-ray Image Classification: A Causal Perspective,2.3,Training Object,"Till now, we need to provide the implementations of Eq. ( 1) in a parameterized method to fit the deep learning model. However, in the medical scenario, C is complicated and hard to obtain, so we simplify the problem and assume a uniform distribution of confounders. Traditionally, the effective learning of useful knowledge in deep models heavily relies on the design of an appropriate loss function. Then, towards effective backdoor adjustment, we utilize different loss functions to drive our deep model to learn causal and spurious features respectively. Figure 3 illustrates the proposed network. Note that the channel and position attention is implemented by adopting an efficient variant of self-attention [12]. We will break the whole framework down in detail below.Given x ∈ R H0×W0×3 as input, we extract its spatial feature F ∈ R H×W ×v using the backbone, where H 0 × W 0 , H × W represent the height and width of the CXR image and the feature map respectively, and v denotes the hidden dimension of the network. Then, we use zero-initialized Q 0 ∈ R k×v as the queries in the cross-attention module inside the transformer, where k is the number of categories, each decoder layer l updates the queries Q l-1 from its previous layer. Here, we denote Q as the causal features and Q as the confounding features:where the tilde means position encodings, the disentangled features yield two branches, which can be fed separately into a point-wise Multi-layer perceptron (MLP) network and get corresponding classification logits via a sigmoid function."
Chest X-ray Image Classification: A Causal Perspective,,Disentanglement.,"As shown in Fig. 3, we try to impel the model to learn both causal and confounding features via the designed model structure and loss function. Specifically, we adopt a CNN-based model to extract the feature of input images, then capture the causal feature and confounding feature by crossattention mechanism. Thus we can make the prediction via MLP and classifiers:where h ∈ R v×k , Φ(•) represents classifier, and z denotes logits. The causal part aims to estimate the really useful feature, so we apply the supervised classification loss in a cross-entropy format:where d is a sample and D is the training data, y is the corresponding label. The confounding part is undesirable for classification, so we follow the work in CAL [19] and push its prediction equally to all categories, then the confounding loss is defined as:where KL is the KL-Divergence, and y unif orm denotes a predefined uniform distribution.Causal Intervention. The idea of the backdoor adjustment formula in Eq. ( 1) is to stratify the confounder and combine confounding and causal features manually, which is also the implementation of the random sample in Fig. 3. For this propose, we stratify the extracted confounding feature and randomly add it to the other CXR images' features, then feed into the classifier as shown in Eq. ( 6), and get a ""intervened graph"", then we have the following loss guided by causal inference: ẑc = Φ(h x + ĥc ), ( 6)where ẑc is the prediction from a classifier on the ""intervened graph"", ĥc is the stratification feature via Eq. (3), D is the estimated stratification set contains trivial features. The training objective of our framework can be defined as:where α 1 and α 2 are hyper-parameters, which decide how powerful disentanglement and backdoor adjustment are. It pushes the prediction stable because of the shared image features according to our detailed results in the next section."
Chest X-ray Image Classification: A Causal Perspective,3,Experiments,
Chest X-ray Image Classification: A Causal Perspective,3.1,Experimental Setup,"We evaluate the common thoracic diseases classification performance on the NIH ChestX-ray14 [21] and CheXpert [6] data sets. NIH consists of 112,120 frontalview CXR images with 14 diseases and we follow the official data split for a fair comparison, and the latter dataset consists of 224,316 images.In our experiments, we adopt ResNet101 [5] as the backbone. Our experiment is operated by using NVIDIA GeForce RTX 3090 with 24 GB memory. We use the Adam optimizer [8] with a weight decay of 1e-2 and the max learning rate is 1e-3. On the NIH data set, we resize the original images to 512 × 512 as the input and 320 × 320 on CheXpert. We evaluate the classification performance of our method with the area under the ROC curve (AUC) for the whole test set."
Chest X-ray Image Classification: A Causal Perspective,3.2,Results and Analysis,"Table 1 illustrates the overall performance of the NIH Chest-Xray14 dataset of our proposed method compared with other previous state-of-art works, the best performance of each pathology is shown in bold. From the experiments on the NIH data set, we can conclude that we eliminate some spurious relationships within and among CXR images from the classification results. Specifically, we can find that we are not only making progress in most categories but also dealing with some pathologies with high symbiotic dependence such as cardiomegaly and infiltration [21]. The visualization results in Fig. 1 prove that the issues raised were addressed. We conduct experiments on the random addition ratio of ""confounding features"" and found that the ratio of 30% to 40% is appropriate. Besides, the α 1 in Eq. 8 works well around 0.4 to 0.7, and α 2 works well around 0.4 to 0.5.Ablation studies on the NIH data set are shown in Table . 2. Where ""+"" denotes utilizing the module whereas ""-"" denotes removing the module. We demonstrate the efficiency of our method from the ablation study, and we can find that our feature extraction and causal learning module play significant roles, respectively. Besides, during the training process, Fig. 4 shows the fluctuation of the classification effect of three classifiers, where the three lines in the diagram correspond to the three classifiers in Fig. 3. We can find the performance of the confounding classifier goes up at first and then down. At the same time, the other two classifiers' performance increased gradually, which is in line with our expectations. After visualization, we found that confounding factors could be ""beneficial"" for classification in some cases (e.g., certain diseases require patients to wear certain medical devices during X-rays), but this is the wrong shortcut, we expect the model to get causal features. Our causal learning framework successfully discards the adverse effect of confounding features and makes the prediction stable.The results on CheXpert also prove the superiority of our method, we achieve the mean AUC of 0.912 on the five challenging pathologies [6], which surpasses the performance of previous SOTA works such as [6] and [15]. Our method may be general and can be applied to many other medical scenario such as glaucoma [24,25] and segmentation task [22,23]. We will apply contrast learning or self supervised learning in our future works inspired by above-mentioned papers."
Chest X-ray Image Classification: A Causal Perspective,4,Conclusion,"In conclusion, we present a novel causal inference-based chest X-ray image multilabel classification framework from a causal perspective, which comprises a feature learning module and a backdoor adjustment-based causal inference module. We find that previous deep learning based strategies are prone to make the final prediction via some spurious correlation, which plays a confounder role then damages the performance of the model. We evaluate our proposed method on two public data sets, and experimental results indicate that our proposed framework and method are superior to previous state-of-the-art methods."
Chest X-ray Image Classification: A Causal Perspective,,Fig. 1 .,
Chest X-ray Image Classification: A Causal Perspective,,,
Chest X-ray Image Classification: A Causal Perspective,,Fig.,
Chest X-ray Image Classification: A Causal Perspective,,Fig. 4 .,
Chest X-ray Image Classification: A Causal Perspective,,Table 1 .,
Chest X-ray Image Classification: A Causal Perspective,,Table 2 .,
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,1,Introduction,"Colorectal Cancer (CRC) has become a major threat to health worldwide. Since most CRCs originate from colorectal polyps, early screening for polyps is necessary. Given its significance, automatic polyp segmentation models [5,8,16,18]  have been designed to aid in screening. For example, ACSNet [21], HRENet [14], LDNet [20] and CCBANet [11] propose to use convolutional neural networks to extract multi-scale contexts for robust predictions. LODNet [2], PraNet [5], and MSNet [23] aim to improve the model's discrimination of polyp boundaries. SANet [19] eliminates the distribution gap between the training set and the testing set, thus improving the model generalization. Recently, TGANet [15] introduces text embeddings to enhance the model's discrimination. Furthermore, Transfuse [22], PPFormer [1], and Polyp-Pvt [3] introduce the Transformer [4] backbone to extract global contexts, achieving a significant performance gain.All above models are fully supervised and require pixel-level annotations. However, pixel-by-pixel labeling is time-consuming and expensive, which hampers practical clinical usage. Besides, many polyps do not have welldefined boundaries. Pixel-level labeling inevitably introduces subjective noise. To address the above limitations, a generalized polyp segmentation model is urgently needed. In this paper, we achieve this goal by a weakly supervised polyp segmentation model (named WeakPolyp) that only uses coarse bounding box annotations. Figure 1(a) shows the differences between our WeakPolyp and fully supervised models. Compared with fully supervised ones, WeakPolyp requires only a bounding box for each polyp, thus dramatically reducing the labeling cost. More meaningfully, WeakPolyp can take existing large-scale polyp detection datasets to assist the polyp segmentation task. Finally, WeakPolyp does not require the labeling for polyp boundaries, avoiding the subjective noise at source. All these advantages make WeakPolyp more clinically practical.However, bounding box annotations are much coarser than pixel-level ones, which can not describe the shape of polyps. Simply adopting these box annotations as supervision introduces too much background noise, thereby leading to suboptimal models. As a solution, BoxPolyp [18] only supervises the pixels with high certainty. However, it requires a fully supervised model to predict the uncertainty map. Unlike BoxPolyp, our WeakPolyp completely follows the weakly supervised form that requires no additional models or annotations. Surprisingly, just by redesigning the supervision loss without any changes to the model structure, WeakPolyp achieves comparable performance to its fully supervised counterpart. Figure 1(b) visualizes some predicted results by WeakPolyp.WeakPolyp is mainly enabled by two novel components: mask-to-box (M2B) transformation and scale consistency (SC) loss. In practice, M2B is applied to transform the predicted mask into a box-like mask by projection and backprojection. Then, this transformed mask is supervised by the bounding box annotation. This indirect supervision avoids the misleading of box-shape bias of annotations. However, many regions in the predicted mask are lost in the projection and therefore get no supervision. To fully explore these regions, we propose the SC loss to provide a pixel-level self-supervision while requiring no annotations at all. Specifically, the SC loss explicitly reduces the distance between predictions of the same image at different scales. By forcing feature alignment, it inhibits the excessive diversity of predictions, thus improving the model generalization.In summary, our contributions are three-fold: (1) We build the WeakPolyp model completely based on bounding box annotations, which largely reduces the labeling cost and achieves a comparable performance to full supervision. (2) We propose the M2B transformation to mitigate the mismatch between the prediction and the supervision, and design the SC loss to improve the robustness of the model against the variability of the predictions. (3) Our proposed WeakPolyp is a plug-and-play option, which can boost the performances of polyp segmentation models under different backbones."
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,2,Method,"Model Components. Fig. 2 depicts the components of WeakPolyp, including the segmentation phase and the supervision phase. For the segmentation phase, we adopt Res2Net [6] as the backbone. For input image I ∈ R H×W , Res2Net extracts four scales of featuresConsidering the computational cost, only f 2 , f 3 and f 4 are utilized. To fuse them, we first apply a 1 × 1 convolutional layer to unify the channels of f 2 , f 3 , f 4 and then use the bilinear upsampling to unify their resolutions. After being transformed to the same size, f 2 , f 3 , f 4 are added together and fed into one 1 × 1 convolutional layer for final prediction. Instead of the segmentation phase, our contributions primarily lie in the supervision phase, including mask-to-box (M2B) transformation and scale consistency (SC) loss. Notably, both M2B and SC are independent of the specific model structure.Model Pipeline. For each input image I, we first resize it into two different scales: I 1 ∈ R s1×s1 and I 2 ∈ R s2×s2 . Then, I 1 and I 2 are sent to the segmentation model and get two predicted masks P 1 and P 2 , both of which have been resized to the same size. Next, an SC loss is proposed to reduce the distance between P 1 and P 2 , which helps suppress the variation of the prediction. Finally, to fit the bounding box annotations (B), P 1 and P 2 are sent to M2B and converted into box-like masks T 1 and T 2 . With T 1 /T 2 and B, we calculate the binary cross entropy (BCE) loss and Dice loss, without worrying about noise interference. Fig. 2. The framework of our proposed WeakPolyp model, which consists of the segmentation phase and the supervision phase. The segmentation phase predicts the polyp mask for each input firstly, and the supervision phase uses the coarse box annotation to guide previous predicted mask. Note that our contributions mainly lie in the supervision phase, where the proposed M2B transformation converts the predicted mask into a box mask to accommodate the bounding box annotation. Besides, another proposed SC loss is introduced to provide dense supervision from multi-scales, which improves the consistency of predictions."
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,2.1,Mask-to-Box (M2B) Transformation,"One naive method to achieve the weakly supervised polyp segmentation is to use the bounding box annotation B to supervise the predicted mask P 1 /P 2 . Unfortunately, models trained in this way show poor generalization. Because there is a strong box-shape bias in B. Training with this bias, the model is forced to predict the box-shape mask, unable to maintain the polyp's contours. To solve this, we innovatively use B to supervise the bounding box mask (i.e.,T 1 /T 2 ) of P 1 /P 2 , rather than P 1 /P 2 itself. This indirect supervision separates P 1 /P 2 from B so that P 1 /P 2 is not affected by the shape bias of B while obtaining the position and extent of polyps. But how to implement the transformation from P 1 /P 2 to T 1 /T 2 ? We design the M2B module, which consists of two steps: projection and back-projection, as shown in Fig. 2.Projection. As shown in Eq. 1, given a predicted mask P ∈ [0, 1] H×W , we project it horizontally and vertically into two vectors P w ∈ [0, 1] 1×W and P h ∈ [0, 1] H×1 . In this projection, instead of using mean pooling, we use max pooling to pick the maximum value for each row/column in P . Because max pooling can completely remove the shape information of the polyp. After projection, only the position and scope of the polyp are stored in P w and P h . Back-projection. Based on P w and P h , we construct the bounding box mask of the polyp by back-projection. As shown in Eq. 2, P w and P h are first repeated into P w and P h with the same size as P . Then, we element-wisely take the minimum of P w and P h to achieve the bounding box mask T . As shown in Fig. 2, T no longer contains the contours of the polyp.Supervision. By M2B, P 1 and P 2 are transformed into T 1 and T 2 , respectively. Because both T 1 /T 2 and B are box-like masks, we directly calculate the supervision loss between them without worrying about the misguidance of box-shape bias. Specifically, we follow [5,19] to adopt BCE loss L BCE and Dice loss L Dice for model supervision, as shown in Eq. 3.Priority. By simple transformation, M2B turns the noisy supervision into a noise-free one, so that the predicted mask is able to preserve the contours of the polyp. Notably, M2B is differentiable, which can be easily implemented with PyTorch and plugged into the model to participate in gradient backpropagation."
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,2.2,Scale Consistency (SC) Loss,"In M2B, most pixels in P are ignored in the projection, thus only a few pixels with high response values are involved in the supervision loss. This sparse supervision may lead to non-unique predictions. As shown in Fig. 3, after M2B projection, five predicted masks with different response values can be transformed into the same bounding box mask. Therefore, we consider introducing the SC loss to achieve dense supervision without annotations, which reduces the degree of freedom of predictions.Method. As shown in Fig. 2, due to the non-uniqueness of the prediction and the scale difference between I 1 and I 2 , P 1 and P 2 differ in response values. But come from the same image I 1 . They should be exactly the same. Given this, as shown in Eq. 4, we build the dense supervision L SC by explicitly reducing the distance between P 1 and P 2 , where (i, j) is the pixel coordinates. Note that only pixels inside bounding box are involved in L SC to emphasize more on polyp regions. Despite its simplicity, L SC brings pixel-level constraints to compensate for the sparsity of L Sum , thus reducing the variety of predictions."
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,2.3,Total Loss,"As shown in Eq. 5, combining L Sum and L SC together, we get WeakPolyp model. Note that WeakPolyp simply replaces the supervision loss without making any changes to the model structure. Therefore, it is general and can be ported to other models. Besides, L Sum and L SC are only used during training. In inference, they will be removed, thus having no effect on the speed of the model."
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,3,Experiments,"Datasets. Two large polyp datasets are adopted to evaluate the model performance, including SUN-SEG [9] and POLYP-SEG. SUN-SEG originates   Compared with Fully Supervised Methods. Table . 3 shows our WeakPolyp is even superior to many previous fully supervised methods: PraNet [5], SANet [19], 2/3D [12] and PNS+ [9], which shows the excellent application prospect of weakly supervised learning in the polyp field."
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,4,Conclusion,"Limited by expensive labeling cost, pixel-level annotations are not readily available, which hinders the development of the polyp segmentation field. In this paper, we propose the WeakPolyp model completely based on bounding box annotations. WeakPolyp requires no pixel-level annotations, thus avoiding the interference of subjective noise labels. More importantly, WeakPolyp even achieves a comparable performance to the fully supervised models, showing the great potential of weakly supervised learning in the polyp segmentation field.In future, we will introduce temporal information into weakly supervised polyp segmentation to further reduce the model's dependence on labeling."
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,,Fig. 1 .,
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,,Fig. 3 .,
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,,Fig. 4 .,
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,,,
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,,Table 1 .,
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,,Table 2 .,"Interestingly, WeakPolyp even surpasses the fully supervised model on SUN-SEG, which indicates that there is a lot of noise in the pixel-level annota-"
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,,Table 3 .,
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,1,Introduction,"Deep learning has achieved remarkable success in various computer vision tasks, such as image generation, translation, and semantic segmentation [3,7,8,16,23]. However, a limitation of deep learning models is their restricted applicability to the specific domains they were trained on. Consequently, these models often struggle to generalize to new and unseen domains. This lack of generalization capability can result in decreased performance and reduced applicability of models, particularly in fields such as medical imaging where data distribution can vary greatly across different domains and institutions [14,18].Methods such as domain adaptation (DA) or domain generalization (DG) have been explored to address the aforementioned problems. These methods aim to leverage learning from domains where information such as annotations exists and apply it to domains where such information is absent. Unsupervised domain adaptation (UDA) aims to solve this problem by simultaneously utilizing learning from a source domain with annotations and a target domain without supervised knowledge. UDA methods are designed to mitigate the issue of domain shift between the source and target domains. Pixel-level approaches, as proposed in [2,4,9,17], focus on adapting the source and target domains at the image level. These UDA methods, based on image-to-image translation, effectively augment the target domain when there is limited domain data. Manipulating pixel spaces is desirable as it generates images that can be utilized beyond specific tasks and easily applied to other applications.In DG, unlike UDA, the model aims to directly generalize to a target domain without joint training or retraining. DG has been extensively studied recently, resulting in various proposed approaches to achieve generalization across domains. One common approach [6] is adversarial training, where a model is trained to be robust to domain shift by incorporating a domain discriminator into the training process. Another popular approach [1,13] involves using domain-invariant features, training the model to learn features not specific to any particular domain but are generalizable across all domains.DG in the medical field includes variations in imaging devices, protocols, clinical centers, and patient populations. Medical generalization is becoming increasingly important as the use of medical imaging data is growing rapidly. Compared to general fields, DG in medical fields is still in its early stages and faces many challenges. One major challenge is the limited amount of annotated data available. Additionally, medical imaging data vary significantly across domains, making it difficult to develop models that generalize well to unseen domains. Recently, researchers have made significant progress in developing domain generalization methods for medical image segmentation. [10] learns a representative feature space through variational encoding with a linear dependency regularization term, capturing the shareable information among medical data collected from different domains. Based on data augmentation, [5,11,21] aims to solve domain shift problems with different distributions. [5], for instance, proposes utilizing domain-discriminative information and content-aware controller to establish the relationship between multiple source domains and an unseen domain. Based on alignment learning, [18] introduces enhancing the discriminative power of semantic features by augmenting them with domain-specific knowledge extracted from multiple source domains. Nevertheless, there exists an opportunity for further enhancement in effectively identifying invariant fea-tures across diverse domains. While current methods have demonstrated notable progress, there is still scope for further advancements to enhance the practical applicability of domain generalization in the medical domain.In recent years, Transformer has gained significant attention in computer vision. Unlike traditional convolutional neural networks (CNNs), which operate locally and hierarchically, transformers utilize a self-attention mechanism to weigh the importance of each element in the input sequence based on its relationship with other elements. Swin Transformer [12] has gained significant attention in computer vision due to its ability to capture global information effectively in an input image or sequence. The capability has been utilized in disentanglement-based methods to extract variant features (e.g., styles) for synthesizing images. [20] has successfully introduced Swin-transformers for disentanglement into StyleGAN modules [8], leading to the generation of high-quality and high-resolution images. The integration of Transformer models into the medical domain holds great promise for addressing the challenges of boosting the performance of domain generalization.In this paper, we present a novel approach for domain generalization in medical image segmentation that addresses the limitations of existing methods. To be specific, our method is based on the disentanglement training strategy to learn invariant features across different domains. We first propose a combination of recent vision transformer architectures and style-based generators. Our proposed method employs a hierarchical combination strategy to learn global and local information simultaneously. Furthermore, we introduce domain-invariant representations by swapping domain-specific features, facilitating the disentanglement of content (e.g., objects) and styles. By incorporating a patch-wise discriminator, our method effectively separates domain-related features from entangled ones, thereby improving the overall performance and interpretability of the model. Our model effectively disentangles both domain-invariant features and domainspecific features separately. Our proposed method is evaluated on a medical image segmentation task, namely retinal fundus image segmentation with four different clinical centers. It achieves superior performance compared to state-ofthe-art methods, demonstrating its effectiveness."
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,2,Methods,
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,2.1,Framework,"Efficiently extracting information from input images is crucial for successful domain generalization, and the process of reconstructing images using this information is also important, as it allows meaningful information to be extracted and, in combination with the learning methods presented later, allows learning to discriminate between domain-relevant and domain-irrelevant information. To this end, we designed an encoder using a transformer structure, which is nowadays widely used in computer vision, and combined it with a StyleGAN-based image decoder. The overall framework of our approach comprises three primary architectures: an encoder denoted as E, a segmentor denoted as S, and an image generator denoted as G. Additionally, the framework includes two discriminators: D and P D, as shown in Fig. 1.The encoder E is constructed using hierarchical transformer architecture, which offers increased efficiency and flexibility through its consideration of multi-scale features, as documented in [12,19,20]. Hierarchical transformers enable efficient extraction of diverse data representations and have computational advantages over conventional vision transformers. In our proposed method, the transformer encoder consists of three key mechanisms: Efficient SA, Mix-FFN, and Patch Merging. Each of these mechanisms plays a significant role in capturing and processing information within the transformer blocks. The Efficient SA mechanism involves computing the query (Q), key (K), and value (V ) heads using the self-attention mechanism. To reduce the computational complexity of the attention process, we choose a reduction ratio (R), which allows us to decrease the computational cost. K is reshaped using the operation Reshape( N R , C • R)(K), where N represents the number of patches and C denotes the channel dimension. In the Mix-FFN mechanism, we incorporate a convolutional layer in the feed-forward network (FFN) to consider the leakage of location information. The process is expressed as:where F in and F out represent the input and output features, respectively. This formulation enables the model to capture local continuity while preserving important information within the feature representation. To ensure the preservation of local continuity across overlapping feature patches, we employ the Patch Merging process. This process combines feature patches by considering patch size (K), stride (S), and padding size (P ). For instance, we design the parameters as K = 7, S = 4, P = 3, which govern the characteristics of the patch merging operation. E takes images I D from multiple domains D : {D 1 , D 2 , ...D N }, and outputs two separated features of F D C which is a domain-invariant feature and F D S , which are domain-related features as (F D C , F D S ) = E(I D ). By disentangling these two, we aim to effectively distinguish what to focus when conducting target task such as segmentation on an unseen domain.For an image generator G, we take the StyleGAN2-based decoder [8] which is capable of generating high-quality images. Combination of Style-based generator with an encoder for conditional image generation or image translation is also showing a good performance on various works [15]. High-quality synthesized images with mixed domain information lead to improved disentanglement."
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,2.2,Loss Function,"The generator aims to synthesize realistic images such as Î(i,j) = G(F Di C , F Dj S ) that matches the distribution of the input style images, while maintaining the consistency of a content features. For this, reconstruction loss term is introduced first to maintain self-consistency:Also, adversarial loss used to train G to synthesize a realistic images.Finally, segmentor S tries to conduct a main task which should be work well on the unseen target domain. To enable this, segmentation decoder only focuses on the disentangled content feature rather than the style features, as in Fig. 1. To utilize high-resolution information, skip connections from an encoder is fed to a segmentation decoder. Segmentation decoder computes losses between segmentation predictions ŷ = S(F D C ) and an segmentation annotation y. We use dice loss functions for segmentation task, as:To better separate domain-invariant contents from domain-specific features, a patch-wise adversarial discriminator P D is included in the training, in a similar manner as introduce in [15]. With an adversarial loss between the patches within an image and between translated images, the encoder is trained to better disentangle styles of a domain as below. The effectiveness of the loss is compared on the experiment session.L padv = -log(P D( Î(i,j) )) for i = j.(Under an assumption that well-trained disentangled representation learning satisfies the identity on content features, we apply an identity loss on both contents and segmentation outputs for a translated images. In addition to a regularization effect, this leads to increased performance and a stability in the training.where) = E( Î(i,j) ) , and ŷ * = S(F Di * C).Therefore, overall loss function becomes as below.where λ 0 , λ 1 , λ 2 , λ 3 , and λ 4 are the weights of L seg , L rec , L identity , L adv , and λ padv , respectively."
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,3,Experiments and Results,"To evaluate the effectiveness of our proposed approach in addressing domain generalization for medical fields, we conduct experiments on a public dataset. The method is trained on three source domains and evaluates its performance on the remaining target domain. We compare our results to those of existing methods, including Fed-DG [11], DoFE [18], RAM-DSIR [22], and DCAC [5]."
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,3.1,Setup,"Dataset. We evaluate our proposed method on a public dataset, called Fundus [18]. The Fundus dataset consists of retinal fundus images for optic cup (OC) and disc (OD) segmentation from four medical centers. Each of four domains has 50/51, 99/60, 320/80 and 320/80 samples for each training and test. For all results from the proposed method, the images are randomly cropped for these data, then resized the cropped region to 256 × 256. The images are normalized to a range of -1 to 1 using the min-max normalization and shift process.Metrics. We adopt the Dice coefficient (Dice), a widely used metric, to assess the segmentation results. Dice is calculated for the entire object region. A higher Dice coefficient indicates better segmentation performance."
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,3.2,Implementation Details,"Our proposed network is implemented based on 2D. The encoder network is applied with four and three downsampling layers and with channel numbers of 32, 64, 160, and 256. Adam optimizer with a learning rate of 0.0002 and momentum of 0.9 and 0.99 is used. We set λ 0 , λ 1 , λ 2 , λ 3 , and λ 4 as 0.1, 1, 0.5, 0.5, and 0.1, respectively. We train the proposed method for 100 epochs on the Fundus dataset with a batch size of 6. Each batch consists of 2 slices from each of the three domains. We use data augmentation to expand the training samples, including random gamma correction, random rotation, and flip. The training is implemented on one NVIDIA RTX A6000 GPU. We evaluate the performance of our proposed method by comparing it to four existing methods, as mentioned earlier. Table 1 shows quantitative results of Dice coefficient. Except for our method, DCAC has shown effectiveness in generalization with an average Dice score of 82.74 and 93.24 for OC and OD, respectively. Our proposed approach demonstrates impressive and effective results across all evaluation metrics. Our proposed method also performs effectively, with an average Dice score of 84.03 and 92.94 for OC and OD, respectively. It demonstrates that our method performs effectively compared to the previous methods. We also perform qualitative comparisons with the other methods, as shown in Fig. 2. Specifically, for an image in Domain 1, as depicted in the first row, the boundary of OC is difficult to distinguish. The proposed method accurately identifies the exact regions of both OC and OD regions. Furthermore, we analyze ablation studies to evaluate the effectiveness of each term in our loss function, including L seg , L rec , L identity , L adv , and L padv . The impact of adding each term sequentially on performance is analyzed, and the results are presented in Table 1. Our findings indicate that each term in the loss function plays a crucial role in generalizing domain features. As each loss term is added, we observed a gradual increase in the quantitative results for all unseen domains. For instance, when the unseen domain is Domain A, the Dice score improved from 75.60 to 76.97 to 85.70 upon adding each loss term."
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,3.3,Results,"To evaluate the effectiveness of our model in extracting variant and invariant features, we conducted t-SNE visualization on the style features of images synthesized using two different methods: the widely-used mixup method as in [22] and our proposed method. As illustrated in Fig. 3, the images generated by our model exhibit enhanced distinguishability compared to the mixup-based mixing visualization. This observation suggests that the distribution of mixed images using our method closely aligns with the original domain, which is better than the mixup-based method. It indicates that our model has successfully learned to extract both domain-variant and domain-invariant features through disentanglement learning, thereby contributing to improved generalizability."
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,4,Conclusion,"Our work addresses the challenge of domain generalization in medical image segmentation by proposing a novel framework for retinal fundus image segmentation. The framework leverages disentanglement learning with adversarial and regularized training to extract invariant features, resulting in significant improvements over existing approaches. Our approach demonstrates the effectiveness of leveraging domain knowledge from multiple sources to enhance the generalization ability of deep neural networks, offering a promising direction for future research in this field."
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,,H,
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,,Fig. 1 .,
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,,Fig. 2 .,
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,,Fig. 3 .,
DiMix: Disentangle-and-Mix Based Domain Generalizable Medical Image Segmentation,,Table 1 .,
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,2.2,Post-processing,"The inspiratory CT scan was registered to the expiratory CT scan by applying 3-D deformable image registration using the image registration tool Elastix (version 5.0.1) [8,15]. A representative registration result is presented in Fig. 1. The subsequent post-processing steps are outlined in Fig. 2. Lung masks were generated using the CT analysis software YACTA [18]. The generated lung masks were applied to the CT images to permit an undeflected focus on the lungs. All CT images were normalized between zero and one. The mean threshold from radiographic assessments, performed separately by two trained radiologists, was used as ground truth. To generate the ground truth segmentation, the radiologist loaded the inspiratory and corresponding expiratory CT scan in our inhouse software. After loading, the scans were displayed next to each other where the radiologist could go through each of them individually. The segmentation was not drawn manually by the radiologist. Instead, we used a patient-specific threshold T. An AT map was generated by classifying all expiratory CT voxels < T as AT. Using an integrated slider functionality, the radiologist was asked to choose T for each patient such that the AT map best describes the trapped air. Since a manual AT assessment is very time-consuming, the slider-based approach provides a good trade-off between time consumption and accuracy. With this technique, we are able to guarantee a high ground truth quality since two trained radiologists selected a personalized threshold for each patient and no generic method was used. 2-D patches were created from the 2-D axial slices of the expiratory, the corresponding registered inspiratory CT scan and the ground truth segmentation. A sliding window with a patch size p of 32 and stride s was used as demonstrated in Fig. 2. The generated patches were then utilized as oneor two-channel input to a densely connected convolutional neural network which was trained to segment AT."
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,2.3,DenseNet Architecture and Training,"A sketch of the DenseNet architecture is presented in Fig. 3. It has a common u-net structure [14] and is adopted from Ram et al. [13]. Each dense block contains four dense block layers where each dense block layer consists of a batch normalization (BN), a rectified linear unit (ReLU) activation function and a 3 × 3 convolution. For downsampling, BN, a ReLU activation, a 1 × 1 convolution and a 2 × 2 max pooling operation with a stride of two are used. Upsampling is implemented using a 3 × 3 transposed convolution with a stride of two.In the last layer, a 1 × 1 convolution and a softmax operation are performed to obtain the output probability map. The network was trained to minimize the Dice loss [10,17]. Weights were optimized using stochastic gradient decent (Momentum) [12] with a momentum term of β = 0.9 and a learning rate of α = 0.001. The implementation is done in PyTorch [11] (version 1.12.1) using Python version 3.10 (Python Software Foundation, Python Language Reference, http://www.python.org). No augmentation has been used. In contrast to Ram et al. [13], the registered inspiratory CT scan was added as second input channel to the network and the model was trained on 2-D patches instead of slices. We argue that, as for the radiologist, using the inspiration CT as second input can add useful information in the form of inspiration expiration differences. Patches were selected over slices to increase the number of training samples. The subsequent experiments were executed on a high performance computing cluster. On the cluster, the method ran on a NVIDIA Tesla V100 HBM2 RAM with 32 GB of memory. "
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,2.4,Model Evaluation,"The DenseNet AT percentages used to compute the correlations presented in Table 1 are computed as the normalized sum of the DenseNet output probabilities using the best model in terms of DICE coefficient evaluated on the validation set. Since the network outputs a probability map, for computing the DICE coefficient over the patches, the output probabilities were converted to binary maps classifying all probabilities ≥ 0.5 as 1, and 0 otherwise. In the same way, the so-called quantitative AT (QAT) values which can be found in the upper right of the overlayed ground truth and DenseNet output images of Fig. 4, were computed as the percentage of AT over the entire lung using the binary segmentation maps."
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,3,Results,"For each model of each parameter combination obtained from cross-validation, the DICE coefficient was computed over the patches from the test set. Resulting mean DICE coefficients and standard deviations over the 5-folds for the different parameter combinations are presented in Table 1. The highest DICE coefficient was achieved when training the network on two channels using a stride of 16.Here, the best model achieved a score of 0.82. It was trained on both, LD and ULD. The scores did not differ noticeably (third decimal) when training on LD only (mean DICE coefficient 0.806 ± 0.213) compared to including both scan protocols, LD and ULD (0.809 ± 0.216).Analyzing the correlations of the percentage of AT in the lungs detected by the best DenseNet model of the five folds between LD and ULD, strong correlations and small ULD-LD differences become apparent for all tested parameter Table 1. Grid search results for the different parameter combinations evaluated on the test set. Mean DICE coefficent and standard deviation (evaluated on patches) computed over the five models. Mean ULD-LD difference of the percentage of AT obtained from the best DenseNet model of the five folds, presented together with the correlations (Pearson's R) between LD and ULD and with the LCI for LD and ULD (p < 0.001 for all R).  combinations (Table 1). The table shows that the mean ULD-LD difference is lower when training with one compared to two channels. Comparing the correlation of the AT percentage in the lungs detected by the best DenseNet models with the LCI for LD and ULD (Table 1), the strongest correlations were achieved when training LD and ULD together using a 2-channel input. Applying a stride of 32 results in a strong correlation for LD (R = 0.77, p < 0.001) and ULD (R = 0.78, p < 0.001) which can be also achieved using overlapping patches with a stride of 16 (LD (R = 0.76, p < 0.001) and ULD (R = 0.78, p < 0.001)). A strong correlation between LD and ULD (R = 0.96, p < 0.001) and small ULD-LD differences (mean difference -1.04 ± 3.25%) can be observed. Figure 4 shows the inputs, the corresponding ground truth segmentation, and the resulting DenseNet output probability map for a representative LD scan (left) and its corresponding ULD scan (right). The model used for evaluation was trained on LD only using a two-channel input and a stride of 32. Comparing the DenseNet output with the ground truth, a good AT segmentation can be observed for LD and ULD. The segmentation results demonstrate that although only trained on LD, a good AT quantification can also be obtained for ULD. The deep learning method detected less AT than the ground truth. This becomes apparent by looking at the QAT values, displayed in white, in the upper right of the overlayed images. However, the difference between the DenseNet and the ground truth segmentations is small for LD (QAT value difference: -2.68%) as well as ULD (QAT value difference: -1.2%). As presented in Table 2, the conventional -856 HU threshold has only a small overlap with the ground truth radiographic assessment resulting in a low slicebased DICE coefficient of 0.379 ± 0.230, suggesting that this threshold is less suitable for children. The subject-specific threshold method from Goris et al. [5], on the other hand, achieves a noticeably higher DICE coefficient of 0.645±0.171. The best DenseNet model achieves the highest DICE coefficient of the compared methods but less strong correlations with the LCI."
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,,Stride # channels LD only DICE (patches) DenseNet AT ULD-LD,
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,4,Discussion,"In this study, we trained a densely connected convolutional neural network to segment AT using 2-D patches of the expiratory and corresponding registered inspiratory CT scan slices. We wanted to evaluate the best settings and the effect of a noticeable dose reduction on AT quantification.Using a smaller stride and respectively more patches only resulted in a slightly higher patch-based DICE coefficient evaluated on the test set (Table 1). No noticeable increase in DICE coefficient could be observed for the models trained on both scan protocols or two input channels, compared to the reference. Only small differences were observed regarding the correlations of the DenseNet AT percentage between LD and ULD, and the DenseNet AT with the LCI for LD and ULD. Since furthermore only small ULD-LD differences were observed, the study indicates that the ULD scan protocol allows a comparable air trapping quantification, in comparison to the standard. Good correlations and small ULD-LD differences are also obtained with the models trained on LD only which proposes that training the model on both, LDCT and ULDCT is not necessarily needed. Adding the registered inspiratory scan as a second input channel to the network shows slight improvements in AT detection compared to a 1-channel approach. Future work will investigate to what extent an improved image registration can further improve the performance of the 2-channel approach. The comparison with other AT quantification methods, more precisely, the largest agreement with the radiographic ground truth segmentation in combination with a less strong correlation with the LCI might eventually indicate that the deep learning method is more sensible and detects structural impairment when function test results are still in a normal range (Table 2). Furthermore, it highlights the importance to distinguish AT severities when comparing structure-function relationships.It is important to note the limitations of this study. First of all, it should be mentioned that generating ground truth is a difficult task even for experienced radiologists, which is not always clearly solvable. In addition, the results presented are limited to the available number of patients. Children were scanned at inspiration and expiration, with two different scan protocols, without leaving the CT table. This results in four scans for each patient and explains the limited availability of patients to be included in the study. The particularity of the dataset clarifies why the model could not easily be tested on an independent test dataset since there is none available obtained in a comparable manner."
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,5,Conclusion,"We were able to show that similar QAT indices can be calculated on ULD CT images despite an 82% reduced dose. QAT values were comparable for ULD and LD across all parameter combinations. The relationship to the LCI was retained. AT is not only an early sign of incipient pulmonary dysfunction in patients with CF, but also in other diseases such as COPD or asthma. We want to investigate how our DenseNet performs on other data sets of patients with CF, COPD or asthma and, if necessary, expand the amount of training data."
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,,Keywords:,
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,,Fig. 1 .,
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,,Fig. 2 .,
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,,Fig. 3 .,
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,,,
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,,Fig. 4 .,
Deep Learning-Based Air Trapping Quantification Using Paired Inspiratory-Expiratory Ultra-low Dose CT,,Table 2 .,
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,1,Introduction,"The success of deep learning for medical data analysis has demonstrated its potential to become a core component of future diagnosis and treatment methodologies. However, in spite of the efforts devoted to improve data efficiency [14], the most effective models still rely on large datasets to achieve high accuracy and generalizability. An effective strategy for obtaining large and diverse datasets is to leverage collaborative efforts based on data sharing principles; however, current privacy regulations often hinder this possibility. As a consequence, small private datasets are still used for training models that tend to overfit, introduce biases and generalize badly on other data sources addressing the same task [24]. As a mitigation measure, generative adversarial networks (GANs) have been proposed to synthesize highly-realistic images, extending existing datasets to include more (and more diverse) examples [17], but they pose privacy concerns as real samples may be encoded in the latent space. K-same techniques [9,15] attempt to reduce this risk by following the k-anonymity principle [21] and replacing real samples with synthetic aggregations of groups of k samples. As a downside, these methods reduce the dataset size by a factor of k, which greatly limits their applicability.To address this issue, we propose an approach, complementing k-same techniques, for generating an extended variant of a dataset by sampling a privacypreserving walk in the GAN latent space. Our method directly optimizes latent points, through the use of an auxiliary identity classifier, which informs on the similarity between training samples and synthetic images corresponding to candidate latent points. This optimized navigation meets three key properties of data synthesis for medical applications: 1) equidistance, encouraging the generation of diverse realistic samples suitable for model training; 2) privacy preservation, limiting the possibility of recovering original samples, and, 3) class-consistency, ensuring that synthesized samples contain meaningful clinical information. To demonstrate the generalization capabilities of our approach, we experimentally evaluate its performance on two medical image tasks, namely, tuberculosis classification using the Shenzhen Hospital X-ray dataseet [5,7,8] and diabetic retinopathy classification on the APTOS dataset [13]. On both tasks, our approach yields classification performance comparable to training with real samples and significantly better than existing k-same techniques such as k-SALSA [9], while keeping the same robustness to membership inference attacks.Contributions: 1) We present a latent space navigation approach that provides a large amount of diverse and meaningful images for model training; 2) We devise an optimization strategy of latent walks that enforces privacy; 3) We carry out several experiments on two medical tasks, demonstrating the effectiveness of our generative approach on model's training and its guarantees to privacy preservation."
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,2,Related Work,"Conventional methods to protect identity in private images have involved modifying pixels through techniques like masking, blurring, and pixelation [3,19]. However, these methods have been found to be insufficient for providing adequate privacy protection [1]. As an alternative, GANs have been increasingly explored to synthesize high-quality images that preserve information from the original distribution, while disentangling and removing privacy-sensitive components [22,23]. However, these methods have been mainly devised for face images and cannot be directly applicable to medical images, since there is no clear distinction between identity and non-identity features [9].Recent approaches, based on the k-same framework [15], employ GANs to synthesize clinically-valid medical images principle by aggregating groups of real samples into synthetic privacy-preserving examples [9,18]. In particular, k-SALSA [9] uses GANs for generating retinal fundus images by proposing a local style alignment strategy to retain visual patterns of the original data. The main downside of these methods is that, in the strive to ensure privacy preservation following the k-anonymity [21] principle, they significantly reduce the size of the original dataset.Our latent navigation strategy complements these approaches by synthesizing large and diverse samples, suitable for downstream tasks. In general, latent space navigation in GANs manipulates the latent vectors to create new images with specific characteristics. While many works have explored this concept to control semantic attributes of generated samples [4,12], to the best of our knowledge, no method has tackled the problem from a privacy-preservation standpoint, especially on a critical domain such as medical image analysis."
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,3,Method,"The proposed Privacy-preserving LAtent Navigation (PLAN) strategy envisages three separate stages: 1) GAN training using real samples; 2) latent privacy-preserving trajectory optimization in the GAN latent space; 3) privacypreserving dataset synthesis for downstream applications. Figure 1 illustrates the overall framework and provides a conceptual interpretation of the optimization objectives.Formally, given a GAN generator G : W → X , we aim to navigate its latent space W to generate samples in image space X in a privacy-preserving way, i.e., avoiding latent regions where real images might be embedded. The expected result is a synthetic dataset that is safe to share, while still including consistent clinical features to be used by downstream tasks (e.g., classification).Our objective is to find a set of latent points W ⊂ W from which it is safe to synthesize samples that are significantly different from training points: given the training set X ⊂ X and a metric d on X , we want to find W such that min x∈ X d (G ( w) , x) > δ, ∀ w ∈ W, for a sufficiently large δ. Manually searching for W, however, may be unfeasible: generating a large W is computationally expensive, as it requires at least | W| forward passes through G, and each synthesized image should be compared to all training images; moreover, randomly sampled latent points might not satisfy the above condition.To account for latent structure, one could explicitly sample away from latent vectors corresponding to real data. Let Ŵi ⊂ W be the set of latent vectors that produce near-duplicates of a training sample x i ∈ X , such that G( ŵi ) ≈ x i , ∀ ŵi ∈ Ŵi . We can thus define Ŵ = N i=1 Ŵi as the set of latent points corresponding to all N samples of the training set: knowledge of Ŵ can be used to move the above constraint from X to W, by finding W such that min ŵ∈ Ŵ d ( w, ŵ) > δ, ∀ w ∈ W. In practice, although Ŵi can be approximated through latent space projection [2,12] from multiple initialization points, its cardinality | Ŵi | cannot be determined a priori as it is potentially unbounded.From these limitations, we pose the search of seeking privacy-preserving latent points as a trajectory optimization problem, constrained by a set of objectives that mitigate privacy risks and enforce sample variability and class consistency. Given two arbitrary latent points (e.g., provided by a k-same aggregation method), w a , w b ∈ W, we aim at finding a latent trajectory WT = [w a = w1 , w2 , . . . , wT -1 , w b = wT ] that traverses the latent space from w a to w b in T steps, such that none of its points can be mapped to any training sample. We design our navigation strategy to satisfy three requirements, which are then translated into optimization objectives:1. Equidistance. The distance between consecutive points in the latent trajectory should be approximately constant, to ensure sample diversity and mitigate mode collapse. We define the equidistance loss, L dist , as follows:where • 2 is the L 2 norm. Note that without any additional constraint, L dist converges to the trivial solution of linear interpolation, which gives no guarantee that the path will not contain points belonging to Ŵ. 2. Privacy preservation. To navigate away from latent regions corresponding to real samples, we employ an auxiliary network φ id , trained on X to perform identity classification. We then set the privacy preservation constraint by imposing that a sampled trajectory must maximize the uncertainty of φ id , thus avoiding samples that could be recognizable from the training set.Assuming φ id to be a neural network with as many outputs as the number of identities in the original dataset, this constraint can be mapped to a privacypreserving loss, L id , defined as the Kullback-Leibler divergence between the softmax probabilities of φ id and the uniform distribution U:where n id is the number of identities. This loss converges towards points with enhanced privacy, on which a trained classifier is maximally uncertain. 3. Class consistency. The latent navigation strategy, besides being privacypreserving, needs to retain discriminative features to support training of downstream tasks on the synthetic dataset. In the case of a downstream classification task, given w a and w b belonging to the same class, all points along a trajectory between w a and w b should exhibit the visual features of that specific class. Moreover, optimizing the constraints in Eq. 1 and Eq. 2 does not guarantee good visual quality, leading to privacy-preserving but useless synthetic samples. Thus, we add a third objective that enforces class-consistency on trajectory points. We employ an additional auxiliary classification network φ class , trained to perform classification on the original dataset, to ensure that sampled latent points share the same visual properties (i.e., the same class) of w a and w b . The corresponding loss L class is as follows:where CE is the cross-entropy between the predicted label for each sample and the target class label y.Overall, the total loss for privacy-preserving latent navigation is obtained as:where λ 1 and λ 2 weigh the three contributions.In a practical application, we employ PLAN in conjunction with a privacypreserving method that produces synthetic samples (e.g., a k-same approach). We then navigate the latent space between random pairs of such samples, and increase the size of the dataset while retaining privacy preservation. The resulting extended set is then used to train a downstream classifier φ down on synthetic samples only. Overall, from an input set of N samples, we apply PLAN to N/2 random pairs, thus sampling T N/2 new points."
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,4,Experimental Results,"We demonstrate the effectiveness and privacy-preserving properties of our PLAN approach on two classification tasks, namely, tuberculosis classification and diabetic retinopathy (DR) classification."
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,4.1,Training and Evaluation Procedure,"Data Preparation. For tuberculosis classification, we employ the Shenzhen Hospital X-ray set1  [5,7,8] that includes 662 frontal chest X-ray images (326 negatives and 336 positives). For diabetic retinopathy classification, we use the APTOS fundus image dataset [13] of retina images labeled by ophthalmologists with five grades of severity. We downsample it by randomly selecting 950 images, equally distributed among classes, to simulate a typical scenario with low data availability (as in medical applications), where GAN-based synthetic sampling, as a form of augmentation, is more needed. All images are resized to 256 × 256 and split into train, validation and test set with 70%, 10%, 20% proportions.Baseline Methods. We evaluate our approach from a privacy-preserving perspective and by its capability to support downstream classification tasks. For the former, given the lack of existing methods for privacy-preserving GAN latent navigation, we compare PLAN to standard linear interpolation. After assessing privacy-preserving performance, we measure the impact of our PLAN sampling strategy when combined to k-SALSA [9] and the latent cluster interpolation approach from [18] (LCI in the following) on the two considered tasks. Implementation Details. We employ StyleGAN2-ADA [11] as GAN model for all baselines, trained in a label-conditioned setting on the original training sets. For all classifiers (φ id , φ class and φ down ) we employ a ResNet-18 network [6]. Classifiers φ id and φ class are trained on the original training set, while φ down (i.e., the task classifier, one for each task) is trained on synthetic samples only. For φ id , we apply standard data augmentation (e.g., horizontal flip, rotation) and add five GAN projections for each identity, to mitigate the domain shift between real and synthetic images. φ down is trained with a learning rate of 0.001, a batch size of 32, for 200 (Shenzhen) and 500 (APTOS) epochs. Model selection is carried out at the best validation accuracy, and results are averaged over 5 runs. When applying PLAN on a pair of latent points, we initialize a trajectory of T = 50 points through linear interpolation, and optimize Eq. 4 for 100 steps using Adam with a learning rate of 0.1; λ 1 and λ 2 are set to 0.1 and 1, respectively. Experiments are performed on an NVIDIA RTX 3090."
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,4.2,Results,"To measure the privacy-preserving properties of our approach, we employ the membership inference attack (MIA) [20], which attempts to predict if a sample was used in a classifier's training set. We use attacker model and settings defined in [10,16], training the attacker on 30% of the training set (seen by PLAN through φ id and φ class ) and 30% of the test set (unseen by PLAN); as a test set for MIA, we reserve 60% of the original test set, leaving 10% as a validation set to select the best attacker. Ideally, if the model preserves privacy, the attacker achieves chance performance (50%), showing inability to identify samples used for training. We also report the FID of the generated dataset, to measure its level of realism, and the mean of the minimum LPIPS [25] (""mmL"" for short) distances between each generated sample and its closest real image, to measure how generated samples differ from real ones. We compare PLAN to a linear interpolation between arbitrary pairs of start and end latent points, and compute the above measures on the images corresponding to the latent trajectories obtained by two approaches. We also report the results of the classifier trained on real data to provide additional bounds for both classification accuracy and privacy-preserving performance.Results in Table 1 demonstrate that our approach performs similarly to training with real data, but with higher accuracy with respect to the linear baseline. Privacy-preserving results, measured through MIA and mmL, demonstrate the reliability of our PLAN strategy in removing sensitive information, reaching the ideal lower bound of MIA accuracy.Figure 2 shows how, for given start and end points, PLAN-generated samples keep high quality but differ significantly from real samples, while latent linear interpolation may lead to near-duplicates. This is confirmed by the higher LPIPS distance between generated samples and the most similar real samples for PLAN. After verifying the generative and privacy-preserving capabilities of our approach, we evaluate its contribution to classification accuracy when combined with existing k-same methods, namely k-SALSA [9] and LCI [18]. Both methods apply latent clustering to synthesize a privacy-preserving dataset, but exhibit low performance transferability to classification tasks, due to the reduced size of the resulting synthetic dataset. We carry out these experiments on APTOS, using k = 5 and k = 10, for comparison with [9] 2 . Results are given in Table 2 and show how our PLAN strategy enhances performance of the two baseline methods, reaching performance similar to training the retinopathy classifier with real samples (i.e., 50.74 on real data vs 44.95 when LCI [18] is combined with PLAN) and much higher than the variants without PLAN. We also measured MIA accuracy between the variants with and without PLAN, and we did not observe significant change among the different configurations: accuracy was at the chance level in all cases, suggesting their privacy-preserving capability.  "
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,5,Conclusion,"We presented PLAN, a latent space navigation strategy designed to reduce privacy risks when using GANs for training models on synthetic data. Experimental results, on two medical image analysis tasks, demonstrate how PLAN is robust to membership inference attacks while effectively supporting model training with performance comparable to training on real data. Furthermore, when PLAN is combined with state-of-the-art k-anonymity methods, we observe a mitigation of performance drop while maintaining privacy-preservation properties. Future research directions will address the scalability of the method to large datasets with a high number of identities, as well as learning latent trajectories with arbitrary length to maximize privacy-preserving and augmentation properties of the synthetic datasets."
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,,Fig. 1 .,
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,,Fig. 2 .,
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,,Table 1 .,
A Privacy-Preserving Walk in the Latent Space of Generative Models for Medical Applications,,Table 2 .,
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,1,Introduction,"Medical image segmentation is a critical task in computer-assisted diagnosis, treatment planning, and intervention. While large-scale transformers have demonstrated impressive performance in various computer vision tasks [7,10,15], such as natural image recognition, detection, and segmentation [5,16], they face significant challenges when applied to medical image analysis. The primary challenge is the scarcity of labeled medical images due to the difficulty in collecting and labeling them, which requires specialized medical knowledge and is timeconsuming [12,23,25]. The second challenge is the ability to identify sparse and obscure patterns in medical images, including blurred and dim images with small segmentation targets. Hence, it is imperative to develop a precise and dataefficient pipeline for medical image analysis networks to enhance their accuracy and reliability in computer-assisted medical diagnoses.Self-supervised learning, a technique for constructing feature embedding spaces by designing pretext tasks, has emerged as a promising solution for addressing the issue of label deficiency. One representative methodology for self-supervised learning is the masked autoencoder (MAE) [11]. MAEs learn to reconstruct input data after randomly masking certain input features. This approach has been successfully deployed in various applications, including image denoising, text completion, anomaly detection, and feature learning. In the field of medical image analysis, MAE pre-training has also been found to be effective [32]. Nevertheless, these studies have a limitation in that they require a large set of unlabeled data and do not prioritize improving output reliability, which may undermine their practicality in the real world.In this paper, we propose Masked Multi-view with Swin (SwinMM), the first comprehensive multi-view pipeline for self-supervised medical image segmentation. We draw inspiration from previous studies [26,28,31,33] and aim to enhance output reliability and data utilization by incorporating multi-view learning into the self-supervised learning pipeline. During the pre-training stage, the proposed approach randomly masks 3D medical images and creates various observations from different views. A masked multi-view encoder processes these observations simultaneously to accomplish four proxy tasks: image reconstruction, rotation, contrastive learning, and a novel proxy task that utilizes a mutual learning paradigm to maximize consistency between predictions from different views. This approach effectively leverages hidden multi-view information from 3D medical data and allows the encoder to learn enriched high-level representations of the original images, which benefits the downstream segmentation task. In the fine-tuning stage, different views from the same image are encoded into a series of representations, which will interact with each other in a specially designed cross-view attention block. A multi-view consistency loss is imposed to generate aligned output predictions from various perspectives, which enhances the reliability and precision of the final output. The complementary nature of the different views used in SwinMM results in higher precision, requiring less training data and annotations, which holds significant potential for advancing the state-of-the-art in this field. In summary, the contributions of our study are as follows:-We present SwinMM, a unique and data-efficient pipeline for 3D medical image analysis, providing the first comprehensive multi-view, self-supervised approach in this field. -Our design includes a masked multi-view encoder and a novel mutual learningbased proxy task, facilitating effective self-supervised pretraining. -We incorporate a cross-view decoder for optimizing the utilization of multiview information via a cross-attention block. -SwinMM delivers superior performance with an average Dice score of 86.18% on the WORD dataset, outperforming other leading segmentation methods in both data efficiency and segmentation performance."
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,2,Method,"Figure 1 provides an overview of SwinMM, comprising a masked multi-view encoder and a cross-view decoder. SwinMM creates multiple views by randomly masking an input image, subsequently feeding these masked views into the encoder for self-supervised pre-training. In the fine-tuning stage, we architect a cross-view attention module within the decoder. This design facilitates the effective utilization of multi-view information, enabling the generation of more precise segmentation predictions."
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,2.1,Pre-training,"Masked Multi-view Encoder. Following [11], we divided the 3D images into sub-volumes of the same size and randomly masked a portion of them, as demonstrated in Fig. 2. These masked 3D patches, from different perspectives, were then utilized for self-supervised pretraining by the masked multi-view encoder.As shown in Fig. 1, the encoder is comprised of a patch partition layer, a patch embedding layer, and four Swin Transformer layers [17]. Notably, unlike typical transformer encoders, our masked multi-view encoder can process multiple inputs from diverse images with varying views, making it more robust for a broad range of applications.Pre-training Strategy. To incorporate multiple perspectives of a 3D volume, we generated views from different observation angles, including axial, coronal, and sagittal. Furthermore, we applied rotation operations aligned with each perspective, consisting of angles of 0 • , 90 • , 180 • , and 270 • along the corresponding direction. To facilitate self-supervised pre-training, we devised four proxy tasks. The reconstruction and rotation tasks measure the model's performance on each input individually, while the contrastive and mutual learning tasks enable the model to integrate information across multiple views.-The reconstruction task compares the difference between unmasked input X and the reconstructed image y rec . Following [11], we adopt Mean-Square-Error (MSE) to compute the reconstruction loss: -The rotation task aims to detect the rotation angle of the masked input along the axis of the selected perspective, with possible rotation angles of 0 • , 90 • , 180 • , and 270 • . The model's performance is evaluated using cross-entropy loss, as shown in Eq. 2, where y rot and y r represent the predicted probabilities of the rotation angle and the ground truth, respectively.-The contrastive learning task aims to assess the effectiveness of a model in representing input data by comparing high-level features of multiple views. Our working assumption is that although the representations of the same sample may vary at the local level when viewed from different perspectives, they should be consistent at the global level. To compute the contrastive loss, we use cosine similarity sim(•), where y con i and y con j represent the contrastive pair, t is a temperature constant, and 1 is the indicator function.-The mutual learning task assesses the consistency of reconstruction results from different views to enable the model to learn aligned information from multi-view inputs. Reconstruction results are transformed into a uniform perspective and used to compute a mutual loss L mul , which, like the reconstruction task, employs the MSE loss. Here, y rec i and y rec j represent the predicted reconstruction from views i and j, respectively.The total pre-training loss is as shown in Eq. 5. The weight coefficients α 1 , α 2 , α 3 and α 4 are set equal in our experiment (α 1 = α 2 = α 3 = α 4 = 1).(5)"
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,2.2,Fine-Tuning,"Cross-View Decoder. The structure of the cross-view decoder, comprising Conv-Blocks for skip connection, Up-Blocks for up-sampling, and a Crossview Attention block for views interaction, is depicted in Fig. 1. The Conv-Blocks operate on different layers, reshaping the latent representations from various levels of the masked multi-view encoder by performing the convolution, enabling them to conform to the feature size in corresponding decoder layers ( H 4,5). At the bottom of the U-shaped structure, the cross-view attention module integrates the information from two views. The representations at this level are assumed to contain similar semantics. The details of the cross-view attention mechanism are presented in Fig. 1 and Eq. 6. In the equation, f i and f j denote the representations of different views, while Q i , K i , and V i refer to the query, key, and value matrices of f i , respectively."
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Cross Attention(f,"Multi-view Consistency Loss. We assume consistent segmentation results should be achieved across different views of the same volume. To quantify the consistency of the multi-view results, we introduce a consistency loss L mc , calculated using KL divergence in the fine-tuning stage, as in previous work on mutual learning [29]. The advantage of KL divergence is that it does not require class labels and has been shown to be more robust during the fine-tuning stage.We evaluate the effectiveness of different mutual loss functions in an ablation study (see supplementary). The KL divergence calculation is shown in Eq. 7:where V i (x m ) and V j (x m ) denote the different view prediction of m-th voxel.N represents the number of voxels of case x. V i (x) and V j (x) denote different view prediction of case x. We measure segmentation performance using L DiceCE , which combines Dice Loss and Cross Entropy Loss according to [24].where p m and y i respectively represent the predicted and ground truth labels for the m-th voxel, while N is the total number of voxels. We used L fin during the fine-tuning stage, as specified in Eq. 9, and added weight coefficients β DiceCE and β mc for different loss functions, both set to a default value of 1. "
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,2.3,Semi-supervised Learning with SwinMM,"As mentioned earlier, the multi-view nature of SwinMM can substantially enhance the reliability and accuracy of its final output while minimizing the need for large, high-quality labeled medical datasets, making it a promising candidate for semi-supervised learning. In this study, we propose a simple variant of SwinMM to handle semi-supervision. As depicted in Fig. 3, we leverage the diverse predictions from different views for unlabeled data and generate aggregated pseudo-labels for the training process. Compared to single-view models, SwinMM's multi-view scheme can alleviate prediction uncertainty by incorporating more comprehensive information from different views, while ensemble operations can mitigate individual bias."
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,3,Experiments,"Datasets and Evaluation. Our pre-training dataset includes 5833 volumes from 8 public datasets: AbdomenCT-1K [19], BTCV [13], MSD [1], TCIA-Covid19 [9], WORD [18], TCIA-Colon [14], LiDC [2], and HNSCC [8]. We choose two popular datasets, WORD (The Whole abdominal ORgan Dataset) and ACDC [3] (Automated Cardiac Diagnosis Challenge), to test the downstream segmentation performance. The accuracy of our segmentation results is evaluated using two commonly used metrics: the Dice coefficient and Hausdorff Distance (HD).Implementation Details. Our SwinMM is trained on 8 A100 Nvidia GPUs with 80G gpu memory. In the pre-training process, we use a masking ratio of 50%, a batch size of 2 on each GPU, and an initial learning rate of 5e-4 and weight decay of 1e-1. In the finetuning process, we apply a learning rate of 3e-4 and a layer-wise learning rate decay of 0.75. We set 100K steps for pre-training and 2500 epochs for fine-tuning. We use the AdamW optimizer and the cosine learning rate scheduler in all experiments with a warm-up of 50 iterations to train our model. We follow the official data-splitting methods on both WORD and ACDC, and report the results on the test dataset. For inference on these datasets, we applied a double slicing window inference, where the window size is 64 × 64 × 64 and the overlapping between windows is 70%."
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,3.1,Results,"Comparing with SOTA Baselines. We compare the segmentation performance of SwinMM with several popular and prominent networks, comprising fully supervised networks, i.e., U-Net [22], Swin UNet [17], VT-UNet [21], UNETR [10], DeepLab V3+ [6], ESPNet [20], DMFNet [4], and LCOVNet [30], as well as self-supervised method Swin UNETR [24]. As shown in Table 1 and Table 2, our proposed SwinMM exhibits remarkable efficiency in medical segmentation by surpassing all other methods and achieves higher average Dice (86.18% on WORD and 90.80% on ACDC) and lower HD (9.35 on WORD and 6.37 on ACDC).Single View vs. Multiple Views. To evaluate the effectiveness of our proposed multi-view self-supervised pretraining pipeline, we compared it with the state-of-the-art self-supervised learning method SwinUNETR [24] on WORD [18] dataset. Specifically, two SwinUNETR-based methods are compared: using fixed single views (Axial, Sagittal, and Coronal) and using ensembled predictions from multiple views (denoted as SwinUNETR-Fuse). Our results, presented in Table 3, show that our SwinMM surpasses all other methods including SwinUNETR-Fuse, highlighting the advantages of our unique multi-view designs. Moreover, by incorporating multi-view ensemble operations, SwinMM can effectively diminish the outliers in hard labels and produce more precise outputs, especially when dealing with harder cases such as smaller organs. The supplementary material provides qualitative comparisons of 2D/3D segmentation outcomes.  "
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,3.2,Ablation Study,"To fairly evaluate the benefits of our proposed multi-view design, we separately investigate its impact in the pre-training stage, the fine-tuning stage, as well as both stages. Additionally, we analyze the role of each pre-training loss functions.Pre-training Loss Functions. The multi-view pre-training is implemented by proxy tasks. The role of each task can be revealed by taking off other loss functions. For cheaper computations, we only pre-train our model on 2639 volumes from 5 datasets (AbdomenCT-1K, BTCV, MSD, TCIA-Covid19, and WORD) in these experiments, and we applied a 50% overlapping window ratio, during testing time. As shown in Table 4, our proposed mutual loss brings a noticeable improvement in Dice (around 1%) over the original SwinUNETR setting. When combining all the proxy tasks, our SwinMM achieves the best performance.Data Efficiency. The data efficiency is evaluated under various semi-supervised settings. Initially, a base model is trained from scratch with a proportion of supervised data from the WORD dataset for 100 epochs. Then, the base model finishes the remaining training procedure with unsupervised data. The proportion of supervised data (denoted as label ratio) varies from 10% to 100%. Table 5 shows SwinMM consistently achieves higher Dice (%) than SwinUNETR, and its superiority is more remarkable when training with fewer supervised data."
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,4,Conclusion,"This paper introduces SwinMM, a self-supervised multi-view pipeline for medical image analysis. SwinMM integrates a masked multi-view encoder in the pre-training phase and a cross-view decoder in the fine-tuning phase, enabling seamless integration of multi-view information, thus boosting model accuracy and data efficiency. Notably, it introduces a new proxy task employing a mutual learning paradigm, extracting hidden multi-view information from 3D medical data. The approach achieves competitive segmentation performance and higher data-efficiency than existing methods and underscores the potential and efficacy of multi-view learning within the domain of self-supervised learning."
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Fig. 1 .,
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Fig. 2 .Fig. 3 .,
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Table 1 .,
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Table 2 .,
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Table 3 .,
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Table 4 .,
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Table 5 .,
SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_47.
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,1,Introduction,"In recent years, machine learning-based medical diagnosis systems have been introduced by many institutions. Although these systems achieve high accuracy in predicting medical conditions, bias has been found in dermatological disease datasets as shown in [6,12,19]. This bias can arise when there is an imbalance in the number of images representing different skin tones, which can lead to inaccurate predictions and misdiagnosis due to biases towards certain skin tones.The discriminatory nature of these models can have a negative impact on society by limiting access to healthcare resources for different sensitive groups, such as those based on race or gender.Several methods are proposed to alleviate the bias in machine learning models, including pre-processing, in-processing, and post-processing strategies. Preprocessing strategies adjust training data before training [14,15] or assign different weights to different data samples to suppress the sensitive information during training [10]. In-processing modifies the model architecture, training strategy, and loss function to achieve fairness, such as adversarial training [1,22] or regularization-based [9,16] methods. Recently, pruning [21] techniques have also been used to achieve fairness in dermatological disease diagnosis. However, these methods may decrease accuracy for both groups and do not guarantee explicit protection for the unprivileged group when enforcing fairness constraints. Postprocessing techniques enhance fairness by adjusting the model's output distribution. This calibration is done by taking the model's prediction, and the sensitive attribute as inputs [4,7,23]. However, pre-processing and post-processing methods have limitations that are not applicable to dermatological disease diagnostic tasks since they need extra sensitive information during the training time.In this paper, we observe that although features from a deep layer of a neural network are discriminative for target groups (i.e., different dermatological diseases), they cause fairness conditions to deteriorate, and we will demonstrate this observation by analyzing the entanglement degree regarding sensitive information with the soft nearest neighbor loss [5] of image features in Sect. 2. This finding is similar to ""overthinking"" phenomenon in neural networks [11], where accuracy decreases as the features come from deeper in a neural network and motivate us to use a multi-exit network [11,18] to address the fairness issue.Through extensive experiments, we demonstrate that our proposed multi-exit convolutional neural network (ME-CNN) can achieve fairness without using sensitive attributes (unawareness) in the training process, which is suitable for dermatological disease diagnosis because the sensitive attributes information exists privacy issues and is not always available. We compare our approach to the current state-of-the-art method proposed in [21] and find that the ME-CNN can achieve similar levels of fairness. To further improve fairness, we designed a new framework for fair multi-exit. With the fairness constraint and the early exit strategy at the inference stage, a sufficient discriminative basis can be obtained based on low-level features when classifying easier samples. This contributes to selecting a more optimal prediction regarding the trade-off between accuracy and fairness for each test instance.The main contributions of the proposed method are as follows:-Our quantitative analysis shows that the features from a deep layer of a neural network are highly discriminative yet cause fairness to deteriorate. -We propose a fairness through unawareness framework and use multi-exit training to improve fairness in dermatological disease classification. -We demonstrate the extensibility of our framework, which can be applied to various state-of-the-art models and achieve further improvement. Through extensive experiments, we show that our approach can improve fairness while keeping competitive accuracy on both the dermatological disease dataset, ISIC 2019 [2], and Fitzpatrick-17k datasets [6]. -For reproducibility, we have released our code at https://github.com/chiuhaohao/Fair-Multi-Exit-Framework"
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,2,Motivation,"In this section, we will discuss the motivation behind our work. The soft nearest neighbor loss (SNNL), as introduced in [5], measures the entanglement degree between features for different classes in the embedded space, and can serve as a proxy for analyzing the degree of fairness in a model. Precisely, we measure the features of SNNL concerning the different sensitive attributes. When the measured SNNL is high, the entangled features are indistinguishable among sensitive attributes. On the other hand, when the SNNL is low, the feature becomes more distinguishable between the sensitive attributes, leading to a biased performance in downstream tasks since the features consist of sensitive information.To evaluate the SNNL, we analyzed the performance of ResNet18 [8] and VGG-11 [17] on the ISIC 2019 and Fitzpatrick-17k datasets. We compute the SNNL of sensitive attributes at different inference positions and observed that the SNNL in both datasets decreased by an average of 1.1% and 0.5%, respectively, for each inference position from shallow to deep in ResNet18. For VGG-11, the SNNL in both datasets decreases by an average of 1.4% and 1.2%, respectively. This phenomenon indicates that the features become more distinguishable to sensitive attributes. The details are provided in the supplementary materials.A practical approach to avoiding using features that are distinguishable to sensitive attributes for prediction is to choose the result at a shallow layer for the final prediction. To the best of our knowledge, we are the first work that leverages the multi-exit network to improve fairness. In Sect. 5, we demonstrate that our framework can be applied to different network architectures and datasets."
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,3,Method,
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,3.1,Problem Formulation,"In the classification task, define input features x ∈ X = R d , target class, y ∈ Y = {1, 2, ..., N }, and sensitive attributes a ∈ A = {1, 2, ..., M }. In this paper, we focus on the sensitive attributes in binary case, that is a ∈ A = {0, 1}. The goal is to learn a classifier f : X → Y that predicts the target class y to achieve high accuracy while being unbiased to the sensitive attributes a."
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,3.2,Multi-Exit (ME) Training Framework,"Our approach is based on the observation that deep neural networks can exhibit bias against certain sensitive groups, despite achieving high accuracy in deeper layers. To address this issue, we propose a framework leveraging an early exit policy, which allows us to select a result at a shallower layer with high confidence while maintaining accuracy and mitigating fairness problems.We illustrate our multi-exit framework in Fig. 1. Our proposed loss function consists of the cross-entropy loss, l t , and a fairness regularization loss, l s , such as the Maximum Mean Discrepancy (MMD) [9] or the Hilbert-Schmidt Independence Criterion (HSIC) [16], which are replicated for each internal classifier (CLF n ) and original final classifier (CLF f ). The final loss is obtained through a weighted sum of each CLF 's loss, i.e., loss = (), where α is determined by the depth of each CLF , similar to [11,13]. Moreover, the hyperparameter λ controls the trade-off between accuracy and fairness. Our approach ensures that the model optimizes for accuracy and fairness, leveraging both shallow and deep layer features in the loss function. Even without any fairness regularization (λ = 0), our experiments demonstrate a notable improvement in fairness (see Sect. 5.2).Our framework can also be extended to other pruning-based fairness methods, such as FairPrune [21]. We first optimize the multi-exit model using the original multi-exit loss function and then prune it using the corresponding pruning strategy. Our approach has been shown to be effective (see Sect. 5.1).During inference, we calculate the softmax score of each internal classifier's prediction, taking the maximum probability value as the confidence score. We use a confidence threshold θ to maintain fairness and accuracy. High-confidence instances exit early, and we select the earliest internal classifier with confidence above θ for an optimal prediction of accuracy and fairness. "
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,4,Experiment,
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,4.1,Dataset,"In this work, we evaluate our method on two dermatological disease datasets, including ISIC 2019 challenge [2,19] and the Fitzpatrick-17k dataset [6]. ISIC 2019 challenge contains 25,331 images in 9 diagnostic categories for target labels, and we take gender as our sensitive attribute. The Fitzpatrick-17k dataset contains 16,577 images in 114 skin conditions of target labels and defines skin tone as the sensitive attribute. Next, we apply the data augmentation, including random flipping, rotation, scaling and autoaugment [3]. After that, we follow the same data split described in [21] to split the data."
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,4.2,Implementation Details and Evaluation Protocol,"We employ ResNet18 and VGG-11 as the backbone architectures for our models. The baseline CNN and the multi-exit models are trained for 200 epochs using an SGD optimizer with a batch size of 256 and a learning rate of 1e-2. Each backbone consisted of four internal classifiers (CLF s) and a final classifier (CLF f ). For ResNet18, the internal features are extracted from the end of each residual block, and for VGG-11 the features are extracted from the last four max pooling layers. The loss weight hyperparameter α is selected based on the approach of [11] and set to [0.3, 0.45, 0.6, 0.75, 0.9] for the multi-exit models and the λ is set to 0.01. The confidence threshold θ of the test set is set to 0.999, based on the best result after performing a grid search on the validation set.To evaluate the fairness performance of our framework, we adopted the multiclass equalized opportunity (Eopp0 and Eopp1) and equalized odds (Eodd) metrics proposed in [7]. Specifically, we followed the approach of [21] for calculating these metrics."
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,5,Results,
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,5.1,Comparison with State-of-the-Art,"In this section, we compare our framework with several baselines, including CNN (ResNet18 and VGG-11), AdvConf [1], AdvRev [22], DomainIndep [20], HSIC [16], and MFD [9]. We also compare our framework to the current state-of-theart method FairPrune [21]. For each dataset, we report accuracy and fairness results, including precision, recall, and F1-score metrics."
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,,ISIC 2019 Dataset.,"In Table 1, ME-FairPrune refers to the FairPrune applied in our framework. It shows that our ME framework has improved all fairness scores and accuracy in all average scores when applied to FairPrune. Additionally, the difference in each accuracy metric is smaller than that of the original FairPrune. This is because the ME framework ensures that the early exited instances have a high level of confidence in their correctness, and the classification through shallower, fairer features further improves fairness. Fitzpatrick-17k Dataset. To evaluate the extensibility of our framework on different model structures, we use VGG-11 as the backbone of the Fitzpatrick-17k dataset. Table 2 demonstrates that the ME framework outperforms all other methods with the best Eopp1 and Eodd scores, showing a 7.5% and 7.9% improvement over FairPrune, respectively. Similar to the ISIC 2019 dataset, our results show better mean accuracy and more minor accuracy differences in all criteria than FairPrune. Furthermore, the F1-score and Precision average value are superior to other methods, which show 8.1% and 4.1% improvement over FairPrune, respectively."
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,5.2,Multi-Exit Training on Different Method,"In this section, we evaluate the performance of our ME framework when applied to different methods. Table 3 presents the results of our experiments on ResNet18, MFD, and HSIC. Our ME framework improved the Eopp1 and Eodd scores of the original ResNet18 model, which did not apply fairness regularization loss, l s , in total loss. Furthermore, our framework achieved comparable performance in terms of fairness to FairPrune, as shown in Table 1. This demonstrates its potential to achieve fairness without using sensitive attributes during training.We also applied our ME framework to MFD and HSIC, which initially exhibited better fairness performance than other baselines. With our framework, these models showed better fairness while maintaining similar levels of accuracy. These findings suggest that our ME framework can improve the fairness of existing models, making them more equitable without compromising accuracy."
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,5.3,Ablation Study,"Effect of Different Confidence Thresholds. To investigate the impact of varying confidence thresholds θ on accuracy and fairness, we apply the ME-FairPrune method to a pre-trained model from the ISIC 2019 dataset and test different thresholds. Our results, shown in Fig. 2(a), indicate that increasing the threshold improves accuracy and fairness. Thus, we recommend setting the threshold to 0.999 for optimal performance. Effect of Early Exits. In Fig. 2(b), we compare ME-FairPrune using an early exit policy with exiting from each specific exit. The results show that across all criteria, no specific CLF outperforms the early exit policy in terms of Eodd, while our early exit policy achieves an accuracy level comparable to the original classifier output CLF f . These findings underscore the importance of the proposed early exit strategy for achieving optimal performance.  "
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,6,Conclusion,We address the issue of deteriorating fairness in deeper layers of deep neural networks by proposing a multi-exit training framework. Our framework can be applied to various bias mitigation methods and uses a confidence-based exit strategy to simultaneously achieve high accuracy and fairness. Our results demonstrate that our framework achieves the best trade-off between accuracy and fairness compared to the state-of-the-art on two dermatological disease datasets.
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,,Fig. 1 .,
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,,Fig. 2 .,
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,,Table 1 .,
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,,Table 2 .,
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,,Table 3 .,
Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 10.
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,1,Introduction,"Magnetic Resonance Imaging (MRI) is a widely applied imaging technique for medical diagnosis since it is non-invasive and able to generate high-quality clinical images without exposing the subject to radiation. The imaging speed is of vital importance for MRI [19] in that long imaging limits the spatial and temporal resolution and induces reconstruction artifacts such as subject motion during the imaging process. One possible way to speed up the imaging process is to use multiple receiver coils, and reduce the amount of captured data by subsampling the k-space and exploiting the redundancy in the measurements [19,24]. In recent years, Deep-Learning (DL) based methods have shown great potential as a data-driven approach in achieving faster imaging and better reconstruction quality. DL-based methods can be categorized into two families: unrolled methods that alternate between measurement consistency and a regularization step based on a feed-forward network [12,35]; conditional generative models that use measurements as guidance during the generative process [15,21]. Compared with unrolled methods, generative approaches have recently been shown to be more robust when test samples are out of the training distribution due to their stronger ability to learn the data manifold [6,15]. Among generative approaches, diffusion models have recently achieved the state of the art performance [5][6][7][8]18,30,33].However, measurements in MRI are often noisy due to the imaging hardware and thermal fluctuations of the subject [26]. As a result, DL-based methods fail dramatically when a distribution shift due to noise or other scanning parameters occurs during training and testing [2,17]. Although diffusion models were shown to be robust against distribution shifts in noisy inverse problems [4,27] and MRI reconstruction, the hyperparameters that balance the measurement consistency and prior are tuned manually during validation where ground truth is available [15]. These hyperparameters may not generalize well to test settings as shown in Fig. 1, and ground truth data may not be available for validation.In this paper, we propose a framework with diffusion models for MRI reconstruction that is robust to measurement noise and distribution shifts. To achieve robustness, we perform test-time tuning when ground truth data is not available at test time, using Stein's Unbiased Risk Estimator (SURE) [31] as a surrogate loss function for the true mean-squared error (MSE). SURE is used to tune the weight of the balance between measurement consistency and the learned prior for each diffusion step such that the measurement consistency adapts to the measurement noise level during the inference process. SURE is then used to perform early stopping to prevent overfitting to measurement noise at inference.We evaluate our framework on FastMRI [36] and Mridata [22], and show that it achieves state-of-the-art performance across different noise levels, acceleration rates and anatomies without fine-tuning the pre-trained network or performing validation tuning, with no access to the target distribution. In summary, our contributions are three-fold:-We propose a test-time hyperparameter tuning algorithm that boosts the robustness of the pre-trained diffusion models against distribution shifts; -We propose to use SURE as a surrogate loss function for MSE and incorporate it into the sampling stage for test-time tuning and early stopping without access to ground truth data from the target distribution; -SMRD achieves state-of-the-art performance across different noise levels, acceleration rates, and anatomies."
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,2,Related Work,"SURE for MRI Reconstruction. SURE has been used for tuning parameters of compressed sensing [14], as well as for unsupervised training of DL-based methods for MRI reconstruction [1]. To the best of our knowledge, SURE has not yet been applied to the sampling stage of diffusion models in MRI reconstruction or in another domain.Adaptation in MRI Reconstruction. Several proposals have been made for adaptation to a target distribution in MRI reconstruction using self-supervised losses for unrolled models [9,34]. Later, [3] proposed single-shot adaptation for test-time tuning of diffusion models by performing grid search over hyperparameters with a ground truth from the target distribution. However, this search is computationally costly, and the assumption of access to samples from the target distribution is limiting for imaging cases where ground truth is not available."
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,3,Method,
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,3.1,Accelerated MRI Reconstruction Using Diffusion Models,"The sensing model for accelerated MRI can be expressed aswhere y is the measurements in the Fourier domain (k-space), x is the real image, S are coil sensitivity maps, F is the Fourier transform, Ω is the undersampling mask, ν is additive noise, and A = ΩF S denotes the forward model. Diffusion models are a recent class of generative models showing remarkable sample fidelity for computer vision tasks [13]. A popular class of diffusion models is score matching with Langevin dynamics [28]. Given i.i.d. training samples from a high-dimensional data distribution (denoted as p(x)), diffusion models can estimate the scores of the noise-perturbed data distributions. First, the data distribution is perturbed with Gaussian noise of different intensities with standard deviation β t for various timesteps t, such that p βt (x|x) = N (x|x, β 2 t I) [28], leading to a series of perturbed data distributions p(x t ). Then, the score function ∇ xt log p(x t ) can be estimated by training a joint neural network, denoted as f (x t ; t), via denoising score matching [32]. After training the score function, annealed Langevin Dynamics can be used to generate new samples [29] break 10: return xt+1 from a noise distribution x 0 ∼ N (0, I), annealed Langevin Dynamics is run for T stepswhere η t is a sampling hyperparameter, and ζ t ∼ N (0, I). For MRI reconstruction, measurement consistency can be incorporated via sampling from the posterior distribution p(x t |y) [15]:The form of ∇ xt log p(x t |y) depends on the specific inference algorithm [6,7]."
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,3.2,Stein's Unbiased Risk Estimator (SURE),"SURE is a statistical technique which serves as a surrogate for the true mean squared error (MSE) when the ground truth is unknown. Given the ground truth image x, the zero-filled image can be formulated as x zf = x + z, where z is the noise due to undersampling. Then, SURE is an unbiased estimator of MSE = xx 2 2 [31] and can be calculated as:where x zf is the input of a denoiser, x is the prediction of a denoiser, N is the dimensionality of x. In practical applications, the noise variance σ 2 is not known a priori. In such cases, it can be assumed that the reconstruction error is not large, and the sample variance between the zero-filled image and the reconstruction can be used to estimate the noise variance, where σ 2 ≈ xx zf 2 2 /N [11]. Then, SURE can be rewritten as:A key assumption behind SURE is that the noise process that relates the zero-filled image to the ground truth is i.i.d. normal, namely z ∼ N(0, σ 2 I). However, this assumption does not always hold in the case of MRI reconstruction due to undersampling in k-space that leads to structured aliasing. In this case, density compensation can be applied to enforce zero-mean residuals and increase residual normality [11]."
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,3.3,SURE-Based MRI Reconstruction with Diffusion Models,"Having access to SURE at test time enables us to monitor it as a proxy for the true MSE. Our goal is to optimize inference hyperparameters in order to minimize SURE. To do this, we first consider the following optimization problem at time-step t:where we introduce a time-dependent regularization parameter λ t . The problem in Eq. 6 can be solved using alternating minimization (which we call AM-Langevin):Equation 8 can be solved using the conjugate gradient (CG) algorithm, with the iterateswhere A H is the Hermitian transpose of A, x zf = A H y is the zero-filled image, and h denotes the full update including the Langevin Dynamics and CG. This allows us to explicitly control the balance between the prior through the score function and the regularization through λ t .Monte-Carlo SURE. Calculating SURE requires evaluating the trace of the Jacobian tr( ∂xt+1 ∂x zf ), which can be computationally intensive. Thus, we approximate this term using Monte-Carlo SURE [25]. Given an N -dimensional noise vector μ from N (0, I) and the perturbation scale , the approximation is: This approximation is typically quite tight for some small value ; see e.g., [11]. Then, at time step t, SURE is given aswhere h(x t , λ t ) is the prediction which depends on the input x zf , shown in Eq. 9.Tuning λ t . By allowing λ t to be a learnable, time-dependent variable, we can perform test-time tuning (TTT) for λ t by updating it in the direction that minimizes SURE. As both SURE(t) and λ t are time-dependent, the gradients can be calculated with backpropagation through time (BPTT). In SMRD, for the sake of computation, we apply truncated BPTT, and only consider gradients from the current time step t. Then, the λ t update rule is:where α is the learning rate for λ t .Early Stopping (ES). Under measurement noise, it is critical to prevent overfitting to the measurements. We employ early-stopping (ES) by monitoring the moving average of SURE loss with a window size w at test time. Intuitively, we perform early stopping when the SURE loss does not decrease over a certain window. We denote the early-stopping iteration as T ES . Our full method is shown in Algorithm 1."
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,4,Experiments,"Experiments were performed with PyTorch on a NVIDIA Tesla V100 GPU [23].For baselines and SMRD, we use the score function from [15] which was trained on a subset of the FastMRI multi-coil brain dataset. We refer the reader to [15] for implementation details regarding the score function. For all AM-Langevin variants, we use 5 CG steps. In SMRD, for tuning λ t , we use the Adam optimizer [16] with a learning rate of α = 0.2 and λ 0 = 2. In the interest of inference speed, we fixed λ t after t = 500, as convergence was observed in earlier iterations. For SURE early stopping, we use window size w = 160. For evaluation, we used Zero-filled 24.5/0.63 24.5/0.61 the multi-coil fastMRI brain dataset [36] and the fully-sampled 3D fast-spin echo multi-coil knee MRI dataset from mridata.org [22] with 1D equispaced undersampling and a 2D Poisson Disc undersampling mask respectively, as in [15]. We used 6 volumes from the validation split for fastMRI, and 3 volumes for Mridata where we selected 32 middle slices from each volume so that both datasets had 96 test slices in total.Noise Simulation. The noise source in MRI acquisition is modeled as additive complex-valued Gaussian noise added to each acquired k-space sample [20].To simulate measurement noise, we add masked complex-gaussian noise to the masked k-space ν ∼ N (0, σ) with standard deviation σ, similar to [10]."
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,5,Results and Discussion,"We compare with three baselines: (1) Csgm-Langevin [15], using the default hyperparameters that were tuned on two validation brain scans at R = 4;(2) Csgm-Langevin with early stopping using SURE loss with window size w = 25;(3) AM-Langevin where λ t = λ 0 and fixed throughout the inference.For tuning AM-Langevin, we used a brain scan for validation at R = 4 in the noiseless case (σ = 0) similar to Csgm-Langevin, and the optimal value was λ 0 = 2.We evaluate the methods across different measurement noise levels, acceleration rates and anatomies using the same pretrained score function from [15]. Table 1 shows a comparison of reconstruction methods applied to the FastMRI brain dataset where R = {4, 8}, and σ = {0, 0.0025, 0.005}. SMRD performs best   2 shows a comparison of reconstruction methods in the cross-dataset setup with the Mridata knee dataset where R = {12, 16}. SMRD outperformed baselines across every R and σ, and is on par with baselines when σ = 0. Figure 2 shows example reconstructions where R = 12, σ = 0. Hallucination artifacts are visible in baselines even with no added measurement noise, whereas SMRD mitigates these artifacts and produces a reconstruction with no hallucinations. Figure 3a shows SSIM on a knee validation scan with varying noise levels σ and λ. As σ increases, the optimal λ value increases as well. Thus, hyperparameters tuned with σ = 0 do not generalize well under measurement noise change, illustrating the need for test-time tuning under distribution shift. Figure 3b shows true MSE vs SURE for an example brain slice where σ = 0.0075, R = 8. SURE accurately estimates MSE, and the increase in loss occurs at similar iterations, enabling us to perform early stopping before true MSE increases. The evolution of images across iterations for this sample is shown in Fig. 4. SMRD accurately captures the correct early stopping point. As a result of early stopping, SMRD mitigates artifacts, and produces a smoother reconstruction. Table 3 shows the ablation study for different components of SMRD. TTT and ES both improve over AM-Langevin, where SMRD works best for all σ while being on par with others on σ = 0."
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,6,Conclusion,"We presented SMRD, a SURE-based TTT method for diffusion models in MRI reconstruction. SMRD does not require ground truth data from the target distribution for tuning as it uses SURE for estimating the true MSE. SMRD surpassed baselines across different shifts including anatomy shift, measurement noise change, and acceleration rate change. SMRD could be helpful to improve the safety and robustness of diffusion models for MRI reconstruction used in clinical settings. While we applied SMRD to MRI reconstruction, future work could explore the application of SMRD to other inverse problems and diffusion sampling algorithms and can be used to tune their inference hyperparameters."
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,Fig. 1 .,
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,Fig. 2 .,
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,Fig. 3 .,
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,Fig. 4 .,
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,,"1: sample x0 ∼ N (0, I) 2: for t ∈ 0, ..., T -1 do 3:"
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,Table 1 .,
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,Table 2 .,
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,Table 3 .,
SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_20.
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,1,Introduction,"Cone Beam Computed Tomography (CBCT) is widely used in dental clinics, as it provides volumetric views of tooth structures for diagnosis, treatment, and surgery. Despite the extensive research on teeth segmentation from CBCT images [6,18], segmenting an individual tooth as a whole has limited applications, e.g., predicting tooth movement in orthodontics. Most dental treatments, including caries, prosthodontics and endodontics, focus on the internal structures of teeth. Thus, the task of segmenting and representing internal tooth structure is important, and can better assist dental diagnosis and treatment planning [4].In this paper, we propose an end-to-end framework for tooth segmentation including internal structures from CBCT which, to the best of our knowledge, is the first work to do so. As shown in Fig. 1(a), a tooth consists of enamel, dentin and pulp which we refer to the internal structure. Since there are a total of 32 tooth classes (including wisdom teeth), the model should be capable of identify 96 tooth classes from a CBCT voxel. Considering the size of CBCT data, the problem poses significant challenges from the perspective of not only segmentation performance, but also computational complexity.We take a hierarchical approach to tackle the challenges, and propose a 3stage process in order to accurately extract structures without compromising the original resolution of CBCT data. Each stage performs precise detection and segmentation for each level of hierarchy in the tooth structure. We propose a novel module called Dual-Hierarchy U-Net (DHU-Net) which is designed to extract and combine hierarchical features so as to effectively leverage hierarchy in the internal tooth structure. The segmentation performance of our model is evaluated for internal structures as well as the whole teeth. Experiments show that our method outperforms state-of-the-art (SOTA) baselines in both cases.Our contributions are summarized as follows: 1) a fully automated, end-toend model for internal tooth segmentation for the first time; 2) a novel 3-stage method with Dual-Hierarchy U-Net module leveraging the hierarchical structures of teeth; 3) the superiority of our model over SOTA baselines.Related Work. 3D tooth segmentation has been actively studied, including knowledge-based approaches, e.g., graph cut [10] and level set methods [7,8,25] which rely on intensity discrepancies between tooth and non-tooth regions. However, these methods can suffer at regions where teeth meet or where intensity values of roots are similar to jawbone. Internal tooth segmentation methods have been proposed, e.g., enamel-dentin segmentation based on watershed algorithm [13], or tooth pulp cavity segmentation [12,22], which however are sensitive to intensity thresholds, and do not simultaneously segment the entire structure. Recently, fully automated segmentation based on deep-learning has been actively explored. ToothNet [3] performs fully automated tooth segmentation using Mask R-CNN [9] which however had limitations, e.g., applicable only to down-sampled CBCT images. Coarse-to-fine segmentation was proposed [5,19], which initially down-sampled and subsequently the full-resolution CBCT images process. SGANet [16] used semantic graph attention based on Graph Convolutional Network [21] to learn and exploit the spatial association among teeth. Prior two-stage approaches [5,14,16,19] extract tooth patches in the 1st stage, and segment the tooth ROIs in the 2nd stage. However, such approaches not only are insufficient for internal segmentation, but also focus on segmenting the individual tooth as a whole, not internal structures."
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,2,Method,
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,2.1,Three-Stage Segmentation Process,"We propose a 3-stage process for the internal tooth segmentation from CBCT images, as shown in Fig. 2. Teeth are categorized into 32 classes of incisors, canines, premolars and molars. Each tooth consists of enamel, dentin and pulp. The union of enamel, dentin and pulp is called the whole tooth. Our method performs coarse to fine segmentation based on the following three levels of hierarchy of tooth structures: see Fig. 1(b). (1) a CBCT voxel is classified into tooth and non-tooth; (2) teeth is categorized into 32 classes; (3) a whole tooth is classified into enamel, dentin, and pulp.Each stage performs the task associated with each level of hierarchy. In Stage 1, a bounding box containing the set of teeth is extracted from CBCT. In Stage 2, 3D patches of individual tooth in 32 classes are extracted. In Stage 3, a tooth patch is segmented into enamel, dentin and pulp structures. We perform a binary segmentation of teeth (versus non-tooth) instead of a simple bounding box regression, considering the importance of extracting accurate bounding boxes. After segmentation, we find a tight bounding box around the teeth set which is then zero-padded for extra margins.We use 3D U-Net [2] for the segmentation of the CBCT image temporarily down-sampled to 128 × 128 × 128 for computational efficiency. Previous methods also proposed to isolate the tooth region, e.g., heuristic thresholding based on Maximum Intensity Projection of CBCT [1]. Our approach may demand more resources, but leads to improved performance, which we show by experiments. In addition, the decoder layers of C-Net utilize Hierarchical Feature Fusion (HFF) module for effective fusion of the features from P-Net and C-Net, as explained in the next section. DHU-Net is inspired by double U-Net [11], however differs from it in several ways: the supervision of P-Net and C-Net outputs with labels at high-and low-level hierarchies, the way input and output of P-Net are combined, and the existence of HFF module."
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,2.3,Hierarchical Feature Fusion (HFF) Module,"One of the properties that made U-Net successful is the combination of encoder and decoder features through skip connections. In the proposed Hierarchical Feature Fusion (HFF) module, the decoder layers at C-Net combines two encoder features from both hierarchies, i.e., P-Net and C-Net: see Fig. 3(a). HFF facilitates the propagation of hierarchical features over the network.As shown in Fig. 3(b), the concept of Channel Attention [23,24] is used in HFF. Attention vectors are created by mixing pooled features using MLPMixer [20]. The feature maps are scaled in a channel-wise manner by the attention vectors, and then fused after applying spatial attention. The overall process allows the model to effectively highlight important channel and spatial features from multiple hierarchies."
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,2.4,Loss Function,"The loss function of DHU-Net is given byL P and L C are binary cross-entropy (CE) loss for P-Net and CE + DICE loss for C-Net, respectively. λ 1 and λ 2 are hyperparameters for balancing losses.The λ 1 and λ 2 are hyperparameters for balancing losses which are set to 2 and 5, respectively. L FTM is Focal Tree-Min Loss [15], a hierarchical loss function encouraging the model to capture hierarchical relationships between the features extracted by P-Net and C-Net, e.g., the features of a whole tooth and its internal structures in Stage 3."
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,3,Experiment,
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,3.1,Dataset,"The dataset consisted of 70 anonymized cases of 3D dental CBCT images collected from the Korea University Anam Hospital. This study was approved by the Institutional Review Board of of the same hospital (IRB No. 2020AN0410). The dimension of CBCT images is 768 × 768 × 576 with the voxel size 0.3 × 0.3 × 0.3 mm 3 . We clipped the intensity values of CBCT images to [-1000, 2500] and applied intensity normalization. All the internal structures of teeth in CBCT images were individually labelled as enamel, dentin, and pulp. The labeling was performed by two experts and cross-checked, with a final inspection performed by a oral & maxillofacial surgeon. The dataset is split in 3:1:1 for train, validation and test with 5-fold nested cross-validation. We use the following metrics: Dice similarity coefficient (DSC), Jaccard index, and Hausdorff distance (HD95). We evaluate the accuracy of tooth identification (in 32 classes) during the patch extraction in Stage 2. We define the metric of detection precision (DP) as DP = |D ∩ G|/|D ∪ G|, where D represents the set of predicted tooth classes in Stage 2, and G represents the ground truth set. All the results are averaged over 10 repetitions of experiments."
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,3.2,Experimental Results,"We evaluated the performance of our model for internal tooth segmentation by comparing with two commonly used models in medical segmentation: U-Net [2] and Attention U-Net [17]. We consider the cases of two-and three-stage process for baselines. For two stages, baselines perform extraction of tooth patches from the CBCT image in the 1st stage, and internal tooth segmentation from the patch in the 2nd stage. The three-stage process is identical to our model, except that the segmentation networks are replaced by the baseline models.Table 1 shows the segmentation performance of internal tooth structures. Our method outperforms the baselines across all the metrics (comparison of Jaccard is provided in Supplementary Materials). By comparing 2-stage and 3-stage processes for baselines, we observe that the 3-stage process leads to the better performance. This shows the importance of reducing detection errors, i.e., accurate extraction of tooth patches in turn enhances the final segmentation performance. Indeed, 3-stage process improves the DP metric in all cases. In addition, by comparing with 3-stage baselines, we observe that DHU-Net outperforms U-Net and Attention U-Net. The results demonstrate the effectiveness of the hierarchical design of deep learning models for analyzing internal tooth structure. Ablation analysis on some components of DHU-Net, i.e., HFF module and hierarchical loss function, is provided in Supplementary Materials.We conducted a qualitative analysis of segmentation results as shown in Fig. 4. We found that the U-Net baseline had a problem of missed detection of teeth, while the 2-stage Attention U-Net showed a cut-out problem. Our model resulted in better representations of the root parts of dentin and pulp compared to the 3-stage baselines, perhaps because our model was better at dealing with the problem of similar intensity values of teeth and the jaw bone.Next, we evaluate the segmentation performance of the whole tooth, which also is an important problem. Our model provides the prediction of the whole tooth, i.e., we can simply take a union of the predicted enamel, dentin and pulp. We selected state-of-the-art methods for tooth segmentation as baselines: C2FSeg [5], MWTNet [1] and SGANet [16]. As shown in Table 2, our approach outperformed the baselines, and proved to be effective for segmenting the whole tooth as well.We observe that by comparing Table 1 and 2, the segmentation performance of whole tooth is higher than that of internal structures. This is reasonable, because the segmentation of finer structures tend to be harder. For example, suppose our model incorrectly classified an enamel voxel as dentin. This does not affect the accuracy of the whole tooth prediction, however, the accuracy of both enamel and dentin predictions will drop in the internal segmentation task."
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,4,Conclusion,"In this work, we proposed a fully automated segmentation of internal tooth structures, which, to the best of our knowledge, is the first attempt. We proposed a 3-stage process to reduce detection error and overcome difficulties in segmentation and computational complexity. We introduced DHU-Net, a segmentation network capable of effectively learning hierarchical features of tooth structures, demonstrating improved segmentation performance for both the whole tooth and internal structures. Our future work include the segmentation of additional structures from CBCT, such as mandible or maxilla, simultaneously with teeth."
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,,Fig. 1 .,
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,,Fig. 2 .,
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,,Fig. 3 .,
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,,Stage 2 :,
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,,,
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,,Table 1 .,
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,,Table 2 .,88.68 ± 1.43 80.59 ± 2.09 3.12 ± 1.45 MWTNet [1] 90.18 ± 1.04 82.62 ± 1.16 2.78 ± 1.38 SGANet [16] 92.16 ± 0.45 86.48 ± 0.78 2.24 ± 0.54 Ours 93.91 ± 0.34 88.67 ± 0.68 1.32 ± 0.30 Fig. 4. Qualitative Analysis of Internal Tooth Segmentation
Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_67.
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,1,Introduction,"Medical image segmentation is one of the most fundamental and challenging tasks in medical image analysis. It aims at classifying each pixel in the image into an anatomical category. With the success of deep neural networks (DNNs), medical image segmentation has achieved great progress in assisting radiologists in contributing to a better disease diagnosis.Until recently, the field of medical image segmentation has mainly been dominated by an encoder-decoder architecture, and the existing state-of-the-art (SOTA) medical segmentation models are roughly categorized into two groups:(1) convolutional neural networks (CNNs) [1,4,6,11,12,19,25,29,33,34,38], and (2) Transformers [2,5,35]. However, despite their recent success, several challenges persist to build a robust medical segmentation model: ❶ Classical deep learning methods require precise pixel/voxel-level labels to tackle this problem [30][31][32]36,37]. Acquiring a large-scale medical dataset with exact pixel-and voxel-level annotations is usually expensive and time-consuming as it requires extensive clinical expertise [10,13,14,16,20]. Prior works [7,15] have used pointlevel supervision on medical image segmentation to refine the boundary prediction, where such supervision requires well-trained model weights and can only capture discrete representations on the pixel-level grids. ❷ Empirically, it has been observed that CNNs inherently store the discrete signal values in a grid of pixels or voxels, which naturally blur the high-frequency anatomical regions, i.e., boundary regions. In contrast, implicit neural representations (INRs), also known as coordinate-based neural representations, are capable of representing discrete data as instances of a continuous manifold, and have shown remarkable promise in computer vision and graphics [22,27,28]. Several questions then arise: how many pixel-or voxel-level labels are needed to achieve good performance? how should those coordinate locations be selected? and how can the selected coordinates and signal values be leveraged efficiently?Orthogonally to the popular belief that the model architecture matters the most in medical segmentation (i.e., complex architectures generally perform better), this paper focuses on an under-explored and alternative direction: towards improving segmentation quality via rectifying uncertain coarse predictions. To this end, we propose a new INR-based framework, MORSE (iMplicit anatOmical Rendering with Stochastic Experts). The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. We think of building a generic implicit neural rendering framework to have finegrained control of segmentation quality, i.e., to adaptively compose coordinatewise point features and rectify uncertain anatomical regions. Specifically, we encode the sampled coordinate-wise point features into a continuous space, and then align position and features with respect to the continuous coordinate.We further hinge on the idea of mixture-of-experts (MoE) to improve segmentation quality. Considering our goal is to rectify uncertain coarse predictions, we regard multi-scale representations from the decoder as experts. During training, experts are randomly activated for features from multiple blocks of the decoder, and correspondingly the INRs of multi-scale representations are sepa- "
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,2,Method,"Let us assume a supervised medical segmentation dataset D = {(x, y)}, where each input x = x 1 , x 2 , ..., x T is a collection of T 2D/3D scans, and y refers to the ground-truth labels. Given an input scan x ∈ R H×W ×d , the goal of medical segmentation is to predict a segmentation map ŷ. Figure 1 illustrates the overview of our MORSE. In the following, we first describe our baseline model f for standard supervised learning, and subsequently present our MORSE. A baseline segmentation model consists of two main components: (1) encoder module, which generates the multi-scale feature maps such that the model is capable of modeling multi-scale local contexts, and (2) decoder module that makes a prediction ŷ using the generated multi-block features of different resolution. The entire model M is trained end-to-end using the supervised segmentation loss L sup [35] (i.e., equal combination of cross-entropy loss and dice loss)."
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,2.1,Stochastic Mixture-of-Experts (SMoE) Module,"Motivation. We want a module that encourages inter-and intra-associations across multi-block features. Intuitively, multi-block features should be specified by anatomical features across each block. We posit that due to the specializationfavored nature of MoE, the model will benefit from explicit use of its own anatomical features at each block by learning multi-scale anatomical contexts with adaptively selected experts. In implementation, our SMoE module follows an MoE design [21], where it treats features from multiple blocks of the decoder as experts. To mitigate potential overfitting and enable parameter-efficient property, we further randomly activate experts for each input during training. Our approach makes three major departures compared to [21] (i.e., SOTA segmention model): (1) implicitly optimized during training since it greatly trims down the training cost and the model scale; (2) using features from the decoder instead of the encoder tailored for our refinement goal; and (3) empirically showing that ""self-slimmable"" attribute delivers sufficiently exploited expressiveness of the model."
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,Modulization.,"We first use multiple small MLPs with the same size to process different block features and then up-sample the features to the size of the input scans, i.e., H × W × d. With N as the total number of layers (experts) in the decoder, we treat these upsampled features [F 1 , F 2 , ..., F N ] as expert features. We then train a gating network G to re-weight the features from activated experts with the trainable weight matrices [W 1 , W 2 , ..., W N ], where W ∈ R H×W ×d . Specifically, the gating network or router G outputs these weight matrices satisfying i W i = 1 H×W ×d using a structure depicted as follows:The gating network first concatenates all the expert features along channels and uses several convolutional layers to getwhere C is the channel dimension. A softmax layer is applied over the last dimension (i.e., N -expert) to output the final weight maps. After that, we feed the resultant output x out to another MLP to fuse multi-block expert features. Finally, the resultant output x out (i.e. the coarse feature) is given as follows:where • denotes the pixel-wise multiplication, andStochastic Routing. The prior MoE-based model [21] are densely activated.That is, a model needs to access all its parameters to process all inputs. One drawback of such design often comes at the prohibitive training cost. Moreover, the large model size suffers from the representation collapse issue [26], further limiting the model's performance. Our proposed SMoE considers randomly activated expert sub-networks to address the issues. In implementation, we simply apply standard dropout to multiple experts with a dropping probability α. For each training iteration, there are dropout masks placed on experts with the probability α. That is, the omission of experts follows a Bernoulli(α) distribution. As for inference, there is no dropout mask and all experts are activated."
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,2.2,Implicit Anatomical Rendering (IAR),"The existing methods generally assume that the semantically correlated information and fine anatomical details have been captured and can be used to obtain high-quality segmentation quality. However, CNNs inherently operate the discrete signals in a grid of pixels or voxels, which naturally blur the high-frequency anatomical regions, i.e., boundary regions. To address such issues, INRs in computer graphics are often used to replace standard discrete representations with continuous functions parameterized by MLPs [27,28]. Our key motivation is that the task of medical segmentation is often framed as a rendering problem that applies implicit neural functions to continuous shape/object/scene representations [22,27]. Inspired by this, we propose an implicit neural rendering framework to further improve segmentation quality, i.e., to adaptively compose coordinatewise point features and rectify uncertain anatomical regions.Point Selection. Given a coarse segmentation map, the rendering head aims at rectifying the uncertain boundary regions. A point selection mechanism is thus required to filter out those pixels where the rendering can achieve maximum segmentation quality improvement. Besides, point selection can significantly reduce computational cost compared to blindly rendering all boundary pixels. Therefore, our MORSE selects N p points for refinement given the coarse segmentation map using an uncertainty-based criterion. Specifically, MORSE first uniformly randomly samples k p N p candidates from all pixels where the hyper-parameter k p ≥ 1, following [9]. Then, based on the coarse segmentation map, MORSE chooses ρN p pixels with the highest uncertainty from these candidates, where 0.5 < ρ < 1. The uncertainty for a pixel is defined as SecondLargest(v)-max(v), where v is the logit vector of that pixel such that the coarse segmentation is given as Softmax(v). The rest (1ρ)N p pixels are sampled uniformly from all the remaining pixels. This mechanism ensures the selected points contain a large portion of points with uncertain segmentation which require refinement.Positional Encoding. It is well-known that neural networks can be cast as universal function approximators, but they are inferior to high-frequency signals due to their limited learning power [18,23]. Unlike [9], we explore using the encoded positional information to capture high-frequency signals, which echoes our theoretical findings in Appendix A. Specifically, for a coordinate-based point the positional encoding function is given as:where x = 2x/H -1 and ỹ = 2y/W -1 are the standardized coordinates with values in between [-1, 1]. The frequency {w i , v i } L i=1 are trainable parameters with Gaussian random initialization, where we set L = 128 [3]. For each selected point, its position encoding will then be concatenated with the coarse features of that point (i.e., x out defined in Sect. 2.1), to output the fine-grained features.Rendering Head. The fine-grained features are then fed to the rendering head whose goal is to rectify the uncertain predictions with respect to these selected points. Inspired by [9], the rendering head adopts 3-layer MLPs design. Since the rendering head is designed to rectify the class label of the selected points, it is trained using the standard cross-entropy loss L rend .Adaptive Weight Adjustment. Instead of directly leveraging pre-trained weights, it is more desirable to train the model from scratch in an end-to-end way. For instance, we empirically observe that directly using coarse masks by pretrained weights to modify unclear anatomical regions might lead to suboptimal results (See Sect. 3.1). Thus, we propose to modify the importance of L rend as:where t is the index of the iteration, T denotes the total number of iterations, and 1{•} denotes the indicator function. Training Objective. As such, the model is trained in an end-to-end manner using total loss L total = L sup + λ t × L rend ."
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,3,Experiments,"Dataset. We evaluate the models on two important medical segmentation tasks. (2) Liver segmentation: Multi-phasic MRI (MP-MRI) dataset is an in-house dataset including 20 patients, each including T1 weighted DCE-MRI images at three-time phases (i.e., pre-contrast, arterial, and venous). Here, our evaluation is conducted via 5-fold cross-validation on the 60 scans. For each fold, the training and testing data includes 48 and 12 cases, respectively.Implementation Details. We use AdamW optimizer [17] with an initial learning rate 5e -4 , and adopt a polynomial-decay learning rate schedule for both datasets. We train each model for 30K iterations. For Synapse, we adopt the input resolution as 256 × 256 and the batch size is 4. For MP-MRI, we randomly crop 96 × 96 × 96 patches and the batch size is 2. For SMoE, following [21], all the MLPs have hidden dimensions [256,256] with ReLU activations, the dimension of expert features [F 1 , F 2 , ..., F N ] are 256. We empirically set α as 0.7. Following [9], N p is set as 2048, and 8192 for training and testing, respectively, and k p , ρ are 3, 0.75. We follow the same gating network design [21], which includes four 3 × 3 convolutional layers with channels [256, 256, 256, N] and ReLU activations.λ rend are set to 0.1. We adopt four representative models, including UNet [25], TransUnet [2], 3D-UNet [4], UNETR [5]. Specifically, we set N for UNet [25], TransUnet [2], 3D-UNet [4], UNETR [5] with 5, 3, 3, 3, respectively. We also use Dice coefficient (DSC), Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD) to evaluate 3D results. We conduct all experiments in the same environments with fixed random seeds (Hardware: Single NVIDIA RTX A6000 GPU; Software: PyTorch 1.12.1+cu116, and Python 3.9.7)."
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,3.1,Comparison with State-of-the-Art Methods,"We adopt classical CNN-and transformer-based models, i.e., 2D-based {UNet [25], TransUnet [2]} and 3D-based {3D-UNet [4], UNETR [5]}, and train them on {2D Synapse, 3D MP-MRI} in an end-to-end manner 3 .Main Results. The results for 2D synapse multi-organ segmentation and 3D liver segmentation are shown in Tables 1 and2, respectively. Visualization of IAR Modules. To better understand the IAR module, we visualize the point features on the coarse prediction and refined prediction after the IAR module in Appendix Fig. 4. As is shown, we can see that IAR help rectify the uncertain anatomical regions for improving segmentation quality (Table 4). "
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,3.2,Ablation Study,"We first investigate our MORSE equipped with UNet by varying α (i.e., stochastic rate) and N (i.e., experts) on Synapse. The comparison results of α and N are reported in Table 3. We find that using α = 0.7 performs the best when the expert capacity is N = 5. Similarly, when reducing the expert number, the performance also drops considerably. This shows our hyperparameter settings are optimal. Moreover, we conduct experiments to study the importance of Adaptive Weight Adjustment (AWA). We see that: (1) Disabling AWA and training L rend from scratch causes unsatisfied performance, as echoed in [9]. (2) Introducing AWA shows a consistent advantage compared to the other. This demonstrates the importance of the Adaptive Weight Adjustment."
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,4,Conclusion,"In this paper, we proposed MORSE, a new implicit neural rendering framework that has fine-grained control of segmentation quality by adaptively composing coordinate-wise point features and rectifying uncertain anatomical regions. We also demonstrate the advantage of leveraging mixture-of-experts that enables the model with better specialization of features maps for improving the performance. Extensive empirical studies across various network backbones and datasets, consistently show the effectiveness of the proposed MORSE. Theoretical analysis further uncovers the expressiveness of our INR-based model."
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,Fig. 1 .,
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,( 1 ),
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,Table 1 .,
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,Table 2 .,
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,,
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,Table 3 .,
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,Table 4 .,
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 54.
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,1,Introduction,"Axillary lymph node (ALN) metastasis is a severe complication of cancer that can have devastating consequences, including significant morbidity and mortality. Early detection and timely treatment are crucial for improving outcomes and reducing the risk of recurrence. In breast cancer diagnosis, accurately segmenting breast lesions in ultrasound (US) videos is an essential step for computer-aided diagnosis systems, as well as breast cancer diagnosis and treatment. However, this task is challenging due to several factors, including blurry lesion boundaries, inhomogeneous distributions, diverse motion patterns, and dynamic changes in lesion sizes over time [12]. The work presented in [10] proposed the first pixel-wise annotated benchmark dataset for breast lesion segmentation in US videos, but it has some limitations. Although their efforts were commendable, this dataset is private and contains only 63 videos with 4,619 annotated frames. The small dataset size increases the risk of overfitting and limits the generalizability capability. In this work, we collected a larger-scale US video breast lesion segmentation dataset with 572 videos and 34,300 annotated frames, of which 222 videos contain ALN metastasis, covering a wide range of realistic clinical scenarios. Please refer to Table 1 for a detailed comparison between our dataset and existing datasets.Although the existing benchmark method DPSTT [10] has shown promising results for breast lesion segmentation in US videos, it only uses the ultrasound image to read memory for learning temporal features. However, ultrasound images suffer from speckle noise, weak boundaries, and low image quality. Thus, there is still considerable room for improvement in ultrasound video breast lesion segmentation. To address this, we propose a novel network called Frequency and Localization Feature Aggregation Network (FLA-Net) to improve breast lesion segmentation in ultrasound videos. Our FLA-Net learns frequency-based temporal features and then uses them to predict auxiliary breast lesion location maps to assist the segmentation of breast lesions in video frames. Additionally, we devise a contrastive loss to enhance the breast lesion location similarity of video frames within the same ultrasound video and to prohibit location similarity of different ultrasound videos. The experimental results unequivocally showcase that our network surpasses state-of-the-art techniques in the realm of both breast lesion segmentation in US videos and two video polyp segmentation benchmark datasets (Fig. 1)."
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,2,Ultrasound Video Breast Lesion Segmentation Dataset,"To support advancements in breast lesion segmentation and ALN metastasis prediction, we collected a dataset containing 572 breast lesion ultrasound videos with 34,300 annotated frames. Table 1 summarizes the statistics of existing breast lesion US video datasets. Among 572 videos, 222 videos with ALN metastasis. Nine experienced pathologists were invited to manually annotate breast lesions at each video frame. Unlike previous datasets [10,12], our dataset has a reserved validation set to avoid model overfitting. The entire dataset is partitioned into training, validation, and test sets in a proportion of 4:2:4, yielding a total of 230 training videos, 112 validation videos, and 230 test videos for comprehensive benchmarking purposes. Moreover, apart from the segmentation annotation, our dataset also includes lesion bounding box labels, which enables benchmarking breast lesion detection in ultrasound videos. More dataset statistics are available in the Supplementary."
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3,Proposed Method,"Figure 2 provides a detailed illustration of the proposed frequency and localization feature aggregation network (FLA-Net). When presented with an ultrasound frame denoted as I t along with its two adjacent video frames (I t-1 and I t-2 ), our initial step involves feeding them through an Encoder, specifically the Res2Net50 architecture [6], to acquire three distinct features labeled as f t , f t-1 , and f t-2 . Then, we devise a frequency-based feature aggregation (FFA) module to integrate frequency features of each video frame. After that, we pass the output features o t of the FFA module into two decoder branches (similar to the UNet decoder [14]): one is the localization branch to predict the localization map of the breast lesions, while another segmentation branch integrates the features of the localization branch to fuse localization feature for segmenting breast lesions. Moreover, we devise a location-based contrastive loss to regularize the breast lesion locations of inter-video frames and intra-video frames."
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3.1,Frequency-Based Feature Aggregation (FFA) Module,"According to the spectral convolution theorem in Fourier theory, any modification made to a single value in the spectral domain has a global impact on all the original input features [1]. This theorem guides the design of FFA module, which has a global receptive field to refine features in the spectral domain. As shown in Fig. 2, our FFA block takes three features (f t ∈ R c×h×w , f t-1 ∈ R c×h×w , and f t-2 ∈ R c×h×w ) as input. To integrate the three input features and extract relevant information while suppressing irrelevant information, our FFA block first employs a Fast Fourier Transform (FFT) to transform the three input features into the spectral domain, resulting in three corresponding spectral domain features ( ft ∈ C c×h×w , ft-1 ∈ C c×h×w , and ft-2 ∈ C c×h×w ), which capture the frequency information of the input features. Note that the current spectral features ( ft , ft-1 , and ft-2 ) are complex numbers and incompatible with the neural layers. Therefore we concatenate the real and imaginary parts of these complex numbers along the channel dimension respectively and thus obtain three new tensors (x t ∈ R 2c×h×w , x t-1 ∈ R 2c×h×w , and x t-2 ∈ R 2c×h×w ) with double channels. Afterward, we take the current frame spectral-domain features x t as the core and fuse the spatial-temporal information from the two auxiliary spectral-domain features (x t-1 and x t-2 ), respectively. Specifically, we first group three features into two groups ({x t , x t-1 } and {x t , x t-2 }) and develop a channel attention function CA(•) to obtain two attention maps. The CA(•) passes an input feature map to a feature normalization, two 1×1 convolution layers Conv(•), a ReLU activation function δ(•), and a sigmoid function σ(•) to compute an attention map. Then, we element-wise multiply the obtained attention map from each group with the input features, and the multiplication results (see y 1 and y 2 ) are then transformed into complex numbers by splitting them into real and imaginary parts along the channel dimension. After that, inverse FFT (iFFT) operation is employed to transfer the spectral features back to the spatial domain, and then two obtained features at the spatial domain are denoted as z 1 and z 2 . Finally, we further element-wisely add z 1 and z 2 and then pass it into a ""BConv"" layer to obtain the output feature o t of our FFA module. Mathematically, o t is computed by o t = BConv(z 1 + z 2 ), where ""BConv"" contains a 3 × 3 convolution layer, a group normalization, and a ReLU activation function."
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3.2,Two-Branch Decoder,"After obtaining the frequency features, we introduce a two-branch decoder consisting of a segmentation branch and a localization branch to incorporate temporal features from nearby frames into the current frame. Each branch is built based on the UNet decoder [14] with four convolutional layers. Let d 1 s and d 2 s denote the features at the last two layers of the segmentation decoder branch, and d 1 l and d 2 l denote the features at the last two layers of the localization decoder branch. Then, we pass d 1 l at the localization decoder branch to predict a breast lesion localization map. Then, we element-wisely add d 1 l and d 1 s , and elementwisely add d 2 l and d 2 s , and pass the addition result into a ""BConv"" convolution layer to predict the segmentation map S t of the input video frame I t .Location Ground Truth. Instead of formulating it as a regression problem, we adopt a likelihood heatmap-based approach to encode the location of breast lesions, since it is more robust to occlusion and motion blur. To do so, we compute a bounding box of the annotated breast lesion segmentation result, and then take the center coordinates of the bounding box. After that, we apply a Gaussian kernel with a standard deviation of 5 on the center coordinates to generate a heatmap, which is taken as the ground truth of the breast lesion localization."
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3.3,Location-Based Contrastive Loss,"Note that the breast lesion locations of neighboring ultrasound video frames are close, while the breast lesion location distance is large for different ultrasound videos, which are often obtained from different patients. Motivated by this, we further devise a location-based contrastive loss to make the breast lesion locations at the same video to be close, while pushing the lesion locations of frames from different videos away. By doing so, we can enhance the breast lesion location prediction in the localization branch. Hence, we devise a location-based contrastive loss based on a triplet loss [15], and the definition is given by:where α is a margin that is enforced between positive and negative pairs. H t and H t-1 are predicted heatmaps of neighboring frames from the same video. N t denotes the heatmap of the breast lesion from a frame from another ultrasound video. Hence, the total loss L total of our network is computed by:where G H t and G S t denote the ground truth of the breast lesion segmentation and the breast lesion localization. We empirically set weights"
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,4,Experiments and Results,"Implementation Details. To initialize the backbone of our network, we pretrained Res2Net-50 [6] on the ImageNet dataset, while the remaining components of our network were trained from scratch. Prior to inputting the training video frames into the network, we resize them to 352 × 352 dimensions. Our network is implemented in PyTorch and employs the Adam optimizer with a learning rate of 5 × 10 -5 , trained over 100 epochs, and a batch size of 24. Training is conducted on four GeForce RTX 2080 Ti GPUs. For quantitative comparisons, we utilize various metrics, including the Dice similarity coefficient (Dice), Jaccard similarity coefficient (Jaccard), F1-score, and mean absolute error (MAE).  "
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,4.1,Comparisons with State-of-the-Arts,"We conduct a comparative analysis between our network and nine state-of-theart methods, comprising four image-based methods and five video-based methods. Four image-based methods are UNet [14], UNet++ [19], TransUNet [4], and SETR [18], while five video-based methods are STM [13], AFB-URR [11], PNS+ [9], DPSTT [10], and DCFNet [16]. To ensure a fair and equitable comparison, we acquire the segmentation results of all nine compared methods by utilizing either their publicly available implementations or by implementing them ourselves. Additionally, we retrain these networks on our dataset and fine-tune their network parameters to attain their optimal segmentation performance, enabling accurate and meaningful comparisons.Quantitative Comparisons. The quantitative results of our network and the nine compared breast lesion segmentation methods are summarized in Table 2. Analysis of the results reveals that, in terms of quantitative metrics, video-based methods generally outperform image-based methods. Among nine compared methods, DCFNet [16] achieves the largest Dice, Jaccard, and F1-score results, while PNS+ [9] and DPSTT [10] have the smallest MAE score. More importantly, our FLA-Net further outperforms DCFNet [16] in terms of Dice, Jaccard, and F1-score metrics, and has a superior MAE performance over PNS+ [9] and DPSTT [10]. Specifically, our FLA-Net improves the Dice score from 0.762 to 0.789, the Jaccard score from 0.659 to 0.687, the F1-score result from 0.799 to 0.815, and the MAE score from 0.036 to 0.033. Metrics UNet [14] UNet++ [19] ResUNet [7] ACSNet [17] PraNet [5] PNSNet [8] Ours Qualitative Comparisons. Figure 3 visually presents a comparison of breast lesion segmentation results obtained from our network and three other methods across various input video frames. Apparently, our method accurately segments breast lesions of the input ultrasound video frames, although these target breast lesions have varied sizes and diverse shapes in the input video frames."
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,4.2,Ablation Study,"To evaluate the effectiveness of the major components in our network, we constructed three baseline networks. The first one (denoted as ""Basic"") removed the localization encoder branch and replaced our FLA modules with a simple feature concatenation and a 1 × 1 convolutional layer. The second and third baseline networks (named ""Basic+FLA"" and ""Basic+LB"") incorporate the FLA module and the localization branch into the basic network, respectively. Table 3 reports the quantitative results of our method and three baseline networks. The superior metric performance of ""Basic+FLA"" and ""Basic+LB"" compared to ""Basic"" clearly indicates that our FLA module and the localization encoder branch effectively enhance the breast lesion segmentation performance in ultrasound videos. Then, the superior performance of ""Basic+FLA+LB"" over ""Basic+FLA"" and ""Basic+LB"" demonstrate that combining our FLA module and the localization encoder branch can incur a more accurate segmentation result. Moreover, our method has larger Dice, Jaccard, F1-score results and a smaller MAE result than ""Basic+FLA+LB"", which shows that our location-based contrastive loss has its contribution to the success of our video breast lesion segmentation method."
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,4.3,Generalizability of Our Network,"To further evaluate the effectiveness of our FLA-Net, we extend its application to the task of video polyp segmentation. Following the experimental protocol employed in a recent study on video polyp segmentation [8], we retrain our network and present quantitative results on two benchmark datasets, namely CVC-300-TV [2] and CVC-612-V [3]. Table 4 showcases the Dice, IoU, S α , E φ , and MAE results achieved by our network in comparison to state-of-the-art methods on these two datasets. Our method demonstrates clear superiority over state-ofthe-art methods in terms of Dice, IoU, E φ , and MAE on both the CVC-300-TV and CVC-612-V datasets. Specifically, our method enhances the Dice score from 0.840 to 0.874, the IoU score from 0.745 to 0.789, the E φ score from 0.921 to 0.969, and reduces the MAE score from 0.013 to 0.010 for the CVC-300-TV dataset. Similarly, for the CVC-612-V dataset, our method achieves improvements of 0.012, 0.014, 0.019, and 0 in Dice, IoU, E φ , and MAE scores, respectively. Although our S α results (0.907 on CVC-300-TV and 0.920 on CVC-612-V) take the 2nd rank, they are very close to the best S α results, which are 0.909 on CVC-300-TV and 0.923 on CVC-612-V. Hence, the superior metric results obtained by our network clearly demonstrate its ability to accurately segment polyp regions more effectively than state-of-the-art video polyp segmentation methods."
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,5,Conclusion,"In this study, we introduce a novel approach for segmenting breast lesions in ultrasound videos, leveraging a larger dataset consisting of 572 videos containing a total of 34,300 annotated frames. We introduce a frequency and location feature aggregation network that incorporates frequency-based temporal feature learning, an auxiliary prediction of breast lesion location, and a location-based contrastive loss. Our proposed method surpasses existing state-of-the-art techniques in terms of performance on our annotated dataset as well as two publicly available video polyp segmentation datasets. These outcomes serve as compelling evidence for the effectiveness of our approach in achieving accurate breast lesion segmentation in ultrasound videos."
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,,Fig. 1 .,
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,,Fig. 2 .,
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,,Fig. 3 .,
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,,Table 1 .,
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,,Table 2 .,
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,,Table 3 .,
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,,Table 4 .,
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,1,Introduction,"Semantic segmentation, an essential component of computer-aided medical image analysis, identifies and highlights regions of interest in various diagnosis tasks. However, this often becomes complicated due to various factors involving image modality and acquisition along with pathological and biological variations [18]. The application of deep learning in this domain has thus certainly benefited in this regard. Most notably, ever since its introduction, the UNet model [19] has demonstrated astounding efficacy in medical image segmentation. As a result, UNet and its derivatives have become the de-facto standard [25].The original UNet model comprises a symmetric encoder-decoder architecture (Fig. 1a) and employs skip-connections, which provide the decoder spatial information probably lost during the pooling operations in the encoder. Although this information propagation through simple concatenation improves the performance, there exists a likely semantic gap between the encoder-decoder feature maps. This led to the development of a second class of UNets (Fig. 1b). U-Net++ [26] leveraged dense connections and MultiResUNet [11] added additional convolutional blocks along the skip connection as a potential remedy.Till this point in the history of UNet, all the innovations were performed using CNNs. However, the decade of 2020 brought radical changes in the computer vision landscape. The long-standing dominance of CNNs in vision was disrupted by vision transformers [7]. Swin Transformers [15] further adapted transformers for general vision applications. Thus, UNet models started adopting transformers [5]. Swin-Unet [9] replaced the convolutional blocks with Swin Transformer blocks and thus initiated a new class of models (Fig. 1c). Nevertheless, CNNs still having various merits in image segmentation, led to the development of fusing those two [2]. This hybrid class of UNet models (Fig. 1d) employs convolutional blocks in the encoder-decoder and uses transformer layers along the skip connections. UCTransNet [22] and MCTrans [24] are two representative models of this class. Finally, there have also been attempts to develop all-transformer UNet architectures (Fig. 1e), for instance, SMESwin Unet [27] uses transformer both in encoder-decoder blocks and the skip-connection.Very recently, studies have begun rediscovering the potential of CNNs in light of the advancements brought by transformers. The pioneering work in this regard is 'A ConvNet for the 20202020ss' [16], which explores the various ideas introduced by transformers and their applicability in convolutional networks. By gradually incorporating ideas from training protocol and micro-macro design choices, this work enabled ResNet models to outperform Swin Transformer models.In this paper, we ask the same question but in the context of UNet models. We investigate if a UNet model solely based on convolution can compete with the transformer-based UNets. In doing so, we derive motivations from the transformer architecture and develop a purely convolutional UNet model. We propose a patch-based context aggregation contrary to window-based self-attention. In addition, we innovate the skip connections by fusing the feature maps from multiple levels of encoders. Extensive experiments on 5 benchmark datasets suggest that our proposed modifications have the potential to improve UNet models."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,2,Method,"Firstly, we analyze the transformer-based UNet models from a high-level. Deriving motivation and insight from this, we design two convolutional blocks to simulate the operations performed in transformers. Finally, we integrate them in a vanilla UNet backbone and develop our proposed ACC-UNet architecture. Leveraging the Long-Range Dependency of Self-attention. Transformers can compute features from a much larger view of context through the use of (windowed) self-attention. In addition, they improve expressivity by adopting inverted bottlenecks, i.e., increasing the neurons in the MLP layer. Furthermore, they contain shortcut connections, which facilitate the learning [7].Adaptive Multi-level Feature Combination Through Channel Attention. Transformer-based UNets fuse the feature maps from multiple encoder levels adaptively using channel attention. This generates enriched features due to the combination of various regions of interest from different levels compared to simple skip-connection which is limited by the information at the current level [22].Based on these observations, we modify the convolutional blocks and skipconnections in a vanilla UNet model to induce the capabilities of long-range dependency and multi-level feature combinations."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,2.2,Hierarchical Aggregation of Neighborhood Context (HANC),"We first explore the possibility of inducing long-range dependency along with improving expressivity in convolutional blocks. We only use pointwise and depthwise convolutions to reduce the computational complexity [8].In order to increase the expressive capability, we propose to include inverted bottlenecks in convolutional blocks [16], which can be achieve by increasing the number of channels from c in to c inv = c in * inv_fctr using pointwise convolution. Since these additional channels will increase the model complexity, we use 3 × 3 depthwise convolution to compensate. An input feature map x in ∈ R cin,n,m is thus transformed to x 1 ∈ R cinv,n,m as (Fig. 2b)Next, we wish to emulate self-attention in our convolution block, which at its core is comparing a pixel with the other pixels in its neighborhood [15]. This comparison can be simplified by comparing a pixel value with the mean and maximum of its neighborhood. Therefore, we can provide an approximate notion of neighborhood comparison by appending the mean and max of the neighboring pixel features. Consecutive pointwise convolution can thus consider these and capture a contrasting view. Since hierarchical analysis is beneficial for images [23], instead of computing this aggregation in a single large window, we compute this in multiple levels hierarchically, for example,patches. For k = 1, it would be the ordinary convolution operation, but as we increase the value of k, more contextual information will be provided, bypassing the need for larger convolutional kernels. Thus, our proposed hierarchical neighborhood context aggregation enriches feature map x 1 ∈ R cinv,n,m with contextual information as x 2 ∈ R cinv * (2k-1),n,m (Fig. 2b), where || corresponds to concatenation along the channel dimensionNext, similar to the transformer, we include a shortcut connection in the convolution block for better gradient propagation. Hence, we perform another pointwise convolution to reduce the number of channels to c in and add with the input feature map. Thus, x 2 ∈ R cinv * (2k-1),n,m becomes x 3 ∈ R cin,n,m (Fig. 2b)Finally, we change the number of filters to c out , as the output, using pointwise convolution (Fig. 2b)Thus, we propose a novel Hierarchical Aggregation of Neighborhood Context (HANC) block using convolution but bringing the benefits of transformers. The operation of this block is illustrated in Fig. 2b."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,2.3,Multi Level Feature Compilation (MLFC),"Next, we investigate the feasibility of multi-level feature combination, which is the other advantage of using transformer-based UNets.Transformer-based skip connections have demonstrated effective feature fusion of all the encoder levels and appropriate filtering from the compiled feature maps by the individual decoders [22,24,27]. This is performed through concatenating the projected tokens from different levels [22]. Following this approach, we resize the convolutional feature maps obtained from the different encoder levels to make them equisized and concatenate them. This provides us with an overview of the feature maps across the different semantic levels. We apply pointwise convolution operation to summarize this representation and merge with the corresponding encoder feature map. This fusion of the overall and individual information is passed through another convolution, which we hypothesize enriches the current level feature with information from other level features.For the features, x 1 , x 2 , x 3 , x 4 from 4 different levels, the feature maps can be enriched with multilevel information as (Fig. 2d)Here, resize i (x j ) is an operation that resizes x j to the size of x i and c tot = c 1 + c 2 + c 3 + c 4 . This operation is done individually for all the different levels.We thus propose another novel block named Multi Level Feature Compilation (MLFC), which aggregates information from multiple encoder levels and enriches the individual encoder feature maps. This block is illustrated in Fig. 2d."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,2.4,ACC-UNet,"Therefore, we propose fully convolutional ACC-UNet (Fig. 2a). We started with a vanilla UNet model and reduced the number of filters by half. Then, we replaced the convolutional blocks from the encoder and decoder with our proposed HANC blocks. We considered inv_fctr = 3, other than the last decoder block at level 3 (inv_fctr = 34) to mimic the expansion at stage 3 of Swin Transformer. k = 3, which considers up to 4 × 4 patches, was selected for all but the bottleneck level (k = 1) and the one next to it (k = 2). Next, we modified the skip connections by using residual blocks (Fig. 2c) to reduce semantic gap [11] and stacked 3 MLFC blocks. All the convolutional layers were batch-normalized [12], activated by Leaky-RELU [17] and recalibrated by squeeze and excitation [10].To summarize, in a UNet model, we replaced the classical convolutional blocks with our proposed HANC blocks that perform an approximate version of self-attention and modified the skip connection with MLFC blocks which consider the feature maps from different encoder levels. The proposed model has 16.77 M parameters, roughly a 2M increase than the vanilla UNet model."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3,Experiments,
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.1,Datasets,"In order to evaluate ACC-UNet, we conducted experiments on 5 public datasets across different tasks and modalities. We used ISIC-2018 [6,21] (dermoscopy, 2594 images), BUSI [3](breast ultrasound, used 437 benign and 210 malignant images similar to [13]), CVC-ClinicDB [4] (colonoscopy, 612 images), COVID [1] (pneumonia lesion segmentation, 100 images), and GlaS [20] (gland segmentation, 85 training, and 80 test images). All the images and masks were resized to 224 × 224. For the GlaS dataset, we considered the original test split as the test data, for the other datasets we randomly selected 20% of images as test data. The remaining 60% and 20% images were used for training and validation and the experiments were repeated 3 times with different random shuffling."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.2,Implementation Details,"We implemented ACC-UNet model in PyTorch and used a workstation equipped with AMD EPYC 7443P 24-Core CPU and NVIDIA RTX A6000 (48G) GPU for our experiments. We designed our training protocol identical to previous works [22], except for using a batch size of 12 throughout our experiments [27]. The models were trained for 1000 epochs [27] and we employed an early stopping patience of 100 epochs. We minimized the combined cross-entropy and dice loss [22] using the Adam [14] optimizer with an initial learning rate of 10 -3 , which was adjusted through cosine annealing learning rate scheduler [13] 1 . We performed online data augmentations in the form of random flipping and rotating [22]."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.3,Comparisons with State-of-the-Art Methods,"We evaluated ACC-UNet against UNet, MultiResUNet, Swin-Unet, UCTransnet, SMESwin-Unet, i.e., one representative model from the 5 classes of UNet, respectively (Fig. 1). Table 1 presents the dice score obtained on the test sets. The results show an interesting pattern. Apparently, for the comparatively larger datasets (ISIC-18) transformer-based Swin-Unet was the 2nd best method, as transformers require more data for proper training [2]. On the other end of the spectrum, lightweight convolutional model (MultiResUNet) achieved the 2nd best score for small datasets (GlaS). For the remaining datasets, hybrid model (UCTransnet) seemed to perform as the 2 nd best method. SMESwin-Unet fell behind in all the cases, despite having such a large number of parameters, which in turn probably makes it difficult to be trained on small-scale datasets.However, our model combining the design principles of transformers with the inductive bias of CNNs seemed to perform best in all the different categories with much lower parameters. Compared to much larger state-of-the-art models, for the 5 datasets, we achieved 0.13%, 0.10%, 0.63%, 0.90%, 0.27% improvements in dice score, respectively. Thus, our model is not only accurate, but it is also efficient in using the moderately small parameters it possesses. In terms of FLOPs, our model is comparable with convolutional UNets, the transformer-based UNets have smaller FLOPs due to the massive downsampling at patch partitioning."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.4,Comparative Qualitative Results on the Five Datasets,"In addition to, achieving higher dice scores, apparently, ACC-UNet generated better qualitative results. Figure 3 presents a qualitative comparison of ACC-UNet with the other models. Each row of the figure comprises one example from each of the datasets and the segmentation predicted by ACC-UNet and the ground truth mask are presented in the rightmost two columns. For the 1 st example from the ISIC-18 dataset, our model did not oversegment but rather followed the lesion boundary. In the 2 nd example from CVC-ClinicDB, our model managed to distinguish the finger from the polyp almost perfectly. Next in the 3 rd example from BUSI, our prediction filtered out the apparent nodule region on the left, which was predicted as a false positive tumor by all the other models. Similarly, in the 4 th sample from the COVID dataset, we were capable to model the gaps in the consolidation of the left lung visually better, which in turn resulted in 2.9% higher dice score than the 2 nd best method. Again, in the final example from the GlaS dataset, we not only successfully predicted the gland at the bottom right corner but also identified the glands at the top left individually, which were mostly missed or merged by the other models, respectively."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.5,Ablation Study,"We performed an ablation study on the CVC-ClinicDB dataset to analyze the contributions of the different design choices in our roadmap (Fig. 4). We started with a UNet model with the number of filters halved as our base model, which results in a dice score of 87.77% with 7.8M parameters. Using depthwise convolutional along with increasing the bottleneck by 4 raised the dice score to 88.26% while slightly reducing the parameters to 7.5M . Next, HANC block was added with k = 3 throughout, which increased the number of parameters by 340% for an increase of 1.1% dice score. Shortcut connections increased the performance by 2.16%. We also slowly reduced both k and inv_fctr which reduced the number of parameters without any fall in performance. Finally, we added the MLFC blocks (4 stacks) and gradually optimized k and inv_fctr along with dropping one MLFC stage, which led to the development of ACC-UNet. Some other inter- esting ablations were ACC-UNet without MLFC (dice 91.9%) or HANC (dice 90.96%, with 25% more filters to keep the number of parameters comparable)."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,4,Conclusions,"Acknowledging the benefits of various design paradigms in transformers, we investigate the suitability of similar ideas in convolutional UNets. The resultant ACC-UNet possesses the inductive bias of CNNs infused with long-range and multi-level feature accumulation of transformers. Our experiments reveals this amalgamation indeed has the potential to improve UNet models. One limitation of our model is the slowdown from concat operations (please see supplementary materials), which can be solved by replacing them. In addition, there are more innovations brought by transformers [16], e.g., layer normalization, GELU activation, AdamW optimizer, these will be explored further in our future work."
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,,Fig. 1 .,
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,,Fig. 2 .,
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,,Fig. 3 .,
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,,Fig. 4 .,
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,,Table 1 .,
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,,34 92.670.57 77.190.87 73.990.53 88.610.61,
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_66.
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,1,Introduction,"Application of DNNs to critical applications like medical imaging requires that a model is not only accurate, but also well calibrated. Practitioners can trust Fig. 1. The graphs show the ECE computed on the sample sets of various sizes (x axis) drawn from a given test distribution: (Left) GBCU [2] (Middle) POCUS [3], and (Right) Diabetic Retinopathy [12]. Notice the large bias and the variance, especially for the small sample sets. In this paper we propose a novel calibration metric, Robust ECE, and a novel train time calibration loss, RCR, especially suited for small datasets, which is a typical scenario for medical image analysis tasks.deployed models if they are certain that the model will give a highly confident answer when it is correct, and uncertain samples will be labeled as such. In case of uncertainty, the doctors can be asked for a second opinion instead of an automated system giving highly confident but incorrect and potentially disastrous predictions [11]. Researchers have shown that modern DNNs are poorly calibrated and overconfident [6,25]. To rectify the problem, various calibration methods have been proposed such as Platt-scaling [26] based post-hoc calibration, or the train-time calibration methods such as MDCA [8]. We note that these methods have been mostly tested on large natural image datasets.Our key observation is that current metrics for calibration are highly unreliable for small datasets. For example, given a particular data distribution, if one measures calibration on various sample sets drawn from the distribution, an ideal metric should give an estimate with low bias and variance. We show that this does not hold true for popular metrics like ECE (c.f. Fig. 1). We investigate the reason for such discrepancy. We observe that all the techniques first divide the confidence range into bins, and then estimate the probability of a model predicting confidence in each bin, along with the accuracy of the model in that bin. Such probability estimates become highly unreliable when the sample set is small. Imagine, seeing one correct sample with confidence 0.7, and then declaring the model under-confident in that bin. Armed with the insight, we go on proposing a new metric called Robust ECE especially suited for small datasets, and an auxiliary RCR loss to calibrate a model at the train time. The proposed loss can be used in addition to an application specific loss to calibrate the model."
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Contributions: (1),"We demonstrate the ineffectiveness of common calibration metrics on medical image datasets with limited number of samples. (2) We propose a novel and robust metric to estimate calibration accurately on small datasets. The metric regularizes the probability of predicting a particular confidence value by estimating a parametric density model for each sample. The calibration estimates using the regularized probability estimates have significantly lower bias, and variance. (3) Finally, we also propose a train-time auxiliary loss for calibrating models trained on small datasets. We validate the proposed loss on several public medical datasets and achieve SOTA calibration results."
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,2,Related Work,"Metrics for Estimating Calibration: Expected Calibration Error (ECE) [21], first proposed in the context of DNNs by [6], divides the predicted confidence values in various bins and then calculates the absolute difference of average confidence and accuracy in that bin. The aggregate over all the bins is outputted as the calibration error. The motivation is to compute the probability of the model outputting a particular confidence, and the accuracy for the samples getting the particular confidence. The expectation of the difference is the calibration error. Since the error relies on accurate computation of probability, the same has large bias and variance when the dataset is small. Static Calibration Error (SCE) [23] extends ECE to multi-class settings by computing ECE for each class and then taking the average. Both ECE and SCE suffer from non-uniform distribution of samples into various bins, resulting in some bins getting small or no allocations. Adaptive binning (AECE) [22] attempts to mitigate the same by adaptively changing the bin sizes according to the given sample set. ECE-KDE [27] uses Dirichlet kernel density estimates for estimating calibration error.Calibrating DNNs: Calibration techniques typically reshape the output confidence vector so as to minimize a calibration loss. The techniques can be broadly categorized into post-hoc and train-time techniques. Whereas post-hoc techniques [6,10,14] use a validation set to learn parameters to reshape the output probability vector, the train time techniques typically introduce an additional auxiliary loss to aid in calibration [8,15,19,20,24,25,30]. While being more intrusive, such techniques are more popular due to their effectiveness. We also follow a similar approach in this paper. Other strategies for calibration includes learning robust representations leading to calibrated confidences [5,9,16,32].Calibration in Medical Imaging: Liang et al. [17] propose DCA loss which has been used for calibration of medical classification models. The loss aims to minimize the difference between predicted confidence and accuracy. However, the datasets demonstrated are quite large and the technique does not indicate any benefits for smaller datasets. Carneiro et al. [4] use MC-Dropout [5] entropy estimation and temperature scaling [6] for calibrating a model trained on colonoscopy polyp classification. Rajaraman et al. [28] demonstrate a few calibration methods for classification on class-imbalanced medical datasets."
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,3,Proposed Methodology,"Model Calibration: Let D be a dataset with N samples:Hence, for a sample i, the model outputs a confidence vector, C i ∈ [0, 1] K , a probability vector denoting the confidence for each class. A prediction y i is made by selecting the class with maximum confidence in C. A model is said to be calibrated if:Expected Calibration Error (ECE): is computed by bin-wise addition of difference between the average accuracy A i and average confidence C i :Here, C j denotes the confidence vector, and y j predicted label of a sample j.The confidences, C[ y], of all the samples being evaluated are split into M equal sized bins with the i th bin having B i number of samples. C i represents the average confidence of samples in the i th bin. The basic idea is to compute the probability of outputting a particular confidence and the associated accuracy, and the expression merely substitutes sample mean in place of true probabilities."
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Static Calibration Error (SCE):,"extends ECE to a multi-class setting as follows:Here B i,k denotes the number of samples of class k in the i th bin."
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,3.1,Proposed Metric: Robust Expected Calibration Error (RECE),"We propose a novel metric which gives an estimate of true ECE with low bias, and variance, even when the sample set is small. RECE incorporates the inherent uncertainty in the prediction of a confidence value, by considering the observed value as a sample from a latent distribution. This not only helps avoid overfitting on outliers, but also regularizes the confidence probability estimate corresponding to each confidence bin. We consider two versions of RECE based on the parameterization of the latent distribution."
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,RECE-G:,"Here, we assume a Gaussian distribution of fixed variance (σ) as the latent distribution for each confidence sample. We estimate the mean of the latent distribution as the observed sample itself. Formally:andTo prevent notation clutter, we usedenotes the probability of the interval [a, b] for a Gaussian distribution with mean μ, and variance σ. In the above expression, the range [ i-1 M , i M ] corresponds to the range of confidence values corresponding to i th bin. We also normalize the weight values over the set of bins. The value of standard deviation σ is taken as a fixed hyper-parameter. Note that the expression is equivalent to sampling infinitely many confidence values from the distribution N (•; c j , σ) for each sample j, and then computing the ECE value from thus computed large sampled dataset."
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,RECE-M:,"Note that RECE-G assumes fixed uncertainty in confidence prediction for all the samples as indicated by the choice of single σ for all the samples. To incorporate sample specific confidence uncertainty, we propose RECE-M in which we generate multiple confidence observations for a sample using test time augmentation. In our implementation, we generate 10 observations using random horizontal flip and rotation. We use the 10 observed values to estimate a Gaussian Mixture Model (denoted as G) with 3 components. We use θ j to denote the estimated parameters of mixture model for sample j. Note that, unlike RECE-G, computation of this metric requires additional inference passes through the model. Hence, the computation is more costly, but may lead to more reliable calibration estimates. Formally, RECE-M is computed as:and"
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,3.2,Proposed Robust Calibration Regularization (RCR) Loss,"Most train time auxiliary loss functions minimize ECE over a mini-batch. When the mini-batches are smaller, the problem of unreliable probability estimation affects those losses as well. Armed with insights from the proposed RECE metric, we apply similar improvements in state of the art MDCA loss [8]. We call the modified loss function as the Robust Calibration Regularization (RCR) loss:The RCR loss can be used as a regularization term along side any application specific loss function as follows:Here, β is a hyper-parameter for the relative weightage of the calibration. As suggested in the MDCA, we also use focal loss [19] for the L application . Our RCR  We give details in Supplementary A loss is also independent of binning scheme and differentiable which allows for its application in multiple problem formulations outside of classification (though not the focus of this paper, and hence, not validated through experiments)."
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,4,Experiments and Results,"Datasets: We use following publicly available datasets for our experiments to demonstrate variety of input modalities, and disease focus. GBCU dataset [2] consists of 1255 ultrasound (US) images used for the classification of gallbladders into normal, benign and malignant. BUSI [1] consists of 830 breast US images divided into normal, benign and malignant. POCUS [3] is a lung US dataset consisting of 2116 images among healthy, pneumonia and covid-19. Covid-CT [33] consists of 746 CT images classified as covid and non-covid. The Kaggle Diabetic Retinopathy (DR) dataset [12] consists of 50089 retina images classified into 5 stages of DR severity. The SIIM-ISIC Melanoma dataset [29] has 33132 dermoscopic images of skin lesions classified into benign and malignant classes. Wherever the train-test splits have been specified (GBCU, POCUS and Covid-CT), we have used the same. For BUSI and Melanoma we create random stratified splits as none are available. For DR we follow the method of [31] and split the dataset into a binary classification problem. We show the generality of our method on natural image datasets with CIFAR10 and CIFAR100 [13].Experimental Setup: We use a ResNet-50 [7] model as a baseline for most of our experiments. The GBCU dataset is trained on the GBCNet architecture [2]. Both are intialized with ImageNet weights. We use SGD optimizer with weight decay 5e-4, momentum 0.9 and step-wise LR decay with factor 0.1. We use LR 0.003 for GBCU and 0.01 for DR while the rest use 0.005. We train the models for 160 epochs with batch size 128. Horizontal flip is the only train-time augmentation used. For the RECE metric, we use σ = 0.1 and M = 15 bins for evaluation. For RCR loss we use β = 1 and focal loss as L application with γ = 1. Comparison between RECE-G, RECE-M and Other Metrics: Figure 2 and Table 1 give the comparison of different metrics computed over increasingly larger sample sets. For these experiments, we randomly sample the required sample size from the test set and compute metrics on them. The process is repeated 20 times and the average value is plotted with 95% confidence intervals. We plot the absolute difference with the baseline being the metric evaluated on the entire dataset. The results show that RECE-G and RECE-M outperform other metrics and are able to converge to the value computed from the whole dataset, using the smallest amount of data. The results for natural datasets are also shown."
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Evaluation of Calibration Methods:,We compare our RCR loss with other SOTA calibration techniques in Table 2. The results show that RCR is able to not only minimize our RECE metric but also other common metrics.
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,5,Conclusion,We demonstrated the ineffectiveness of existing calibration metrics for medical datasets with limited samples and propose a robust calibration metric to accurately estimates calibration independent of dataset size. We also proposed a novel loss to calibrate models using proposed calibration metric.
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Fig. 2 .,
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Fig. 3 .,
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Table 1 .,
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Table 2 .,
How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging?,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 15.
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,1,Introduction,"The human brain is a complex inter-wired system that emerges spontaneous functional fluctuations [2]. Like normal aging is characterized by brain structure and function changes contributing to cognitive decline, neuropathology events are also accompanied by network dysfunctions in both structural connectivity (SC) and functional connectivity (FC) [1,14]. Therefore, it is critical to understand the SC-FC relationship underlying the shift from healthy brain aging to the neurodegeneration diseases such as Alzheimer's disease (AD), which is imperative for the design and determination of effective interventions [6].A growing body of research studies the statistical association between SC and FC from the perspectives of connectivity [9], sub-networks [8], and network topology [15]. For instance, SC-FC coupling at each brain region was constructed by calculating the Spearman-rank correlation between a row of the SC matrix and the corresponding row of the FC matrix in [9]. In the past decade, graphtheory-based analysis has been widely used in many connectome-based studies to capture topological differences between healthy and disease connectomes that reflect network segregation (such as clustering coefficients), integration (such as nodal centrality), and organization (such as rich-club structure) [18]. In this context, topological characteristics have been compared between SC and FC in [15], where SC was found to have a relatively stable and efficient structure to support FC that is more changeable and flexible.However, current state-of-the-art SC-FC coupling methods lack integrated neuroscience insight at a system level. Specifically, many SC-FC coupling methods are mainly designed to find a statistical association between SC and FC topology patterns, lacking a principled system-level integration to characterize the coupling mechanism of how neural population communicates and emerges remarkable brain functions on top of the structural connectomes. To address this limitation, we present a new approach to elucidate the complex SC-FC relationship by characterizing the dynamical behaviors underlying a dissected mechanism. As shown in Fig. 1 (top), it might be challenging to directly link SC with FC. Alternatively, we sought to leverage the capital of well-studied biophysics models in neuroscience, acting as a stepping stone, to uncover the SC-FC coupling mechanisms, which allows us to generate novel SC-FC coupling biomarkers with great neuroscience insight (bottom).In this regard, we conceptualize the human brain as a complex system. With that being said, spontaneous functional fluctuation is not random. Instead, there is a coherent system-level mechanism that supports oscillatory neural activities throughout the brain anatomy. Therefore, we assume that each brain region is associated with a neural population, which manifests frequency-specific spontaneous neural oscillations. Inspired by the success of Kuramoto model [12] in modeling coupled synchronization in complex systems, we conceptualize that these oscillatory neural units are physically coupled via nerve fibers (observed in diffusion-weighted MRI images). To that end, the coupled phase oscillation process on top of the SC topology is supposed to emerge the manifestation of selforganized fluctuation patterns as observed in the blood-oxygen-level-dependent (BOLD) signal. Furthermore, we propose a novel graph neural network (GNN) to learn the dynamics of SC-FC coupling mechanism from a vast number of structural and functional human connectome data, which offers a new window to understand the evolving landscape of SC-FC relationships through the lens of phase oscillations.Fig. 1. Top: Conventional approaches mainly focus on the statistical association between SC and FC phenotypes. Bottom: We offer a new window to understand SC-FC coupling mechanisms through the lens of dynamics in a complex system that can be characterized using machine learning techniques.In the neuroscience field, tremendous efforts have been made to elucidate the biological mechanism underlying the spatiotemporally organized low-frequency fluctuations in BOLD data during the resting state. In spite of the insightful mathematical formulation and physics principles, the tuning of model parameters heavily relies on neuroscience prior knowledge and thus affects the model replicability. On the flip side, machine learning is good at data fitting in a data-hungry manner, albeit through a ""black-box"" learning mechanism. Taking together, we have laid the foundation of our proposed deep model on the principle of the Kuramoto model, which allows us to characterize the SC-FC relationships with mathematical guarantees. Specifically, we first translate the Kuramoto model into a GNN architecture, where we jointly learn the neural oscillation process at each node (brain region) and allow the oscillation state (aka. graph embedding vector) to diffuse throughout the SC-constrained topological pathways. The driving force of our deep model is to dissect the non-linear mechanism of coupled synchronization which can replicate the self-organized patterns of slow functional fluctuations manifested in BOLD signals. Following the notion of complex system theory, we further propose to yield new SC-FC coupling biomarkers based on learned system dynamics. We have evaluated the statistical power and clinical value of our new biomarkers in recognizing the early sign of neurodegeneration using the ADNI dataset, where the promising result indicates great potential in other network neuroscience studies."
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,2,Method,"Suppose the brain network of SC G = (Ξ, W ) consists of N brain regions Ξ = {ξ i |i = 1, ..., N } and the region-to-region structural connectivities W = [w ij ] ∈ R N ×N measured from diffusion-weighted images. On the other hand, the mean time course of BOLD signal x i (t)(i = 1, ..., N, t = 1, ..., T ) at each brain region forms a data matrix X(t) = [x i (t)] T t=1 ∈ R N ×T , which characterizes whole-brain functional fluctuations. In our work, we conceptualize that the human brain is a complex system where distinct brain regions are physically wired (coupled) via neuronal fibers. On top of this, the status of neural oscillation at each brain region is determined by an intrinsic state variable of brain rhythm v i (t). Multiple oscillators in the brain, each with their own frequency and phase, align their oscillations over time, which gives rise to the ubiquitous self-organized patterns of spontaneous functional fluctuations. To test this hypothesis, we present a deep model to reproduce the topology of traditional FC matrixmeasured by Pearson's correlation [3], from the phase information of neural activities, where the synchronization of coupled oscillators is constrained by Kuramoto model [12]."
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,2.1,Generalized Kuramoto Model for Coupled Neural Oscillations,"The Kuramoto family of coupled oscillators is a fundamental example of a nonlinear oscillator that exhibits various qualitative behaviors observed in physical systems. Each individual oscillator in the Kuramoto family has an inherent natural frequency denoted as ω and is subject to global coupling mediated by a sinusoidal interaction function. The dynamics of each oscillator are governed by the following partial differential equation (PDE), as described in [4]:where θ i denotes the phase of the oscillator i for the Kuramoto model, K ij denotes the relative coupling strength from node i to node j, ω i is the natural frequency associated with node i. The Kuramoto model is a well-established tool for studying complex systems, with its primary application being the analysis of coupled oscillators through pairwise phase interaction. The model enables each oscillator to adjust its phase velocity based on inputs from other oscillators via a pairwise phase interaction function denoted as K ij . The Kuramoto model's versatility is due to its ability to generate interpretable models for various complex behaviors by modifying the network topology and coupling strength. However, capturing higher-order dynamics is challenging with the classic Kuramoto model due to its pre-defined dynamics (K ij sin(θ i, θ j ) in Eq. 1). To address this limitation, we propose a more general formulation to model a nonlinear dynamical system as:where the system dynamics is determined by the state variable of brain rhythm v i on each node. Compared to Eq. 1, we estimate the natural frequency ω i through a non-linear function f (•), which depends on the current state variable v i and the neural activity proxy x i . Since the Hilbert transform (H(•)) has been widely used in functional neuroimaging research to extract the phase and amplitude information from BOLD signals [5,13], we further formulate the frequency function as f (v i , p i ), where p i = H(x i ) represents the phase information of time course x i by Hilbert transform. Second, we introduce the coupling physics function c(•, •) to characterize the nonlinear relationship between any two state variables v i and v j , where their coupling strength is measured by the structural connectivity w ij . Following the spirit of the reaction-diffusion model in systems biology [11], the first and second terms in Eq. 2 act as the reaction process (predicting the intrinsic state variable v i from the proxy signal x i ) and graph diffusion process (exchanging the state information throughout the SC network), respectively. Taking together, we present a deep Kuramoto model to reproduce FC network, where the functional fluctuations emerge from an evolving system of coupled neural oscillations. "
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,2.2,Deep Kuramoto Model for SC-FC Coupling Mechanism,"The overview of our deep Kuramoto model is shown in Fig. 2. The input consists of (1) time-invariant coupling information from the SC matrix (top-right corner), (2) time-evolving phase information at each node p i (t) (top-left corner). As shown in the blue box, our physics-guided deep Kuramoto model is designed to capture the dynamics of neural oscillations in a spatio-temporal learning scenario. At each time point t, we deploy a fully-connected network (FCN) and a GNN to predict the first and second terms in Eq. 2, respectively, based on the current state v i at each node ξ i . Specifically, we deploy the FCN for the reaction process f ϕ (t) = σ (β 1 v(t) + β 2 p(t) + μ), where σ(•) denotes the sigmoid function. The parameters ϕ = {β 1 , β 2 , μ} are shared across nodes {v i }. Meanwhile, we use a GNN to learn the hyper-parameters in the coupling function c(•) by c ϑ (t) = δ(W V ϑ), δ is the ReLu(•) function and ϑ denotes the learnable parameter in the diffusion process.The backbone of our deep Kuramoto model is a neuronal oscillation component where the evolution of state v(t) is governed by the PDE in Eq. 2.Under the hood, we discretize the continuous process by recursively applying the following operations at the current time point T : ( 1 To do so, we integrate the classic hybrid PDE solver [10] in our deep model.The driving force of our deep Kuramoto model to minimize the discrepancy between the observed BOLD signal x t and the reconstructed counterpart xt = f -1 (v t )) which is supposed to emerge from the intrinsic neural oscillation process. In training our deep Kuramoto model, we use a variant of stochastic gradient descent (SGD), Adma, with a learning rate of 0.001, to optimize the network parameters. As well the detailed parameter setting is as follows: epoch = 500, dropout = 0.5, hidden dimension = 64."
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,2.3,Novel SC-FC Coupling Biomarkers,"The valuable bi-product of our deep Kuramoto model of neural oscillation is a system-level explanation of how the neuro-system dynamics is associated with phenotypes such as clinical outcomes. In doing so, we introduce the Kuramoto order parameters φ t to quantify the synchronization level at time t as φ t = 1 N real{ N i=1 e iv(t) }, where real(•) denotes the real part of the complex number. In complex system area, φ is described as the synchronization level, aka. the metastability of the system [17], transiting from complete chaos (φ t = 0) and fully synchronization (φ t = 1).Empirical SC-FC Coupling Biomarkers. As a proof-of-concept approach, we propose a novel SC-FC coupling biomarker Φ = (φ t0 , φ t1 , ..., φ tT ) (bottom right corner in Fig. 2) which records the evolution of system metastability underlying the neural activity. Since the neuroscience intuition of Φ is in line with the functional dynamics, we expect the SC-FC biomarker Φ to allow us to recognize subtle network dysfunction patterns between healthy and disease connectomes. To make the coupling biomarker invariant to the length of the time course, we further present a global summary of Φ by counting the number of temporal transitions (called metastability transition count) between the minimal (less-synchronized) and maximum (less-chaotic) metastability statuses, called SC-FC-META."
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,,SC-FC Coupling Network for Disease Diagnosis.,"To leverage the rich system-level heuristics from Kuramoto model, it is straightforward to integrate a classification branch on top of Φ which is trained to minimize the cross-entropy loss in classifying healthy and disease subjects. Thus, the tailored deep Kuramoto model for early diagnosis is called SC-FC-Net. "
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,3,Experiments,"In this study, we evaluate the statistical power and clinical value of our learningbased SC-FC coupling biomarkers in separating Alzheimer's Disease (AD) from cognitively normal (CN) subjects using ADNI dataset [16]. The canonical Automated Anatomical Labeling (AAL) atlas [19] is used to parcellate the entire brain into 90 regions for each scan on which we construct 90 × 90 SC and mean BOLD signals at each brain region. There are in total of 250 subjects (73 AD vs. 177 CN). We examine the performance by SC-FC-META (empirical SC-FC coupling biomarker) and SC-FC-Net (physics-guided deep model) with comparison to graph convolutional network (GCN), recurrent neural network (RNN) and a PDE-based counterpart method (LTCNet) [10]."
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,3.1,Validating the Neuroscience Insight of Deep Kuramoto Model,"The main hypothesis is that spontaneous functional fluctuations arise from the phase oscillations of coupled neural populations. In this regard, we spotlight the topological difference between the conventional FC by Pearson's correlation on BOLD signal and the reproduced FC based on the state variable of brain rhythm {v i (t)}. First, we display the population-average of conventional FC (Fig. 3 left) and our reproduced FC matrix (Fig. 3 right). Through visual inspection, the network topology between two FC matrices is very consistent, indicating the validity of applying our deep Kuromoto model in resting-state fMRI studies. Furthermore, we detect the hub nodes based on FC connectivity degree for each FC matrix [7] and evaluate the consensus of hub node detection results between conventional and reproduced FC matrices. As shown in the middle of Fig. 3, the majority of hub nodes have been found in both FC matrics, providing the quantitative evidence of model validity. "
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,3.2,Evaluation on Empirical Biomarker of SC-FC-META,"First, the CN vs. AD group comparison on SC-FC coupling biomarker is shown in Fig. 4(a), where our novel SC-FC-META biomarker exhibits a significant difference at the level of p < 0.01. Second, we display two typical examples of less-synchronized status (φ t → 0) and less-chaotic status (φ t → 1) in Fig. 4(b), where we follow the convention of Kumamoto model to position all phases at time t along the circle. Thus, the synchronization level can be visually examined based on the (phase) point clustering pattern on the circle. Third, the timeevolving curve of metastability in CN (in purple) and AD (in brown) subjects are shown in Fig. 4(c), which provides a new insight into the pathophysiological mechanism of disease progression in the perspective of the system's stability level. Notably, there are much more transitions in AD than CN subjects, indicating that alterations in SC-FC coupling mechanism render AD subjects manifest reduced control over the synchronization from the coupled neural oscillations."
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,3.3,Evaluation on SC-FC-Net in Diagnosing AD,"Herein, we first assess the accuracy of diagnosing AD using SC-FC-META and SC-FC-Net. The former method is a two-step approach where we first calculate the SC-FC-META biomarker for each subject and then train a SVM. While our SC-FC-Net is an end-to-end solution. The classification results are shown in Fig. 4(d), where the deep model SC-FC-Net (in green) achieves much more accurate classification results than the shallow model (in blue). Furthermore, we compare the classification result by SC-FC-Net with other deep models (trained on BOLD time course only) in Fig. 4(e). Our SC-FC-Net shows in average 2.3% improvement over the current state-of-the-art methods, indicating the potential of investigating SC-FC coupling mechanism in disease diagnosis."
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,4,Conclusion,"We introduce a novel approach that combines physics and deep learning to investigate the neuroscience hypothesis that spontaneous functional fluctuations arise from a dynamic system of neural oscillations. Our successful deep model has led to the discovery of new biomarkers that capture the synchronization level of neural oscillations over time, enabling us to identify the coupling between SC and FC in the brain. We have utilized these new SC-FC coupling biomarkers to identify brains at risk of AD, and have obtained promising classification results. This approach holds great promise for other neuroimaging applications."
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,,Fig. 2 .,
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,,,
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,,Fig. 3 .,
Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases,,Fig. 4 .,
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,1,Introduction,"Continual advancement of deep learning algorithms for medical image analysis has increased the potential for their adoption at scale. Across a wide range of medical applications including skin lesion classification [8,31], detection of diabetic retinopathy in fundus images [14], detection of large vessel occlusions in CT [20], and detection of pneumonia in chest x-ray [24], deep learning algorithms have pushed the boundaries close to or beyond human performance.However, with these innovations has come increased scrutiny of the integrity of these models in safety critical applications. Prior work [7,10,17] has found that deep neural networks are capable of exploiting spurious features and other shortcuts in the data that are not causally linked to the task of interest such as using dermascopic rulers as cues to predict melanoma [2,35,36] or associating the presence of a chest drain with pneumothorax in chest X-ray analysis [21]. The exploitation of such shortcuts by DNNs may have serious bias/fairness implications [11,12] and negative ramifications for model generalization [7,21].As attention to these issues grows, recent legislation has been proposed that would require the algorithmic auditing and impact assessment of ML-based automated decision systems [37]. However, without clearly defined strategies for selecting attributes to audit for bias, impact assessments risk being constrained to only legally protected categories and may miss more subtle shortcuts and data flaws that prevent the achievement of important model goals [23,28]. Our goal in this work is to develop objective methods for generating data-driven hypotheses about the relative level of risk of various attributes to better support the efficient, comprehensive auditing of any model trained on the same data.Our method generates targeted hypotheses for model audits by assessing (1) how feasible it is for a downstream model to detect and exploit the presence of a given attribute from the image alone (detectability), and (2) how much information the model would gain about the task labels if said attribute were known (utility). Causally irrelevant attributes with high utility and detectability become top priorities when performing downstream model audits. We demonstrate high utility complicates attempts to draw conclusions about the detectability of attributes and show our approach succeeds where unconditioned approaches fail. We rigorously validate our approach using a range of synthetic artifacts which allow us to expedite the auditing of models via the use of true counterfactuals. We then apply our method to a popular skin lesion dataset where we identify a previously unreported potential shortcut."
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,2,Related Work,"Issues of bias and fairness are of increasing concern in the research community. Recent works such as [13,29,30] identify cases where trained DNNs exhibit performance disparities across protected groups for chest x-ray classification tasks.Of interest to this work, [22] used Mutual Information-based analysis to examine the robustness of DNNs on dermascopy data and observed performance disparities with respect to typical populations of interest (i.e., age, sex) as well as less commonly audited dataset properties (e.g., image hue, saturation). In addition to observing biased performance in task models, [11,12] show that patient race (and potentially other protected attributes) may be implicitly encoded in representations extracted by DNNs on chest x-ray images. A more general methodology for performing algorithmic audits in medical imaging is also proposed in [18]. In contrast to our work, these methods focus on individual, biased task models without considering the extent to which those biases are induced by the causal structure of the training/evaluation data.In addition to model auditing methods, a number of metrics have been proposed to quantify bias [9]. A recent study [1] compared several and recommend normalized pointwise mutual information due to its ability to measure associations in the data while accounting for chance. Also relevant to this work, [16] provides an analysis of fairness metrics and guidelines for metric selection in the presence of dataset bias. However, these studies focus primarily on biases identifiable through dataset attributes alone and do not consider whether those attributes are detectable in the image data itself.Lastly, [28] found pervasive data cascades where data quality issues compound and cause adverse downstream impacts for vulnerable groups. However, their study was qualitative and no methods for automated dataset auditing were introduced. Bissoto et al. [3,4] consider the impact of bias in dermatological data by manipulating images to remove potential causally-relevant features while measuring a model's ability to still perform the lesion classification task. Closest to our work, [25] takes a causal approach to shortcut identification by using conditional dependence tests to determine whether DNNs rely on specific dataset attributes for their predictions. In contrast, our work focuses on screening datasets for attributes that induce bias in task models. As a result, we directly predict attribute values to act as a strong upper bound on detectability and use normalized, chance-adjusted dependence measures to obtain interpretable metrics that we show correlate well with the performance of task models. "
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,3,Methods,"To audit at the dataset level, we perform a form of causal discovery to identify likely relationships between the task labels, dataset attributes represented as image metadata, and features of the images themselves (as illustrated in Fig. 1).We start from a set of labels {Y }, attributes {A}, and images {X}. We assume that Y (the disease) is the causal parent of X (the image) given that the disease affects the image appearance but not vice versa [6]. Then the dataset auditing procedure aims to assess the existence and relative strengths of the following two relationships: (1) Utility: A ↔ Y and (2) Detectability: A → X. The utility measures whether a given attribute shares any relationship with the label. The presence of this relationship for A that are not clinically relevant (e.g., sensor type or settings) represents increased potential for biased outcomes. However, not every such attribute carries the same risk for algorithmic bias. Crucially, relationship (2) relates to the detectability of the attribute itself. If our test for (2) finds the existence of relationship A → X is probable, we consider the attribute detectable. Dataset attributes identified as having positive utility with respect to the label (1) and detectable in the image (2) are classified as potential shortcuts and pose the greatest risk to models trained on this dataset.Causal Discovery with Mutual Information. Considering attributes in isolation, we assess attribute utility and detectability from an information theoretic perspective. In particular, we recognize first that the presence of a relationship between A and Y can be measured via their Mutual Information: MI(A; Y ) = H(Y )-H(Y |A). MI measures the information gained (or reduction in uncertainty) about Y by observing A (or vice versa) and MI(A; Y ) = 0 occurs when A and Y are independent. We rely on the faithfulness assumption which implies that a causal relationship exists between A and Y when MI(A; Y ) > 0. From an auditing perspective, we aim to identify the presence and relative magnitude of the relationship but not necessarily the nature of it.Attributes identified as having a relationship with Y are then assessed for their detectability (i.e., condition (2)). We determine detectability by training a DNN on the data to predict attribute values. Because we wish to audit the entire dataset for bias, we cannot rely on a single train/val/test split. Instead, we partition the dataset into k folds (typically 3) and finetune a sufficiently expressive DNN on the train split of each fold to predict the given attribute A. We then generate unbiased predictions for the entire dataset by taking the output Â from each DNN evaluated on their respective test split. We measure the Conditional Mutual Information over all predictions: CM I( Â; A|Y ) = H( Â|Y )-H( Â|A, Y ). CM I(A, Â|Y ) measures information shared between attribute A and its prediction Â when controlling for information provided by Y . Since relationship A and Y was established via MI(A; Y ), we condition on label Y to understand the extent to which attribute A can be predicted from images when accounting for features associated with Y that may also improve the prediction of A. Similar to MI, CM I( Â; A|Y ) > 0 implies A → Â exists.To determine independence and account for bias and dataset specific effects, we include permutation-based shuffle tests from [26,27]. These approaches replace values of A with close neighbors to approximate the null hypothesis that the given variables are conditionally independent. By calculating the percentile of CM I(A; Â|Y ) among all CM I(A π ; Â|Y ) (where A π are permutations of A), we estimate the probability our samples are independent while adjusting for estimator bias and dataset-specific effects. To make CMI and MI statistics interpretable for magnitude-based comparison between attributes, we include adjustments for underlying distribution entropy and chance as per [1,34] (See supplement). "
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4,Experiments and Discussion,"To demonstrate the effectiveness of our method, we first conduct a series of experiments using synthetically-altered skin lesion data from the HAM10000 dataset where we precisely create, control, and assess biases in the dataset. After establishing the accuracy and sensitivity of our method on synthetic data, we apply our method to the natural attributes of HAM10000 in Experiment 5 (Sect. 4.5)."
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,,Datasets:,"We use publicly available skin lesion data from the HAM10000 [33] dataset with additional public metadata from [2]. The dataset consists of 10,015 dermascopic images collected from two sites, we filter so only one image per lesion is retained, leaving 7,387 images. The original dataset has seven diagnostic categories: we focus on predicting lesion malignancy as a challenging and practical task. While we recognize the importance of demonstrating the applicability of the methodology over many datasets, here we use trials where we perturb this dataset with a variety of synthetic, realistic artifacts (e.g., Fig. 2), and control association with the malignant target label. With this procedure, we create multiple variants of the dataset with attributes that have known utility and detectability as well as ground truth counterfactuals for task model evaluation. Further details are available in the supplementary materials.Training Protocol: For attribute prediction networks used by our detectability procedure, we finetune ResNet18 [15] models with limited data augmentation. For the malignancy prediction task, we use Swin Transformer [19] tiny models with RandAugment augmentation to show detectability results generalize to stronger architectures. All models were trained using class-balanced sampling with a batch size of 128 and the AdamW optimizer with a learning rate of 5e-5, linear decay schedule, and default weight decay and momentum parameters. For each trial, we use three-fold cross-validation and subdivide each training fold in a (90:10) train:validation split to select the best models for the relevant test fold. By following this procedure, we get unbiased artifact predictions over the entire dataset for use by MI estimators by aggregating predictions over all test folds. We generally measure model performance via the Receiver Operating Characteristic Area Under the Curve (AUC)."
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.1,Experiment 1: Induced Bias Versus Relationship Strength,"For this experiment, we select an artifact that we are certain is visible (JPEG compression at quality 30 applied to 1000 images), and seek to understand how the relationship between attribute and task label influences the task model's reliance on the attribute. The artifact is introduced with increasing utility such that the probability of the artifact is higher for cases that are malignant. Then, we create a worst case counterfactual set, where each malignant case does not have the artifact, and each benign case does. In Fig. 3a, we see performance rapidly declines below random chance as utility increases."
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.2,Experiment 2: Detectability of Known Invisible Artifacts,"In the previous section, we showed that the utility A ↔ Y directly impacts the task model bias, given A is visible in images. However, it is not always obvious whether an attribute is visible. In Reading Race, Gichoya et al. showed racial identity can be predicted with high AUC from medical images where this information is not expected to be preserved. Here, we show CMI represents a promising method for determining attribute detectability while controlling for attribute information communicated through labels and not through images.Specifically, we consider the case of an ""invisible artifact"". We make no changes to the images, but instead create a set of randomized labels for our nonexistent artifact that have varying correlation with the task labels (A ↔ Y ). As seen in Fig. 4, among cases where the invisible artifact and task label have a reasonable association, models tasked with predicting the invisible artifact perform well above random chance, seemingly indicating that these artifacts are visible in images. However, by removing the influence of task label and related image features by calculating MI(A, Â|Y ), we clearly see that the artifact predictions are independent of the labels, meaning there is no visible attribute. Instead, all information about the attribute is inferred from the task label."
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.3,Experiment 3: Conditioned Detectability Versus Ground Truth,"To verify that the conditional independence testing procedure does not substantially reduce our ability to correctly identify artifacts that truly are visible, we introduce Gaussian noise with standard deviation decreasing past human perceptible levels. In Experiment 2 (Fig 4A ) the performance of detecting artifact presence is artificially inflated because of a relationship between disease and artifact. Here the artifact is introduced at random so AUC is an unbiased measure of detectability. In Table 1, we see the drop in CMI percentile from conditioning is minimal, indicating sensitivity even to weakly detectable attributes."
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.4,Experiment 4: Relationship and Detectability vs Induced Bias,"Next, we consider how utility and detectability together relate to bias. We introduce a variety of synthetic artifacts and levels of bias and measure the drop in   AUC that occurs when evaluated on a test set with artifacts introduced in the same ratio as training versus the worst case ratio as defined in Experiment 1.Of 36 unique attribute-bias combinations trialed, 32/36 were correctly classified as visible via permutation test with 95% cutoff percentile. The remaining four cases were all compression at quality 90 and had negligible impact on task models (mean drop in AUC of -0.0003 ± .0006). In Fig. 3b, we see the relative strength of the utility (A ↔ Y ) correlates with the AUC drop observed. This implies utility represents a useful initial metric to predict the risk of an attribute. The detectability, CM I(A; Â|Y ), decreases as utility, MI(A; Y ), increases, implying the two are not independent. Intuitively, when A and Y are strongly related (Utility is high), knowledge of the task label means A is nearly determined, so learning Â does not convey much new information and detectability is smaller. To combat this, we use a conditional permutation method [27] for judging whether or not an artifact is present. Further, detectability among attributes with equal utility for each level above 0 have statistically significant correlations with drops in AUC (Kendall's τ of 0.800, 0.745, 0.786, 0.786, 0.716 respectively). From this, we expect that for attributes with roughly equal utility, more detectable attributes are more likely to result in biased task models. "
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,4.5,Experiment 5: HAM10000 Natural Attributes,"Last, we run our screening procedure over the natural attributes of HAM10000 and find that all pass the conditional independence tests of detectability. Based on our findings, we place the attributes in the following order of concern: (1) Data source, (2) Fitzpatrick Skin Scale, (3) Ruler Presence, (4) Gentian marking presence (we skip localization, age and sex due to clinical relevance [5]). From Fig. 5 we see data source is both more detectable and higher in utility than other variables of interest, representing a potential shortcut. To the best of our knowledge, we are the first to document this concern, though recent independent work supports our result that differences between the sets are detectable [32]."
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,5,Conclusions,"Our proposed method marks a positive step forward in anticipating and detecting unwanted bias in machine learning models. By focusing on dataset screening, we aim to prevent downstream models from inheriting biases already present and exploitable in the data. While our screening method naturally includes common auditing hypotheses (e.g., bias/fairness for vulnerable groups), it is capable of generating targeted hypotheses on a much broader set of attributes ranging from sensor information to clinical collection site. Future work could develop unsupervised methods for discovering additional high risk attributes without annotations. The ability to identify and investigate these hypotheses provides broad benefit for research, development, and regulatory efforts aimed at producing safe and reliable AI models."
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,,Fig. 1 .,
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,,Fig. 2 .,
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,,Fig. 3 .,
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,,Fig. 4 .,
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,,1. 0,
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,,Fig. 5 .,
Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models,,Table 1 .,
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,1,Introduction,"Object detection and identification are key steps in many biomedical imaging applications [13]. Digital imaging is essential in dentistry, providing practitioners with detailed internal and surface-level information for the accurate diagnosis and effective treatment planning of endodontic and orthodontic procedures [18]. Intra-oral scans (IOS) are a specific type of digital dental imagery that produce 3D impressions of the dental arches, commonly referred to as dental casts [12]. Surface level visualizations can be utilized for the automatic design and manufacturing of aligners and dental appliances [18].A precursor to fully automated workflows consists of accurate detection and recognition of dental structures [4,9,11]. The difficulty of the tasks stems from inherent anatomical variability among individuals, as well as the presence of confounding factors such as treatment-related artifacts and noise [4,8]. To date, a limited number of studies have been conducted to experiment with algorithms performing instance segmentation in dental casts [3,15,17,[19][20][21]. Tian et al. [17] proposed a multi-level instance segmentation framework based on CNNs, investigating the impact of incorporating a broad classification stage that classified structures into incisors, canines, premolars, and molars. Xu et al. [19] adopted a similar broad-to-narrow classification strategy, developing two CNNs for labeling mesh faces based on handcrafted features. Sun et al. [16] applied the FeaStNet graph CNN algorithm for dental vertex labeling. Cui et al. [3] developed a tooth detection pipeline that included tooth centroid localization followed by instance segmentation applied on cropped sub-point clouds surrounding the centroids.The identification of dental instances is a complex task due to the presence of anatomical variations such as crowding, missing teeth, and ""shark teeth"" (or double teeth) [7]. Xu et al. [19] employed PCA analysis to correct mislabeled pairs of teeth caused by missing or decayed teeth. Sun et al. [16] analyzed crown shapes and the convexity of the border region to address ambiguous labeling of neighboring dental instances. However, previous studies have not effectively addressed the issue of tooth labeling in the presence of dental abnormalities such as misaligned and double teeth.The labeling of dental casts presents a non-trivial challenge of identifying objects of similar shapes that are geometrically connected and may have duplicated or missing elements. This study introduces an assignment theory-based approach for recognizing objects based on their positional inter-dependencies. We developed a distance-based dental model of jaw anatomy. The model was transformed into a cost function for a bipartite graph using a convolutional neural network. To compute the optimal labeling path in the graph, we introduced a novel loss term based on assignment theory into the objective function. The assignment theory-based framework was tested on a large database of dental casts and achieved almost perfect labeling of the teeth."
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,2,Method,
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,2.1,Generation of Candidate Labels,"The database utilized in the present study comprised meshes that represented dental casts, with the lower and upper jaws being depicted as separate entities.Each mesh vertex was associated with a label following the World Dental Federation (FDI) tooth numbering system. A large proportion of the samples in the dataset was associated with individuals who had healthy dentition. A subset of patients presented dental conditions, including misaligned, missing, and duplicated teeth. Furthermore, the dataset consisted of patients with both permanent and temporary dentition.The dental cast labeling task was divided into two stages: the detection of candidate teeth (1), which involved identifying vertices forming instances of teeth, and the assignment of labels to the candidate teeth, with geometric and anatomical considerations (2). The process of detecting candidate teeth consisted of indirect instance segmentation. The dental casts were converted to binary volumetric images with a voxel resolution of 1mm; voxels containing vertices were assigned a value of 1, while those without vertices were assigned a value of 0 [5]. The binary images were then segmented using two separate 3D U-net models, each specifically trained for either the upper or lower dental cast types. The models were trained to segment 17 different structures. When applied to a new volumetric dental cast, the models generated 17 probability maps: one for each of the 16 tooth types, corresponding to the full set of normal adult human teeth present in each jaw, plus an additional one for non-dental structures like gums. The outputs were converted to vertex labels over the input dental cast, by finding spatial correspondences between voxels and vertices."
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,2.2,Dental Anatomical Model,"The difficulty of segmenting dental structures stems from the high inter-personal shape and position variability, artifacts (e.g. fillings, implants, braces), embedded, and missing teeth [1,4,6]. These challenges combined with the tendency for neighboring instances to have similar shapes affect the performance and accuracy of the U-Net models. As a result, the output produced by the segmentors may contain missed or incorrectly assigned labels.To address these challenges and build an accurate instance segmentation pipeline, we first generated a dental anatomical model that provided a framework for understanding the expected positions of the teeth within the jaw. The dental model relied on the relative distances between teeth, instead of their actual positions, for robustness against translation and rotation transformations. The initial step in modeling the jaws was calculating the centroids of the dental instances by averaging the coordinates of their vertices. The spatial relationship between two teeth centroids, c 1 and c 2 , was described by the displacement vector, d = c 1 -c 2 . To evaluate the relative position of two instances of types t 1 and t 2 , we calculated the means (μ x , μ y , μ z ) and standard deviations (σ x , σ y , σ z ) for each dimension (x, y, z) of the displacement vectors corresponding to instances of types t 1 and t 2 in patients assigned to the training set. The displacement rating r for the two instances was computed as the average of the univariate Gaussian probability density function evaluated at each dimension (d x , d y , d z ) of the displacement vector d: (1)The dental model of a patient consisted of a 3D matrix A of shape m×m×4, where m corresponded to the total number of tooth types. For each pair of dental instances i and j, the elements a ij0 , a ij1 , a ij2 correspond to the x, y, and z coordinates of the displacement vector, respectively, while a ij3 represents the relative position score. The presence of anatomical anomalies, such as missing teeth and double teeth, did not impact the information embedded in the dental model. If tooth i was absent, the values for the displacement vectors and position ratings in both row i and column i of the dental model A were set to zero."
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,2.3,The Assignment Problem,"The next step was evaluating and correcting the candidate tooth labels generated by the U-net models using the dental anatomical model. We reformulated the task as finding the optimal label assignment. Assignment theory aims to solve the similar task of assigning m jobs to m workers in a way that minimizes expenses or maximizes productivity [14]. For the purpose of dental cast labeling, the number of jobs m is defined as t teeth types present in a typical healthy individual and d double teeth observed in patients with the ""shark teeth"" condition, m = t+d. To ensure that the number of jobs matched the number of workers, k ""dummy"" teeth without specific locations were added to the n teeth candidates generated by the U-net. The dental model was transformed into a cost matrix B of dimensions m × m, where each element b ij represented the cost associated with assigning label j to candidate instance i. We solved the assignment problem using the Hungarian method [10]. The optimal assignment solution C * for n candidate teeth was not affected by the presence of k ""dummy"" teeth, provided that all elements b ij in the cost matrix B where either i or j corresponds to a ""dummy"" instances were assigned the maximum cost value of q. Proposition 1. Let the set of candidate teeth be C and the number of possible candidate teeth m = t + s, such that |C| < m. The optimal label assignment to the candidate instances is C * = f * (C), where f * ∈ F is the optimal assignment function. Let us assume that there exists only one optimal assignment function f * . The sum of costs associated with assigning candidate teeth to their optimal labels according to f * is less than the sum of costs associated with any other assignment function p ∈ F\{f * }, x∈C B x,f * (x) < x∈C B x,p(x) . The inclusion of r dummy teeth, with r = m -|C|, and maximum assignment cost q cannot alter the optimal assignment C * .Proof. Let's assume that the addition of one dummy object θ with maximum assignment cost q changes the optimal assignment to g ∈ F\{f * } on the set C ∪ {θ}. The assignment cost of the new candidate setconsidering that B θ,g(θ) = q. The definition of the optimal label assignment function f * states that its cumulative assignment cost is smaller than the cumulative assignment cost of any g ∈ F\{f * } on the set C ∪ {θ}, which indicates thatThis contradicts the assumption that g is the optimal assignment for C ∪ {θ}. This proof can be generalized to the case where multiple dummy teeth are added to the candidate set.In other words, Proposition 1 states that adding ""dummy"" instances to the candidate teeth sets does not affect the optimal assignment of the non-dummy objects. The dummy teeth played a dual role in our analysis. On one hand, they could be used to account for the presence of double teeth in patients with the ""shark teeth"" condition. On the other hand, they could be assigned to missing teeth in patients with missing dentition."
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,2.4,DentAssignNet,"The optimal assignment solution f * ensured that each candidate tooth would be assigned a unique label. We integrated the assignment solver into a convolutional neural network for labeling candidate teeth, entitled DentAssignNet (Fig. 1).The input to the convolutional neural network consisted of a matrix A of shape m × m × 4, which represented a dental model as introduced in Sect. 2.2. For each pair of dental instances i and j, the element a ij included the coordinates of the displacement vector and the relative position score. The architecture of the network was formed of 3 convolutional blocks. Each convolutional block in the model consisted of the following components: a convolutional layer, a rectified linear unit (ReLU) activation function, a max pooling layer, and batch normalization. The convolutional and pooling operations were applied exclusively along the rows of the input matrices because the neighboring elements in each row were positionally dependent. The output of the convolutional neural network was a cost matrix B of shape m × m, connecting candidate instances to potential labels. The assignment solver transformed the matrix B into the optimal label assignment C * . The loss function utilized during the training phase was a weighted sum of two binary cross entropy losses: between the convolutional layer's output Ŷ and the ground truth Y, and between Ŷ multiplied by the optimal assignment solution Ŷ and the ground truth Y.The direct application of the assignment solver on a dental model A with dimensions m × m × 4 was not possible, as the solver required the weights associated with the edges of the bipartite graph connecting candidate instances to labels. By integrating the optimal assignment solution in the loss function, Den-tAssignNet enhanced the signal corresponding to the most informative convolutional layer output cells in the task of labeling candidate teeth."
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,3,Experiment and Results,
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,3.1,Database,"The database employed in this study was introduced as part of the 3D Teeth Scan Segmentation and Labeling Challenge held at MICCAI 2022 [2]. It featured 1200 dental casts, depicting lower and upper jaws separately. The dental structures were acquired using intra-oral scanners (IOS) and modeled as meshes. The average number of vertices per mesh was 117377. The cohort consisted of 600 patients, with an equal distribution of male and female individuals. Approximately 70% of the patients were under 16 years of age, while around 27% were between 16 and 59 years of age, and the remaining 3% were over 60 years old."
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,3.2,Experiment Design,"To obtain the candidate tooth labels, we employed the U-net architecture on the volumetric equivalent of the dental mesh. Considering that the majority of the samples in the database featured individuals with healthy dentition, a series of transformations were applied to the U-net results to emulate the analysis of cases with abnormal dentition. To simulate the simultaneous occurrence of both permanent and temporary dental instances of the same type, we introduced artificial centroids with labels copied from existing teeth. They were placed at random displacements, ranging from d min to d max millimeters, from their corresponding true teeth, with an angulation that agreed with the shape of the patient's dental arch. The protocol for constructing the database utilized by our model involved the following steps. Firstly, we calculated the centroids of each tooth using the vertex labels generated by the U-Net model. Subsequently, we augmented the U-net results by duplicating, removing, and swapping teeth. The duplication procedure was performed with a probability of p d = 0.5, with the duplicate positioned at a random distance between d min = 5 millimeters and d max = 15 millimeters from the original. The teeth removal was executed with a probability of p e = 0.5, with a maximum of e = 4 teeth being removed. Lastly, the swapping transformation was applied with a probability of p w = 0.5, involving a maximum of w = 2 tooth pairs being swapped. Given that neighboring dental instances tend to share more similarities than those that are further apart, we restricted the label-swapping process to instances that were located within two positions of each other."
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,3.3,Results,"Each sample in the database underwent 10 augmentations, resulting in 9000 training samples, 1000 validation samples, and 2000 testing samples for both jaws. The total number of possible tooth labels was set to m = 17, consisting of t = 16 distinct tooth types and d = 1 double teeth. The training process involved 100 epochs, and the models were optimized using the RMSprop algorithm with a learning rate of 10 -4 and a weight decay of 10 -8 . The weighting coefficient in the assignment-based loss was λ = 0.8. The metrics reported in this section correspond to the detection and identification of teeth instances. The U-net models achieved detection accuracies of 0.989 and 0.99 for the lower and upper jaws, respectively. The metrics calculated in the ablation study take into account only the dental instances that were successfully detected by the candidate teeth proposing framework. Table 1 presents identification rates for the candidate dental instances (prior to and following ablation) and the performance of DentAssignNet. Our framework achieved identification accuracies of 0.992 and 0.991 for the lower and upper jaws, respectively. There was a significant improvement in performance compared to the U-net results (0.972 and 0.971) and the artificially ablated input teeth (0.888 and 0.887). Figure 2 depicts the ability of DentAssignNet to handle patients with healthy dentition (row 1), erupting teeth (row 2), and missing teeth (row 3). For comparison purposes, we refer to the results of the 3D Teeth Scan Segmentation and Labeling Challenge at MICCAI 2022 [2]. The challenge evaluated the algorithms based on teeth detection, labeling, and segmentation metrics, on a private dataset that only the challenge organizers could access. Hoyeon Lim et al. adapted the Point Group method with a Point Transformer backbone and achieved a labeling accuracy of 0.910. Mathieu Leclercq et al. used a modified 2D Residual U-Net and achieved a labeling accuracy of 0.922. Shaojie Zhuang et al. utilized PointNet++ with cast patch segmentation and achieved a labeling accuracy of 0.924. Our identification accuracies of 0.992 and 0.991 for the lower and upper jaw, respectively, compare favorably to the results from the challenge. However, it must be noted that this is not a direct comparison as the results were achieved on different segments of the dental cast challenge database. Additionally, the metrics used in the challenge were specifically designed to accommodate the dual task of detection and identification, calculating labeling accuracy relative to all dental instances, including those that were not detected."
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,4,Conclusion,"We proposed a novel framework utilizing principles of assignment theory for the recognition of objects within structured, multi-object environments with missing or duplicate instances. The multi-step pipeline consisted of detecting and assigning candidate labels to the objects using U-net (1), modeling the environment considering the positional inter-dependencies of the objects (2), and finding the optimal label assignment using DentAssignNet (3). Our model was able to effectively recover most teeth misclassifications, resulting in identification accuracies of 0.992 and 0.991 for the lower and upper jaws, respectively."
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,,Fig. 1 .,
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,,Fig. 2 .,
Assignment Theory-Augmented Neural Network for Dental Arch Labeling,,Table 1 .,
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,1,Introduction,"Prostate cancer is a leading cause of cancer-related deaths in adult males, as reported in studies, such as [17]. A common treatment option for prostate cancer is external beam radiation therapy (EBRT) [4], where CT scanning is a cost-effective tool for the treatment planning process compared with the more expensive magnetic resonance imaging (MRI). As a result, precise prostate segmentation in CT images becomes a crucial step, as it helps to ensure that the radiation doses are delivered effectively to the tumor tissues while minimizing harm to the surrounding healthy tissues.Due to the relatively low spatial resolution and soft tissue contrast in CT images compared to MRI images, manual prostate segmentation in CT images can be time-consuming and may result in significant variations between operators [10]. Several automated segmentation methods have been proposed to alleviate these issues, especially the fully convolutional networks (FCN) based U-Net [19] (an encoder-decoder architecture with skip connections to preserve details and extract local visual features) and its variants [14,23,26]. Despite good progress, these methods often have limitations in capturing long-range relationships and global context information [2] due to the inherent bias of convolutional operations. Researchers naturally turn to ViT [5], powered with self-attention (SA), for more possibilities: TransUNet first [2] adapts ViT to medical image segmentation tasks by connecting several layers of the transformer module (multi-head SA) to the FCN-based encoder for better capturing the global context information from the high-level feature maps. TransFuse [25] and MedT [21] use a combined FCN and Transformer architecture with two branches to capture global dependency and low-level spatial details more effectively. Swin-UNet [1] is the first U-shaped network based purely on more efficient Swin Transformers [12] and outperforms models with FCN-based methods. UNETR [6] and SiwnUNETR [20] are Transformer architectures extended for 3D inputs.In spite of the improved performance for the aforementioned ViT-based networks, these methods utilize the standard or shifted-window-based SA, which is the fine-grained local SA and may overlook the local and global interactions [18,24]. As reported by [20], even pre-trained with a massive amount of medical data using self-supervised learning, the performance of prostate segmentation task using high-resolution and better soft tissue contrast MRI images has not been completely satisfactory, not to mention the lower-quality CT images. Additionally, the unclear boundary of the prostate in CT images derived from the low soft tissue contrast is not properly addressed [7,22].Recently, Focal Transformer [24] is proposed for general computer vision tasks, in which focal self-attention is leveraged to incorporate both fine-grained local and coarse-grained global interactions. Each token attends its closest surrounding tokens with fine granularity, and the tokens far away with coarse granularity; thus, focal SA can capture both short-and long-range visual dependencies efficiently and effectively. Inspired by this work, we propose the FocalUNETR (Focal U-NEt TRansformers), a novel focal transformer architecture for CTbased medical image segmentation (Fig. 1A). Even though prior works such as Psi-Net [15] incorporates additional decoders to enhance boundary detection and distance map estimation, they either lack the capacity for effective global context capture through FCN-based techniques or overlook the significance of considering the randomness of the boundary, particularly in poor soft tissue contrast CT images for prostate segmentation. In contrast, our approach utilizes a multi-task learning strategy that leverages a Gaussian kernel over the boundary of the ground truth segmentation mask [11] as an auxiliary boundary-aware contour regression task (Fig. 1B). This serves as a regularization term for the main task of generating the segmentation mask. And the auxiliary task enhances the model's generalizability by addressing the challenge of unclear boundaries in low-contrast CT images. In this paper, we make several new contributions. First, we develop a novel focal transformer model (FocalUNETR) for CT-based prostate segmentation, which makes use of focal SA to hierarchically learn the feature maps accounting for both short-and long-range visual dependencies efficiently and effectively. Second, we also address the challenge of unclear boundaries specific to CT images by incorporating an auxiliary task of contour regression. Third, our methodology advances state-of-the-art performance via extensive experiments on both realworld and benchmark datasets."
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,2,Methods,
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,2.1,FocalUNETR,"Our FocalUNETR architecture (Fig. 1) follows a multi-scale design similar to [6,20], enabling us to obtain hierarchical feature maps at different stages. The input medical image X ∈ R C×H×W is first split into a sequence of tokens with dimension H H × W W , where H, W represent spatial height and width, respectively, and C represents the number of channels. These tokens are then projected into an embedding space of dimension D using a patch of resolution (H , W ). The SA is computed at two focal levels [24]: fine-grained and coarse-grained, as illustrated in Fig. 2A. The focal SA attends to fine-grained tokens locally, while summarized tokens are attended to globally (reducing computational cost). We perform focal SA at the window level, where a feature map of x ∈ R d×H ×W with spatial size H × W and d channels is partitioned into a grid of windows with size s w × s w . For each window, we extract its surroundings using focal SA.For window-wise focal SA [24], there are three terms {L, s w , s r }. Focal level L is the number of granularity levels for which we extract the tokens for our focal SA. We present an example, depicted in Fig. 2B, that illustrates the use of two focal levels (fine and coarse) for capturing the interaction of local and global context for optimal boundary-matching between the prediction and the ground truth for prostate segmentation. Focal window size s l w is the size of the sub-window on which we get the summarized tokens at level l ∈ {1, . . . , L}. Focal region size s l r is the number of sub-windows horizontally and vertically in attended regions at level l. The focal SA module proceeds in two main steps, subwindow pooling and attention computation. In the sub-window pooling step, an input feature map x ∈ R d×H ×W is split into a grid of sub-windows with size {s l w , s l w }, followed by a simple linear layer f l p to pool the sub-windows spatially. The pooled feature maps at different levels l provide rich information at both fine-grained and coarse-grained, where)×(s l w ×s l w ) . After obtaining the pooled feature mapsx l L 1 , we calculate the query at the first level and key and value for all levels using three linear projection layers f q , f k , and f v :For the queries inside the i-th window Q i ∈ R d×sw×sw , we extract the s l r × s l r keys and values from K l and V l around the window where the query lies in and then gather the keys and values from all L to obtainFinally, a relative position bias is added to compute the focal SA forwhere B = {B l } L 1 is the learnable relative position bias [24]. The encoder utilizes a patch size of 2×2 with a feature dimension of 2×2×1 = 4 (i.e., a single input channel CT) and a D-dimensional embedding space. The overall architecture of the encoder comprises four stages of focal transformer blocks, with a patch merging layer applied between each stage to reduce the resolution by a factor of 2. We utilize an FCN-based decoder (Fig. 1A) with skip connections to connect to the encoder at each resolution to construct a ""Ushaped"" architecture for our CT-based prostate segmentation task. The output of the encoder is concatenated with processed input volume features and fed into a residual block. A final 1 × 1 convolutional layer with a suitable activation function, such as Softmax, is applied to obtain the required number of class-based probabilities."
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,2.2,The Auxiliary Task,"For the main task of mask prediction (as illustrated in Fig. 1A), a combination of Dice loss and Cross-Entropy loss is employed to evaluate the concordance of the predicted mask and the ground truth on a pixel-wise level. The objective function for the segmentation head is given by: L seg = L dice (p i , G) + L ce (p i , G), where pi represents the predicted probabilities from the main task and G represents the ground truth mask, both given an input image i. The predicted probabilities, pi , are derived from the main task through the application of the FocalUNETR model to the input CT image.To address the challenge of unclear boundaries in CT-based prostate segmentation, an auxiliary task is introduced for the purpose of predicting boundaryaware contours to assist the main prostate segmentation task. This auxiliary task is achieved by attaching another convolution head after the extracted feature maps at the final stage (see Fig. 1B). The boundary-aware contour, or the induced boundary-sensitive label, is generated by considering pixels near the boundary of the prostate mask. To do this, the contour points and their surrounding pixels are formulated into a Gaussian distribution using a kernel with a fixed standard deviation of σ (in this specific case, e.g., σ = 1.6) [7,11,13]. The resulting contour is a heatmap in the form of a Heatsum function [11]. We predict this heatmap with a regression task trained by minimizing mean-squared error instead of treating it as a single-pixel boundary segmentation problem. Given the ground truth of contour G C i , induced from the segmentation mask for input image i, and the reconstructed output probability pC i , we use the following loss function:where N is the total number of images for each batch. This auxiliary task is trained concurrently with the main segmentation task.A multi-task learning approach is adopted to regularize the main segmentation task through the auxiliary boundary prediction task. The overall loss function is a combination of L seg and L reg : L tol = λ 1 L seg + λ 2 L reg , where λ 1 and λ 2 are hyper-parameters that weigh the contribution of the mask prediction loss and contour regression loss, respectively, to the overall loss. The optimal setting of λ 1 = λ 2 = 0.5 is determined by trying different settings."
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,3,Experiments and Results,
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,3.1,Datasets and Implementation Details,"To evaluate our method, we use a large private dataset with 400 CT scans and a large public dataset with 300 CT scans (AMOS [9]). As far as we know, the AMOS dataset is the only publicly available CT dataset including prostate ground truth. We randomly split the private dataset with 280 scans for training, 40 for validation, and 80 for testing. The AMOS dataset has 200 scans for training and 100 for testing [9]. Although the AMOS dataset includes the prostate class, it mixes the prostate (in males) and the uterus (in females) into one single class labeled PRO/UTE. We filter out CT scans missing the PRO/UTE ground-truth segmentation.Regarding the architecture, we follow the hyperparameter settings suggested in [24], with 2 focal levels, transformer blocks of depths [2,2,6,2], and head numbers [4,8,16,32] for each of the four stages. We then create FocalUNETR-S and FocalUNETR-B with D as 48 and 64, respectively. These settings have 27.3 M and 48.3 M parameters, which are comparable to other state-of-the-art models in size.For the implementation, we utilize a server equipped with 8 Nvidia A100 GPUs, each with 40 GB of memory. All experiments are conducted in PyTorch, and each model is trained on a single GPU. We interpolate all CT scans into an isotropic voxel spacing of [1.0 × 1.0 × 1.5] mm for both datasets. Houndsfield unit (HU) range of [-50, 150] is used and normalized to [0, 1]. Subsequently, each CT scan is cropped to a 128 × 128 × 64 voxel patch around the prostate area, which is used as input for 3D models. For 2D models, we first slice each voxel patch in the axial direction into 64 slices of 128 × 128 images for training and stack them back for evaluation. For the private dataset, we train models for 200 epochs using the AdamW optimizer with an initial learning rate of 5e -4 . An exponential learning rate scheduler with a warmup of 5 epochs is applied to the optimizer. The batch size is set to 24 for 2D models and 1 for 3D models. We use random flip, rotation, and intensity scaling as augmentation transforms with probabilities of 0.1, 0.1, and 0.2, respectively. We also tried using 10% percent of AMOS training set as validation data to find a better training parameter setting and re-trained the model with the full training set. However, we did not get improved performance compared with directly applying the training parameters learned from tuning the private dataset. We report the Dice Similarity Coefficient (DSC, %), 95% percentile Hausdorff Distance (HD, mm), and Average Symmetric Surface Distance (ASSD, mm) metrics."
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,3.2,Experiments,"Comparison with State-of-the-Art Methods. To demonstrate the effectiveness of FocalUNETR, we compare the CT-based prostate segmentation performance with three 2D U-Net-based methods: U-Net [19], UNet++ [26], and Attention U-Net (AttUNet) [16], two 2D transformer-based segmentation methods: TransUNet [2] and Swin-UNet [1], two 3D U-Net-based methods: U-Net (3D) [3] and V-Net [14], and two 3D transformer-based models: UNETR [6] and SiwnUNETR [20]. nnUNet [8] is used for comparison as well. Both 2D and 3D models are included as there is no conclusive evidence for which type is better for this task [22]. All methods (except nnUNet) follow the same settings as FocalUNETR and are trained from scratch. TransUNet and Swin-UNet are the only methods that are pre-trained on ImageNet. Detailed information regarding the number of parameters, FLOPs, and average inference time can be found in the supplementary materials.Quantitative results are presented in Table 1, which shows that the proposed FocalUNETR, even without co-training, outperforms other FCN and Transformer baselines (2D and 3D) in both datasets for most of the metrics. The AMOS dataset mixes the prostate(males)/uterus(females, a relatively small portion). The morphology of the prostate and uterus is significantly different. Consequently, the models may struggle to provide accurate predictions for this specific portion of the uterus. Thus, the overall performance of FocalUNETR is overshadowed by this challenge, resulting in only moderate improvement over the baselines on the AMOS dataset. However, the performance margin significantly improves when using the real-world (private) dataset. When co-trained with the auxiliary contour regression task using the multi-task training strategy, the performance of FocalUNETRs is further improved. In summary, these observations indicate that incorporating FocalUNETR and multi-task training Qualitative results of several representative methods are visualized in Fig. 3. The figure shows that our FocalUNETR-B and FocalUNETR-B* generate more accurate segmentation results that are more consistent with the ground truth than the results of the baseline models. All methods perform well for relatively easy cases (1 st row in Fig. 3), but the FocalUNETRs outperform the other methods. For more challenging cases (rows 2-4 in Fig. 3), such as unclear boundaries and mixed PRO/UTE labels, FocalUNETRs still perform better than other methods. Additionally, the FocalUNETRs are less likely to produce false positives (see more in supplementary materials) for CT images without a foreground ground truth, due to the focal SA mechanism that enables the model to capture global context and helps to identify the correct boundary and shape of the prostate. Overall, the FocalUNETRs demonstrate improved segmentation capabilities while preserving shapes more precisely, making them promising tools for clinical applications. Ablation Study. To better examine the efficacy of the auxiliary task for FocalUNETR, we selected different settings of λ 1 and λ 2 for the overall loss function L tol on the private dataset. The results (Table 2) indicate that as the value of λ 2 is gradually increased and that of λ 1 is correspondingly decreased (thereby increasing the relative importance of the auxiliary contour regression task), segmentation performance initially improves. However, as the ratio of contour information to segmentation mask information becomes too unbalanced, performance begins to decline. Thus, it can be inferred that the optimal setting for these parameters is when both λ 1 and λ 2 are set to 0.5."
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,4,Conclusion,"In summary, the proposed FocalUNETR architecture has demonstrated the ability to effectively capture local visual features and global contexts in CT images by utilizing the focal self-attention mechanism. The auxiliary contour regression task has also been shown to improve the segmentation performance for unclear boundary issues in low-contrast CT images. Extensive experiments on two large CT datasets have shown that the FocalUNETR outperforms state-ofthe-art methods for the prostate segmentation task. Future work includes the evaluation of other organs and extending the focal self-attention mechanism for 3D inputs."
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,,Fig. 1 .,
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,,Fig. 2 .,
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,,Fig. 3 .,
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,,Table 1 .,
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,,.23 (1.16) 4.85 (1.05) 1.81 (0.21) 83.79 (1.97) 8.31 (1.45),
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,,Table 2 .,
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,1,Introduction,"Although deep learning has achieved great success in various computer vision tasks [11], the requirement of large amounts of labeled data limits its application in Y. Zhang and D. Lu-Contributed equally. Fig. 1. Overview of the proposed framework incorporated into the DPA method [15]. Two classification constraints (organ and modality), and a center constraint are applied on the disentangled latent representation in addition to the original loss(es) of the baseline model.the field of medical image analysis. Annotated abnormal images are difficult to acquire, especially for rare or new diseases, such as the COVID-19, whereas normal images are much easier to obtain. Therefore, many efforts [13][14][15][16] have been made on deep learning based medical anomaly detection, which aims to learn the distribution of normal patterns from healthy subjects and detect the anomalous ones as outliers.Due to the absence of anomalous subjects, most previous studies adopted the encoder-decoder structure or generative adversarial network (GAN) as backbone to obtain image reconstruction error as the metric for recognition of outliers [5,13,20]. Other approaches [14][15][16] learned the normal distribution 1 along with the decision boundaries to differentiate anomalous subjects from normal ones. Despite the improvement they achieved over traditional one-class classification methods [1,9,12], all these works committed to train a specific network to detect the anomalies of each organ, neglecting the intrinsic similarity among different organs. We hypothesize that there are underlying patterns in the normal images of various organs and modalities despite their seemly different appearance, and a model, which can fully exploit their latent information, not only has better generalization ability to recognize the anomalies within different organs/modalities, but also can achieve superior performance for detecting the anomalies of each of them.To this end, we propose a novel model-agnostic framework, denoted as Multi-Anomaly Detection with Disentangled Representation (MADDR), for the simultaneous detection of anomalous images within different organs and modalities. As displayed in Fig. 1, to fully explore the underlying patterns of normal images as well as bridge the appearance gap among different organs and modalities, the latent representation z is disentangled into three parts, i.e., two categorical parts (z o and z m , corresponding to organ and modality, respectively) and a continuous variable (z c ). The first two parts represent the categorical information for the distinction of specific organs and modalities, respectively, while the last part denotes the feature representation for characterizing each individual subject. Specifically, we propose to impose an organ classification constraint (L o ) as well as a modality classification constraint (L m ) on the categorical parts to leverage the categorical information, and a center constraint on the continuous variable part to compact the feature representation of the normal distribution so that the outliers can be easier to identify. It is worth mentioning that the categorical label of medical images are easy to obtain because such information should be recorded during image acquisition, and with the disentanglement strategy, the potential contradiction between the classification constraint (which aims to separate images of different organs and different modalities) and the center constraint (which tends to compact the feature representation) can be avoided. Our contributions can be summarized as follows:-To the best of our knowledge, this is the first study to detect the anomalies of multiple organs and modalities with a single network. We show that introducing images from different organs and modalities with the proposed framework not only extends the generalization ability of the network towards the recognition of the anomalies within various data, but also improves its performance on each single kind. -We propose to disentangle the latent representation into three parts so that the categorical information can be fully exploited through the classification constraints, and the feature representation of normal images is tightly clustered with the center constraint for better identification of anomalous pattern. -Extensive experiments demonstrate the superiority of the proposed framework regarding the medical anomaly detection task, as well as its universal applicability to various baseline models. Moreover, the effectiveness of each component is evaluated and discussed with thorough ablation study."
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2,Methodology,"Unlike previous approaches which trained a separate model to capture the anomalies for each individual organ and modality, in this study we aim to exploit the normal images of multiple organs and modalities to train a generic network towards better anomaly detection performance for each of them. The proposed framework is model-agnostic and can be readily applied to most standard anomaly detection methods. For demonstration, we adopt four state-of-the-art anomaly detection methods, i.e., deep perceptual autoencoder (DPA) [15], memory-augmented autoencoder (MemAE) [4], generative adversarial networks based anomaly detection (GANomaly) [2], and fast unsupervised anomaly detection with generative adversarial networks (f-AnoGAN) [13] as baseline methods.In this section, we present the proposed universal framework for the anomaly detection task of medical images in details."
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2.1,Framework Overview,"We first formulate the anomaly detection task for images with various organs and modalities. For a dataset targeting a specific organ k (k ∈ [0, K]) and modality l (l ∈ [0, L]), there is a training dataset D k,l with only normal images and a test set D t k,l with both normal and abnormal images. In this study, we use N k to represent total number of images targeting organ k and N l to represent total number of images belong to modality l. The goal of our study is to train a generic model with these multi-organ and multi-modality normal images to capture the intrinsic normal distribution of training sets, such that the anomalies in test sets can be recognized as outliers.To better elaborate the process of applying our MADDR framework on standard anomaly detection methods, we first briefly introduce the workflow of a baseline method, DPA [15]. As shown in Fig. 1, the network of DPA consists of an autoencoder and a pre-trained feature extractor. Through autoencoder, the images are encoded into latent representations and then reconstructed into the original image space. The relative perceptual loss is adopted as the objective function for the optimization of autoencoder and the measurement of anomaly, which is defined as, wherex and x denote the input image and the reconstructed one, respectively. f (x) = f (x)-μ σ is the normalized feature with mean μ and standard deviation σ pre-calculated on a large dataset, where f (•) represents the mapping function of the pre-trained feature extractor. By comparing the features of original images and the reconstructed ones through the relative perceptual loss, the subjects with loss larger than the threshold are recognized as abnormal ones. To fully exploit the underlying patterns in the normal images of various organs and modalities, we incorporate additional constraints on the encoded latent representations.Specifically, our MADDR approach encourages the model to convert the input image x into a latent representation z, which consists of disentangled category and individuality information. To be more precise, the encoded latent representation z is decomposed into three parts, i.e., the organ category part z o , the modality category part z m and the continuous variable part z c . Here, z o and z m represent the categorical information (which is later converted into the probabilities of x belonging to each organ and modality through two separate fully-connected layers), and z c denotes the feature representation for characterizing each individual image (which should be trained to follow the distribution of normal images). Leveraging the recorded categorical information of the images, we impose two classification constraints on z o and z m , respectively, along with a center constraint on z c to compact the cluster of feature representation in addition to the original loss(es) of the baseline methods. In this study, we evaluate the proposed model-agnostic framework on four cutting-edge methods [2,4,13,15] with their networks as baseline models, and their original losses along with the proposed constraints as the training objective functions. If we use L b , L o , L m and L c to represent the loss of the baseline method, the organ classification constraint, the modality classification constraint and the center constraint, respectively, the overall loss function of the proposed framework can be formulated as:where X and Y denote the set of images and labels, respectively, while λ 1 , λ 2 and λ 3 are the weights to balance different losses, and set to 1 in this study (results of exploratory experiment are displayed in the supplementary material)."
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2.2,Organ and Modality Classification Constraints,"Benefiting from the acquisition procedure of medical images, the target organ and modality of each scan should be recorded and can be readily used in our study. Based on the assumption that a related task could provide auxiliary guidance for network training towards superior performance regrading the original task, we introduce two additional classification constraints (organ and modality classification) to fully exploit such categorical information. Through an additional organ classifier (a fully-connected layer), the organ classification constraint is applied on the transformed category representation z o by distinguishing different organs. A similar constraint is also applied on the modality representation z m . Considering the potential data imbalance issue among images of different organs and modalities, we adopt the focal loss [7] as the classification constraints by adding a modulating factor to the cross entropy loss. Using z i o to represent the organ category representation of image i, the organ classification loss can be formulated as:where α k o denotes the weight to balance the impact of different organs and P k (z i o ) represents the probability of image i belonging to class k. The focusing parameter γ can reduce the contribution of easy samples to the loss function and extend the range of loss values for comparison.Similarity, with the modality category representation z i m , the modality classification loss can be written as:"
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2.3,Center Constraint,"Intuitively, the desired quality of a representation is to have similar feature embeddings for images of the same class. Because in the anomaly detection task, all the images used for training belong to the same normal group, we impose a center constraint so that the features from the normal images are tightly clustered to the center and the encoded features of abnormal images lying far from the normal cluster are easy to identify. However, directly compacting the latent representation into a cluster is potentially contradictory to the organ and modality classification tasks which aim to separate different organs and modalities. To avoid the contradiction, we propose to impose the center constraint only on the continuous variable part z c with Euclidean distance as the measurement of the compactness. Similar to [10], if we use z i c to represent the continuous variable representation of image i, the measurement of compactness can be defined as:where L b represents the number of images in batch b, B denotes the number of batches andis the mean of the rest images in the same batch as image i."
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,2.4,Optimization and Inference,"To demonstrate the effectiveness of the proposed framework, we inherit the network architectures and most procedures of the baseline methods for a fair comparison. During the optimization stage, there are only two differences: 1) we introduce three additional losses as stated above; 2) considering the limited number of images, mixup [17] is applied for data augmentation to better bridge the gap among different organs and modalities. For other factors, such as optimization algorithm and related hyperparameters, we follow the original settings of the baseline methods. During inference, the same metrics of the baseline methods are adopted to measure the anomaly scores. For the details about the baseline methods, please refer to their original studies [2,4,13,15]."
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,3,Experiments,
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,3.1,Experimental Setting,"We evaluate our method on three benchmark datasets targeting various organs and modalities as stated below.-The LiTS-CT dataset [3] consists of 3D volumetric data with rare healthy subjects.To ensure a sufficient amount of training data, we remove the 2D abnormal slices from some patients and use the rest normal slices for training. Therefore, 186 axial CT slices from 77 3D volumes are used as normal data for training, 192 normal and 105 abnormal slices from 27 3D volumes for validation, and 164 normal and 249 abnormal slices from the rest 27 3D volumes for testing. -The Lung-X-rays [6] is a small dataset consists of 2D frontal chest X-ray images primarily from a hospital clinical routine. Following the same data split protocol provided by [19], the images are divided into three groups for training (228 normal images), validation (33 normal and 34 abnormal images) and testing (65 normal and 67 abnormal images). -The Lung-CT [18] contains 2D slices regarding COVID-19. An official data split is provided, which contains 234 normal images for training, 58 normal and 60 abnormal images for validation, and 105 normal and 98 abnormal images for testing.To evaluate the performance of proposed framework, we adopt three widely used metrics, including the area under the curve (AUC) of the receiver operating characteristic, F1-score and accuracy (ACC). Grid search is performed to find the optimal threshold based on the F1-score of the validation set. For all three metrics, a higher score implies better performance. The experiments are repeated three times with different random seeds to verify the robustness of the framework and provide more reliable results.The framework is implemented with PyTorch 1.4 toolbox [8] using an NVIDIA Titan X GPU. As detailed in the supplementary material, we keep most parameters the same as the baseline methods, except for the original dimension of DPA's latent feature, which is increased from 16 to 128 to ensure sufficient model capacity for valid latent representation disentanglement of each image. For the hyper-parameters of focal loss, both α k o and α l m are set as 0.25, and γ is set to 2. For preprocessing, we first "
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,3.2,Comparison Study,"The quantitative results of the proposed framework and the state-of-the-art methods are displayed in Table 1. In four experiments with different baseline methods, we can observe significant improvement in all three metrics on various organs and modalities, demonstrating that the proposed framework can effectively boost the anomaly detection performance regardless the baseline approaches. In addition, the baseline methods need to train separate networks for different organs and modalities, while with the proposed framework, a generic network can be applied to recognize the abnormalities of all datasets. In the supplementary material, we further present the t-SNE visualization of the continuous variable part of the latent representation to show that introducing the proposed MADDR framework can deliver obviously more tightly compacted normal distributions, leading to better identification of the abnormal outliers."
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,3.3,Ablation Study,"To further demonstrate the effectiveness of each component, we perform an ablation study with the following variants: 1) the baseline DPA method (the dimensions of z is 16) which trains networks for each organ and modality separately; 2) the baseline DPA method with the dimensions of z increased to 128; 3) using the images of LiTS, Lung-CT and Lung-X-rays datasets together to train the same DPA network; The results of all these variants are displayed in Table 2. As shown in variant 3, directly introducing more datasets for network training does not necessarily improve the performance on each dataset, due to the interference of organ and modality information. However, with the additional classification constraints, the organ and modality information can be effectively separated from the latent representation of normal distribution, such that better anomaly detection performance can be achieved for each dataset, as shown in variants 4, 7 and 10. Furthermore, variants 5, 8 and 11 show that the center constraint can tightly compact the feature representation of normal distribution so that abnormal outliers can be easier to identify. Last but not the least, the proposed feature split strategy can help disentangle the characteristic of normal distribution from classification information, as demonstrated by variants 6, 9 and 12. For the impact of hyper-parameters, including the weights for different loss terms and the dimensions of the disentangled category representations, please refer to the supplementary material."
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,4,Conclusion,"Unlike previous studies which committed to train exclusive networks to recognize the anomalies of specific organs and modalities separately, in this work we hypothesized that normal images of various organs and modalities could be combined and utilized to train a generic network and superior performance could be achieved for the recognition of each type of anomaly with proper methodology. With the proposed model-agnostic framework, the organ/modality classification constraint and the center constraint were imposed on the disentangled latent representation to fully utilize the available information as well as improve the compactness of representation to facilitate the identification of outliers. Four state-of-the-art methods were adopted as baseline models for thorough evaluation, and the results on various organs and modalities demonstrated the validity of our hypothesis as well as the effectiveness of the proposed framework."
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,,,
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,,,
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,,Table 1 .,
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,,Table 2 .,
A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_23.
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,1,Introduction,"Supervised learning for medical image segmentation requires a large amount of per-voxel annotated data [4,8,9,17,19]. Since both expertise and time are needed to produce accurate contouring annotations, the labelled data are very expensive to acquire, especially in 3D volumetric images [18] such as MRI. Semi-supervised medical image segmentation becomes an important topic in recent years, where costly per-voxel annotations are available for a subset of training data. In this study, we focus on semi-supervised LA segmentation by exploring both labelled and unlabelled data.Consistency regularization methods are widely studied in semi-supervised segmentation models. Consistent predictions are enforced by perturbing input images [11,23], learned features [14], and networks [3,20,22,24]. Other consistency-based methods adopt adversarial losses to learn consistent geometric representations in the dataset [10,26], enforcing local and global structural consistency [6], and building task-level regularization [12]. Among these methods, initialization perturbation [3] combined with entropy minimization [5] demonstrates outstanding performances. These methods [3,22] require two segmentation networks/streams with different initialization to be consistent between the two predictions by pseudo labeling/sharpening from the other network/stream. However, entropy minimization [5] based methods [3,22] give up a great amount of information contained in network predictions, forcing networks to agree with each other even in ambiguous regions. But such cross guidance on ambiguous regions may be meaningless and unreliable [24]. More concretely, many parts of the target in medical images can be extremely confusing. E.g., some boundaries like the outer branches, can even confuse radiologists. In this case, it may be difficult to train two reliable classifiers to simultaneously distinguish the confusing foreground from the background by entropy minimizationbased methods. This is because the penalties for misclassifications on the confusing region and the confident region are equal. Meanwhile, it also makes networks inevitably plagued with confirmation bias [2]. In the early optimization stage, the pseudo labels are not stable. Thus, as the training process goes on, the two segmentation networks are prone to overfit the erroneous pseudo labels.Motivated by Knowledge Distillation (KD) [7], we propose Deep Mutual Distillation (DMD), advocating to generalize the original Deep Mutual Learning (DML) [25] by introducing temperature scaling, and reformulate a symmetric online mutual distillation process to combat the clear drawback in entropy minimization [5] under medical image tasks. With the temperature scaling, the highentropy distilled probabilities are more informative than low-entropy sharpened probabilities, therefore offering more meaningful mutual guidance, especially on ambiguous regions. Furthermore, due to the class imbalance problem in medical images, i.e., targets are usually very small compared with the whole volume, we exploit the Dice loss [1] as the consistency regularization to supervise the mutual distillation of two networks. To the best of our knowledge, KD [7] is overlooked in the semi-supervised medical image segmentation field. Our DMD is conceptually simple yet computationally efficient. Experiments on MICCAI 2018 Atrial Segmentation Challenge and ACDC datasets show that DMD works favorably especially when annotated data is very small. Without bells and whistles, DMD achieves 89.70% in terms of Dice score on LA when only 10% training data are labelled, with a significant 1.15% improvement compared with state-of-the-arts. "
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,2,Method,
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,2.1,Overview,"As shown in Fig. 1, we illustrate some consistency-based methods with different entropy guidance. From Fig. 1 (a) to (c), the entropy used as guidance for network learning is increasing. In Cross Pseudo Supervision (CPS) [3], the hard pseudo segmentation map is used as guidance to supervise the other segmentation network. In DML [25], a two-way KL mimicry loss is applied directly to the probability distribution learned by the softmax layer. In our proposed DMD, we generalize DML [25] by introducing a temperature scaling strategy and further increasing the entropy of the probability distribution. Considering the class imbalance between foreground and background pixels under medical image segmentation tasks [16], we design a Dice [1]-based distillation loss."
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,2.2,Deep Mutual Learning,"The original DML [25] deals with the standard M-class classification problem. Given two initialization perturbed networks f θj , j ∈ {1, 2}, we obtain their raw logit predictions on the same sample point x i ∈ X in parallel as z m j = f θj (x i ) for class m. The probability of class m from f θj is given by standard softmax function:The critical part of mutual learning contains a 2-way KL mimicry loss:where L ml is obtained on both labelled and unlabelled sample points, p 1 , p 2 being posterior probability predictions of corresponding networks. Together with standard supervised loss obtained on labelled sample points, and the trade-off weight λ, we get the final DML [25] objective:where λ is set to 1 in DML [25]."
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,2.3,Deep Mutual Distillation,"The original p m j of DML [25] can be considered as a special case of online knowledge distillation [7] with temperature T set to 1, where each network serves as both teacher and student symmetrically. However, T = 1 makes a great amount of information from both networks still masked within p m j . Therefore, we generalize DML [25] to Deep Mutual Distillation(DMD) by setting T greater than 1 as in KD [7]. In DMD, the distilled probability p m j,T is obtained by:In the case of the binary segmentation task, we replace softmax with sigmoid to get the distilled per-pixel probability mask p j,T from f θj :However, using KL-divergence-based loss in KD [7] cannot handle class imbalance between foreground and background pixels [16]. Hence, we replace the original 2-way KL-divergence mimicry loss with Dice loss [1] to alleviate this problem, obtaining our new distillation loss:Together with standard supervised loss obtained on labelled sample points, and the trade-off weight λ, we get our final DMD objective:where L distill is obtained on both labelled and unlabelled sample points. Here, we also adopt Dice loss [1] for L sup , under the context of highly class imbalanced medical image segmentation tasks [1].With the temperature scaling, each network under DMD learns from each other through the distilled high-entropy probabilities, which are more informative, especially on ambiguous regions. The distillation [7] also makes p j,T become soft labels [7], which reduces the influence of confirmation bias [2] throughout the training process. Both advantages make DMD outperforms current state-of-theart methods. We also carry out comprehensive ablation studies to demonstrate the effectiveness of DMD design in Sect. 3.3. "
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,3,Experiments and Results,
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,3.1,Experimental Setup,"Dataset: We evaluated our proposed DMD on the 2018 Atria Segmentation Challenge (LA) 1 , which provides a 80/20 split for training/validation on 3D MR imaging scans and corresponding LA segmentation mask, with an isotropic resolution of 0.625×0.625×0.625mm 3 . We also extended our experiments on the Automated Cardiac Diagnosis Challenge (ACDC) 2 . We report the performance on the validation set, following the same settings from previous methods [6,10,[20][21][22]24,26] for fair comparisons."
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Evaluation Metric:,"The performance of our method is quantitatively evaluated in terms of Dice, Jaccard, the average surface distance (ASD), and the 95% Hausdorff Distance (95HD) as previous methods [6,10,[20][21][22]24,26].Implementation Details: We implement DMD using PyTorch [15]. We adopt VNet [1] as the backbone for both of the segmentation networks. We first randomly initialize two networks, then we train both networks under the scheme of DMD using SGD optimizer for 6k iterations simultaneously, with an initial learning rate (LR) 0.01 decayed by 0.1 every 2.5k iterations following [22]. Other data pre-processing and augmentation details are kept the same as [22]. For other hyper-parameters in DMD, we set the trade-off weight λ to 4, and the temperature T to 2/1.93 for the 8/16 label scenario for the best performance. It is interesting to observe that with less labelled data, T is prone to be set to a bigger value since less labelled training data will usually lead to more ambiguous regions. After training, we only use one network for generating results for evaluation, without using any ensembling methods. "
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,3.2,Quantitative Evaluation,"We compare DMD with previous state-of-the-arts [6,10,[20][21][22]24,26], following the measurements from MC-Net [22]. Table 1 shows that our method outperforms state-of-the-art methods with a significant improvement over 8 label scenarios under all 4 metrics, and achieves state-of-the-art on the 16 label scenario under almost all metrics on LA dataset. We do not compare the performance of SS-Net [21] on 16 labels as SS-Net [21] does not report this. We can see that even with an extremely small amount of labelled samples, networks in DMD are still able to formulate a certain representation of the unlabelled data and transfer such meaningful knowledge via high-entropy probabilities with each other by the efficient distilling process. Distillation is more informative and greatly benefits training on complex medical images with confusing regions. We further extended our experiments on the ACDC dataset shown in Table 2.To study how consistency-based methods with different entropy guidance affect performances, we implement CPS [3] and DML [25] for LA segmentation. From Table 3, we can see general improvements from low-entropy methods to high-entropy methods from CPS [3] to our proposed DMD (from top to bottom in the first column in Table 3). In these entropy minimization methods, i.e., CPS [3] and MC-Net [22], networks are forced to assign sharpened labels on all parts of unlabelled samples, including ambiguous areas. Thus, networks are forced to be exposed to the risk of confirmation bias [2] of each other, which limits their performances. In Sect. 3.3, we further study how the temperature T affects DMD performances, where T controls the entropy in DMD guidance. Furthermore, we show in Fig. 2 our method can lead to better pseudo-labels when the training process is going on, compared with entropy-minimization methods like CPS. Besides, we provide the gradient visualization for L distill on an unlabelled sample point in Fig. 3(a). We can see that when using Dice [1] for distillation, the gradient is enhanced more on the foreground, especially on the boundary of the object predicted by the segmentation network than 2-way KL-divergence. We can also see that using Dice [1], the segmentation network better captures the shape of the object, thus providing better guidance for the other network than using 2-way KL-divergence."
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,3.3,Parameter Analysis,"Here, we first demonstrate the effectiveness of temperature scaling T and the choice of KL-divergence-based and Dice [1]-based L distill on LA with 8 labelled data. In order to do so, we conduct independent experiments to study the influence of T for each choice of L distill . For a fair comparison, we choose different trade-off weights λ in Eq. 7 for each choice to get the corresponding best performance, denoted as λ dice and λ KL , where we set λ dice = 1 and λ KL = 4. Figure 4 shows how T affects DMD performances on the validation set, with corresponding fixed λ dice and λ KL . Experiments show that slightly higher T improves over the performance, and we can also see that Dice loss [1] outperforms 2-way KL-divergence loss under various T . It is interesting to observe that when T increases, the performance decreases by using the KL-divergence loss. We suspect that due to the complex background context and class imbalance problem, the learning of two networks is heavily influenced by the background noise. We also provide an ablation study on the influence of trade-off weight λ in Fig. 3(b), where we set T = 2, and choose Dice for L distill . Then, to see how the performance changes w.r.t. λ, we vary λ and fix T = 2. As shown in Fig. 3(a), the performance is not sensitive within the range of λ ∈ [3.5, 5].  "
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,4,Conclusions,"We revisit Knowledge Distillation and have presented a novel and simple semisupervised medical segmentation method through Deep Mutual Distillation. We rethink and analyze consistency regularization-based methods with the entropy minimization, and point out that cross guidance with low entropy on extremely ambiguous regions may be unreliable. We hereby propose to introduce a temperature scaling strategy into the network training and propose a Dice-based distillation loss to alleviate the influence of the background noise when the temperature T > 1. Our DMD works favorably for semi-supervised medical image segmentation, especially when the number of training data is small (e.g., 10% training data are labelled in LA). Compared with all prior arts, a significant improvement up to 1.15% in the Dice score is achieved in LA dataset. Ablation studies with the consistency-based methods of different entropy guidance further verify our assumption and design."
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Fig. 1 .,
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Fig. 3 .,
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Fig. 4 .,
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Table 1 .,
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Table 2 .,
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Table 3 .,Fig. 2. Entropy-minimization methods like CPS[3] do worse in refining pseudo labels throughout the training process compared to our method. Red mask: pseudo label; White background: ground truth.
Deep Mutual Distillation for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 52.
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,1,Introduction,"Deep neural networks (DNNs) have achieved remarkable success in important areas of various domains, such as computer vision, machine learning and natural language processing. Nevertheless, there exists growing evidence that suggests that these models are poorly calibrated, leading to overconfident predictions that may assign high confidence to incorrect predictions [5,6]. This represents a major problem, as inaccurate uncertainty estimates can have severe consequences in safety-critical applications such as medical diagnosis. The underlying cause of network miscalibration is hypothesized to be the high capacity of these models, which makes them susceptible to overfitting on the negative log-likelihood loss that is conventionally used during training [6].In light of the significance of this issue, there has been a surge in popularity for quantifying the predictive uncertainty in modern DNNs. A simple approach involves a post-processing step that modifies the softmax probability predictions of an already trained network [4,6,23,24]. Despite its efficiency, this family of approaches presents important limitations, which include i) a dataset-dependency on the value of the transformation parameters and ii) a large degradation observed under distributional drifts [20]. A more principled solution integrates a term that penalizes confident output distributions into the learning objective, which explicitly maximizes the Shannon entropy of the model predictions during training [21]. Furthermore, findings from recent works on calibration [16,17] have demonstrated that popular classification losses, such as Label Smoothing (LS) [22] and Focal Loss (FL) [10], have a favorable effect on model calibration, as they implicitly integrate an entropy maximization objective. Following these works, [11,18] presented a unified view of state-of-the-art calibration approaches [10,21,22] showing that these strategies can be viewed as approximations of a linear penalty imposing equality constraints on logit distances. The associated equality constraint results in gradients that continually push towards a non-informative solution, potentially hindering the ability to achieve the optimal balance between discriminative performance and model calibration. To alleviate this limitation, [11,18] proposed a simple and flexible alternative based on inequality constraints, which imposes a controllable margin on logit distances. Despite the progress brought by these methods, none of them explicitly considers pixel relationships, which is fundamental in the context of image segmentation.Indeed, the nature of structured predictions in segmentation, involves pixelwise classification based on spatial dependencies, which limits the effectiveness of these strategies to yield performances similar to those observed in classification tasks. In particular, this potentially suboptimal performance can be attributed to the uniform (or near-to-uniform) distribution enforced on the softmax/logits distributions, which disregards the spatial context information. To address this important issue, Spatially Varying Label Smoothing (SVLS) [7] introduces a soft labeling approach that captures the structural uncertainty required in semantic segmentation. In practice, smoothing the hard-label assignment is achieved through a Gaussian kernel applied across the one-hot encoded ground truth, which results in soft class probabilities based on neighboring pixels. Nevertheless, while the reasoning behind this smoothing strategy relies on the intuition of giving an equal contribution to the central label and all surrounding labels combined, its impact on the training, from an optimization standpoint, has not been studied.The contributions of this work can be summarized as follows:-We provide a constrained-optimization perspective of Spatially Varying Label Smoothing (SVLS) [7], demonstrating that it imposes an implicit constraint on a soft class proportion of surrounding pixels. Our formulation shows that SVLS lacks a mechanism to control explicitly the importance of the constraint, which may hinder the optimization process as it becomes challenging to balance the constraint with the primary objective effectively. -Following our observations, we propose a simple and flexible solution based on equality constraints on the logit distributions. The proposed constraint is enforced with a simple linear penalty, which incorporates an explicit mechanism to control the weight of the penalty. Our approach not only offers a more efficient strategy to model the logit distributions but implicitly decreases the logit values, which results in less overconfident predictions. -Comprehensive experiments over multiple medical image segmentation benchmarks, including diverse targets and modalities, show the superiority of our method compared to state-of-the-art calibration losses."
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,2,Methodology,"Formulation. Let us denote the training dataset asΩn representing the n th image, Ω n the spatial image domain, and y (n) ∈ Y ⊂ R K its corresponding ground-truth label with K classes, provided as a one-hot encoding vector. Given an input image x (n) , a neural network parameterized by θ generates a softmax probability vector, defined aswhere s is obtained after applying the softmax function over the logits l (n) ∈ R Ωn×K . To simplify the notations, we omit sample indices, as this does not lead to any ambiguity."
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,2.1,A Constrained Optimization Perspective of SVLS,"Spatially Varying Label Smoothing (SVLS) [7] considers the surrounding class distribution of a given pixel p in the ground truth y to estimate the amount of smoothness over the one-hot label of that pixel. In particular, let us consider that we have a 2D patch x of size d 1 × d 2 and its corresponding ground truth y1 . Furthermore, the predicted softmax in a given pixel is denoted as s = [s 0 , s 1 , ..., s k-1 ]. Let us now transform the surrounding patch of the segmentation mask around a given pixel into a unidimensional vector y ∈ R d , where d = d 1 ×d 2 . SVLS employs a discrete Gaussian kernel w to obtain soft class probabilities from one-hot labels, which can also be reshaped into w ∈ R d . Following this, for a given pixel p, and a class k, SVLS [7] can be defined as:Thus, once we replace the smoothed labels ỹk p in the standard cross-entropy (CE) loss, the new learning objective becomes:where s k p is the softmax probability for the class k at pixel p (the pixel in the center of the patch). Now, this loss can be decomposed into:with p denoting the index of the pixel in the center of the patch. Note that the term in the left is the cross-entropy between the posterior softmax probability and the hard label assignment for pixel p. Furthermore, let us denotey k i w i as the soft proportion of the class k inside the patch/mask y, weighted by the filter values w. By replacing τ k into the Eq. 3, and removing | d i w i | as it multiplies both terms, the loss becomes:As τ is constant, the second term in Eq. 4 can be replaced by a Kullback-Leibler (KL) divergence, leading to the following learning objective:where c = stands for equality up to additive and/or non-negative multiplicative constant. Thus, optimizing the loss in SVLS results in minimizing the crossentropy between the hard label and the softmax probability distribution on the pixel p, while imposing the equality constraint τ = s, where τ depends on the class distribution of surrounding pixels. Indeed, this term implicitly enforces the softmax predictions to match the soft-class proportions computed around p."
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,2.2,Proposed Constrained Calibration Approach,"Our previous analysis exposes two important limitations of SVLS: 1) the importance of the implicit constraint cannot be controlled explicitly, and 2) the prior τ is derived from the σ value in the Gaussian filter, making it difficult to model properly. To alleviate this issue, we propose a simple solution, which consists in minimizing the standard cross-entropy between the softmax predictions and the one-hot encoded masks coupled with an explicit and controllable constraint on the logits l. In particular, we propose to minimize the following constrained objective:where τ now represents a desirable prior, and τ = l is a hard constraint. Note that the reasoning behind working directly on the logit space is two-fold. First, observations in [11] suggest that directly imposing the constraints on the logits results in better performance than in the softmax predictions. And second, by imposing a bounded constraint on the logits values2 , their magnitudes are further decreased, which has a favorable effect on model calibration [17]. We stress that despite both [11] and our method enforce constraints on the predicted logits, [11] is fundamentally different. In particular, [11] imposes an inequality constraint on the logit distances so that it encourages uniform-alike distributions up to a given margin, disregarding the importance of each class in a given patch. This can be important in the context of image segmentation, where the uncertainty of a given pixel may be strongly correlated with the labels assigned to its neighbors. In contrast, our solution enforces equality constraints on an adaptive prior, encouraging distributions close to class proportions in a given patch.Even though the constrained optimization problem presented in Eq. 6 could be solved by a standard Lagrangian-multiplier algorithm, we replace the hard constraint by a soft penalty of the form P(|τ -l|), transforming our constrained problem into an unconstrained one, which is easier to solve. In particular, the soft penalty P should be a continuous and differentiable function that reaches its minimum when it verifies P(|τ -l|) ≥ P(0), ∀ l ∈ R K , i.e., when the constraint is satisfied. Following this, when the constraint |τ -l| deviates from 0 the value of the penalty term increases. Thus, we can approximate the problem in Eq. 6 as the following simpler unconstrained problem:where the penalty is modeled here as a ReLU function, whose importance is controlled by the hyperparameter λ."
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,3,Experiments,
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,3.1,Setup,"Datasets. FLARE Challenge [12] contains 360 volumes of multi-organ abdomen CT with their corresponding pixel-wise masks, which are resampled to a common space and cropped to 192 × 192 × 30. ACDC Challenge [3]  Evaluation Metrics. To assess the discriminative performance of the evaluated models, we resort to standard segmentation metrics in medical segmentation, which includes the DICE coefficient (DSC) and the 95% Hausdorff Distance (HD). To evaluate the calibration performance, we employ the expected calibration error (ECE) [19] on foreground classes, as in [7], and classwise expected calibration error (CECE) [9], following [16,18] (more details in Supp. Material).Implementation Details . We benchmark the proposed model against several losses, including state-of-the-art calibration losses. These models include the compounded CE + Dice loss (CE+DSC), FL [10], Entropy penalty (ECP) [21], LS [22], SVLS [7] and MbLS [11]. Following the literature, we consider the hyperparameters values typically employed and select the value which provided the best average DSC on the validation set across all the datasets. More concretely, for FL, γ values of 1, 2, and 3 are considered, whereas 0.1, 0.2, and 0.3 are used for α and λ in LS and ECP, respectively. We consider the margins of MbLS to be 3, 5, and 10, while fixing λ to 0.1, as in [18]. In the case of SVLS, the one-hot label smoothing is performed with a kernel size of 3 and σ = [0.5, 1, 2].For training, we fixed the batch size to 16, epochs to 100, and used ADAM [8], with a learning rate of 10 -3 for the first 50 epochs, and reduced to 10 -4 afterwards. Following [18], the models are trained on 2D slices, and the evaluation is performed over 3D volumes. Last, we use the following prior τ k = d i=1 y k i , which is computed over a 3 × 3 patch, similarly to SVLS."
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,3.2,Results,"Comparison to State-of-the-Art. Table 1 reports the discriminative and calibration results achieved by the different methods. We can observe that, across all the datasets, the proposed method consistently outperforms existing approaches, always ranking first and second in all the metrics. Furthermore, while other methods may obtain better performance than the proposed approach in a single metric, their superiority strongly depends on the selected dataset. For example, ECP [21] yields very competitive performance on the FLARE dataset, whereas it ranks among the worst models in ACDC or BraTS.To have a better overview of the performance of the different methods, we follow the evaluation strategies adopted in several MICCAI Challenges, i.e., sumrank [14] and mean-case-rank [13]. As we can observe in the heatmaps provided in Fig. 1, our approach yields the best rank across all the metrics in both strategies,  clearly outperforming any other method. Interestingly, some methods such as FL or ECP typically provide well-calibrated predictions, but at the cost of degrading their discriminative performance.Ablation Studies. 1-Constraint over logits vs softmax. Recent evidence [11] suggests that imposing constraints on the logits presents a better alternative than its softmax counterpart. To demonstrate that this observation holds in our model, we present the results of our formulation when the constraint is enforced on the softmax distributions, i.e., replacing l by s (Table 2, top), which yields inferior results. 2-Choice of the penalty. To solve the unconstrained problem in Eq. 7, we can approximate the second term with a liner penalty, modeled as a ReLU function. Nevertheless, we can resort to other polynomial penalties, e.g., quadratic penalties, whose main difference stems from the more aggressive behavior of quadratic penalties over larger constraint violations. The results obtained when the linear penalty is replaced by a quadratic penalty are reported in Table 2 (middle). From these results, we can observe that, while a quadratic penalty could achieve better results in a particular dataset (e.g., ACDC or calibration performance on BraTS), a linear penalty yields more consistent results across datasets. 3-Patch size. For a fair comparison with SVLS, we used a patch of size 3 × 3 in our model. Nevertheless, we now investigate the impact of employing a larger patch to define the prior τ , whose results are presented in Table 2 (bottom). Even though a larger patch seems to bring comparable results in one dataset, the performance on the other two datasets is largely degraded, which potentially hinders its scalability to other applications. We believe that this is due to the higher degree of noise in the class distribution, particularly when multiple organs overlap, as the employed patch covers a wider region.Impact of the Prior. A benefit of the proposed formulation is that diverse priors can be enforced on the logit distributions. Thus, we now assess the impact of different priors τ in our formulation (See Supplemental Material for a detailed explanation). The results presented in Table 3 reveal that selecting a suitable prior can further improve the performance of our model. Magnitude of the Logits. To empirically demonstrate that the proposed solution decreases the logit values, we plot average logit distributions across classes on the FLARE test set (Fig. 2). In particular, we first separate all the voxels based on their ground truth labels. Then, for each category, we average the per-voxel vector of logit predictions (in absolute value). We can observe that, compared to SVLS and MbLS, -which also imposes constraints on the logits-, our approach leads to much lower logit values, particularly compared to SVLS."
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,4,Conclusion,"We have presented a constrained-optimization perspective of SVLS, which has revealed two important limitations of this method. First, the implicit constraint enforced by SVLS cannot be controlled explicitly. And second, the prior imposed in the constraint is directly derived from the Gaussian kernel used, which makes it hard to model. In light of these observations, we have proposed a simple alternative based on equality constraints on the logits, which allows to control the importance of the penalty explicitly, and the inclusion of any desirable prior in the constraint. Our results suggest that the proposed method improves the quality of the uncertainty estimates, while enhancing the segmentation performance."
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,,Fig. 1 .,
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,,Fig. 2 .,
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,,,
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,,Table 1 . Comparison to state-of-the-art.,
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,,Table 2 .,
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,,Table 3 .,
Trust Your Neighbours: Penalty-Based Constraints for Model Calibration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 55.
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,1,Introduction,"Segmentation is key in medical image analysis and is primarily achieved with pixel-wise classification neural networks [4,14,20]. Recently, methods that use contours defined by points [10,13] have been shown more suitable for organs with a regular shape (e.g. lungs, heart) while predicting the organ outline similarly to how experts label data [10,13]. While various uncertainty methods have been investigated for both pixel-wise image segmentation [6,16,29] and landmark regression [25,27], few uncertainty methods for point-defined contours in the context of segmentation exists to date. Uncertainty can be epistemic or aleatoric by nature [16]. Epistemic uncertainty models the network uncertainty by defining the network weights as a probabilistic distribution instead of a single value, with methods such as Bayesian networks [5], MC dropout [11,12] and ensembles [19]. Aleatoric uncertainty is the uncertainty in the data. Most pixel-wise segmentation methods estimate perpixel aleatoric uncertainty by modeling a normal distribution over each output logit [16]. For regression, it is common practice to assume that each predicted output is independent and identically distributed, and follows an univariate normal distribution. In that case, the mean and variance distribution parameters μ and σ are learned with a loss function that maximizes their log-likelihood [16].Other methods estimate the aleatoric uncertainty from multiple forward passes of test-time augmentation [1,29]. Some methods do not explicitly model epistemic nor aleatoric uncertainty, but rather use custom uncertainty losses [9] or add an auxiliary confidence network [8]. Other works predict uncertainty based on an encoded prior [15] or by sampling a latent representation space [3,18]. The latter however requires a dataset containing multiple annotations per image to obtain optimal results.Previous methods provide pixel-wise uncertainty estimates. These estimates are beneficial when segmenting abnormal structures that may or may not be present. However, they are less suited for measuring uncertainty on organ delineation because their presence in the image are not uncertain.In this work, we propose a novel method to estimate aleatoric uncertainty of point-wise defined contours, independent on the model's architecture, without compromising the contour estimation performance. We extend state-of-the-art point-regression networks [21] by modeling point coordinates with Gaussian and skewed-Gaussian distributions, a novel solution to predict asymmetrical uncertainty. Conversely, we demonstrate that the benefits of point-based contouring also extend to uncertainty estimation with highly interpretable results."
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,2,Method,"Let's consider a dataset made of N pairs {x i , y k i } N i=1 , each pair consisting of an image x i ∈ R H×W of height H and width W , and a series of K ordered points y k i , drawn by an expert. Each point series defines the contour of one or more organs depending on the task. A simple way of predicting these K points is to regress 2K values (x-y coordinates) with a CNN, but doing so is sub-optimal due to the loss of spatial coherence in the output flatten layer [21]. As an alternative, Nabili et al. proposed the DSNT network (differentiable spatial to numerical transform) designed to extract numerical coordinates of K points from the prediction of K heatmaps [21] (c.f. the middle plots in Fig. 1 for an illustration).Inspired by this work, our method extends to the notion of heatmaps to regress univariate, bivariate, and skew-bivariate uncertainty models."
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,2.1,Contouring Uncertainty,"Univariate Model -In this approach, a neural network f θ (•) is trained to generate K heatmaps Z k ∈ R H×W which are normalized by a softmax function so that their content represents the probability of presence of the center c k of each landmark point. Two coordinate maps I ∈ R H×W and J ∈ R H×W , where"
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,H,", are then combined to these heatmaps to regress the final position μ k and the corresponding variance (σ kx , σ k y ) of each landmark point through the following two equations:where •, • F is the Frobenius inner product, corresponds to the Hadamard product, and (σ ky ) 2 is computed similarly. Thus, for each image x i , the neural network f θ (x i ) predicts a tuple (μ i , σ i ) with μ i ∈ R 2K and σ i ∈ R 2K through the generation of K heatmaps. The network is finally trained using the following univariate aleatoric loss adapted from [16] where y k i is the k th reference landmark point of image x i . Bivariate Model -One of the limitations of the univariate model is that it assumes no x-y covariance on the regressed uncertainty. This does not hold true in many cases, because the uncertainty can be oblique and thus involve a nonzero x-y covariance. To address this, one can model the uncertainty of each point with a 2 × 2 covariance matrix, Σ, where the variances are expressed with Eq. 2 and the covariance is computed as follows:The network f θ (x i ) thus predicts a tuple (μ i , Σ i ) for each image x i , with μ i ∈ R K×2 and Σ i ∈ R K×2×2 . We propose to train f θ using a new loss function L N2 :Asymmetric Model -One limitation of the bivariate method is that it models a symmetric uncertainty, an assumption that may not hold in some cases as illustrated on the right side of Fig. 2. Therefore we developed a third approach based on a bivariate skew-normal distribution [2]:where φ n is a multivariate normal, Φ 1 is the cumulative distribution function of a unit normal, Σ = ω Σω and α ∈ R n is the skewness parameter. Note that this is a direct extension of the multivariate normal as the skew-normal distribution is equal to the normal distribution when α = 0.The corresponding network predicts a tuple (μ, Σ, α) with μ ∈ R K×2 , Σ ∈ R K×2×2 and α ∈ R K×2 . The skewness output α is predicted using a sub-network whose input is the latent space of the main network (refer to the supplementary material for an illustration). This model is trained using a new loss function derived from the maximum likelihood estimate of the skew-normal distribution:"
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,2.2,Visualization of Uncertainty,"As shown in Fig. 2, the predicted uncertainty can be pictured in two ways: (i) either by per-point covariance ellipses [left] and skewed-covariance profiles [right] or (ii) by an uncertainty map to express the probability of wrongly classifying pixels which is highest at the border between 2 classes. In our formalism, the probability of the presence of a contour (and thus the separation between 2 classes) can be represented by the component of the uncertainty that is perpendicular to the contour. We consider the perpendicular normalized marginal distribution at each point (illustrated by the green line). This distribution also happens to be a univariate normal [left] or skew-normal [right] distribution [2].From these distributions, we draw isolines of equal uncertainty on the inside and outside of the predicted contour. By aggregating multiple isolines, we construct a smooth uncertainty map along the contours (illustrated by the white-shaded areas). Please refer to the supp. material for further details on this procedure.3 Experimental Setup"
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,3.1,Data,"CAMUS. The CAMUS dataset [20] contains cardiac ultrasounds from 500 patients, for which two-chamber and four-chamber sequences were acquired.Manual annotations for the endocardium and epicardium borders of the left ventricle (LV) and the left atrium were obtained from a cardiologist for the end-diastolic (ED) and end-systolic (ES) frames. The dataset is split into 400 training patients, 50 validation patients, and 50 testing patients. Contour points were extracted by finding the basal points of the endocardium and epicardium and then the apex as the farthest points along the edge. Each contour contains 21 points.Private Cardiac US. This is a proprietary multi-site multi-vendor dataset containing 2D echocardiograms of apical two and four chambers from 890 patients. Data comes from patients diagnosed with coronary artery disease, COVID, or healthy volunteers. The dataset is split into a training/validation set (80/20) and an independent test set from different sites, comprised of 994 echocardiograms from 684 patients and 368 echocardiograms from 206 patients, respectively. The endocardium contour was labeled by experts who labeled a minimum of 7 points based on anatomical landmarks and add as many other points as necessary to define the contour. We resampled 21 points equally along the contour.JSRT. The Japanese Society of Radiological Technology (JSRT) dataset consists of 247 chest X-Rays [26]. We used the 120 points for the lungs and heart annotation made available by [10]. The set of points contains specific anatomical points for each structure (4 for the right lung, 5 for the left lung, and 4 for the heart) and equally spaced points between each anatomical point. We reconstructed the segmentation map with 3 classes (background, lungs, heart) with these points and used the same train-val-test split of 70%-10%-20% as [10]."
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,3.2,Implementation Details,"We used a network based on ENet [24] for the ultrasound data and on DeepLabV3 [7] for the JSRT dataset to derive both the segmentation maps and regress the per-landmark heatmaps. Images were all reshaped to 256 × 256 and B-Splines were fit on the predicted landmarks to represent the contours.Training was carried out with the Adam optimizer [17] with a learning rate of 1 × 10 -3 and with ample data augmentation (random rotation and translations, brightness and contrast changes, and gamma corrections). Models were trained with early stopping and the models with best validation loss were retained for testing."
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,3.3,Evaluation Metrics,To assess quality of the uncertainty estimates at image and pixel level we use:Correlation. The correlation between image uncertainty and Dice was computed using the absolute value of the Pearson correlation score. We obtained image uncertainty be taking the sum of the uncertainty map and dividing it by the number of foreground pixels.
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,Maximum Calibration Error (MCE),". This common uncertainty metric represents the probability if a classifier (here a segmentation method) of being correct by computing the worst case difference between its predicted confidence and its actual accuracy [23].Uncertainty Error Mutual-Information. As proposed in [15], uncertainty error mutual-information measures the degree of overlap between the unthresholded uncertainty map and the pixel-wise error map."
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,4,Results,"We computed uncertainty estimates for both pixel-wise segmentation and contour regression methods to validate the hypothesis that uncertainty prediction is better suited to per-landmark segmentation than per-pixel segmentation methods. For a fair comparison, we made sure the segmentation models achieve similar segmentation performance, with the average Dice being .90 ± .02 for CAMUS, .86 ± .02 for Private US., and .94 ± .02 for JSRT.For the pixel-wise segmentations, we report results of a classical aleatoric uncertainty segmentation method [16] as well as a Test Time Augmentation (TTA) method [29]. For TTA, we used the same augmentations as the ones used during training. We also computed epistemic uncertainty with MC-Dropout [6] for which we selected the best results of 10%, 25%, and 50% dropout rates. The implementation of MC-Dropout for regression was trained with the DSNT layer [21] and mean squared error as a loss function.As for the landmark prediction, since no uncertainty estimation methods have been proposed in the literature, we adapted the MC-Dropout method to it. We also report results for our method using univariate, (N 1 ), bivariate, (N 2 ) and bivariate skew-normal distributions (SN 2 ).The uncertainty maps for TTA and MC-Dropout (i.e. those generating multiple samples) were constructed by computing the pixel-wise entropy of multiple Fig. 4. Reliability diagrams [22] for the 3 datasets. For uncertainty (u) bounded by 0 and 1, confidence (c) is defined as c = 1u [28] forward passes. It was found that doing so for the aleatoric method produces better results than simply taking the variance. The uncertainty map for the landmark predictions was obtained with the method described in Sect. 2.2.Quantitative results are presented in Table 1 and qualitative results are shown in Fig. 3. As can be seen, our uncertainty estimation method is globally better than the other approaches except for the correlation score on the CAMUS dataset which is slightly larger for TTA. Furthermore, our point-based aleatoric uncertainty better detects regions of uncertainty consistently, as reflected in the Mutual Information (MI) metric. The reliability diagrams in Fig. 4 show that our method is systematically better aligned to perfect calibration (dashed line) for all datasets, which explains why our method has a lower MCE. With the exception of the Private Cardiac US dataset, the skewed normal distribution model shows very similar or improved results for both correlation and mutual information compared to the univariate and bivariate models. It can be noted, however, that in specific instances, the asymmetric model performs better on Private Cardiac US dataset (c.f. column 2 and 3 in Fig. 3). This confirms that it is better capturing asymmetric errors over the region of every contour point."
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,5,Discussion and Conclusion,"The results reported before reveal that approaching the problem of segmentation uncertainty prediction via a regression task, where the uncertainty is expressed in terms of landmark location, is globally better than via pixel-based segmentation methods. It also shows that our method (N 1 , N 2 and SN 2 ) is better than the commonly-used MC-Dropout. It can also be said that our method is more interpretable as is detailed in Sect. 2.2 and shown in Fig. 3.The choice of distribution has an impact when considering the shape of the predicted contour. For instance, structures such as the left ventricle and the myocardium wall in the ultrasound datasets have large components of their contour oriented along the vertical direction which allows the univariate and bivariate models to perform as well, if not better, than the asymmetric model. However, the lungs and heart in chest X-Rays have contours in more directions and therefore the uncertainty is better modeled with the asymmetric model.Furthermore, it has been demonstrated that skewed uncertainty is more prevalent when tissue separation is clear, for instance, along the septum border (CAMUS) and along the lung contours (JSRT). The contrast between the left ventricle and myocardium in the images of the Private Cardiac US dataset is small, which explains why the simpler univariate and bivariate models perform well. This is why on very noisy and poorly contrasted data, the univariate or the bivariate model might be preferable to using the asymmetric model.While our method works well on the tasks presented, it is worth noting that it may not be applicable to all segmentation problems like tumour segmentation. Nevertheless, our approach is broad enough to cover many applications, especially related to segmentation that is later used for downstream tasks such as clinical metric estimation. Future work will look to expand this method to more general distributions, including bi-modal distributions, and combine the aleatoric and epistemic uncertainty to obtain the full predictive uncertainty."
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,Fig. 1 .,
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,Fig. 2 .,
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,Fig. 3 .,
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,Table 1 .,
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_21.
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,1,Introduction,"The shortage of labeled data is a significant challenge in medical image segmentation, as acquiring large amounts of labeled data is expensive and requires specialized knowledge. This shortage limits the performance of existing segmentation  models. To address this issue, researchers have proposed various semi-supervised learning (SSL) techniques that incorporate both labeled and unlabeled data to train models for both natural [2,4,12,13,15,16] and medical images [10,11,14,[18][19][20][21]. However, most of these methods do not consider the class imbalance issue, which is common in medical image datasets. For example, multi-organ segmentation from CT scans requires to segment esophagus, right adrenal gland, left adrenal gland, etc., where the class ratio is quite imbalanced; see Fig 1(a). As for liver tumor segmentation from CT scans, usually the ratio for liver and tumor is larger than 16:1.Recently, some researchers proposed class-imbalanced semi-supervised methods [1,10] and demonstrated substantial advances in medical image segmentation tasks. Concretely, Basak et al. [1] introduced a robust class-wise sampling strategy to address the learning bias by maintaining performance indicators on the fly and using fuzzy fusion to dynamically obtain the class-wise sampling rates. However, the proposed indicators can not model the difficulty well, and the benefits may be overestimated due to the non-representative datasets used (Fig. 1(a)). Lin et al. [10] proposed CLD to address the data bias by weighting the overall loss function based on the voxel number of each class. However, this method fails due to the easily over-fitted CPS (Cross Pseudo Supervision) [4] baseline, ignoring unlabeled data in weight estimation and the fixed class-aware weights.In this work, we explore the importance of heterogeneity in solving the over-fitting problem of CPS (Fig. 2) and propose a novel DHC (Dual-debiased Heterogeneous Co-training) framework with two distinct dynamic weighting strategies leveraging both labeled and unlabeled data, to tackle the class imbalance issues and drawbacks of the CPS baseline model. The key idea of heterogeneous co-training is that individual learners in an ensemble model should be both accurate and diverse, as stated in the error-ambiguity decomposition [8].To achieve this, we propose DistDW (Distribution-aware Debiased Weighting) and DiffDW (Diff iculty-aware Debiased Weighting) strategies to guide the two sub-models to tackle different biases, leading to heterogeneous learning directions. Specifically, DistDW solves the data bias by calculating the imbalance ratio with the unlabeled data and forcing the model to focus on extreme minority classes through careful function design. Then, after observing the inconsistency between the imbalance degrees and the performances (see Fig. 1(b)), DiffDW is designed to solve the learning bias. We use the labeled samples and the corresponding labels to measure the learning difficulty from learning speed and Dice value aspects and slow down the speeds of the easier classes by setting smaller weights. DistDW and DiffDW are diverse and have complementary properties (Fig. 1(c)), which satisfies the design ethos of a heterogeneous framework.The key contributions of our work can be summarized as follows: 1) we first state the homogeneity issue of CPS and improve it with a novel dual-debiased heterogeneous co-training framework targeting the class imbalance issue; 2) we propose two novel weighting strategies, DistDW and DiffDW, which effectively solve two critical issues of SSL: data and learning biases; 3) we introduce two public datasets, Synapse [9] and AMOS [7], as new benchmarks for class-imbalanced semi-supervised medical image segmentation. These datasets include sufficient classes and significant imbalance ratios (> 500 : 1), making them ideal for evaluating the effectiveness of class-imbalance-targeted algorithm designs."
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,2,Methods,"Figure 3 shows the overall framework of the proposed DHC framework. DHC leverages the benefits of combining two diverse and accurate sub-models with two distinct learning objectives: alleviating data bias and learning bias. To achieve this, we propose two dynamic loss weighting strategies, DistDW (Distributionaware Debiased Weighting) and DiffDW (Difficulty-aware Debiased Weighting), to guide the training of the two sub-models. DistDW and DiffDW demonstrate complementary properties. Thus, by incorporating multiple perspectives and sources of information with DistDW and DiffDW, the overall framework reduces over-fitting and enhances the generalization capability."
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,2.1,Heterogeneous Co-training Framework with Consistency Supervision,"Assume that the whole dataset consists of N L labeled samples {(x l i , y i )} NL i=1 and N U unlabeled samples {x u i } NU i=1 , where x i ∈ R D×H×W is the input volume and y i ∈ R K×D×H×W is the ground-truth annotation with K classes (including background). The two sub-models of DHC complement each other by minimizing the following objective functions with two diverse and accurate weighting strategies:where pis the output probability map and ŷ(• y) is the supervised cross entropy loss function to supervise the output of labeled data, andis the unsupervised loss function to measure the prediction consistency of two models by taking the same input volume x i . Note that both labeled and unlabeled data are used to compute the unsupervised loss. Finally, we can obtain the total loss: L total = L s + λL u , we empirically set λ as 0.1 and follow [10] to use the epoch-dependent Gaussian ramp-up strategy to gradually enlarge the ratio of unsupervised loss. W dif f i and W dist i are the dynamic class-wise loss weights obtained by the proposed weighting strategies, which will be introduced next."
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,2.2,Distribution-aware Debiased Weighting (DistDW),"To mitigate the data distribution bias, we propose a simple yet efficient reweighing strategy, DistDW. DistDW combines the benefits of the SimiS [3], which eliminate the weight of the largest majority class, while preserving the distinctive weights of the minority classes (Fig. 4(c)). The proposed strategy rebalances the learning process by forcing the model to focus more on the minority classes. Specifically, we utilize the class-wise distribution of the unlabeled pseudo labels p u by counting the number of voxels for each category, denoted as N k , k = 0, ..., K. We construct the weighting coeffcient for k th category as follows:where β is the momentum parameter, set to 0.99 experimentally."
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,2.3,Difficulty-aware Debiased Weighting (DiffDW),"After analyzing the proposed DistDW, we found that some classes with many samples present significant learning difficulties. For instance, despite having the second highest number of voxels, the stomach class has a much lower Dice score than the aorta class, which has only 20% of the voxels of the stomach (Fig. 1(b)). Blindly forcing the model to prioritize minority classes may further exacerbate the learning bias, as some challenging classes may not be learned to an adequate extent. To alleviate this problem, we design DiffDW to force the model to focus on the most difficult classes (i.e. the classes learned slower and with worse performances) rather than the minority classes. The difficulty is modeled in two ways: learning speed and performance. We use Population Stability Index [6] to measure the learning speed of each class after the t th iteration:where λ k denotes the Dice score of k th class in t th iteration and = λ k,t -λ k,t-1 . du k,t and dl k,t denote classes not learned and learned after the t th iteration. I(•) is the indicator function. τ is the number accumulation iterations and set to 50 empirically. Then, we define the difficulty of k th class after t th iteration as, where is a smoothing item with minimal value. The classes learned faster have smaller d k,t , the corresponding weights in the loss function will be smaller to slow down the learn speed. After several iterations, the training process will be stable, and the difficulties of all classes defined above will be similar. Thus, we also accumulate 1λ k,t for τ iterations to obtain the reversed Dice weight w λ k,t and weight d k,t . In this case, classes with lower Dice scores will have larger weights in the loss function, which forces the model to pay more attention to these classes. The overall difficulty-aware weight of k th class is defined as:α is empirically set to 1  5 in the experiments to alleviate outliers. The difficulty-aware weights for all classes are"
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,3,Experiments,"Dataset and Implementation Details. We introduce two new benchmarks on the Synapse [9] and AMOS [7] datasets for class-imbalanced semi-supervised medical image segmentation. The Synapse dataset has 13 foreground classes, including spleen (Sp), right kidney (RK), left kidney (LK), gallbladder (Ga), esophagus (Es), liver(Li), stomach(St), aorta (Ao), inferior vena cava (IVC), portal & splenic veins (PSV), pancreas (Pa), right adrenal gland (RAG), left adrenal gland (LAG) with one background and 30 axial contrast-enhanced abdominal CT scans. We randomly split them as 20, 4 and 6 scans for training, validation, and testing, respectively. Compared with Synapse, the AMOS dataset excludes PSV but adds three new classes: duodenum(Du), bladder(Bl) and prostate/uterus(P/U). 360 scans are divided into 216, 24 and 120 scans for training, validation, and testing. We ran experiments on Synapse three times with different seeds to eliminate the effect of randomness due to the limited samples. More training details are in the supplementary material.Comparison with State-of-the-Art Methods. We compare our method with several state-of-the-art semi-supervised segmentation methods [4,11,19,21]. Moreover, simply extending the state-of-the-art semi-supervised classification methods [2,3,5,15,17], including class-imbalanced designs [3,5,17] to segmentation, is a straightforward solution to our task. Therefore, we extend these methods to segmentation with CPS as the baseline. As shown in Table 1 and2, the general semi-supervised methods which do not consider the class imbalance problem fail to capture effective features of the minority classes and lead to terrible performances (colored with red). The methods considered the class imbalance problem have better results on some smaller minority classes such as gallbladder, portal & splenic veins and etc. However, they still fail in some minority classes (Es, RAG, and LAG) since this task is highly imbalanced. Our proposed DHC outperforms these methods, especially in those classes with very few samples. Note that our method performs better than the fully-supervised method for the RAG segmentation. Furthermore, our method outperforms SOTA methods on   Synapse by larger margins than the AMOS dataset, demonstrating the more prominent stability and effectiveness of the proposed DHC framework in scenarios with a severe lack of data. Visualization results in Fig. 5 show our method performs better on minority classes which are pointed with green arrows. More results on datasets with different labeled ratios can be found in the supplementary material.  Ablation Study. To validate the effectiveness of the proposed DHC framework and the two learning strategies, DistDW and DiffDW, we conduct ablation experiments, as shown in Table 3. DistDW ('DistDW-DistDW') alleviates the bias of baseline on majority classes and thus segments the minority classes (RA, LA, ES, etc.) very well. However, it has unsatisfactory results on the spleen and stomach, which are difficult classes but down-weighted due to the larger voxel numbers. DiffDW ('DiffDW-DiffDW') shows complementary results with DistDW, it has better results on difficult classes (e.g. , stomach since it is hollow inside). When combining these two weighting strategies in a heterogeneous cotraining way ('DiffDW-DistDW', namely DHC), the Dice score has 5.12%, 5.6% and 13.78% increase compared with DistDW, DiffDW, and the CPS baseline. These results highlight the efficacy of incorporating heterogeneous information in avoiding over-fitting and enhancing the performance of the CPS baseline."
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,4,Conclusion,"This work proposes a novel Dual-debiased Heterogeneous Co-training framework for class-imbalanced semi-supervised segmentation. We are the first to state the homogeneity issue of CPS and solve it intuitively in a heterogeneous way. To achieve it, we propose two diverse and accurate weighting strategies: DistDW for eliminating the data bias of majority classes and DiffDW for eliminating the learning bias of well-performed classes. By combining the complementary properties of DistDW and DiffDW, the overall framework can learn both the minority classes and the difficult classes well in a balanced way. Extensive experiments show that the proposed framework brings significant improvements over the baseline and outperforms previous SSL methods considerably."
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Fig. 1 .,
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Fig. 2 .,
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Fig. 3 .,
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Fig. 4 .,
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,,
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Fig. 5 .,
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Table 1 .,
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Table 2 .,
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Table 3 .,
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 56.
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,1,Introduction,"Data-driven deep networks maintain the great potential to achieve superior performance under the large-scale medical data [23,30,34]. However, privacy concerns from patients and institutions prevent centralized training from access to the data in multiple centers. Federated learning (FL) thereby becomes a promising compromise that preserves privacy by distributing the model to data sources to train a global model without sharing their data directly [27].A major challenge to FL in real-world scenarios is domain shift, which refers to the difference of marginal data distributions across centers and induces significant performance degradation [9,22,30]. Current methods to address the problem of domain shift can be categorized into two directions. One is federated domain generalization (FedDG) [5,22], which tackles the domain shift between training and testing clients. FedDG aims at obtaining a generalizable global model, but the optimal performance on local training clients cannot be guaranteed. Another direction is personalized federated learning (PFL) [1,7,18,20,29,31], which tackles the domain shifts among training clients by personalizing the global model locally. However, both FedDG and PFL only consider the partial objective of GPFL, ignoring either personalization or generalization in real-world scenarios.In this paper, we focus on generalized and personalized federated learning (GPFL), which considers both generalization and personalization to holistically combat the domain shift. We notice that one recent work IOP-FL [13] also studied the problem of GPFL, but it mainly resorts to the model personalization for the unseen clients by test-time training on deployment stage, which did not directly consider enhancing both objectives of GPFL in the training phase.We seek a more effective and efficient solution to GPFL in this work. Specifically, our intuition is based on the following conjecture: a more generalizable global model can facilitate the local models to better adapt to the corresponding local distribution, and better adapted local models can then provide positive feedbacks to the global model with improved gradients.Based on the above intuition, we propose a novel method named GRAdient CorrEction (GRACE) that can achieve both generalization and personalization during training by enhancing the model consistency on both the client side and the server side. By analyzing the federated training stage in Fig. 1(a), we discover a significant discrepancy in the feature distributions between the global and local models due to domain shifts, which will influence the training process on both client and server side. To address this problem, we aim to correct the inconsistent gradients on both sides. On the client side, we leverage a meta-learning strategy to align the feature spaces of global and local models while fitting the local data distribution, as depicted in Fig. 1(b). Furthermore, on the server side, Fig. 1. The overall framework of (a) the federated learning system which has two stages for algorithm design and (b) our GRACE method on both the client and server sides.we estimate the gradient consistency by computing the cosine similarity among the gradients from clients and re-weight the aggregation weights to mitigate the negative effect of domain shifts on the global model update. Through these two components, GRACE preserves the generalizability in global model as much as possible when personalizing it on local distributions. Comprehensive experiments are conducted on two medical FL benchmarks, which show that GRACE outperforms state-of-the-art methods in terms of both generalization and personalization. We also perform insightful analysis to validate the rationale of our algorithm design. N and we denote the weight vector as p = [p 1 , . . . , p M ] ∈ R M . This method implicitly assumes that all clients share the same data distribution, thus failing to adapt to domain shift scenarios."
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,2,Method,"To solve the GPFL problem, we propose a GRAdient CorrEction (GRACE) method for both local client training and server aggregation during the federated training stage. Unlike IPO-FL [13], our method constrains the model's generalizability and personality only during the federated training stage. Our motivation comes from an observation that domain shift causes a significant mismatch between the feature spaces of the local models and the initial global model after client training at each round. It leads to inconsistent gradients uploaded from the biased local models, which hurts the generalizability of the global model. Therefore, in GRACE, we alleviate the inconsistency between gradients obtained from different clients. The details of GRACE are elaborated in the following sections."
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,2.2,Local Training Phase: Feature Alignment & Personalization,"We calibrate the gradient during local training via a feature alignment constraint by meta-learning, which preserves the generalizable feature while adapting to the local distributions. We conduct the client-side gradient correction in two steps. Meta-Train: We denote θ t,k m as the personalized model parameter at the local update step k of client m in round t and η as the local learning rate. The first step is a personalization step by the task loss:where (x tr , y tr ) ∈ D m is the sampled data and θ t,k m is the updated parameter that will be used in the second step.Meta-Update: After optimizing the local task objective, we need a metaupdate to virtually evaluate the updated parameters θ t,k m on the held-out metatest data (x te , y te ) ∈ D m with a meta-objective L meta m . We add the feature alignment regularizer into the loss function of meta-update:where h(•; φ) is the feature extractor part of model f (•; θ) and φ is the corresponding parameter, and β is the weight for alignment loss which has a default value of 1.0. Here, we apply three widely-used alignment losses to minimize the discrepancy in the feature space: CORAL [32], MMD [16], and adversarial training [11]. We show that varying alignment loss functions can boost the generalization capability during local training and report the results in Table 4."
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,2.3,Aggregation Phase: Consistency-Enhanced Re-weighting,"We introduce a novel aggregation method on the server side that corrects the global gradient by enhancing the consistency of the gradients received from training clients. We measure the consistency of two gradient vectors by their cosine similarity and use the average cosine similarity among all uploaded gradients as an indicator of gradient quality on generalization. The corrected global gradient is:""•"" means element multiplication of two vectors and ""Norm"" means to normalize the new weight vector p with M m=1 p m = 1, p m ∈ (0, 1). Then the updated global model for round t + 1 will be θ t+1 g = θ t g -η g Δθ t g , where η g is the global learning rate with default value 1. Theoretical Analysis: We prove that the re-weighting method in Eq.(3) will enhance the consistency of the global gradient based on the FedAvg. First, we define the averaged cosine similarity of Δθ t m as c t m , where c t m = 1 M M n=1 σ t mn . Then, the consistency degree of the global gradient in FedAvg is M m=1 p m c t m . Thus, the consistency degree after applying our GRACE method hasNote that, we can easily prove the inequality in Eq. ( 4) by using the generalized arithmetic-geometric mean inequality, which holds when 3 Experiments"
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,3.1,Dataset and Experimental Setting,"We evaluate our approach on two open source federated medical benchmarks: Fed-ISIC2019 and Fed-Prostate. The former is a dermoscopy image classification task [9] among eight different melanoma classes, which is a 6-client federated version of ISIC2019 [6,8,34]. And the latter is a federated prostate segmentation task [13] with T2-weighted MRI images from 6 different domains [15,21,23,28]. We follow the settings of [9] for Fed-ISIC2019 and [22] for Fed-Prostate."
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,3.2,Comparison with SOTA Methods,"We conduct the leave-one-client-out experiment for both benchmarks. In each experiment, one client is selected as the unseen client and the model is trained on the remaining clients. The average performance on all internal clients' test set is the in-domain personalization results, while the unseen client's performance is the out-of-domain generalization results. The final results of each method are the average of all leave-one-domain-out splits, and all results are over three independent runs. (Please see the details of experimental setup in open-source code.) Performance of In-Domain Personalization. For a fair comparison, the baseline method FedAvg [27] and several current SOTA personalized FL methods are chosen. Ditto [18] and FedMTL [31] treat the personalized process as a kind of multi-task learning and generate different model parameters for each client. FedBN [20] keeps the parameters of BatchNorm layers locally as considering those parameters to contain domain information. FedRep [1], Fed-BABU [29], FedPer [1] and FedRoD [4] all use a personalized head to better fit the local data distribution, where FedRoD also retains a global head for OOD generalization. Besides FedRoD, PerFedAvg [10] and pFedMe [33] can also implement personalized and generalized in FL by federated meta-learning. These three methods are also involved in the comparison of out-of-domain generalization. Table 1 presents the results of two tasks. GRACE achieves comparable personalization performance on the in-domain clients with SOTA methods. Note that GRACE outperforms most of PFL methods in the table and other methods like FedRoD and Per-FedAvg also achieve good results, which means that improving generalization is also beneficial for model personalization. Performance of Out-of-Domain Generalization. For out-of-domain comparison, we select several FL methods that aim to solve the data heterogeneity problem, such as FedProx [19], Scaffold [14] and MOON [17]. Some FedDG methods are also chosen for comparison, like ELCFS [22] and HarmoFL [12]. (We mark the A-distance corresponding to the round.)"
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,3.3,Further Analysis,"Ablation Study on Different Parts of Our Method. The detailed ablation studies are shown in Table 4 to further validate the effectiveness of each component in GRACE. Three widely-used approaches from domain adaptation/generalization area are used for feature alignment loss in local training.In Table 4, ""Adv."" [11] means using adversarial training between features from the global model and the local model, and ""CORAL"" [32] and ""MMD"" [16] are classic regularization losses for domain alignment. From the table, our framework can obtain performance improvements on different alignment approaches. Considering both performance and efficiency, we prefer ""CORAL"" for alignment. Add the server-side correction on top of MOON can also obtain some gains, but the overall effect is limited, since its alignment loss might push the current feature away from features in previous round and thus reduces the discriminativeness.Visualization of the Feature Alignment Loss in Eq. (2). The A-distance measurement is used to evaluate the dissimilarity between the local and global models, which is suggested to measure the cross-domain discrepancy in the domain adaptation theory [2]. We follow the proxy implementation in [24] and trace the curve of A-distance on FedAvg and our method on each client throughout the training process in Fig. 2(a). The curves demonstrate a substantial reduction of feature discrepancy compared with FedAvg. It validates the efficacy of our algorithm design and corroborates our claim that generalization and personalization are compatible objectives. Our personalized local models can close the distance with the global model while preserving a good fit for local data distribution. In addition, we use t-SNE [26] to visualize the feature distributions in Fig. 2(b) and GRACE can reduce the discrepancy between global and local features.Loss Curves Comparison of FedAvg and Our Method. Fig. 3 shows the loss curves of FedAvg and our GRACE method with only server-side aggregation (Model A in Table 4). Our method can achieve better global minimization. "
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,4,Conclusion,"We introduce GPFL for multi-center distributed medical data with domain shift problems, which aims to achieve both generalization for unseen clients and personalization for internal clients. Existing approaches only focus on either generalization (FedDG) or personalization (PFL). We argue that a more generalizable global model can facilitate the local models to adapt to the clients' distribution, and the better-adapted local models can contribute higher quality gradients to the global model. Thus, we propose a new method GRAdient CorrEction (GRACE), which corrects the model gradient at both the client and server sides during training to enhance both the local personalization and the global generalization. The experimental results on two medical benchmarks show that GRACE can enhance both local adaptation and global generalization and outperform existing SOTA methods in generalization and personalization."
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,,2. 1,
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,,Fig. 2 .,
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,,Fig. 3 .,
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,,,
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,,Table 1 .,
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,,Table 2 .,
GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_2. The results are summarized in Table 2, and according to the comparison, GRACE shows a significant improvement in the unseen client. Combining Table 1 and2, it indicates that our gradient correction on both the local and global sides effectively enhances the generalization of the global model based on personalization. Generalization with TTDA. Considering the promise of recent test-data domain adaptation (TTDA) techniques for domain generalization in medical imaging [13,25], we also compare GRACE under the TTDA scenario with IOP-FL [13], DSBN [3] and Tent [35]). As shown in Table 3, GRACE obtains better results combined with DSBN and Tent, which means our method is orthogonal with TTDA, and TTDA can benefit from our generalized global model."
Deployment of Image Analysis Algorithms Under Prevalence Shifts,1,Introduction,"Model Re-calibration: After a prevalence shift models need to be recalibrated. This has important implications on the decisions made based on predicted class scores (see next point). Note in this context that deep neural networks tend not to be calibrated after training in the first place [15]. Decision Rule: A decision rule is a strategy transforming continuous predicted class scores into a single classification decision. Simply using the argmax operator ignores the theoretical boundary conditions derived from Bayes theory. Importantly, argmax relies on the predicted class scores to be calibrated and is thus highly sensitive to prevalence shifts [13]. Furthermore, it only yields the optimal decision for specific metrics. Analogously, tuned decision rules may not be invariant to prevalence shifts.Performance Assessment: Class frequencies observed in one test set are in general not representative of those encountered in practice. This implies that the scores for widely used prevalence-dependent metrics, such as Accuracy, F1 Score, and Matthews Correlation Coefficient (MCC), would substantially differ when assessed under the prevalence shift towards clinical practice [27].This importance, however, is not reflected in common image analysis practice. Through a literature analysis, we found that out of a total of 53 research works published between 01/2020 and beginning of 03/2023 that used any of the data included in our study, only one explicitly mentioned re-calibration. Regarding the most frequently implemented decision rules, roughly three quarters of publications did not report any strategy, which we strongly assume to imply use of the default argmax operator. Moreover, both our analysis and previous work show Accuracy and F1 Score to be among the most frequently used metrics for assessing classification performance in comparative medical image analysis [26,27], indicating that severe performance deviations under potential prevalence shifts are a widespread threat.Striving to bridge the translational gap in AI-based medical imaging research caused by prevalence shifts, our work provides two main contributions: First, we demonstrate the potential consequences of ignoring prevalence shifts on a diverse set of medical classification tasks. Second, we assemble a comprehensive workflow for image classification, which is robust to prevalence shifts. As a key advantage, our proposal requires only an estimate of the expected prevalences rather than annotated deployment data and can be applied to any given black box model."
Deployment of Image Analysis Algorithms Under Prevalence Shifts,2,Methods,
Deployment of Image Analysis Algorithms Under Prevalence Shifts,2.1,Workflow for Prevalence-Aware Image Classification,"Our workflow combines existing components of validation in a novel manner. As illustrated in Fig. 1, it leverages estimated deployment prevalences to adjust an already trained model to a new environment. We use the following terminology.Re-calibration: We refer to the output of a model ϕ : X → R C before applying the softmax activation as ϕ(x). It can be re-calibrated by applying a transformation f . Taking the softmax of ϕ(x) (no re-calibration) or of f (ϕ(x)), we obtain predicted class scores s x . The probably most popular re-calibration approach is referred to as ""temperature scaling"" [15] and requires only a single parameter t ∈ R to be estimated: f temp (ϕ(x)) = ϕ(x)/t. The transformation parameter(s) is/are learned with minimization of the cross-entropy loss.Decision Rule: A decision rule d is a deterministic algorithm that maps predicted class scores s x to a final prediction d(s x ) ∈ Y . The most widely used decision rule is the argmax operator, although various alternatives exist [27].To overcome problems caused by prevalence shifts, we propose the following workflow (Fig. 1b).Step 1: Estimate the deployment prevalences: The first step is to estimate the prevalences in the deployment data D dep , e.g., based on medical records, epidemiological research, or a data-driven approach [23,32]. The workflow requires an underlying anticausal connection of image and label, i.e., a label y causes the image x (e.g., presence of a disease has a visual effect) [8,11], to be verified at this point.Step 2: Perform prevalence-aware re-calibration: Given a shift of prevalences between the calibration and deployment dataset (P D cal = P D dep ), we can assume the likelihoods P (x|y = k) to stay identical for an anticausal problem (note that we are ignoring manifestation and acquisition shifts during deployment [8]). Under mild assumptions [23,41], weight adaptation in the loss function optimally solves the prevalence shift for a classifier. In the presence of prevalence shifts, we therefore argue for adaptation of weights in the cross-entropy loss i -w(y i ) log(s i (y i )) according to the expected prevalences; more precisely, for class k we use the weight w(k) = P D dep (k)/P D cal (k) during the learning of the transformation parameters [11,34,41]. Furthermore, since temperature scaling's single parameter t is incapable of correcting the shift produced by a mismatch in prevalences, we add a bias term b ∈ R C to be estimated alongside t as suggested by [2,6,29]. We refer to this re-calibration approach as ""affine scaling"":Step 3: Configure validation metric with deployment prevalences: Prevalence-dependent metrics, such as Accuracy, MCC, or the F1 Score, are widely used in image analysis due to their many advantages [27]. However, they reflect a model's performance only with respect to the specific, currently given prevalence. This problem can be overcome with the metric Expected Cost (EC) [13]. In its most general form, we can express EC as EC = k P D (k) j c kj R kj , where c kj refers to the ""costs"" we assign to the decision of classifying a sample of class k as j and R kj is the fraction of all samples with reference class k that have been predicted as j. Note that the standard 0-1 costs (c kk = 0 for all k and c kj = 1 for k = j) reduces to EC being 1 minus Accuracy. To use EC as a robust estimator of performance, we propose replacing the prevalences P D (k) with those previously estimated in step 1 [13].Step 4: Set prevalence-aware decision rule: Most counting metrics [27] require some tuning of the decision rule during model development, as the argmax operator is generally not the optimal option. This tuning relies on data from the development phase and the resulting decision rule is likely dependent on development prevalences and does not generalize (see Sec. 3). On the other hand, EC, as long as the predicted class scores are calibrated, yields the optimal decision rule argmin k j c jk s x (j) [3,16]. For standard 0-1 costs, this simplifies to the argmax operator.Step 5: External validation: The proposed steps for prevalence-aware image classification have strong theoretical guarantees, but additional validation on the actual data of the new environment is indispensable for monitoring [33]."
Deployment of Image Analysis Algorithms Under Prevalence Shifts,2.2,Experimental Design,"The purpose of our experiments was twofold: (1) to quantify the effect of ignoring prevalence shifts when validating and deploying models and (2) to show the value of the proposed workflow. The code for our experiments is available at https:// github.com/IMSY-DKFZ/prevalence-shifts.Medical Image Classification Tasks. To gather a wide range of image classification tasks for our study, we identified medical image analysis tasks that are publicly available and provide at least 1000 samples. This resulted in 30 tasks covering the modalities laparoscopy [22,38], gastroscopy/colonoscopy [5,30], magnetic resonance imaging (MRI) [4,9], X-ray [1,18,20,31], fundus photography [24], capsule endoscopy [35], and microscopy [14] (Fig. 2). We split each task as follows: 30% of the data -referred to as ""deployment test set"" D depwas used as a hold-out split to sample subsets D dep (r) representing a deployment scenario with IR r. The remaining data set made up the ""development data"", comprising the ""development test set"" D test (10%; class-balanced), the ""training set"" (50%) and the ""validation set"" (10%; also used for calibration).Experiments. For all experiments, the same neural network models served as the basis. To mimic a prevalence shift, we sub-sampled datasets D dep (r) from the deployment test sets D dep according to IRs r ∈ [1,10] with a step size of 0.5. The experiments were performed with the popular prevalence-dependent metrics Accuracy, MCC, and F1 Score, as the well as EC with 0-1 costs. For our empirical analyses, we trained neural networks (specifications: see Table 2 Suppl.) for all 30 classification tasks introduced in Sect. 2.2. In the interest of better reproducibility and interpretability, we focused on a homogeneous workflow (e.g., by fixing hyperparameters across tasks) rather than aiming to achieve the best possible Accuracy for each individual task. The following three experiments were performed. (1) To assess the effects of prevalence shifts on model calibration, we measured miscalibration on the deployment test set D dep (r) as a function of the increasing IR r for five scenarios: no re-calibration, temperature scaling, and affine scaling (the latter two with and without weight adaptation). Furthermore, (2) to assess the effects of prevalence shifts on the decision rule, for the 24 binary tasks, we computed -with and without re-calibration and for varying IR r -the differences between the metric scores on D dep (r) corresponding to an optimal decision rule and two other decision rules: argmax and a cutoff that was tuned on D test . Lastly, (3) to assess the effects of prevalence shifts on the generalizability of validation results, we measured the absolute difference between the metric scores obtained on the development test data D test and those obtained on the deployment test data D dep (r) with varying IR r. The scores were computed for the argmax decision rule for both non-re-calibrated and re-calibrated predicted class scores. To account for potential uncertainty in estimating deployment prevalences, we repeated all experiments with slight perturbation of the true prevalences. To this end, we drew the prevalence for each class from a normal distribution with a mean equal to the real class prevalence and fixed standard deviation (std). We then set a minimal score of 0.01 for each class and normalized the resulting distribution. "
Deployment of Image Analysis Algorithms Under Prevalence Shifts,3,Results,"Effects of Prevalence Shifts on Model Calibration. In general, the calibration error increases with an increasing discrepancy between the class prevalences in the development and the deployment setting (Fig. 3). The results clearly demonstrate that a simple accuracy-preserving temperature scaling-based method is not sufficient under prevalence shifts. Only our proposed method, which combines an affine transformation with a prevalence-driven weight adjustment, consistently features good calibration performance. This also holds true when perturbing the deployment prevalences, as demonstrated in Fig. 7 (Suppl.). For the inspected range (up to r = 10), miscalibration can be kept constantly close to 0. Note that CWCE is a biased estimator of the canonical calibration error [27], which is why we additionally report the Brier Score (BS) as an overall performance measure (Fig. 6 Suppl.).Effects of Prevalence Shifts on the Decision Rule. Figure 4 supports our proposal: An argmax-based decision informed by calibrated predicted class scores (top right) and assessed with the Expected Cost (EC) metric (identical to the blue Accuracy line in this case) yields optimal results irrespective of prevalence shifts. In fact, this approach substantially increases the quality of the decisions when compared to a baseline without re-calibration, as indicated by an average relative decrease of EC by 25%. This holds true in a similar fashion for perturbed versions of the re-calibration (Fig. 8 Suppl.). The results further show that argmax is not the best decision rule for F1 Score and MCC (Fig. 4 "
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,top).,"Importantly, decision rules optimized on a development dataset do not generalize to unseen data under prevalence shifts (Fig. 4 bottom). "
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,Effects of Prevalence Shifts on the Generalizability of Validation,"Results. As shown in Fig. 5, large deviations from the metric score obtained on the development test data of up to 0.41/0.18 (Accuracy), 0.35/0.46 (F1 Score), and 0.27/0.32 (MCC), can be observed for the re-calibrated/non-re-calibrated case. In contrast, the proposed variation of Expected Cost (EC) enables a reliable estimation of performance irrespective of prevalence shifts, even when the prevalences are not known exactly (Fig. 9 Suppl.). The same holds naturally true for the prevalence-independent metrics Balanced Accuracy (BA) and Area under the Receiver Operating Curve (AUROC) (Fig. 9 Suppl.)."
Deployment of Image Analysis Algorithms Under Prevalence Shifts,4,Discussion,"Important findings, some of which are experimental confirmations of theory, are:1. Prevalence shifts lead to miscalibration. A weight-adjusted affine recalibration based on estimated deployment prevalences compensates for this effect. 2. Argmax should not be used indiscriminately as a decision rule. For the metric EC and specializations thereof (e.g., Accuracy), optimal decision rules may be derived from theory, provided that the predicted class scores are calibrated. This derived rule may coincide with argmax, but for other common metrics (F1 Score, MCC) argmax does not lead to optimal results. 3. An optimal decision rule, tuned on a development dataset, does not generalize to datasets with different prevalences. Prevalence-aware setting of the decision rule requires data-driven adjustment or selection of a metric with a Bayes theory-driven optimal decision rule. 4. Common prevalence-dependent metrics, such as MCC and F1 Score, do not give robust estimations of performance under prevalence shifts. EC, with adjusted prevalences, can be used in these scenarios.These findings have been confirmed by repeated experiments using multiple random seeds for dataset splitting and model training. Overall, we present strong evidence that the so far uncommon metric EC offers key advantages over established metrics. Due to its strong theoretical foundation and flexibility in configuration it should, from our perspective, evolve to a default metric in image classification. Note in this context that while our study clearly demonstrates the advantages of prevalence-independent metrics, prevalence-dependent metrics can be much better suited to reflect the clinical interest [27].In conclusion, our results clearly demonstrate that ignoring potential prevalence shifts may lead to suboptimal decisions and poor performance assessment. In contrast to prior work [25], our proposed workflow solely requires an estimation of the deployment prevalences -and no actual deployment data or model modification. It is thus ideally suited for widespread adoption as a common practice in prevalence-aware image classification."
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,Fig. 1 .,
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,Fig. 2 .,
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,Fig. 3 .,
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,Fig. 4 .,
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,Fig. 5 .,
Deployment of Image Analysis Algorithms Under Prevalence Shifts,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_38.
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,1,Introduction,"Cone-Beam Computed Tomography (CBCT) scans are widely used for guidance and verification in the operating room. The clinical value of this imaging modality is however limited by artifacts originating from patient and device motion or metal objects in the X-Ray beam. To compensate these effects, knowledge about the relative acquisition geometry between views can be exploited. This so-called epipolar geometry is widely used in computer vision and can be applied to CBCT imaging due to the similar system geometry [3,11].By formulating and enforcing consistency conditions based on this geometrical relationship, motion can be compensated [4,8], beam-hardening effects can be reduced [11], and multi-view segmentations can be refined [2,7]. Motion and beam hardening effects can be corrected by optimizing for consistency through either updating the projection matrices or image values while the respective other is assumed fixed.In segmentation refinement, the principal idea is to incorporate the known acquisition geometry to unify the binary predictions on corresponding detector pixels. This inter-view consistency can be iteratively optimized to reduce falsepositives in angiography data [8]. Alternatively, an entire stack of segmented projection images can be backprojected, the reconstructed volume thresholded and re-projected to obtain 3D consistent masks [2].In this work, we explore the idea of incorporating epipolar geometry into the learning-based segmentation process itself instead of a separate post-processing step. A differentiable image transform operator is embedded into the model architecture, which translates intermediate features across views allowing the model to adjust its predictions to this conditional information. By making this information accessible to neural networks and enabling dual-view joint processing, we expect benefits for projection domain processing tasks such as inpainting, segmentation or regression. As a proof-of-concept, we embed the operator into a segmentation model and evaluate its influence in a simulation study. To summarize, we make the following contributions:-We analytically derive formulations for forward-and backward pass of the view translation operator -We provide an open-source implementation thereof which is compatible with real-world projection matrices and PyTorch framework -As an example of its application, we evaluate the operator in a simulation study to investigate its effect on projection domain segmentation"
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,2,Methods,"In the following sections we introduce the geometrical relationships between epipolar views, define a view translation operator, and analytically derive gradients needed for supervised learning.Epipolar Geometry and the Fundamental Matrix. A projection matrix P ∈ R 3×4 encodes the extrinsic device pose and intrinsic viewing parameters of the cone-beam imaging system. These projection matrices are typically available for images acquired with CBCT-capable C-Arm systems. Mathematically, this non-linear projective transform maps a point in volume coordinates to detector coordinates in homogeneous form [1]. When two projection images of the same scene are available, the two detector coordinate systems can be linked through epipolar geometry as depicted in Fig. 1. The Fundamental matrix F ∈ R 3×3 directly encodes the inherent geometric relation between two detector coordinate systems. More specifically, a point u in one projection image is mapped onto a line l through l = F u , where l, u are vectors in the 2D projective homogeneous coordinate space P 2+ (notation from [1]). Given projection matrices P, P ∈ R 3×4 , the fundamental matrix can be derived aswhere • + denotes the pseudo-inverse, c ∈ P 3+ is the camera center in homogeneous world coordinates, and [•] × constructs the tensor-representation of a cross product. Note, that the camera center can be derived as the kernel of the projection c = ker(P ). Additional details on epipolar geometry can be found in literature [3].The Epipolar View Translation Operator (EVT). The goal of the proposed operator is to provide a neural network with spatially registered feature information from a second view of known geometry. Consider the dual view setup as shown in Fig. 1. Epipolar geometry dictates that a 3D landmark detected at a detector position u in projective view P is located somewhere along the epipolar line l in the respective other view P . Naturally this only holds true as long as the landmark is within the volume of interest (VOI) depicted in both images.To capture this geometric relationship and make spatially corresponding information available to the model, an epipolar map Ψ is computed from the input image p. As shown in Eq. 2, each point u in the output map Ψ , is computed as the integral along its epipolar line l = F u . Gradient Derivation. To embed an operator into a model architecture, the gradient with respect to its inputs and all trainable parameters needs to be computed. As the proposed operator contains no trainable parameters, only the gradient with respect to the input is derived.The forward function for one output pixel Y u = Ψ (u ) can be described through a 2D integral over the image coordinates uwhere X u denotes the value in the input image p(u, v) at position u. Here, the indicator function δ(•) signals if the coordinate u lies on the epipolar line defined by F and u :After calculation of a loss L, it is backpropagated through the network graph. At the operator, the loss arrives w.r.t. to the predicted consistency map ∂L ∂Y . From this image-shaped loss, the gradient w.r.t. the input image needs to be derived. By marginalisation over the loss image ∂L ∂Y , the contribution of one intensity value X u in the input image can be written asDeriving Eq. 3 w.r.t. X u eliminates the integral and only leaves the indicator function as an implicit form of the epipolar line. With this inserted in Eq. 5, the loss for one pixel in the input image can be expressed asNote, that forward (Eq. 3) and backward formulation (Eq. 6) are similar and can thus be realised by the same operator. Due to the symmetry shown in Fig. 1, the backward function can be efficiently formulated similar to Eq. 3 by integration along the line l defined by the reversed Fundamental matrix F asImplementation. The formulations above used to derive the gradient assume a continuous input and output distribution. To implement this operation on discrete images, the integral is replaced with a summation of constant step size of one pixel and bi-linear value interpolation. As the forward and backward functions compute the epipolar line given the current pixel position, slight mismatches in interpolation coefficients might occur. However, well-tested trainable reconstruction operators use similar approximate gradients and are proven to converge regardless [6]. The differentiable operator is implemented as a PyTorch function using the torch.utils.cpp_extension. To enable parallel computation, the view transformation is implemented as a CUDA kernel. The source code will be made public upon publication.1 Fig. 2. Dual View Segmentation Model with embedded EVT operator. The operator is embedded at three different scale-levels in the decoder section of a 2D U-Net [5]. By exchanging spatially translated feature maps after each up-block features from the second view are considered during mask decoding."
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,3,Experiments,"As a proof-of-concept application, we assess the operator's influence on the task of projection domain metal segmentation. To reconstruct a CBCT scan, 400 projection images are acquired in a circular trajectory over an angular range of 200 • . To experimentally validate the effects of the proposed image operator, the images are re-sampled into orthogonal view pairs and jointly segmented using a model with an embedded EVT operator.Dual View U-Net with EVT Operator. To jointly segment two projection images, the Siamese architecture shown in Fig. 2 is used. It comprises two U-Net backbones with shared weights which process the two given views. The EVT operator is embedded as a skip connection between the two mirrored models to spatially align feature maps with the respective other view. Each view is fed through the model individually up to the point where epipolar information is added. There, the forward pass is synchronized and the translated feature maps from the respective other view are concatenated. We place the operator at three positions in the model -right after each upsampling block except the last. In the decoder block, feature maps are arguably more sparse. Intuitively, this increases the value of the proposed operator as fewer objects are in the same epipolar plane and thus correspondence is more directly inferable.Compared Models. To investigate the effects of the newly introduced operator, the architecture described above is compared to variants of the U-Net architecture. As a logical baseline, the plain U-Net architecture from [5] is trained to segment each projection image individually. Additionally, the same architecture is provided with two projection images concatenated along the channel axis. This approach verifies that any changes in segmentation performance are attributable to the feature translation operator and not simply due to providing a second view.Data. For the purpose of this study, we use a simulated projection image dataset. Analogous to DeepDRR [9], we use an analytical polychromatic forward model to generate X-Ray images from given CBCT volume data. In total, 29 volumes with approximate spatial dimensions 16 cm 3 from 4 anatomical regions are used (18 × spine, 4 × elbow, 5 × knee, and 2 × wrist). To simulate realistic metal shapes, objects are selected from a library of surgical tools made available by Nuvasive (San Diego, USA) and Königsee Implantate (Allendorf, Germany). From this primary data, six spine scans are selected for testing, and three are selected for validation. Metal implants are manually positioned relative to the anatomy using 3D modelling tools. The metal objects are assembled such that they resemble frequently conducted procedures including pedicle screw placement and k-wire insertions. In total, there are 12 unique scenes fitted to the scans in the test set, and 5 in the validation set.The training set consists of randomly selected and assembled metal objects. Each of the remaining 20 volumes is equipped with n ∈ {4, 6, 8, 10} randomly positioned (non-overlapping) metal objects creating 80 unique scenes.During simulation, the objects are randomly assigned either iron or titanium as a material which influences the choice of attenuation coefficients. For each scene, 100 projection images are generated whose central ray angles on the circular trajectory are approximately 2 • apart."
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,3.1,Model Training,"The three models are trained using the Adam optimizer with the dice coefficient as a loss function and a learning rate of 10 -5 . The best model is selected on the validation loss. As a data augmentation strategy, realistic noise of random strength is added to simulate varying dose levels or patient thickness [10]. We empirically choose the range of noise through the parameter photon count #p ∈ [10 2 , 10 4 ]. During validation and testing, the noise is set to a medium noise level #p = 10 3 . Furthermore, each projection image is normalized to its own mean and standard deviation. The models are trained for 200 epochs on an NVIDIA A100 graphics card. "
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,4,Results,"Quantitative. The per-view averaged segmentation test set statistics are shown in Table 1. The model predicting a single view yields a dice score of 0.916±0.119, thus outperforming the model which is fed two projection images at an average dice similarity of 0.889 ± 0.122. The model equipped with our operator, which also is presented with two views, but translates feature maps internally, yields the highest dice score of 0.950 ± 0.067.Qualitative. To illustrate the reported quantitative results, the segmentation prediction is compared on two selected projection images in Fig. 3. Test 1 shows a lateral view of a spine with 6 pedicle screws and tulips inserted into the 3 central "
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,5,Discussion and Conclusion,"Building upon previous work on epipolar geometry and the resulting consistency conditions in X-Ray imaging, we propose a novel approach to incorporate this information into a model. Rather than explicitly formulating the conditions and optimizing for them, we propose a feature translation operator that allows the model to capture these geometric relationships implicitly.As a proof-of-concept study, we evaluate the operator on the task of projection domain segmentation. The operator's introduction enhances segmentation performance compared to the two baseline methods, as shown by both qualitative and quantitative results. Primarily we found the information from a second view made available by the operator to improve the segmentation in two ways:(1) Reduction of false positive segmentations (2) Increased sensitivity in strongly attenuated areas. Especially for the segmentation of spinal implants, the model performance on lateral images was improved by epipolar information from an orthogonal anterior-posterior projection. Lateral images are usually harder to segment because of the drastic attenuation gradient as illustrated in Fig. 3. It is noteworthy that the U-Net architecture utilizing two images as input exhibits inferior performance compared to the single view model. The simple strategy of incorporating supplementary views into the network fails to demonstrate any discernible synergistic effect, likely due to the use of the same model complexity for essentially conducting two segmentation tasks simultaneously.The simulation study's promising results encourage exploring the epipolar feature transform as a differentiable operator. Future work involves evaluating the presented segmentation application on measured data, analyzing the optimal integration of the operator into a network architecture, and investigating susceptibility to slight geometry calibration inaccuracies. As the U-Net's generalization on real data has been demonstrated, we expect no issues with our method.In conclusion, this work introduces an open-source2 differentiable operator to translate feature maps along known projection geometry. In addition to analytic derivation of gradients, we demonstrate that these geometry informed epipolar feature maps can be integrated into a model architecture to jointly segment two projection images of the same scene.Data Use Declaration. The work follows appropriate ethical standards in conducting research and writing the manuscript, following all applicable laws and regulations regarding treatment of animals or human subjects, or cadavers of both kind. All data acquisitions were done in consultation with the Institutional Review Board of the University Hospital of Erlangen, Germany."
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,,Fig. 1 .,
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,,Fig. 3 .,
Enabling Geometry Aware Learning Through Differentiable Epipolar View Translation,,Table 1 .,
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,1,Introduction,"Invariance to geometrical transformations has been long sought-after in the field of machine learning [6,12]. The strength of equipping models with inductive biases to these transformations was shown by the introduction of convolutional neural networks (CNNs) [13]. Following the success of CNNs, [7] generalized the convolution operator to commute with geometric transformation groups other than translations, introducing group-convolutional neural networks (G-CNNs), which have been shown to outperform conventional CNNs [2,11,19,20].Early G-CNNs were mainly concerned with operating on 2D inputs. With the increase in computing power, G-CNNs were extended to 3D G-CNNs. Volumetric data is prevalent in many medical settings, such as in analyzing protein structures [15] and medical image analysis [5,21,26,27]. Equivariance to symmetries such as scaling and rotations is essential as these symmetries often naturally occur in volumetric data. Equivariance to the group of 3D rotations, SO (3), remains a non-trivial challenge for current approaches due to its complex structure and non-commutative properties [19].An important consideration regarding 3D convolutions that operate on volumetric data is overfitting. Due to the dense geometric structure in volumetric data and the high parameter count in 3D convolution kernels, 3D convolutions are highly susceptible to overfitting [14]. G-CNNs have been shown to improve generalization compared to CNNs [20,23]. However, G-CNNs operating on discrete subgroups can exhibit overfitting to these discrete subgroups [3], failing to obtain equivariance on the full continuous group. This effect is amplified for 3D G-CNNs, limiting their improved generalization capabilities."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,,Contributions.,"In this work, we introduce regular continuous group convolutions equivariant to SE(3), the group of roto-translations. Motivated by the work on separable group convolutions [11], we separate our SE(3) kernel in a continuous SO(3) and a spatial convolution kernel. We randomly sample discrete equidistant SO(3) grids to approximate the continuous group integral. The continuous SO(3) kernels are parameterized via radial basis function (RBF) interpolation on a similarly equidistantly spaced grid. We evaluate our method on several challenging volumetric medical image classification tasks from the MedM-NIST [26,27] dataset. Our approach consistently outperforms regular CNNs and discrete SE(3) subgroup equivariant G-CNNs and shows significantly improved generalization capabilities. To this end, this work offers the following contributions.1. We introduce separable regular SE(3) equivariant group convolutions that generalize to the continuous setting using RBF interpolation and randomly sampling equidistant SO(3) grids. 2. We show the advantages of our approach on volumetric medical image classification tasks over regular CNNs and discrete subgroup equivariant G-CNNs, achieving up to a 16.5% gain in accuracy over regular CNNs. 3. Our approach generalizes to SE(n) and requires no additional hyperparameters beyond setting the kernel and sample resolutions. 4. We publish our SE(3) equivariant group convolutions and codebase for designing custom regular group convolutions as a Python package.1 Paper Outline. The remainder of this paper is structured as follows. Section 2 provides an overview of current research in group convolutions. Section 3 introduces the group convolution theory and presents our approach to SE(3) equivariant group convolutions. Section 4 presents our experiments and an evaluation of our results. We give our concluding remarks in Sect. 5."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,2,Literature Overview,"Since the introduction of the group convolutional neural network (G-CNN), research in G-CNNs has grown in popularity due to their improved performance and equivariant properties over regular CNNs. Work on G-CNNs operating on volumetric image data has primarily been focused on the 3D roto-translation group SE(3) [19,20,22]. CubeNet was the first introduced 3D G-CNN, operating on the rotational symmetries of the 3D cube [22]. The approach presented in [20] similarly works with discrete subsets of SE(3). These approaches are not fully equivariant to SE (3). Steerable 3D G-CNNs construct kernels through a linear combination of spherical harmonic functions, obtaining full SE(3) equivariance [19]. Other approaches that are fully SE(3) equivariant are the Tensor-Field-Network [18] and N-Body networks [17]. However, these operate on point clouds instead of 3D volumes."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,3,Separable SE(n) Equivariant Group Convolutions,"This work introduces separable SE(3) equivariant group convolutions. However, our framework generalizes to SE(n). Hence, we will describe it as such. Section 3.1 presents a brief overview of the regular SE(n) group convolution. Section 3.2 introduces our approach for applying this formulation to the continuous domain."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,3.1,Regular Group Convolutions,"The traditional convolution operates on spatial signals, i.e., signals defined on R n . Intuitively, one signal (the kernel) is slid across the other signal. That is, a translation is applied to the kernel. From a group-theoretic perspective, this can be viewed as performing the group action from the translation group. The convolution operator can then be formulated in terms of the group action. By commuting to a group action, the group convolution produces an output signal that is equivariant to the transformation imposed by the corresponding group.The SE(n) Group Convolution Operator. Instead of operating on signals defined on R n , SE(n)-convolutions operate on signals defined on the group SE(n) = R n SO(n). Given an n-dimensional rotation matrix R, and SE(3)signals f and k, the SE(n) group convolution is defined as follows:The Lifting Convolution Operator. Input data is usually not defined on SE(n). Volumetric images are defined on R 3 . Hence, the input signal should be lifted to SE(n). This is achieved via a lifting convolution, which accepts a signal f defined on R n and applies a kernel k defined on SO(n), resulting in an output signal on SE(n). The lifting convolution is defined as follows:The lifting convolution can be seen as a specific case of group convolution where the input is implicitly defined on the identity group element."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,3.2,Separable SE(n) Group Convolution,"The group convolution in Eq. 1 can be separated into a convolution over R followed by a convolution over R n by assumingThis improves performance and significantly reduces computation time [11].The Separable SE(n) Kernel. Let i and o denote in the input and output channel indices, respectively. We separate the SE(3) kernel as follows:Here, k SO(n) performs the channel mixing, after which a depth-wise separable spatial convolution is performed. This choice of separation is not unique. The channel mixing could be separated from the SO(n) kernel. However, this has been shown to hurt model performance [11].Discretizing the Continuous SO(n) Integral. The continuous group integral over SO(n) in Eq. 1 can be discretized by summing over a discrete SO(n) grid. By randomly sampling the grid elements, the continuous group integral can be approximated [24]. However, randomly sampled kernels may not capture the entirety of the group manifold. This will result in a noisy estimate. Therefore, we constrain our grids to be uniform, i.e., grid elements are spaced equidistantly.Similarly to the authors of [2], we use a repulsion model to generate SO(n) grids of arbitrary resolution."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,,Continuous SO(n) Kernel with Radial Basis Function Interpolation.,"The continuous SO(n) kernel is parameterized via a similarly discrete SO(n) uniform grid. Each grid element R i has corresponding learnable parameters k i . We use radial basis function (RBF) interpolation to evaluate sampled grid elements. Given a grid of resolution N , the continuous kernel k SO(n) is evaluated for any R as:Here, a d,ψ (R, R i ) represents the RBF interpolation coefficient of R corresponding to R i obtained using Gaussian RBF ψ and Riemannian distance d. The uniformity constraint on the grid allows us to scale ψ to the grid resolution dynamically. This ensures that the kernel is smooth and makes our approach hyperparameter-free."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4,Experiments and Evaluation,"In this section, we present our results and evaluation. "
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.1,Evaluation Methodology,"From here on, we refer to our approach as the SE(3)-CNN. We evaluate the SE(3)-CNNs for different group kernel resolutions. The sample and kernel resolutions are kept equal. We use a regular CNN as our baseline model. We also compare discrete SE(3) subgroup equivariant G-CNNs. K-CNN and T-CNN are equivariant to the 180 and 90 • rotational symmetries, containing 4 and 12 group elements, respectively. All models use the same ResNet [8] architecture consisting of an initial convolution layer, two residual blocks with two convolution layers each, and a final linear classification layer. Batch normalization is applied after the first convolution layer. In the residual blocks, we use instance normalization instead. Max spatial pooling with a resolution of 2 × 2 × 2 is applied after the first residual block. Global pooling is applied before the final linear layer to produce SE(3) invariant feature descriptors. The first layer maps to 32 channels. The residual blocks map to 32 and 64 channels, respectively. For the G-CNNs, the first convolution layer is a lifting convolution, and the remainders are group convolutions. All spatial kernels have a resolution of 7 × 7 × 7. Increasing the group kernel resolution increases the number of parameters. Hence, a second baseline CNN with twice the number of channels is included. The number of parameters of the models is presented in Table 1.We evaluate the degree of SE(3) equivariance obtained by the SE(3)-CNNs on OrganMNIST3D [4,25] and rotated OrganMNIST3D. For rotated Organ-MNIST3D, samples in the test set are randomly rotated. We further evaluate FractureMNIST3D [9], NoduleMNIST3D [1], AdrenalMNIST3D [27], and SynapseMNIST3D [27] from the MedMNIST dataset [26,27]. These volumetric image datasets form an interesting benchmark for SE(3) equivariant methods, as they naturally contain both isotropic and anisotropic features. All input data has a single channel with a resolution of 28 × 28 × 28. Each model is trained for 100 epochs with a batch size of 32 and a learning rate of 1 × 10 -4 using the Adam [10] optimizer on an NVIDIA A100 GPU. The results are averaged over three training runs with differing seeds."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.2,SE(3) Equivariance Performance,"Table 1 shows the accuracies and accuracy drops obtained by the evaluated models on the OrganMNIST3D test set and rotated test set. The decrease in accuracy is calculated as the percentage of the difference between the test scores on the test set and the rotated test set. Both baselines suffer from a high accuracy drop. This is expected, as these models are not equivariant to SE(3). K-CNN and T-CNN fare better. Due to its higher SO(3) kernel resolution, T-CNN outperforms K-CNN. However, these methods do not generalize to the SE(3) group. The SE(3)-CNNs obtain significantly lower drops in accuracy, showing their improved generalization to SE(3). The SE(3)-CNNs at sample resolutions of 12 and 16 also reach higher accuracies than both baseline models. As the sample resolution increases, performance on the standard test set shows a more pronounced increase than on the rotated test set. This results in a slight increase in accuracy drop. At a sample resolution of 16, accuracy on the standard test set decreases while the highest accuracy is obtained on the rotated test set. A high degree of SE(3) equivariance seems disadvantageous on OrganMNIST3D. This would also explain why T-CNN achieved the highest accuracy on the standard test set, as this model generalizes less to SE(3). OrganMNIST3D contains samples aligned to the abdominal window, resulting in high isotropy. This reduces the advantages of SE(3) equivariance."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.3,Performance on MedMNIST,"The accuracies obtained on FractureMNIST3D, NoduleMNIST3D, AdrenalM-NIST3D, and SynapseMNIST3D are reported in Table 2. The SE(3)-CNNs obtain the highest accuracies on all datasets. On FreactureMNIST3D, the highest accuracy is achieved by the SE(3)-CNN (12). Both K-CNN and T-CNN achieve an accuracy very similar to the baseline models. The baseline-big model slightly outperforms both K-CNN and T-CNN. On NoduleMNIST3D, SE(3)-CNN (6) and SE(3)-CNN (8) achieve the highest accuracy, with SE(3)-CNN (12) performing only slightly lower. K-CNN and T-CNN outperform both baseline models. On AdrenalMNIST3D, the differences in accuracy between all models are the lowest. SE(3)-CNN ( 16) obtains the highest accuracy, whereas the baseline model obtains the lowest. The baseline-big model outperforms K-CNN and T-CNN. On SynapseMNIST3D, we again observe a significant difference in performance between the SE(3)-CNNs and the other models. SE(3)-CNN (6) obtained the highest performance. T-CNN outperforms K-CNN and both baseline models. However, the baseline-big model outperforms K-CNN. On NoduleMNIST3D and AdrenalMNIST3D, only a slight performance gain is achieved by the SE(3)-CNNs. This is likely due to the isotropy of the samples in these datasets. In these cases, SE(3) equivariance is less beneficial. In contrast, FractureMNIST3D and SynapseMNIST3D are more anisotropic, resulting in significant performance gains of up to 16.5%.  "
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.4,Model Generalization,"The scores obtained on both the train set and test sets of SynapseMNIST3D in Figs. 1a and1b, respectively. We observed similar behavior on all datasets. Figure 1 shows a stark difference between the SE(3)-CNNs and the other models. The baselines and K-CNN and T-CNN converge after a few epochs on the train set. SE(3)-CNN ( 16) requires all 100 epochs to converge on the train set. SE(3)-CNN (4) does not converge within 100 epochs. This improvement during the training window is also observed in the test scores. This suggests that SE(3)-CNNs suffer less from overfitting, which results in improved model generalization. K-CNN and T-CNN behave similarly to the baselines. We hypothesize that this results from the weight-sharing that occurs during the RBF interpolation.We do observe a higher variance in scores of the SE(3)-CNNs, which we attribute to the random nature of the convolution kernels."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,4.5,Future Work,"With an increase in the sample resolution, a better approximation to SE(3) equivariance is achieved. However, we observe that this does not necessarily improve model performance, e.g., in the case of isotropic features. This could indicate that the equivariance constraint is too strict. We could extend our approach to learn partial equivariance. Rather than sampling on the entire SO(3) manifold, each group convolution layer could learn sampling in specific regions. This suggests a compelling extension of our work, as learning partial invariance has shown to increase model performance [16]."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,5,Conclusion,"This work proposed an SE(3) equivariant separable G-CNN. Equivariance is achieved by sampling uniform kernels on a continuous function over SO(3) using RBF interpolation. Our approach requires no additional hyper-parameters compared to CNNs. Hence, our SE(3) equivariant layers can replace regular convolution layers. Our approach consistently outperforms CNNs and discrete subgroup equivariant G-CNNs on challenging medical image classification tasks. We showed that 3D CNNs and discrete subgroup equivariant G-CNNs suffer from overfitting. We showed significantly improved generalization capabilities of our approach. In conclusion, we have demonstrated the advantages of equivariant methods in medical image analysis that naturally deal with rotation symmetries. The simplicity of our approach increases the accessibility of these methods, making them available to a broader audience."
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,,,
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,,Fig. 1 .,
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,,Table 1 .,
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis,,Table 2 .,
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,1,Introduction,"Tau accumulation in the form of neurofibrillary tangles in the brain is an important pathology hallmark in Alzheimer's disease (AD) [1,2]. With the rapid development of imaging technology, tau positron emission tomography (PET) allows us to measure the local concentration level of tau pathology in-vivo, which is proven to be a valuable tool for the differential diagnosis of dementia in routine clinical practice. As the converging consensus that the disease progression is closely associated with the spreading of tau aggregates [3], it is vital to characterize the spatiotemporal patterns of tau propagation from the longitudinal tau-PET scans.The human brain is a complex system that is biologically wired by white matter fibers [4]. Such an optimized information-exchanging system supports transient self-organized functional fluctuations. Unfortunately, the concept of fast transport also applies to toxic tau proteins that hijack the network to spread rapidly throughout the brain. As shown in Fig. 1(a), the stereotypical spreading of the tau pathology facilitates the increase of whole-brain tau SUVR (standard uptake value ratio) as the stage of the disease progresses from mild to severe. In this context, many graph diffusion models have been proposed to model the temporal patterns of tau propagation. For example, the network diffusion model [5,6] has been used to predict the future accumulation of pathological burdens where the spreading pathways are constrained by the network topology. However, current computational models usually assume the system dynamics is a linear process [7][8][9], where such gross simplification might be responsible for the inconsistent findings on tau propagation. For instance, the eigenvectors of graph Laplacian matrix (corresponding to the adjacency matrix of the underlying brain network) have been widely used as the basis functions to fit the longitudinal changes of pathological burdens on each brain region. Supposing that future changes follow the same dynamics, we can forecast the tau accumulations via extrapolation in the temporal domain. It is clear that these methods only model the focal change at each node, with no power to explain the tau propagation mechanism behind the longitudinal change, such as the questions ""Which regions are actively disseminating tau aggregates?"", ""Does no change of tau SUVR indicate not being affected or just passing on the tau aggregates?"".To answer these fundamental questions, we put our spotlight on the spreading flows of tau aggregates. As shown in Fig. 1(b), it is computationally challenging to find the directed region-to-region flows that can predict the tau accumulations over time. We cast it into a well-posed problem by assuming the local development of tau pathology and the spreading of tau aggregates form a dynamic energy transport system [10]. In the analogy of gravity that makes water flow downward, the cascade of tau build-up generates a potential energy field (PEF) that drives the spreading of tau aggregates to propagate from high to low tau SUVR regions. As we constrain the tau spreading flows on top of the network topology, we translate the tau-specific transport equation into an equivalent graph neural network (GNN) [11], where the layer-by-layer manner allows us to effectively characterize the tau spreading flows from a large amount of longitudinal tau-PET scans. Since the deep model of GNN often yields over-smoothed PEF, we further tailor a new transport equation by introducing the total variation (TV) on the gradient of PEF, which prompts a new deep model (coined TauFlowNet) free of the vanishing flow issue. Specifically, we trace the root cause of the over-smoothing issue in GNN up to the 2 -norm Lagrangian mechanics of graph diffusion process that essentially encourages minimizing the squared energy changes. Thus, one possible solution is to replace the 2 -based regularization term with the TV constraint on the gradient of PEF. After that, the Euler-Lagrange (E-L) equation of the new Lagrangian mechanics describes new dynamics of tau propagation steered by a collection of max flows that minimize the absolute value of overall potential energies in the system. In this regard, we present a generative adversarial network (GAN) to find the max flows (in the discriminator model) that (i) follow the physics principle of transport equation (in the generator model) and (ii) accurately predict the future tau accumulation (as part of the loss function). Therefore, our TauFlowNet is an explainable deep model to the extent that the physics principle provides the system-level underpinning of tau spreading flows and the application value (such as prediction accuracy) is guaranteed by the mathematics insight and the power of deep learning.We have applied our TauFlowNet on the longitudinal neuroimaging data in ADNI dataset. We compare the prediction accuracy of future tau accumulation with the counterpart methods and explore the propagation mechanism of tau aggregates as the disease progresses, where our physics-informed deep model yields more accurate and interpretable results. The promising results demonstrate great potential in discovering novel neurobiological mechanisms of AD through the lens of machine learning."
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,2,Methods,"In the following, we first elucidate the relationship between GNN, E-L equation, and Lagrangian mechanics, which sets the stage for the method formulation and deep model design of our TauFlowNet in Sect. 2.1. Then, we propose the TV-based graph regularization for GAN-based deep learning in Sect. 2.2, which allows us to characterize the spreading flow of tau aggregates from longitudinal tau-PET scans.Suppose the brain network is represented by a graph G = (V , W ) with N nodes (brain regions) V = {v i |i = 1, . . . , N } and the adjacency matrix W = w ij N i,j=1 ∈ R N ×N describing connectivity strength between any two nodes. For each node v i , we have a graph embedding vectorindicates the feature difference between v i and v j weighed by the connectivity strength w ij . Thus, the graph diffusion process [12] can be formulated as ∂x (t)  ∂t = div(∇ G x(t)), where the evolution of embedding vectors x = [x i ] N i=1 is due to network flux measured by the divergence. Several decades ago, the diffusion process ∂x (t)  ∂t = div(∇x(t)) has been widely studied in image processing [13], which is the E-L equation of the functional min x |∇x| 2 dx. By replacing the 1D gradient operator (∇x) ij = x ix j defined in the Euclidean space with the graph gradient (∇ G x) ij , it is straightforward to find that the governing equation in graph diffusion process2 dx on top of the graph topology.The GNN depth is blamed for over-smoothing [14][15][16] in graph representation learning. We attribute this to the isotropic smoothing mechanism formulated in the 2 -norm. Connecting GNN to calculus of variations provides a principled way to design new models with guaranteed mathematics and explainability."
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,2.1,Problem Formulation for Discovering Spreading Flow of Tau Propagation,"Neuroscience Assumption. Our brain's efficient information exchange facilitates the rapid spread of toxic tau proteins throughout the brain. To understand this spread, it's essential to measure the intrinsic flow information (such as flux and bandwidth) of tau aggregates in the complex brain network.Problem Formulation from the Perspective of Machine Learning. The overarching goal is to estimate the time-dependent flow field, where f ij (t) stands for the directed flow from the region v i to v j . As the toy example shown in Fig. 1(b), there are numerous possible solutions for F given the longitudinal change x. To cast this ill-posed problem into a well-defined formulation, we conceptualize that the tau propagation in each brain forms a unique dynamic transport system of the brain network, and the spreading flow is driven by a tau-specific potential energy field, where u i (t) is output of a nonlinear process φ reacting to the tau accumulation x i at the underlying region v i , i.e., u i = φ(x i ). The potential energy field drives the flow of tau aggregates in the brain, similar to the gravity field driving water flow. Thereby the spreading of tau is defined by the gradient of potential energy between connected regions:Thus, the fundamental insight of our model is that the spreading flow f ij (t) is formulated as an ""energy transport"" process of the tau potential energy field. Taking together, the output of our model is a mechanistic equation M (•) of the dynamic system that can predict the future flow based on the history flow sequences, i.e.,Transport Equation for Tau Propagation in the Brain. A general continuity transport equation [10] can be formulated in a partial differential equation (PDE) as:where q is the flux of the potential energy u (conserved quantity). The intuition of Eq. 2 is that the change of energy density (measured by regional tau SUVR x) leads to the energy transport throughout the brain (measured by the flux of PEF). As flux is often defined as the rate of flow, we further define the energy flow as q ij = α • f ij , where α is a learnable parameter characterizing the contribution of the tau flow f ij to the potential energy flux q ij . By plugging u t = φ(x t ) and q ij = α • f ij into Eq. 2, the energy transport process of tau spreading flow f can be described as:Note, φ and α are trainable parameters that can be optimized through the supervised learning schema described below."
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,2.2,TauFlowNet: An Explainable Deep Model Principled with TV-Based Lagrangian Mechanics,"To solve the flow field f in Eq. 3, the naïve deep model is a two-step approach (shown in the left red panel of Fig. 2).(1) Estimate the PEF u by fixing the flow f . By letting f = ∇ G u (in Eq. 1), the solution of u follows a reaction process u = φ(x) and a graph diffusion process ∂u ∂t = -α -1 div ∇ G u = -α -1 u, where = div(∇ G ) is the graph Laplacian operator. The parameters of φ and α can be learned using a multi-layer perceptron (MLP) and a graph convolution layer (GCN), respectively. Thus, the input is the observed tau SUVR x t and the loss function aims to minimize the prediction error from x t to x t+1 .(2) Calculate spreading flow f . Given u, it is straightforward to compute each flow f ij (t) by Eq. 1. In Sect. 2.1, we have pointed out that the GNN architecture is equivalent to the graph diffusion component in Eq. 3. Since the PDE of the graph diffusion process ∂u ∂t =u is essentially the Euler-Lagrange (E-L) equation of the quadratic functional J (u) = min u ∇ G u 2 du, the major issue is the ""over-smoothness"" in u that might result in vanishing flows (i.e., f → 0).To address the over-smoothing issue, we propose to replace the quadratic Laplacian regularizer with total variation, i.e., J TV (u) = min u ∇ G u du, which has been successfully applied in image denoising [17] and reconstruction [18]. Since |•| in J TV is not differentiable at 0, we introduce the latent flow variable f and reformulate the TVbased functional as J TV (u, f ) = min u (f ⊗∇ G u)du, where ⊗ is Hadamard operation between two matrices. Recall that the flow f ij has directionality. Thus, the engineering trick of element-wise operation f ij ∇ G u ij keeps the degree always non-negative as we take the absolute value, which allows us to avoid the undifferentiable challenge.After that, we boil down the minimization of J TV (u) into a dual min-max functional aswhere we maximize f such that J TV (u, f ) is close enough to J TV (u). In this regard, the E-L equation from the Gâteaux variations leads to two coupled PDEs:The alternative solution for Eq. 4 is that we minimize PEF u through the Lagrangian mechanics defined in the transport equation ∂u ∂t = -φ α -1 div(f ) where the system dynamics is predominated by the maximum flow field f . Since the accurate estimation of flow field f (t) and PEF u(t) is supposed to predict the future tau accumulation x(t +1) by, we can further tailor the min-max optimization for Eq. 4 into a supervised learning scenario as the TauNetFlow described next."
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,,TauFlowNet: A GAN Network Architecture of TV-Based Transport,"Equation. Here, we present an explainable deep model to uncover the spreading flow of tau aggregates f from the longitudinal tau-PET scans. Our deep model is trained to learn the system dynamics (in Eq. 4), which can predict future tau accumulations. The overall network architecture of TauFlowNet is shown in Fig. 2, which consists of a generator (left) and a discriminator module (right). The generator is essentially our initial GNN model of the transport equation that consists of a reaction process φ and a graph diffusion process. Specifically, the generator consists of (i) a MLP to project the input regional tau SUVR x t into the potential energy filed u t through a nonlinear reaction process u t = φ(x t ) (green dashed box), (ii) a GCN layer to transport potential energy along the connectome pathways, resulting in the u t+1 = GCN (u t ) (purple dashed box), and (iii) another MLP to generate x t+1 from u t+1 via x t+1 = φ -1 (u t+1 ) (red dashed box). The discriminator module is designed to synthesize the future PEF u t+1 based on the current PEF u t and current estimation of tau spreading flow f ij (orange dash box), i.e., ũt+1 = u t + N j=1 f ij (t). Then, we train another GCN layer to generate the synthesized xt+1 from ũt+1 via xt+1 = GCN (ũ t+1 ) (blue dashed box). The driving force of our TauFlowNet is to minimize (1) the MAE (mean absolute error) between the output of the generator x t+1 and the observed tau SUVR, and (2) the distance between the synthesized tau SUVR xt+1 (from the discriminator) and the output of generator x t+1 (from the transport equation). In the spirit of probabilistic GAN [19], we use one loss function L D = D(x t+1 ) + [m -D(G(x t ))] + to train the discriminator (D) and the other one L G = D(G(x t )) to train the generator (G), where m denotes the positive margin and the operator [•] + = max(0, •). Minimizing L G is similar to maximizing the second term of L D except the non-zero gradient when D(G(x t )) ≥ m."
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,3,Experiments,"In this section, we evaluated the performance of the proposed TauFlowNet for uncovering the latent flow of tau spreading on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset (https://adni.loni.usc.edu/). In total, 163 subjects with longitudinal tau-PET scans are used for training and testing the deep model. In addition, each subject has T1-weighted MRI and diffusion-weighted imaging (DWI) scan, from which we construct the structural connectome. Destrieux atlas [20] is used to parcellate each brain into 160 regions of interest (ROIs), which consist of 148 cortical regions (frontal lobe, insula lobe, temporal lobe, occipital lobe, parietal lobe, and limbic lobe) and 12 sub-cortical regions (left and right hippocampus, caudate, thalamus, amygdala, globus pallidum, and putamen). Following the clinical outcomes, we partition the subjects into the cognitive normal (CN), early-stage mild cognitive impairment (EMCI), late-stage MCI (LMCI), and AD groups. We compare our TauFlowNet with classic graph convolutional network (GCN) [21] and deep neural network (DNN) [22]. We use 5-fold cross-validation to evaluate the prediction performance and examine the spreading patterns of tau aggregates in different clinic groups."
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,3.1,Evaluate the Prediction Accuracy of Future Tau Accumulation,"We first evaluate the prediction performance between the ground truth and the estimated SUVR values, where we use the mean absolute error (MAE) to quantify the prediction accuracy. The statistics of MAE by our TauFlowNet, GCN, and DNN are shown in the first column (with shade) of Table 1. To further validate the robustness of our model, we add uncorrelated additive Gaussian noises to the observed SUVR measurements. The prediction accuracies with respect to different noise levels are listed in the rest columns of Table 1. It is clear that our TauFlowNet consistently outperforms the other two deep models. The performance of GCN is worse than DNN within the same network depth, which might be due to the over-smoothing issue.As part of the ablation study, we implement the two-step approach (the beginning of Sect. 2.2), where we train the model (MLP + GCN) shown in the left panel of Fig. 2 to obtain further tau accumulation. Since the deep model in this two-step approach is formalized from the PDE, we call this degraded version as PDENet. We display the result of in the last row of Table 1. Compared to PDENet, our TauFlowNet (in GAN architecture) takes advantage of TV constraint to avoid over-smoothing and integrates two steps (i.e., estimating PEF and uncovering spreading flows) into a unified neural network, thus significantly enhancing the prediction accuracy. "
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,3.2,Examine Spatiotemporal Patterns of the Spreading Flow of Tau Aggregates,"We examine the pattern of spreading flows on an individual basis (Fig. 3a) and cross populations (Fig. 3b). First, we visualize the top flows (ranked in terms of flow volume) uncovered in a CN subject. It is apparent that subcortex-cortex flows are the predominant patterns, where most of the tau aggregates spread from subcortical regions (globus pallidus, hippocampus, and putamen) to the temporal lobe, limbic lobe, parietal lobe, and insula lobe. Note, we find inferior temporal gyrus (t 6 ) and entorhinal cortex (t 8 ) are actively involved in the subcortex-cortex flows, which are the footprints of early stage tau propagation frequently reported in many pathology studies [23]. Second, we show the top-ranked population-wise average tau spreading flows for CN, EMCI, LMCI, and AD groups in Fig. 3b. As the disease progresses, the subcortex-cortex flows gradually switch to cortex-cortex flows. After tau aggregates leave the temporal lobe, the tau propagation becomes widespread throughout the entire cortical region. "
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,4,Conclusion,"In this paper, we propose a physics-informed deep neural network (TauFlowNet) by combining the power of dynamic systems (with well-studied mechanisms) and machine learning (fine-tuning the best model) to discover the novel propagation mechanism of tau spreading flow from the longitudinal tau-PET scans. We have evaluated our TauFlowNet on ADNI dataset in forecasting tau accumulation and elucidating the spatiotemporal patterns of tau propagation in the different stages of cognitive decline. Our physics-informed deep model outperforms existing state-of-the-art methods in terms of prediction accuracy and model explainability. Since the region-to-region spreading flow provides rich information for understanding the tau propagation mechanism, our learning-based method has great applicability in current AD studies."
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,,Fig. 1 .,
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,,Fig. 2 .,
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,,Fig. 3 .,
TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation,,Table 1 .,
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,1,Introduction,"Lowering radiation dose is desired in the computed tomography (CT) examination. Various strategies, i.e., lowering incident photons directly (low-mAs), reducing sampling views (sparse-view) and reducing sampling angles (limitedview) can be used for low-dose CT imaging. However, the reconstructed images under these conditions would suffer from severe quality degradation. A number of reconstruction algorithms have been proposed to improve low-dose CT image quality [1][2][3][4][5][6]. Among them, deep learning (DL)-based methods have shown great promise for low-dose CT imaging, including methods that learn noise distribution features from the image domain to directly reduce noise and artifacts in the reconstructed image [1,2], as well as methods to improve the reconstruction quality based on the sinogram domain [3]. In addition, cross-domain learning methods are able to learn CT data features from dual domains to construct models that approximate traditional reconstruction process [4][5][6].However, most DL-based CT reconstruction methods are condition-specific, i.e., dose-specific, and geometry-specific. In the dose-specific case, these methods are constructed on the dataset at one specific dose level, which might fail to obtain promising result at other dose levels. Centralized learning via collecting data at different dose levels is an alternative way, but it is difficult to collect sufficient data efficiently. In the geometry-specific case, the DL-based methods, especially the cross-domain learning methods, usually reconstruct the final image from the measured sinogram data with a specific imaging geometry that takes the geometry parameters into account during reconstruction. However, the geometry parameters in the scanner are vendor-specific and different from each other. Then the DL-based methods trained on data from one geometry would fail to be transferred to those from the other geometry due to the different characteristics distributions and big data heterogeneity among different geometries. Xia et al. constructed a framework for modulating deep learning models based on CT imaging geometry parameters to improve the reconstruction performance of the DL models under multiple CT imaging geometries [7], but the method did not consider model degradation due to variations in scanning conditions. Multi-task learning methods can be used to address this issue, but they are limited by the tedious design of auxiliary tasks, which leads to lower efficiency [8,9], and the privacy issues caused by the sharing of data are also limitations of these methods.Different from the centralized learning, federated learning has potential to the train model on decentralized data without the need to centralized or share data, which provides significant benefits over centralized learning methods [10][11][12][13]. Federated learning has made achievements in medical imaging [14] and applications in CT reconstruction, for example, Li et al. presented a semi-centralized federated learning method to promote the generalization performance of the learned global model [15], Yang et al. propose a hypernetwork-based federated learning method to construct personalized CT imaging models for local clients [16]. However these methods are constructed on image domain and do not consider the perturbations of multi-source CT data on the sinogram domain, thus the local specificity is insufficien and the generalization of the model still needs to be improved.Inspired by the previous work [4] and federated learning framework, we propose a condition generalization method under a federated learning framework to reconstruct CT images from different conditions, i.e., different dose levels, different geometries, and different sampling shcemes. The proposed method is termed as federated condition generalization (FedCG). Specifically, the proposed FedCG method can be treated as the extension of iRadonMAP [4] to the federated learning framework. And it leverages a cross-domain learning approach: individual-client sinogram learning, and cross-client image reconstruction for condition generalization. In each client, the sinogram at each individual condition is processed similarly to that in iRadonMAP, then the latent characteristics of reconstructed CT images at all conditions are processed through the framework via a condition generalization network in the server. The condition generalization network considers latent common characteristics in the CT images at all conditions and preserves the client-specific characteristics in each condition. Different from the existing FL framework, the server in the proposed FedCG holds a large amount of labeled data that is closer to real world. We validate the proposed FedCG method on two simulation studies, including three different dose levels at the same geometry, and three different sampling shcemes at the different geometries. The effectiveness of FedCG has been validated with significant performance improvements on both tasks compared with a number of competing methods and FL methods, as well as comprehensive ablation studies. "
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,2,Method,
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,2.1,iRadonMAP,"As a cross-domain learning framework for Radon inversion in CT reconstruction, iRadonMAP consists of a sinogram domain sub-network, an image domain subnetwork, and a learnable back-projection layer, which can be written as follows [4]:where μ is the final image, p is the sinogram data, F s (•; θ s ) and F i (•; θ i ) denote the sinogram domain sub-network (i.e., SinoNet) and the image domain subnetwork (i.e., ImageNet) of iRadonMAP,) is learnable back projection layer, the details of F R -1 (•; θ R -1 ) are available in [4]. θ denotes the parameters of networks.Although iRadonMAP can obtain promising reconstruction result, the generalization is still an area that is poorly exploited. When iRadonMAP is trained on CT data with a particular condition (i.e., specific dose level and imaging geometry) and is inferred on CT data with a different condition, it would become unstable as a result of data heterogeneity among different conditions. Federated learning framework is an alternative strategy to address this issue."
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,2.2,Proposed FedCG Method,"In this study, inspired by the previous work [4] and federated learning framework, we propose a condition generalization method under a federated learning framework (FedCG) to reconstruct CT images from different conditions as an extension of the cross-domain learning framework iRadonMAP in federal learning. Specifically, the proposed FedCG is characterized by the following aspects: Individual-client Sinogram Learning. Due to the big data heterogeneity among different conditions and data privacy preservation, as shown in Fig. 1, in the proposed FedCG, the sinogram data at each condition is processed via SinoNet and learnable back-projection layer as in the iRadonMAP, and the corresponding parameters are not exchanged to communication and augment data privacy when the clients have unique distributions. Furthermore, the individualclient sinogram learning strategy allows for processing condition-specific sionogram data, which can vary flexibly across clients and among different conditions. Then ImageNet can be utilized to reconstruct final CT images wherein the corresponding parameters are updated with the help of federated learning strategy. The central server collects the information from all clients without directly sharing the private data of each client.Central Data Guidance Training. Different from the existing federated learning framework, the central server has labeled data that is closer to real world, as shown in Fig. 1. In each training round, the central server model can obtain well-trained model with the labeled data, and then the corresponding parameters in the ImageNet can be updated as follows:where ω 0 is the weight of central model, ω k = n k n , where n k denotes the number of local iterations.Condition Generalization in FL. To fully consider the unique distribution in each client, inspired by Xia et al. [7], we introduce condition generalization (CG) network to learn the deep features across scanners and protocols. Specifically, the CG network in each client generates a specific normalized parameters vector according to the imaging geometry and scanning protocol:where ρ represents the condition parameter in each client, g 1 , • • • , g n are normalized imaging geometric parameters. C is a parameter that represents protocol parameter (i.e., dose level, sparse views, and limited angles). In the kth local client, ρ k is fed into the local model along with the input data. And in the central server, all condition parameters are fed into the central model. As shown in Fig. 1, multilayer perceptron (MLP) which consists of fully connected layers map ρ into high-dimensional condition vectors, and the vectors are used to modulate the feature map of the ImageNet as follows:h 1 , h 2 are MLPs with shared parameters. f is the feature map of network layer, f is the modulated feature map. Then, the total loss function of the proposed FedCG can be written as follows:where μ * 0 is the noise-free image in the central server, μ * k is the noise-free image in the kth client, μ 0 is the output image of central ImageNet, μ k is the output image of iRadonMAP in the kth client, p * k is the noise-free sinogram in the kth client, p k is the intermediate output of SinoNet in the kth client, λ is the parameter that controls the singoram loss function in the local client."
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,3,Experiments,
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,3.1,Dataset,"The experiments are carried out on three publicly available datasets (120, 000 CT images) [17][18][19], six private datasets from different scanners (Data #1: 1100 brain CT images, Data #2: 1500 chest CT images, Data #3: 847 body phantom CT images, Data #4: 1600 body phantom CT images, Data #5: 1561 abdomen CT images, Data #6: 1598 abdomen CT images). In the experiment, two different conditions are presented, i.e., different dose levels with different geometries (Condition #1), and different sampling shcemes with different geometries (Condition #2).In the Condition #1, the three publicly available datasets are collected for the central server, Data #1, Data #2, and Data #3 are selected for three local clients (Client #1, Client #2 and Client #3), respectively. We obtained the corresponding low-dose sinogram data at different dose levels from the normal-dose CT images based on the previous study, respectively [20]. The X-ray intensities of Clients #1, #2, #3 are 5e5, 2e5, 1e5. In Condition #2, the simulated limitedangle CT images, sparse-view CT images, and ultra-low-dose CT images from the publicly available datasets are collected for the central server. Data #4 contains simulated limited-angle cases (120 degrees of parallel beam, with full angles of 180 degrees), Data #5 contains simulated sparse-view cases (144 views, with full views of 1152) and Data #6 contains simulated ultra-low-dose cases (X-ray intensity of 5e4) are icollected for the three local clients, respectively. In the both experiments, ninety percent of data are used for training and the remaining for testing for all clients."
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,3.2,Implementation Details,"FedCG is constructed by Pytorch toolbox [21], training with an NVIDIA RTX A6000 graphics processing unit with 48 GB memory, and the CT simulation and reconstruction are carried out by the Astra toolbox [22]. The iRadonMAPs in all the local clients and the ImageNet in the central server are optimized by the RMSProp optimizer, and the learning rate of all the models is 2e-5. The number of training rounds is set to 1000, and the central ImageNet has 100 iterations per round while each local client has 10. ω 0 in the Eq. 2 is empirically set to 0.6. More details on the imaging geometries and architecture of the iRdaonMAP can be found in the supplementary materials."
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,4,Result,"In this work, five algorithms are selected for comparison. The classical FBP algorithm and the iRadonMAPs trained on condition-specific dataset, FedAvg [10], Fedprox [11] and FedBN [12]. Peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) and root mean square error (RMSE) are used to quantify reconstruction performance. "
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,4.1,Reuslt on Condition #1,"Figure 2 shows results reconstructed by all the competing methods on Condition #1 wherein the normal-dose FBP images are chosen as ground truth for comparison. The results show that the proposed FedCG produces the sharpest images with fine details at all clients, as apparent from the zoomed-in regions of interest (ROIs). Although iRadonMAP can suppress noise-induced artifacts to some extent, it might introduce undesired artifacts as indicated by the red arrows. FedAvg and FedProx can also produce sharp images, but the reconstruction of the fine structure details is less detailed than that in FedCG results. Moreover, the shifted values occur in both FedAvg and FedProx results, as indicated by the blue arrows. FedBN can reconstruct the textures with less noise, but with unsatisfactory performance in the fine texture recovery as indicated by purple arrows. Furthermore, the quantitative measurements also indicate that the proposed FedCG can obtain the best performance among all the competing methods. The possible reason might be that the labeled data in the central server provide sufficient prior information to promote FedCG reconstruction performance. More experimental results are listed in the supplementary materials."
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,4.2,Result on Condition #2,Figure 3 shows the results reconstructed by all the competing methods on Condition #2 wherein the normal-dose FBP images are chosen as ground truth for comparison. It can be observed that the iRadonMAP can produce promising reconstruction results that are closest to the ground truth as it is trained with the 
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,4.3,Ablation Experiments,"ω 0 plays a key role in the proposed FedCG reconstruction performance, then we conduct ablation experiments with different ω 0 on settings Condition #1. Figure 4 shows the mean value of PSNR, SSIM, and RMSE with different ω 0 settings. From the results, when ω 0 = 0, FedCG approaches to FedAvg with poor performance. When ω 0 ∈ [0, 0.2], both Client #1 and Client #2 obtain degraded reconstruction performance and Client #3 obtains improved reconstruction performance. And when ω 0 > 0.3, we can see that the reconstruction accuracy increases with increasing ω 0 for Client #1 and Client #2, but reconstruction accuracy for Client #3 degrades with increasing ω 0 . Setting ω 0 = 0.6 yields the best results with the weighted central server parameters leading substantial improvement in all quantitative measurements for Client #1 and Client #2, while setting ω 0 = 0.3 for client #3. The possible reason might be that the similar information are shared among the central server and Client #1, Client #2, but there is large heterogeneity between central server and Client #3. When ω 0 > 0.6, the performance for all clients degrades obviously due to an overly large weight which affects the unique information of each client. "
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,5,Conclusion,"In this work, we propose a condition generalization method under a federated learning framework to reconstruct CT images on different conditions. Experiments on different dose levels with different geometries, and different sampling shcemes with different geometries show that the proposed FedCG achieves improved reconstruction performance compared with the other competing methods at all the cases qualitatively and quantitatively."
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,,Fig. 1 .,
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,,Fig. 2 .,
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,,Fig. 3 .,
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,,Fig. 4 .,
Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_5.
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,1,Introduction,"Primary liver cancer is one of the most common and deadly cancer diseases in the world, and liver resection is a highly effective treatment [11,14]. The Couinaud segmentation [7] based on CT images divides the liver into eight functionally independent regions, which intuitively display the positional relationship between Couinaud segments and intrahepatic lesions, and helps surgeons for make surgical planning [3,13]. In clinics, Couinaud segments obtained from manual annotation are tedious and time-consuming, based on the vasculature used as rough guide (Fig. 1). Thus, designing an automatic method to accurately segment Couinaud segments from CT images is greatly demanded and has attracted tremendous research attention.However, automatic and accurate Couinaud segmentation from CT images is a challenging task. Since it is defined based on the anatomical structure of live vessels, even no intensity contrast (Fig. 1.(b)) can be observed between different Couinaud segments, and the uncertainty of boundary (Fig. 1.(d)) often greatly affect the segmentation performance. Previous works [4,8,15,19] mainly rely on handcrafted features or atlas-based models, and often fail to robustly handle those regions with limited features, such as the boundary between adjacent Couinaud segments. Recently, with the advancement of deep learning [5,10,18], many CNN-based algorithms perform supervised training through pixel-level Couinaud annotations to automatically obtain segmentation results [1,9,21]. Unfortunately, the CNN models treat all voxel-wise features in the CT image equally, cannot effectively capture key anatomical regions useful for Couinaud segmentation. In addition, all these methods deal with the 3D voxels of the liver directly without considering the spatial relationship of the different Couinaud segments, even if this relationship is very important in Couinaud segmentation. It can supplement the CNN-based method and improve the segmentation performance in regions without intensity contrast.In this paper, to tackle the aforementioned challenges, we propose a pointvoxel fusion framework that represents the liver CT in continuous points to better learn the spatial structure, while performing the convolutions in voxels to obtain the complementary semantic information of the Couinaud segments. Specifically, the liver mask and vessel attention maps are first extracted from the CT images, which allows us to randomly sample points embedded with vessel structure prior in the liver space and voxelize them into a voxel grid. Subsequently, points and voxels pass through two branches to extract features. The point-based branch extracts the fine-grained feature of independent points and explores spatial topological relations. The voxel-based branch is composed of a series of convolutions to learn semantic features, followed by de-voxelization to convert them back to points. Through the operation of voxelization and devoxelization at different resolutions, the features extracted by these two branches can achieve multi-scale fusion on point-based representation, and finally output the Couinaud segment category of each point. Extensive experiments on two publicly available datasets named 3Dircadb [20] and LiTS [2] demonstrate that our proposed framework achieves state-of-the-art (SOTA) performance, outperforming cutting-edge methods quantitatively and qualitatively. "
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,2,Method,"The overview of our framework to segment Couinaud segments from CT images is shown in Fig. 2, including the liver segmentation, vessel attention map generation, point data sampling and multi-scale point-voxel fusion network."
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,2.1,Liver Mask and Vessel Attention Map Generation,"Liver segmentation is a fundamental step in Couinaud segmentation task. Considering that the liver is large and easy to identify in the abdominal organs, we extracted the liver mask through a trained 3D UNet [6]. Different from liver segmentation, since we aim to use the vessel structure as a rough guide to improving the performance of the Couinaud segmentation, we employ another 3D UNet [6] to generate the vessel attention map more easily. Specifically, given a 3D CT image containing only the area covered by the liver mask (L), the 3D UNet [6] output a binary vessel mask (M ). A morphological dilation is then used to enclose more vessel pixels in the M -covered area, generating a vessel attention map (M ). We employ the BCE loss to supervise the learning process."
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,2.2,Couinaud Segmentation,"Based on the above work, we first use the M and the L to sample get point data, which can convert into a voxel grid through re-voxelization. The converted voxel grid embeds the vessel prior and also dilutes the liver parenchyma information. Inspired by [12], a novel multi-scale point-voxel fusion network then is proposed to simultaneously process point and voxel data through point-based branch and voxel-based branch, respectively, aiming to accurately perform Couinaud segmentation. The details of this part of our method are described below."
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Continuous Spatial Point Sampling Based on the Vessel Attention,"Map. In order to obtain the topological relationship between Couinaud segments, a direct strategy is to sample the coordinate point data with 3D spatial information from liver CT and perform point-wise classification. Hence, we first convert the image coordinate points I = i 1 , i 2 , ..., i t , i t ∈ R 3 in liver CT into the world coordinate points P = p 1 , p 2 , ..., p t , p t ∈ R 3 :where Spacing represents the voxel spacing in the CT images, Direction represents the direction of the scan, and Origin represents the world coordinates of the image origin. Based on equation(1), we obtain the world coordinate p t = (x t , y t , z t ) corresponding to each point i t in the liver space. However, directly feeding the transformed point data as input into the point-based branch undoubtedly ignores the vessel structure, which is crucial for Couinaud segmentation. where R denotes the rounding integer function. Based on this, we achieve arbitrary resolution sampling in the continuous space covered by the M . "
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Re,"where r denotes the voxel resolution, I [•] is the binary indicator of whether the coordinate pt belongs to the voxel grid (u, v, w), f t,c denotes the cth channel feature corresponding to pt , and N u,v,w is the number of points that fall in that voxel grid. Note that the re-voxelization in the model is used three times (as shown in Fig. 2), and the f t,c in the first operation is the coordinate and intensity, with c = 4. Moreover, due to the previously mentioned point sampling strategy, the converted voxel grid also inherits the vessel structure from the point data and dilutes the unimportant information in the CT images.Multi-scale Point-Voxel Fusion Network. Intuitively, due to the image intensity between different Couinaud segments being similar, the voxel-based CNN model is difficult to achieve good segmentation performance. We propose a multi-scale point-voxel fusion network for accurate Couinaud segmentation, take advantage of the topological relationship of coordinate points in 3D space, and leverage the semantic information of voxel grids. As shown in Fig. 2, our method has two branches: point-based and voxel-based. The features extracted by these two branches on multiple scales are fused to provide more accurate and robust Couinaud segmentation performance. Specifically, in the point-based branch, the input point data {(p t , f t )} passes through an MLP, denoted as E p , which aims to extract fine-grained features with topological relationships. At the same time, the voxel grid {V u,v,w } passes the voxel branch based on convolution, denoted as E v , which can aggregate the features of surrounding points and learn the semantic information in the liver 3D space. We re-transform the features extracted from the voxel-based branch to point representation through trilinear interpolation, to combine them with fine-grained features extracted from the point-based branch, which provide complementary information:where the superscript 1 of (p t , f 1 t ) indicates that the fused point data and corresponding features f 1 t are obtained after the first round of point-voxel operation. Then, the point data (p t , f 1 t ) is voxelized again and extracted point features and voxel features through two branches. Note that the resolution of the voxel grid in this round is reduced to half of the previous round. After three rounds of pointvoxel operations, we concatenate the original point feature f t and the features f 1 t , f 2 t , f 3 t with multiple scales, then send them into a point-wise decoder D, parameterized by a fully connected network, to predict the corresponding Couinaud segment category:where {0, 1, ..., 7} denotes the Couinaud segmentation category predicted by our model for the point p t . We employ the BCE loss and the Dice loss to supervise the learning process. More method details are shown in the supplementary materials."
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3,Experiments,
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3.1,Datasets and Evaluation Metrics,"We evaluated the proposed framework on two publicly available datasets, 3Dircadb [20] and LiTS [2]. The 3Dircadb dataset [20] contains 20 CT images with spacing ranging from 0.56 mm to 0.87 mm, and slice thickness ranging from 1 mm to 4 mm with liver and liver vessel segmentation labels. The LiTS dataset [2] consists of 200 CT images, with a spacing of 0.56 mm to 1.0 mm and slice thickness of 0.45 mm to 6.0 mm, and has liver and liver tumour labels, but without vessels. We annotated the 20 subjects of the 3Dircadb dataset [20] with the Couinaud segments and randomly divided 10 subjects for training and another 10 subjects for testing. For LiTS dataset [2], we observed the vessel structure on CT images, annotated the Couinaud segments of 131 subjects, and randomly selected 66 subjects for training and 65 for testing.We have used three widely used metrics, i.e., accuracy (ACC, in %), Dice similarity metric (Dice, in %), and average surface distance (ASD, in mm) to evaluate the performance of the Couinaud segmentation."
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3.2,Implementation Details,"The proposed framework was implemented on an RTX8000 GPU using PyTorch. Based on the liver mask has been extracted, we train a 3D UNet [6] on the 3Diradb dataset [20] to generate the vessel attention map of two datasets. Then, we sample T = 20, 000 points in each epoch to train our proposed multi-scale point-voxel fusion network each epoch. We perform scaling within the range of 0.9 to 1.1, arbitrary axis flipping, and rotation in the range of 0 to 5 • C on the input point data as an augmentation strategy. Besides, we use the stochastic gradient descent optimizer with a learning rate of 0.01, which is reduced to 0.9 times for every 50 epochs of training. All our experiments were trained 400 epochs, with a random seed was 2023, and then we used the model with the best performance on the training set to testing. "
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3.3,Comparison with State-of-the-Art Methods,"We compare our framework with several SOTA approaches, including voxelbased 3D UNet [6], point-based PointNet2Plus [16], and the methods of Jia et al. [9]. The method of Jia et al. is a 2D UNet [17] with dual attention to focus on the boundary of the Couinaud segments and is specifically used for the Couinaud segmentation task. We use PyTorch to implement this model and maintain the same implementation details as other methods. The quantitative and qualitative comparisons are shown in Table 1 and Fig. 3, respectively.Quantitative Comparison. Table 1 summarizes the overall comparison results under three metrics. By comparing the first two rows, we can see that Point-Net2Plus [16] and 3D UNet [6] have achieved close performance in the LiTS dataset [2], which demonstrates the potential of the point-based methods in the Couinaud segmentation task. In addition, the third row shows that Jia et al.'s [9] 2D UNet [17] as the backbone method performs worst on all metrics, further demonstrating the importance of spatial relationships. Finally, our proposed point-voxel fusion segmentation framework achieves the best performance. Especially on the 3Diradb dataset [20] with only 10 training subjects, the ACC and Dice achieved by our method exceed PointNet2Plus [16] and 3D UNet [6] by nearly 10 points, and the ASD is also greatly reduced, which demonstrates the effectiveness of the combining point-based and voxel-based methods.Qualitative Comparison. To further evaluate the effectiveness of our method, we also provide qualitative results, as shown in Fig. 3. The first two rows show that the vessel structure is used as the boundary guidance for Couinaud segmentation, but voxel-based 3D UNet [6] fails to accurately capture this key structural relationship, resulting in inaccurate boundary segmentation. Note that, it can be seen that our method can learn the boundary guidance provided by the portal vein (the last two rows), to deal with the uncertain boundary more robustly.Besides, compared with the 3D view, it is obvious that the voxel-based CNN methods are easy to pay attention to the local area and produce a large area of error segmentation, so the reconstructed surface is uneven. The point-based method obtains smooth 3D visualization results, but it is more likely to cause  segmentation blur in boundary areas with high uncertainty. Our method combines the advantages of point-based and voxel-based methods, and remedies their respective defects, resulting in smooth and accurate Couinaud segmentation."
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3.4,Ablation Study,"To further study the effectiveness of our proposed framework, we compared two ablation experiments: 1) random sampling of T points in the liver space, with-out considering the guidance of vascular structure, and 2) considering only the voxel-based branch, where the Couinaud segments mask is output by a CNN decoder. Figure 4 shows the ablation experimental results obtained on all the Couinaud segments of two datasets, under the Dice and the ASD metrics. It can be seen that our full method is significantly better than the CNN branch joint decoder method on both metrics of two datasets, which demonstrates the performance gain by the combined point-based branch. In addition, compared with the strategy of random sampling, our full-method reduces the average ASD by more than 2mm on eight Couinaud segments. This is because to the vessel structure-guided sampling strategy can increase the important data access between the boundaries of the Couinaud segments. Besides, perturbations are applied to the points in the coverage area of the vessel attention map, so that our full method performs arbitrary point sampling in the continuous space near the vessel, and is encouraged to implicitly learn the Couinaud boundary in countless points."
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,4,Conclusion,"We propose a multi-scale point-voxel fusion framework for accurate Couinaud segmentation that takes advantage of the topological relationship of coordinate points in 3D space, and leverages the semantic information of voxel grids. Besides, the point sampling strategy embedded with vascular prior increases the access of our method to important regions, and also improves the segmentation accuracy and robustness in uncertain boundaries. Experimental results demonstrate the effectiveness of our proposed method against other cutting-edge methods, showing its potential to be applied in the preoperative application of liver surgery."
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Fig. 1 .,
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Fig. 2 .,
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Fig. 3 .,
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Fig. 4 .,
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,,
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,,
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Table 1 .,
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 45.
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,1,Introduction,"Segmentation of the pulmonary vessels is the foundation for the clinical diagnosis of pulmonary vascular diseases such as pulmonary embolism (PE), pulmonary hypertension (PH) and lung cancer [9]. Accurate vascular quantitative analysis is crucial for physicians to study and apply in treatment planning, as well as making surgical plans. Although contrast-enhanced CT images have better contrast for pulmonary vessels compared to non-contrast CT images, the acquisition of contrast-enhanced CT images needs to inject a certain amount of contrast agent to the patients. Some patients have concerns about the possible risk of contrast media [2]. At the same time, non-contrast CT is the most widely used imaging modality for visualizing, diagnosing, and treating various lung diseases.In the literature, several conventional methods [5,16] have been proposed for the segmentation of pulmonary vessels in contrast-enhanced CT images. Most of these methods employed manual features to segment peripheral intrapulmonary vessels. In recent years, deep learning-based methods have emerged as promising approaches to solving challenging medical image analysis problems and have demonstrated exciting performance in segmenting various biological structures [10,11,15,17]. However, for vessel segmentation, the widely used models, such as U-Net and its variants, limit their segmentation accuracy on low-contrast small vessels due to the loss of detailed information caused by the multiple downsampling operations. Accordingly, Zhou et al. [17] proposed a nested structure UNet++ to redesign the skip connections for aggregating multi-scale features and improve the segmentation quality of varying-size objects. Also, some recent methods combine convolutional neural networks (CNNs) with transformer or non-local block to address this issue [3,6,13,18]. Wang et al. [13] replaced the original skip connections with transformer blocks to better merge the multi-scale contextual information. For this task, Cui et al. [1] also proposed an orthogonal fused U-Net++ for pulmonary peripheral vessel segmentation. However, all these methods ignored the significant variability in HU values of pulmonary vessels at different regions.To summarize, there exist several challenges for pulmonary vessel segmentation in non-contrast CT images: (1) The contrast between pulmonary vessels and background voxels is extremely low (Fig. 1(c)); (2) Pulmonary vessels have a complex structure and significant variability in vessel appearance, with different scales in different areas. The central extrapulmonary vessels near the heart have a large irregular ball-like shape, while the shape of the intrapulmonary vessels is delicate and tubular-like (Fig. 1(a) and (b)). Vessels become thinner as they get closer to the peripheral lung; (3) HU values of vessels in different regions vary significantly, ranging from -850 HU to 100 HU. Normally, central extrapulmonary vessels have higher HU values than peripheral intrapulmonary vessels. Thus, we set different ranges of HU values to better visualize the vessels in Fig. 1(d) and(e).To address the above challenges, we propose a H ierarchical E nhancement N etwork (HENet) for pulmonary vessel segmentation in non-contrast CT images by enhancing the representation of vessels at both image-and feature-level. For the input CT images, we propose an Auto Contrast Enhancement (ACE) module to automatically adjust the range of HU values in different areas of CT images. It mimics the radiologist in setting the window level (WL) and window width (WW) to better enhance vessels from surrounding voxels, as shown in Fig. 1(d) and (e). Also, we propose a Cross-Scale Non-local Block (CSNB) to replace the skip connections in vanilla U-Net [11] structure for the aggregation of multi-scale feature maps. It helps to form local-to-global information connections to enhance vessel information at the feature-level, and address the complex scale variations of pulmonary vessels."
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,2,Method,"The overview of the proposed method is illustrated in Fig. 2. Our proposed Hierarchical Enhancement Network (HENet) consists of two main modules: (1) Auto Contrast Enhancement (ACE) module, and (2) Cross-Scale Non-local Block (CSNB) as the skip connection bridge between encoders and decoders. In this section, we present the design of these proposed modules. First, the ACE module is developed to enhance the contrast of vessels in the original CT images for the following vessel segmentation network. After that, we introduce the CSNB module to make the network pay more attention to multi-scale vessel information in the latent feature space."
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,2.1,Auto Contrast Enhancement,"In non-contrast CT images, the contrast between pulmonary vessels and the surrounding voxels is pretty low. Also, the HU values of vessels in different regions vary significantly as ranging from -850 HU to 100 HU. Normally, radiologists have to manually set the suitable window level (WL) and window width (WW) for different regions in images to enhance vessels according to the HU value range of surrounding voxels, just as different settings to better visualize the extrapulmonary and intrapulmonary vessels (Fig. 1(d) and(e)). Instead of a fixed WL/WW as employed in existing methods, we address it by adding an ACE module to automatically enhance the contrast of vessels.The ACE module leverages convolution operations to generate dynamic WL and WW for the input CT images according to the HU values covered by the kernel. Here we set the kernel size as 15 × 15 × 15. First, we perform min-max normalization to linearly transform the HU values of the original image X to the range (-1, 1). Then, it passes through a convolution layer to be downsampled into half-size of the original shape, which is utilized to derive the following shift map and scale map. Here, the learned shift map and scale map act as the window level and window width settings of the ""width/level"" scaling in CT images. We let values in the shift map be the WL, so the tanh activation function is used to limit them within (-1, 1). The values in the scale map denote the half of WW, and we perform the sigmoid activation function to get the range (0, 1). It matches the requirement of the positive integer for WW. After that, the shift map and scale map will be upsampled by the nearest neighbor interpolation into the original size of the input X. This operation can generate identical shift and scale values with each 2 × 2 × 2 window, avoiding sharp contrast changes in the neighboring voxels. The upsampled shift map and scale map are denoted as M shif t and M scale , respectively, and then the contrast enhancement image X ACE can be generated through:It can be observed that the intensity values of input X are re-centered and re-scaled by M shif t and M scale (Fig. 3(c)). The clip operation (clip(•)) truncates the final output into the range [-1, 1], which sets the intensity value above 1 to 1, and below -1 to -1. In our experiments, we find that a large kernel size for learning of M shif t and M scale could deliver better performance, which can capture more information on HU values from the CT images."
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,2.2,Cross-Scale Non-local Block,"There are studies [14,18] showing that non-local operations could capture longrange dependency to improve network performance. To segment pulmonary vessels with significant variability in scale and shape, we design a Cross-Scale Nonlocal Block (CSNB) to fuse the local features extracted by CNN backbone from different scales, and to accentuate the cross-scale dependency to address the complex scale variations of pulmonary vessels.Inspired by [18], our CSNB incorporates 6 modified Asymmetric Non-local Blocks (ANBs), which integrate pyramid sampling modules into the non-local blocks to largely reduce the computation and memory consumption. As illustrated in Fig. 2, the CSNB works as the information bridge between encoders and decoders while also ensuring the feasibility of experiments involving large 3D data. Specifically, the I 1 ∼ I 4 are the inputs of CSNB, and O 1 ∼ O 4 are the outputs. Within the CSNB, there are three levels of modified ANBs, we denote them as ANB-H (ANB-Head) and ANB-P (ANB-Post). For the two ANBs in each level, the ANB-H has two input feature maps, and the lower-level feature maps (denoted as F l ) contain more fine-grained information than the higherlevel feature maps (denoted as F h ). We use F h to generate embedding Q, while embeddings K and V are derived from F l . By doing this, CSNB can enhance the dependencies of cross-scale features. The specific computation of ANB proceeds as follows: First, three 1×1×1 convolutions (denoted as Conv(•)) are applied to transform F h and F l into different embeddings Q, K, and V ; then, spatial pyramid pooling operations (denoted as P (•)) are implemented on K and V . The calculation can be expressed as:Next, these three embeddings are reshaped towhere N represents the total count of the spatial locations, i.e., N = D × H × W and S is equivalent to the concatenated output size after the spatial pyramid pooling, i.e., setting S = 648. The similarity matrix between Q and K p is obtained through matrix multiplication and normalized by softmax function to get a unified similarity matrix. The attention output is acquired by:where the output O ∈ R N × Ĉ . The final output of ANB is given as:where the final convolution is used as a weighting parameter to adjust the importance of this non-local operation and recover the channel dimension to C h . ANB-P has the same structure as ANB-H, but the inputs F h and F l here are the same, which is the output of ANB-H at the same level. The ANB-P is developed to further enhance the intra-scale connection of the fused features in different regions, which is equivalent to the self-attention mechanism. Note that, O 1 is directly skipped from I 1 . For the first level of CSNB, the input F l of ANB-H is the I 1 , while for the other levels, the input F l is the output of ANB-P of the above level. That is, each level of CSNB can fuse the feature maps from its corresponding level with the fused feature maps from all of the above lower levels. Thereby, the response of multi-scale vessels can be enhanced."
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,3,Experiments and Results,"Dataset and Evaluation Metrics. We use a total of 160 non-contrast CT images with the inplane size of 512 × 512, where the slice number varies from 217 to 622. The axial slices have the same spacing ranging from 0.58 to 0.86 mm, and the slice thickness varies from 0.7 to 1.0 mm. The annotations of pulmonary vessels are semi-automatically segmented in 3D by two radiologists using the 3D Slicer software. This study is approved by the ethical committee of West China Hospital of Sichuan University, China. These cases are randomly split into a training set (120 scans) and a testing set (40 scans). The quantitative results are reported by Dice Similarity Coefficient (Dice), mean Intersection over Union (mIoU), False Positive Rate (FPR), Average Surface Distance (ASD), and Hausdorff Distance (HD). For the significance test, we use the paired t-test. Implementation Details. Our experiments are implemented using Pytorch framework and trained using a single NVIDIA-A100 GPU. We pre-process the data by truncating the HU value to the range of [-900, 900] and then linearly scaling it to [-1, 1]. In the training stage, we randomly crop sub-volumes with the size of 192 × 192 × 64 near the lung field, and then the cropped sub-volumes are augmented by random horizontal and vertical flipping with a probability of 0.5. In the testing phase, we perform the sliding window average prediction with strides of (64, 64, 32) to cover the entire CT images. For a fair comparison, we use the same hyper-parameter settings and Dice similarity coefficient loss across all experiments. In particular, we use the same data augmentation, no post-processing scheme, Adam optimizer with an initial learning rate of 10 -4 , and train for 800 epochs with a batch size of 4. In our experiments, we use a two-step optimization strategy: 1) first, train the ACE module with the basic U-Net; 2) Integrate the trained ACE module and a new CSNB module into the U-Net, and fix the parameters of ACE module when training this network.Ablation Study. We conduct ablation studies to validate the efficacy of the proposed modules in our HENet by combining them with the baseline U-Net [11]. The quantitative results are summarized in Table 1. Compared to the baseline, both ACE module and CSNB lead to better performance. With the two components, our HENet has significant improvements over baseline on all the metrics. For regional measures Dice and mIoU, it improves by 3.02% and 2.32% respectively. For surface-aware measures ASD and HD, it improves by 35% and 53%, respectively. Results demonstrate effectiveness of the proposed ACE module and CSNB.To validate the efficacy of ACE module, we show the qualitative result in Fig. 3. As shown in Fig. 3(c), the ACE module effectively enhances the contrast of pulmonary vessels at the image-level. We also visualize the summation of feature maps from the final decoder in Fig. 3(d) and (e). As can be seen, the baseline U-Net can focus on local features for certain intrapulmonary vessels, but it fails to activate complete vascular regions of multiple scales. Comparison with State-of-the-Art Methods. Since we adopt U-Net as our baseline, we compare our method with several state-of-the-art encoder-decoder CNNs and the transformer-based method VT-UNet [8] within a considerable computational complexity. We also compare our method with state-of-the-art deep learning-based vessel segmentation methods, including clDice [12], CS 2 -Net [7], and OF-Net [1]. The quantitative and qualitative results are presented in Table 2 and Fig. 4, respectively. As shown in Table 2, our method outperforms the competing methods and achieves the best Dice, mIoU, ASD, and HD. CS 2 -Net performs best on FPR,  but our method has better Dice and mIoU than CS 2 -Net (increasing 0.56% and 0.43%, respectively), indicating the under-segmentation of CS 2 -Net and more vessels being correctly segmented by our method. In the first row of qualitative results (Fig. 4), the competing methods can produce satisfactory results for the overall structure but generate many false positives. Furthermore, due to low contrast between small intrapulmonary vessels and the surrounding voxels, results of competing methods exist many discontinuities (the second row), while our method obtains more connective segmentation for these small vessels. Also, for the segmentation of large extrapulmonary vessels (the last row), our method can produce more accurate results. Note that, although clDice can also yield connective results for small vessels, their FPR is 0.16% higher than ours. This implies that clDice tends to over-segment vessels, and it cannot obtain precise segmentation for the large extrapulmonary vessels. Results proved the superiority of our method."
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,4,Conclusion,"In this paper, we have proposed a hierarchical enhancement network to enhance the representation of vessels at both image-and feature-level for pulmonary vessel segmentation first time in non-contrast CT images. In the proposed HENet, an Auto Contrast Enhancement module is designed to enhance vessels in different regions of the input CT. And the Cross-Scale Non-local Block is further designed as the information bridge between encoders and decoders, to enhance the ability to capture and integrate vascular features of multiple scales. Experimental results show that our method outperforms the competing methods and demonstrates effectiveness of the proposed ACE module and CSNB. At the same time, it can be observed that the learning of M shif t and M scale is only supervised by the final segmentation loss. One of our future research directions is to develop explicit constraints for the ACE module to better re-center and re-scale the CT images."
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,,Fig. 1 .,
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,,Fig. 2 .,
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,,Fig. 3 .,
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,,Fig. 4 .,
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,,Table 1 .,
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,,Table 2 .,
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,,25 ± 0.13 Ours 85.90 ± 2.92 87.34,± 2.29 0.72 ± 0.41 9.43 ± 10.91 0.27 ± 0.14
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,,Acknowledgments,". This work was supported in part by National Key Research and Development Program of China (2021ZD0111100), National Natural Science Foundation of China (62131015), and Science and Technology Commission of Shanghai Municipality (STCSM) (21010502600)."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,1,Introduction,"Image segmentation is a fundamental task in medical image analysis. One of the key design choices in many segmentation pipelines that are based on neural networks lies in the selection of the loss function. In fact, the choice of loss function goes hand in hand with the metrics chosen to assess the quality of the predicted segmentation [46]. The intersection-over-union (IoU) and the Dice score are commonly used metrics because they reflect both size and localization agreement, and they are more in line with perceptual quality compared to, e.g., pixel-wise accuracy [9,27]. Consequently, directly optimizing the IoU or the Dice score using differentiable surrogates as (a part of) the loss function has become prevalent in semantic segmentation [2,9,20,24,47]. In medical imaging in particular, the Dice score and the soft Dice loss (SDL) [30,42] have become the standard practice, and some reasons behind its superior functioning have been uncovered and further optimizations have been explored [3,9,45].Another mechanism to further improve the predicted segmentation that has gained significant interest in recent years, is the use of soft labels during training. Soft labels can be the result of data augmentation techniques such as label smoothing (LS) [21,43] and are integral to regularization methods such as knowledge distillation (KD) [17,36]. Their role is to provide additional regularization so as to make the model less prone to overfitting [17,43] and to combat overconfidence [14], e.g., providing superior model calibration [31]. In medical imaging, soft labels emerge not only from LS or KD, but are also present inherently due to considerable intra-and inter-rater variability. For example, multiple annotators often disagree on organ and lesion boundaries, and one can average their annotations to obtain soft label maps [12,23,25,41].This work investigates how the medical imaging community can combine the use of SDL with soft labels to reach a state of synergy. While the original SDL surrogate was posed as a relaxed form of the Dice score, naively inputting soft labels to SDL is possible (e.g. in open-source segmentation libraries [6,19,20,51]), but it tends to push predictions towards 0-1 outputs rather than make them resemble the soft labels [3,32,47]. Consequently, the use of SDL when dealing with soft labels might not align with a user's expectations, with potential adverse effects on the Dice score, model calibration and volume estimation [3].Motivated by this observation, we first (in Sect. 2) propose two probabilistic extensions of SDL, namely, Dice semimetric losses (DMLs). These losses satisfy the conditions of a semimetric and are fully compatible with soft labels. In a standard setting with hard labels, DMLs are identical to SDL and can safely replace SDL in existing implementations. Secondly (in Sect. 3), we perform extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks to empirically confirm the potential synergy of DMLs with soft labels (e.g. averaging, LS, KD) over hard labels (e.g. majority voting, random selection)."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,2,Methods,"We adopt the notation from [47]. In particular, we denote the predicted segmentation as ẋ ∈ {1, ..., C} p and the ground-truth segmentation as ẏ ∈ {1, ..., C} p , where C is the number of classes and p the number of pixels. For a class c, we define the set of predictions as x c = { ẋ = c}, the set of ground-truth as y c = { ẏ = c}, the union as u c = x c ∪y c , the intersection as v c = x c ∩y c , the symmetric difference (i.e., the set of mispredictions) as x c i the cardinality of the relevant set. Moreover, when the context is clear, we will drop the superscript c."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,2.1,Existing Extensions,"If we want to optimize the Dice score, hence, minimize the Dice loss Δ Dice = 1 -Dice in a continuous setting, we need to extend Δ Dice with Δ Dice such that it can take any predicted segmentation x ∈ [0, 1] p as input. Hereinafter, when there is no ambiguity, we will use x and x interchangeably.The soft Dice loss (SDL) [42] extends Δ Dice by realizing that when x, y ∈ {0, 1} p , |v| = x, y , |x| = x 1 and |y| = y 1 . Therefore, SDL replaces the set notation with vector functions:The soft Jaccard loss (SJL) [33,37] can be defined in a similar way:A major limitation of loss functions based on L 1 relaxations, including SDL, SJL, the soft Tversky loss [39] and the focal Tversky loss [1], as well as those relying on the Lovasz extension, such as the Lovasz hinge loss [49], the Lovasz-Softmax loss [2] and the PixIoU loss [50], is that they cannot handle soft labels [47]. That is, when y is also in [0, 1] p . In particular, both SDL and SJL do not reach their minimum at x = y, but instead they drive x towards the vertices {0, 1} p [3,32,47]. Take for example y = 0.5; it is straightforward to verify that SDL achieves its minimum at x = 1, which is clearly erroneous.Loss functions that utilize L 2 relaxations [9,30] do not exhibit this problem [47], but they are less commonly employed in practice and are shown to be inferior to their L 1 counterparts [9,47]. To address this, Wang and Blaschko [47] proposed two variants of SJL termed as Jaccard Metric Losses (JMLs). These two variants, Δ JML,1 andJMLs are shown to be a metric on [0, 1] p , according to the definition below.Note that reflexivity and positivity jointly imply x = y ⇔ f (x, y) = 0, hence, a loss function that satisfies these conditions will be compatible with soft labels."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,2.2,Dice Semimetric Losses,"We focus here on the Dice loss. For the derivation of the Tversky loss and the focal Tversky loss, please refer to our full paper on arXiv. Since Dice"
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,,2-Δ IoU,". There exist several alternatives to define Δ IoU , but not all of them are feasible, e.g., SJL. Generally, it is easy to verify the following proposition:Proposition 1. Δ Dice satisfies reflexivity and positivity iff Δ IoU does.Among the definitions of Δ IoU , Wang and Blaschko [47] found only two candidates as defined in Eq. ( 3) satisfy reflexivity and positivity. Following Proposition 1, we transform these two IoU losses and define Dice semimetric losses (DMLs)Δ Dice that is defined over integers does not satisfy the triangle inequality [11], which is shown to be helpful in KD [47]. Nonetheless, we can consider a weaker form of the triangle inequality:(Functions that satisfy the relaxed triangle inequality for some fixed scalar ρ and conditions (i)-(iii) of a metric are called semimetrics. Δ Dice is a semimetric on {0, 1} p [11]. Δ DML,1 and Δ DML,2 , which extend Δ Dice to [0, 1] p , remain semimetrics in the continuous space:Theorem 1. Δ DML,1 and Δ DML,2 are semimetrics on [0, 1] p .The proof can be found in Appendix A. Moreover, DMLs have properties that are similar to JMLs and they are presented as follows:The proofs are similar to those given in [47]. Importantly, Theorem 2 indicates that we can safely substitute the existing implementation of SDL with DMLs and no change will be incurred, as they are identical when only hard labels are presented."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3,Experiments,"In this section, we provide empirical evidence of the benefits of using soft labels. In particular, using QUBIQ [29], which contains multi-rater information, we show that models trained with averaged annotation maps can significantly surpass those trained with majority votes and random selections. Leveraging LiTS [4] and KiTS [16], we illustrate the synergistic effects of integrating LS and KD with DMLs."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.1,Datasets,"QUBIQ is a recent challenge held at MICCAI 2020 and 2021, specifically designed to evaluate the inter-rater variability in medical imaging. Following [23,41], we use QUBIQ 2020, which contains 7 segmentation tasks in 4 different CT and MR datasets: Prostate (55 cases, 2 tasks, 6 raters), Brain Growth (39 cases, 1 task, 7 raters), Brain Tumor (32 cases, 3 tasks, 3 raters), and Kidney (24 cases, 1 task, 3 raters). For each dataset, we calculate the average Dice score between each rater and the majority votes in Table 1. In some datasets, such as Brain Tumor T2, the inter-rater disagreement can be quite substantial. In line with [23], we resize all images to 256 × 256. LiTS contains 201 high-quality CT scans of liver tumors. Out of these, 131 cases are designated for training and 70 for testing. As the ground-truth labels for the test set are not publicly accessible, we only use the training set. Following [36], all images are resized to 512×512 and the HU values of CT images are windowed to the range of [-60, 140]. KiTS includes 210 annotated CT scans of kidney tumors from different patients. In accordance with [36], all images are resized to 512 × 512 and the HU values of CT images are windowed to the range of [-200, 300]."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.2,Implementation Details,"We adopt a variety of backbones including ResNet50/18 [15], EfficientNetB0 [44] and MobileNetV2 [40]. All these models that have been pretrained on Ima-geNet [7] are provided by timm library [48]. We consider both UNet [38] and DeepLabV3+ [5] as the segmentation method.We train the models using SGD with an initial learning rate of 0.01, momentum of 0.9, and weight decay of 0.0005. The learning rate is decayed in a poly policy with an exponent of 0.9. The batch size is set to 8 and the number of epochs is 150 for QUBIQ, 60 for both LiTS and KiTS. We leverage a mixture of CE and DMLs weighted by 0.25 and 0.75, respectively. Unless otherwise specified, we use Δ DML,1 by default.In this work, we are mainly interested in how models can benefit from the use of soft labels. The superiority of SDL over CE has been well established in the medical imaging community [9,20], and our preliminary experiments also confirm this, as shown in Table 5 (Appendix C). Therefore, we do not include any further comparison with CE in this paper. "
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.3,Evaluation,"We report both the Dice score and the expected calibration error (ECE) [14]. For QUBIQ experiments, we additionally present the binarized Dice score (BDice), which is the official evaluation metrics used in the QUBIQ challenge. To compute BDice, both predictions and soft labels are thresholded at different probability levels (0.1, 0.2, ..., 0.8, 0.9). We then compute the Dice score at each level and average these scores with all thresholds.For all experiments, we conduct 5-fold cross validation, making sure that each case is presented in exactly one validation set, and report the mean values in the aggregated validation set. We perform statistical tests according to the procedure detailed in [9] and highlight results that are significantly superior (with a significance level of 0.05) in red."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.4,Results on QUBIQ,"In Table 2, we compare different training methods on QUBIQ using UNet-ResNet50. This comparison includes both hard labels, obtained through (i) majority votes [25] and (ii) random sampling each rater's annotation [22], as well as soft labels derived from (i) averaging across all annotations [12,25,41] and (ii) label smoothing [43].In the literature [12,25,41], annotations are usually averaged with uniform weights. We additionally consider weighting each rater's annotation by its Dice score with respect to the majority votes, so that a rater who deviates far from the majority votes receives a low weight. Note that for all methods, the Dice score and ECE are computed with respect to the majority votes, while BDice is calculated as illustrated in Sect. 3.3.Generally, models trained with soft labels exhibit improved accuracy and calibration. In particular, averaging annotations with uniform weights obtains the highest BDice, while a weighted average achieves the highest Dice score. It is worth noting that the weighted average significantly outperforms the majority votes in terms of the Dice score which is evaluated based on the majority votes themselves. We hypothesize that this is because soft labels contain extra interrater information, which can ease the network optimization at those ambiguous regions. Overall, we find the weighted average outperforms other methods, with the exception of Brain Tumor T2, where there is a high degree of disagreement among raters.We compare our method with state-of-the-art (SOTA) methods using UNet-ResNet50 in Table 3. In our method, we average annotations with uniform weights for Brain Tumor T2 and with each rater's Dice score for all other datasets. Our method, which simply averages annotations to produce soft labels obtains superior results compared to methods that adopt complex architectures or training techniques.  "
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.5,Results on LiTS and KiTS,"Wang and Blaschko [47] empirically found that a well-calibrated teacher can distill a more accurate student. Concurrently, Menon et al. [28] argued that the effectiveness of KD arises from the teacher providing an estimation of the Bayes class-probabilities p * (y|x) and this can lower the variance of the student's empirical loss. That is, the bias of the estimation is bounded above by the calibration error and this explains why the calibration of the teacher would be important for the student. Inspired by this, we apply a recent kernel density estimator (KDE) [35] that provides consistent estimation of E[y|f (x)]. We then adopt it as a post-hoc calibration method to replace the temperature scaling to calibrate the teacher in order to improve the performance of the student. For more details of KDE, please refer to our full paper on arXiv.In Table 4, we compare models trained with hard labels, LS [43] and KD [17] on LiTS and KiTS, respectively. For all KD experiments, we use UNet-ResNet50 as the teacher. Again, we obtain noticeable improvements in both the Dice score and ECE. It is worth noting that for UNet-ResNet18 and UNet-EfficientNetB0 on LiTS, the student's Dice score exceeds that of the teacher."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.6,Ablation Studies,"In Table 6 (Appendix C), we compare SDL with DMLs. For QUBIQ, we train UNet-ResNet50 with soft labels obtained from weighted average and report BDice. For LiTS and KiTS, we train UNet-ResNet18 with KD and present the Dice score. For a fair comparison, we disable KDE in all KD experiments.We find models trained with SDL can still benefit from soft labels to a certain extent because (i) models are trained with a mixture of CE and SDL, and CE is compatible with soft labels; (ii) although SDL pushes predictions towards vertices, it can still add some regularization effects in a binary segmentation setting. However, SDL is notably outperformed by DMLs. As for DMLs, we find Δ DML,1 is slightly superior to Δ DML,2 and recommend using Δ DML,1 in practice.In Table 7 (Appendix C), we ablate the contribution of each KD term on LiTS and KiTS with a UNet-ResNet18 student. In the table, CE and DML represent adding the CE and DML term between the teacher and the student, respectively. In Table 8 (Appendix C), we illustrate the effect of bandwidth that controls the smoothness of KDE. Results shown in the tables verify the effectiveness of the proposed loss and the KDE method."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,4,Future Works,"In this study, our focus is on extending the Dice loss within the realm of medical image segmentation. It may be intriguing to apply DMLs in the context of longtailed classification [26]. Additionally, while we employ DMLs in the label space, it holds potential for measuring the similarity of two feature vectors [18], for instance, as an alternative to cosine similarity."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,5,Conclusion,"In this work, we introduce the Dice semimetrics losses (DMLs), which are identical to the soft Dice loss (SDL) in a standard setting with hard labels, but are fully compatible with soft labels. Our extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks validate that incorporating soft labels leads to higher Dice score and lower calibration error, indicating that these losses can find wide application in diverse medical image segmentation problems. Hence, we suggest to replace the existing implementation of SDL with DMLs."
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,,,
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,,Table 1 .,
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,,Table 2 .,
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,,Table 3 .,
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,,Table 4 .,
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_46.
Fully Bayesian VIB-DeepSSM,2,Background,"We denote a set of paired training data as D = {X , Y}. X = {x n } N n=1 is a set of N unsegmented images, where x n ∈ R H×W ×D . Y = {y n } N n=1 is the set of PDMs comprised of M 3D correspondence points, where y n ∈ R 3M . VIB utilizes a stochastic latent encoding Z = {z n } N n=1 , where z n ∈ R L and L 3M . InBayesian modeling, model parameters Θ are obtained by maximizing the likelihood p(y|x, Θ). The predictive distribution is found by marginalizing over Θ, which requires solving for the posterior p(Θ|D). In most cases, p(Θ|D) is not analytically tractable; thus, an approximate posterior q(Θ) is found via variational inference (VI). Bayesian networks maximize the VI evidence lower bound (ELBO) by minimizing:where p(Θ) is the prior on network weights, and β is a weighting parameter. The deep Variational Information Bottleneck (VIB) [3] model learns to predict y from x using a low dimensional stochastic encoding z. The VIB architecture comprises of a stochastic encoder parameterized by φ, q(z|x, φ), and a decoder parameterized by θ, p(y|z, θ) (Fig. 1). VIB utilizes VI to derive a theoretical lower bound on the information bottleneck objective:The entropy of the p(y|z) distribution (computed via sampling) captures aleatoric uncertainty. The VIB objective has also been derived using an alternative motivation: Bayesian inference via optimizing a PAC style upper bound on the true negative log-likelihood risk [4]. Through this PAC-Bayes lens, it has been proven that VIB is half Bayesian, as the Bayesian strategy is applied to minimize an upper bound with respect to the conditional expectation of y, but Maximum Likelihood Estimation (MLE) is used to approximate the expectation over inputs. The VIB objective can be made a fully valid bound on the true risk by applying an additional PAC-Bound with respect to the parameters, resulting in a fully Bayesian VIB that captures epistemic uncertainty in addition to aleatoric."
Fully Bayesian VIB-DeepSSM,3,Methods,
Fully Bayesian VIB-DeepSSM,3.1,Bayesian Variational Information Bottleneck,"In fully-Bayesian VIB (BVIB), rather than fitting the model parameters Θ = {φ, θ} via MLE, we use VI to approximate the posterior p(Θ|D). There are now two intractable posteriors p(z|x, φ) and p(Θ|x, y). The first is approximated via q(z|x, φ) as in Eq. 2 and the second is approximated by q(Θ) as in Eq. 1.Minimizing these two KL divergences via a joint ELBO gives the objective (see Appendix A) for derivation details):where Θ ∼ q(Θ) and ẑ ∼ q(z|x, φ). This objective is equivalent to the BVIB objective acquired via applying a PAC-Bound with respect to the conditional expectation of targets and then another with respect to parameters [4]. This is expected, as it has been proven that the VI formulation using ELBO and the PAC-Bayes formulation with negative log-likelihood as the risk metric are algorithmically identical [23]. Additionally, this matches the objective derived for the Bayesian VAE when y = x [9]. Implementing BVIB requires defining a prior distribution for the latent representation p(z) and the network weights p(Θ). Following VIB, we define, p(z) = N (z|0, I). Different methods exist for defining p(Θ), and multiple approaches are explored in the following section."
Fully Bayesian VIB-DeepSSM,3.2,Proposed BVIB-DeepSSM Model Variants,"In adapting VIB-DeepSSM to be fully Bayesian, we propose utilizing two approaches that have demonstratively captured epistemic uncertainty without significantly increasing computational and memory costs: concrete dropout [13] and batch ensemble [26]. Additionally, we propose a novel integration for a more flexible, multimodal posterior approximation. Concrete Dropout (CD) utilizes Monte Carlo dropout sampling as a scalable solution for approximate VI [13]. Epistemic uncertainty is captured by the spread of predictions with sampled dropout masks in inference. CD automatically optimizes layer-wise dropout probabilities along with the network weights.Naive Ensemble (NE) models combine outputs from several networks for improved performance. Networks trained with different initialization converge to different local minima, resulting in test prediction disagreement [12]. The spread in predictions effectively captures epistemic uncertainty [20]. NE models are computationally expensive, as cost increases linearly with number of members.Batch Ensemble (BE) [26] compromises between a single network and NE, balancing the trade-off between accuracy and running time and memory. In BE, each weight matrix is defined to be the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. BE provides an ensemble from one network, where the only extra computation cost is the Hadamard product, and the only added memory overhead is sets of 1D vectors.Novel Integration of Dropout and Ensembling: Deep ensembles have historically been considered a non-Bayesian competitor for uncertainty estimation. However, recent work argues that ensembles approximate the predictive distribution more closely than canonical approximate inference procedures (i.e., VI) and are an effective mechanism for approximate Bayesian marginalization [28]. Combining traditional Bayesian methods with ensembling improves the fidelity of approximate inference via multimodal marginalization, resulting in a more robust, accurate model [27]. In concrete dropout, the approximate variational distribution is parameterized via a concrete distribution. While this parameterization enables efficient Bayesian inference, it greatly limits the expressivity of the approximate posterior. To help remedy this, we propose integrating concrete dropout and ensembling (BE-CD and NE-CD) to acquire a multimodal approximate posterior on weights for increased flexibility and expressiveness. While ensembling has previously been combined with MC dropout for regularization [26], this combination has not been proposed with the motivation of multimodal marginalization for improved uncertainty calibration. "
Fully Bayesian VIB-DeepSSM,3.3,BVIB-DeepSSM Implementation,"We compare the proposed BVIB approaches with the original VIB-DeepSSM formulation [2]. All models have the overall structure shown in Fig. 1, comprised of a 3D convolutional encoder(f e ) and fully connected decoder (f d ). CD models have concrete dropout following every layer, BE weights have four members (the maximum GPU memory would allow), and four models were used to create NE models for a fair comparison. Following [2], burn-in is used to convert the loss from deterministic (L2) to probabilistic (Eqs 10, 3, 13) [2]. This counteracts the typical reduction in accuracy that occurs when a negative log-likelihood based loss is used with a gradient-based optimizer [21]. An additional dropout burn-in phase is used for CD models to increase the speed of convergence [2]. All models were trained until the validation accuracy had not decreased in 50 epochs. A table of model hyperparameters and tested ranges is proved in Appendix C. The training was done on Tesla V100 GPU with Xavier initialization [15], Adam optimization [17]. The prediction uncertainty is a sum of the epistemic (variance resulting from marginalizing over Θ) and aleatoric (variance resulting from marginalizing over z) uncertainty (see Appendix B for calculation details)."
Fully Bayesian VIB-DeepSSM,4,Results,"We expect well-calibrated prediction uncertainty to correlate with the error, aleatoric uncertainty to correlate with the input image outlier degree (given that it is data-dependent), and epistemic uncertainty to correlate with the shape outlier degree (i.e., to detect out-of-distribution data). The outlier degree value for each mesh and image is quantified by running PCA (preserving 95% of variability) and then considering the Mahalanobis distance of the PCA scores to the mean (within-subspace distance) and the reconstruction error (off-subspace distance). The sum of these values provides a measure of similarity to the whole set in standard deviation units [19]. Experiments are designed to evaluate this expected correlation as well as accuracy, which is calculated as the root mean square error (RMSE) between the true and predicted points. Additionally, we quantify the surface-to-surface distance between a mesh reconstructed from the predicted PDM (predicted mesh) and the ground truth segmented mesh. The reported results are an average of four runs for each model, excluding the NE models, which ensemble the four runs."
Fully Bayesian VIB-DeepSSM,4.1,Supershapes Experiments,"Supershapes (SS) are synthetic 3D shapes parameterized by variables that determine the curvature and number of lobes [14]. We generated 1200 supershapes with lobes randomly selected between 3 and 7 and curvature parameters randomly sampled from a χ 2 distribution with 4 • of freedom. Corresponding 3D images were generated with foreground and background intensity values modeled as Gaussian distributions with different means and equal variance. Images were blurred with a Gaussian filter (size randomly selected between 1 and 8) to mimic diffuse shape boundaries. Figure 2A displays example shape meshes and images with corresponding outlier degrees, demonstrating the wide variation. We randomly split the mesh/image pairs to create a training set of size 1000, a validation set of size 100, and a testing set of size 100. ShapeWorks [8] was used to optimize PDMs of 128 points on the training set. Target PDMs were then optimized for validation and test sets, keeping the training PDMs fixed so that the test set statistics were not captured by the training PDMs.Figure 2B demonstrates that all BVIB models performed similarly or better than the baseline VIB in terms of RMSE and surface-to-surface distance, with the BE models performing best. Interestingly, the BE models were more accurate than the NE. This effect could result from the random sign initialization of BE fast weights, which increases members diversity. Adding CD hurt the accuracy slightly, likely because the learning task is made more difficult when layer-wise dropout probabilities are added as variational parameters. However, CD is the cheapest way to add epistemic uncertainty and improve prediction uncertainty calibration. Figure 2C demonstrates prediction uncertainty is well-calibrated for all models (with an error correlation greater than 0.7) and NE-CD-BVIB achieves the best correlation. The aleatoric and epistemic uncertainty correlation was similar across models, with the ensemble-based models performing best."
Fully Bayesian VIB-DeepSSM,4.2,Left Atrium Experiments,"The left atrium (LA) dataset comprises 1041 anonymized LGE MRIs from unique patients. The images were manually segmented at the University of Utah Division of Cardiovascular Medicine with spatial resolution 0.65 × 0.65 × 2.5 mm 3 , and the endocardium wall was used to cut off pulmonary veins.The images were cropped around the region of interest, then downsampled by a factor of 0.8 for memory purposes. This dataset contains significant shape variations, including overall size, LA appendage size, and pulmonary veins' number and length. The input images vary widely in intensity and quality, and LA boundaries are blurred and have low contrast with the surrounding structures. Shapes and image pairs with the largest outlier degrees were held out as outlier test sets, resulting in a shape outlier test set of 40 and image outlier test set of 78. We randomly split the remaining samples (90%, 10%, 10%) to get a training set of 739, a validation set of 92, and an inlier test set of 92. The target PDMs were optimized with ShapeWorks [8] to have 1024 particles.The accuracy and uncertainty calibration analysis in Figs. 3B and3C show similar results to the supershapes experiment. In both experiments, the proposed combination of dropout and ensembling provided the best-calibrated prediction uncertainty, highlighting the benefit of multimodal Bayesian marginalization. Additionally, the proposed combination gave more accurate predictions on the LA outlier test sets, suggesting improved robustness. BE-CD-BVIB provided the best prediction uncertainty for the LA and the second best (just behind NE-CD-BVIB) for the SS. BE-CD-BVIB is a favorable approach as it does not require training multiple models as NE does and requires relatively low memory addition to the base VIB model. Further qualitative LA results are provided in Appendix F in the form of heat maps of the error and uncertainty on test meshes. Here we can see how the uncertainty correlates locally with the error. As expected, both are highest in the LA appendage and pulmonary veins region, where LA's and the segmentation process vary the most. It is worth noting a standard normal prior was used for p(z) in all models. Defining a more flexible prior, or potentially learning the prior, could provide better results and will be considered in future work."
Fully Bayesian VIB-DeepSSM,5,Conclusion,"The traditional computational pipeline for generating Statistical Shape Models (SSM) is expensive and labor-intensive, which limits its widespread use in clinical research. Deep learning approaches have the potential to overcome these barriers by predicting SSM from unsegmented 3D images in seconds, but such a solution cannot be deployed in a clinical setting without calibrated estimates of epistemic and aleatoric uncertainty. The VIB-DeepSSM model provided a principled approach to quantify aleatoric uncertainty but lacked epistemic uncertainty. To address this limitation, we proposed a fully Bayesian VIB model that can predict anatomical SSM with both forms of uncertainty. We demonstrated the efficacy of two practical and scalable approaches, concrete dropout and batch ensemble, and compared them to the baseline VIB and naive ensembling. Finally, we proposed a novel combination of dropout and ensembling and showed that the proposed approach provides improved uncertainty calibration and model robustness on synthetic supershape and real left atrium datasets. While combining Bayesian methods with ensembling increases memory costs, it enables multimodal marginalization improving accuracy. These contributions are an important step towards replacing the traditional SSM pipeline with a deep network and increasing the feasibility of fast, accessible SSM in clinical research and practice."
Fully Bayesian VIB-DeepSSM,,Fig. 1 .,
Fully Bayesian VIB-DeepSSM,,Fig. 2 .,
Fully Bayesian VIB-DeepSSM,,Fig. 3 .,
Fully Bayesian VIB-DeepSSM,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_34.
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,1,Introduction,"Deep learning have become the de facto solution for medical image segmentation. Nevertheless, despite their ability to learn highly discriminative features, these models have shown to be poorly calibrated, often resulting in over-confident predictions, even when they are wrong [1]. When a model is miscalibrated, there is little correlation between the confidence of its predictions and how accurate such predictions actually are [2]. This results in a major problem, which can have catastrophic consequences in medical diagnosis systems where decisions may depend on predicted probabilities. As shown in [3], the uncertainty estimates inferred from segmentation models can provide insights into the confidence of any particular segmentation mask, and highlight areas of likely errors for the practitioner. In order to improve the accuracy and reliability of these models, it is crucial to develop both accurate and well-calibrated systems. Despite the growing popularity of calibration for image classification [1,4,5], the impact of miscalibrated networks on image segmentation, especially in the realm of biomedical images, has only recently begun to be explored [6]."
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Contribution.,"In this work, we propose a novel method based on entropy maximization to enhance the quality of pixel-level segmentation posteriors. Our hypothesis is that penalizing low entropy on the probability estimates for erroneous pixel predictions during training should help to avoid overconfident estimates in situations of high uncertainty. The underlying idea is that, if a pixel is difficult to classify, it is better assigning uniformly distributed (i.e. high entropy) probabilities to all classes, rather than being overconfident on the wrong class. To this end, we design two simple regularization terms which push the estimated posteriors for misclassified pixels towards a uniform distribution by penalizing low entropy predictions. We benchmark the proposed method in two challenging medical image segmentation tasks. Last, we further show that assessing segmentation models only from a discriminative perspective does not provide a complete overview of the model performance, and argue that including calibration metrics should be preferred. This will allow to not only evaluate the segmentation power of a given model, but also its reliability, of pivotal importance in healthcare."
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Related Work.,"Obtaining well-calibrated probability estimates of supervised machine learning approaches has attracted the attention of the research community even before the deep learning era, including approaches like histogram [7] or Bayesian binning [8]. Nevertheless, with the increase of popularity of deep neural networks, several works to directly address the calibration of these models have recently emerged. For instance, Bayesian neural networks learn a posterior distribution over parameters that quantifies parameter uncertainty -a type of epistemic uncertainty-, providing a natural approach to quantify model uncertainty. Among others, well-known Bayesian methods include variational inference [9], dropout-based variational inference [10] or stochastic expectation propagation [11]. A popular non-Bayesian method is ensemble learning, a simple strategy that improves both the robustness and calibration performance of predictive models [12][13][14][15]. However, even though this technique tends to improve the networks calibration, it does not directly promote uncertainty awareness. Furthermore, ensembling typically requires retraining several models from scratch, incurring into computationally expensive steps for large datasets and complex models. Guo et al. [1] empirically evaluated several post training ad-hoc calibration strategies, finding that a simple temperature scaling of logits yielded the best results. A drawback of this simple strategy, though, is that calibration performance largely degrades under data distribution shift [16].Another alternative is to address the calibration problem during training, for example by clamping over-confident predictions. In [17], authors proposed to regularize the neural network output by penalizing low entropy output distributions, which was achieved by integrating an entropy regularized term into the main learning objective. We want to emphasize that, even though the main motivation in [17] was to achieve better generalization by avoiding overfitting, recent observations [18] highlight that these techniques have a favorable effect on model calibration. In a similar line of work, [5] empirically justified the excellent performance of focal loss to learn well-calibrated models. More concretely, authors observed that focal loss [19] minimizes a Kullback-Leibler (KL) divergence between the predicted softmax distribution and the target distribution, while increasing the entropy of the predicted distribution.An in-depth analysis of the calibration quality obtained by training segmentation networks with the two most commonly used loss functions, Dice coefficient and cross entropy, was conducted in [6]. In line with [20,21], authors showed that loss functions directly impact calibration quality and segmentation performance, noting that models trained with soft Dice loss tend to be poorly calibrated and overconfident. Authors also highlight the need to explore new loss functions to improve both segmentation and calibration quality. Label smoothing (LS) has also been proposed to improve calibration in segmentation models. Islam et al. [22] propose a label smoothing strategy for image segmentation by designing a weight matrix with a Gaussian kernel which is applied across the one-hot encoded expert labels to obtain soft class probabilities. They stress that the resulting label probabilities for each class are similar to one-hot within homogeneous areas and thus preserve high confidence in non-ambiguous regions, whereas uncertainty is captured near object boundaries. Our proposed method achieves the same effect but generalized to different sources of uncertainty by selectively maximizing the entropy only for difficult to classify pixels."
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,2,Maximum Entropy on Erroneous Predictions,"Let us have a training dataset D = {(x, y) n } 1≤n≤|D| , where x n ∈ R Ωn denotes an input image and y n ∈ {0, 1} Ωn×K its corresponding pixel-wise one-hot label. Ω n denotes the spatial image domain and K the number of segmentation classes. We aim at training a model, parameterized by θ, which approximates the underlying conditional distribution p(y|x, θ), where θ is chosen to optimize a given loss function. The output of our model, at a given pixel i, is given as ŷi , whose associated class probability is p(y|x, θ). Thus, p(ŷ i,k = k|x i , θ) will indicate the probability that a given pixel (or voxel) i is assigned to the class k ∈ K. For simplicity, we will denote this probability as pi,k .Since confident predictions correspond to low entropy output distributions, a network is overconfident when it places all the predicted probability on a single class for each training example, which is often a symptom of overfitting [23]. Therefore, maximizing the entropy of the output probability distribution encourages high uncertainty (or low confidence) in the network predictions. In contrast to prior work [17], which penalizes low entropy in the entire output distributions, we propose to selectively penalize overconfidence exclusively for those pixels which are misclassified, i.e. the more challenging ones. To motivate our strategy, we plot the distribution of the magnitude of softmax probabilities in Fig. 1.b. It can be observed that for models trained with standard L dice loss [20], most of the predictions lie in the first or last bin of the histogram We hypothesize that encouraging the network to assign high entropy values solely to erroneous predictions (i.e. uniformly distributed probabilities) will help to penalize overconfidence in complex scenarios. To this end, for every training iteration we define the set of misclassified pixels as ŷw = {y i |ŷ i = y i }. We can then compute the entropy for this set as:where | • | is used to denote the set cardinality. As we aim at maximizing the entropy of the output probabilities ŷw (Eq. ( 1)), this equals to minimizing the negative entropy, i.e., min θ -H(ŷ w ). From now, we will use L H (ŷ w ) = H(ŷ w ) to refer to the additional loss term computing the entropy for the misclassified pixels following Eq. 1. Note that given a uniform distribution q, maximizing the entropy of y w boils down to minimizing the Kullback-Leibler (KL) divergence between y w and q. In what follows, we define another term based on this idea."
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Proxy for Entropy Maximization:,"In addition to explicitly maximizing the entropy of predictions (or to minimizing the negative entropy) as proposed in Eq. 1, we resort to an alternative regularizer, which is a variant of the KL divergence [24]. The idea is to encourage the output probabilities in y w (the misclassified pixels) to be close to the uniform distribution (i.e. all elements in the probability simplex vector q are equal to1 K ), resulting in max-uncertainty. This term is:with q being the uniform distribution and the symbol K = representing equality up to an additive or multiplicative constant associated with the number of classes. We refer the reader to the Appendix I in [24] for the Proof of this KL divergence variant, as well as its gradients. It is important to note that despite both terms, (1) and ( 2), push ŷw towards a uniform distribution, their gradient dynamics are different, and thus the effect on the weight updates differs. Here we perform an experimental analysis to assess which term leads to better performance. We will use L KL (ŷ w ) = D KL (q||ŷ w ) to refer to the additional loss based on Eq. 2.Global Learning Objective: Our final loss function takes the following form: L = L Seg (y, ŷ) -λL me (ŷ w ), where ŷ is the entire set of pixel predictions, L Seg the segmentation loss 1 , L me is one of the proposed maximum entropy regularization terms and λ balances the importance of each objective. Note that L me can take the form of the standard entropy definition, i.e. L me (ŷ w ) = L H (ŷ w ) (eq. ( 1)) or the proxy for entropy maximization using the KL divergence, i.e. L me (ŷ w ) = L KL (ŷ w ) (eq. ( 2)). While the first term will account for producing good quality segmentations the second term will penalize overconfident predictions only for challenging pixels, increasing the awareness of the model about the more uncertain image regions, maintaining high confidence in regions that are actually identified correctly.Baseline Models: We trained baseline networks using a simple loss composed of a single segmentation objective L Seg , without adding any regularization term. We used the two most popular segmentation losses: cross-entropy (L CE ) and the negative soft Dice coefficient (L dice ) as defined by [20]. Furthermore, we also compare our method to state-of-the-art calibration approaches. First, due to its similarity with our work, we include the confidence penalty loss proposed in [17], which discourages all the neural network predictions from being overconfident by penalizing low-entropy distributions. This is achieved by adding a low-entropy penalty term over all the pixels (in contrast with our method that only penalizes the misclassified pixels), which can be defined as:We train two baseline models using the aforementioned regularizer L H (ŷ), considering cross-entropy (L CE ) and Dice losses (L dice ). We also assess the performance of focal-loss [19], since recent findings [5] demonstrated the benefits of using this objective to train well-calibrated networks.Post-hoc Calibration Baselines. We also included two well known calibration methods typically employed for classification [1]: isotonic regression (IR) [25] and Platt scaling (PS) [26]. Differently from our methods which only use the training split, IR and PS are trained using validation data [1], keeping the original network parameters fixed. This is an advantage of our approaches since they do not require to keep a hold-out set for calibration. We apply IR and PS to the predictions of the vanilla baseline models trained with L dice and L CE models."
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,3,Experiments and Results,"Dataset and Network Details. We benchmark the proposed method in the context of Left Atrial (LA) cavity and White Matter Hyperintensities (WMH) segmentation in MR images. For LA, we used the Atrial Segmentation Challenge dataset [27], which provides 100 3D gadolinium-enhanced MR imaging scans (GE-MRIs) and LA segmentation masks for training and validation. These scans have an isotropic resolution of 0.625 × 0.625 × 0.625 mm 3 . We used the splits and pre-processed data from [28] (80 scans for training and 20 for evaluation -5% of training images were used for validation). The WMH dataset [29] consists of 60 MR images binary WMH masks. Each subject includes a co-registered 3D T1-weighted and a 2D multi-slice FLAIR of 1 × 1 × 3 mm. We split the dataset into independent training (42), validation (3) and test (15) sets. We benchmark our proposed method with two state-of-the-art DNN architectures (UNet [30] and ResUNet [31]) implemented using Tensorflow 2.32 (results for ResUNet are included in the Supp. Mat.). During training, for the WMH dataset we extract patches of size 64 × 64 × 64, and we train the networks until convergence by randomly sampling patches so that the central pixel corresponds to foreground label with 0.9 probability to account for label imbalance. For LA dataset all the scans were cropped to size 144 × 144 × 80 and centered at the heart region. We used Adam optimizer with a batch size of 64 for WMH and 2 for LA. The learning rate was set to 0.0001, and reduced by a factor of 0.85 every 10 epochs. Hyper-parameters were chosen using the validation split, and results reported on the hold-out test set."
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Training Details and Evaluation Metrics.,"As baselines, we used networks trained with L CE and L dice only. We also included the aforementioned posthoc calibration methods (namely IR and PS) as post-processing step for these vanilla models. We also implemented the confidence penalty-based method [17] previously discussed by adding the entropy penalizer L H (ŷ) and using the hyperparameter β = 0.2 suggested by the authors. We also include the focal-loss (L F L ) with γ = 2, following the authors' findings [19] and compare with the proposed regularizers which penalize low entropy in wrongly classified pixels: L H (ŷ w ) (Eq. 1) and L KL (ŷ w ) (Eq. 2). We performed grid search with different λ, and we found empirically that 0.3 works best for WMH models trained with L CE and 1.0 for L dice . For the LA dataset, we chose 0.1 for L CE and 0.5 for L dice . For each setting we trained 3 models and report the average results.To assess segmentation performance we resort to Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), whereas we use standard calibration metrics: Brier score [32], Stratified Brier score [33] (adapted to image segmentation following [15]) and Expected Calibration Error [8]. We also employ reliability diagrams, depicting the observed frequency as a function of the class probability. Note that in a perfectly calibrated model, the frequency on each bin matches the confidence, and hence all the bars lie on the diagonal."
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Results.,"Our main goal is to improve the estimated uncertainty of the predictions, while retaining the segmentation power of original losses. Thus, we first assess whether integrating our regularizers leads to a performance degradation. Table 1 reports the results across the different datasets with the UNet model (results for ResUNet are included in the Supp. Mat.). First, we can observe that adding the proposed regularizers does not result in a remarkable loss of segmentation performance. Indeed, in some cases, e.g., L dice + L KL ( Ŷw ) in WMH, the proposed model outperforms the baseline by more than 3% in terms of HD. Furthermore, this behaviour holds when the standard CE loss is used in conjunction with the proposed terms, suggesting that the overall segmentation performance is not negatively impacted by adding our regularizers into the main learning objective. Last, it is noteworthy to mention that even though L FL sometimes outperforms the baselines, it typically falls behind our two losses. In terms of qualitative performance, Fig. 1.a depicts exemplar cases of the improvement in probability maps obtained for each loss function in LA segmentation.Regarding calibration performance, recent empirical evidence [6] shows that, despite leading to strong predictive models, CE and specially Dice losses result in highly-confident predictions. The results obtained for calibration metrics (Brier and ECE in Table 1 and reliability plots in Fig. 2) are in line with these obser- vations. These results evidence that regardless of the dataset, networks trained with any of these losses as a single objective, lead to worse calibrated models compared to the proposed penalizers. Explicitly penalizing low-entropy predictions over all the pixels, as in [17], typically improves calibration. Nevertheless, despite the gains observed with [17], empirical results demonstrate that penalizing low-entropy values only over misclassified pixels brings the largest improvements, regardless of the main segmentation loss used. In particular, the proposed MEEP regularizers outperform the baselines in all the three calibration metrics and in both datasets, with improvements ranging from 1% to 13%, except for Brier+ in WMH. However, in this case, even though IR achieves a better Brier + , it results in worse ECE.When evaluating the proposed MEEP regularizers (L KL ( Ŷw ) and L H ( Ŷw )) combined with the segmentation losses based on DSC and CE, we observe that DSC with L KL ( Ŷw ) consistently achieves better performance in most of the cases. However, for CE, both regularizers alternate best results, which depend on the dataset used. We hypothesize that this might be due to the different gradient dynamics shown by the two regularizers3 . Regarding the focal loss, even though it improves model calibration when compared with the vanilla models, we observe that the proposed regularizers achieve better calibration metrics."
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,4,Conclusions,"In this paper, we presented a simple yet effective approach to improve the uncertainty estimates inferred from segmentation models when trained with popular segmentation losses. In contrast to prior literature, our regularizers penalize highconfident predictions only on misclassified pixels, increasing network uncertainty in complex scenarios. In addition to directly maximizing the entropy on the set of erroneous pixels, we present a proxy for this term, formulated with a KL regularizer modeling high uncertainty over those pixels. Comprehensive results on two popular datasets, losses and architectures demonstrate the potential of our approach. Nevertheless, we have also identified several limitations. For example, we have not assessed the effect of the proposed regularizers under severe domain shift (e.g. when testing on images of different organs). In this case it is not clear whether the model will output highly uncertain posteriors, or result again on overconfident but wrong predictions."
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Fig. 1 .,
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Fig. 2 .,
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Table 1 .,
Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 27.
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,1,Introduction,"Dynamic contrast-enhanced (DCE) cardiac MRI (CMRI) is an established medical imaging modality for detecting coronary artery disease and stress-induced myocardial blood flow abnormalities. Free-breathing CMRI protocols are preferred over breath-hold exam protocols due to the greater patient comfort and applicability to a wider range of patient cohorts who may not be able to perform consecutive breath-holds during the exam. Once the CMRI data is acquired, a key initial step for accurate analysis of the DCE scan is contouring or segmentation of the left ventricular myocardium. In settings where non-rigid motion correction (MoCo) fails or is unavailable, this process can be a time-consuming and labor-intensive task since a typical DCE scan includes over 300 time frames.Deep neural network (DNN) models have been proposed as a solution to this exhausting task [3,23,26,28]. However, to ensure trustworthy and reliable results in a clinical setting, it is necessary to identify potential failures of these models. Incorporating a quality control (QC) tool in the DCE image segmentation pipeline is one approach to address such concerns. Moreover, QC tools have the potential to enable a human-in-the-loop framework for DNN-based analysis [15], which is a topic of interest especially in medical imaging [2,18]. In a human-A.I collaboration framework, time/effort efficiency for the human expert should be a key concern. For free-breathing DCE-CMRI datasets, this time/effort involves QC of DNN-derived segmentations for each time frame. Recent work in the field of medical image analysis [5,9,14,20,24,25] and specifically in CMRI [7,16,17,22,27] incorporate QC and uncertainty assessment to assess/interpret DNN-derived segmentations. Still, a QC metric that can both temporally and spatially localize uncertain segmentation is lacking for dynamic CMRI.Our contributions in this work are two-fold: (i) we propose an innovative spatiotemporal dynamic quality control (dQC) tool for model-agnostic test-time assessment of DNN-derived segmentation of free-breathing DCE CMRI; (ii) we show the utility of the proposed dQC tool for improving the performance of DNN-based analysis of external CMRI datasets in a human-in-the-loop framework. Specifically, in a scenario where only 10% of the dataset can be referred to the human expert for correction, although random selection of cases does not improve the performance (p = n.s. for Dice), our dQC-guided selection yields a significant improvement (p < 0.001 for Dice). To the best of our knowledge, this work is the first to exploit the test-time agreement/disagreement between spatiotemporal patch-based segmentations to derive a dQC metric which, in turn, can be used for human-in-the-loop analysis of dynamic CMRI datasets."
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,2,Methods,
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,2.1,Training/testing Dynamic CMRI Datasets,"Our training/validation dataset (90%/10% split) consisted of DCE CMRI (stress first-pass perfusion) MoCo image-series from 120 subjects, which were acquired using 3T MRI scanners from two medical centers over 48-60 heartbeats in 3 short-axis myocardial slices [29]. The training set was extensively augmented by simulating breathing motion patterns and artifacts in the MoCo image-series, using random rotations (±50 • ), shear (±10 • ), translations (±2 pixels), scaling (range: [0.9, 1.1]), flat-field correction with 50% probability (σ ∈ [0, 5]), and gamma correction with 50% probability (γ ∈ [0.5, 1.5]). To assess the generalization of our approach, an external dataset of free-breathing DCE images from 20 subjects acquired at a third medical center was used. Local Institutional Review Board approval and written consent were obtained from all subjects."
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,2.2,Patch-Based Quality Control,"Patch-based approaches have been widely used in computer vision applications for image segmentation [1,4] as well as in the training of deep learning models [6,10,12,13,21]. In this work, we train a spatiotemporal (2D+time) DNN to segment the myocardium in DCE-CMRI datasets. Given that each pixel is present in multiple patches, we propose to further utilize this patch-based approach at test-time by analyzing the discordance of DNN inference (segmentation output) of each pixel across multiple overlapping patches to obtain a dynamic quality control map.Let Θ(w) be a patch extraction operator decomposing dynamic DCE-CMRI image I ∈ R M ×N ×T into spatiotemporal patches θ ∈ R K×K×T by using a sliding window with a stride w in each spatial direction. Also, let Γ m,n be the set of overlapping spatiotemporal patches that include the spatial location (m, n) in them. Also, p i m,n (t) ∈ R T denotes the segmentation DNN's output probability score for the i th patch at time t and location (m, n). The binary segmentation result S ∈ R M ×N ×T is derived from the mean of the probability scores from the patches that are in Γ m,n followed by a binarization operation. Specifically, for a given spatial coordinate (m, n) and time t, the segmentation solution is:The patch-combination operator, whereby probability scores from multiple overlapping patches are averaged, is denoted by Θ -1 (w).The dynamic quality control (dQC) map M ∈ R M ×N ×T is a space-time object and measures the discrepancy between different segmentation solutions obtained at space-time location (m, n, t) and is computed as:where std is the standard deviation operator. Note that to obtain S and M, the same patch combination operator Θ -1 (w) was used with w M < w S . Further, we define 3 quality-control metrics based on M that assess the segmentation quality at different spatial levels: pixel, frame, and slice (image series). First, Q pixel m,n (t) ∈ R is the value of M at space-time location (m, n, t) normalized by the segmentation area at time t:Next, Q frame (t) ∈ R T quantifies the per-frame segmentation uncertainty as perframe energy in M normalized by the corresponding per-frame segmentation area at time t:where • F is the Frobenius norm and M(t) ∈ R M ×N denotes frame t of the dQC map M. Lastly, Q slice assesses the overall segmentation quality of the acquired myocardial slice (image series) as the average of the per-frame metric along time:"
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,2.3,DQC-Guided Human-in-the-Loop Segmentation Correction,"As shown in Fig. 1, to demonstrate the utility of the proposed dQC metric, low confidence DNN segmentations in the test set, detected by the dQC metric Q frame , were referred to a human expert for refinement who was instructed to correct two types of error: (i) anatomical infeasibility in the segmentation (e.g., noncontiguity of myocardium); (ii) inclusion of the right-ventricle, left-ventricular blood pool, or regions outside of the heart in the segmented myocardium."
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,2.4,DNN Model Training,"We used a vanilla U-Net [19] as the DNN time frames stacked in channels, and optimized cross-entropy loss using Adam. We used He initializer [8], batch size of 128, and linear learning rate drop every two epochs, with an initial learning rate of 5 × 10  3 Results"
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,3.1,Baseline Model Performance,"The ""baseline model"" performance, i.e., the DNN output without the human-inthe-loop corrections, yielded an average spatiotemporal (2D+time) Dice score of 0.767 ± 0.042 for the test set, and 16.2% prevalence of non-contiguous segmentations, which is one of the criteria for failed segmentation (e.g., S(t 1 ) in Fig. 1) as described in Sect. 2.3. Inference times on a modern workstation for segmentation of one acquired slice in the test set and for generation of the dQC-map were 3 s and 3 min, respectively."
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,3.2,Human-in-the-Loop Segmentation Correction,"Two approaches were compared for human-in-the-loop framework: (i) referring the top 10% most uncertain time frames detected by our proposed dQC tool (Fig. 1), and (ii) randomly selecting 10% of the time frames and referring them for human correction. The initial prevalence of non-contiguous (failed) segmentations among the dQC-selected vs. randomly-selected time frames was 46.8% and 17.5%, respectively. The mean 2D Dice score for dQC-selected frames was 0.607 ± 0.217 and, after human expert corrections, it increased to 0.768 ± 0.147 (p < 0.001). On the other hand, the mean 2D Dice for randomly selected frames was initially 0.765 ± 0.173 and, after expert corrections, there was only a small increase to 0.781 ± 0.134 (p = n.s.). Overall, the human expert corrected 87.1% of the dQC-selected and 40.3% of the randomly-selected frames. Table 1 shows spatiotemporal (2D+time) cumulative results which contain all time frames including not selected frames for correction demonstrating that dQC-guided correction resulted in a notable reduction of failed segmentation prevalence from 16.2% to 11.3%, and in a significant improvement of the mean 2D+time Dice score. In contrast, the random selection of time frames for humanexpert correction yielded a nearly unchanged performance compared to baseline. To calculate the prevalence of failed segmentations with random frame selection, a total of 100 Monte Carlo runs were carried out."
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,3.3,Difficulty Grading of DCE-CMRI Time Frames vs. Q frame,"To assess the ability of the proposed dQC tool in identifying the most challenging time frames in a DCE-CMRI test dataset, a human expert reader assigned ""difficulty grades"" to each time frame in our test set. The criterion for difficulty was inspired by clinicians' experience in delineating endo-and epicardial contours. Specifically, we assigned the following two difficulty grades: (i) Grade 1: both the endo-and epicardial contours are difficult to delineate from the surrounding tissue; (ii) Grade 0: at most one of the endo-or epicardial contours are challenging to delineate.To better illustrate, a set of example time frames from the test set and the corresponding grades are shown in Fig. 2. The frequency of Grade 1 and Grade 0 time frames in the test set was 14.7% and 85.3%, respectively. Next, we compared the agreement of Q frame values with difficulty grades through a binary classifier whose input is dynamic Q frame values for each acquired slice. Note that each Q frame yields a distinct classifier due to variation in heart size (hence in dQC maps M) across the dataset. In other words, we obtained as many classifiers as the number of slices in the test set with a data-adaptive approach. The classifiers resulted in a mean area under the receiver-operating characteristics curve of 0.847 ± 0.109."
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,3.4,Representative Cases,"Figure 3 shows two example test cases with segmentation result, dQC maps, and Q frame . In (a), the highest Q frame was observed at t = 22, coinciding with the failed segmentation result indicated by the yellow arrow (also see the peak in the adjoining plot). In (b), the segmentation errors in the first 6 time frames (yellow arrows) are accurately reflected by the Q frame metric (see adjoining plot) after  which the dQC metric starts to drop. Around t = 15 it increases again, which corresponds to the segmentation errors starting at t = 16."
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,4,Discussion and Conclusion,"In this work, we proposed a dynamic quality control (dQC) method for DNNbased segmentation of dynamic (time resolved) contrast enhanced (DCE) cardiac MRI. Our dQC metric leverages patch-based analysis by analyzing the discrepancy in the DNN-derived segmentation of overlapping patches and enables automatic assessment of the segmentation quality for each DCE time frame.To validate the proposed dQC tool and demonstrate its effectiveness in temporal localization of uncertain image segmentations in DCE datasets, we considered a human-A.I. collaboration framework with a limited time/effort budget (10% of the total number of images), representing a practical clinical scenario for the eventual deployment of DNN-based methods in dynamic CMRI.Our results showed that, in this setting, the human expert correction of the dQC-detected uncertain segmentations results in a significant performance (Dice score) improvement. In contrast, a control experiment using the same number of randomly selected time frames for referral showed no significant increase in the Dice score, showing the ability of our proposed dQC tool in improving the efficiency of human-in-the-loop analysis of dynamic CMRI by localization of the time frames at which the segmentation has high uncertainty. In the same experiment, dQC-guided corrections resulted in a superior performance in terms of reducing failed segmentations, with a notably lower prevalence vs. random selection (11.3% vs. 14.4%). This reduced prevalence is potentially impactful since quantitative analysis of DCE-CMRI data is sensitive to failed segmentations.A limitation of our work is the subjective nature of the ""difficulty grade"" which was based on feedback from clinical experts. Since the data-analysis guidelines for DCE CMRI by the leading society [11] do not specify an objective grading system, we were limited in our approach to direct clinical input. Any such grading system may introduce some level of subjectivity."
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,,- 4 .,
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,,Fig. 1 .,
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,,Fig. 2 .,
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,,Fig. 3 .,
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,,Table 1 .,
Temporal Uncertainty Localization to Enable Human-in-the-Loop Analysis of Dynamic Contrast-Enhanced Cardiac MRI Datasets,,Image Segmentation I,
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,1,Introduction,"The medical imaging segmentation plays a crucial role in the computer-assisted diagnosis and monitoring of diseases. In recent years, deep neural networks have demonstrated remarkable results in automatic medical segmentation [3,10,22]. However, the process of collecting large-scale and sufficiently annotated medical datasets remains expensive and tedious, requiring domain knowledge and clinical experience. To mitigate the annotation cost, various techniques have been developed to train models using as few annotations as possible, including semi-supervised learning [1,18,20], and weakly supervised learning [5,6,19,26]. In this study, we focus on annotation-efficient learning and propose a universal framework for training segmentation models with scribble and point annotation.Reducing the number of annotations from dense annotations to scribbles or even points poses two challenges in segmentation: (1) how to obtain sufficient supervision to train a network and (2) how to generate high-quality pseudo labels. In the context of scribble-supervised segmentation, various segmentation methods have been explored, including machine learning or other algorithms [7,23,28], as well as deep learning networks [11,[13][14][15]26,32]. However, these methods only use scribble annotations, excluding point annotations, and their performance remains inferior to training with dense annotations, limiting their practical use in clinical settings. Pseudo labeling [12] is widely used to generate supervision signals for unlabeled images/pixels from imperfect annotations [4,30]. Recently, some works [17,30,31] have demonstrated that semi-supervised learning can benefit from high-quality pseudo labels. In this study, we propose generating pseudo labels by randomly mixing prediction and texture-oriented pseudo label, which can address the inherent weakness of the previous methods.This study aims to address the challenges of obtaining sufficient supervisions and generating high-quality pseudo labels. Previous works [33,34] have demonstrated repetitive patterns in texture and feature spaces under single-task learning in both natural and medical images. This observation raises the question of whether similar feature patterns exist in multi-task branches. To investigate this question, we conducted experiments and validated our findings in Fig. 1. We first trained a network with segmentation and reconstruction branches and computed the feature distance distributions between one point and the rest of the regions for each class on both the segmentation and reconstruction feature maps. The features are extracted from the last conv layers in their branches.Figure 1 (c) shows there are similar patterns between reconstruction and segmentation feature distance maps for each class at the global level. Black color indicates smaller distances. The feature distances in segmentation maps appear to be cleaner than those in the recon maps. It suggests that segmentation features possess task-specific information, while recon features can be seen as a broader set with segmentation information.Taking inspiration from the similar feature patterns observed in segmentation and reconstruction features, we propose a novel framework that utilizes a memory bank to generate pseudo labels. Our framework consists of an encoder that extracts visual features, as well as two decoders: one for segmenting target objects using scribble or point annotations, and another for reconstructing the input image. To address the challenge of seeking sufficient supervision, we employ the reconstruction branch as an auxiliary task to provide additional supervision and enable the network to learn visual representations. To tackle the challenge of generating high-quality pseudo labels, we use a VQ memory bank to store texture-oriented and global features, which we use to generate the pseudo labels. We then combine information from the global dataset and local image to generate improved, confident pseudo labels.The contributions of this work can be summarized as follows. Firstly, a universal framework for annotation-efficient medical segmentation is proposed, which is capable of handling both scribble-supervised and point-supervised segmentation. Secondly, an auxiliary reconstruction branch is employed to provide more supervision and backwards sufficient gradients to learn visual representations. Thirdly, a novel pseudo label generation method from memory bank is proposed, which utilizes the VQ memory bank to store texture-oriented and global features to generate high-quality pseudo labels. To boost the model training, we generate high-quality pseudo labels by mixing the segmentation prediction and pseudo labels from the VQ bank. Finally, experimental results on public MRI segmentation datasets demonstrate the effectiveness of the proposed method. Specifically, our method outperforms existing scribble-supervised segmentation approaches on the ACDC dataset and also achieves better performance than several semi-supervised methods."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,2,Method,"In this study, we focus on the problem of annotation-efficient medical image segmentation and propose a universal and adaptable framework for both scribblesupervised and point-supervised learning, as illustrated in Fig. 2. These annotations involve only a subset of pixels in the image and present two challenges: seeking sufficient supervisions to train the network and generating high-quality pseudo labels. To overcome these challenges, we draw inspiration from the recent success of self-supervised learning and propose a framework that includes a reconstruction branch as an auxiliary task and a novel pseudo label generation method using VQ bank memory. "
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Overview:,"The proposed framework for annotation-efficient medical image segmentation is illustrated in Fig. 2, which consists of a segmentation task and an auxiliary reconstruction task. Firstly, visual features are extracted using one encoder f , and then fed into one decoder g seg to learn from scribble or point annotations to segment target objects, as well as one decoder g recon to reconstruct the input image. The memory bank in the reconstruction branch is utilized to generate the pseudo labels, which are then used to assist in the training of the segmentation branch. The entire network is trained in an end-to-end manner.Feature Extraction: In this work, we employ a U-Net [22] as the encoder to extract features from the input image x. The size of the input patch is H × W , and the resulting feature map F has the same size as the input patch. It is worth noting that the U-Net backbone used in our work can be replaced with other state-of-the-art structures. Our focus is on designing a universal framework for annotation-efficient medical segmentation, rather than on optimizing the network architecture for a specific task."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Segmentation Branch:,"The segmentation branch g seg takes the feature map F as input and produces the final segmentation masks based on the available scribble or point annotations. Following recent works such as [13] [24] and [19], we utilize the partial cross-entropy loss to train the decoder L pCE (y, s) = c i∈ωs log y c i , where s denotes the annotation set with reduced annotation efficiency, and y c i is the predicted probability of pixel i belonging to class c. The set of labeled pixels in s is denoted by ω s . To note that the number of pixels s in point annotations is much less than that in scribble annotations, with around s < 10 for each class. Reconstruction Branch: To address the first challenge of seeking sufficient supervisions to train a network with reduced annotations, we propose an auxiliary reconstruction branch. This branch is designed to add more supervision and provide sufficient gradients for learning visual representations. The reconstruction branch has the same decoder structure as the segmentation branch, except for the final prediction layer. We employ the mean squared error loss for the reconstruction task, given by L recon = |x -y recon | 2 F , where x is the input image and y recon is the predicted image.VQ Memory Bank: Motivated by the similar feature patterns observed in medical images, we utilize the Vector Quantization (VQ) memory bank to store texture-oriented and global features, which are then employed for pseudo label generation. The pseudo label generation process involves three stages, as illustrated in Fig. 3."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Memory Bank Definition.,"In accordance with the VQVAE framework [27] [34], we use a memory bank E to encode and store the visual features on reconstruction branch of the entire dataset. The memory bank E is defined as a dictionary of latent vectors E := e 1 , e 2 , ..., e n , where e i ∈ R 1×64 represents the stored feature in the dictionary and n = 512 is the total size of the memory.Memory Update Stage. The feature map F recon is obtained from the last layer in the reconstruction branch and is utilized to update the VQ memory bank and retrieve an augmented feature Frecon . For each spatial location f j ∈ R 1×64 in F recon ∈ R 64×256×256 , we use L2 is used to compute the distance between f j and e k and find the nearest feature e i ∈ R 1×64 in the VQ memory bank, as follows: fj = e i , i = arg min k |f j -e k | 2 2 . Following [27], we use the VQ loss to update the memory bank and encoder,, where sg denotes the stop-gradient operator."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Pseudo Label Table Update,"Stage. The second stage mainly updates a pseudo label table, using the labelled regions on the reconstruction features and assigning pseudo labels on memory vectors. In particular, it uses the labelled pixels and their corresponding reconstruction features and finds the nearest vectors in the memory bank. As shown in Fig. 3, for the features extracted from the first class regions F 1 , its nearest memory vector is e 2 . Then, we need to assign probability [1, 0, 0] on e 2 and [0.7, 0.1, 0.2] is stored probabilities after doing Exponentially Moving Average (EMA) and delay is 0.9 in our implementation. We do the same thing for the rest labelled pixels. The pseudo label table is updated on each iteration and records the average values for each vectors.Pseudo Label Generation Stage. The third stage utilizes the pseudo label table to generate the pseudo labels. It takes the feature map F recon as inputs, then finds their nearest memory vectors, and retrieve the pseudo label according to the vector indices. The generated pseudo label is generated by the repetitive texture patterns on the reconstruction branch, which would include the segmenation information as well as other things."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Pseudo Label Generation:,"The generation of pseudo labels from the reconstruction branch is based on a texture-oriented and global view, as the memory bank stores the features extracted from the entire dataset. However, relying solely on it may not be sufficient, and it is necessary to incorporate more segmentation-specific information from the segmentation branch. Therefore, we leverage both approaches to enhance the model training.To incorporate both the segmentation-specific information and the textureoriented and global information, we dynamically mix the predictions y 1 from the segmentation branch and the pseudo labels y 2 from the VQ memory bank to generate the final pseudo labels y * [36]  [19]. Specifically, we use the following equation:, where α is uniformly sampled from [0, 1]. The argmax function is used to generate hard pseudo labels. We then use the generated y * to supervise y 1 and assist in the network training. The pseudo label loss is defined as L pl (P L, y 1 ) = 0.5 × L dice (y * , y 1 ), where L dice is the dice loss, which can be substituted with other segmentation loss functions such as cross-entropy loss.Loss Function: Finally, our loss function is calculated aswhere λ V Q is hyper weights with λ V Q = 0.1."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,3,Experiment,
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,3.1,Experimental Setting,"We use the PyTorch [21] platform to implement our model with the following parameter settings: mini-batch size (32), learning rate (3.0e-2), and the number of iterations (60000). We employ the default initialization of PyTorch (1.8.0) to initialize the model. We evaluated our proposed universal framework on scribble and point annotations using the ACDC dataset [2]. The dataset comprises 200 short-axis cine-MRI scans collected from 100 patients, with each patient having two annotated end-diastolic (ED) and end-systolic (ES) phases scans. Each scan has three structures with dense annotation, namely, the right ventricle (RV), myocardium (Myo), and left ventricle (LV). Following previous studies [1,19,26] and consistent with the dataset's convention, we performed 2D slice segmentation instead of 3D volume segmentation. Scribble annotations are simulated by ITK-SNAP. To simulate point annotations, we randomly generated five points for each class. During testing, we predicted the segmentation slice by slice and combined them to form a 3D volume."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,3.2,Performance Comparisons,"We conducted an evaluation of our model on the ACDC dataset, utilizing the 3D Dice Coefficient (DSC) and the 95% Hausdorff Distance (95) as the metrics. In this study, we compare our proposed model with various state-ofthe-art methods and designed baselines. These include: (1) scribble-supervised segmentation methods such as pCE only [15] (lower bound), the model using pseudo labels generated by Random Walker (RW) [7], Uncertainty-aware Selfensembling and Transformation-consistent Model (USTM) [16], Scribble2Label (S2L) [13], Mumford-shah Loss (MLoss) [11], Entropy Minimization (EM) [8] and Regularized Loss (RLoss) [24]; (2) widely-used semi-supervised segmentation methods, including Deep Adversarial Network (DAN) [37], Adversarial Entropy Minimization (AdvEnt) [29], Mean Teacher (MT) [25], and Uncertainty Aware Mean Teacher (UAMT) [35]. Additionally, we also conduct partially supervised (PS) learning, where only 10% labeled data is used to train the networks.  Table 1 presents the performance comparisons of our proposed method with state-of-the-art methods on the ACDC dataset. Our proposed method employing scribble annotations achieves superior performance over existing semi-supervised and weakly-supervised methods. Furthermore, our proposed method utilizing a few point annotations demonstrates comparable performance with weaklysupervised methods and outperforms semi-supervised segmentation methods by a significant margin. Despite achieving slightly lower performance than fully supervised methods, our proposed method requires much lower annotation costs. We present the qualitative comparison results in Fig. 4. The visual analysis of the results indicates that our proposed methods using scribble even point annotation perform well in terms of visual similarity with the ground truth. These results demonstrate the effectiveness of using scribble or point annotations as a potential way to reduce the annotation cost."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,3.3,Ablation Study,We investigate the effect of our proposed method on the ACDC datasets.
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Effect of the Auxiliary Task:,"We designed a baseline by removing the reconstruction branch and VQ memory. The results in Table 2 show a significant performance gap, indicating the importance of the reconstruction branch in stabilizing the training process. We also use local pixel-wise contrastive learning [9] to replace reconstruction and keep the rest same. Results are 0.85 for point."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Effect of Pseudo Labels:,"We also designed a baseline using only the predictions as pseudo labels. In Table 2, the performance drop highlights the effectiveness of the texture-orient and global information in the VQ memory bank. The size and dimension of embedding of VQ bank are 512 and 64 the default setting in VQVAE. Model results (sizes of bank 64, 256, 512) are 0.865, 0.866, 0.866. We find 20-24 vectors are commonly used as clusters of 95% features and can set 64 as bank size to save memory."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Effect of Different Levels of Aannotations:,"We also evaluated the impact of using different levels of annotations in point-supervised learning, ranging from more annotations, e.g. 10 points, to fewer annotations, e.g. 2 points. The results in Table 2 indicate that clicking points is a promising data annotation approach to reduce annotation costs. Overall, our findings suggest that the proposed universal framework could effectively leverage different types of annotations and provide high-quality segmentation results with less annotation costs."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,4,Conclusion,"In this study, we introduce a universal framework for annotation-efficient medical segmentation. Our framework leverages an auxiliary reconstruction branch to provide additional supervision to learn visual representations and a novel pseudo label generation method from memory bank, which utilizes the VQ memory bank to store global features to generate high-quality pseudo labels. We evaluate the proposed method on a publicly available MRI segmentation dataset, and the experimental results demonstrate its effectiveness."
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Fig. 1 .,
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Fig. 2 .,
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Fig. 3 .,
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Fig. 4 .,
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Table 1 .,
Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory,,Table 2 .,
