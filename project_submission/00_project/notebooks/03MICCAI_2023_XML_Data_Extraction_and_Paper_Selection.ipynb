{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objective of the Notebook:**\n",
    "***\n",
    "This notebook preprocesses and analyzes XML documents from the MICCAI 2023 conference with the aim to:\n",
    "- **Extract Structured Information:** Transform raw XML data into a clean, organized format for detailed analysis.\n",
    "- **Identify Key Content:** Extract headers, titles, and related text from XML documents.\n",
    "- **Focus on Specific Research:** Identify and extract papers related to cancer topics.\n",
    "- **Data Aggregation:** Aggregate data into structured dataframes to facilitate further analysis and insights extraction.\n",
    "\n",
    "#### To succesfully run the notebook:\n",
    "1. Run \"Option 2\"\n",
    "2. Option 1 requires manually adjusting throughout the way. These adjustments are described and solved.\n",
    "\n",
    "#### Input Data Expected:\n",
    "- **XML Documents:** Properly formatted XML documents from MICCAI 2023 stored in a predefined directory on the user's system, accessible to the script.\n",
    "\n",
    "#### Output Data/Files Generated:\n",
    "- [Access all outputs](00MICCAI_total_outputs/03MICCAI_all_outputs)\n",
    "- **Structured Data CSV:** `03MICCAI_notebook_df_paper_extractions_all_cleaned_.csv` containing parsed and cleaned information.\n",
    "- **Cancer-Related CSV:** `03MICCAI_notebook_df_paper_extractions_cancer.csv` aggregating information from cancer-related papers.\n",
    "- **Refined Categorization CSVs:** Files like `03MICCAI_notebook_df_paper_extractions_patients_and_cancer.csv` based on refined categorizations, such as mentions of patients in cancer-related papers.\n",
    "\n",
    "#### Assumptions or Important Notes:\n",
    "- **GROBID Client Requirement:** A running instance of the GROBID client on the local machine is crucial for processing XML documents.\n",
    "- **Document Structure:** Assumes XML documents adhere to a consistent structure suitable for automated parsing.\n",
    "- **File Renaming:** Includes functionality to rename files based on titles extracted from XML, indicating each document can be uniquely identified by its title.\n",
    "- **Expected Headers and Formats:** Specific headers and textual formats are anticipated within the XML files for accurate parsing.\n",
    "- **Error Handling:** Minimal error handling suggests XML files should not contain corrupt data or unexpected formatting errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input and Output Data**\n",
    "\n",
    "| Type         | Description | File/Folder Name                                          |\n",
    "|--------------|-------------|-----------------------------------------------------------|\n",
    "| **Input**    | PDF Documents from MICCAI 2023 stored in a predefined directory on the user's system. | Predefined directory containing PDF files. |\n",
    "| **Input**    | XML Documents from MICCAI 2023 stored in a predefined directory on the user's system. | Predefined directory containing XML files. |\n",
    "| **Output**   | Structured Dataframe containing parsed and cleaned information. | `03MICCAI_notebook_df_paper_extractions_all_cleaned_.csv` |\n",
    "| **Output**   | CSV aggregating information from cancer-related papers. | `03MICCAI_notebook_df_paper_extractions_cancer.csv` |\n",
    "| **Output**   | CSV featuring refined categorizations such as patient mentions in cancer-related papers. | `03MICCAI_notebook_df_paper_extractions_patients_and_cancer.csv` |\n",
    "| **Output**   | CSV of 100 randomly selected papers for annotation and further analysis. | `03MICCAI_notebook_100_randomly_selected_papers.csv` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Automated Extraction and Analysis of MICCAI 2023 XML Documents**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries and installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lxml\n",
    "\n",
    "# Run GROBID in terminal before running the notebook\n",
    "# Installation and running commands\n",
    "# wget https://github.com/kermitt2/grobid/archive/0.8.0.zip\n",
    "# unzip 0.8.0.zip\n",
    "# cd grobid-0.8.0\n",
    "# ./gradlew run\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xml.etree import ElementTree as et\n",
    "from lxml import etree \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, width=100):\n",
    "    \"\"\"\n",
    "    A simple function to wrap text at a given width.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return text  # Handle NaN values\n",
    "    \n",
    "    wrapped_lines = []\n",
    "    for paragraph in text.split('\\n'):  # Splitting by existing newlines to preserve paragraph breaks\n",
    "        line = ''\n",
    "        for word in paragraph.split():\n",
    "            if len(line) + len(word) + 1 > width:\n",
    "                wrapped_lines.append(line)\n",
    "                line = word\n",
    "            else:\n",
    "                line += (' ' + word if line else word)\n",
    "        wrapped_lines.append(line)\n",
    "    return '\\n'.join(wrapped_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "client = GrobidClient(grobid_server='http://localhost:8070')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section convert PDF files into XML. Furthermore, I manually had to rename papers in XML files, and adjust data frames where extractions were not executed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where the PDF articles are stored (in this case, the MICCAI articles)\n",
    "process_file = '../miccai_articles' \n",
    "\n",
    "# Process the full text of the PDF articles using GROBID\n",
    "client.process('processFulltextDocument', process_file, output=\"./03MICCAI_notebook_GROBID_processed_volumes\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming XML files by folder path\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error in Renaming two GROBID XML files: Manually corrections on two XML files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "File #1.\n",
    "FileNotFoundError: [Errno 2] No such file or directory: '03MICCAI_notebook_GROBID_processed_volumes/vol7/paper_44.grobid.tei.xml' \n",
    "-> '03MICCAI_notebook_GROBID_processed_volumes/vol7/Full_Image-Index_Remainder_Based_Single_Low-Dose_DR/CT_Self-supervised_Denoising.xml'\n",
    "\n",
    "Solution: Removing \"/CT_Self-supervised_Denoising\" from the XML file in paper_44 in vol7 and save the updated version\n",
    "Re-run the code block again\n",
    "\n",
    "File #2\n",
    "FileNotFoundError: [Errno 2] No such file or directory: '03MICCAI_notebook_GROBID_processed_volumes/vol9/paper_49.grobid.tei.xml' \n",
    "-> '03MICCAI_notebook_GROBID_processed_volumes/vol9/A_Patient-Specific_Self-supervised_Model_for_Automatic_X-Ray/CT_Registration.xml'\n",
    "\n",
    "Solution: Removing \"/CT_Registration\" from the XML file in paper_49 in vol9 and save the updated version\n",
    "Re-run the code block again\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas._libs import missing\n",
    "\n",
    "'''\n",
    "The following code block is used to extract the title of the papers from the XML files,\n",
    "and rename the XML files based on the title of the papers.\n",
    "'''\n",
    "\n",
    "def find_title(element):\n",
    "    \"\"\"Recursively search for the title element in the XML structure.\"\"\"\n",
    "    if 'title' in element.tag.lower() and element.text:\n",
    "        return element.text.strip()\n",
    "    for child in element:\n",
    "        title = find_title(child)\n",
    "        if title:\n",
    "            return title\n",
    "    return None\n",
    "\n",
    "def rename_xml_files_in_folder(folder_path):\n",
    "    \"\"\"Rename XML files based on their title tags.\"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith('.xml'):  # Skip non-XML files\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            tree = et.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            paper_title = find_title(root)\n",
    "            if paper_title:\n",
    "                new_filename = paper_title.replace(\" \", \"_\") + '.xml'\n",
    "                new_file_path = os.path.join(folder_path, new_filename)\n",
    "                os.rename(file_path, new_file_path)\n",
    "                print(f\"Renamed '{filename}' to '{new_filename}'\")\n",
    "            else:\n",
    "                print(f\"Title not found in '{filename}'. Skipping.\")\n",
    "        except et.ParseError as e:\n",
    "            print(f\"Error parsing '{filename}': {e}\")\n",
    "\n",
    "# Path to the processed volumes\n",
    "path = '03MICCAI_notebook_GROBID_processed_volumes'\n",
    "\n",
    "# Rename the XML files in the sub folders to the actual title of the article\n",
    "for volume_number in range(1, 11):\n",
    "    # Construct the path to the volume folder\n",
    "    folder_path = f'{path}/vol{volume_number}'\n",
    "    print(f\"Processing '{folder_path}'...\")\n",
    "\n",
    "    # Rename the XML files in the folder\n",
    "    rename_xml_files_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The missing XML files in the processed volumes are:\n",
    "- vol6: paper_15.grobid.tei.xml\n",
    "- vol7: paper_13.grobid.tei.xml\n",
    "\n",
    "They were localised by re-running the code block 'process_file = '../vol6' and 'process_file = '../vol7' \n",
    "in the client.process() function and stores into two separate folders named './06' and './07'. \n",
    "From here, the missing XML files were identified and the corresponding XML files were renamed to the correct title\n",
    "and moved to the correct folder in the processed volumes folder (03MICCAI_notebook_GROBID_processed_volumes).\n",
    "'''\n",
    "# For finding missing XML files in the processed volumes:\n",
    "# vol6: paper_15.grobid.tei.xml\n",
    "# process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/project_submission/00_project_notebook/miccai_papers/vol6' \n",
    "# client.process('processFulltextDocument', process_file, output=\"./06\", force=True)\n",
    "\n",
    "# vol7: paper_13.grobid.tei.xml\n",
    "# process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/project_submission/00_project_notebook/miccai_papers/vol7' \n",
    "# client.process('processFulltextDocument', process_file, output=\"./07\", force=True)\n",
    "\n",
    "\n",
    "# Missing XML files localised from the client.process() function\n",
    "missing_files = {'06/paper_15.grobid.tei.xml', '07/paper_13.grobid.tei.xml'}\n",
    "\n",
    "# Rename the missing files XML files in the sub folders to the actual title of the article\n",
    "for file_path in missing_files:\n",
    "    tree = et.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    paper_title = find_title(root)\n",
    "    if paper_title:\n",
    "        new_filename = paper_title.replace(\" \", \"_\") + '.xml'\n",
    "        new_file_path = os.path.join(os.path.dirname(file_path), new_filename)\n",
    "        os.rename(file_path, new_file_path)\n",
    "        print(f\"Renamed '{file_path}' to '{new_filename}'\")\n",
    "    else:\n",
    "        print(f\"Title not found in '{file_path}'. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **First run of the notebook resulted in manually renaming the xml files by**:\n",
    "***\n",
    "\n",
    "1. localising the text in the Abstract tag in the xml files named \"Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml\".\n",
    "2. searching for matching text in the original pdf files stored in the \"miccai_papers\" directory and related subdirectories.\n",
    "    - if the text is found, the title of the paper is copied from the pdf file and used to rename the xml file.\n",
    "3. renaming the title of the xml file by:\n",
    "    - localising the title tag in the xml file,\n",
    "    - pasting the title of the paper from the pdf file, and\n",
    "    - saving the updated xml file.\n",
    "\n",
    "**Renaming files**:\n",
    "\n",
    "- **File #1**: Dual Conditioned Diffusion Models for Out-of-Distribution Detection: Application to Fetal Ultrasound Videos\n",
    "    - **original**: path + '/vol1/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "    - **renamed to**: '03MICCAI_notebook_GROBID_processed_volumes/vol1/Dual_Conditioned_Diffusion_Models_for_Out-of-Distribution_Detection:_Application_to_Fetal_Ultrasound_Videos.xml'\n",
    "\n",
    "- **File #2**: COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation\n",
    "    - **original**: path + '/vol2/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "    - **renamed to**: '03MICCAI_notebook_GROBID_processed_volumes/vol2/COLosSAL:_A_Benchmark_for_Cold-Start_Active_Learning_for_3D_Medical_Image_Segmentation.xml'\n",
    "\n",
    "- **File #3**: Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality\n",
    "    - **original**: path + '/vol4/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "    - **renamed to**: (details not provided)\n",
    "\n",
    "- **File #4**: (...)\n",
    "\n",
    "- **File #6**: Pelphix: Surgical Phase Recognition from X-Ray Images in Percutaneous Pelvic Fixation\n",
    "    - **original**: path + '/vol9/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "    - **renamed to**: '03MICCAI_notebook_GROBID_processed_volumes/vol9/Pelphix:_Surgical_Phase_Recognition_from_X-Ray_Images_in_Percutaneous_Pelvic_Fixation.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the processed volumes folder\n",
    "path = \"03MICCAI_notebook_GROBID_processed_volumes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST RUN - RENAMING THE XML FILES BASED ON THE TITLE TAGS:\n",
    "########################################################################################\n",
    "\n",
    "# Load and parse the XML file \n",
    "# file_path = path + '/vol1/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol2/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol4/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol6/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol7/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol9/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "# Load and parse the XML file\n",
    "tree = et.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Since XML namespaces can complicate direct tag access, we find the title tag dynamically.\n",
    "# This approach is based on the assumption that titles are relatively unique in structure.\n",
    "\n",
    "# Attempt to extract the paper title. This might need adjustments based on the actual structure.\n",
    "title = None\n",
    "for elem in root.iter():\n",
    "    if 'title' in elem.tag.lower():\n",
    "        title = elem.text\n",
    "        break\n",
    "\n",
    "title_clean = title.strip().replace(\" \", \"_\") if title else \"Untitled_Document\"\n",
    "\n",
    "# Attempt a more generic search for the title, considering common patterns in scholarly articles\n",
    "# We'll look for title elements that might be nested within other elements (like \"titleStmt\" or \"fileDesc\" in TEI format)\n",
    "\n",
    "def find_title(element):\n",
    "    \"\"\"\n",
    "    Recursively search for the title element in the XML structure.\n",
    "    \"\"\"\n",
    "    if 'title' in element.tag.lower() and element.text:\n",
    "        return element.text.strip()\n",
    "    for child in element:\n",
    "        title = find_title(child)\n",
    "        if title:\n",
    "            return title\n",
    "    return None\n",
    "\n",
    "# Attempt to find the title using the recursive search\n",
    "paper_title = find_title(root)\n",
    "paper_title_clean = paper_title.replace(\" \", \"_\") if paper_title else \"Untitled_Document\"\n",
    "\n",
    "\n",
    "# Define the new file path with the clean title\n",
    "new_file_path = os.path.join(os.path.dirname(file_path), f\"{paper_title_clean}.xml\")\n",
    "\n",
    "# Rename the file\n",
    "os.rename(file_path, new_file_path)\n",
    "\n",
    "new_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND RUN - RENAMING THE MISSING XML FILES BY SAME METHOD:\n",
    "########################################################################################\n",
    "\n",
    "''' \n",
    "2 unique titles missing, 1 in vol6 and vol7 each, where both files were named \n",
    "\"Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml\" instead of the actual title\n",
    "in the previous code block. \n",
    "\n",
    "The missing files are:\n",
    "- vol6: A Multi-task Method for Immunofixation Electrophoresis Image Classification (paper_15)\n",
    "- vol7: DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction (paper_13)\n",
    "\n",
    "Now, we will rename the files based on the actual titles.\n",
    "'''\n",
    "\n",
    "# 2 unique titles missing, 1 in vol6 and vol7 each:\n",
    "# vol6: A Multi-task Method for Immunofixation Electrophoresis Image Classification (paper_15)\n",
    "#file_path = '06/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "# vol7: DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction (paper_13)\n",
    "# file_path = '07/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "# Load and parse the XML file\n",
    "tree = et.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Since XML namespaces can complicate direct tag access, we find the title tag dynamically.\n",
    "# This approach is based on the assumption that titles are relatively unique in structure.\n",
    "\n",
    "# Attempt to extract the paper title. This might need adjustments based on the actual structure.\n",
    "title = None\n",
    "for elem in root.iter():\n",
    "    if 'title' in elem.tag.lower():\n",
    "        title = elem.text\n",
    "        break\n",
    "\n",
    "title_clean = title.strip().replace(\" \", \"_\") if title else \"Untitled_Document\"\n",
    "\n",
    "# Attempt a more generic search for the title, considering common patterns in scholarly articles\n",
    "# We'll look for title elements that might be nested within other elements (like \"titleStmt\" or \"fileDesc\" in TEI format)\n",
    "\n",
    "def find_title(element):\n",
    "    \"\"\"\n",
    "    Recursively search for the title element in the XML structure.\n",
    "    \"\"\"\n",
    "    if 'title' in element.tag.lower() and element.text:\n",
    "        return element.text.strip()\n",
    "    for child in element:\n",
    "        title = find_title(child)\n",
    "        if title:\n",
    "            return title\n",
    "    return None\n",
    "\n",
    "# Attempt to find the title using the recursive search\n",
    "paper_title = find_title(root)\n",
    "paper_title_clean = paper_title.replace(\" \", \"_\") if paper_title else \"Untitled_Document\"\n",
    "\n",
    "\n",
    "# Define the new file path with the clean title\n",
    "new_file_path = os.path.join(os.path.dirname(file_path), f\"{paper_title_clean}.xml\")\n",
    "\n",
    "# Rename the file\n",
    "os.rename(file_path, new_file_path)\n",
    "\n",
    "new_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree \n",
    "\n",
    "# Function to parse the XML files and extract the headers\n",
    "def parse_xml_and_extract_headers(file_path):\n",
    "    tree = etree.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    # Extract the paper title by XPath in the XML's structure\n",
    "    paper_title_element = root.find('.//tei:title', ns)\n",
    "\n",
    "    # If the title is not found, set a default value\n",
    "    paper_title = paper_title_element.text if paper_title_element is not None else \"No Title Found\"\n",
    "\n",
    "    # Extract all headers in the document\n",
    "    headers = root.xpath('//tei:head', namespaces=ns)\n",
    "    print(f\"Found {len(headers)} headers in '{paper_title}'\")\n",
    "    \n",
    "    data = [] # List to store the extracted data\n",
    "    for header in headers:\n",
    "        # Use XPath string() function to get all text within the <p> tags, including nested elements\n",
    "        text_content = ''.join(header.getparent().xpath('.//tei:p//text()', namespaces=ns))\n",
    "\n",
    "        # Organize the extracted data into a dictionary of key-value pairs\n",
    "        data.append({\n",
    "            'Paper Title': paper_title,\n",
    "            'Header Number': header.get('n'),\n",
    "            'Header Title': header.text,\n",
    "            'Text': text_content  # Updated to use text_content\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(data, columns=['Paper Title', 'Header Number', 'Header Title', 'Text'])\n",
    "    return df\n",
    "\n",
    "# Path to the processed volumes folder to extract the headers from the XML files and create a DataFrame with all headers \n",
    "def process_xml_folder(folder_path):\n",
    "    all_data_frames = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".xml\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = parse_xml_and_extract_headers(file_path)\n",
    "            all_data_frames.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single one\n",
    "    if all_data_frames:\n",
    "        final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Folder path - where XML files are be stored \n",
    "folder_path = '03MICCAI_notebook_GROBID_processed_volumes/*'\n",
    "\n",
    "# Create a folder to store the DataFrames\n",
    "output_folder = '03MICCAI_notebook_GROBID_dataframes'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# # Process the folder and create a DataFrame with all headers\n",
    "for volume_number in range(1, 11):\n",
    "    folder_path = f'{path}/vol{volume_number}'\n",
    "    print(f\"Creating a DataFrame with Extracted Data From'{folder_path}'...\")\n",
    "    df_headers = process_xml_folder(folder_path)\n",
    "    df_headers.to_csv(f\"03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol{volume_number}_headers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block is used to clean the DataFrames created from the XML files for each volume.\n",
    "The cleaning process involves removing rows where both 'Header Title' and 'Text' are NaN or just 'Text' is NaN.\n",
    "\n",
    "The cleaned DataFrames are saved to CSV files in a new folder named: \n",
    "- '03MICCAI_notebook_cleaned_dataframes'.\n",
    "\n",
    "The DataFrames are loaded from the: \n",
    "- '03MICCAI_notebook_GROBID_dataframes' folder and saved to the '03MICCAI_notebook_cleaned_dataframes' folder.\n",
    "\"\"\"\n",
    "\n",
    "def process_xml_folder(folder_path):\n",
    "    df = pd.read_csv(folder_path)\n",
    "    return df\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Remove rows where both 'Header Title' and 'Text' are NaN or just 'Text' is NaN\n",
    "    df_cleaned = df.dropna(subset=['Header Title', 'Text'], how='all')\n",
    "    df_cleaned = df_cleaned.dropna(subset=['Text'], how='any')\n",
    "    return df_cleaned\n",
    "\n",
    "# Dictionary to store cleaned DataFrames\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "# Base path where all processed volumes are stored\n",
    "base_path = '03MICCAI_notebook_GROBID_dataframes'\n",
    "\n",
    "\n",
    "output_folder = '03MICCAI_notebook_cleaned_dataframes'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "#/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol1_headers.csv\n",
    "# Cleaning the DataFrames created from the XML files for each volume\n",
    "for volume_number in range(1, 11):\n",
    "    folder_path = f'{base_path}/03MICCAI_notebook_df_vol{volume_number}_headers.csv'\n",
    "    print(f\"Cleaning DataFrame for '{folder_path}'...\")\n",
    "    # Process the XML files in the folder and create a DataFrame\n",
    "    df_headers = process_xml_folder(folder_path)\n",
    "    # Clean the DataFrame\n",
    "    df_cleaned = clean_dataframe(df_headers)\n",
    "    # Save the cleaned DataFrame to a CSV file in the output folder\n",
    "    df_cleaned.to_csv(f\"03MICCAI_notebook_cleaned_dataframes/03MICCAI_notebook_df_vol{volume_number}_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique paper titles in the cleaned DataFrames for volume 10\n",
    "len(df_cleaned['Paper Title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the folder with the cleaned_dataframes\n",
    "folder_path = '03MICCAI_notebook_cleaned_dataframes'\n",
    "\n",
    "# Verify unique title counts in the individual and saved dataframes before combining\n",
    "total_unique = 0\n",
    "for volume_number in range(1, 11):\n",
    "    file_path = f'{folder_path}/03MICCAI_notebook_df_vol{volume_number}_cleaned.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    unique_in_df = len(df['Paper Title'].unique())\n",
    "    print(f\"Unique titles in vol{volume_number}: {unique_in_df}\")\n",
    "    total_unique += unique_in_df\n",
    "\n",
    "print(f\"Sum of unique titles from individual DataFrames: {total_unique}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: 2 unique title were missing from vol6 and vol7. \n",
    "To solve this issue I had to manually localise the missing papers and add them into the dataframe by \n",
    "processing the sub folders into the client.process() function, where I got:\n",
    "\n",
    "- vol6: paper_15.grobid.tei.xml\n",
    "- vol7: paper_13.grobid.tei.xml\n",
    "\n",
    "The missing XML files were renamed based on the actual title of the papers and moved to the correct folder in the processed volumes folder.\n",
    "\n",
    "Secondly, I had to manually rename title in the vol-related dataframes to the correct title of the papers since some of the XML files were named\n",
    "\"Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml\" instead of the actual title of the papers. These papers were lost in the\n",
    "process of creating the dataframes and concatenating them into a single dataframe.\n",
    "\n",
    "Therefore, the final results are as follows:\n",
    "- vol6: A Multi-task Method for Immunofixation Electrophoresis Image Classification\n",
    "- vol7: DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction\n",
    "\n",
    "The final results are stored in the '03MICCAI_notebook_cleaned_dataframes' folder as CSV files in the format:\n",
    "- '03MICCAI_notebook_df_vol{volume_number}_cleaned.csv'.\n",
    "\n",
    "You can access the final results here:\n",
    "- the '00MICCAI_total_outputs' > '03MICCAI_all_outputs' folder.  \n",
    "\"\"\"\n",
    "\n",
    "# 2 unique titles missing, 1 in vol6 and vol7 each:\n",
    "# vol6: A Multi-task Method for Immunofixation Electrophoresis Image Classification (paper_15)\n",
    "# vol7: DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction\n",
    "\n",
    "# 2 titles are named \"Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023\" in the dataframes\n",
    "# vol6: AR2T:_Adaptive_Robust_Regression_Tree_for_Medical_Image_Segmentation\n",
    "# vol7: Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia\n",
    "# rename the files in the folder and re-run the code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all cleaned DataFrames into a single DataFrame\n",
    "all_cleaned_dataframes = []\n",
    "for volume_number in range(1, 11):\n",
    "    file_path = f'{folder_path}/03MICCAI_notebook_df_vol{volume_number}_cleaned.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    all_cleaned_dataframes.append(df)\n",
    "\n",
    "# Convert the list of DataFrames into a single DataFrame\n",
    "df_all_cleaned = pd.concat(all_cleaned_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_cleaned['Paper Title'].nunique()) # 730 unique titles in the combined DataFrame\n",
    "df_all_cleaned.head()\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "# df_all_cleaned.to_csv(\"03MICCAI_notebook_df_paper_extractions_all_cleaned_.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned = df_all_cleaned.fillna(0)\n",
    "\n",
    "# Lowercase all text in the 'Text' column\n",
    "df_all_cleaned['Text'] = df_all_cleaned['Text'].str.lower()\n",
    "df_all_cleaned['Text'] = df_all_cleaned['Text'].apply(wrap_text, width = 80)\n",
    "\n",
    "# Regular expression with str.replace to remove the volume information\n",
    "df_all_cleaned['Paper Title'] = df_all_cleaned['Paper Title'].str.replace(r'\\s*\\(vol\\d+\\)', '', regex=True)\n",
    "\n",
    "# Rename the columns to lowercase\n",
    "df_all_cleaned.rename(columns={'Paper Title': 'title', 'Header Number':'header_no', 'Header Title': 'header_title', 'Text':'text', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "df_all_cleaned.to_csv('03MICCAI_notebook_df_paper_extractions_all_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for 'cancer' in the Text column, case insensitive\n",
    "cancer_papers_mask = df_all_cleaned['text'].str.contains('cancer|tumor|tumour', case=False, na=False)\n",
    "papers_with_cancer = df_all_cleaned[cancer_papers_mask]\n",
    "\n",
    "# Get the unique titles of papers that mention 'cancer'\n",
    "unique_titles_with_cancer = papers_with_cancer['title'].unique()\n",
    "\n",
    "# Extract all headers and their related text for papers that mention 'cancer'\n",
    "extracted_info = pd.DataFrame()\n",
    "for title in unique_titles_with_cancer:\n",
    "    paper_info = df_all_cleaned[df_all_cleaned['title'] == title]\n",
    "    extracted_info = pd.concat([extracted_info, paper_info])\n",
    "\n",
    "# Reset index of the resulting DataFrame\n",
    "extracted_info.reset_index(drop=True, inplace=True)\n",
    "\n",
    "unique_paper_titles_with_cancer = extracted_info['title'].unique()\n",
    "print(len(unique_paper_titles_with_cancer)) # Number of unique papers that mention 'cancer' is 263\n",
    "\n",
    "# Save the extracted information to a CSV file for further analysis or processing\n",
    "\"\"\"The manually processed and implemented CSV file is stored in the '00MICCAI_total_outputs' > '03MICCAI_all_outputs' folder\"\"\"\n",
    "# extracted_info.to_csv(\"03MICCAI_notebook_df_paper_extractions_cancer.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "extracted_info.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'age': ['age', 'age', 'young', 'old', 'gender'],\n",
    "    'gender': ['gender', 'sex', 'women', 'woman', 'female', 'male'],\n",
    "    'ethnicity': ['ethnicity', 'ethnicities', 'race', 'white patients', 'black patients'],\n",
    "    'location_info': ['geolocation', 'geographical', 'geographic', 'country', 'countries', \n",
    "                    'city', 'cities', 'hospital', 'hospitals', 'clinic', 'clinics', 'continent',\n",
    "                    'province', 'state', 'region', 'town', 'village', 'area', 'district'],\n",
    "    'patients': ['patient', 'patients'],\n",
    "    'dataset_info': ['dataset', 'datasets', 'data set', 'data sets', 'publicly', 'public', 'private', 'open access', 'open-access'],\n",
    "    'bias_info': ['bias', 'biases', 'fairness'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Flatten the list of all keywords excluding 'patients' to avoid redundancy\n",
    "all_keywords = sum([kw for cat, kw in categories.items() if cat != 'patients'], [])\n",
    "\n",
    "# Filter papers that mention 'patient' or 'patients'\n",
    "scope_mask = extracted_info['text'].str.contains('patient|patients', case=False, na=False)\n",
    "papers_with_patients = extracted_info[scope_mask]\n",
    "\n",
    "# Prepare a list to collect paper info dictionaries\n",
    "papers_info_list = []\n",
    "\n",
    "# Iterate over unique titles in the filtered DataFrame\n",
    "for title in papers_with_patients['title'].unique():\n",
    "    paper_info = papers_with_patients[papers_with_patients['title'] == title]\n",
    "    # Initialize a dictionary for the current paper with zeros for all keywords\n",
    "    paper_keywords = dict.fromkeys(all_keywords, 0)\n",
    "    paper_keywords['title'] = title\n",
    "    # Check for each keyword in the text of the paper\n",
    "    for keyword in all_keywords:\n",
    "        if any(paper_info['text'].str.contains(keyword, case=False, na=False)):\n",
    "            paper_keywords[keyword] = 1\n",
    "    # Collect the keyword matches for the current paper\n",
    "    papers_info_list.append(paper_keywords)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "keywords_per_paper = pd.DataFrame(papers_info_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file for further analysis\n",
    "# papers_with_patients.to_csv(\"03MICCAI_notebook_df_paper_extractions_patients_and_cancer.csv\", index=False)\n",
    "\n",
    "# The total number of unique titles will be 156 if the extractions of missing papers hasn't been processed manually in previous code\n",
    "print('Number of unique titles for papers containing the keyword <patient/patients>:', len(papers_with_patients['title'].unique())) # 155\n",
    "papers_with_patients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For the final step, we will randomly select 100 unique papers from the DataFrame containing papers with patients.\n",
    "The selected papers will be saved to a CSV file for annotation and further analysis.\n",
    "\n",
    "The selected papers will be saved to a CSV file named '03MICCAI_notebook_100_randomly_selected_papers.csv'.\n",
    "\"\"\"\n",
    "\n",
    "# Name of the notebook for saving the selected papers\n",
    "notebook_name = '03MICCAI_notebook_'\n",
    "\n",
    "# Check if the number of unique titles is at least 100\n",
    "unique_titles = papers_with_patients['title'].nunique()\n",
    "if unique_titles < 100:\n",
    "    print(f\"Warning: Only {unique_titles} unique papers found, less than 100.\")\n",
    "\n",
    "# Randomly select 1000 unique titles\n",
    "selected_titles = papers_with_patients['title'].drop_duplicates().sample(n=min(100, unique_titles), random_state=32)\n",
    "\n",
    "# Filter the original DataFrame to include only the selected titles\n",
    "selected_papers_df = papers_with_patients[papers_with_patients['title'].isin(selected_titles)]\n",
    "\n",
    "# Save selected_papers_df DataFrame with 100 randomly selected papers and their related rows\n",
    "#selected_papers_df.to_csv(notebook_name + '100_randomly_selected_papers.csv')\n",
    "\n",
    "# Print the number of unique titles in the selected DataFrame\n",
    "print(f\"Number of unique titles in the selected DataFrame: {selected_papers_df['title'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2\n",
    "***\n",
    "\n",
    "This section produces the output files used for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries and installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lxml\n",
    "\n",
    "# Run GROBID in terminal before running the notebook\n",
    "# Installation and running commands\n",
    "# wget https://github.com/kermitt2/grobid/archive/0.8.0.zip\n",
    "# unzip 0.8.0.zip\n",
    "# cd grobid-0.8.0\n",
    "# ./gradlew run\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xml.etree import ElementTree as et\n",
    "from lxml import etree \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, width=100):\n",
    "    \"\"\"\n",
    "    A simple function to wrap text at a given width.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return text  # Handle NaN values\n",
    "    \n",
    "    wrapped_lines = []\n",
    "    for paragraph in text.split('\\n'):  # Splitting by existing newlines to preserve paragraph breaks\n",
    "        line = ''\n",
    "        for word in paragraph.split():\n",
    "            if len(line) + len(word) + 1 > width:\n",
    "                wrapped_lines.append(line)\n",
    "                line = word\n",
    "            else:\n",
    "                line += (' ' + word if line else word)\n",
    "        wrapped_lines.append(line)\n",
    "    return '\\n'.join(wrapped_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning, filtering and selecting subset of MICCAI 2023 articles for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the correct output of manually processed papers, which where previously stored in '00MICCAI_total_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol1_headers.csv'...\n",
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol2_headers.csv'...\n",
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol3_headers.csv'...\n",
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol4_headers.csv'...\n",
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol5_headers.csv'...\n",
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol6_headers.csv'...\n",
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol7_headers.csv'...\n",
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol8_headers.csv'...\n",
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol9_headers.csv'...\n",
      "Cleaning DataFrame for '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol10_headers.csv'...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following code block is used to clean the DataFrames created from the XML files for each volume.\n",
    "The cleaning process involves removing rows where both 'Header Title' and 'Text' are NaN or just 'Text' is NaN.\n",
    "\n",
    "The cleaned DataFrames are saved to CSV files in a new folder named: \n",
    "- '03MICCAI_notebook_cleaned_dataframes'.\n",
    "\n",
    "The DataFrames are loaded from the: \n",
    "- '03MICCAI_notebook_GROBID_dataframes' folder and saved to the '03MICCAI_notebook_cleaned_dataframes' folder.\n",
    "\"\"\"\n",
    "\n",
    "def process_xml_folder(folder_path):\n",
    "    df = pd.read_csv(folder_path)\n",
    "    return df\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Remove rows where both 'Header Title' and 'Text' are NaN or just 'Text' is NaN\n",
    "    df_cleaned = df.dropna(subset=['Header Title', 'Text'], how='all')\n",
    "    df_cleaned = df_cleaned.dropna(subset=['Text'], how='any')\n",
    "    return df_cleaned\n",
    "\n",
    "# Dictionary to store cleaned DataFrames\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "# Base path where all processed volumes are stored\n",
    "#base_path = '03MICCAI_notebook_GROBID_dataframes'\n",
    "base_path = '00MICCAI_total_outputs/03MICCAI_all_outputs/03MICCAI_notebook_GROBID_dataframes'\n",
    "\n",
    "\n",
    "output_folder = '03MICCAI_notebook_cleaned_dataframes'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "#/03MICCAI_notebook_GROBID_dataframes/03MICCAI_notebook_df_vol1_headers.csv\n",
    "# Cleaning the DataFrames created from the XML files for each volume\n",
    "for volume_number in range(1, 11):\n",
    "    folder_path = f'{base_path}/03MICCAI_notebook_df_vol{volume_number}_headers.csv'\n",
    "    print(f\"Cleaning DataFrame for '{folder_path}'...\")\n",
    "    # Process the XML files in the folder and create a DataFrame\n",
    "    df_headers = process_xml_folder(folder_path)\n",
    "    # Clean the DataFrame\n",
    "    df_cleaned = clean_dataframe(df_headers)\n",
    "    # Save the cleaned DataFrame to a CSV file in the output folder\n",
    "    df_cleaned.to_csv(f\"03MICCAI_notebook_cleaned_dataframes/03MICCAI_notebook_df_vol{volume_number}_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of unique paper titles in the cleaned DataFrames for volume 10\n",
    "len(df_cleaned['Paper Title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique titles in vol1: 73\n",
      "Unique titles in vol2: 73\n",
      "Unique titles in vol3: 72\n",
      "Unique titles in vol4: 75\n",
      "Unique titles in vol5: 76\n",
      "Unique titles in vol6: 77\n",
      "Unique titles in vol7: 75\n",
      "Unique titles in vol8: 65\n",
      "Unique titles in vol9: 70\n",
      "Unique titles in vol10: 74\n",
      "Sum of unique titles from individual DataFrames: 730\n"
     ]
    }
   ],
   "source": [
    "# Read in the folder with the cleaned_dataframes\n",
    "folder_path = '03MICCAI_notebook_cleaned_dataframes'\n",
    "\n",
    "# Verify unique title counts in the individual and saved dataframes before combining\n",
    "total_unique = 0\n",
    "for volume_number in range(1, 11):\n",
    "    file_path = f'{folder_path}/03MICCAI_notebook_df_vol{volume_number}_cleaned.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    unique_in_df = len(df['Paper Title'].unique())\n",
    "    print(f\"Unique titles in vol{volume_number}: {unique_in_df}\")\n",
    "    total_unique += unique_in_df\n",
    "\n",
    "print(f\"Sum of unique titles from individual DataFrames: {total_unique}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Header Number</th>\n",
       "      <th>Header Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>To reduce radiologists' reading burden and mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Method</td>\n",
       "      <td>Notation. We first formally define the problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Stage 1-Proxy Task to Detect Synthetic Anomalies</td>\n",
       "      <td>AMAE starts the first training stage using onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Stage 2-MAE Inter-Discrepancy Adaptation</td>\n",
       "      <td>The proposed MAE adaptation scheme is inspired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>Datasets. We evaluated our method on three pub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Paper Title Header Number  \\\n",
       "0  AMAE: Adaptation of Pre-trained Masked Autoenc...           1.0   \n",
       "1  AMAE: Adaptation of Pre-trained Masked Autoenc...           2.0   \n",
       "2  AMAE: Adaptation of Pre-trained Masked Autoenc...           2.1   \n",
       "3  AMAE: Adaptation of Pre-trained Masked Autoenc...           2.2   \n",
       "4  AMAE: Adaptation of Pre-trained Masked Autoenc...           3.0   \n",
       "\n",
       "                                       Header Title  \\\n",
       "0                                      Introduction   \n",
       "1                                            Method   \n",
       "2  Stage 1-Proxy Task to Detect Synthetic Anomalies   \n",
       "3          Stage 2-MAE Inter-Discrepancy Adaptation   \n",
       "4                                       Experiments   \n",
       "\n",
       "                                                Text  \n",
       "0  To reduce radiologists' reading burden and mak...  \n",
       "1  Notation. We first formally define the problem...  \n",
       "2  AMAE starts the first training stage using onl...  \n",
       "3  The proposed MAE adaptation scheme is inspired...  \n",
       "4  Datasets. We evaluated our method on three pub...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all cleaned DataFrames into a single DataFrame\n",
    "all_cleaned_dataframes = []\n",
    "for volume_number in range(1, 11):\n",
    "    file_path = f'{folder_path}/03MICCAI_notebook_df_vol{volume_number}_cleaned.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    all_cleaned_dataframes.append(df)\n",
    "\n",
    "# Convert the list of DataFrames into a single DataFrame\n",
    "df_all_cleaned = pd.concat(all_cleaned_dataframes, ignore_index=True)\n",
    "df_all_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    }
   ],
   "source": [
    "print(df_all_cleaned['Paper Title'].nunique()) # 730 unique titles in the combined DataFrame\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "df_all_cleaned.to_csv(\"03MICCAI_notebook_df_paper_extractions_all_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned = df_all_cleaned.fillna(0)\n",
    "\n",
    "# Lowercase all text in the 'Text' column\n",
    "df_all_cleaned['Text'] = df_all_cleaned['Text'].str.lower()\n",
    "df_all_cleaned['Text'] = df_all_cleaned['Text'].apply(wrap_text, width = 80)\n",
    "\n",
    "# Regular expression with str.replace to remove the volume information\n",
    "df_all_cleaned['Paper Title'] = df_all_cleaned['Paper Title'].str.replace(r'\\s*\\(vol\\d+\\)', '', regex=True)\n",
    "\n",
    "# Rename the columns to lowercase\n",
    "df_all_cleaned.rename(columns={'Paper Title': 'title', 'Header Number':'header_no', 'Header Title': 'header_title', 'Text':'text', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "df_all_cleaned.to_csv('03MICCAI_notebook_df_paper_extractions_all_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>header_no</th>\n",
       "      <th>header_title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>chest radiographs (chest x-rays) represent the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Related Work</td>\n",
       "      <td>weakly supervised pathology detection. due to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Model</td>\n",
       "      <td>figure 1 provides an overview of our method. g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Inference</td>\n",
       "      <td>during inference, the trained model predicts a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>Training</td>\n",
       "      <td>the anatomical region detector is trained usin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title header_no  header_title  \\\n",
       "0  Anatomy-Driven Pathology Detection on Chest X-...       1.0  Introduction   \n",
       "1  Anatomy-Driven Pathology Detection on Chest X-...       2.0  Related Work   \n",
       "2  Anatomy-Driven Pathology Detection on Chest X-...       3.1         Model   \n",
       "3  Anatomy-Driven Pathology Detection on Chest X-...       3.2     Inference   \n",
       "4  Anatomy-Driven Pathology Detection on Chest X-...       3.3      Training   \n",
       "\n",
       "                                                text  \n",
       "0  chest radiographs (chest x-rays) represent the...  \n",
       "1  weakly supervised pathology detection. due to ...  \n",
       "2  figure 1 provides an overview of our method. g...  \n",
       "3  during inference, the trained model predicts a...  \n",
       "4  the anatomical region detector is trained usin...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for 'cancer' in the Text column, case insensitive\n",
    "cancer_papers_mask = df_all_cleaned['text'].str.contains('cancer|tumor|tumour', case=False, na=False)\n",
    "papers_with_cancer = df_all_cleaned[cancer_papers_mask]\n",
    "\n",
    "# Get the unique titles of papers that mention 'cancer'\n",
    "unique_titles_with_cancer = papers_with_cancer['title'].unique()\n",
    "\n",
    "# Extract all headers and their related text for papers that mention 'cancer'\n",
    "extracted_info = pd.DataFrame()\n",
    "for title in unique_titles_with_cancer:\n",
    "    paper_info = df_all_cleaned[df_all_cleaned['title'] == title]\n",
    "    extracted_info = pd.concat([extracted_info, paper_info])\n",
    "\n",
    "# Reset index of the resulting DataFrame\n",
    "extracted_info.reset_index(drop=True, inplace=True)\n",
    "\n",
    "unique_paper_titles_with_cancer = extracted_info['title'].unique()\n",
    "print(len(unique_paper_titles_with_cancer)) # Number of unique papers that mention 'cancer' is 263\n",
    "\n",
    "# Save the extracted information to a CSV file for further analysis or processing\n",
    "\"\"\"The manually processed and implemented CSV file was stored in the '00MICCAI_total_outputs' > '03MICCAI_all_outputs' folder\"\"\"\n",
    "extracted_info.to_csv(\"03MICCAI_notebook_df_paper_extractions_cancer.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "extracted_info.head() # 263 unique papers that mention 'cancer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique titles for papers containing the keyword <patient/patients>: 155\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>header_no</th>\n",
       "      <th>header_title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>training dataset. we train on the chest imagen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Self-supervised Learning for Physiologically-B...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Dataset</td>\n",
       "      <td>the dataset is composed of 23 oncological pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Self-supervised Learning for Physiologically-B...</td>\n",
       "      <td>0</td>\n",
       "      <td>(Color figure online)</td>\n",
       "      <td>the most important design choice is the select...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Self-supervised Learning for Physiologically-B...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>even though the choice of the final activation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>AME-CAM: Attentive Multiple-Exit CAM for Weakl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Conclusion</td>\n",
       "      <td>in this work, we propose a brain tumor segment...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title header_no  \\\n",
       "5   Anatomy-Driven Pathology Detection on Chest X-...       3.4   \n",
       "18  Self-supervised Learning for Physiologically-B...       2.4   \n",
       "20  Self-supervised Learning for Physiologically-B...         0   \n",
       "21  Self-supervised Learning for Physiologically-B...       4.0   \n",
       "32  AME-CAM: Attentive Multiple-Exit CAM for Weakl...       5.0   \n",
       "\n",
       "             header_title                                               text  \n",
       "5                 Dataset  training dataset. we train on the chest imagen...  \n",
       "18                Dataset  the dataset is composed of 23 oncological pati...  \n",
       "20  (Color figure online)  the most important design choice is the select...  \n",
       "21             Discussion  even though the choice of the final activation...  \n",
       "32             Conclusion  in this work, we propose a brain tumor segment...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = {\n",
    "    'age': ['age', 'age', 'young', 'old', 'gender'],\n",
    "    'gender': ['gender', 'sex', 'women', 'woman', 'female', 'male'],\n",
    "    'ethnicity': ['ethnicity', 'ethnicities', 'race', 'white patients', 'black patients'],\n",
    "    'location_info': ['geolocation', 'geographical', 'geographic', 'country', 'countries', \n",
    "                    'city', 'cities', 'hospital', 'hospitals', 'clinic', 'clinics', 'continent',\n",
    "                    'province', 'state', 'region', 'town', 'village', 'area', 'district'],\n",
    "    'patients': ['patient', 'patients'],\n",
    "    'dataset_info': ['dataset', 'datasets', 'data set', 'data sets', 'publicly', 'public', 'private', 'open access', 'open-access'],\n",
    "    'bias_info': ['bias', 'biases', 'fairness'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Flatten the list of all keywords excluding 'patients' to avoid redundancy\n",
    "all_keywords = sum([kw for cat, kw in categories.items() if cat != 'patients'], [])\n",
    "\n",
    "# Filter papers that mention 'patient' or 'patients'\n",
    "scope_mask = extracted_info['text'].str.contains('patient|patients', case=False, na=False)\n",
    "papers_with_patients = extracted_info[scope_mask]\n",
    "\n",
    "# Prepare a list to collect paper info dictionaries\n",
    "papers_info_list = []\n",
    "\n",
    "# Iterate over unique titles in the filtered DataFrame\n",
    "for title in papers_with_patients['title'].unique():\n",
    "    paper_info = papers_with_patients[papers_with_patients['title'] == title]\n",
    "    # Initialize a dictionary for the current paper with zeros for all keywords\n",
    "    paper_keywords = dict.fromkeys(all_keywords, 0)\n",
    "    paper_keywords['title'] = title\n",
    "    # Check for each keyword in the text of the paper\n",
    "    for keyword in all_keywords:\n",
    "        if any(paper_info['text'].str.contains(keyword, case=False, na=False)):\n",
    "            paper_keywords[keyword] = 1\n",
    "    # Collect the keyword matches for the current paper\n",
    "    papers_info_list.append(paper_keywords)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "keywords_per_paper = pd.DataFrame(papers_info_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file for further analysis\n",
    "papers_with_patients.to_csv(\"03MICCAI_notebook_df_paper_extractions_patients_and_cancer.csv\", index=False)\n",
    "\n",
    "print('Number of unique titles for papers containing the keyword <patient/patients>:', len(papers_with_patients['title'].unique())) # 155\n",
    "papers_with_patients.head() # 155 unique papers that mention 'patient/patients'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique titles in the selected DataFrame: 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For the final step, we will randomly select 100 unique papers from the DataFrame containing papers with patients.\n",
    "The selected papers will be saved to a CSV file for annotation and further analysis.\n",
    "\n",
    "The selected papers will be saved to a CSV file named '03MICCAI_notebook_100_randomly_selected_papers.csv'.\n",
    "\"\"\"\n",
    "\n",
    "# Name of the notebook for saving the selected papers\n",
    "notebook_name = '03MICCAI_notebook_'\n",
    "\n",
    "# Check if the number of unique titles is at least 100\n",
    "unique_titles = papers_with_patients['title'].nunique()\n",
    "if unique_titles < 100:\n",
    "    print(f\"Warning: Only {unique_titles} unique papers found, less than 100.\")\n",
    "\n",
    "# Randomly select 1000 unique titles\n",
    "selected_titles = papers_with_patients['title'].drop_duplicates().sample(n=min(100, unique_titles), random_state=32)\n",
    "\n",
    "# Filter the original DataFrame to include only the selected titles\n",
    "selected_papers_df = papers_with_patients[papers_with_patients['title'].isin(selected_titles)]\n",
    "\n",
    "# Save selected_papers_df DataFrame with 100 randomly selected papers and their related rows\n",
    "selected_papers_df.to_csv(notebook_name + '100_randomly_selected_papers.csv')\n",
    "\n",
    "# Print the number of unique titles in the selected DataFrame\n",
    "print(f\"Number of unique titles in the selected DataFrame: {selected_papers_df['title'].nunique()}\") # 100 unique titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If GROBID did not run succesfully:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrames for all papers, cancer papers, patient papers, and 100 randomly selected papers\n",
    "'''\n",
    "If the previous code blocks did not run successfully, this code block will run already processed and stored\n",
    "CSV files in the '00MICCAI_total_outputs' > '03MICCAI_all_outputs' folder.\n",
    "'''\n",
    "path = '00MICCAI_total_outputs/03MICCAI_all_outputs/'\n",
    "\n",
    "all_papers = pd.read_csv(path + '03MICCAI_notebook_df_paper_extractions_all_cleaned.csv')\n",
    "print(f'Total number of unique papers: {all_papers[\"title\"].nunique()}') # 730\n",
    "\n",
    "cancer_papers = pd.read_csv(path + '03MICCAI_notebook_df_paper_extractions_cancer.csv')\n",
    "print(f'Total number of unique papers mentioning \"cancer\": {cancer_papers[\"title\"].nunique()}') # 263\n",
    "\n",
    "patient_cancer_papers = pd.read_csv(path + '03MICCAI_notebook_df_paper_extractions_patients_and_cancer.csv')\n",
    "print(f'Total number of unique papers mentioning \"patient/patients\": {patient_cancer_papers[\"title\"].nunique()}') # 155\n",
    "\n",
    "rand_selected_papers = pd.read_csv(path + '03MICCAI_notebook_100_randomly_selected_papers.csv')\n",
    "print(f'Total number of unique papers in the randomly selected 100 papers: {rand_selected_papers[\"title\"].nunique()}') # 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
