<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images</title>
				<funder ref="#_HxztEYe">
					<orgName type="full">Royal Society</orgName>
				</funder>
				<funder>
					<orgName type="full">Wellcome Leap Dynamic Resilience</orgName>
				</funder>
				<funder ref="#_MFTAH8X">
					<orgName type="full">MRC</orgName>
				</funder>
				<funder ref="#_p8GCtA5">
					<orgName type="full">NVIDIA Academic Hardware Grant Program</orgName>
				</funder>
				<funder ref="#_6nPrMTP">
					<orgName type="full">ERC IMI</orgName>
				</funder>
				<funder>
					<orgName type="full">Boehringer Ingelheim Ltd</orgName>
				</funder>
				<funder ref="#_PmqEEzf">
					<orgName type="full">UKRI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaodan</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Heart and Lung Institute</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Federico</forename><surname>Felder</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Heart and Lung Institute</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Royal Brompton Hospital</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Nan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Heart and Lung Institute</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giorgos</forename><surname>Papanastasiou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<settlement>Essex</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Walsh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Heart and Lung Institute</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Royal Brompton Hospital</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
							<email>g.yang@imperial.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Bioengineering Department and Imperial-X</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<postCode>W12 7SL</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">National Heart and Lung Institute</orgName>
								<orgName type="institution" key="instit2">Imperial College London</orgName>
								<address>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Cardiovascular Research Centre</orgName>
								<orgName type="institution">Royal Brompton Hospital</orgName>
								<address>
									<postCode>SW3 6NP</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">School of Biomedical Engineering &amp; Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="13" to="22"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">669002DC0AE9F0003A9545A41EDECAE0</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Synthetic Data Augmentation • Medical Image Synthesis</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>images and elicit further developments for utility-aware deep generative models in medical image synthesis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>London WC2R 2LS, UK</head><p>Abstract. Synthetic images generated from deep generative models have the potential to address data scarcity and data privacy issues. The selection of synthesis models is mostly based on image quality measurements, and most researchers favor synthetic images that produce realistic images, i.e., images with good fidelity scores, such as low Fréchet Inception Distance (FID) and high Peak Signal-To-Noise Ratio (PSNR). However, the quality of synthetic images is not limited to fidelity, and a wide spectrum of metrics should be evaluated to comprehensively measure the quality of synthetic images. In addition, quality metrics are not truthful predictors of the utility of synthetic images, and the relations between these evaluation metrics are not yet clear. In this work, we have established a comprehensive set of evaluators for synthetic images, including fidelity, variety, privacy, and utility. By analyzing more than 100k chest X-ray images and their synthetic copies, we have demonstrated that there is an inevitable trade-off between synthetic image fidelity, variety, and privacy. In addition, we have empirically demonstrated that the utility score does not require images with both high fidelity and high variety. For intra-and cross-task data augmentation, mode-collapsed images and low-fidelity images can still demonstrate high utility. Finally, our experiments have also showed that it is possible to produce images with both high utility and privacy, which can provide a strong rationale for the use of deep generative models in privacy-preserving applications. Our study can shore up comprehensive guidance for the evaluation of synthetic Supplementary Information The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fig. <ref type="figure">1</ref>. The major contribution (a) and conclusions (b,c) of this work. Our study proposes a comprehensive and disentangled four-dimensional framework for synthetic medical images, incorporating fidelity, variety, privacy, and utility. We conduct extensive experiments to investigate the interplay between these dimensions and identify a set of best practices (c) for selecting synthetic models for downstream tasks.</p><p>In 2002, SMOTE <ref type="bibr" target="#b0">[1]</ref> was proposed to generate synthetic samples to increase the accuracy of classification tasks. Since then, synthetic data has emerged as a promising solution for addressing the data scarcity problem by generating additional training data to supplement the limited real-world datasets. In addition, the potential of synthetic data for privacy preservation has led to the development of generative deep learning models that have shown promise in producing high-quality synthetic data which maintain the statistical properties of the original data while preserving the privacy of individuals in the training data.</p><p>Deep learning practitioners have been using various metrics to evaluate synthetic images, including Fréchet Inception Distance (FID) <ref type="bibr" target="#b3">[4]</ref>, Inception Score (IS) <ref type="bibr" target="#b9">[10]</ref>, precision, and recall <ref type="bibr" target="#b6">[7]</ref>. However, these measurements are entangled, i.e., they are only able to measure image quality holistically, not a specific aspect. For example, FID is defined by</p><formula xml:id="formula_0">FID = |(|μ 1 -μ 2 |)| 2 + Tr(Σ 1 + Σ 2 -2 (Σ 1 Σ 2 ).<label>(1)</label></formula><p>Here, where μ 1 , μ 2 Σ 1 , and Σ 2 are the mean vectors and covariance matrices of the feature representations of two sets of images. A high difference between the image diversity (Σ 1 and Σ 2 ) also leads to a high FID score, which further complicates fidelity evaluation. Another entangled fidelity evaluation is the precision. As is shown in Fig. <ref type="figure" target="#fig_0">2</ref> (a), a high-precision matrix cannot identify non-authentic synthetic images which are copies of real data. Thus, a high-precision matrix can either be caused by high fidelity, or a high privacy breach.</p><p>When evaluating these entangled metrics, it is difficult to find the true weakness and strengths of synthetic models. In addition, large-scale experiments are currently the only way to measure the utility of synthetic data. The confusion of evaluation metrics and this time and resource-consuming evaluation of synthetic data utility increase the expenses of synthetic model selection and hinder the real-world application of synthetic data.</p><p>In this study, we aim to provide a set of evaluation metrics that are mathematically disentangled and measure the potential correlation between different aspects of the synthetic image as in Fig. <ref type="figure">1 (a)</ref>. Then, we aim to analyze the predictive ability of these proposed metrics to image utilities for different downstream tasks and provide a set of best practices for selecting synthetic models in various clinical scenarios. We compare two state-of-the-art deep generative models with different parameters using a large open-access X-ray dataset that contains more than 100k data <ref type="bibr" target="#b4">[5]</ref>. Through our experiments, we empirically show the negative correlations among synthetic image fidelity, variety, and privacy (Fig. <ref type="figure">1  (b)</ref>). After analyzing their impacts on downstream tasks, we discovered that the common problems in data synthesis, i.e., mode collapse and low fidelity, can sometimes be a merit according to the various motivations of different downstream tasks.</p><p>Overall, our study contributes new insights into the use of synthetic data for medical image analysis and provides a more objective and reproducible approach to evaluating synthetic data quality. By addressing these fundamental questions, our work provides a valuable foundation for future research and practical applications of synthetic data in medical imaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Generative Models and Evaluation Metrics</head><p>In this study, we conducted an empirical evaluation using two state-of-theart deep generative models: StyleGAN2, which has brought new standards for generative modeling regarding image quality <ref type="bibr" target="#b10">[11]</ref> and Latent Diffusion Models (LDM) <ref type="bibr" target="#b8">[9]</ref>. We proposed an analysis framework for synthetic images based on four key dimensions: fidelity, variety, utility, and privacy. Manual evaluation of synthetic image fidelity typically involves human experts assessing whether synthetic images appear realistic. However, this evaluation can be subjective and have high intra-observer variance. While many algorithms can be used to measure synthetic image quality, most of them are designed to capture more than one of the four key dimensions.</p><p>With this motivation, we aimed to redefine the conventional evaluation metrics of synthetic images and disentangle them into four independent properties. In this study, we employed a metric-based membership inference attack in an unsupervised manner to evaluate the privacy of our model. We presume that synthetic records should have some similarity with the records used to generate them <ref type="bibr" target="#b1">[2]</ref>. Since differential privacy settings could significantly reduce image fidelity, we chose not to perform differential privacy analysis in this study.  -Copy set S copy i : Synthetic sets that are realistic but are also copies of this real image r i . Synthetic images will belong to this set if this synthetic image is closer to r i than any other real images, i.e., D(s j , r i ) &lt; D(r i , r i,1 ). -Real set S real i : Synthetic sets that are realistic and are not copies of this real image r i . Synthetic images will belong to this set if this synthetic image is in the kth-nearest neighbor of r i , i.e., D(s j , r i ) ∈ [(D(r i , r i,1 ), D(r i , r i,k )] -Non-real set S non-real i : Synthetic sets that are not realistic compared to r i . Synthetic images will belong to this set if this synthetic image is not the kth-nearest neighbor of r i , i.e., D(s j , r i ) &gt; D(r i , r i,k )</p><p>We can compute the privacy preservation score of a single synthetic image s j |j ∈ [0, N)] with the definition of three sets. If the synthetic data p j is in the copy set of any real data, the privacy protection ability of this p j is 0, i.e.,</p><formula xml:id="formula_1">p j = 0 if ∃i ∈ [0, M) → s j ∈ S copy i 1 otherwise . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>For the synthetic data, the overall privacy protection ability P ∈ [0, 1] was then defined by</p><formula xml:id="formula_3">P = N 0 p j N . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>With this privacy definition, we have adjusted the original fidelity evaluation <ref type="bibr" target="#b6">[7]</ref> shown in Eq. ( <ref type="formula" target="#formula_5">4</ref>) to a privacy violation considered formula f p in Eq. 5,</p><formula xml:id="formula_5">f j = 0 if ∃i ∈ [0, M) → s j ∈ S copy i ∪ S real i 1 otherwise and F = N 0 f j N ;<label>(4)</label></formula><formula xml:id="formula_6">f p j = 0 if ∃i ∈ [0, M) → s j ∈ S real i 1 otherwise and F p = N 0 f p j N . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>The measurements of image distance can be tricky due to the high resolution of the original images (512×512). Thus, we first used VQ-VAE <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref> to quantize all images to integral latent feature maps, i.e., each pixel in the latent feature maps is a Q-way categorical variable, sampling from 0 to Q -1, and then compute the Hamming distance between these images. In our experiments, we used Q = 256.</p><p>Variety. To measure the variety, we introduced the JPEG file size of the mean image. The lossless JPEG file size of the group average image was used to measure the inner class variety in the ImageNet dataset <ref type="bibr" target="#b2">[3]</ref>. This approach was justified by the authors who presumed that a dataset containing diverse images would result in a blurrier average image, thus reducing the lossless JPEG file size of the mean image. To ensure that the variety score is positively correlated with the true variety, we normalized it to [0, 1] across all groups of synthetic images, and then subtracted it from 1 to obtain the final variety score. It is worth noting that variety can also be quantified by the standard deviation of the discrete latent features. However, in our study, we chose to measure variety in the original image space to better align with human perception.</p><p>Utility. We divided our X-ray dataset into four groups: training datasets A1 and A2, validation set B, and testing set C. We also included an additional open-access pediatric X-ray dataset, D. For our simulation, we treated A1 as a local dataset and A2 as a remote dataset that cannot be accessed by A1. We evaluated the utility of synthetic data in two conditions:</p><p>1. A1 vs. adding synthetic data generated from A1. In this condition, no privacy issue is considered. 2. A1 vs. adding synthetic data generated from A2. In this condition, synthetic data will be evaluated using privacy protection skills.</p><p>Under both conditions, we evaluated the intra-task augmentation utility and cross-task augmentation utility to simulate real-world use cases for synthetic data. Intra-task augmentation utility is measured by the percentage improvement in classification accuracy of C when adding synthetic data to the training dataset. We used a paired Wilcoxon signed-rank test to assess the significance of the accuracy improvement. If the improvement is significant, it indicates that the synthetic images are useful. We compared the augmentation utility with simple augmentations, such as random flipping, rotating, and contrasting. The cross-task augmentation utility is determined by the power of features extracted from the models trained with synthetic data. We used the models to extract features from D and trained a Support Vector Machine classifier on these features to measure accuracy. This allowed us to evaluate whether synthetic images can provide powerful features that facilitate downstream tasks. Similarly, the cross-task augmentation utility is also the percentage improvement in classification accuracy compared to the model trained only on A1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Settings and Parameters</head><p>We primarily evaluated the performance of synthetic data on the CheXpert dataset, with a focus on identifying the presence of pleural effusion (PE). To perform our evaluation, we split the large dataset into four subsets: A1 (15004 with PE and 5127 without PE), A2 (30692 with PE and 10118 without PE), B (3738 with PE and 1160 PE), and C (12456 with PE and 3863 without PE). To evaluate the cross-task utility of synthetic models, we used an X-ray dataset D<ref type="foot" target="#foot_0">1</ref> consisting of 5863 images of pediatric patients with pneumonia and normal controls. We resized all X-ray images to a resolution of 512×512 before evaluation.</p><p>For the StyleGAN2 method, we utilized six truncation parameters during sampling to generate six sets of synthetic images (φ ∈ [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]). In total, we trained 16 classification models on different combinations of datasets, including A1, A1+A2, A1+StyleGAN2-synthesized A1 (6 models), A1+LDMsynthesized A1, A1+StyleGAN2-synthesized A2 (6 models), and A1+LDM-synthesized A2. For further information on implementation details, hyperparameters and table values, please refer to our supplementary file and our publicized codes https://github.com/ayanglab/MedSynAnalyzer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Proposed Metrics Match with Human Perception</head><p>In our work, we proposed to use VQ-VAE to extract discrete features from original high-resolution X-ray images. To prove the validity of our method, we selected twenty images from each synthetic dataset and dataset A1 and invited two clinicians to rate the fidelity and variety manually. The human perceptual fidelity is rated from 0 to 1; and the human perceptual variety is computed by the percentage of different scans identified from the selected twenty synthetic images, i.e., if they thought all twenty patients were derived from the same scan, the human perceptual variety score is 1/20 = 0.05. To assure a fair comparison, we allow discussion between them. The result is shown in Fig. <ref type="figure" target="#fig_0">2 (c</ref>). The fidelity and variety score calculated with our method matched perfectly with human perception (p &lt; 0.05), and FID, which was highly influenced by mode collapse and increased over the diversity, failed to provide a valid analysis of image fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Trade-Off Between Fidelity and Variety</head><p>All of our experiments showed a strong negative correlation between variety and fidelity, with a Pearson's correlation coefficient of -0.92 (p &lt; 0.01). As it is widely known in GAN-based models, fidelity and variety are in conflict <ref type="bibr" target="#b5">[6]</ref>. In this study, we further validated this by introducing the LDM model and demonstrating empirically that deep generative models inevitably face the trade-off between variety and fidelity (as shown by the grey lines in Fig. <ref type="figure" target="#fig_2">3 (a-b</ref>)). In this study, we cannot find a significant correlation of utility between neither dimension (fidelity, variety nor privacy), indicating that there is currently no way to measure the utility except for large-scale experiments. However, we did observe a similar pattern of utility in our experiments shown in Fig. <ref type="figure" target="#fig_2">3 (a-b</ref>). First, the intra-task augmentation utility favors synthetic data with higher fidelity (i.e., high F ), even under mode collapses. For instance, when φ = 0.2 for StyleGAN, a mode collapse was observed. The model is producing images that look similar to each other, resulting in a mean image with a sharp contrast (Fig. <ref type="figure" target="#fig_3">4</ref> (a4)). However, this mode collapse seems to highlight the difference between the presence and non-presence of PE (Fig. <ref type="figure" target="#fig_3">4 (c</ref>)), leading to a performance improvement in intra-task augmentation. The PE in X-ray images is fluid in the lowest part of the chest, forming a concave line obscuring the costophrenic angle and part or all of the hemidiaphragm. These opacity differences were highlighted in the mode-collapsed images, which, on the other hand, improved the classification accuracy of PE identification.</p><p>For cross-task augmentation, synthetic data with a higher variety is favored for its utility as mode collapse can limit the focus of classification networks and lead to poor generalization performance on other tasks. For instance, a network trained to focus on lung opacity differences near the hemidiaphragm may not help in the accurate diagnosis of pediatric pneumonia, which is the motivation behind dataset D. As shown in Fig. <ref type="figure" target="#fig_2">3</ref> (b), synthetic data with low variety but high fidelity is unable to contribute to powerful feature extraction. Therefore, variety in synthetic data is crucial for effective cross-task augmentation.</p><p>As mentioned, we invited two radiologists to visually assess synthetic images. The visual inspection showed that all twenty LDM-synthesized images were easily recognized as fake due to their inability to capture the texture of X-ray images, as shown in the supplementary file. However, the shape and boundaries of the lungs were accurately captured by LDM. Despite their low visual fidelity, we demonstrated that these synthetic images still contribute to powerful feature extraction, which is crucial for cross-task utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">What Kind of Synthetic Data is Desired by Downstream Tasks</head><p>When Privacy is an Issue?</p><p>It is also discussed in the literature about the dilemma between utility and privacy <ref type="bibr" target="#b11">[12]</ref>. We reached a similar conclusion, i.e., there is a trade-off between utility and privacy in intra-class classification tasks, for which fidelity is considered to be crucial. Thus, a shift in the image domain could lead to a decrease in intra-task utility. However, as shown in Fig. <ref type="figure" target="#fig_3">4</ref> (d), we observed that privacy and utility are not always conflicting. As discussed earlier, for cross-task augmentation, the utility favors synthetic images with high variety rather than high fidelity. Therefore, we demonstrated that it is possible to achieve both privacy and utility in cross-task augmentation scenarios. Figure <ref type="figure" target="#fig_3">4 (d)</ref> shows an interesting positive correlation between privacy and cross-task utility. However, it is important to note that this does not imply a causal relationship between privacy and cross-task utility. Rather, the positive correlation is caused by the mode collapse during synthesis. Mode collapse can lead to a lack of diversity in generated data, which in turn can make it easier to identify individuals or sensitive information in the generated data, i.e., mode collapses are more likely to result in a high possibility of privacy breach as well as low cross-task utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a four-dimensional evaluation metric for synthetic images, including a novel privacy evaluation score and utility evaluation score. Through intensive experiments in over 100k chest X-ray images, we drew three major conclusions which we can envision that have broad applicability in medical image synthesis and analysis.</p><p>Firstly, there is an inevitable trade-off among different aspects of synthetic images, especially between fidelity and variety. Secondly, different downstream tasks require different properties of synthetic images, and synthetic images do not necessarily have to reach high metric scores across all aspects to be useful. Traditionally, low fidelity and mode collapses have been treated as disadvantages in data synthesis, and numerous algorithms have been proposed to fix these issues. However, our work demonstrates that these failures of synthetic data do not always sabotage their utility as expected. Lastly, we have showed that it is possible to achieve both privacy and utility in transfer learning problems.</p><p>In conclusion, our work contributes to the development of synthetic data as a valuable solution to enrich real-world datasets, to evaluate thoroughly medical image synthesis as a pathway to overall enhance medical image analysis tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustrative examples of entangled privacy and fidelity (a), our proposed algorithms to adjust the fidelity computation (b) and their validity measured by Pearson's correlation analysis (c). In (b), we selected the 5th-nearest neighbor to compute the fidelity, i.e., k = 5. The human experts were two radiologists with one and two years of experience, respectively.</figDesc><graphic coords="4,44,79,54,47,334,48,61,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fidelity</head><label></label><figDesc>and Privacy. Considering we have M real image points and N synthetic image points. For each real image {r i |i ∈ [0, M)}, we select its k nearest neighbors {r i,1 , r i,2 ,. . . , r i,k } according to the similarity D(r i , r j ) of these images in the image space. Then, as shown in Fig.2(b), we split the synthetic dataset into three sets according to these k nearest neighbors:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The relationships between fidelity, variety, and utility of adding additional synthetic data. The sizes of points in (a) and (b) are the improvement brought by adding synthetic data compared to no synthetic data being added. It should be noted that we visualized the point size in a power of 7 to better compare the improvements. Green points indicate significance using Signed Ranked T-test, and red indicates no significance. (Color figure online)</figDesc><graphic coords="7,58,98,54,02,334,48,83,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example synthetic images sampled from the StyleGAN2 model with a truncation φ = 0.2 trained on A2 with different sampling seeds (1-3), as well as the mean image of the two groups calculated on the entire synthetic dataset (4). All the images in (a-b) were normalized to [0,1], and the difference between row (a) and row (b) isshown in row (c). The presence of PE in X-ray images is characterized by increased opacity of the lung near the hemidiaphragm, which matches the group difference of mode-collapsed images. Therefore, even in the case of severe mode collapse during synthesis, these synthetic images can still contribute to the improvement of intra-task classification accuracy.</figDesc><graphic coords="8,44,79,53,87,334,48,142,84" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia? resource=download.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This study was supported in part by the <rs type="funder">ERC IMI</rs> (<rs type="grantNumber">101005122</rs>), the <rs type="grantNumber">H2020 (952172</rs>), the <rs type="funder">MRC</rs> (<rs type="grantNumber">MC/PC/21013</rs>), the <rs type="funder">Royal Society</rs> (<rs type="grantNumber">IEC\NSFC\211235</rs>), the <rs type="funder">NVIDIA Academic Hardware Grant Program</rs>, the <rs type="projectName">SABER</rs> project supported by <rs type="funder">Boehringer Ingelheim Ltd</rs>, <rs type="funder">Wellcome Leap Dynamic Resilience</rs>, and the <rs type="funder">UKRI</rs> <rs type="grantName">Future Leaders Fellowship</rs> (<rs type="grantNumber">MR/V023799/1</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6nPrMTP">
					<idno type="grant-number">101005122</idno>
				</org>
				<org type="funding" xml:id="_MFTAH8X">
					<idno type="grant-number">H2020 (952172</idno>
				</org>
				<org type="funding" xml:id="_HxztEYe">
					<idno type="grant-number">MC/PC/21013</idno>
				</org>
				<org type="funded-project" xml:id="_p8GCtA5">
					<idno type="grant-number">IEC\NSFC\211235</idno>
					<orgName type="project" subtype="full">SABER</orgName>
				</org>
				<org type="funding" xml:id="_PmqEEzf">
					<idno type="grant-number">MR/V023799/1</idno>
					<orgName type="grant-name">Future Leaders Fellowship</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gan-leaks: A taxonomy of membership inference attacks against generative models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3372297.3417238</idno>
		<ptr target="https://doi.org/10.1145/3372297.3417238" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2020 ACM SIGSAC Conference on Computer and Communications Security<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="343" to="362" />
		</imprint>
	</monogr>
	<note>CCS &apos;20</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are GANs created equal? a large-scale study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stylegan-xl: Scaling stylegan to large diverse datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2022 Conference Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Synthetic data-anonymisation groundhog day</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oprisanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Troncoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">st USENIX Security Symposium (USENIX Security 22)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1451" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
