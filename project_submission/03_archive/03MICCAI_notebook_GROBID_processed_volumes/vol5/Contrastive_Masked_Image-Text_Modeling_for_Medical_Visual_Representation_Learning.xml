<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning</title>
				<funder ref="#_uBwsafw">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Advanced Medical Computing and Analysis</orgName>
								<orgName type="department" key="dep2">Massachusetts General Hospital</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aoxiao</forename><surname>Zhong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dufan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Advanced Medical Computing and Analysis</orgName>
								<orgName type="department" key="dep2">Massachusetts General Hospital</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Advanced Medical Computing and Analysis</orgName>
								<orgName type="department" key="dep2">Massachusetts General Hospital</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
							<email>li.quanzheng@mgh.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Advanced Medical Computing and Analysis</orgName>
								<orgName type="department" key="dep2">Massachusetts General Hospital</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data Science Office</orgName>
								<orgName type="institution">Massachusetts General Brigham</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="493" to="503"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">A8C9179D173014C0D0F0AD57985405FF</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image-text representation learning</term>
					<term>masked autoencoding</term>
					<term>contrastive learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning (SSL) of visual representations from paired medical images and text reports has recently shown great promise for various downstream tasks. However, previous work has focused on investigating the effectiveness of two major SSL techniques separately, i.e., contrastive learning and masked autoencoding, without exploring their potential synergies. In this paper, we aim to integrate the strengths of these two techniques by proposing a contrastive masked image-text modeling framework for medical visual representation learning. On one hand, our framework conducts cross-modal contrastive learning between masked medical images and text reports, with a representation decoder being incorporated to recover the misaligned information in the masked images. On the other hand, to further leverage masked autoencoding, a masked image is also required to be able to reconstruct the original image itself and the masked information in the text reports. With pre-training on a large-scale medical image and report dataset, our framework shows complementary benefits of integrating the two SSL techniques on four downstream classification datasets. Extensive evaluations demonstrate consistent improvements of our method over state-of-the-art approaches, especially when very scarce labeled data are available. code is available at https://github.com/cchen-cc/CMITM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning models have demonstrated undoubted potential in achieving expert-level medical image interpretation when powered by large-scale labeled datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>. However, medical image annotations require expert knowledge thus are extremely costly and difficult to obtain at scale. Such an issue has even become a bottleneck for advancing deep learning models in medical applications. To tackle this issue, recent efforts have resorted to the text reports that are paired with the medical images, aiming to leverage the detailed text interpretation provided by radiologists to assist the representation learning of medical images without relying on any manual labels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. The learned image representations have proven to be generalizable to other downstream tasks of medical image analysis, which can significantly reduce the amount of labeled data required for fine-tuning. This topic has been actively studied and seen rapid progress with evaluation on chest X-ray datasets, because of both the clinical importance of radiograph screening and the availability of large-scale datasets of public chest X-ray images with paired radiology reports <ref type="bibr" target="#b13">[14]</ref>.</p><p>The current mainstream approaches for medical image-text pre-training are based on the popular self-supervised learning (SSL) technique known as contrastive learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>, which maximizes agreement between global and local representations of paired images and reports with a contrastive loss <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. These methods have demonstrated the effectiveness of using medical reports as a form of free supervision to enhance the learning of general image representations. Another well-demonstrated SSL method is masked autoencoding, which achieves representation learning via solving the pretext task of recovering masked image patches with unmasked ones <ref type="bibr" target="#b7">[8]</ref>. Until very recently, the potential of masked autoencoding had only begun to be explored for medical image-text pre-training. In a latest work, Zhou et al. <ref type="bibr" target="#b27">[28]</ref> propose to learn radiograph representations with a unified framework that requires the unmasked image patches to recover masked images and complete text reports. While both contrastive learning and masked autoencoding have demonstrated their ability to learn effective image representations, the two major SSL techniques have only been separately explored. The latest research has started to combine the two SSL techniques for joint benefits, but their focus is on the image domain rather than cross-modal learning in medical images and paired text reports <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref>. It remains an interesting and unexplored question whether contrastive learning and masked autoencoding can benefit each other and how to jointly exploit their strengths for medical image-text pre-training.</p><p>In fact, the learning principles of contrastive learning and masked autoencoding suggest that they could be complementary to each other. Contrastive imagetext learning explicitly discriminates the positive and negative pairs of images and text reports, making it good at promoting strong discriminative capabilities of image representations. Instead, masked autoencoding aims to reconstruct masked image/text tokens, which emphasizes learning local image structures, but may be less effective in capturing discriminative representations. This motivates us to propose a novel contrastive masked image-text modeling (CMITM) method for medical visual representation learning. Our framework is designed to accomplish three self-supervised learning tasks: First, aligning the representations of masked images with text reports. Second, reconstructing the masked images themselves. Third, reconstructing the masked text reports using the learned image representations. To reduce the information misalignment between the masked images and text reports, we incorporate a representation decoder to recover the missed information in images, which benefits the cross-modal learning. Moreover, the synergy of contrastive learning and masked autoencoding is unleashed via a cascaded training strategy. Our framework is pre-trained on a large-scale medical dataset MIMIC-CXR with paired chest X-ray images and reports, and is extensively validated on four downstream classification datasets with improved fine-tuning performance. Combining the two techniques yields consistent performance increase and the improvement of our method even surpasses the benefits of adding data from 1% to 100% labels on CheXpert dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates our contrastive masked image-text modeling (CMITM) framework. In this section, we first present how the cross-modal contrastive learning and masked autoencoding are realized in the framework respectively. Then we introduce the training procedures and implementation details of the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cross-Modal Contrastive Learning with Masked Images</head><p>Cross-modal contrastive learning has demonstrated to be an effective tool to align the representations of a medical image with that of its paired text report. In this way, the network is guided to interpret the image contents with the knowledge provided by medical reports. Different from previous methods, the cross-modal contrastive learning in our framework is between the representations of masked images and unmasked reports, aiming to integrate the benefits of both contrastive learning and masked image modeling.</p><p>Specifically, denote D = {x v,i , x t,i } N i=1 as the multi-modal dataset consisting of N pairs of medical images x v,i and medical reports x t,i . Each input image is split into 16 × 16 non-overlap patches and tokenized as image tokens a v,i , and each text report is also tokenized as text tokens a t,i . A random subset of image patches is masked out following masked autoencoder (MAE) <ref type="bibr" target="#b7">[8]</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the unmasked patches are forwarded to the image encoder E v , which embeds the inputs by a linear projection layer with added positional embeddings and then applies a series of transformer blocks to obtain token representations of unmasked patches q um v,i . Directly utilizing the representations of only unmasked patches to perform contrastive learning with the text could be less effective, since a large portion of image contents has been masked out and the information from the images and texts are misaligned. To recover the missing information in the images, we feed both the encoded visible patches q um v,i and trainable mask tokens q m v,i with added positional embeddings e p v,i to a representation decoder D r with two layers of transformer blocks. The representation decoder aims to output the representations of all image patches, i.e., qv,i = D r ([q um v,i , q m v,i ] + e p v,i ). Such a design helps to avoid that the contrastive learning is confused by the misaligned information between masked images and text reports. Finally we apply a global average pooling operation and a project layer h v to obtain the image embeddings z v,i , i.e., z v,i = h v (AvgPool(q v,i ). For text branch, we consider the full reports could give more meaningful guidance to image understanding than masked ones. So we forward the full text tokens without masking to the text encoder E t and the text project layer h t to obtain the global text embeddings z t,i , i.e., z t,i = h t (E t (a t,i )). To ensure that the embeddings of images are aligned with those of paired texts while remaining distant from unpaired texts, we employ cross-modal contrastive learning with the following symmetric InfoNCE loss <ref type="bibr" target="#b18">[19]</ref>.</p><formula xml:id="formula_0">Lc = - 1 B B i=1 log exp(s vt i,i /τ ) exp(s vt i,k /τ ) + log exp(s tv i,i /τ ) exp(s tv i,k /τ ) , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where s vt i,j = z T v,i z t,j , s tv i,j = z T t,i z v,j , τ denotes the temperature which is set to be 0.07 following common practice, and B is the number of image-report pairs in a batch. The cross-modal contrastive loss is used to supervise the network training associated with the data flow in orange line in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Masked Image-Text Modeling</head><p>The masked image-text modeling component in our framework consists of two parallel tasks, i.e., masked image reconstruction with image only information and masked text reconstruction with cross-modal information. We follow the design in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref> for the masked image-text modeling since our main focus is whether masked autoencoding and contrastive learning can have joint benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Image Reconstruction.</head><p>As aforementioned, the input images are masked and processed by the image encoder E v to obtain q um v,i . As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, besides the representation decoder to reconstruct image representations, we also connect both the encoded visible patches and learnable unmasked tokens with added positional embeddings to an image decoder D v to reconstruct the pixel values of masked patches, i.e., xv,i = D v ([q um v,i , q m v,i ] + e p v,i )). The image decoder consists of a series of transformer blocks and a linear projection layer to predict the values for each pixel in a patch. To enhance the learning of local details, the image decoder is required to reconstruct a high-resolution patch, which is twice the input resolution <ref type="bibr" target="#b27">[28]</ref>. The training of image reconstruction is supervised by a mean squared error loss L vr which computes the difference between the reconstructed and original pixel values for the masked patches:</p><formula xml:id="formula_2">Lvr = 1 B B i=1 (x m v,i -norm(x m v,i )) 2 |x m v,i | , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where xm v,i , xm v,i denote the predicted and the original high-resolution masked patches, | • | calculates the number of masked patches, and norm denotes the pixel normalization with the mean and standard deviation of all pixels in a patch suggested in MAE <ref type="bibr" target="#b7">[8]</ref>. The loss is only computed on the masked patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modal Masked Text Reconstruction.</head><p>To make the most of the text reports paired with imaging data for learning visual representations, the task of cross-modal masked text modeling aims to encourage the encoded visible image tokens q um v,i to participate in completing the masked text reports. Specifically, besides the full texts, we also forward a masked text report with a masking ratio of 50% to the text encoder E t . Following <ref type="bibr" target="#b27">[28]</ref>, this value is deliberately set to be higher than the masking ratio of 15% in BERT <ref type="bibr" target="#b4">[5]</ref> in order to enforce the image encoder to better understand the image contents by trying to reconstruct a large portion of masked texts. Then the global embedding of corresponding unmasked image patches AvgPool(q um v,i ) is added to the text token embeddings q um t,i to form a multi-modal embeddings. To reconstruct the masked text tokens, the multi-modal embeddings are processed by the text encoder E t and a text token classifier D t , i.e., ât,i = D t (E t (q um t,i + AvgPool(q um v,i ))). The training of text reconstruction is supervised by the cross entropy loss between the predictions and original text tokens as follows:</p><formula xml:id="formula_4">Ltr = 1 B B i=1 H(a m t,i , âm t,i ),<label>(3)</label></formula><p>where a m t,i , âm t,i denote the original and recovered masked text tokens respectively, H denotes the cross entropy loss. Similar to masked image reconstruction, the loss is also only computed on the masked text tokens. The image and text reconstruction losses are used to supervise the network training associated with the data flow in gray line in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Procedures and Implementation Details</head><p>Training a framework that combines cross-modal contrastive learning and masked autoencoding is non-trivial. As observed in prior work <ref type="bibr" target="#b12">[13]</ref>, forming the training as a parallel multi-task learning task can lead to decreased performance, which might be caused by the conflicting gradients of the contrastive and reconstruction losses. Similar phenomenon has also been observed in our experiments. We therefore adopt a cascaded training strategy, that is the framework is first trained with the reconstruction loss L r = L vr + L tr and is further trained with the contrastive loss L c . Such a training order is considered based on the insights that masked autoencoding focuses more on the lower layers with local details while contrastive learning is effective in learning semantic information for higher layers. Specifically, the first stage of pre-training follows <ref type="bibr" target="#b27">[28]</ref> to employ the loss L r for training 200 epochs. The model is trained using AdamW <ref type="bibr" target="#b15">[16]</ref> optimizer with a learning rate of 1.5e-4 and a weight decay of 0.05. In the second stage of pre-training, the image encoder, text encoder, and representation decoder are further trained with the loss L c for 50 epochs. Similarly a AdamW optimizer with a learning rate of 2e-5 and a weight decay of 0.05 is adopted. The framework is implemented on 4 pieces of Tesla V100 GPU with a batch size of 256. For network configurations, we use ViT-B/16 <ref type="bibr" target="#b5">[6]</ref> as the image encoder and BERT <ref type="bibr" target="#b4">[5]</ref> as the text encoder. The image decoder and representation decoder consists of four transformer blocks and two transformer blocks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To validate our framework, the image encoder of the pre-trained model is used to initialize a classification network with a ViT-B/16 backbone and a linear classification head. We adopt the fine-tuning strategy as used in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>, where both the encoder and classification head are fine-tuned. This fine-tuning setting reflects how the pre-trained weights can be applied in practical applications. For each dataset, the model is fine-tuned with 1%, 10%, and 100% labeled training data to extensively evaluate the data efficiency of different pre-trained models. The dataset split remains consistent across all approaches.</p><p>Pre-training Dataset. To pre-train our framework, we utilize MIMIC-CXR dataset <ref type="bibr" target="#b13">[14]</ref>, which is a large public chest X-ray collection. The dataset contains 377,110 images extracted from 227,835 radiographic studies and each radiograph is associated with one radiology report. For pre-training, images are resized and randomly cropped into the size of 448 × 448 and 224 × 224 as the high-resolution image reconstruction ground truth and low-resolution inputs respectively.</p><p>Fine-Tuning Datasets. We transfer the learned image representations to four datasets for chest X-ray classification. NIH ChestX-ray <ref type="bibr" target="#b23">[24]</ref> includes 112,120 chest X-ray images with 14 disease labels. Each chest radiograph can associate with multiple diseases. We follow <ref type="bibr" target="#b27">[28]</ref> to split the dataset into 70%/10%/20% for training, validation, and testing. CheXpert <ref type="bibr" target="#b11">[12]</ref> comprises 191,229 chest X-ray images for multi-label classification, i.e., atelectasis, cardiomegaly, consolidation, edema, and pleural effusion. We follow previous work to use the official validation set as the test images and randomly split 5,000 images from training data as the validation set. RSNA Pneumonia <ref type="bibr" target="#b20">[21]</ref> dataset contains 29,684 chest X-rays for a binary classification task of distinguishing between normal and pneumonia. Following <ref type="bibr" target="#b27">[28]</ref>, the dataset is split as training/validation/testing with 25,184/1,500/3,000 images respectively. COVIDx <ref type="bibr" target="#b22">[23]</ref> is a three-class classification dataset with 29,986 chest radiographs from 16,648 patients. The task either masked autoencoding or contrastive learning alone, showing the effectiveness of combining the two SSL techniques. It can be observed that our method shows the most obvious improvements in scenarios with limited data. When fine-tuning with 1% labeled data, our method outperforms the current bestperforming method MRM by 1.0% on NIH ChestX-ray dataset and 1.5% on COVIDx dataset. On CheXpert dataset, MRM model shows 0.2% performance gain when increasing labeled data from 1% to 100%, but with our method, fine-tuning on 1% labeled data already outperforms MRM model fine-tuned on 100% labeled data. These results may indicate that masked autoencoding and contrastive learning benefit each other more in data-scare scenarios.</p><p>Ablation Study. We perform ablation analysis on RSNA and COVIDx datasets with 1% labeled data to investigate the effect of each component in the proposed method. In Fig. <ref type="figure">2</ref>, we can see that removing either the masked image-text modeling (MITM) or cross-modal contrastive learning (CCL) in our method lead to a decrease in fine-tuning results. This again reflects the complementary role of the two SSL components. In Table <ref type="table" target="#tab_1">2</ref>, we show that the designs of cascaded training strategy, image masking in the contrastive learning, and representation decoder play important role to the performance of our method. Notably, results significantly decrease if not using cascaded training, that means directly using reconstruction loss and contrastive loss for joint training bring negative effects. These ablation results show that combing the two SSL approaches is non-trivial and requires careful designs to make it to be effective. For masking ratio, previous work <ref type="bibr" target="#b27">[28]</ref> has shown that the masking ratio of 75% works well in masked medical image-text reconstruction. So we directly adopt the masking ratio of 75% for the masked image-text modeling in the first pre-training stage, but we analyze how the performance changes with the masking ratio for the contrastive learning in the second pre-training stage. As shown in Fig. <ref type="figure">3</ref>, it is interesting to see that the optimal masking ratio is also 75%. This might indicate that the masking ratio should keep as the same during the cascaded training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present a novel framework for medical visual representation learning by integrating the strengths of both cross-modal contrastive learning and masked image-text modeling. With careful designs, the effectiveness of our method is demonstrated on four downstream classification datasets, consistently improving data efficiency under data-scarce scenarios. This shows the complementary benefits of the two SSL techniques in medical visual representation learning. One limitation of the work is that the pre-training model is evaluated solely on classification tasks. A compelling extension of this work would be to conduct further evaluation on a broader spectrum of downstream tasks, including organ segmentation, lesion detection, and image retrieval, thereby providing a more comprehensive evaluation of our model's capabilities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of proposed contrastive masked image-text modeling (CMITM) framework for medical visual representation learning. The masked image patches are required to align with the text reports, reconstructing original images, and reconstructing original text reports. The data flow in gray and orange line corresponds to first-and secondstage of pre-training respectively.</figDesc><graphic coords="3,60,48,53,87,331,75,150,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison with different pre-training methods on four chest X-ray datasets when fine-tuning with 1%, 10%, and 100% training data.</figDesc><table><row><cell>Fig. 2. Effects of two SSL components in</cell></row><row><cell>our method (MITM and CCL).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation on removing either design in our framework. Effect of masking ratio for cross-modal contrastive learning in our method.</figDesc><table><row><cell>Methods</cell><cell cols="2">RSNA COVIDx</cell></row><row><cell>CMITM (ours)</cell><cell cols="2">91.6 79.5</cell></row><row><cell>-cascaded training</cell><cell>90.8</cell><cell>76.2</cell></row><row><cell>-image masking</cell><cell>90.9</cell><cell>77.0</cell></row><row><cell cols="2">-representation decoder 91.2</cell><cell>79.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">NIH</rs> <rs type="grantNumber">R01HL159183</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uBwsafw">
					<idno type="grant-number">R01HL159183</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Comparison with State-of-the-Art Methods. We compare our method with four state-of-the-art SSL methods including two masked autoencodingbased methods and two contrastive learning-based methods. MAE (CVPR 2022) <ref type="bibr" target="#b7">[8]</ref> is the representative work on masked image autoencoding. MRM (ICLR 2023) <ref type="bibr" target="#b27">[28]</ref> is the latest work on medical image-text pre-training by using both the self-and report-completion objectives based on the masked record modeling. GLoRIA (ICCV 2021) <ref type="bibr" target="#b9">[10]</ref> and MGCA (NeurIPS 2022) <ref type="bibr" target="#b21">[22]</ref> are two cross-modal medical visual representation learning methods based on multi-scale image-text contrastive learning. For a fair comparison, the results of MRM model on the datasets NIH ChestX-ray, CheXpert, and RSNA are directly obtained from original paper since we use the same data split and fine-tuning strategy as theirs. The other comparison results are obtained by re-implementing corresponding methods with their released code with the same network backbone and fine-tuning strategy as ours. We also compare with models fine-tuned with random initialization and with weights pre-trained on ImageNet data, denoted as "Random init" and "ImageNet init" respectively. We use the area under the ROC curve (AUC) on NIH ChestX-ray, CheXpert, and RSNA datasets and accuracy (ACC) on COVIDx dataset as the evaluation metric following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Table <ref type="table">1</ref> shows the results on four downstream datasets for chest X-ray classification. We can see that, compared to "Random init" and "ImageNet init" models, pre-training on medical datasets significantly improve the fine-tuning performance on all the datasets. This shows the importance of medical visual representation learning. Compared to MAE model that only uses image data for pre-training, the other methods leveraging cross-modal image-text pre-training obtain higher performance, demonstrating the great benefits of detailed description in text reports. Our CMITM model generally outperforms methods that use</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Making the most of text semantics to improve biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20059-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20059-51" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022. ECCV 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13696</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-modal masked autoencoders for medical vision-and-language pre-training</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_65</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-965" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="679" to="689" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clinically applicable deep learning for diagnosis and referral in retinal disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1342" to="1350" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gloria: a multimodal global-local representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.13532</idno>
		<title level="m">Contrastive masked autoencoders are stronger vision learners</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CheXpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Layer grafted pre-training: bridging contrastive learning and masked image modeling for label-efficient representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.14138</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal representation learning via maximization of local mutual information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-326" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Essert</surname></persName>
		</editor>
		<meeting><address><addrLine>Strasbourg, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27 -October 1, 2021. 2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="273" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.16870</idno>
		<title level="m">A simple, efficient and scalable contrastive masked autoencoder for learning visual representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Radiological reports improve pretraining for localized imaging tasks on chest X-rays</title>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-962" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">CheXNet: radiologist-level pneumonia detection on chest Xrays with deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">180041</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-granularity crossmodal alignment for generalized medical visual representation learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vardhanabhuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="33536" to="33549" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Covid-Net: a tailored deep convolutional neural network design for detection of Covid-19 cases from chest X-ray images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ChestX-ray8: hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Medklip: medical knowledge enhanced language-image pre</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2023" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Advancing radiograph representation learning with masked record modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023 (2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
