<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Paula</forename><surname>López Diez</surname></persName>
							<idno type="ORCID">0000-0002-9346-743X</idno>
							<affiliation key="aff0">
								<orgName type="department">DTU Compute</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<settlement>Kongens Lyngby</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><surname>Margeta</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Oticon Medical, Research and Technology</orgName>
								<address>
									<settlement>Vallauris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Nova Dubnica</orgName>
								<orgName type="institution">KardioMe, Research and Development</orgName>
								<address>
									<country key="SK">Slovakia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Khassan</forename><surname>Diab</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Tashkent International Clinic</orgName>
								<address>
									<settlement>Tashkent</settlement>
									<country key="UZ">Uzbekistan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">François</forename><surname>Patou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Oticon Medical, Research and Technology</orgName>
								<address>
									<settlement>Smørum</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rasmus</forename><forename type="middle">R</forename><surname>Paulsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DTU Compute</orgName>
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<settlement>Kongens Lyngby</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="652" to="662"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1A096E32B58DE2E5C6166A7861E78729</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_63</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised</term>
					<term>Classification</term>
					<term>DeepDiffusion</term>
					<term>Inner Ear</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The identification of congenital inner ear malformations is a challenging task even for experienced clinicians. In this study, we present the first automated method for classifying congenital inner ear malformations. We generate 3D meshes of the cochlear structure in 364 normative and 107 abnormal anatomies using a segmentation model trained exclusively with normative anatomies. Given the sparsity and natural unbalance of such datasets, we use an unsupervised method for learning a feature representation of the 3D meshes using DeepDiffusion. In this approach, we use the PointNet architecture for the network-based unsupervised feature learning and combine it with the diffusion distance on a feature manifold. This unsupervised approach captures the variability of the different cochlear shapes and generates clusters in the latent space which faithfully represent the variability observed in the data. We report a mean average precision of 0.77 over the seven main pathological subgroups diagnosed by an ENT (Ear, Nose, and Throat) surgeon specialized in congenital inner ear malformations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inner ear malformations are found in 20-30% of children with congenital hearing loss <ref type="bibr" target="#b0">[1]</ref>. While the prevalence of bilateral congenital hearing loss is estimated to be 1.33 per 1000 live births in North America and Europe, it is much higher in sub-Saharan Africa (19 per 1,000 newborns) and South Asia (up to 24 per 1,000) <ref type="bibr" target="#b7">[8]</ref>. Early detection of sensorineural hearing loss is crucial for appropriate intervention, such as cochlear implant therapy, which is prescribed to approximately 80,000 infants and toddlers annually worldwide <ref type="bibr" target="#b15">[16]</ref>. Radiological examination is essential for an early diagnosis of congenital inner ear malformation, particularly when cochlear implant therapy is planned. However, detecting and classifying such malformations from standard imaging modalities is a complex task even for expert clinicians, and presents challenges during CI surgery <ref type="bibr" target="#b1">[2]</ref>. Previous studies have proposed methods to classify congenital inner ear malformations based on explicit measurements and visual analysis of CT scans <ref type="bibr" target="#b4">[5]</ref>. These methods are time-consuming and subject to clinician subjectivity. A suggested approach for the automated detection of inner ear malformation has relied on deep reinforcement learning trained for landmark location in normal anatomies based on an anomaly detection technique <ref type="bibr" target="#b8">[9]</ref>. However, this method is only limited to the detection of a malformation but does not attempt to classify them.</p><p>Currently, supervised deep metric learning garners significant interest due to its exceptional efficacy in data clustering and pathology classification. Most of these approaches are fully supervised and use supervisory signals that model the training by creating tuples of labeled training data. These tuples are then used to optimize the intra-class distance of the different samples in the latent space, as has been done mostly for 2D images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> and 2D representation of 3D images <ref type="bibr" target="#b3">[4]</ref>. Several recent studies have demonstrated promising outcomes from unsupervised contrastive learning from natural images. However, their utility in the medical image domain is limited due to the high degree of inter-class similarity. Particularly in heterogeneous real clinical datasets in which the image quality and appearance can significantly impact the performance of such methods, rendering them less effective. In <ref type="bibr" target="#b21">[22]</ref> an unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text in 2D images is proposed. Typically, in 3D images, an unsupervised lowdimensional representation is utilized for further clustering, as demonstrated in <ref type="bibr" target="#b13">[14]</ref>. Nonetheless, such approaches are commonly developed using quite homogeneous datasets that are not representative of real-world applications and the diverse clinical settings in which they must operate.</p><p>Our objective is to develop a fully automated pipeline for the classification of inner ear malformations, utilizing a relatively large and unique dataset of such anomalies. The pipeline's design necessitates a profound comprehension of this data type and the congenital malformations themselves. Given the CT scans in this region are complex, and the images originate from diverse sources, we employ an unsupervised approach, uniquely based on the 3D shape of the cochlear structure. We have observed that the cochlear structure can be roughly but consistently segmented by a 3D-UNET model trained exclusively on normal cochlear anatomies. We then use these segmentations and adopt an entirely unsupervised approach, meaning the deep learning model is trained from scratch on these segmentations, and the class labels are not used for training. To map these shapes to an optimal latent space representation, we utilize DeepDiffusion, which combines the diffusion distance on a feature manifold with the feature learning of the encoder.</p><p>In this paper, we present the first automatic approach for the classification of congenital inner ear malformations. We use an unsupervised method to find the latent space representation of cochlear shapes, which allows for their further classification. We demonstrate that shapes from a segmentation model trained on normative cases, albeit imperfect, can be used to represent abnormalities. Moreover, our results indicate the potential for successfully applying this approach to other anatomies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>Our dataset comprises a total of 485 clinical CT scans, consisting of 364 normal scans and 121 scans with various types of inner ear malformations. The distribution of inner ear scans for each type of malformation is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We utilized the region-of-interest (ROI) extraction technique developed by <ref type="bibr" target="#b17">[18]</ref>, which involves selecting anatomical points of interest that are not part of the inner ear region to achieve a standardized and robust image orientation. To ensure consistency, all images were resampled to a spacing of 0.125 mm, and their intensities were normalized by scaling the 5 th and 95 th percentiles of the intensity distribution of each image to 0 and 1, respectively. Figure <ref type="figure" target="#fig_0">1</ref> also shows the data split used for training our model. We chose to use an approximate 50% split for abnormal cases, while the vast majority of normal cases, approximately 86%, were used for training. Other configurations were explored, including using only normal cases for training. However, it was demonstrated that while this approach may work for anomaly detection, it does not adequately categorize the different types of malformations. Our aim is to find a parametrized shape that is representative of the anatomy of the patient. We decided to focus on the cochlear structure as it is the main structure of interest when trying to identify a malformation in the inner ear. To obtain a 3D segmentation of this structure we use the 3D-UNET <ref type="bibr" target="#b18">[19]</ref> presented in <ref type="bibr" target="#b9">[10]</ref> which has been trained exclusively in normal anatomies (130 images from diverse imaging equipment) and built using MONAI <ref type="bibr" target="#b11">[12]</ref>. Even though no abnormal anatomies have been used for training, given the high contrast between the soft tissue of the cochlear structure and the bony structure that surrounds it, the model still performs quite well to segment the abnormal cases. This can be seen in Fig. <ref type="figure" target="#fig_1">2</ref> where an example of each of the types of malformations used in this study and an anatomically normal case are shown. The largest connected component of the segmentation has been selected to generate the final 3D meshes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Anatomical Representation</head><p>An overview of our pipeline is presented in Fig. <ref type="figure" target="#fig_2">3</ref>. Each 3D mesh obtained from a CT image is transformed into a 1024 point cloud using the Ohbuchi method <ref type="bibr" target="#b12">[13]</ref>. Each shape is then normalized by centering its origin in its center of gravity and enclosing the shape within a unit sphere, resulting in the point cloud representation of the shape S. Before the shape S is fed to the encoder, the shape is augmented into shape Ŝ with a probability of 0.8. This augmentation consists of a random rotation with U(-5 • ,5 • ), an anisotropic scaling sampled from U(0.8,1), and a shearing and translation in each axes sampled from U(-0.2,0.2) for both actions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Diffusion Algorithm</head><p>The DeepDiffusion (DD) algorithm <ref type="bibr" target="#b6">[7]</ref> incorporates the manifold ranking <ref type="bibr" target="#b22">[23]</ref> technique, which uses similarity diffusion on the manifold graph to learn a distance metric among the samples. The DD algorithm optimizes both the feature extraction and the embeddings produced by the encoder, which results in salient features in a continuous and smooth latent space. In this latent space, the Euclidean distance among the latent features approximates the diffusion distance on the latent feature manifold. The crux behind this algorithm is the latent manifold ranking loss (LMR) which is computed using both intrinsic and extrinsic features. The LMR consists of a fitting term, L fit , a smoothing term, L smooth , and a balancing term, λ.</p><formula xml:id="formula_0">LMR = arg min M,θ L fit ± λL smooth<label>(1)</label></formula><p>Where θ characterizes the encoder and M ∈ R N X P represents the latent feature manifold formed by the training samples, where N is the number of data samples and P is the output dimensions of the encoder. The extrinsic feature f is defined as the output of the encoder and has dimension P . M is initialized by stacking together the embeddings of the first forward pass through the encoder which has been randomly initialized as this has been shown to perform better than randomly initializing the weights of M itself as shown in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Every training sample has its unique identification number (ID b ) which is used to specify a diffusion source y b that is consistent throughout the training procedure. L fit constrains the ranking vector r b to being close to the diffusion source y b , which is defined as the vector containing one-hot encoding of ID b . The ranking vector is defined as r b = softmax(f b M T ) and represents the probabilistic similarities between the feature f b and all the intrinsic features contained in M . The fitting term is therefore defined as</p><formula xml:id="formula_1">L fit = b CrossEntropy(r b , y b )</formula><p>its minimization results in all the extrinsic features being embedded farther away from each other as they are being pulled toward their respective and unique diffusion source vectors. The smoothing term is defined as</p><formula xml:id="formula_2">L smooth = b n w bn Dissimilarity(r b , t n ) (2)</formula><p>where the dissimilarity operator is the Jensen-Shannon divergence <ref type="bibr" target="#b5">[6]</ref> and t n = softmax(m n M T ) being m n the n th row of the matrix M so that t n contains the ranking score of the intrinsic feature m n to all the intrinsic features. w bn indicates the similarity between the extrinsic feature f b and the neighboring intrinsic feature m n and it is defined as:</p><formula xml:id="formula_3">w bn = f b m T n , m n ∈ kNN(f b ) 0 otherwise (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Minimizing L smooth pulls extrinsic features and their neighboring intrinsic features together which implies that an extrinsic feature is more likely to be projected onto the surface of the latent feature manifold of the intrinsic features when L smooth is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>For our encoder, we use the PointNet <ref type="bibr" target="#b2">[3]</ref> architecture which takes 1024 3D points as input, applies input and feature transformations, and then aggregates point features by max pooling to a feature of dimensionality 1024 which is then compressed into dimensionality 254 with two sets of fully connected layers. The network has been trained by using mini-batch (of size 8) gradient descent using the Adam optimizer with a learning rate of 10 -8 and ReLU as the activation function. The DD algorithm is implemented in PyTorch <ref type="bibr" target="#b16">[17]</ref> and the code used for this study is available at https://github.com/paulalopez10/ Deep-Diffusion-Unsupervised-Classification-3D-Mesh. The models are trained on an NVIDIA GeForce RTX 3070 Laptop GPU with 8GB VRAM. The different hyper-parameters related to the approach have been explored and it has been empirically found for this specific configuration λ = 0.6 and k = 10 produce the best results that will be analyzed in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We evaluate the classification performance of our pipeline by analyzing the embeddings generated by the trained encoder. To visualize the projection of the features of the test in 2D we use the U-MAP <ref type="bibr" target="#b10">[11]</ref>, as illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>. The U-MAP visualization demonstrates the clustering of different classes in the latent space. Furthermore, it is very interesting to notice how the latent space representation displays the anatomical changes of the anatomy where the more extreme types of malformations (CA and CC) are the most distant to the normative cochlear structures. The transition between the different classes shown in the latent space properly represents the pathological variations in this anatomy.</p><p>We have also included, in Fig. <ref type="figure" target="#fig_3">4</ref>, the projection of the features projected in the 2D-PCA space defined by the training set, where both, training and testing, sets are included to show not only the clustering in this space but also the similar distribution of the different classes in both sets within the PCA projection. For a further analysis of the performance, we compute some evaluation metrics based on the pairwise cosine distance between samples that can be seen in Fig. <ref type="figure" target="#fig_4">5 c</ref>). The average ROC and precision-recall curves for each of the classes can be seen in Fig. <ref type="figure" target="#fig_4">5 a</ref>) and b). To calculate those, each test feature vector f b is considered to be the centroid of a kNN(f b ) which consists in the k nearest  features from other samples using the cosine pairwise distance shown in Fig. <ref type="figure" target="#fig_4">5  c</ref>). We vary k until all the features from the corresponding class are within the cluster and compute the precision and false positive rate per the different recall steps, the shown results are the average among each class and overall. With the same procedure, different evaluation metrics have been obtained and are shown in Table <ref type="table" target="#tab_0">1</ref>. These metrics encompass the area under the curve (AUC) for the curves shown in Fig. <ref type="figure" target="#fig_4">5 a</ref>) and b), both for the average curve and the optimal curve for each class. Furthermore, the maximum and average accuracy has been computed together with the maximum f1-score. Considering the dataset's significant class imbalance, these metrics provide a comprehensive assessment of the performance achieved. Finally, the mean average precision is also included in the table together with the optimal one for each class. The optimal or maximum value of each metric corresponds to when the optimal sample within our test features distribution is being evaluated as the centroid of its own class and the mean values are the average over all the samples. We can observe how a bigger variance is obtained for the classes that contain a few examples as it is expected, given the nature and distribution of our dataset shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented the first approach for the automatic classification of congenital inner ear malformations. We show how using the 3D shape information of the cochlea obtained with a model only trained in normative anatomies is enough to classify the malformations and reduces the influence of the image's source, which is crucial in a clinical application setting.</p><p>Our method shows a mean average precision of 0.77 with a mean ROC-AUC of 0.91, indicating its effectiveness in classifying inner ear malformations. Furthermore, the representation of the different cases in the latent space shows spatial relation between classes, which is correlated with the anatomical appearance of the different malformations. These promising results pave the way towards assisting clinicians in the challenging assessment of congenital inner ear malformations potentially leading to improved patient outcome of cochlear surgery.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Distribution of cases among the different classes and the split used for our approach. Cochlear aplasia (CA), common cavity (CC) incomplete partitioning type I, II, and III (IP-I, IP-II, IP-III), cochlear hypoplasia (CH), and normal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Representative 3D-UNET segmentation meshes from each type of cochlear anatomy used in this study. Top row shows the 3D mesh with the original CT scan image; the bottom row shows exclusively the 3D mesh.</figDesc><graphic coords="4,56,46,302,12,339,43,153,61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Sketch of the DeepDiffusion used for latent space representation of the cochlear 3D meshes. The pointcloud extracted from the mesh is fed to the PointNet encoder which generates the corresponding latent feature which is optimized by minimizing the LMR loss so the encoder and the latent feature manifold are optimized for the comparison of data samples.</figDesc><graphic coords="5,81,87,54,32,203,38,90,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Top: U-MAP representation of the test features where it can be observed how the different classes group together and how the anatomical variation is represented as there is a progression from the most abnormal cases towards fully normal cases. Bottom: Test and train features projected into the 2D PCA space defined by the training samples, where the classes are separated and show consistency between training and testing samples.</figDesc><graphic coords="7,57,33,124,82,319,06,311,65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Evaluation plots. a)Mean ROC curves for each class b)Mean Recall-Precision curve for each class c)Pairwise cosine distance between test embeddings used to evaluate the performance.</figDesc><graphic coords="8,123,24,65,39,265,84,93,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation metrics reported in our experiment. ROC-Receiver operating characteristic, AUC-Area under the curve, AP-Average precision, PR -Precision-recall</figDesc><table><row><cell></cell><cell>CA</cell><cell>CC</cell><cell>IP-I</cell><cell cols="3">IP-II IP-III CH</cell><cell cols="2">NORMAL Overall</cell></row><row><cell>Max Accuracy</cell><cell>0.98</cell><cell>0.96</cell><cell>0.92</cell><cell>0.96</cell><cell>0.98</cell><cell>0.99</cell><cell>0.91</cell><cell>0.93</cell></row><row><cell>Mean Accuracy</cell><cell>0.73</cell><cell>0.77</cell><cell>0.87</cell><cell>0.77</cell><cell>0.96</cell><cell>0.69</cell><cell>0.75</cell><cell>0.78</cell></row><row><cell></cell><cell>±0.25</cell><cell>± 0.26</cell><cell>± 0.03</cell><cell>± 0.08</cell><cell>± 0.03</cell><cell>± 0.01</cell><cell>± 0.05</cell><cell>± 0.12</cell></row><row><cell cols="2">Max ROC-AUC 0.99</cell><cell>0.99</cell><cell>0.98</cell><cell>0.96</cell><cell>0.99</cell><cell>0.98</cell><cell>0.99</cell><cell>0.99</cell></row><row><cell cols="2">Mean ROC-AUC 0.75</cell><cell>0.79</cell><cell>0.94</cell><cell>0.84</cell><cell>0.98</cell><cell>0.71</cell><cell>0.95</cell><cell>0.91</cell></row><row><cell></cell><cell>± 0.25</cell><cell>± 0.25</cell><cell>± 0.04</cell><cell>± 0.10</cell><cell>± 0.02</cell><cell>± 0.08</cell><cell>± 0.09</cell><cell>± 0.13</cell></row><row><cell>Max AP</cell><cell>0.57</cell><cell>0.70</cell><cell>0.87</cell><cell>0.88</cell><cell>0.92</cell><cell>0.51</cell><cell>0.99</cell><cell>0.91</cell></row><row><cell>Mean AP</cell><cell>0.41</cell><cell>0.38</cell><cell>0.70</cell><cell>0.65</cell><cell>0.82</cell><cell>0.50</cell><cell>0.94</cell><cell>0.77</cell></row><row><cell></cell><cell>± 0.42</cell><cell>± 0.33</cell><cell>± 0.23</cell><cell>± 0.32</cell><cell>± 0.25</cell><cell>± 0.42</cell><cell>± 0.12</cell><cell>± 0.29</cell></row><row><cell>Max f1-score</cell><cell>0.67</cell><cell>0.57</cell><cell>0.67</cell><cell>0.88</cell><cell>0.86</cell><cell>0.67</cell><cell>0.91</cell><cell>0.75</cell></row><row><cell>Max PR-AUC</cell><cell>0.63</cell><cell>0.80</cell><cell>0.84</cell><cell>0.85</cell><cell>0.91</cell><cell>0.71</cell><cell>0.95</cell><cell>0.88</cell></row><row><cell>Mean PR-AUC</cell><cell>0.40</cell><cell>0.37</cell><cell>0.67</cell><cell>0.62</cell><cell>0.78</cell><cell>0.48</cell><cell>0.90</cell><cell>0.74</cell></row><row><cell></cell><cell>± 0.26</cell><cell>± 0.25</cell><cell>± 0.04</cell><cell>± 0.11</cell><cell>± 0.02</cell><cell>± 0.08</cell><cell>± 0.10</cell><cell>± 0.24</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Genetics of inner ear malformations: a review</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brotto</surname></persName>
		</author>
		<idno type="DOI">10.3390/audiolres11040047</idno>
		<ptr target="https://doi.org/10.3390/audiolres11040047" />
	</analytic>
	<monogr>
		<title level="j">Audiol. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="524" to="536" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Further evidence of the relationship between cochlear implant electrode positioning and hearing outcomes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakravorti</surname></persName>
		</author>
		<idno type="DOI">10.1097/MAO.0000000000002204</idno>
		<ptr target="https://doi.org/10.1097/MAO.0000000000002204" />
	</analytic>
	<monogr>
		<title level="j">Otol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="617" to="624" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaichun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.16</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.16" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A dual-transformation with contrastive learning framework for lymph node metastasis prediction in pancreatic cancer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2023.102753</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1361841523000142" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102753</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel three-step process for the identification of inner ear malformation types</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Dhanasingh</surname></persName>
		</author>
		<idno type="DOI">10.1002/lio2.936</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/10.1002/lio2.936" />
	</analytic>
	<monogr>
		<title level="j">Laryngoscope Investigative Otolaryngology</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jensen-shannon divergence and hilbert space embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fuglede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Topsoe</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISIT.2004.1365067</idno>
		<ptr target="https://doi.org/10.1109/ISIT.2004.1365067" />
	</analytic>
	<monogr>
		<title level="m">International Symposium onInformation Theory</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="31" to="2004" />
		</imprint>
	</monogr>
	<note>ISIT 2004. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deepdiffusion: unsupervised learning of retrieval-adapted representations via diffusion-based ranking on latent feature manifold</title>
		<author>
			<persName><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2022.3218909</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2022.3218909" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="116287" to="116301" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Congenital hearing loss</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Korver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Rev. Disease Primers</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for detection of inner ear abnormal anatomy in computed tomography</title>
		<author>
			<persName><forename type="first">P</forename><surname>López Diez</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_67" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A web-based automated image processing research platform for cochlear implantation-related studies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Margeta</surname></persName>
		</author>
		<idno type="DOI">10.3390/jcm11226640</idno>
		<ptr target="https://www.mdpi.com/2077-0383/11/22/6640" />
	</analytic>
	<monogr>
		<title level="j">J. Clin. Med</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">22</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Umap: Uniform manifold approximation and projection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Großberger</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.00861</idno>
		<ptr target="https://doi.org/10.21105/joss.00861" />
	</analytic>
	<monogr>
		<title level="j">J. Open Source Softw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><surname>Monai-Consortium</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7459814</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7459814" />
		<title level="m">Monai: Medical open network for AI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shape-similarity search of 3D models by using enhanced shape functions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minamitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Appl. Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="70" to="85" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient feature embedding of 3d brain mri images for content-based image retrieval with deep metric learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Onga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fujiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iyatomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3764" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep metric learning for cervical image classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53266" to="53275" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Infant hearing loss: from diagnosis to therapy official report of xxi conference of Italian society of pediatric otorhinolaryngology</title>
		<author>
			<persName><forename type="first">G</forename><surname>Paludetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Otorhinolaryngol. Italica</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">347</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate localization of inner ear regions of interests using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Radutoiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Patou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Margeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Paulsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>López Diez</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-21014-3_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-21014-3_43" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="416" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep metric learning for otitis media classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Sundgaard</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2021.102034</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1361841521000803" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102034</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Triplet attention and dual-pool contrastive learning for clinic-driven multi-label medical image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2023.102772</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1361841523000336" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">102772</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v182/zhang22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Machine Learning for Healthcare Conference. Proceedings of Machine Learning Research</title>
		<meeting>the 7th Machine Learning for Healthcare Conference. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="2" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ranking on data manifolds</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2003/file/2" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>c3ddf4bf13852db711dd1901fb517fa-Paper.pdf</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
