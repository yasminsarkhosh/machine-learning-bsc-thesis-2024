<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinyi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado Anschutz Medical Campus</orgName>
								<address>
									<settlement>Aurora</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bennett</forename><surname>Chin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado Anschutz Medical Campus</orgName>
								<address>
									<settlement>Aurora</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Silosky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado Anschutz Medical Campus</orgName>
								<address>
									<settlement>Aurora</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Litwiller</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">GE Healthcare</orgName>
								<address>
									<settlement>Denver</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Debashis</forename><surname>Ghosh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado Anschutz Medical Campus</orgName>
								<address>
									<settlement>Aurora</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Fuyong</forename><surname>Xing</surname></persName>
							<email>fuyong.xing@cuanschutz.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado Anschutz Medical Campus</orgName>
								<address>
									<settlement>Aurora</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="116" to="126"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">393EE3B6D1217A8FC384F395C14E73D1</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-05-13T19:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lesion detection</term>
					<term>PET images</term>
					<term>domain generalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have recently achieved impressive performance of automated tumor/lesion quantification with positron emission tomography (PET) imaging. However, deep learning usually requires a large amount of diverse training data, which is difficult for some applications such as neuroendocrine tumor (NET) image quantification, because of low incidence of the disease and expensive annotation of PET data. In addition, current deep lesion detection models often suffer from performance degradation when applied to PET images acquired with different scanners or protocols. In this paper, we propose a novel single-source domain generalization method, which learns with human annotation-free, list mode-synthesized PET images, for hepatic lesion identification in real-world clinical PET data. We first design a specific data augmentation module to generate out-of-domain images from the synthesized data, and incorporate it into a deep neural network for cross domain-consistent feature encoding. Then, we introduce a novel patch-based gradient reversal mechanism and explicitly encourage the network to learn domain-invariant features. We evaluate the proposed method on multiple cross-scanner 68 Ga-DOTATATE PET liver NET image datasets. The experiments show that our method significantly improves lesion detection performance compared with the baseline and outperforms recent state-of-the-art domain generalization approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have recently shown impressive performance on lesion quantification in positron emission tomography (PET) images <ref type="bibr" target="#b5">[6]</ref>; however, they usually rely on a large amount of well-annotated, diverse data for model training. This is difficult or even infeasible for some applications such as lesion identification in neuroendocrine tumor (NET) images, because NETs are rare tumors and lesion annotation in low-resolution, noisy PET images is expensive. To address the data shortage issue, we propose to train a deep model for lesion detection with synthesized PET images generated from list mode PET data, which is low-cost and does not require human effort for manual data annotation.</p><p>Synthesized PET images may exhibit a different data distribution from real clinical images (see Fig. <ref type="figure" target="#fig_0">1</ref>), i.e., a domain shift, which can pose significant challenges to model generalization. To address domain shifts, domain adaptation requires access to target data for model training <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>, while domain generalization (DG) trains a model with only source data <ref type="bibr" target="#b39">[39]</ref> and has recently attracted increasing attention in medical imaging <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref>. Most of current DG methods rely on multiple sources of data to learn a generalizable model, i.e., multisource DG (MDG); however, multi-source data collection is often difficult in real practice due to privacy concerns or budget deficits. Although single-source DG (SDG) using only one source dataset has been applied to medical images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>, very few studies focus on SDG with PET imaging and the current SDG methods may not be suitable for lesion identification on PET data. For instance, many existing methods use a complicated, multi-stage model design pipeline <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>, which introduces an additional layer of algorithm variability. This situation will become worse for PET images, which typically have a poor signal-to-noise ratio and low spatial resolution. Several other SDG approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">34]</ref> leverage unique characteristics of the imaging modalities, e.g., color spectrum of histological stained images, which are not applicable to PET data.</p><p>In this paper, we propose a novel single-stage SDG framework, which learns with human annotation-free, list mode-synthesized PET images for generalizable lesion detection in real clinical data. Compared with domain adaptation and MDG, the proposed method, while more challenging, is quite practical for real applications due to the relatively cheaper NET data collection and annotation. Specifically, we design a new data augmentation module, which generates out-of-domain samples from single-source data with multi-scale random convolutions. We integrate this module into a deep lesion detection neural network and introduce a cross-domain consistency constraint for feature encoding between original synthesized and augmented images. Furthermore, we incorporate a novel patch-based gradient reversal mechanism into the network and accomplish a pretext task of domain classification, which explicitly promotes domain-invariant, generalizable representation learning. Trained with a single-source synthesized dataset, the proposed method provides superior performance of hepatic lesion detection in multiple cross-scanner real clinical PET image datasets, compared with the reference baseline and recent state-of-the-art SDG methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Figure <ref type="figure" target="#fig_1">2</ref> presents the proposed SDG framework. Given a source-domain dataset of list mode-synthesized 3D PET images and corresponding lesion labels (X S , Y S ), the goal of the framework is to learn a lesion detection model H , composed of E and D, which generalizes to real clinical PET image data. The framework first feeds synthesized images X S into a random-convolution data augmentation module A and generates out-of-domain samples X A = A(X S ). Then, it provides both original and augmented images, X S and X A , to a feature encoder E , which is followed by a decoder D for lesion detection. The framework imposes a crossdomain consistency constraint on the encoder E to promote consistent feature learning between X S and X A . Meanwhile, it uses a patch gradient reversalbased domain classifier C to differentiate X A from X S and further encourages the encoder E to learn domain-agnostic representations for H .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Synthesized Data Augmentation</head><p>In the synthesized PET image dataset, each subject have multiple simulated lesions of varying size with known boundaries <ref type="bibr" target="#b10">[11]</ref>, and thus no human annotation is required. However, this synthesized dataset presents a significant domain shift from real clinical data, as they have markedly different image textures and voxel intensity values (see Fig. <ref type="figure" target="#fig_0">1</ref>). Inspired by previous domain generalization work <ref type="bibr" target="#b39">[39]</ref>, we introduce a specific data augmentation module to generate out-of-domain samples from this single-source synthesized dataset for generalizable model learning (see Fig. <ref type="figure" target="#fig_1">2</ref>). Specifically, we tailor a random convolution technique <ref type="bibr" target="#b33">[33]</ref> for synthesized PET image augmentation with the following substantial improvement: 1) extend it from single value-prediction image classification to a more challenging dense prediction task of lesion detection; 2) refine it to produce realistic augmented images where the organ regions are brighter than image background, instead of randomly switching the foreground and background intensity values; 3) place a cross-domain consistency constraint on the encoding features, rather than output predictions, of original synthesized and augmented images, so as to directly encourage consistent representation learning between the source and other domains. This module can preserve global shapes or the structure of objects (e.g., lesions and livers) in images but distorts local textures, so that the lesion detection model learned with these augmented images can generalize to unseen real-world PET image data, which typically have high lesion heterogeneity and divergent texture styles.</p><p>Given a synthesized input image x S ∈ X S , our data augmentation module A first performs a random convolution operation R(x S ) with a k × k kernel R, where the kernel size k and the convolutional weights are randomly sampled from a multi-scale set K = {1, 3, 5, 7} and a normal distribution N (0, 1/k 2 ), respectively. Then, inspired by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>, we mix R(x S ) and x S to generate a new mixed image x M via a convex combination,</p><formula xml:id="formula_0">x M = αx S + (1 -α)R(x S ),</formula><p>where α ∈ [0, 1] is randomly sampled from a uniform distribution U(0, 1). This data mixing strategy allows continuous interpolation between the source domain and a randomly generated out-of-distribution domain to improve model generalizability. Finally, if the foreground (i.e., lesion region) of x M has a higher mean intensity value than the background (non-lesion region), we use x M as the final augmented image, x A = x M . Otherwise, we invert the image intensity of x M to obtain</p><formula xml:id="formula_1">x A = x max M 1 + x min M 1 -x M , where x max M /x min M</formula><p>is the maximum/minimum intensity of x M and 1 is a matrix with all elements being one and the same dimension as x M . This intensity inversion operation is to ensure the lesion region has higher intensity values than other regions, mimicking the image characteristics of real-world PET data in our experiments. Here we calculate the mean intensity value of the background from the regions that have a distance greater than half of the image width from the closest lesion center.</p><p>In our modeling, for each synthesized training image x S , we generate multiple augmented images (i.e., 3), {x i A } 3 i=1 , and feed them into the encoder E for feature learning. Due to the distance preservation property of random convolutions <ref type="bibr" target="#b33">[33]</ref>, the module A changes local textures but preserves object shapes at different scales, and thus x S and {x i A } 3 i=1 should have identical semantic content, such as lesion presence, quantity and positions. Therefore, they should have consistent representations in the feature space, i.e., E (x S ) ≈ E (x i A ), i = 1, 2, 3. To this end, we place a cross-domain consistency loss L con on top of the encoder E as</p><formula xml:id="formula_2">L con = 1 3 3 i=1 L i con ,<label>(1)</label></formula><formula xml:id="formula_3">L i con = E x S ∼X S [ 1 |E (x S )| ||E (x S ) -E (x i A )|| 2 F ], ∀i ∈ {1, 2, 3},<label>(2)</label></formula><p>where E is an expectation operator, |E (x S )| is the number of elements in E (x S ), and || • || F denotes the Frobenius norm. Unlike the previously reported work <ref type="bibr" target="#b33">[33]</ref> promotes consistent output-layer predictions, the loss L con in Eq. ( <ref type="formula" target="#formula_2">1</ref>) directly encourages the encoder E to extract cross-domain consistent representations, which improves model generalization more effectively for dense prediction tasks <ref type="bibr" target="#b7">[8]</ref>, such as lesion detection. We hypothesize that forcing similar feature encoding between x S and x i A can facilitate image content preservation for lesion detection. In addition, we adopt a mean squared error (MSE) to measure the consistency, different from <ref type="bibr" target="#b33">[33]</ref> using the Kullback-Leibler divergence for image classification, which is not suitable for our application. Note that the convolution weights in module A are randomly sampled within each iteration and are not updated during model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Patch Gradient Reversal</head><p>Because of random convolution weights, the original synthesized X S and augmented X A data can have substantially different image appearances. Consequently, the use of the loss L con in Eq. (1) may not be sufficient to enforce consistent feature encoding. To address this issue, we propose to use a pretext task as an additional information resource for the encoder E and to further promote domain-agnostic representation learning. Specifically, we incorporate a domain classifier C on top of the encoder E to perform a pretext task of domain discrimination, i.e., predict whether each input image is from the original synthesized data X S or augmented data X A . This domain classification accompanies the main task of lesion detection (see Fig. <ref type="figure" target="#fig_1">2</ref>) to assist with feature learning. In this way, the encoder E improves feature invariance to domain changes by penalizing domain classification accuracy, while retaining feature discriminativeness to lesion prediction via the decoder D. This is different from other methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref> that use intrinsic supervision signals within a single image to perform an auxiliary task, e.g., solving jigsaw puzzles, for model generalization enhancement.</p><p>In general, the classifier C will encourage the encoder E to learn discriminative features for accurate domain classification. In order to make features invariant to different domains, we reverse the gradient propagated from the domain classifier C with a multiplication of -1 <ref type="bibr" target="#b2">[3]</ref> and send this reversed gradient to the encoder E , while keeping all the other gradient flows unchanged during the backpropagation for model training. Note that the computation in forward propagation of our network is the same as that in a standard feed-forward neural network. Compared with <ref type="bibr" target="#b2">[3]</ref>, we make the following significant improvements: 1) Instead of back propagating the reversed gradient from a single-valued prediction of the domain label of the entire input image, we introduce a patch-based gradient reversal to enhance feature representation invariance to local texture or style changes. Inspired by <ref type="bibr" target="#b8">[9]</ref>, we design the domain classifier C with a fully convolutional network and produce a prediction map, where each element corresponds to a local patch of input image, i.e., conducting small patch categorization. We then apply the reversal operation to the gradient propagated from the prediction map and feed it into the encoder E for feature learning. 2) Motivated by <ref type="bibr" target="#b16">[17]</ref>, we remove the sigmoid layer in <ref type="bibr" target="#b2">[3]</ref> and replace the cross-entropy loss by an MSE loss, which can facilitate the adversarial training caused by the gradient reversal. With the MSE loss, the patch-based gradient reversal penalizes image structures and enhances feature robustness and invariance to style shifts at the local-patch level, so that the lesion detection model H (i.e., E followed by D) learned with source data annotations is directly applicable to unseen domains <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>, based on the covariate shift assumption <ref type="bibr" target="#b19">[20]</ref>.</p><p>Formally, let X = {X S , X A } denote the input data for the encoder E and Z = {Z S , Z A } represent the corresponding domain category labels, with Z S and Z A for the original source images X S and corresponding random convolutionaugmented image X A , respectively. Each label z ∈ Z is a 3D image with all voxel intensity being 0's for z ∈ Z S or 1's for z ∈ Z A . We define the domain classification objective L cls as follows</p><formula xml:id="formula_4">L cls = E (x,z )∼(X ,Z ) [ 1 |z| ||z -ẑ|| 2 F ],<label>(3)</label></formula><p>where ẑ = C (E (x )) is the prediction of x . For source-domain data (X S , Y S ), the augmented images X A have the same gold-standard lesion labels Y A = Y S , each of which is a 3D binary image with 1 s for lesion voxels and 0 s for non-lesion regions. Let Y = {Y S , Y A }. We formulate the lesion detection objective L det as</p><formula xml:id="formula_5">L det = βE (x,y )∼(X ,Y ) [ -1 |y| |y | j=1 (γy j log ŷj + (1 -y j ) log(1 -ŷj ))] +E (x,y )∼(X ,Y ) [1 - 2 |y | j=1 y j ŷj + |y | j=1 y j + |ŷ | j=1 ŷj + ],<label>(4)</label></formula><p>where the first and second terms in Eq. ( <ref type="formula" target="#formula_5">4</ref>) are a weighted binary cross-entropy loss and a Dice loss, respectively. We add a smooth term, = 10 -6 , to the Dice loss to avoid division by zero. The y j and ŷj are the j-th values of y and corresponding prediction ŷ, respectively. The β controls the relative importance between the two losses, and γ emphasizes the lesions in each image. The combo loss L det can further help address the data imbalance issue <ref type="bibr" target="#b21">[22]</ref>, i.e., lesions have significantly fewer voxels than the non-lesion regions including the background. With the losses in Eqs. ( <ref type="formula" target="#formula_2">1</ref>)-( <ref type="formula" target="#formula_5">4</ref>), we define the following full objective as</p><formula xml:id="formula_6">L = L det + λ con L con + λ cls L cls , (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where λ con and λ cls are weighting parameters. Note that while we minimize L for model training, we reverse the gradient propagated from the domain classifier C before sending it to the encoder E during the backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. We evaluate the proposed method with multiple 68 Ga-DOTATATE PET liver NET image datasets that are acquired using different PET/CT scanners and/or imaging protocols. The synthesized source-domain dataset contains 103 simulated subjects, with an average of 5 lesions and 153 transverse slices per subject. This dataset is synthesized using list mode data from a single real, healthy subject acquired on a GE Discovery MI PET/CT scanner with list mode reconstruction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">37]</ref>. We collect two additional real 68 Ga-DOTATATE PET liver NET image datasets that serve as unseen domains. The first dataset (Real1) has 123 real subjects with about 230 hepatic lesions in total and is acquired using clinical reconstructions with a photomultiplier tube-based PET/CT scanner (GE Discovery STE). The second real-world dataset (Real2) consists of 65 cases with around 113 lesions and is acquired from clinical reconstructions using a digital PET/CT scanner (GE Discovery MI). Following <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">38]</ref>, we randomly split the synthesized dataset and the Real1 dataset into 60%, 20% and 20% for training, validation and testing, respectively. Due to the relatively small size of Real2, we use a two-fold cross-validation for model evaluation on this dataset. Here we split the real datasets to learn fully supervised models for a comparison with the proposed method. Implementation Details and Evaluation Metrics. We implement the encoder E and the decoder D with a U-Net architecture <ref type="bibr" target="#b18">[19]</ref>, with four downsampling and upsampling layers in the encoder and decoder, respectively. We build the domain classifier C using three stacked stride-1 convolutional layers of kernel size of 4, and each convolution is followed by a batch normalization and a leaky ReLU activation <ref type="bibr" target="#b15">[16]</ref>. We set β = 6, γ = 5 in Eq. ( <ref type="formula" target="#formula_5">4</ref>) and λ con = 1, λ cls = 1 in Eq. <ref type="bibr" target="#b4">(5)</ref>. We train the model using stochastic gradient descent with Nesterov momentum with learning rate = 5 × 10 -4 , momentum = 0.99 and batch size = 1. We perform standard image augmentation including random scaling, noise adding and image contrast adjustment before applying random convolutions in the module A. In the testing stage, we adopt the model H to produce a prediction map for each input image, and identify lesions with a threshold (i.e., 0.1) to binarize the map followed by a connected component analysis, which helps detect individual lesions by identifying connected regions from the binarized map. We use precision, recall and F 1 score as model evaluation metrics <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">38]</ref>.</p><p>Comparison with State of the Art. We compare our method with several recent state-of-the-art SDG approaches, including causality-inspired SDG (CISDG) <ref type="bibr" target="#b17">[18]</ref>, RandConv <ref type="bibr" target="#b33">[33]</ref>, and learning to diversify (L2D) <ref type="bibr" target="#b26">[27]</ref>. We run each model 5 times with different random seeds and report the mean and standard deviation. Table <ref type="table" target="#tab_0">1</ref> presents the comparison results on the two unseen-domain datasets. Our method significantly outperforms the state-of-the-art approaches in terms of F 1 score, with p-value &lt; 0.05 in Student's t-test for almost all cases on both datasets. In addition, our method gives lower standard deviation of F 1 than others. This indicates that compared with the competitor approaches, our method is relatively more effective and stable in learning generalizable representations for lesion detection in a very challenging situation, i.e., learning with a single-source synthesized PET image dataset to generalize to real clinical data.</p><p>The qualitative results are provided in the Supplementary Material.</p><p>Ablation Study. In Table <ref type="table" target="#tab_0">1</ref>, the Baseline represents a lesion detection model trained with the source data but without the data augmentation module A, L con or L cls . We then evaluate different variants of our method by sequentially adding one component to the Baseline model: 1) using only the module A for model training (Aug.), 2) using module A and L con (Aug.+L con ), and 3) using module A, L con and L cls (Ours). We also report the performance of the model, Aug.+L con +gGR, which does not use the proposed patch-based gradient reversal but outputs a single-value prediction for the entire input image, i.e., global gradient reversal (gGR). The U pper-bound means training with real-world images and gold-standard labels from the testing datasets. We note that using the data augmentation module A can significantly improve the lesion detection performance compared with the Baseline on the Real1 dataset, and combining data augmentation and patch gradient reversal can further close the gap to the U pper-bound model. Our method also outperforms the Baseline model by a large margin on the Real2 dataset, suggesting the effectiveness of our method. Effects of Parameters. We evaluate the effects of λ con and λ cls of our method on lesion detection in Fig. <ref type="figure" target="#fig_2">3</ref>. The lesion detection performance improves when increasing λ con from 0.1 to 1. However, a further emphasis on consistent feature encoding, e.g., λ con ≥ 5, decreases the F 1 score. This suggests the importance of an appropriate λ con value. In addition, we observe a similar trend of the F 1 curve for the λ cls , especially for the Real1 dataset, and this indicates the necessity of the domain classification pretext task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a novel SDG framework that uses only a single dataset for hepatic lesion detection in real clinical PET images, without any human data annotations. With a specific data augmentation module and a new patch-based gradient reversal, the framework can learn domain-invariant representations and generalize to unseen domains. The experiments show that our method outperforms the reference baseline and recent state-of-the-art SDG approaches on cross-scanner or -protocol real PET image datasets. A potential limitation may be the need of a proper selection of weights for different tasks during model training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example PET images. Diseased subjects with (a) simulated and (c) real lesions (red arrows), and normal (b) synthesized and (d) real subjects without lesions. (Color figure online)</figDesc><graphic coords="2,78,48,54,65,295,24,70,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed SDG framework for generalizable lesion detection. The A, E , D and C represents the data augmentation module, feature encoder, decoder and domain classifier, respectively. The L det , L cls and Lcon denote the losses for lesion detection, domain classification and cross-domain consistency, respectively.</figDesc><graphic coords="3,104,31,53,72,215,68,110,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The F1 score of our method with different values of λcon (left) and λ cls (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Domain generalization evaluation on different datasets. Each method is run 5 times, and the mean and standard deviation (std) of each metric (%) are reported: mean (std). The * means a statistically significant difference (p-value &lt; 0.05) between our method and others. The highest F1 score is highlighted with bold font.</figDesc><table><row><cell></cell><cell>Real1</cell><cell></cell><cell>Real2</cell></row><row><cell></cell><cell>F1</cell><cell>Precision Recall</cell><cell>F1</cell><cell>Precision Recall</cell></row><row><cell>CMSDG [18]</cell><cell cols="4">57.7* (3.2) 64.2 (13.0) 54.6 (7.9) 49.8 (6.5) 45.9 (8.3) 56.5 (9.8)</cell></row><row><cell>RandConv [33]</cell><cell cols="4">58.1* (1.3) 72.8 (10.4) 49.4 (5.9) 48.1* (2.3) 81.2 (4.4) 34.8 (2.1)</cell></row><row><cell>L2D [27]</cell><cell cols="4">46.4* (1.6) 41.1 (4.8) 54.7 (7.6) 41.9* (4.4) 30.1 (4.5) 71.0 (3.6)</cell></row><row><cell>Baseline</cell><cell cols="4">39.4* (1.9) 30.9 (2.4) 54.6 (2.2) 45.8* (5.0) 66.7 (10.0) 37.4 (4.2)</cell></row><row><cell>Aug.</cell><cell cols="4">51.5* (5.0) 45.4 (7.4) 60.3 (1.1) 44.3* (3.7) 53.5 (12.6) 41.6 (8.3)</cell></row><row><cell>Aug.+Lcon</cell><cell cols="4">55.1* (3.3) 60.0 (14.5) 52.3 (7.5) 44.5* (2.2) 67.0 (12.3) 35.0 (5.5)</cell></row><row><cell cols="5">Aug.+Lcon+gGR 58.7* (1.8) 62.5 (4.7) 55.5 (3.2) 48.2* (1.7) 69.6 (6.5) 37.8 (3.5)</cell></row><row><cell>Ours</cell><cell cols="4">63.1 (0.5) 74.2 (9.2) 55.6 (5.4) 53.8 (2.1) 58.0 (4.3) 50.9 (6.1)</cell></row><row><cell>U pper-bound</cell><cell cols="4">75.5 (4.3) 81.7 (3.6) 70.6 (7.0) 63.5 (7.6) 57.5 (7.3) 75.7 (5.1)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 12.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalizing nucleus recognition model in multi-source Ki67 immunohistochemistry stained images via domain-specific pruning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-327" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="277" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>D'innocente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
		<respStmt>
			<orgName>ICLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation for medical image analysis: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TBME</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The first MICCAI challenge on pet tumor segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ouahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="177" to="195" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">AugMix: a simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
		<respStmt>
			<orgName>ICLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeepLesionBrain: towards a broader deep-learning generalization for multiple sclerosis lesion segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Kamraoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page">102312</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A physics-guided modular deep-learning based automated framework for tumor segmentation in pet</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">245032</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Domain generalization for medical imaging classification with lineardependency regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3118" to="3129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain generalization for mammography detection via multi-style and multi-view contrastive learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-210" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="98" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Single-domain generalization in medical image segmentation via test-time adaptation from shape dictionary</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1756" to="1764" />
		</imprint>
		<respStmt>
			<orgName>AAAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shape-aware meta-learning for generalizing prostate MRI segmentation to unseen domains</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_46</idno>
		<idno>978-3-030-59713-9 46</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="475" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Causality-inspired single-source domain generalization for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Stat. Plan. Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lesion detection and characterization with context driven approximation in thoracic FDG PET-CT images of NSCLC studies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="408" to="421" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combo loss: handling input and output imbalance in multiorgan segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Taghanaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CMIG</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="24" to="33" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain generalization for prostate segmentation in transrectal ultrasound images: a multi-center study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vesal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102620</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
			<biblScope unit="page" from="10506" to="10518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning from extrinsic and intrinsic supervisions for domain generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-710" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="159" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A generalizable and robust deep learning algorithm for mitosis detection in multicenter breast histopathological images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">102703</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to diversify for single domain generalization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="834" to="843" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated liver lesion detection in 68Ga DOTATATE PET/CT using a deep fully convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wehrend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EJNMMI Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey of unsupervised deep domain adaptation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep label fusion: a generalizable hybrid multi-atlas and deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Wisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khandelwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedIA</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">102683</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved domain generalization for cell detection in histopathology images via test-time stain augmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-715" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="150" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial consistency for single domain generalization in medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="671" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-164" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Robust and generalizable visual representation learning via random convolutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
		<respStmt>
			<orgName>ICLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning domainagnostic visual representation for computational pathology using medicallyirrelevant style transfer augmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Banda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3945" to="3954" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">CutMix: regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">mixup: beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
		<respStmt>
			<orgName>ICLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimization-based image reconstruction from low-count, listmode TOF-pet data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TBME</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="936" to="946" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep neural network for automatic characterization of lesions on 68Ga-PSMA-11 PET/CT</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EJNMMI</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="603" to="613" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain generalization: a survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4396" to="4415" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
